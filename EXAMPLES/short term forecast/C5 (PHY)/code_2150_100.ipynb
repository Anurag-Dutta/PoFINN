{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.140000\n",
       "1       88.911429\n",
       "2       88.682857\n",
       "3       88.454286\n",
       "4       88.225714\n",
       "          ...    \n",
       "2245    61.612981\n",
       "2246    61.605056\n",
       "2247    61.597131\n",
       "2248    61.589205\n",
       "2249    61.581280\n",
       "Name: C5, Length: 2250, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c5_interpolated_2150_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.140000\n",
       "1       88.911429\n",
       "2       88.682857\n",
       "3       88.454286\n",
       "4       88.225714\n",
       "          ...    \n",
       "2145     1.209929\n",
       "2146     0.710071\n",
       "2147     0.204438\n",
       "2148     0.000000\n",
       "2149     0.000000\n",
       "Name: C5, Length: 2150, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2150)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.140000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.911429</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.682857</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.454286</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.225714</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     89.140000  0.000298\n",
       "1     88.911429  0.000298\n",
       "2     88.682857  0.000297\n",
       "3     88.454286  0.000297\n",
       "4     88.225714  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.8407  # Value for C0\n",
    "K0 = -0.0015  # Value for K0\n",
    "K1 = -0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0118    # Value for b\n",
    "c = 2.5775    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    89.140000    88.911429    88.682857    88.454286    88.225714   \n",
      "351    88.911429    88.682857    88.454286    88.225714    87.997143   \n",
      "352    88.682857    88.454286    88.225714    87.997143    87.768571   \n",
      "353    88.454286    88.225714    87.997143    87.768571    87.540000   \n",
      "354    88.225714    87.997143    87.768571    87.540000    87.311429   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.997143    87.768571    87.540000    87.311429    87.095798  ...   \n",
      "351    87.768571    87.540000    87.311429    87.095798    87.039776  ...   \n",
      "352    87.540000    87.311429    87.095798    87.039776    86.983754  ...   \n",
      "353    87.311429    87.095798    87.039776    86.983754    86.927731  ...   \n",
      "354    87.095798    87.039776    86.983754    86.927731    86.871709  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   79.161181    0.000263   79.132236    0.000263   79.103291    0.000263   \n",
      "351   79.132236    0.000263   79.103291    0.000263   79.074346    0.000262   \n",
      "352   79.103291    0.000263   79.074346    0.000262   79.045401    0.000262   \n",
      "353   79.074346    0.000262   79.045401    0.000262   79.016457    0.000262   \n",
      "354   79.045401    0.000262   79.016457    0.000262   78.987512    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   79.074346    0.000262   79.045401    0.000262  \n",
      "351   79.045401    0.000262   79.016457    0.000262  \n",
      "352   79.016457    0.000262   78.987512    0.000262  \n",
      "353   78.987512    0.000262   78.958567    0.000262  \n",
      "354   78.958567    0.000262   78.929622    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1700, 1, 251) (1700, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 2s 24ms/step - loss: 5125.5435 - val_loss: 3943.7644\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5021.6123 - val_loss: 3880.3940\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4943.7314 - val_loss: 3808.7930\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4863.1719 - val_loss: 3743.7173\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4787.7402 - val_loss: 3679.8794\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4713.6030 - val_loss: 3617.1379\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4640.5820 - val_loss: 3555.3662\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4568.5605 - val_loss: 3494.4937\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4497.4775 - val_loss: 3434.4788\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4427.2935 - val_loss: 3375.2949\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4357.9800 - val_loss: 3316.9202\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4289.5190 - val_loss: 3259.3403\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4221.8921 - val_loss: 3202.5405\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4155.0874 - val_loss: 3146.5095\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4089.0925 - val_loss: 3091.2378\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4023.8962 - val_loss: 3036.7148\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3959.4885 - val_loss: 2982.9316\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3895.8625 - val_loss: 2929.8804\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3833.0066 - val_loss: 2877.5535\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 3770.9150 - val_loss: 2825.9419\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 3709.5779 - val_loss: 2775.0400\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3648.9897 - val_loss: 2724.8389\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3589.1423 - val_loss: 2675.3323\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3530.0271 - val_loss: 2626.5142\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3471.6394 - val_loss: 2578.3762\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3413.9707 - val_loss: 2530.9133\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3357.0144 - val_loss: 2484.1179\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 3300.7651 - val_loss: 2437.9846\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3245.2153 - val_loss: 2392.5063\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3190.3589 - val_loss: 2347.6772\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3136.1904 - val_loss: 2303.4915\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3082.7021 - val_loss: 2259.9419\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3029.8887 - val_loss: 2217.0234\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2977.7439 - val_loss: 2174.7307\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2926.2625 - val_loss: 2133.0571\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2875.4377 - val_loss: 2091.9973\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2825.2637 - val_loss: 2051.5452\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2775.7354 - val_loss: 2011.6951\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2726.8469 - val_loss: 1972.4419\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2678.5918 - val_loss: 1933.7795\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2630.9653 - val_loss: 1895.7034\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2583.9612 - val_loss: 1858.2068\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2537.5752 - val_loss: 1821.2852\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 2491.8005 - val_loss: 1784.9329\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2446.6323 - val_loss: 1749.1445\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2402.0652 - val_loss: 1713.9149\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2358.0940 - val_loss: 1679.2391\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2314.7131 - val_loss: 1645.1112\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2271.9172 - val_loss: 1611.5270\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 2229.7026 - val_loss: 1578.4811\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2188.0620 - val_loss: 1545.9678\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2146.9919 - val_loss: 1513.9830\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2106.4861 - val_loss: 1482.5211\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2066.5405 - val_loss: 1451.5769\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2027.1494 - val_loss: 1421.1464\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1988.3088 - val_loss: 1391.2238\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1950.0132 - val_loss: 1361.8049\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1912.2573 - val_loss: 1332.8839\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1875.0375 - val_loss: 1304.4572\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1838.3478 - val_loss: 1276.5195\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1802.1840 - val_loss: 1249.0657\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1766.5411 - val_loss: 1222.0914\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1731.4139 - val_loss: 1195.5918\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1696.7998 - val_loss: 1169.5627\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1662.6921 - val_loss: 1143.9990\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1629.0868 - val_loss: 1118.8969\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1595.9796 - val_loss: 1094.2506\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1563.3652 - val_loss: 1070.0553\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1531.2399 - val_loss: 1046.3082\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1499.5986 - val_loss: 1023.0037\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1468.4371 - val_loss: 1000.1367\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1437.7510 - val_loss: 977.7044\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1407.5363 - val_loss: 955.7010\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1377.7876 - val_loss: 934.1223\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1348.5007 - val_loss: 912.9642\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1319.6722 - val_loss: 892.2227\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1291.2969 - val_loss: 871.8919\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1263.3705 - val_loss: 851.9691\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1235.8892 - val_loss: 832.4493\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1208.8484 - val_loss: 813.3280\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1182.2443 - val_loss: 794.6017\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1156.0718 - val_loss: 776.2656\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1130.3279 - val_loss: 758.3152\n",
      "Epoch 84/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1105.0079 - val_loss: 740.7465\n",
      "Epoch 85/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1080.1069 - val_loss: 723.5546\n",
      "Epoch 86/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1055.6215 - val_loss: 706.7368\n",
      "Epoch 87/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1031.5474 - val_loss: 690.2877\n",
      "Epoch 88/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1007.8802 - val_loss: 674.2032\n",
      "Epoch 89/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 984.6163 - val_loss: 658.4797\n",
      "Epoch 90/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 961.7512 - val_loss: 643.1124\n",
      "Epoch 91/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 939.2812 - val_loss: 628.0977\n",
      "Epoch 92/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 917.2022 - val_loss: 613.4312\n",
      "Epoch 93/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 895.5100 - val_loss: 599.1091\n",
      "Epoch 94/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 874.2010 - val_loss: 585.1265\n",
      "Epoch 95/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 853.2705 - val_loss: 571.4798\n",
      "Epoch 96/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 832.7151 - val_loss: 558.1656\n",
      "Epoch 97/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 812.5302 - val_loss: 545.1786\n",
      "Epoch 98/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 792.7127 - val_loss: 532.5157\n",
      "Epoch 99/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 773.2581 - val_loss: 520.1721\n",
      "Epoch 100/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 754.1626 - val_loss: 508.1441\n",
      "Epoch 101/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 735.4220 - val_loss: 496.4276\n",
      "Epoch 102/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 717.0328 - val_loss: 485.0190\n",
      "Epoch 103/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 698.9911 - val_loss: 473.9135\n",
      "Epoch 104/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 681.2925 - val_loss: 463.1078\n",
      "Epoch 105/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 663.9339 - val_loss: 452.5977\n",
      "Epoch 106/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 646.9108 - val_loss: 442.3788\n",
      "Epoch 107/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 630.2194 - val_loss: 432.4481\n",
      "Epoch 108/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 613.8564 - val_loss: 422.8001\n",
      "Epoch 109/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 597.8176 - val_loss: 413.4323\n",
      "Epoch 110/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 582.0990 - val_loss: 404.3401\n",
      "Epoch 111/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 566.6970 - val_loss: 395.5201\n",
      "Epoch 112/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 551.6083 - val_loss: 386.9673\n",
      "Epoch 113/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 536.8281 - val_loss: 378.6784\n",
      "Epoch 114/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 522.3534 - val_loss: 370.6497\n",
      "Epoch 115/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 508.1800 - val_loss: 362.8772\n",
      "Epoch 116/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 494.3044 - val_loss: 355.3565\n",
      "Epoch 117/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 480.7228 - val_loss: 348.0844\n",
      "Epoch 118/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 467.4314 - val_loss: 341.0566\n",
      "Epoch 119/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 454.4266 - val_loss: 334.2691\n",
      "Epoch 120/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 441.7046 - val_loss: 327.7183\n",
      "Epoch 121/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 429.2617 - val_loss: 321.4004\n",
      "Epoch 122/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 417.0943 - val_loss: 315.3116\n",
      "Epoch 123/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 405.1986 - val_loss: 309.4476\n",
      "Epoch 124/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 393.5710 - val_loss: 303.8050\n",
      "Epoch 125/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 382.2077 - val_loss: 298.3793\n",
      "Epoch 126/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 371.1048 - val_loss: 293.1675\n",
      "Epoch 127/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 360.2594 - val_loss: 288.1657\n",
      "Epoch 128/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 349.6673 - val_loss: 283.3696\n",
      "Epoch 129/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 339.3249 - val_loss: 278.7758\n",
      "Epoch 130/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 329.2288 - val_loss: 274.3805\n",
      "Epoch 131/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 319.3753 - val_loss: 270.1796\n",
      "Epoch 132/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 309.7607 - val_loss: 266.1696\n",
      "Epoch 133/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 300.3814 - val_loss: 262.3470\n",
      "Epoch 134/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 291.2340 - val_loss: 258.7075\n",
      "Epoch 135/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 282.3150 - val_loss: 255.2480\n",
      "Epoch 136/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 273.6206 - val_loss: 251.9642\n",
      "Epoch 137/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 265.1475 - val_loss: 248.8528\n",
      "Epoch 138/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 256.8922 - val_loss: 245.9098\n",
      "Epoch 139/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 248.8508 - val_loss: 243.1319\n",
      "Epoch 140/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 241.0201 - val_loss: 240.5150\n",
      "Epoch 141/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 233.3963 - val_loss: 238.0557\n",
      "Epoch 142/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 225.9764 - val_loss: 235.7505\n",
      "Epoch 143/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 218.7565 - val_loss: 233.5954\n",
      "Epoch 144/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 211.7332 - val_loss: 231.5870\n",
      "Epoch 145/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 204.9031 - val_loss: 229.7218\n",
      "Epoch 146/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 198.2628 - val_loss: 227.9961\n",
      "Epoch 147/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 191.8089 - val_loss: 226.4065\n",
      "Epoch 148/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 185.5384 - val_loss: 224.9494\n",
      "Epoch 149/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 179.4476 - val_loss: 223.6212\n",
      "Epoch 150/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 173.5329 - val_loss: 222.4187\n",
      "Epoch 151/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 167.7913 - val_loss: 221.3380\n",
      "Epoch 152/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 162.2192 - val_loss: 220.3759\n",
      "Epoch 153/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 156.8135 - val_loss: 219.5290\n",
      "Epoch 154/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 151.5708 - val_loss: 218.7938\n",
      "Epoch 155/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 146.4880 - val_loss: 218.1669\n",
      "Epoch 156/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 141.5616 - val_loss: 217.6451\n",
      "Epoch 157/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 136.7885 - val_loss: 217.2249\n",
      "Epoch 158/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 132.1657 - val_loss: 216.9032\n",
      "Epoch 159/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 127.6898 - val_loss: 216.6765\n",
      "Epoch 160/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 123.3576 - val_loss: 216.5416\n",
      "Epoch 161/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 119.1659 - val_loss: 216.4955\n",
      "Epoch 162/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 115.1119 - val_loss: 216.5348\n",
      "Epoch 163/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 111.1925 - val_loss: 216.6563\n",
      "Epoch 164/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 107.4044 - val_loss: 216.8571\n",
      "Epoch 165/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 103.7444 - val_loss: 217.1339\n",
      "Epoch 166/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 100.2096 - val_loss: 217.4838\n",
      "Epoch 167/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 96.7971 - val_loss: 217.9036\n",
      "Epoch 168/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 93.5042 - val_loss: 218.3905\n",
      "Epoch 169/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 90.3277 - val_loss: 218.9415\n",
      "Epoch 170/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 87.2647 - val_loss: 219.5536\n",
      "Epoch 171/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 84.3124 - val_loss: 220.2240\n",
      "Epoch 172/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 81.4678 - val_loss: 220.9499\n",
      "Epoch 173/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 78.7283 - val_loss: 221.7285\n",
      "Epoch 174/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 76.0909 - val_loss: 222.5570\n",
      "Epoch 175/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 73.5530 - val_loss: 223.4328\n",
      "Epoch 176/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 71.1119 - val_loss: 224.3531\n",
      "Epoch 177/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 68.7647 - val_loss: 225.3155\n",
      "Epoch 178/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 66.5092 - val_loss: 226.3171\n",
      "Epoch 179/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 64.3426 - val_loss: 227.3557\n",
      "Epoch 180/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 62.2620 - val_loss: 228.4286\n",
      "Epoch 181/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 60.2652 - val_loss: 229.5336\n",
      "Epoch 182/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 58.3497 - val_loss: 230.6679\n",
      "Epoch 183/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 56.5131 - val_loss: 231.8296\n",
      "Epoch 184/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 54.7528 - val_loss: 233.0162\n",
      "Epoch 185/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 53.0666 - val_loss: 234.2255\n",
      "Epoch 186/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 51.4520 - val_loss: 235.4556\n",
      "Epoch 187/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 49.9067 - val_loss: 236.7039\n",
      "Epoch 188/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 48.4285 - val_loss: 237.9687\n",
      "Epoch 189/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 47.0151 - val_loss: 239.2478\n",
      "Epoch 190/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 45.6645 - val_loss: 240.5391\n",
      "Epoch 191/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 44.3746 - val_loss: 241.8410\n",
      "Epoch 192/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 43.1430 - val_loss: 243.1515\n",
      "Epoch 193/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 41.9678 - val_loss: 244.4689\n",
      "Epoch 194/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 40.8470 - val_loss: 245.7912\n",
      "Epoch 195/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 39.7787 - val_loss: 247.1172\n",
      "Epoch 196/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 38.7608 - val_loss: 248.4448\n",
      "Epoch 197/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 37.7916 - val_loss: 249.7724\n",
      "Epoch 198/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 36.8692 - val_loss: 251.0989\n",
      "Epoch 199/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 35.9917 - val_loss: 252.4225\n",
      "Epoch 200/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 35.1576 - val_loss: 253.7419\n",
      "Epoch 201/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 34.3651 - val_loss: 255.0559\n",
      "Epoch 202/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 33.6123 - val_loss: 256.3628\n",
      "Epoch 203/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.8979 - val_loss: 257.6618\n",
      "Epoch 204/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2202 - val_loss: 258.9515\n",
      "Epoch 205/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 31.5776 - val_loss: 260.2308\n",
      "Epoch 206/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 30.9687 - val_loss: 261.4988\n",
      "Epoch 207/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 30.3920 - val_loss: 262.7541\n",
      "Epoch 208/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 29.8462 - val_loss: 263.9964\n",
      "Epoch 209/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 29.3298 - val_loss: 265.2243\n",
      "Epoch 210/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 28.8415 - val_loss: 266.4369\n",
      "Epoch 211/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 28.3801 - val_loss: 267.6335\n",
      "Epoch 212/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 27.9444 - val_loss: 268.8136\n",
      "Epoch 213/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 27.5331 - val_loss: 269.9764\n",
      "Epoch 214/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 27.1451 - val_loss: 271.1212\n",
      "Epoch 215/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 26.7792 - val_loss: 272.2472\n",
      "Epoch 216/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 26.4345 - val_loss: 273.3543\n",
      "Epoch 217/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 26.1097 - val_loss: 274.4417\n",
      "Epoch 218/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 25.8041 - val_loss: 275.5091\n",
      "Epoch 219/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 25.5166 - val_loss: 276.5561\n",
      "Epoch 220/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 25.2462 - val_loss: 277.5820\n",
      "Epoch 221/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.9921 - val_loss: 278.5872\n",
      "Epoch 222/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.7535 - val_loss: 279.5706\n",
      "Epoch 223/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.5295 - val_loss: 280.5327\n",
      "Epoch 224/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3193 - val_loss: 281.4732\n",
      "Epoch 225/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.1222 - val_loss: 282.3914\n",
      "Epoch 226/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 23.9374 - val_loss: 283.2881\n",
      "Epoch 227/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 23.7644 - val_loss: 284.1622\n",
      "Epoch 228/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.6023 - val_loss: 285.0146\n",
      "Epoch 229/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 23.4507 - val_loss: 285.8444\n",
      "Epoch 230/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 23.3089 - val_loss: 286.6525\n",
      "Epoch 231/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 23.1763 - val_loss: 287.4386\n",
      "Epoch 232/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.0523 - val_loss: 288.2027\n",
      "Epoch 233/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9366 - val_loss: 288.9448\n",
      "Epoch 234/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.8285 - val_loss: 289.6654\n",
      "Epoch 235/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 22.7277 - val_loss: 290.3643\n",
      "Epoch 236/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.6336 - val_loss: 291.0421\n",
      "Epoch 237/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.5459 - val_loss: 291.6991\n",
      "Epoch 238/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.4641 - val_loss: 292.3348\n",
      "Epoch 239/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.3880 - val_loss: 292.9503\n",
      "Epoch 240/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.3171 - val_loss: 293.5453\n",
      "Epoch 241/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.2511 - val_loss: 294.1205\n",
      "Epoch 242/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.1896 - val_loss: 294.6757\n",
      "Epoch 243/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.1325 - val_loss: 295.2116\n",
      "Epoch 244/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.0793 - val_loss: 295.7289\n",
      "Epoch 245/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.0299 - val_loss: 296.2274\n",
      "Epoch 246/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.9840 - val_loss: 296.7081\n",
      "Epoch 247/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.9413 - val_loss: 297.1703\n",
      "Epoch 248/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.9016 - val_loss: 297.6149\n",
      "Epoch 249/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.8648 - val_loss: 298.0427\n",
      "Epoch 250/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.8306 - val_loss: 298.4539\n",
      "Epoch 251/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.7989 - val_loss: 298.8484\n",
      "Epoch 252/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.7695 - val_loss: 299.2270\n",
      "Epoch 253/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 21.7422 - val_loss: 299.5904\n",
      "Epoch 254/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.7169 - val_loss: 299.9386\n",
      "Epoch 255/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.6934 - val_loss: 300.2723\n",
      "Epoch 256/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.6716 - val_loss: 300.5913\n",
      "Epoch 257/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.6515 - val_loss: 300.8972\n",
      "Epoch 258/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.6327 - val_loss: 301.1891\n",
      "Epoch 259/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.6153 - val_loss: 301.4680\n",
      "Epoch 260/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.5993 - val_loss: 301.7350\n",
      "Epoch 261/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.5844 - val_loss: 301.9892\n",
      "Epoch 262/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 21.5706 - val_loss: 302.2314\n",
      "Epoch 263/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.5579 - val_loss: 302.4626\n",
      "Epoch 264/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.5460 - val_loss: 302.6829\n",
      "Epoch 265/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.5351 - val_loss: 302.8928\n",
      "Epoch 266/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.5250 - val_loss: 303.0922\n",
      "Epoch 267/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.5156 - val_loss: 303.2813\n",
      "Epoch 268/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.5069 - val_loss: 303.4614\n",
      "Epoch 269/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4989 - val_loss: 303.6322\n",
      "Epoch 270/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4916 - val_loss: 303.7945\n",
      "Epoch 271/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4847 - val_loss: 303.9482\n",
      "Epoch 272/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 21.4784 - val_loss: 304.0935\n",
      "Epoch 273/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4726 - val_loss: 304.2317\n",
      "Epoch 274/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4672 - val_loss: 304.3621\n",
      "Epoch 275/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 21.4623 - val_loss: 304.4852\n",
      "Epoch 276/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 21.4577 - val_loss: 304.6019\n",
      "Epoch 277/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 21.4536 - val_loss: 304.7120\n",
      "Epoch 278/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4497 - val_loss: 304.8164\n",
      "Epoch 279/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4462 - val_loss: 304.9146\n",
      "Epoch 280/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 21.4429 - val_loss: 305.0067\n",
      "Epoch 281/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 21.4400 - val_loss: 305.0938\n",
      "Epoch 282/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 21.4373 - val_loss: 305.1758\n",
      "Epoch 283/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 21.4348 - val_loss: 305.2529\n",
      "Epoch 284/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 21.4326 - val_loss: 305.3254\n",
      "Epoch 285/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4305 - val_loss: 305.3936\n",
      "Epoch 286/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 21.4286 - val_loss: 305.4574\n",
      "Epoch 287/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 21.4269 - val_loss: 305.5172\n",
      "Epoch 288/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4254 - val_loss: 305.5732\n",
      "Epoch 289/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4241 - val_loss: 305.6255\n",
      "Epoch 290/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4229 - val_loss: 305.6748\n",
      "Epoch 291/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4218 - val_loss: 305.7209\n",
      "Epoch 292/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4208 - val_loss: 305.7637\n",
      "Epoch 293/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4200 - val_loss: 305.8036\n",
      "Epoch 294/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4193 - val_loss: 305.8412\n",
      "Epoch 295/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4186 - val_loss: 305.8756\n",
      "Epoch 296/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4182 - val_loss: 305.9081\n",
      "Epoch 297/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4178 - val_loss: 305.9384\n",
      "Epoch 298/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4174 - val_loss: 305.9665\n",
      "Epoch 299/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4171 - val_loss: 305.9923\n",
      "Epoch 300/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4169 - val_loss: 306.0166\n",
      "Epoch 301/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 21.4167 - val_loss: 306.0386\n",
      "Epoch 302/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4167 - val_loss: 306.0592\n",
      "Epoch 303/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4167 - val_loss: 306.0782\n",
      "Epoch 304/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4167 - val_loss: 306.0959\n",
      "Epoch 305/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4168 - val_loss: 306.1121\n",
      "Epoch 306/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4169 - val_loss: 306.1266\n",
      "Epoch 307/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4171 - val_loss: 306.1405\n",
      "Epoch 308/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4173 - val_loss: 306.1528\n",
      "Epoch 309/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4176 - val_loss: 306.1641\n",
      "Epoch 310/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4179 - val_loss: 306.1749\n",
      "Epoch 311/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4182 - val_loss: 306.1846\n",
      "Epoch 312/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4186 - val_loss: 306.1937\n",
      "Epoch 313/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4189 - val_loss: 306.2016\n",
      "Epoch 314/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 21.4193 - val_loss: 306.2090\n",
      "Epoch 315/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4197 - val_loss: 306.2157\n",
      "Epoch 316/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4201 - val_loss: 306.2214\n",
      "Epoch 317/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 21.4206 - val_loss: 306.2270\n",
      "Epoch 318/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4211 - val_loss: 306.2317\n",
      "Epoch 319/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4216 - val_loss: 306.2363\n",
      "Epoch 320/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4220 - val_loss: 306.2402\n",
      "Epoch 321/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4225 - val_loss: 306.2432\n",
      "Epoch 322/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4231 - val_loss: 306.2466\n",
      "Epoch 323/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4236 - val_loss: 306.2491\n",
      "Epoch 324/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4242 - val_loss: 306.2513\n",
      "Epoch 325/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4247 - val_loss: 306.2533\n",
      "Epoch 326/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 21.4253 - val_loss: 306.2548\n",
      "Epoch 327/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4259 - val_loss: 306.2563\n",
      "Epoch 328/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4265 - val_loss: 306.2573\n",
      "Epoch 329/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4270 - val_loss: 306.2584\n",
      "Epoch 330/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4276 - val_loss: 306.2589\n",
      "Epoch 331/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4282 - val_loss: 306.2595\n",
      "Epoch 332/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4288 - val_loss: 306.2597\n",
      "Epoch 333/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 21.4293 - val_loss: 306.2597\n",
      "Epoch 334/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4299 - val_loss: 306.2599\n",
      "Epoch 335/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4305 - val_loss: 306.2596\n",
      "Epoch 336/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4310 - val_loss: 306.2592\n",
      "Epoch 337/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4317 - val_loss: 306.2591\n",
      "Epoch 338/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4322 - val_loss: 306.2587\n",
      "Epoch 339/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4328 - val_loss: 306.2582\n",
      "Epoch 340/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4334 - val_loss: 306.2578\n",
      "Epoch 341/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4340 - val_loss: 306.2575\n",
      "Epoch 342/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4345 - val_loss: 306.2567\n",
      "Epoch 343/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4350 - val_loss: 306.2559\n",
      "Epoch 344/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4356 - val_loss: 306.2548\n",
      "Epoch 345/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 21.4362 - val_loss: 306.2538\n",
      "Epoch 346/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4368 - val_loss: 306.2532\n",
      "Epoch 347/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4372 - val_loss: 306.2519\n",
      "Epoch 348/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4378 - val_loss: 306.2510\n",
      "Epoch 349/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4383 - val_loss: 306.2496\n",
      "Epoch 350/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4389 - val_loss: 306.2486\n",
      "Epoch 351/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4395 - val_loss: 306.2474\n",
      "Epoch 352/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4400 - val_loss: 306.2464\n",
      "Epoch 353/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4405 - val_loss: 306.2451\n",
      "Epoch 354/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4410 - val_loss: 306.2440\n",
      "Epoch 355/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4415 - val_loss: 306.2428\n",
      "Epoch 356/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4420 - val_loss: 306.2416\n",
      "Epoch 357/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4425 - val_loss: 306.2403\n",
      "Epoch 358/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4430 - val_loss: 306.2393\n",
      "Epoch 359/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4434 - val_loss: 306.2376\n",
      "Epoch 360/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 21.4440 - val_loss: 306.2368\n",
      "Epoch 361/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4445 - val_loss: 306.2355\n",
      "Epoch 362/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4449 - val_loss: 306.2346\n",
      "Epoch 363/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4454 - val_loss: 306.2332\n",
      "Epoch 364/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4459 - val_loss: 306.2322\n",
      "Epoch 365/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4463 - val_loss: 306.2307\n",
      "Epoch 366/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4468 - val_loss: 306.2298\n",
      "Epoch 367/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4472 - val_loss: 306.2286\n",
      "Epoch 368/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4476 - val_loss: 306.2275\n",
      "Epoch 369/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4480 - val_loss: 306.2263\n",
      "Epoch 370/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4485 - val_loss: 306.2252\n",
      "Epoch 371/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4489 - val_loss: 306.2242\n",
      "Epoch 372/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4493 - val_loss: 306.2229\n",
      "Epoch 373/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 21.4498 - val_loss: 306.2221\n",
      "Epoch 374/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4501 - val_loss: 306.2210\n",
      "Epoch 375/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4505 - val_loss: 306.2201\n",
      "Epoch 376/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4509 - val_loss: 306.2191\n",
      "Epoch 377/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4513 - val_loss: 306.2180\n",
      "Epoch 378/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4516 - val_loss: 306.2171\n",
      "Epoch 379/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4521 - val_loss: 306.2161\n",
      "Epoch 380/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4524 - val_loss: 306.2153\n",
      "Epoch 381/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4528 - val_loss: 306.2144\n",
      "Epoch 382/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4531 - val_loss: 306.2134\n",
      "Epoch 383/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4534 - val_loss: 306.2125\n",
      "Epoch 384/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4538 - val_loss: 306.2115\n",
      "Epoch 385/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4541 - val_loss: 306.2108\n",
      "Epoch 386/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4544 - val_loss: 306.2098\n",
      "Epoch 387/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 21.4548 - val_loss: 306.2086\n",
      "Epoch 388/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4551 - val_loss: 306.2078\n",
      "Epoch 389/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4554 - val_loss: 306.2068\n",
      "Epoch 390/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4558 - val_loss: 306.2061\n",
      "Epoch 391/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4560 - val_loss: 306.2051\n",
      "Epoch 392/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4564 - val_loss: 306.2044\n",
      "Epoch 393/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4566 - val_loss: 306.2037\n",
      "Epoch 394/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4569 - val_loss: 306.2033\n",
      "Epoch 395/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4572 - val_loss: 306.2024\n",
      "Epoch 396/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4575 - val_loss: 306.2012\n",
      "Epoch 397/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4578 - val_loss: 306.2005\n",
      "Epoch 398/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4581 - val_loss: 306.1994\n",
      "Epoch 399/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4584 - val_loss: 306.1990\n",
      "Epoch 400/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 21.4586 - val_loss: 306.1988\n",
      "Epoch 401/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4589 - val_loss: 306.1979\n",
      "Epoch 402/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4591 - val_loss: 306.1968\n",
      "Epoch 403/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4594 - val_loss: 306.1964\n",
      "Epoch 404/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4596 - val_loss: 306.1959\n",
      "Epoch 405/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4598 - val_loss: 306.1953\n",
      "Epoch 406/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4601 - val_loss: 306.1945\n",
      "Epoch 407/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4603 - val_loss: 306.1938\n",
      "Epoch 408/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4606 - val_loss: 306.1931\n",
      "Epoch 409/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4608 - val_loss: 306.1923\n",
      "Epoch 410/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4610 - val_loss: 306.1919\n",
      "Epoch 411/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4612 - val_loss: 306.1916\n",
      "Epoch 412/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4614 - val_loss: 306.1912\n",
      "Epoch 413/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 21.4616 - val_loss: 306.1904\n",
      "Epoch 414/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4618 - val_loss: 306.1898\n",
      "Epoch 415/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4620 - val_loss: 306.1893\n",
      "Epoch 416/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4622 - val_loss: 306.1891\n",
      "Epoch 417/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4623 - val_loss: 306.1885\n",
      "Epoch 418/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4626 - val_loss: 306.1880\n",
      "Epoch 419/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4628 - val_loss: 306.1874\n",
      "Epoch 420/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4629 - val_loss: 306.1869\n",
      "Epoch 421/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4631 - val_loss: 306.1866\n",
      "Epoch 422/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4633 - val_loss: 306.1862\n",
      "Epoch 423/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4634 - val_loss: 306.1855\n",
      "Epoch 424/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4636 - val_loss: 306.1848\n",
      "Epoch 425/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4638 - val_loss: 306.1842\n",
      "Epoch 426/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 21.4640 - val_loss: 306.1840\n",
      "Epoch 427/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4641 - val_loss: 306.1837\n",
      "Epoch 428/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4643 - val_loss: 306.1833\n",
      "Epoch 429/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4644 - val_loss: 306.1825\n",
      "Epoch 430/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4646 - val_loss: 306.1823\n",
      "Epoch 431/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4647 - val_loss: 306.1819\n",
      "Epoch 432/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4649 - val_loss: 306.1815\n",
      "Epoch 433/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4650 - val_loss: 306.1811\n",
      "Epoch 434/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4651 - val_loss: 306.1804\n",
      "Epoch 435/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4653 - val_loss: 306.1798\n",
      "Epoch 436/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4654 - val_loss: 306.1793\n",
      "Epoch 437/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4656 - val_loss: 306.1791\n",
      "Epoch 438/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4657 - val_loss: 306.1790\n",
      "Epoch 439/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 21.4658 - val_loss: 306.1787\n",
      "Epoch 440/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4659 - val_loss: 306.1781\n",
      "Epoch 441/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4661 - val_loss: 306.1778\n",
      "Epoch 442/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4662 - val_loss: 306.1775\n",
      "Epoch 443/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4663 - val_loss: 306.1774\n",
      "Epoch 444/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4664 - val_loss: 306.1770\n",
      "Epoch 445/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4665 - val_loss: 306.1768\n",
      "Epoch 446/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4666 - val_loss: 306.1764\n",
      "Epoch 447/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4667 - val_loss: 306.1761\n",
      "Epoch 448/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4668 - val_loss: 306.1760\n",
      "Epoch 449/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4669 - val_loss: 306.1757\n",
      "Epoch 450/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4670 - val_loss: 306.1753\n",
      "Epoch 451/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 21.4671 - val_loss: 306.1750\n",
      "Epoch 452/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4672 - val_loss: 306.1744\n",
      "Epoch 453/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4673 - val_loss: 306.1742\n",
      "Epoch 454/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4674 - val_loss: 306.1737\n",
      "Epoch 455/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4676 - val_loss: 306.1737\n",
      "Epoch 456/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4676 - val_loss: 306.1733\n",
      "Epoch 457/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4677 - val_loss: 306.1730\n",
      "Epoch 458/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4678 - val_loss: 306.1730\n",
      "Epoch 459/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4678 - val_loss: 306.1727\n",
      "Epoch 460/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4679 - val_loss: 306.1725\n",
      "Epoch 461/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4680 - val_loss: 306.1722\n",
      "Epoch 462/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4681 - val_loss: 306.1720\n",
      "Epoch 463/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 21.4681 - val_loss: 306.1716\n",
      "Epoch 464/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4682 - val_loss: 306.1714\n",
      "Epoch 465/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4683 - val_loss: 306.1711\n",
      "Epoch 466/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4684 - val_loss: 306.1710\n",
      "Epoch 467/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4685 - val_loss: 306.1705\n",
      "Epoch 468/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4685 - val_loss: 306.1701\n",
      "Epoch 469/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4686 - val_loss: 306.1699\n",
      "Epoch 470/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4686 - val_loss: 306.1695\n",
      "Epoch 471/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4688 - val_loss: 306.1694\n",
      "Epoch 472/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4688 - val_loss: 306.1694\n",
      "Epoch 473/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4689 - val_loss: 306.1694\n",
      "Epoch 474/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4689 - val_loss: 306.1693\n",
      "Epoch 475/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 21.4690 - val_loss: 306.1691\n",
      "Epoch 476/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4691 - val_loss: 306.1690\n",
      "Epoch 477/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4691 - val_loss: 306.1688\n",
      "Epoch 478/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4692 - val_loss: 306.1685\n",
      "Epoch 479/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4692 - val_loss: 306.1679\n",
      "Epoch 480/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4693 - val_loss: 306.1679\n",
      "Epoch 481/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4693 - val_loss: 306.1674\n",
      "Epoch 482/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4694 - val_loss: 306.1671\n",
      "Epoch 483/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4694 - val_loss: 306.1670\n",
      "Epoch 484/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4695 - val_loss: 306.1669\n",
      "Epoch 485/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4695 - val_loss: 306.1667\n",
      "Epoch 486/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4696 - val_loss: 306.1667\n",
      "Epoch 487/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 21.4696 - val_loss: 306.1665\n",
      "Epoch 488/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4696 - val_loss: 306.1663\n",
      "Epoch 489/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4697 - val_loss: 306.1663\n",
      "Epoch 490/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4697 - val_loss: 306.1659\n",
      "Epoch 491/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4698 - val_loss: 306.1659\n",
      "Epoch 492/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4699 - val_loss: 306.1658\n",
      "Epoch 493/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4699 - val_loss: 306.1658\n",
      "Epoch 494/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4699 - val_loss: 306.1658\n",
      "Epoch 495/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4699 - val_loss: 306.1653\n",
      "Epoch 496/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4700 - val_loss: 306.1655\n",
      "Epoch 497/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 21.4700 - val_loss: 306.1652\n",
      "Epoch 498/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 21.4700 - val_loss: 306.1652\n",
      "Epoch 499/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4701 - val_loss: 306.1648\n",
      "Epoch 500/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4701 - val_loss: 306.1648\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.8407, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0015, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0118, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.5775, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 450ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.57586601e+01, 6.57390523e+01, 6.57194444e+01, 6.56998366e+01,\n",
       "        6.56802288e+01, 6.56606209e+01, 6.56410131e+01, 6.56214052e+01,\n",
       "        6.56017974e+01, 6.55821895e+01, 6.55625817e+01, 6.55429739e+01,\n",
       "        6.55233660e+01, 6.89433473e+01, 6.88509104e+01, 6.87584734e+01,\n",
       "        6.86656036e+01, 6.85735994e+01, 6.84811625e+01, 6.83887255e+01,\n",
       "        6.82962885e+01, 6.82038515e+01, 6.80792017e+01, 6.79731513e+01,\n",
       "        6.78771008e+01, 6.77810504e+01, 6.76850000e+01, 6.75889496e+01,\n",
       "        6.74928984e+01, 6.73968468e+01, 6.73007952e+01, 6.72047436e+01,\n",
       "        6.71086920e+01, 6.70126404e+01, 6.69165888e+01, 6.68205372e+01,\n",
       "        6.67244856e+01, 6.66284340e+01, 6.65323824e+01, 6.64363308e+01,\n",
       "        6.63402792e+01, 6.62442276e+01, 6.61481760e+01, 6.60521244e+01,\n",
       "        6.59560728e+01, 6.58600212e+01, 6.57639696e+01, 6.77290616e+01,\n",
       "        6.76030112e+01, 6.74769608e+01, 6.73509104e+01, 6.72248599e+01,\n",
       "        6.70988095e+01, 6.69727591e+01, 6.68467087e+01, 6.67206583e+01,\n",
       "        6.66437909e+01, 6.65756400e+01, 6.65093371e+01, 6.64421102e+01,\n",
       "        6.63748833e+01, 6.63076564e+01, 6.62404295e+01, 6.61732026e+01,\n",
       "        6.61059757e+01, 6.60387488e+01, 6.59715219e+01, 6.59042951e+01,\n",
       "        6.58921335e+01, 6.58837302e+01, 6.58753268e+01, 6.58669234e+01,\n",
       "        6.58585201e+01, 7.39060822e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.69568800e-02, 0.00000000e+00, 0.00000000e+00, 7.89334118e-01,\n",
       "        6.73275681e+01, 8.21176618e-02, 2.89564043e-01, 0.00000000e+00,\n",
       "        5.25830090e-01, 0.00000000e+00, 2.19398841e-01, 0.00000000e+00,\n",
       "        1.33891419e-01, 3.85468423e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 9.29413199e-01, 8.49735737e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.52344757e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([62.36586732, 62.3579422 , 62.35001708, 62.34209196, 62.33416684,\n",
       "       62.32624172, 62.31831659, 62.31039147, 62.30246635, 62.29454123,\n",
       "       62.28661611, 62.27869099, 62.27076587, 62.26284075, 62.25491562,\n",
       "       62.2469905 , 62.23906538, 62.23114026, 62.22321514, 62.21529002,\n",
       "       62.2073649 , 62.19943978, 62.19151465, 62.18358953, 62.17566441,\n",
       "       62.16773929, 62.15981417, 62.15188905, 62.14396393, 62.13603881,\n",
       "       62.12811368, 62.12018856, 62.11226344, 62.10433832, 62.0964132 ,\n",
       "       62.08848808, 62.08056296, 62.07263784, 62.06471271, 62.05678759,\n",
       "       62.04886247, 62.04093735, 62.03301223, 62.02508711, 62.01716199,\n",
       "       62.00923687, 62.00131174, 61.99338662, 61.9854615 , 61.97753638,\n",
       "       61.96961126, 61.96168614, 61.95376102, 61.9458359 , 61.93791077,\n",
       "       61.92998565, 61.92206053, 61.91413541, 61.90621029, 61.89828517,\n",
       "       61.89036005, 61.88243493, 61.8745098 , 61.86658468, 61.85865956,\n",
       "       61.85073444, 61.84280932, 61.8348842 , 61.82695908, 61.81903396,\n",
       "       61.81110883, 61.80318371, 61.79525859, 61.78733347, 61.77940835,\n",
       "       61.77148323, 61.76355811, 61.75563298, 61.74770786, 61.73978274,\n",
       "       61.73185762, 61.7239325 , 61.71600738, 61.70808226, 61.70015714,\n",
       "       61.69223201, 61.68430689, 61.67638177, 61.66845665, 61.66053153,\n",
       "       61.65260641, 61.64468129, 61.63675617, 61.62883104, 61.62090592,\n",
       "       61.6129808 , 61.60505568, 61.59713056, 61.58920544, 61.58128032])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.063608031272963\n",
      "18.972254383937198\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
