{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.140000\n",
       "1       88.911429\n",
       "2       88.682857\n",
       "3       88.454286\n",
       "4       88.225714\n",
       "          ...    \n",
       "1845    64.758753\n",
       "1846    64.748483\n",
       "1847    64.738212\n",
       "1848    64.727941\n",
       "1849    64.717670\n",
       "Name: C5, Length: 1850, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c5_interpolated_1750_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.140000\n",
       "1       88.911429\n",
       "2       88.682857\n",
       "3       88.454286\n",
       "4       88.225714\n",
       "          ...    \n",
       "1745    65.536438\n",
       "1746    65.529902\n",
       "1747    65.523366\n",
       "1748    65.516830\n",
       "1749    65.510294\n",
       "Name: C5, Length: 1750, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1750)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.140000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.911429</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.682857</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.454286</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.225714</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     89.140000  0.000298\n",
       "1     88.911429  0.000298\n",
       "2     88.682857  0.000297\n",
       "3     88.454286  0.000297\n",
       "4     88.225714  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAAsTAAALEwEAmpwYAAAkyUlEQVR4nO3deXxU9b3G8c93skISAgkhLAECsgYpWwQXwFYUwYpYFZVapdVebKtVi63FaxevvW1ve7VqUVtRbN0qKGrFilXEq4CCEBCIgEgIW8IWSFgDCSG/+8ccbEwjJGQ5M5Pn/XrlxZnfnJk8cxKeOfnNmTPmnENERCJXwO8AIiLSuFT0IiIRTkUvIhLhVPQiIhFORS8iEuGi/Q5QXdu2bV1mZqbfMUREwsry5cv3OOfSarou5Io+MzOTnJwcv2OIiIQVM9vyZddp6kZEJMKp6EVEIpyKXkQkwqnoRUQinIpeRCTC1arozWyMma03szwzm1rD9SPNbIWZVZjZVVXGB5rZYjNbY2arzeyahgwvIiKndsqiN7Mo4FFgLJAFTDSzrGqrbQW+Dfyt2ngpcINzrh8wBnjIzFrXM7OIiNRBbfbohwJ5zrl851w5MBMYX3UF59xm59xqoLLa+GfOuQ3e8nZgN1DjAf31deDoMR6c9xkrt+1rjLsXEQlbtSn6TsC2KpcLvLE6MbOhQCywsYbrJptZjpnlFBUV1fWuAXCV8PD8DeRsLj6t24uIRKomeTHWzDoAzwLfcc5VVr/eOTfdOZftnMtOSzu9Hf5WLaKJDhh7D5fXM62ISGSpTdEXAp2rXM7wxmrFzFoBbwD3OOeW1C1e7ZkZbRJiKT6kohcRqao2Rb8M6Glm3cwsFrgWmFObO/fWfxV4xjk3+/Rj1k5qQqz26EVEqjll0TvnKoBbgbeAdcCLzrk1ZnafmV0GYGZnmVkBMAF43MzWeDe/GhgJfNvMVnpfAxvjgQCkJMRSfLisse5eRCQs1erslc65ucDcamO/qLK8jOCUTvXbPQc8V8+MtZaaGEduwb6m+nYiImEhot4Zq6kbEZF/F1FFn5IQy8GjFZRX/NuBPSIizVbEFT1ASan26kVEToiook/1in6vDrEUEflcRBX9iT36Ys3Ti4h8LqKKPjXR26PXIZYiIp+LrKJPiAO0Ry8iUlVEFX1yixiiAqaiFxGpIqKKPhAw2rSM0bH0IiJVRFTRQ/AF2b2HNEcvInJCRBa9pm5ERP4l4oo+NSFOUzciIlVEXNFrj15E5Isirui7pLRkX+kxCkpK/Y4iIhISIq7oL+7XHoA3Vu/wOYmISGiIuKLvktqSARnJ/ENFLyICRGDRA4wb0JHcwv1s3nPY7ygiIr6LyKK/pH8HAP6xervPSURE/BeRRd+xdQuyu7bR9I2ICBFa9BCcvvl050E27DrodxQREV9FbNGP7d+egMHr2qsXkWYuYou+XVI8w7ql8o/V23HO+R1HRMQ3EVv0AJcP6kh+0WFmLy/wO4qIiG8iuuivGtKZc7qn8vPXPuHTnQf8jiMi4ouILvqogPHwxIG0io/h+8+t4ODRY35HEhFpchFd9BCcq582cRBbi0uZ+nKu5utFpNmJ+KIHGNY9lZ9c3Js3cnfw9Ieb/Y4jItKkalX0ZjbGzNabWZ6ZTa3h+pFmtsLMKszsqmrXTTKzDd7XpIYKXleTR3Tnwr7t+PXcdXy8tcSvGCIiTe6URW9mUcCjwFggC5hoZlnVVtsKfBv4W7XbpgC/BIYBQ4Ffmlmb+seuu0DAeGDCQNJbxXPL8yso0TnrRaSZqM0e/VAgzzmX75wrB2YC46uu4Jzb7JxbDVRWu+3FwDznXLFzrgSYB4xpgNynJbllDH+6bgh7DpXzoxdXUlmp+XoRiXy1KfpOwLYqlwu8sdqo1W3NbLKZ5ZhZTlFRUS3v+vT0z0jmF+OyeG99Ebe+sIIDOhJHRCJcSLwY65yb7pzLds5lp6WlNfr3u25YF+4e24e31uxi3LRFfFK4v9G/p4iIX2pT9IVA5yqXM7yx2qjPbRuNmXHz+Wcwa/LZlFdUcsVjH/Ls4s069FJEIlJtin4Z0NPMuplZLHAtMKeW9/8WMNrM2ngvwo72xkJCdmYKb9w2gvN6pPLz19Zw6wsf601VIhJxTln0zrkK4FaCBb0OeNE5t8bM7jOzywDM7CwzKwAmAI+b2RrvtsXArwg+WSwD7vPGQkZKQiwzJp3F1LF9+OcnO7l02iKdLkFEIoqF2nRFdna2y8nJ8eV752wu5gfPryAxPpp/3j6S2OiQeAlDROSUzGy5cy67puvUZFVkZ6bwP1f2J7/oMM8s3ux3HBGRBqGir+Zrvdtxfq80Hp6/gT2HyvyOIyJSbyr6asyMn1+axZHy4zzw9md+xxERqTcVfQ16tEtk0rmZzFy2lTXbdYy9iIQ3Ff2XuG1UT9q0jOW/Xl+r4+tFJKyp6L9EcosYfjy6N0s3FTM3d6ffcURETpuK/iSuOaszfTu04jdz13H02HG/44iInBYV/UlEBYxfjsuicN8Rpi/I9zuOiMhpUdGfwtndU/l6/w489l4e2/cd8TuOiEidqehrYerYPjgHd8xcyY79KnsRCS8q+lronNKS317Rn9zC/Yz+wwJeWLpVR+KISNhQ0dfSFYMzeOuOkZzZKZm7X8nlWzM+Yltxqd+xREROSUVfB11SW/L8d4fx62+cyapt+xn94AL++sEmfSShiIQ0FX0dBQLGdcO68taPRjK0Wwr3vr6Wa6YvJr/okN/RRERqpKI/TZ1at+Cv3zmL+ycMYP3Og4x9eCHTF2zkuPbuRSTEqOjrwcy4akgG70w5n5G90vjN3E+54k8f8tmug35HExH5nIq+AbRrFc/064cwbeIgthWX8vU/LmTa/A0cO17pdzQRERV9QzEzxg3oyLwfjWTMmR14YN5njH/kA539UkR8p6JvYKmJcUybOIjHrx9C0aEyrvzTh8xft8vvWCLSjKnoG8nF/doz97YR9GyXxORnlzNr2Va/I4lIM6Wib0RpSXHMnHw25/Voy09fzuXhdzboHbUi0uRU9I0sIS6aGZOyuWJwJx585zP+89VPqNCLtCLShKL9DtAcxEQFeGDCADokx/Po/22k6GAZ0yYOokVslN/RRKQZ0B59EzEzfnJxH+4b34/5n+7iuieXUHK43O9YItIMqOib2A3nZPKn6wbzyfYDXPnnD3ViNBFpdCp6H4w5swPP3TSMPQeDh1/qWHsRaUwqep8M7ZbC7O+fS1TAuObxJXyYt8fvSCISoVT0PuqVnsQrPziXTq1bMOkvS3ltZaHfkUQkAtWq6M1sjJmtN7M8M5taw/VxZjbLu/4jM8v0xmPM7GkzyzWzdWZ2dwPnD3sdklvw4vfOYXCXNtw+cyVP6EPIRaSBnbLozSwKeBQYC2QBE80sq9pqNwElzrkewIPA77zxCUCcc64/MAS4+cSTgPxLcosYnr5xKF/v34Ffz13Hf/9jrT7MREQaTG326IcCec65fOdcOTATGF9tnfHA097ybGCUmRnggAQziwZaAOXAgQZJHmHiY6KYNnEQ3z43kycXbeL2WSspqzjudywRiQC1KfpOwLYqlwu8sRrXcc5VAPuBVIKlfxjYAWwF7nfOFVf/BmY22cxyzCynqKiozg8iUgQCxi/HZXH32D68vmo7335qGQeOHvM7loiEucZ+MXYocBzoCHQD7jSz7tVXcs5Nd85lO+ey09LSGjlSaDMzbj7/DB68ZgDLNhdz9Z8Xs+vAUb9jiUgYq03RFwKdq1zO8MZqXMebpkkG9gLfBP7pnDvmnNsNfABk1zd0c/CNQRn85Ttnsa24lCse07H2InL6alP0y4CeZtbNzGKBa4E51daZA0zylq8C3nXB0zRuBS4AMLME4Gzg04YI3hyM6JnGrJvPoayikq//cRFXP76YF3O2cbiswu9oIhJGrDanzTWzS4CHgCjgKefcr83sPiDHOTfHzOKBZ4FBQDFwrXMu38wSgb8QPFrHgL845/73ZN8rOzvb5eTk1OcxRZy9h8qYuWwbs5cXsGnPYVrGRnFJ/w5cNSSDoZkpBALmd0QR8ZmZLXfO1ThjUquib0oq+i/nnGPF1hJeyingH6t3cKisgi4pLblycAZXDulERpuWfkcUEZ+o6CNQaXkFb63ZyUs5BXy4cS8A556RyoTsDMb066BTIIs0Myr6CFdQUsrLywuZvWIb24qPkBgXzaVf6cCE7AwGd2lD8C0NIhLJVPTNRGWlY+nmYl7KKWBu7g6OHDtO97YJXDkkgysHZ9A+Od7viCLSSFT0zdChsgrm5u5g9vIClm4qJmAwvGcaE4ZkMObM9sRE6Xx2IpFERd/Mbdl7mJeXF/DyikIK9x3hisGd+MPVA/2OJSIN6GRFr926ZqBragJTRvdm4V1f4wdfPYNXVhTy+qrtfscSkSaiom9GAgFjykW9GNi5Nfe8msv2fUf8jiQiTUBF38xERwV46JqBVFQ67nxxlU6HLNIMqOibocy2Cdw7rh+L8/fy5CJ90IlIpFPRN1PBN1a153/fWq8TpolEOBV9M2Vm/PaK/rRpGcsdM1dy9Jg+5EQkUqnom7E2CbHcP2EAG3Yf4n/e1ElFRSKVir6ZG9krje+cl8lfP9zMe+t3+x1HRBqBil746Zg+9E5P4iezV7P3UJnfcUSkganohfiYKB66diD7S48x9ZVcQu3d0iJSPyp6AaBvh1bcNaY389buYuaybae+gYiEDRW9fO7G87oxvEdb7nt9LZv2HPY7jog0EBW9fC4QMO6fMIDY6AB3zPyYY8cr/Y4kIg1ARS9f0D45nt9e0Z9VBfuZNn+D33FEpAGo6OXfnPjg8Uf+L4+czcV+xxGRelLRS43uvawfGW1acseslRw8eszvOCJSDyp6qVFiXDQPXjOA7fuOcO+ctX7HEZF6UNHLlxrSNYVbL+jJyysK9EElImFMRS8nddsFPRjcpTX/+Uou24pL/Y4jIqdBRS8nFR0V4OFrBwFw+8yPqdAhlyJhR0Uvp9Q5pSW/uaI/K7bu42EdcikSdlT0UivjBnRkgnfI5eKNe/2OIyJ1UKuiN7MxZrbezPLMbGoN18eZ2Szv+o/MLLPKdV8xs8VmtsbMcs0svgHzSxO697J+dEtN4EezVlJyuNzvOCJSS6csejOLAh4FxgJZwEQzy6q22k1AiXOuB/Ag8DvvttHAc8D3nHP9gK8COig7TCXERfPHiYPYe7iMu15erbNcioSJ2uzRDwXynHP5zrlyYCYwvto644GnveXZwCgzM2A0sNo5twrAObfXOafPrAtjZ3ZK5qdj+jBv7S6e+2ir33FEpBaia7FOJ6DqeWsLgGFfto5zrsLM9gOpQC/AmdlbQBow0zn3++rfwMwmA5MBunTpUtfHIE3sxvO6sXDDHu6ds4bZywvI7tqG7K5tGJLZhnZJmpkTCTW1Kfr63v9w4CygFJhvZsudc/OrruScmw5MB8jOztZ8QIgLBIyHrx3IEwvzWbaphOeWbGHGok0AdElp+XnpZ3dNoWe7RAIB8zmxSPNWm6IvBDpXuZzhjdW0ToE3L58M7CW497/AObcHwMzmAoOB+UhYa90ylp9c3AeA8opKPtm+n+WbS8jZUsyCDUW88nHwV6RVfDSDT+zxd01hYOfWtIiN8jO6SLNTm6JfBvQ0s24EC/1a4JvV1pkDTAIWA1cB7zrnTkzZ3GVmLYFy4HyCL9ZKBImNDjC4SxsGd2nDf9Ad5xxb9paSs6WE5VuKydlcwnvriwCIDhj9OrZiSNcUsjODTwDtWmm6R6QxWW2OnDCzS4CHgCjgKefcr83sPiDHOTfHO2TyWWAQUAxc65zL9277LeBuwAFznXN3nex7ZWdnu5ycnHo8JAlF+0rLWbG1hJzNJeRsKWHVtn2UVQTfZds5pQWj+qTzk4t7kxDX2LOJIpHJmxbPrvG6UDtETkXfPJRXVLJm+36Wbylh6aZi5q3bRfe2CTx63WD6tG/ldzyRsKOil5D3Yd4ebpsZPPf9feP7cXV2Z4JH6IpIbZys6HUKBAkJ5/Zoy9zbh5Od2YafvpzLlBdXcbiswu9YIhFBRS8ho11SPM/cOIwfXdiLv68sZNwji/h05wG/Y4mEPRW9hJSogHH7hT15/rvDOHi0gvGPfMCsZVt1ugWRelDRS0g694y2zL1thKZyRBqAil5CVlpSHM/cOIwpF/XiNU3liJw2Fb2EtKiAcduonjxXZSpn5lJN5YjUhYpewsKJqZyzMlOY+kouP5q1UlM5IrWkopewkZYUx9M3DmXKRb2Ys2o74x5ZxLodmsoRORUVvYSVE1M5z3/3bA4ereDyRzWVI3IqKnoJS+eckfqFqZw7Zq3kkKZyRGqkopewdWIq586LevH6qu1cNk1TOSI1UdFLWIsKGD88MZVTFpzK+fP7Gyn3zowpIip6iRDnnJHKm7ePYGSvNP7nzU+55I8LWbxxr9+xREKCil4iRtvEOJ64IZsZk7I5euw4E59Ywu0zP2b3gaN+RxPxlYpeIs6ovum8M+V8brugB2/m7mTUA+/z1KJNVBzXdI40Typ6iUjxMVFMGd2bt340koFdWnPfP9Yy7pEPWL6lxO9oIk1ORS8RrVvbBJ65cSiPXTeYksPlXPmnD7lr9ir2HirzO5pIk1HRS8QzMy7p34H5d57PzSO788qKQi544H2e/2gLxyv1RiuJfCp6aTYS4qK5+5K+zL19BH3aJ3HPq59wxWMfkFuw3+9oIo1KRS/NTq/0JGZOPpuHrhlI4b6jXPboIn7291z2lx7zO5pIo1DRS7NkZlw+qBPv/vh8Jp2Tyd8+2soFD7zH7OUFOm+ORBwVvTRrreJjuPeyfsy5dThdUlvy45dWcfXji3UqBYkoKnoR4MxOybz8vXP53ZX9ydt9iEunLeJX/1jLwaOazpHwp6IX8QQCxjVndeHdO7/K1dmdeeqDTYx64H3mrNqu6RwJayp6kWraJMTy2yv68+oPzqNdqzhue+FjrnvyI/J2H/I7mshpUdGLfImBnVvz2i3D+dX4fuQW7mfswwv4/T8/5Uj5cb+jidSJil7kJKICxvXnZPLunV9l3ICOPPbeRi78w/u8vWanpnMkbNSq6M1sjJmtN7M8M5taw/VxZjbLu/4jM8usdn0XMztkZj9uoNwiTSotKY4/XD2QWZPPJiEuisnPLue7T+ewrbjU72gip3TKojezKOBRYCyQBUw0s6xqq90ElDjnegAPAr+rdv0fgDfrH1fEX8O6p/LGbSO455K+LMnfy4V/eJ9p8zdQVqHpHAldtdmjHwrkOefynXPlwExgfLV1xgNPe8uzgVFmZgBmdjmwCVjTIIlFfBYTFeA/RnbnnTvP58K+6Tww7zPGPLSQBZ8V+R1NpEa1KfpOwLYqlwu8sRrXcc5VAPuBVDNLBH4K/NfJvoGZTTazHDPLKSrSfxYJDx2SW/DodYN55sahANzw1FJueX4FO/frg04ktDT2i7H3Ag865056XJpzbrpzLts5l52WltbIkUQa1sheafzzjhHceVEv3lm3i1EPvMeTC/M5pg86kRBRm6IvBDpXuZzhjdW4jplFA8nAXmAY8Hsz2wzcAfynmd1av8gioScuOoofjurJO1POZ1j3VP77jXVc+sdFLN1U7Hc0kVoV/TKgp5l1M7NY4FpgTrV15gCTvOWrgHdd0AjnXKZzLhN4CPiNc+6RhokuEno6p7RkxqRspl8/hENlFVz9+GLufHEVe/RBJ+KjUxa9N+d+K/AWsA540Tm3xszuM7PLvNVmEJyTzwOmAP92CKZIc2FmjO7XnnemnM8tXzuDOasKueD+93htZfU/hEWahoXamz6ys7NdTk6O3zFEGkze7kNMfXk1OVtKuGl4N+4e24foKL1XURqWmS13zmXXdJ1+20QaWY92ibww+Wy+fW4mMxZt4lszPtJUjjQpFb1IE4iJCnDvZf148JoBfLx1H+OmLWLVtn1+x5JmQkUv0oS+MSiDl79/LgEzJjy+mBeXbTv1jUTqSUUv0sTO7JTM6z8cztDMFO56eTU/+3su5RU65l4aj4pexAcpCbH89TtncfP53XluyVYmPrGEXQf0jlppHCp6EZ9ERwW4e2xfHvnmINbtOMCl0xaRs1lvsJKGp6IX8dmlX+nIqz84j4TYKCY+sYRnF2/Wue6lQanoRUJA7/ZJvHbrcIb3aMvPX1vDXbNXc/SYTn0sDUNFLxIiklvEMGPSWdw2qicvLS/g6scXU7jviN+xJAKo6EVCSCBgTLmoF0/ckE1+0WHGTVvEhxv3+B1LwpyKXiQEXZSVzmu3nkdKQizXz1jKkwvzNW8vp01FLxKizkhL5O+3nMdFfdP57zfWcfvMlRwqq/A7loShaL8DiMiXS4yL5k/fGsxj723kgbfXs2JrCV/r3Y62iXG0TYolNSGOtKRY2ibGkZoYR0JsFN6neIp8TkUvEuLMjFu+1oNh3VL41RvrmLNqO/uPHKtx3fiYwOeln5YYfAIIXv7XcltvOblFDIGAnhSaAxW9SJjIzkzhtVvOA6C8opLiw+XsOVTmfQWX91ZZLtx3lFUF+yk+XM7xyn+f348OGG0T4zijXQI92yXRu30SvdIT6ZmeRKv4mKZ+eNKIVPQiYSg2OkD75HjaJ8efct3KSse+I8eCTwgHy9hzuDz476Eydh44ysbdh3gxZxul5f86br9Dcjw905Po7RV/r/QkerZLJCFOlRGO9FMTiXCBgJGSEEtKQiy90pNqXKey0lG47wif7TrI+l0H2bDrEOt3HmRJ/t4vnHAto00LeqcneeWfSK/0JHq0SyQ+JqqpHo6cBhW9iBAIGJ1TWtI5pSWj+qZ/Pn680rG1uJT1Ow+yocqTwIINRRw7HpwOChgM6Nyam4Z3Y0y/9vr0rBCkjxIUkTo7drySzXsO89muQ6zfeYA5q7azeW8pnVNa8N3h3ZmQnUHLWO1HNqWTfZSgil5E6u14pWPe2p08viCfj7fuo3XLGG44uys3nJtJ28Q4v+M1Cyp6EWkyOZuLeXxBPvPW7iIuOsCVQzL4jxHd6dY2we9oEe1kRa+/rUSkQWVnppCdmcLGokM8uTCf2csLeGHpVkZnpTN55BkM6drG74jNjvboRaRRFR0s4+kPN/Pski3sP3KMIV3bMHlkdy7qm643bDUgTd2IiO8Ol1XwUs42nly0iYKSI3Rvm8B3R3TnisGddHhmA1DRi0jIqDheyZuf7GT6gnxyC/fTNjGWSedkcv05XWndMtbveGFLRS8iIcc5x+L8vUxfkM9764toERPFNWd15qbh3eic0tLveGFHRS8iIW39zoNMX5DPnFWFHK90XH92V6aM7k1yC51zp7ZOVvS1egubmY0xs/VmlmdmU2u4Ps7MZnnXf2Rmmd74RWa23MxyvX8vqNcjEZGI1Lt9Eg9cPYCFd13AdcO68uySLVxw/3u8mLONyhpOyCZ1c8qiN7Mo4FFgLJAFTDSzrGqr3QSUOOd6AA8Cv/PG9wDjnHP9gUnAsw0VXEQiT/vkeH51+Zm8/sPhZLZN4K7Zq7nyzx/ySeF+v6OFtdrs0Q8F8pxz+c65cmAmML7aOuOBp73l2cAoMzPn3MfOue3e+BqghZnpbXIiclL9Oibz0s3ncP+EAWwrLmXcI4u459Vc9pWW+x0tLNWm6DsB26pcLvDGalzHOVcB7AdSq61zJbDCOVdW/RuY2WQzyzGznKKiotpmF5EIFggYVw3JYP6dX2XSOZm8sHQrX7v/PV5YulXTOXXUJKeZM7N+BKdzbq7peufcdOdctnMuOy0trSkiiUiYSG4Rw72X9eON20bQs10Sd7+Syzce+4BV2/b5HS1s1KboC4HOVS5neGM1rmNm0UAysNe7nAG8CtzgnNtY38Ai0jz17dCKWTefzUPXDGT7/qNc/tgH3P3KaooPazrnVGpT9MuAnmbWzcxigWuBOdXWmUPwxVaAq4B3nXPOzFoDbwBTnXMfNFBmEWmmzIzLB3Xi3TvP56bzuvFiTgEXPPAezy3ZUuPHJUrQKYvem3O/FXgLWAe86JxbY2b3mdll3mozgFQzywOmACcOwbwV6AH8wsxWel/tGvxRiEizkhQfw88uzeLN20fQp30SP/v7J4x/dBErtpb4HS0k6Q1TIhLWnHO8vnoHv35jLbsOlDFhSAY/Hdun2Z0HX6cpFpGIZWZcNqAjF/Rpx7T5G5ixaBNvrdnJ97/ag0u/0kGnU0B79CISYfJ2H+TeOWtZlLcHCL6Ie1FWOqOz0unXsRVmkXlqZJ3rRkSanS17DzNv7S7eXrOLnC3FVDro1LrF56V/VrcUYiLog8xV9CLSrO09VMb8T3fz9ppdLNxQRFlFJa3ioxnVN1j6I3ulkRAX3jPZKnoREU9peQULN+zh7TW7mP/pLvaVHiM2OsDwHm0ZnZXOqL7ppCWF3wu5ejFWRMTTMjaai/u15+J+7ak4XknOlhLeXrOLt9fu5N1Pd2OWy+AubRidlc5FWel0T0v0O3K9aY9eRITgYZqf7jzI22t2MW/dTj4pPABAj3aJ9OvYipax0bSMjfK+TrIc5y3HBJeb6nUA7dGLiJyCmdG3Qyv6dmjF7Rf2pHDfEeat2ck763azcts+SsuPU1pWQemx49Rl/zgmymgRE0VCXPAJ4cxOyYzomcaInm1JbxXfeA+oCu3Ri4jUgXOOsopKDpdVBMu//Dil5V+y7D0xlHrr7j9yjBVbS9hzKHh+nl7piZ+X/rBuqbSIPf0PSdcevYhIAzEz4mOiiI+J+rdzsddGZWVwimjhhiIWbtjDs0u2MGPRJmKjAozul84j3xzc4JlV9CIiTSgQMLI6tiKrYytuPv8Mjh47ztJNxSzcUERsdOPM56voRUR8FB8TxcheaYzs1XifxRE5bwsTEZEaqehFRCKcil5EJMKp6EVEIpyKXkQkwqnoRUQinIpeRCTCqehFRCJcyJ3rxsyKgC31uIu2wJ4GitPYwikrhFfecMoK4ZU3nLJCeOWtT9auzrka33UVckVfX2aW82Un9gk14ZQVwitvOGWF8MobTlkhvPI2VlZN3YiIRDgVvYhIhIvEop/ud4A6CKesEF55wykrhFfecMoK4ZW3UbJG3By9iIh8USTu0YuISBUqehGRCBcxRW9mY8xsvZnlmdnUEMjT2cz+z8zWmtkaM7vdG7/XzArNbKX3dUmV29zt5V9vZhf7kHmzmeV6uXK8sRQzm2dmG7x/23jjZmZ/9PKuNrOG//yzL8/Zu8r2W2lmB8zsjlDatmb2lJntNrNPqozVeVua2SRv/Q1mNqmJ8/6vmX3qZXrVzFp745lmdqTKdv5zldsM8X6H8rzHZE2Utc4/+6bqjC/JO6tK1s1mttIbb5xt65wL+y8gCtgIdAdigVVAls+ZOgCDveUk4DMgC7gX+HEN62d5ueOAbt7jiWrizJuBttXGfg9M9ZanAr/zli8B3gQMOBv4yMef/U6gayhtW2AkMBj45HS3JZAC5Hv/tvGW2zRh3tFAtLf8uyp5M6uuV+1+lnqPwbzHNLaJstbpZ9+UnVFT3mrXPwD8ojG3baTs0Q8F8pxz+c65cmAmMN7PQM65Hc65Fd7yQWAd0OkkNxkPzHTOlTnnNgF5BB+X38YDT3vLTwOXVxl/xgUtAVqbWQcf8o0CNjrnTvZu6ibfts65BUBxDTnqsi0vBuY554qdcyXAPGBMU+V1zr3tnKvwLi4BMk52H17mVs65JS7YTM/wr8fYqFlP4st+9k3WGSfL6+2VXw28cLL7qO+2jZSi7wRsq3K5gJOXapMys0xgEPCRN3Sr9+fwUyf+fCc0HoMD3jaz5WY22RtLd87t8JZ3AunecijkBbiWL/4nCdVtC3XflqGSG+BGgnuRJ3Qzs4/N7H0zG+GNdSKY8YSmzluXn32obNsRwC7n3IYqYw2+bSOl6EOWmSUCLwN3OOcOAH8CzgAGAjsI/tkWKoY75wYDY4FbzGxk1Su9PYmQOR7XzGKBy4CXvKFQ3rZfEGrb8mTM7B6gAnjeG9oBdHHODQKmAH8zs1Z+5fOEzc++mol8cUelUbZtpBR9IdC5yuUMb8xXZhZDsOSfd869AuCc2+WcO+6cqwSe4F9TCL4/BudcoffvbuBVL9uuE1My3r+7vdV9z0vwCWmFc24XhPa29dR1W/qe28y+DVwKXOc9OeFNg+z1lpcTnOvu5WWrOr3TZHlP42cfCts2GrgCmHVirLG2baQU/TKgp5l18/byrgXm+BnIm3ubAaxzzv2hynjVeexvACdeiZ8DXGtmcWbWDehJ8MWXpsqbYGZJJ5YJvhD3iZfrxNEek4DXquS9wTti5Gxgf5Vpiabyhb2hUN22VdR1W74FjDazNt5UxGhvrEmY2RjgLuAy51xplfE0M4vylrsT3J75XuYDZna29/t/Q5XH2NhZ6/qzD4XOuBD41Dn3+ZRMo23bxniV2Y8vgkcufEbwGfCeEMgznOCf5quBld7XJcCzQK43PgfoUOU293j519MIRyucIm93gkcerALWnNiGQCowH9gAvAOkeOMGPOrlzQWymzhvArAXSK4yFjLbluAT0A7gGMH51JtOZ1sSnBvP876+08R58wjOY5/4/f2zt+6V3u/ISmAFMK7K/WQTLNmNwCN4775vgqx1/tk3VWfUlNcb/yvwvWrrNsq21SkQREQiXKRM3YiIyJdQ0YuIRDgVvYhIhFPRi4hEOBW9iEiEU9GLiEQ4Fb2ISIT7fxNe/TebWu7fAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.8407  # Value for C0\n",
    "K0 = -0.0015  # Value for K0\n",
    "K1 = -0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0118    # Value for b\n",
    "c = 2.5775    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    89.140000    88.911429    88.682857    88.454286    88.225714   \n",
      "351    88.911429    88.682857    88.454286    88.225714    87.997143   \n",
      "352    88.682857    88.454286    88.225714    87.997143    87.768571   \n",
      "353    88.454286    88.225714    87.997143    87.768571    87.540000   \n",
      "354    88.225714    87.997143    87.768571    87.540000    87.311429   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.997143    87.768571    87.540000    87.311429    87.095798  ...   \n",
      "351    87.768571    87.540000    87.311429    87.095798    87.039776  ...   \n",
      "352    87.540000    87.311429    87.095798    87.039776    86.983754  ...   \n",
      "353    87.311429    87.095798    87.039776    86.983754    86.927731  ...   \n",
      "354    87.095798    87.039776    86.983754    86.927731    86.871709  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   79.161181    0.000263   79.132236    0.000263   79.103291    0.000263   \n",
      "351   79.132236    0.000263   79.103291    0.000263   79.074346    0.000262   \n",
      "352   79.103291    0.000263   79.074346    0.000262   79.045401    0.000262   \n",
      "353   79.074346    0.000262   79.045401    0.000262   79.016457    0.000262   \n",
      "354   79.045401    0.000262   79.016457    0.000262   78.987512    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   79.074346    0.000262   79.045401    0.000262  \n",
      "351   79.045401    0.000262   79.016457    0.000262  \n",
      "352   79.016457    0.000262   78.987512    0.000262  \n",
      "353   78.987512    0.000262   78.958567    0.000262  \n",
      "354   78.958567    0.000262   78.929622    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1300, 1, 251) (1300, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "17/17 [==============================] - 3s 32ms/step - loss: 5346.6704 - val_loss: 4325.6558\n",
      "Epoch 2/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 5248.1499 - val_loss: 4262.3218\n",
      "Epoch 3/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 5193.2358 - val_loss: 4217.5913\n",
      "Epoch 4/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 5143.9868 - val_loss: 4173.2700\n",
      "Epoch 5/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 5095.2065 - val_loss: 4129.4004\n",
      "Epoch 6/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 5046.8838 - val_loss: 4085.9456\n",
      "Epoch 7/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4998.9785 - val_loss: 4042.8765\n",
      "Epoch 8/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 4951.4678 - val_loss: 4000.1738\n",
      "Epoch 9/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4904.3320 - val_loss: 3957.8252\n",
      "Epoch 10/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4857.5601 - val_loss: 3915.8210\n",
      "Epoch 11/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4811.1421 - val_loss: 3874.1560\n",
      "Epoch 12/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4765.0718 - val_loss: 3832.8232\n",
      "Epoch 13/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 4719.3447 - val_loss: 3791.8191\n",
      "Epoch 14/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 4673.9556 - val_loss: 3751.1389\n",
      "Epoch 15/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4628.9009 - val_loss: 3710.7798\n",
      "Epoch 16/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4584.1782 - val_loss: 3670.7393\n",
      "Epoch 17/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 4539.7827 - val_loss: 3631.0134\n",
      "Epoch 18/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4495.7124 - val_loss: 3591.6008\n",
      "Epoch 19/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4451.9644 - val_loss: 3552.4978\n",
      "Epoch 20/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 4408.5371 - val_loss: 3513.7031\n",
      "Epoch 21/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4365.4272 - val_loss: 3475.2139\n",
      "Epoch 22/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4322.6323 - val_loss: 3437.0278\n",
      "Epoch 23/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 4280.1509 - val_loss: 3399.1440\n",
      "Epoch 24/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4237.9814 - val_loss: 3361.5586\n",
      "Epoch 25/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4196.1196 - val_loss: 3324.2712\n",
      "Epoch 26/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4154.5654 - val_loss: 3287.2791\n",
      "Epoch 27/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4113.3164 - val_loss: 3250.5815\n",
      "Epoch 28/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4072.3706 - val_loss: 3214.1748\n",
      "Epoch 29/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4031.7256 - val_loss: 3178.0581\n",
      "Epoch 30/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3991.3804 - val_loss: 3142.2297\n",
      "Epoch 31/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3951.3330 - val_loss: 3106.6882\n",
      "Epoch 32/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 3911.5811 - val_loss: 3071.4312\n",
      "Epoch 33/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 3872.1238 - val_loss: 3036.4575\n",
      "Epoch 34/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 3832.9585 - val_loss: 3001.7646\n",
      "Epoch 35/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 3794.0845 - val_loss: 2967.3516\n",
      "Epoch 36/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 3755.4988 - val_loss: 2933.2168\n",
      "Epoch 37/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 3717.2009 - val_loss: 2899.3586\n",
      "Epoch 38/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 3679.1882 - val_loss: 2865.7744\n",
      "Epoch 39/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3641.4600 - val_loss: 2832.4639\n",
      "Epoch 40/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3604.0142 - val_loss: 2799.4250\n",
      "Epoch 41/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 3566.8494 - val_loss: 2766.6562\n",
      "Epoch 42/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3529.9631 - val_loss: 2734.1560\n",
      "Epoch 43/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 3493.3552 - val_loss: 2701.9229\n",
      "Epoch 44/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 3457.0229 - val_loss: 2669.9551\n",
      "Epoch 45/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3420.9653 - val_loss: 2638.2505\n",
      "Epoch 46/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 3385.1807 - val_loss: 2606.8093\n",
      "Epoch 47/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 3349.6677 - val_loss: 2575.6289\n",
      "Epoch 48/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 3314.4248 - val_loss: 2544.7078\n",
      "Epoch 49/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 3279.4504 - val_loss: 2514.0444\n",
      "Epoch 50/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 3244.7427 - val_loss: 2483.6379\n",
      "Epoch 51/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 3210.3010 - val_loss: 2453.4863\n",
      "Epoch 52/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 3176.1233 - val_loss: 2423.5886\n",
      "Epoch 53/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 3142.2085 - val_loss: 2393.9429\n",
      "Epoch 54/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 3108.5544 - val_loss: 2364.5479\n",
      "Epoch 55/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3075.1602 - val_loss: 2335.4028\n",
      "Epoch 56/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3042.0249 - val_loss: 2306.5059\n",
      "Epoch 57/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3009.1462 - val_loss: 2277.8547\n",
      "Epoch 58/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2976.5232 - val_loss: 2249.4497\n",
      "Epoch 59/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2944.1543 - val_loss: 2221.2886\n",
      "Epoch 60/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2912.0386 - val_loss: 2193.3696\n",
      "Epoch 61/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2880.1736 - val_loss: 2165.6924\n",
      "Epoch 62/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2848.5596 - val_loss: 2138.2546\n",
      "Epoch 63/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2817.1941 - val_loss: 2111.0554\n",
      "Epoch 64/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2786.0757 - val_loss: 2084.0938\n",
      "Epoch 65/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2755.2034 - val_loss: 2057.3677\n",
      "Epoch 66/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 2724.5757 - val_loss: 2030.8770\n",
      "Epoch 67/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2694.1919 - val_loss: 2004.6187\n",
      "Epoch 68/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 2664.0505 - val_loss: 1978.5928\n",
      "Epoch 69/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2634.1492 - val_loss: 1952.7981\n",
      "Epoch 70/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2604.4880 - val_loss: 1927.2332\n",
      "Epoch 71/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2575.0654 - val_loss: 1901.8956\n",
      "Epoch 72/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 2545.8801 - val_loss: 1876.7855\n",
      "Epoch 73/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2516.9297 - val_loss: 1851.9014\n",
      "Epoch 74/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 2488.2141 - val_loss: 1827.2413\n",
      "Epoch 75/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 2459.7312 - val_loss: 1802.8042\n",
      "Epoch 76/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2431.4812 - val_loss: 1778.5901\n",
      "Epoch 77/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 2403.4622 - val_loss: 1754.5964\n",
      "Epoch 78/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2375.6721 - val_loss: 1730.8226\n",
      "Epoch 79/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2348.1108 - val_loss: 1707.2671\n",
      "Epoch 80/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2320.7764 - val_loss: 1683.9283\n",
      "Epoch 81/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2293.6682 - val_loss: 1660.8059\n",
      "Epoch 82/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2266.7847 - val_loss: 1637.8983\n",
      "Epoch 83/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2240.1248 - val_loss: 1615.2045\n",
      "Epoch 84/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2213.6875 - val_loss: 1592.7230\n",
      "Epoch 85/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2187.4712 - val_loss: 1570.4534\n",
      "Epoch 86/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2161.4751 - val_loss: 1548.3936\n",
      "Epoch 87/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2135.6975 - val_loss: 1526.5422\n",
      "Epoch 88/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2110.1377 - val_loss: 1504.8995\n",
      "Epoch 89/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 2084.7952 - val_loss: 1483.4631\n",
      "Epoch 90/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 2059.6675 - val_loss: 1462.2324\n",
      "Epoch 91/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 2034.7540 - val_loss: 1441.2065\n",
      "Epoch 92/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 2010.0543 - val_loss: 1420.3833\n",
      "Epoch 93/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 1985.5663 - val_loss: 1399.7632\n",
      "Epoch 94/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 1961.2893 - val_loss: 1379.3433\n",
      "Epoch 95/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1937.2222 - val_loss: 1359.1238\n",
      "Epoch 96/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1913.3636 - val_loss: 1339.1035\n",
      "Epoch 97/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1889.7129 - val_loss: 1319.2805\n",
      "Epoch 98/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1866.2688 - val_loss: 1299.6543\n",
      "Epoch 99/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1843.0303 - val_loss: 1280.2236\n",
      "Epoch 100/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1819.9957 - val_loss: 1260.9880\n",
      "Epoch 101/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1797.1643 - val_loss: 1241.9452\n",
      "Epoch 102/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1774.5354 - val_loss: 1223.0950\n",
      "Epoch 103/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1752.1079 - val_loss: 1204.4363\n",
      "Epoch 104/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 1729.8802 - val_loss: 1185.9679\n",
      "Epoch 105/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 1707.8517 - val_loss: 1167.6880\n",
      "Epoch 106/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 1686.0209 - val_loss: 1149.5967\n",
      "Epoch 107/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1664.3870 - val_loss: 1131.6923\n",
      "Epoch 108/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1642.9491 - val_loss: 1113.9742\n",
      "Epoch 109/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 1621.7062 - val_loss: 1096.4408\n",
      "Epoch 110/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1600.6569 - val_loss: 1079.0909\n",
      "Epoch 111/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1579.8002 - val_loss: 1061.9249\n",
      "Epoch 112/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1559.1356 - val_loss: 1044.9406\n",
      "Epoch 113/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1538.6619 - val_loss: 1028.1364\n",
      "Epoch 114/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1518.3774 - val_loss: 1011.5124\n",
      "Epoch 115/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1498.2816 - val_loss: 995.0671\n",
      "Epoch 116/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1478.3737 - val_loss: 978.7996\n",
      "Epoch 117/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1458.6523 - val_loss: 962.7092\n",
      "Epoch 118/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1439.1171 - val_loss: 946.7940\n",
      "Epoch 119/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1419.7660 - val_loss: 931.0539\n",
      "Epoch 120/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1400.5986 - val_loss: 915.4875\n",
      "Epoch 121/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1381.6138 - val_loss: 900.0936\n",
      "Epoch 122/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1362.8112 - val_loss: 884.8716\n",
      "Epoch 123/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1344.1893 - val_loss: 869.8202\n",
      "Epoch 124/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 1325.7467 - val_loss: 854.9387\n",
      "Epoch 125/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1307.4830 - val_loss: 840.2259\n",
      "Epoch 126/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1289.3971 - val_loss: 825.6807\n",
      "Epoch 127/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1271.4882 - val_loss: 811.3022\n",
      "Epoch 128/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1253.7546 - val_loss: 797.0895\n",
      "Epoch 129/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1236.1960 - val_loss: 783.0416\n",
      "Epoch 130/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1218.8115 - val_loss: 769.1577\n",
      "Epoch 131/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1201.5999 - val_loss: 755.4364\n",
      "Epoch 132/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1184.5599 - val_loss: 741.8771\n",
      "Epoch 133/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1167.6914 - val_loss: 728.4789\n",
      "Epoch 134/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1150.9928 - val_loss: 715.2405\n",
      "Epoch 135/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1134.4635 - val_loss: 702.1608\n",
      "Epoch 136/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1118.1021 - val_loss: 689.2391\n",
      "Epoch 137/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1101.9082 - val_loss: 676.4747\n",
      "Epoch 138/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1085.8809 - val_loss: 663.8666\n",
      "Epoch 139/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1070.0186 - val_loss: 651.4135\n",
      "Epoch 140/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1054.3206 - val_loss: 639.1144\n",
      "Epoch 141/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1038.7861 - val_loss: 626.9680\n",
      "Epoch 142/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1023.4143 - val_loss: 614.9746\n",
      "Epoch 143/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1008.2042 - val_loss: 603.1322\n",
      "Epoch 144/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 993.1547 - val_loss: 591.4405\n",
      "Epoch 145/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 978.2649 - val_loss: 579.8975\n",
      "Epoch 146/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 963.5338 - val_loss: 568.5034\n",
      "Epoch 147/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 948.9610 - val_loss: 557.2568\n",
      "Epoch 148/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 934.5451 - val_loss: 546.1568\n",
      "Epoch 149/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 920.2853 - val_loss: 535.2023\n",
      "Epoch 150/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 906.1808 - val_loss: 524.3926\n",
      "Epoch 151/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 892.2307 - val_loss: 513.7271\n",
      "Epoch 152/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 878.4338 - val_loss: 503.2033\n",
      "Epoch 153/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 864.7892 - val_loss: 492.8221\n",
      "Epoch 154/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 851.2963 - val_loss: 482.5821\n",
      "Epoch 155/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 837.9539 - val_loss: 472.4813\n",
      "Epoch 156/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 824.7610 - val_loss: 462.5198\n",
      "Epoch 157/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 811.7173 - val_loss: 452.6962\n",
      "Epoch 158/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 798.8215 - val_loss: 443.0107\n",
      "Epoch 159/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 786.0727 - val_loss: 433.4607\n",
      "Epoch 160/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 773.4700 - val_loss: 424.0462\n",
      "Epoch 161/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 761.0125 - val_loss: 414.7661\n",
      "Epoch 162/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 748.6992 - val_loss: 405.6194\n",
      "Epoch 163/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 736.5294 - val_loss: 396.6052\n",
      "Epoch 164/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 724.5023 - val_loss: 387.7227\n",
      "Epoch 165/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 712.6166 - val_loss: 378.9709\n",
      "Epoch 166/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 700.8718 - val_loss: 370.3484\n",
      "Epoch 167/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 689.2668 - val_loss: 361.8549\n",
      "Epoch 168/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 677.8004 - val_loss: 353.4895\n",
      "Epoch 169/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 666.4722 - val_loss: 345.2510\n",
      "Epoch 170/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 655.2813 - val_loss: 337.1381\n",
      "Epoch 171/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 644.2266 - val_loss: 329.1506\n",
      "Epoch 172/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 633.3072 - val_loss: 321.2874\n",
      "Epoch 173/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 622.5225 - val_loss: 313.5469\n",
      "Epoch 174/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 611.8712 - val_loss: 305.9290\n",
      "Epoch 175/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 601.3523 - val_loss: 298.4324\n",
      "Epoch 176/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 590.9654 - val_loss: 291.0563\n",
      "Epoch 177/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 580.7095 - val_loss: 283.7995\n",
      "Epoch 178/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 570.5836 - val_loss: 276.6617\n",
      "Epoch 179/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 560.5870 - val_loss: 269.6414\n",
      "Epoch 180/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 550.7184 - val_loss: 262.7375\n",
      "Epoch 181/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 540.9772 - val_loss: 255.9495\n",
      "Epoch 182/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 531.3626 - val_loss: 249.2766\n",
      "Epoch 183/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 521.8735 - val_loss: 242.7175\n",
      "Epoch 184/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 512.5093 - val_loss: 236.2711\n",
      "Epoch 185/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 503.2686 - val_loss: 229.9373\n",
      "Epoch 186/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 494.1510 - val_loss: 223.7139\n",
      "Epoch 187/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 485.1553 - val_loss: 217.6010\n",
      "Epoch 188/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 476.2807 - val_loss: 211.5973\n",
      "Epoch 189/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 467.5264 - val_loss: 205.7024\n",
      "Epoch 190/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 458.8919 - val_loss: 199.9143\n",
      "Epoch 191/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 450.3756 - val_loss: 194.2330\n",
      "Epoch 192/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 441.9769 - val_loss: 188.6572\n",
      "Epoch 193/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 433.6950 - val_loss: 183.1860\n",
      "Epoch 194/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 425.5289 - val_loss: 177.8183\n",
      "Epoch 195/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 417.4776 - val_loss: 172.5534\n",
      "Epoch 196/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 409.5404 - val_loss: 167.3903\n",
      "Epoch 197/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 401.7162 - val_loss: 162.3280\n",
      "Epoch 198/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 394.0046 - val_loss: 157.3655\n",
      "Epoch 199/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 386.4042 - val_loss: 152.5023\n",
      "Epoch 200/500\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 378.9146 - val_loss: 147.7369\n",
      "Epoch 201/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 371.5344 - val_loss: 143.0685\n",
      "Epoch 202/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 364.2628 - val_loss: 138.4963\n",
      "Epoch 203/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 357.0989 - val_loss: 134.0193\n",
      "Epoch 204/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 350.0420 - val_loss: 129.6364\n",
      "Epoch 205/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 343.0911 - val_loss: 125.3470\n",
      "Epoch 206/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 336.2455 - val_loss: 121.1501\n",
      "Epoch 207/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 329.5045 - val_loss: 117.0446\n",
      "Epoch 208/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 322.8666 - val_loss: 113.0296\n",
      "Epoch 209/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 316.3313 - val_loss: 109.1042\n",
      "Epoch 210/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 309.8975 - val_loss: 105.2674\n",
      "Epoch 211/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 303.5646 - val_loss: 101.5182\n",
      "Epoch 212/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 297.3313 - val_loss: 97.8558\n",
      "Epoch 213/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 291.1969 - val_loss: 94.2790\n",
      "Epoch 214/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 285.1609 - val_loss: 90.7873\n",
      "Epoch 215/500\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 279.2218 - val_loss: 87.3794\n",
      "Epoch 216/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 273.3791 - val_loss: 84.0545\n",
      "Epoch 217/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 267.6318 - val_loss: 80.8115\n",
      "Epoch 218/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 261.9789 - val_loss: 77.6496\n",
      "Epoch 219/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 256.4194 - val_loss: 74.5676\n",
      "Epoch 220/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 250.9525 - val_loss: 71.5650\n",
      "Epoch 221/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 245.5777 - val_loss: 68.6407\n",
      "Epoch 222/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 240.2939 - val_loss: 65.7934\n",
      "Epoch 223/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 235.1001 - val_loss: 63.0228\n",
      "Epoch 224/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 229.9954 - val_loss: 60.3273\n",
      "Epoch 225/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 224.9792 - val_loss: 57.7064\n",
      "Epoch 226/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 220.0503 - val_loss: 55.1589\n",
      "Epoch 227/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 215.2076 - val_loss: 52.6839\n",
      "Epoch 228/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 210.4505 - val_loss: 50.2805\n",
      "Epoch 229/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 205.7783 - val_loss: 47.9479\n",
      "Epoch 230/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 201.1898 - val_loss: 45.6849\n",
      "Epoch 231/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 196.6842 - val_loss: 43.4906\n",
      "Epoch 232/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 192.2605 - val_loss: 41.3642\n",
      "Epoch 233/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 187.9181 - val_loss: 39.3045\n",
      "Epoch 234/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 183.6557 - val_loss: 37.3109\n",
      "Epoch 235/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 179.4730 - val_loss: 35.3823\n",
      "Epoch 236/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 175.3686 - val_loss: 33.5178\n",
      "Epoch 237/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 171.3419 - val_loss: 31.7161\n",
      "Epoch 238/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 167.3916 - val_loss: 29.9768\n",
      "Epoch 239/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 163.5173 - val_loss: 28.2986\n",
      "Epoch 240/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 159.7179 - val_loss: 26.6807\n",
      "Epoch 241/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 155.9926 - val_loss: 25.1223\n",
      "Epoch 242/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 152.3405 - val_loss: 23.6221\n",
      "Epoch 243/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 148.7607 - val_loss: 22.1796\n",
      "Epoch 244/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 145.2522 - val_loss: 20.7935\n",
      "Epoch 245/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 141.8143 - val_loss: 19.4631\n",
      "Epoch 246/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 138.4460 - val_loss: 18.1874\n",
      "Epoch 247/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 135.1467 - val_loss: 16.9655\n",
      "Epoch 248/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 131.9153 - val_loss: 15.7964\n",
      "Epoch 249/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 128.7508 - val_loss: 14.6792\n",
      "Epoch 250/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 125.6525 - val_loss: 13.6130\n",
      "Epoch 251/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 122.6195 - val_loss: 12.5969\n",
      "Epoch 252/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 119.6509 - val_loss: 11.6299\n",
      "Epoch 253/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 116.7459 - val_loss: 10.7112\n",
      "Epoch 254/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 113.9034 - val_loss: 9.8398\n",
      "Epoch 255/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 111.1228 - val_loss: 9.0148\n",
      "Epoch 256/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 108.4031 - val_loss: 8.2354\n",
      "Epoch 257/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 105.7437 - val_loss: 7.5005\n",
      "Epoch 258/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 103.1434 - val_loss: 6.8095\n",
      "Epoch 259/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 100.6017 - val_loss: 6.1613\n",
      "Epoch 260/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 98.1174 - val_loss: 5.5550\n",
      "Epoch 261/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 95.6900 - val_loss: 4.9898\n",
      "Epoch 262/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 93.3184 - val_loss: 4.4647\n",
      "Epoch 263/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 91.0018 - val_loss: 3.9789\n",
      "Epoch 264/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 88.7392 - val_loss: 3.5316\n",
      "Epoch 265/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 86.5302 - val_loss: 3.1218\n",
      "Epoch 266/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 84.3737 - val_loss: 2.7486\n",
      "Epoch 267/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 82.2688 - val_loss: 2.4113\n",
      "Epoch 268/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 80.2150 - val_loss: 2.1090\n",
      "Epoch 269/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 78.2111 - val_loss: 1.8408\n",
      "Epoch 270/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 76.2563 - val_loss: 1.6058\n",
      "Epoch 271/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 74.3502 - val_loss: 1.4032\n",
      "Epoch 272/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 72.4916 - val_loss: 1.2322\n",
      "Epoch 273/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 70.6799 - val_loss: 1.0920\n",
      "Epoch 274/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 68.9141 - val_loss: 0.9817\n",
      "Epoch 275/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 67.1936 - val_loss: 0.9005\n",
      "Epoch 276/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 65.5174 - val_loss: 0.8476\n",
      "Epoch 277/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 63.8850 - val_loss: 0.8222\n",
      "Epoch 278/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 62.2953 - val_loss: 0.8234\n",
      "Epoch 279/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 60.7478 - val_loss: 0.8506\n",
      "Epoch 280/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 59.2417 - val_loss: 0.9029\n",
      "Epoch 281/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 57.7760 - val_loss: 0.9794\n",
      "Epoch 282/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 56.3501 - val_loss: 1.0795\n",
      "Epoch 283/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 54.9633 - val_loss: 1.2025\n",
      "Epoch 284/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 53.6146 - val_loss: 1.3474\n",
      "Epoch 285/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 52.3036 - val_loss: 1.5136\n",
      "Epoch 286/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 51.0294 - val_loss: 1.7003\n",
      "Epoch 287/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 49.7914 - val_loss: 1.9068\n",
      "Epoch 288/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 48.5884 - val_loss: 2.1325\n",
      "Epoch 289/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 47.4202 - val_loss: 2.3763\n",
      "Epoch 290/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 46.2860 - val_loss: 2.6379\n",
      "Epoch 291/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 45.1849 - val_loss: 2.9164\n",
      "Epoch 292/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 44.1163 - val_loss: 3.2111\n",
      "Epoch 293/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 43.0794 - val_loss: 3.5214\n",
      "Epoch 294/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 42.0737 - val_loss: 3.8466\n",
      "Epoch 295/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 41.0984 - val_loss: 4.1859\n",
      "Epoch 296/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 40.1530 - val_loss: 4.5389\n",
      "Epoch 297/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 39.2366 - val_loss: 4.9046\n",
      "Epoch 298/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 38.3486 - val_loss: 5.2827\n",
      "Epoch 299/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 37.4885 - val_loss: 5.6724\n",
      "Epoch 300/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 36.6554 - val_loss: 6.0731\n",
      "Epoch 301/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 35.8490 - val_loss: 6.4842\n",
      "Epoch 302/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 35.0682 - val_loss: 6.9051\n",
      "Epoch 303/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 34.3128 - val_loss: 7.3352\n",
      "Epoch 304/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 33.5820 - val_loss: 7.7740\n",
      "Epoch 305/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 32.8752 - val_loss: 8.2209\n",
      "Epoch 306/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 32.1919 - val_loss: 8.6752\n",
      "Epoch 307/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 31.5314 - val_loss: 9.1366\n",
      "Epoch 308/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 30.8931 - val_loss: 9.6043\n",
      "Epoch 309/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 30.2766 - val_loss: 10.0780\n",
      "Epoch 310/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 29.6811 - val_loss: 10.5570\n",
      "Epoch 311/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 29.1062 - val_loss: 11.0409\n",
      "Epoch 312/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 28.5514 - val_loss: 11.5291\n",
      "Epoch 313/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 28.0160 - val_loss: 12.0214\n",
      "Epoch 314/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 27.4995 - val_loss: 12.5170\n",
      "Epoch 315/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.0016 - val_loss: 13.0156\n",
      "Epoch 316/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 26.5215 - val_loss: 13.5166\n",
      "Epoch 317/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 26.0589 - val_loss: 14.0199\n",
      "Epoch 318/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 25.6131 - val_loss: 14.5248\n",
      "Epoch 319/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 25.1837 - val_loss: 15.0309\n",
      "Epoch 320/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 24.7703 - val_loss: 15.5378\n",
      "Epoch 321/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 24.3724 - val_loss: 16.0453\n",
      "Epoch 322/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 23.9894 - val_loss: 16.5528\n",
      "Epoch 323/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 23.6210 - val_loss: 17.0600\n",
      "Epoch 324/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 23.2667 - val_loss: 17.5664\n",
      "Epoch 325/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 22.9261 - val_loss: 18.0720\n",
      "Epoch 326/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 22.5987 - val_loss: 18.5762\n",
      "Epoch 327/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 22.2843 - val_loss: 19.0788\n",
      "Epoch 328/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 21.9822 - val_loss: 19.5793\n",
      "Epoch 329/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 21.6921 - val_loss: 20.0775\n",
      "Epoch 330/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 21.4137 - val_loss: 20.5732\n",
      "Epoch 331/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 21.1465 - val_loss: 21.0659\n",
      "Epoch 332/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 20.8902 - val_loss: 21.5558\n",
      "Epoch 333/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 20.6444 - val_loss: 22.0420\n",
      "Epoch 334/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 20.4088 - val_loss: 22.5249\n",
      "Epoch 335/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 20.1829 - val_loss: 23.0036\n",
      "Epoch 336/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 19.9666 - val_loss: 23.4784\n",
      "Epoch 337/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 19.7593 - val_loss: 23.9490\n",
      "Epoch 338/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 19.5609 - val_loss: 24.4149\n",
      "Epoch 339/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 19.3709 - val_loss: 24.8762\n",
      "Epoch 340/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 19.1892 - val_loss: 25.3326\n",
      "Epoch 341/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 19.0153 - val_loss: 25.7839\n",
      "Epoch 342/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.8490 - val_loss: 26.2302\n",
      "Epoch 343/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.6901 - val_loss: 26.6709\n",
      "Epoch 344/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 18.5382 - val_loss: 27.1062\n",
      "Epoch 345/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.3930 - val_loss: 27.5358\n",
      "Epoch 346/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.2544 - val_loss: 27.9596\n",
      "Epoch 347/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.1220 - val_loss: 28.3777\n",
      "Epoch 348/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.9957 - val_loss: 28.7896\n",
      "Epoch 349/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 17.8751 - val_loss: 29.1956\n",
      "Epoch 350/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.7600 - val_loss: 29.5953\n",
      "Epoch 351/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.6503 - val_loss: 29.9887\n",
      "Epoch 352/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 17.5457 - val_loss: 30.3759\n",
      "Epoch 353/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 17.4460 - val_loss: 30.7566\n",
      "Epoch 354/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 17.3510 - val_loss: 31.1310\n",
      "Epoch 355/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 17.2605 - val_loss: 31.4987\n",
      "Epoch 356/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 17.1743 - val_loss: 31.8599\n",
      "Epoch 357/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 17.0922 - val_loss: 32.2145\n",
      "Epoch 358/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 17.0141 - val_loss: 32.5626\n",
      "Epoch 359/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 16.9398 - val_loss: 32.9040\n",
      "Epoch 360/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 16.8691 - val_loss: 33.2387\n",
      "Epoch 361/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 16.8019 - val_loss: 33.5669\n",
      "Epoch 362/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 16.7379 - val_loss: 33.8883\n",
      "Epoch 363/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 16.6771 - val_loss: 34.2032\n",
      "Epoch 364/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 16.6194 - val_loss: 34.5116\n",
      "Epoch 365/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 16.5645 - val_loss: 34.8130\n",
      "Epoch 366/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 16.5124 - val_loss: 35.1082\n",
      "Epoch 367/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 16.4629 - val_loss: 35.3966\n",
      "Epoch 368/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 16.4159 - val_loss: 35.6786\n",
      "Epoch 369/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 16.3712 - val_loss: 35.9540\n",
      "Epoch 370/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 16.3289 - val_loss: 36.2231\n",
      "Epoch 371/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 16.2887 - val_loss: 36.4857\n",
      "Epoch 372/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 16.2506 - val_loss: 36.7421\n",
      "Epoch 373/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 16.2144 - val_loss: 36.9919\n",
      "Epoch 374/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 16.1802 - val_loss: 37.2359\n",
      "Epoch 375/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 16.1477 - val_loss: 37.4735\n",
      "Epoch 376/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 16.1169 - val_loss: 37.7052\n",
      "Epoch 377/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 16.0877 - val_loss: 37.9307\n",
      "Epoch 378/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 16.0600 - val_loss: 38.1502\n",
      "Epoch 379/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 16.0338 - val_loss: 38.3638\n",
      "Epoch 380/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 16.0089 - val_loss: 38.5717\n",
      "Epoch 381/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 15.9854 - val_loss: 38.7739\n",
      "Epoch 382/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 15.9631 - val_loss: 38.9703\n",
      "Epoch 383/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 15.9420 - val_loss: 39.1612\n",
      "Epoch 384/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.9221 - val_loss: 39.3468\n",
      "Epoch 385/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 15.9032 - val_loss: 39.5269\n",
      "Epoch 386/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 15.8853 - val_loss: 39.7019\n",
      "Epoch 387/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 15.8683 - val_loss: 39.8717\n",
      "Epoch 388/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 15.8522 - val_loss: 40.0362\n",
      "Epoch 389/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 15.8371 - val_loss: 40.1959\n",
      "Epoch 390/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 15.8227 - val_loss: 40.3504\n",
      "Epoch 391/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 15.8091 - val_loss: 40.5003\n",
      "Epoch 392/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 15.7963 - val_loss: 40.6453\n",
      "Epoch 393/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.7841 - val_loss: 40.7858\n",
      "Epoch 394/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.7726 - val_loss: 40.9216\n",
      "Epoch 395/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 15.7617 - val_loss: 41.0531\n",
      "Epoch 396/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.7514 - val_loss: 41.1804\n",
      "Epoch 397/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.7417 - val_loss: 41.3034\n",
      "Epoch 398/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.7325 - val_loss: 41.4222\n",
      "Epoch 399/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.7238 - val_loss: 41.5368\n",
      "Epoch 400/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.7156 - val_loss: 41.6477\n",
      "Epoch 401/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.7078 - val_loss: 41.7546\n",
      "Epoch 402/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.7004 - val_loss: 41.8578\n",
      "Epoch 403/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.6935 - val_loss: 41.9574\n",
      "Epoch 404/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 15.6869 - val_loss: 42.0534\n",
      "Epoch 405/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.6807 - val_loss: 42.1460\n",
      "Epoch 406/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.6748 - val_loss: 42.2352\n",
      "Epoch 407/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.6692 - val_loss: 42.3209\n",
      "Epoch 408/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 15.6640 - val_loss: 42.4036\n",
      "Epoch 409/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.6590 - val_loss: 42.4833\n",
      "Epoch 410/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 15.6544 - val_loss: 42.5597\n",
      "Epoch 411/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 15.6499 - val_loss: 42.6332\n",
      "Epoch 412/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.6458 - val_loss: 42.7040\n",
      "Epoch 413/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.6418 - val_loss: 42.7720\n",
      "Epoch 414/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 15.6381 - val_loss: 42.8372\n",
      "Epoch 415/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.6346 - val_loss: 42.9001\n",
      "Epoch 416/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 15.6313 - val_loss: 42.9604\n",
      "Epoch 417/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.6281 - val_loss: 43.0180\n",
      "Epoch 418/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 15.6251 - val_loss: 43.0735\n",
      "Epoch 419/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.6223 - val_loss: 43.1264\n",
      "Epoch 420/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.6198 - val_loss: 43.1775\n",
      "Epoch 421/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.6173 - val_loss: 43.2264\n",
      "Epoch 422/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 15.6149 - val_loss: 43.2731\n",
      "Epoch 423/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.6127 - val_loss: 43.3178\n",
      "Epoch 424/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.6106 - val_loss: 43.3606\n",
      "Epoch 425/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.6087 - val_loss: 43.4015\n",
      "Epoch 426/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.6068 - val_loss: 43.4407\n",
      "Epoch 427/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 15.6051 - val_loss: 43.4780\n",
      "Epoch 428/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.6035 - val_loss: 43.5136\n",
      "Epoch 429/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.6020 - val_loss: 43.5479\n",
      "Epoch 430/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.6005 - val_loss: 43.5805\n",
      "Epoch 431/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5992 - val_loss: 43.6117\n",
      "Epoch 432/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 15.5979 - val_loss: 43.6413\n",
      "Epoch 433/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5968 - val_loss: 43.6697\n",
      "Epoch 434/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 15.5957 - val_loss: 43.6970\n",
      "Epoch 435/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5946 - val_loss: 43.7226\n",
      "Epoch 436/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5936 - val_loss: 43.7471\n",
      "Epoch 437/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5927 - val_loss: 43.7704\n",
      "Epoch 438/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5919 - val_loss: 43.7926\n",
      "Epoch 439/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 15.5911 - val_loss: 43.8138\n",
      "Epoch 440/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 15.5904 - val_loss: 43.8341\n",
      "Epoch 441/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5897 - val_loss: 43.8531\n",
      "Epoch 442/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5891 - val_loss: 43.8712\n",
      "Epoch 443/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5885 - val_loss: 43.8885\n",
      "Epoch 444/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 15.5879 - val_loss: 43.9049\n",
      "Epoch 445/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5875 - val_loss: 43.9203\n",
      "Epoch 446/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5870 - val_loss: 43.9350\n",
      "Epoch 447/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5866 - val_loss: 43.9489\n",
      "Epoch 448/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5862 - val_loss: 43.9622\n",
      "Epoch 449/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5859 - val_loss: 43.9749\n",
      "Epoch 450/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5856 - val_loss: 43.9867\n",
      "Epoch 451/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5853 - val_loss: 43.9979\n",
      "Epoch 452/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5851 - val_loss: 44.0088\n",
      "Epoch 453/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5848 - val_loss: 44.0187\n",
      "Epoch 454/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5846 - val_loss: 44.0283\n",
      "Epoch 455/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 15.5844 - val_loss: 44.0373\n",
      "Epoch 456/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 15.5843 - val_loss: 44.0459\n",
      "Epoch 457/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5842 - val_loss: 44.0540\n",
      "Epoch 458/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5840 - val_loss: 44.0614\n",
      "Epoch 459/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 15.5840 - val_loss: 44.0686\n",
      "Epoch 460/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 15.5839 - val_loss: 44.0753\n",
      "Epoch 461/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 15.5839 - val_loss: 44.0817\n",
      "Epoch 462/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 15.5838 - val_loss: 44.0875\n",
      "Epoch 463/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5838 - val_loss: 44.0930\n",
      "Epoch 464/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5838 - val_loss: 44.0984\n",
      "Epoch 465/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5839 - val_loss: 44.1035\n",
      "Epoch 466/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5839 - val_loss: 44.1083\n",
      "Epoch 467/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5839 - val_loss: 44.1125\n",
      "Epoch 468/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5839 - val_loss: 44.1164\n",
      "Epoch 469/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5840 - val_loss: 44.1202\n",
      "Epoch 470/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 15.5841 - val_loss: 44.1238\n",
      "Epoch 471/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5842 - val_loss: 44.1271\n",
      "Epoch 472/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5842 - val_loss: 44.1301\n",
      "Epoch 473/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5844 - val_loss: 44.1329\n",
      "Epoch 474/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5845 - val_loss: 44.1357\n",
      "Epoch 475/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5846 - val_loss: 44.1383\n",
      "Epoch 476/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5847 - val_loss: 44.1406\n",
      "Epoch 477/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5849 - val_loss: 44.1429\n",
      "Epoch 478/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5850 - val_loss: 44.1451\n",
      "Epoch 479/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5851 - val_loss: 44.1469\n",
      "Epoch 480/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 15.5853 - val_loss: 44.1486\n",
      "Epoch 481/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5854 - val_loss: 44.1505\n",
      "Epoch 482/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5855 - val_loss: 44.1519\n",
      "Epoch 483/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 15.5857 - val_loss: 44.1533\n",
      "Epoch 484/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 15.5859 - val_loss: 44.1545\n",
      "Epoch 485/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 15.5860 - val_loss: 44.1555\n",
      "Epoch 486/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 15.5862 - val_loss: 44.1566\n",
      "Epoch 487/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 15.5864 - val_loss: 44.1577\n",
      "Epoch 488/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5866 - val_loss: 44.1587\n",
      "Epoch 489/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5868 - val_loss: 44.1595\n",
      "Epoch 490/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5870 - val_loss: 44.1603\n",
      "Epoch 491/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5871 - val_loss: 44.1610\n",
      "Epoch 492/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 15.5873 - val_loss: 44.1615\n",
      "Epoch 493/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 15.5875 - val_loss: 44.1620\n",
      "Epoch 494/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 15.5877 - val_loss: 44.1625\n",
      "Epoch 495/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5879 - val_loss: 44.1628\n",
      "Epoch 496/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5881 - val_loss: 44.1632\n",
      "Epoch 497/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 15.5883 - val_loss: 44.1637\n",
      "Epoch 498/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.5884 - val_loss: 44.1639\n",
      "Epoch 499/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 15.5886 - val_loss: 44.1640\n",
      "Epoch 500/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 15.5888 - val_loss: 44.1641\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.8407, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0015, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0118, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.5775, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 381ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.89433473e+01, 6.89125350e+01, 6.88817227e+01, 6.88509104e+01,\n",
       "        6.88200980e+01, 6.87892857e+01, 6.87584734e+01, 6.87276611e+01,\n",
       "        6.86968487e+01, 6.86660364e+01, 6.86352241e+01, 6.86044118e+01,\n",
       "        6.85735994e+01, 6.85427871e+01, 6.85119748e+01, 6.84811625e+01,\n",
       "        6.84503501e+01, 6.84195378e+01, 6.83887255e+01, 6.83579132e+01,\n",
       "        6.83271008e+01, 6.82962885e+01, 6.82654762e+01, 6.82346639e+01,\n",
       "        6.82038515e+01, 6.81632353e+01, 6.81212185e+01, 6.80792017e+01,\n",
       "        6.80371849e+01, 6.79951681e+01, 6.79531513e+01, 6.79111345e+01,\n",
       "        6.78691177e+01, 6.78271008e+01, 6.77850840e+01, 6.77430672e+01,\n",
       "        6.77010504e+01, 6.76590336e+01, 6.76170168e+01, 6.75750000e+01,\n",
       "        6.75329832e+01, 6.74909664e+01, 6.74489496e+01, 6.74069328e+01,\n",
       "        6.73649160e+01, 6.73228992e+01, 6.72808823e+01, 6.72388655e+01,\n",
       "        6.71968487e+01, 6.71548319e+01, 6.71128151e+01, 6.70707983e+01,\n",
       "        6.70287815e+01, 6.69867647e+01, 6.69447479e+01, 6.69027311e+01,\n",
       "        6.68607143e+01, 6.68186975e+01, 6.67766807e+01, 6.67346639e+01,\n",
       "        6.66960784e+01, 6.66736695e+01, 6.66512605e+01, 6.66288515e+01,\n",
       "        6.66064426e+01, 6.65840336e+01, 6.65616246e+01, 6.65392157e+01,\n",
       "        6.65168067e+01, 6.64943978e+01, 6.64719888e+01, 6.64495798e+01,\n",
       "        6.64271709e+01, 6.64047619e+01, 6.63823529e+01, 6.63599440e+01,\n",
       "        6.63375350e+01, 6.63151261e+01, 6.62927171e+01, 6.62703081e+01,\n",
       "        7.42470398e+01, 4.97134417e-01, 2.81131804e-01, 4.12033588e-01,\n",
       "        3.44011962e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.00841518e-02, 0.00000000e+00, 4.81220871e-01,\n",
       "        1.58677518e-01, 0.00000000e+00, 0.00000000e+00, 9.91864726e-02]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([65.50375817, 65.49722222, 65.49068627, 65.48415033, 65.47761438,\n",
       "       65.47107843, 65.46454248, 65.45800654, 65.45147059, 65.44493464,\n",
       "       65.43839869, 65.43186275, 65.4253268 , 65.41879085, 65.4122549 ,\n",
       "       65.40571895, 65.39918301, 65.39264706, 65.38611111, 65.37957516,\n",
       "       65.37303922, 65.36650327, 65.35996732, 65.35343137, 65.34689542,\n",
       "       65.34035948, 65.33382353, 65.32728758, 65.32075163, 65.31421569,\n",
       "       65.30767974, 65.30114379, 65.29460784, 65.2880719 , 65.28153595,\n",
       "       65.275     , 65.26846405, 65.2619281 , 65.25539216, 65.24885621,\n",
       "       65.24232026, 65.23578431, 65.22924837, 65.22271242, 65.21617647,\n",
       "       65.20964052, 65.20310458, 65.19656863, 65.19003268, 65.18349673,\n",
       "       65.17696078, 65.17042484, 65.16388889, 65.15735294, 65.15081699,\n",
       "       65.14428105, 65.1377451 , 65.13120915, 65.1246732 , 65.11813725,\n",
       "       65.11160131, 65.10506536, 65.09768908, 65.0874183 , 65.07714753,\n",
       "       65.06687675, 65.05660598, 65.0463352 , 65.03606443, 65.02579365,\n",
       "       65.01552288, 65.0052521 , 64.99498133, 64.98471055, 64.97443978,\n",
       "       64.964169  , 64.95389823, 64.94362745, 64.93335668, 64.9230859 ,\n",
       "       64.91281513, 64.90254435, 64.89227358, 64.8820028 , 64.87173203,\n",
       "       64.86146125, 64.85119048, 64.8409197 , 64.83064893, 64.82037815,\n",
       "       64.81010738, 64.7998366 , 64.78956583, 64.77929505, 64.76902428,\n",
       "       64.7587535 , 64.74848273, 64.73821195, 64.72794118, 64.7176704 ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.295653097911984\n",
      "14.24079366639582\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
