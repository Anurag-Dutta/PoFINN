{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.140000\n",
       "1       88.911429\n",
       "2       88.682857\n",
       "3       88.454286\n",
       "4       88.225714\n",
       "          ...    \n",
       "1945    63.817052\n",
       "1946    63.810049\n",
       "1947    63.803046\n",
       "1948    63.796043\n",
       "1949    63.789041\n",
       "Name: C5, Length: 1950, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c5_interpolated_1850_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.140000\n",
       "1       88.911429\n",
       "2       88.682857\n",
       "3       88.454286\n",
       "4       88.225714\n",
       "          ...    \n",
       "1845    65.892038\n",
       "1846    65.860021\n",
       "1847    65.828004\n",
       "1848    65.795987\n",
       "1849    65.763970\n",
       "Name: C5, Length: 1850, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1850)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.140000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.911429</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.682857</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.454286</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.225714</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     89.140000  0.000298\n",
       "1     88.911429  0.000298\n",
       "2     88.682857  0.000297\n",
       "3     88.454286  0.000297\n",
       "4     88.225714  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.8407  # Value for C0\n",
    "K0 = -0.0015  # Value for K0\n",
    "K1 = -0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0118    # Value for b\n",
    "c = 2.5775    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    89.140000    88.911429    88.682857    88.454286    88.225714   \n",
      "351    88.911429    88.682857    88.454286    88.225714    87.997143   \n",
      "352    88.682857    88.454286    88.225714    87.997143    87.768571   \n",
      "353    88.454286    88.225714    87.997143    87.768571    87.540000   \n",
      "354    88.225714    87.997143    87.768571    87.540000    87.311429   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.997143    87.768571    87.540000    87.311429    87.095798  ...   \n",
      "351    87.768571    87.540000    87.311429    87.095798    87.039776  ...   \n",
      "352    87.540000    87.311429    87.095798    87.039776    86.983754  ...   \n",
      "353    87.311429    87.095798    87.039776    86.983754    86.927731  ...   \n",
      "354    87.095798    87.039776    86.983754    86.927731    86.871709  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   79.161181    0.000263   79.132236    0.000263   79.103291    0.000263   \n",
      "351   79.132236    0.000263   79.103291    0.000263   79.074346    0.000262   \n",
      "352   79.103291    0.000263   79.074346    0.000262   79.045401    0.000262   \n",
      "353   79.074346    0.000262   79.045401    0.000262   79.016457    0.000262   \n",
      "354   79.045401    0.000262   79.016457    0.000262   78.987512    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   79.074346    0.000262   79.045401    0.000262  \n",
      "351   79.045401    0.000262   79.016457    0.000262  \n",
      "352   79.016457    0.000262   78.987512    0.000262  \n",
      "353   78.987512    0.000262   78.958567    0.000262  \n",
      "354   78.958567    0.000262   78.929622    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 1, 251) (1400, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "18/18 [==============================] - 2s 30ms/step - loss: 5380.1821 - val_loss: 4277.3535\n",
      "Epoch 2/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5278.0234 - val_loss: 4221.4463\n",
      "Epoch 3/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5215.7197 - val_loss: 4165.6274\n",
      "Epoch 4/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5153.8774 - val_loss: 4110.4463\n",
      "Epoch 5/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5091.8281 - val_loss: 4045.5173\n",
      "Epoch 6/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 5019.0771 - val_loss: 3988.3518\n",
      "Epoch 7/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4955.5459 - val_loss: 3931.9250\n",
      "Epoch 8/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4893.0317 - val_loss: 3876.4790\n",
      "Epoch 9/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4831.4912 - val_loss: 3821.8625\n",
      "Epoch 10/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4770.7754 - val_loss: 3767.9746\n",
      "Epoch 11/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4710.0986 - val_loss: 3711.8384\n",
      "Epoch 12/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4645.9463 - val_loss: 3654.7070\n",
      "Epoch 13/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4582.4912 - val_loss: 3598.8557\n",
      "Epoch 14/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4520.4609 - val_loss: 3544.2273\n",
      "Epoch 15/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4459.6265 - val_loss: 3490.5918\n",
      "Epoch 16/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4399.7808 - val_loss: 3437.8071\n",
      "Epoch 17/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4340.7944 - val_loss: 3385.7876\n",
      "Epoch 18/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4282.5894 - val_loss: 3334.4778\n",
      "Epoch 19/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4225.1128 - val_loss: 3283.8374\n",
      "Epoch 20/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4168.3281 - val_loss: 3233.8389\n",
      "Epoch 21/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4112.2051 - val_loss: 3184.4595\n",
      "Epoch 22/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 4056.7224 - val_loss: 3135.6802\n",
      "Epoch 23/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 4001.8616 - val_loss: 3087.4861\n",
      "Epoch 24/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3947.6072 - val_loss: 3039.8660\n",
      "Epoch 25/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 3893.9478 - val_loss: 2992.8069\n",
      "Epoch 26/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3840.8711 - val_loss: 2946.3008\n",
      "Epoch 27/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3788.3665 - val_loss: 2900.3372\n",
      "Epoch 28/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3736.4263 - val_loss: 2854.9099\n",
      "Epoch 29/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 3685.0415 - val_loss: 2810.0103\n",
      "Epoch 30/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3634.2051 - val_loss: 2765.6316\n",
      "Epoch 31/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3583.9102 - val_loss: 2721.7678\n",
      "Epoch 32/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3534.1499 - val_loss: 2678.4136\n",
      "Epoch 33/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3484.9180 - val_loss: 2635.5625\n",
      "Epoch 34/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3436.2092 - val_loss: 2593.2090\n",
      "Epoch 35/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3388.0178 - val_loss: 2551.3486\n",
      "Epoch 36/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3340.3391 - val_loss: 2509.9756\n",
      "Epoch 37/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3293.1665 - val_loss: 2469.0864\n",
      "Epoch 38/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3246.4966 - val_loss: 2428.6750\n",
      "Epoch 39/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3200.3247 - val_loss: 2388.7378\n",
      "Epoch 40/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3154.6445 - val_loss: 2349.2698\n",
      "Epoch 41/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 3109.4541 - val_loss: 2310.2671\n",
      "Epoch 42/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3064.7473 - val_loss: 2271.7258\n",
      "Epoch 43/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3020.5200 - val_loss: 2233.6421\n",
      "Epoch 44/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 2976.7695 - val_loss: 2196.0115\n",
      "Epoch 45/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2933.4897 - val_loss: 2158.8293\n",
      "Epoch 46/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2890.6787 - val_loss: 2122.0930\n",
      "Epoch 47/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 2848.3311 - val_loss: 2085.7976\n",
      "Epoch 48/500\n",
      "18/18 [==============================] - 0s 17ms/step - loss: 2806.4438 - val_loss: 2049.9402\n",
      "Epoch 49/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 2765.0127 - val_loss: 2014.5175\n",
      "Epoch 50/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2724.0339 - val_loss: 1979.5256\n",
      "Epoch 51/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 2683.5051 - val_loss: 1944.9607\n",
      "Epoch 52/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 2643.4216 - val_loss: 1910.8197\n",
      "Epoch 53/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 2603.7798 - val_loss: 1877.0980\n",
      "Epoch 54/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2564.5764 - val_loss: 1843.7937\n",
      "Epoch 55/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2525.8081 - val_loss: 1810.9021\n",
      "Epoch 56/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2487.4717 - val_loss: 1778.4211\n",
      "Epoch 57/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2449.5632 - val_loss: 1746.3470\n",
      "Epoch 58/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2412.0798 - val_loss: 1714.6758\n",
      "Epoch 59/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 2372.5396 - val_loss: 1678.1746\n",
      "Epoch 60/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 2329.4309 - val_loss: 1642.1532\n",
      "Epoch 61/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 2287.6890 - val_loss: 1607.5897\n",
      "Epoch 62/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 2247.4312 - val_loss: 1574.1530\n",
      "Epoch 63/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 2208.3086 - val_loss: 1541.6093\n",
      "Epoch 64/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 2170.1028 - val_loss: 1509.8210\n",
      "Epoch 65/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 2132.6829 - val_loss: 1478.7010\n",
      "Epoch 66/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 2095.9641 - val_loss: 1448.1925\n",
      "Epoch 67/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2059.8894 - val_loss: 1418.2526\n",
      "Epoch 68/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 2024.4154 - val_loss: 1388.8506\n",
      "Epoch 69/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1989.5105 - val_loss: 1359.9613\n",
      "Epoch 70/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1955.1489 - val_loss: 1331.5647\n",
      "Epoch 71/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1921.3098 - val_loss: 1303.6447\n",
      "Epoch 72/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1887.9755 - val_loss: 1276.1864\n",
      "Epoch 73/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1855.1305 - val_loss: 1249.1785\n",
      "Epoch 74/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1822.7625 - val_loss: 1222.6089\n",
      "Epoch 75/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1790.8600 - val_loss: 1196.4683\n",
      "Epoch 76/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1759.4128 - val_loss: 1170.7484\n",
      "Epoch 77/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1728.4116 - val_loss: 1145.4407\n",
      "Epoch 78/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1697.8480 - val_loss: 1120.5388\n",
      "Epoch 79/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1667.7145 - val_loss: 1096.0345\n",
      "Epoch 80/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1638.0033 - val_loss: 1071.9227\n",
      "Epoch 81/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1608.7090 - val_loss: 1048.1973\n",
      "Epoch 82/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1579.8239 - val_loss: 1024.8510\n",
      "Epoch 83/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1551.3429 - val_loss: 1001.8807\n",
      "Epoch 84/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1523.2606 - val_loss: 979.2795\n",
      "Epoch 85/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1495.5710 - val_loss: 957.0434\n",
      "Epoch 86/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1468.2692 - val_loss: 935.1675\n",
      "Epoch 87/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1441.3505 - val_loss: 913.6468\n",
      "Epoch 88/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1414.8096 - val_loss: 892.4772\n",
      "Epoch 89/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1388.6431 - val_loss: 871.6544\n",
      "Epoch 90/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1362.8457 - val_loss: 851.1748\n",
      "Epoch 91/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1337.4135 - val_loss: 831.0334\n",
      "Epoch 92/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1312.3417 - val_loss: 811.2271\n",
      "Epoch 93/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1287.6272 - val_loss: 791.7510\n",
      "Epoch 94/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1263.2656 - val_loss: 772.6020\n",
      "Epoch 95/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1239.2533 - val_loss: 753.7770\n",
      "Epoch 96/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1215.5865 - val_loss: 735.2719\n",
      "Epoch 97/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 1192.2615 - val_loss: 717.0825\n",
      "Epoch 98/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1169.2748 - val_loss: 699.2063\n",
      "Epoch 99/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1146.6229 - val_loss: 681.6394\n",
      "Epoch 100/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1124.3019 - val_loss: 664.3781\n",
      "Epoch 101/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1102.3091 - val_loss: 647.4198\n",
      "Epoch 102/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1080.6404 - val_loss: 630.7608\n",
      "Epoch 103/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1059.2930 - val_loss: 614.3974\n",
      "Epoch 104/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1038.2629 - val_loss: 598.3269\n",
      "Epoch 105/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1017.5478 - val_loss: 582.5463\n",
      "Epoch 106/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 997.1439 - val_loss: 567.0519\n",
      "Epoch 107/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 977.0485 - val_loss: 551.8409\n",
      "Epoch 108/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 957.2581 - val_loss: 536.9105\n",
      "Epoch 109/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 937.7700 - val_loss: 522.2575\n",
      "Epoch 110/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 918.5809 - val_loss: 507.8790\n",
      "Epoch 111/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 899.6882 - val_loss: 493.7711\n",
      "Epoch 112/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 881.0883 - val_loss: 479.9316\n",
      "Epoch 113/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 862.7789 - val_loss: 466.3577\n",
      "Epoch 114/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 844.7563 - val_loss: 453.0458\n",
      "Epoch 115/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 827.0182 - val_loss: 439.9940\n",
      "Epoch 116/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 809.5618 - val_loss: 427.1987\n",
      "Epoch 117/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 792.3838 - val_loss: 414.6570\n",
      "Epoch 118/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 775.4814 - val_loss: 402.3662\n",
      "Epoch 119/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 758.8524 - val_loss: 390.3239\n",
      "Epoch 120/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 742.4931 - val_loss: 378.5263\n",
      "Epoch 121/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 726.4014 - val_loss: 366.9714\n",
      "Epoch 122/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 710.5741 - val_loss: 355.6562\n",
      "Epoch 123/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 695.0089 - val_loss: 344.5777\n",
      "Epoch 124/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 679.7028 - val_loss: 333.7336\n",
      "Epoch 125/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 664.6533 - val_loss: 323.1212\n",
      "Epoch 126/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 649.8577 - val_loss: 312.7372\n",
      "Epoch 127/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 635.3130 - val_loss: 302.5793\n",
      "Epoch 128/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 621.0170 - val_loss: 292.6447\n",
      "Epoch 129/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 606.9670 - val_loss: 282.9310\n",
      "Epoch 130/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 593.1597 - val_loss: 273.4349\n",
      "Epoch 131/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 579.5930 - val_loss: 264.1547\n",
      "Epoch 132/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 566.2646 - val_loss: 255.0864\n",
      "Epoch 133/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 553.1713 - val_loss: 246.2287\n",
      "Epoch 134/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 540.3110 - val_loss: 237.5777\n",
      "Epoch 135/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 527.6807 - val_loss: 229.1322\n",
      "Epoch 136/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 515.2780 - val_loss: 220.8885\n",
      "Epoch 137/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 503.1006 - val_loss: 212.8442\n",
      "Epoch 138/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 488.3557 - val_loss: 200.3093\n",
      "Epoch 139/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 470.8973 - val_loss: 190.5373\n",
      "Epoch 140/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 456.4193 - val_loss: 181.4960\n",
      "Epoch 141/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 442.8630 - val_loss: 173.0376\n",
      "Epoch 142/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 430.0050 - val_loss: 165.0313\n",
      "Epoch 143/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 417.6911 - val_loss: 157.4003\n",
      "Epoch 144/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 405.8298 - val_loss: 150.0969\n",
      "Epoch 145/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 394.3636 - val_loss: 143.0896\n",
      "Epoch 146/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 383.2540 - val_loss: 136.3550\n",
      "Epoch 147/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 372.4726 - val_loss: 129.8759\n",
      "Epoch 148/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 361.9977 - val_loss: 123.6377\n",
      "Epoch 149/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 351.8119 - val_loss: 117.6292\n",
      "Epoch 150/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 341.9013 - val_loss: 111.8408\n",
      "Epoch 151/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 332.2538 - val_loss: 106.2634\n",
      "Epoch 152/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 322.8588 - val_loss: 100.8890\n",
      "Epoch 153/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 313.7077 - val_loss: 95.7117\n",
      "Epoch 154/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 304.7921 - val_loss: 90.7248\n",
      "Epoch 155/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 296.1050 - val_loss: 85.9222\n",
      "Epoch 156/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 287.6395 - val_loss: 81.2989\n",
      "Epoch 157/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 279.3896 - val_loss: 76.8498\n",
      "Epoch 158/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 271.3495 - val_loss: 72.5700\n",
      "Epoch 159/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 263.5138 - val_loss: 68.4553\n",
      "Epoch 160/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 255.8776 - val_loss: 64.5011\n",
      "Epoch 161/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 248.4361 - val_loss: 60.7033\n",
      "Epoch 162/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 241.1848 - val_loss: 57.0581\n",
      "Epoch 163/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 234.1194 - val_loss: 53.5615\n",
      "Epoch 164/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 227.2354 - val_loss: 50.2098\n",
      "Epoch 165/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 220.5293 - val_loss: 46.9993\n",
      "Epoch 166/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 213.9968 - val_loss: 43.9268\n",
      "Epoch 167/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 207.6344 - val_loss: 40.9885\n",
      "Epoch 168/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 201.4385 - val_loss: 38.1817\n",
      "Epoch 169/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 195.4055 - val_loss: 35.5024\n",
      "Epoch 170/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 189.5321 - val_loss: 32.9478\n",
      "Epoch 171/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 183.8147 - val_loss: 30.5149\n",
      "Epoch 172/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 178.2502 - val_loss: 28.2004\n",
      "Epoch 173/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 172.8357 - val_loss: 26.0016\n",
      "Epoch 174/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 167.5680 - val_loss: 23.9154\n",
      "Epoch 175/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 162.4438 - val_loss: 21.9390\n",
      "Epoch 176/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 157.4603 - val_loss: 20.0693\n",
      "Epoch 177/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 152.6145 - val_loss: 18.3038\n",
      "Epoch 178/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 147.9036 - val_loss: 16.6398\n",
      "Epoch 179/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 143.3249 - val_loss: 15.0744\n",
      "Epoch 180/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 138.8754 - val_loss: 13.6050\n",
      "Epoch 181/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 134.5525 - val_loss: 12.2291\n",
      "Epoch 182/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 130.3534 - val_loss: 10.9439\n",
      "Epoch 183/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 126.2756 - val_loss: 9.7472\n",
      "Epoch 184/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 122.3167 - val_loss: 8.6361\n",
      "Epoch 185/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 118.4737 - val_loss: 7.6083\n",
      "Epoch 186/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 114.7441 - val_loss: 6.6614\n",
      "Epoch 187/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 111.1258 - val_loss: 5.7930\n",
      "Epoch 188/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 107.6160 - val_loss: 5.0008\n",
      "Epoch 189/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 104.2125 - val_loss: 4.2823\n",
      "Epoch 190/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 100.9130 - val_loss: 3.6353\n",
      "Epoch 191/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 97.7150 - val_loss: 3.0575\n",
      "Epoch 192/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 94.6162 - val_loss: 2.5467\n",
      "Epoch 193/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 91.6143 - val_loss: 2.1007\n",
      "Epoch 194/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 88.7071 - val_loss: 1.7174\n",
      "Epoch 195/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 85.8924 - val_loss: 1.3945\n",
      "Epoch 196/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 83.1679 - val_loss: 1.1300\n",
      "Epoch 197/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 80.5315 - val_loss: 0.9218\n",
      "Epoch 198/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 77.9814 - val_loss: 0.7679\n",
      "Epoch 199/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 75.5151 - val_loss: 0.6663\n",
      "Epoch 200/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 73.1308 - val_loss: 0.6149\n",
      "Epoch 201/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 70.8262 - val_loss: 0.6119\n",
      "Epoch 202/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 68.5994 - val_loss: 0.6554\n",
      "Epoch 203/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 66.4484 - val_loss: 0.7434\n",
      "Epoch 204/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 64.3715 - val_loss: 0.8741\n",
      "Epoch 205/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 62.3665 - val_loss: 1.0457\n",
      "Epoch 206/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 60.4317 - val_loss: 1.2564\n",
      "Epoch 207/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 58.5651 - val_loss: 1.5044\n",
      "Epoch 208/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 56.7651 - val_loss: 1.7881\n",
      "Epoch 209/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 55.0297 - val_loss: 2.1057\n",
      "Epoch 210/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 53.3571 - val_loss: 2.4557\n",
      "Epoch 211/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 51.7457 - val_loss: 2.8363\n",
      "Epoch 212/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 50.1938 - val_loss: 3.2460\n",
      "Epoch 213/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 48.6996 - val_loss: 3.6831\n",
      "Epoch 214/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 47.2616 - val_loss: 4.1463\n",
      "Epoch 215/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 45.8780 - val_loss: 4.6340\n",
      "Epoch 216/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 44.5474 - val_loss: 5.1447\n",
      "Epoch 217/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 43.2681 - val_loss: 5.6770\n",
      "Epoch 218/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 42.0385 - val_loss: 6.2297\n",
      "Epoch 219/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 40.8572 - val_loss: 6.8011\n",
      "Epoch 220/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 39.7227 - val_loss: 7.3901\n",
      "Epoch 221/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 38.6336 - val_loss: 7.9953\n",
      "Epoch 222/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 37.5884 - val_loss: 8.6156\n",
      "Epoch 223/500\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 36.5857 - val_loss: 9.2497\n",
      "Epoch 224/500\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 35.6242 - val_loss: 9.8961\n",
      "Epoch 225/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 34.7026 - val_loss: 10.5540\n",
      "Epoch 226/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 33.8194 - val_loss: 11.2223\n",
      "Epoch 227/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 32.9736 - val_loss: 11.8996\n",
      "Epoch 228/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 32.1637 - val_loss: 12.5850\n",
      "Epoch 229/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 31.3885 - val_loss: 13.2775\n",
      "Epoch 230/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 30.6470 - val_loss: 13.9762\n",
      "Epoch 231/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 29.9377 - val_loss: 14.6799\n",
      "Epoch 232/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 29.2599 - val_loss: 15.3879\n",
      "Epoch 233/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 28.6120 - val_loss: 16.0990\n",
      "Epoch 234/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 27.9934 - val_loss: 16.8126\n",
      "Epoch 235/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 27.4026 - val_loss: 17.5277\n",
      "Epoch 236/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 26.8388 - val_loss: 18.2436\n",
      "Epoch 237/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 26.3010 - val_loss: 18.9594\n",
      "Epoch 238/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 25.7882 - val_loss: 19.6745\n",
      "Epoch 239/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 25.2994 - val_loss: 20.3881\n",
      "Epoch 240/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 24.8337 - val_loss: 21.0996\n",
      "Epoch 241/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 24.3901 - val_loss: 21.8081\n",
      "Epoch 242/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 23.9679 - val_loss: 22.5133\n",
      "Epoch 243/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 23.5661 - val_loss: 23.2146\n",
      "Epoch 244/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 23.1839 - val_loss: 23.9111\n",
      "Epoch 245/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.8206 - val_loss: 24.6024\n",
      "Epoch 246/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.4753 - val_loss: 25.2880\n",
      "Epoch 247/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.1473 - val_loss: 25.9675\n",
      "Epoch 248/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 21.8359 - val_loss: 26.6404\n",
      "Epoch 249/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 21.5404 - val_loss: 27.3060\n",
      "Epoch 250/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 21.2600 - val_loss: 27.9643\n",
      "Epoch 251/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 20.9942 - val_loss: 28.6148\n",
      "Epoch 252/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 20.7421 - val_loss: 29.2572\n",
      "Epoch 253/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 20.5034 - val_loss: 29.8911\n",
      "Epoch 254/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 20.2772 - val_loss: 30.5160\n",
      "Epoch 255/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 20.0632 - val_loss: 31.1319\n",
      "Epoch 256/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 19.8606 - val_loss: 31.7385\n",
      "Epoch 257/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 19.6690 - val_loss: 32.3355\n",
      "Epoch 258/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.4880 - val_loss: 32.9225\n",
      "Epoch 259/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 19.3168 - val_loss: 33.4995\n",
      "Epoch 260/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 19.1552 - val_loss: 34.0662\n",
      "Epoch 261/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 19.0026 - val_loss: 34.6230\n",
      "Epoch 262/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 18.8585 - val_loss: 35.1688\n",
      "Epoch 263/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 18.7226 - val_loss: 35.7042\n",
      "Epoch 264/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 18.5945 - val_loss: 36.2290\n",
      "Epoch 265/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 18.4737 - val_loss: 36.7427\n",
      "Epoch 266/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 18.3599 - val_loss: 37.2457\n",
      "Epoch 267/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 18.2527 - val_loss: 37.7381\n",
      "Epoch 268/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 18.1518 - val_loss: 38.2195\n",
      "Epoch 269/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 18.0568 - val_loss: 38.6897\n",
      "Epoch 270/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 17.9675 - val_loss: 39.1491\n",
      "Epoch 271/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 17.8835 - val_loss: 39.5977\n",
      "Epoch 272/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 17.8045 - val_loss: 40.0353\n",
      "Epoch 273/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 17.7303 - val_loss: 40.4619\n",
      "Epoch 274/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 17.6607 - val_loss: 40.8778\n",
      "Epoch 275/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 17.5952 - val_loss: 41.2827\n",
      "Epoch 276/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 17.5339 - val_loss: 41.6771\n",
      "Epoch 277/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 17.4762 - val_loss: 42.0610\n",
      "Epoch 278/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 17.4222 - val_loss: 42.4344\n",
      "Epoch 279/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 17.3715 - val_loss: 42.7972\n",
      "Epoch 280/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 17.3241 - val_loss: 43.1498\n",
      "Epoch 281/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 17.2796 - val_loss: 43.4920\n",
      "Epoch 282/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 17.2379 - val_loss: 43.8245\n",
      "Epoch 283/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 17.1989 - val_loss: 44.1469\n",
      "Epoch 284/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 17.1624 - val_loss: 44.4596\n",
      "Epoch 285/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 17.1282 - val_loss: 44.7628\n",
      "Epoch 286/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 17.0961 - val_loss: 45.0565\n",
      "Epoch 287/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 17.0662 - val_loss: 45.3407\n",
      "Epoch 288/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 17.0382 - val_loss: 45.6157\n",
      "Epoch 289/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 17.0121 - val_loss: 45.8821\n",
      "Epoch 290/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.9876 - val_loss: 46.1393\n",
      "Epoch 291/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.9648 - val_loss: 46.3875\n",
      "Epoch 292/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.9435 - val_loss: 46.6276\n",
      "Epoch 293/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 16.9235 - val_loss: 46.8593\n",
      "Epoch 294/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.9049 - val_loss: 47.0829\n",
      "Epoch 295/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.8875 - val_loss: 47.2985\n",
      "Epoch 296/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.8713 - val_loss: 47.5065\n",
      "Epoch 297/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 16.8562 - val_loss: 47.7070\n",
      "Epoch 298/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.8420 - val_loss: 47.8999\n",
      "Epoch 299/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.8288 - val_loss: 48.0858\n",
      "Epoch 300/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.8165 - val_loss: 48.2645\n",
      "Epoch 301/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.8050 - val_loss: 48.4361\n",
      "Epoch 302/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.7944 - val_loss: 48.6015\n",
      "Epoch 303/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.7844 - val_loss: 48.7604\n",
      "Epoch 304/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.7750 - val_loss: 48.9126\n",
      "Epoch 305/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.7664 - val_loss: 49.0587\n",
      "Epoch 306/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.7583 - val_loss: 49.1990\n",
      "Epoch 307/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.7508 - val_loss: 49.3336\n",
      "Epoch 308/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.7438 - val_loss: 49.4626\n",
      "Epoch 309/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.7372 - val_loss: 49.5860\n",
      "Epoch 310/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.7312 - val_loss: 49.7042\n",
      "Epoch 311/500\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 16.7255 - val_loss: 49.8172\n",
      "Epoch 312/500\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 16.7202 - val_loss: 49.9252\n",
      "Epoch 313/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.7153 - val_loss: 50.0289\n",
      "Epoch 314/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.7107 - val_loss: 50.1275\n",
      "Epoch 315/500\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 16.7065 - val_loss: 50.2221\n",
      "Epoch 316/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 16.7025 - val_loss: 50.3122\n",
      "Epoch 317/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 16.6989 - val_loss: 50.3982\n",
      "Epoch 318/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6955 - val_loss: 50.4799\n",
      "Epoch 319/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6923 - val_loss: 50.5581\n",
      "Epoch 320/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6894 - val_loss: 50.6327\n",
      "Epoch 321/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6867 - val_loss: 50.7038\n",
      "Epoch 322/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6841 - val_loss: 50.7715\n",
      "Epoch 323/500\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 16.6818 - val_loss: 50.8355\n",
      "Epoch 324/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 16.6796 - val_loss: 50.8966\n",
      "Epoch 325/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6776 - val_loss: 50.9547\n",
      "Epoch 326/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6757 - val_loss: 51.0097\n",
      "Epoch 327/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6740 - val_loss: 51.0619\n",
      "Epoch 328/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6725 - val_loss: 51.1116\n",
      "Epoch 329/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6710 - val_loss: 51.1589\n",
      "Epoch 330/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6696 - val_loss: 51.2033\n",
      "Epoch 331/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 16.6685 - val_loss: 51.2458\n",
      "Epoch 332/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6673 - val_loss: 51.2858\n",
      "Epoch 333/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6662 - val_loss: 51.3234\n",
      "Epoch 334/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6653 - val_loss: 51.3593\n",
      "Epoch 335/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6645 - val_loss: 51.3932\n",
      "Epoch 336/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6637 - val_loss: 51.4249\n",
      "Epoch 337/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 16.6631 - val_loss: 51.4554\n",
      "Epoch 338/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6624 - val_loss: 51.4836\n",
      "Epoch 339/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6619 - val_loss: 51.5107\n",
      "Epoch 340/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6613 - val_loss: 51.5358\n",
      "Epoch 341/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6609 - val_loss: 51.5596\n",
      "Epoch 342/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6605 - val_loss: 51.5817\n",
      "Epoch 343/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 16.6602 - val_loss: 51.6030\n",
      "Epoch 344/500\n",
      "18/18 [==============================] - 0s 17ms/step - loss: 16.6599 - val_loss: 51.6224\n",
      "Epoch 345/500\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 16.6597 - val_loss: 51.6412\n",
      "Epoch 346/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 16.6595 - val_loss: 51.6586\n",
      "Epoch 347/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 16.6593 - val_loss: 51.6751\n",
      "Epoch 348/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 16.6592 - val_loss: 51.6906\n",
      "Epoch 349/500\n",
      "18/18 [==============================] - 0s 18ms/step - loss: 16.6591 - val_loss: 51.7045\n",
      "Epoch 350/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 16.6591 - val_loss: 51.7181\n",
      "Epoch 351/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 16.6591 - val_loss: 51.7306\n",
      "Epoch 352/500\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 16.6591 - val_loss: 51.7422\n",
      "Epoch 353/500\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 16.6592 - val_loss: 51.7534\n",
      "Epoch 354/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 16.6592 - val_loss: 51.7632\n",
      "Epoch 355/500\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 16.6593 - val_loss: 51.7731\n",
      "Epoch 356/500\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 16.6594 - val_loss: 51.7817\n",
      "Epoch 357/500\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 16.6595 - val_loss: 51.7898\n",
      "Epoch 358/500\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 16.6598 - val_loss: 51.7975\n",
      "Epoch 359/500\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 16.6599 - val_loss: 51.8047\n",
      "Epoch 360/500\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 16.6601 - val_loss: 51.8111\n",
      "Epoch 361/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6603 - val_loss: 51.8173\n",
      "Epoch 362/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6605 - val_loss: 51.8228\n",
      "Epoch 363/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6608 - val_loss: 51.8282\n",
      "Epoch 364/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6611 - val_loss: 51.8329\n",
      "Epoch 365/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 16.6613 - val_loss: 51.8373\n",
      "Epoch 366/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6616 - val_loss: 51.8413\n",
      "Epoch 367/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6619 - val_loss: 51.8450\n",
      "Epoch 368/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6622 - val_loss: 51.8484\n",
      "Epoch 369/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6625 - val_loss: 51.8519\n",
      "Epoch 370/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6629 - val_loss: 51.8548\n",
      "Epoch 371/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6631 - val_loss: 51.8574\n",
      "Epoch 372/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6635 - val_loss: 51.8598\n",
      "Epoch 373/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6638 - val_loss: 51.8615\n",
      "Epoch 374/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6642 - val_loss: 51.8636\n",
      "Epoch 375/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.6645 - val_loss: 51.8651\n",
      "Epoch 376/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 16.6649 - val_loss: 51.8667\n",
      "Epoch 377/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 16.6653 - val_loss: 51.8679\n",
      "Epoch 378/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6656 - val_loss: 51.8692\n",
      "Epoch 379/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6660 - val_loss: 51.8701\n",
      "Epoch 380/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6664 - val_loss: 51.8710\n",
      "Epoch 381/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6667 - val_loss: 51.8718\n",
      "Epoch 382/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6671 - val_loss: 51.8724\n",
      "Epoch 383/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6675 - val_loss: 51.8728\n",
      "Epoch 384/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6679 - val_loss: 51.8732\n",
      "Epoch 385/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6683 - val_loss: 51.8735\n",
      "Epoch 386/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6687 - val_loss: 51.8736\n",
      "Epoch 387/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 16.6691 - val_loss: 51.8736\n",
      "Epoch 388/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6694 - val_loss: 51.8736\n",
      "Epoch 389/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.6699 - val_loss: 51.8737\n",
      "Epoch 390/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6703 - val_loss: 51.8737\n",
      "Epoch 391/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6707 - val_loss: 51.8737\n",
      "Epoch 392/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6711 - val_loss: 51.8736\n",
      "Epoch 393/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6714 - val_loss: 51.8731\n",
      "Epoch 394/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6718 - val_loss: 51.8726\n",
      "Epoch 395/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6723 - val_loss: 51.8725\n",
      "Epoch 396/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.6727 - val_loss: 51.8721\n",
      "Epoch 397/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6731 - val_loss: 51.8718\n",
      "Epoch 398/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.6734 - val_loss: 51.8714\n",
      "Epoch 399/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.6738 - val_loss: 51.8706\n",
      "Epoch 400/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6743 - val_loss: 51.8701\n",
      "Epoch 401/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 16.6746 - val_loss: 51.8697\n",
      "Epoch 402/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6750 - val_loss: 51.8693\n",
      "Epoch 403/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6754 - val_loss: 51.8686\n",
      "Epoch 404/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6758 - val_loss: 51.8682\n",
      "Epoch 405/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6762 - val_loss: 51.8677\n",
      "Epoch 406/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6766 - val_loss: 51.8670\n",
      "Epoch 407/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6769 - val_loss: 51.8662\n",
      "Epoch 408/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6773 - val_loss: 51.8656\n",
      "Epoch 409/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6777 - val_loss: 51.8648\n",
      "Epoch 410/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6781 - val_loss: 51.8644\n",
      "Epoch 411/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6785 - val_loss: 51.8638\n",
      "Epoch 412/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6789 - val_loss: 51.8630\n",
      "Epoch 413/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6792 - val_loss: 51.8622\n",
      "Epoch 414/500\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 16.6796 - val_loss: 51.8616\n",
      "Epoch 415/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6799 - val_loss: 51.8611\n",
      "Epoch 416/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 16.6803 - val_loss: 51.8604\n",
      "Epoch 417/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 16.6807 - val_loss: 51.8596\n",
      "Epoch 418/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 16.6810 - val_loss: 51.8589\n",
      "Epoch 419/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6814 - val_loss: 51.8584\n",
      "Epoch 420/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6818 - val_loss: 51.8576\n",
      "Epoch 421/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6821 - val_loss: 51.8567\n",
      "Epoch 422/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6825 - val_loss: 51.8560\n",
      "Epoch 423/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 16.6828 - val_loss: 51.8551\n",
      "Epoch 424/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6831 - val_loss: 51.8542\n",
      "Epoch 425/500\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 16.6835 - val_loss: 51.8535\n",
      "Epoch 426/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6838 - val_loss: 51.8530\n",
      "Epoch 427/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6842 - val_loss: 51.8523\n",
      "Epoch 428/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6845 - val_loss: 51.8516\n",
      "Epoch 429/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6848 - val_loss: 51.8508\n",
      "Epoch 430/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6852 - val_loss: 51.8502\n",
      "Epoch 431/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6855 - val_loss: 51.8495\n",
      "Epoch 432/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6858 - val_loss: 51.8490\n",
      "Epoch 433/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.6861 - val_loss: 51.8480\n",
      "Epoch 434/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.6865 - val_loss: 51.8476\n",
      "Epoch 435/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.6868 - val_loss: 51.8470\n",
      "Epoch 436/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.6871 - val_loss: 51.8466\n",
      "Epoch 437/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 16.6874 - val_loss: 51.8459\n",
      "Epoch 438/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6877 - val_loss: 51.8453\n",
      "Epoch 439/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6880 - val_loss: 51.8448\n",
      "Epoch 440/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6883 - val_loss: 51.8440\n",
      "Epoch 441/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 16.6886 - val_loss: 51.8433\n",
      "Epoch 442/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 16.6889 - val_loss: 51.8430\n",
      "Epoch 443/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6892 - val_loss: 51.8420\n",
      "Epoch 444/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6894 - val_loss: 51.8415\n",
      "Epoch 445/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6898 - val_loss: 51.8410\n",
      "Epoch 446/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6900 - val_loss: 51.8402\n",
      "Epoch 447/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6904 - val_loss: 51.8395\n",
      "Epoch 448/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6906 - val_loss: 51.8391\n",
      "Epoch 449/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 16.6909 - val_loss: 51.8387\n",
      "Epoch 450/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6911 - val_loss: 51.8382\n",
      "Epoch 451/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.6914 - val_loss: 51.8378\n",
      "Epoch 452/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6916 - val_loss: 51.8375\n",
      "Epoch 453/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6919 - val_loss: 51.8366\n",
      "Epoch 454/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6922 - val_loss: 51.8360\n",
      "Epoch 455/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6924 - val_loss: 51.8354\n",
      "Epoch 456/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6927 - val_loss: 51.8349\n",
      "Epoch 457/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.6929 - val_loss: 51.8342\n",
      "Epoch 458/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6932 - val_loss: 51.8338\n",
      "Epoch 459/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 16.6934 - val_loss: 51.8337\n",
      "Epoch 460/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6937 - val_loss: 51.8329\n",
      "Epoch 461/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6940 - val_loss: 51.8323\n",
      "Epoch 462/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.6942 - val_loss: 51.8318\n",
      "Epoch 463/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6944 - val_loss: 51.8313\n",
      "Epoch 464/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6946 - val_loss: 51.8306\n",
      "Epoch 465/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6949 - val_loss: 51.8303\n",
      "Epoch 466/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.6951 - val_loss: 51.8299\n",
      "Epoch 467/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6953 - val_loss: 51.8296\n",
      "Epoch 468/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.6955 - val_loss: 51.8289\n",
      "Epoch 469/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6958 - val_loss: 51.8284\n",
      "Epoch 470/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.6960 - val_loss: 51.8281\n",
      "Epoch 471/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6962 - val_loss: 51.8276\n",
      "Epoch 472/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6964 - val_loss: 51.8271\n",
      "Epoch 473/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6966 - val_loss: 51.8268\n",
      "Epoch 474/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6968 - val_loss: 51.8265\n",
      "Epoch 475/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6970 - val_loss: 51.8263\n",
      "Epoch 476/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6971 - val_loss: 51.8258\n",
      "Epoch 477/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6973 - val_loss: 51.8251\n",
      "Epoch 478/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.6976 - val_loss: 51.8246\n",
      "Epoch 479/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.6978 - val_loss: 51.8244\n",
      "Epoch 480/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6980 - val_loss: 51.8240\n",
      "Epoch 481/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6981 - val_loss: 51.8235\n",
      "Epoch 482/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6984 - val_loss: 51.8232\n",
      "Epoch 483/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.6985 - val_loss: 51.8228\n",
      "Epoch 484/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6987 - val_loss: 51.8224\n",
      "Epoch 485/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6989 - val_loss: 51.8221\n",
      "Epoch 486/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6990 - val_loss: 51.8213\n",
      "Epoch 487/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.6992 - val_loss: 51.8211\n",
      "Epoch 488/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 16.6994 - val_loss: 51.8207\n",
      "Epoch 489/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.6996 - val_loss: 51.8204\n",
      "Epoch 490/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.6997 - val_loss: 51.8201\n",
      "Epoch 491/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.6999 - val_loss: 51.8197\n",
      "Epoch 492/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.7000 - val_loss: 51.8193\n",
      "Epoch 493/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 16.7002 - val_loss: 51.8188\n",
      "Epoch 494/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 16.7004 - val_loss: 51.8185\n",
      "Epoch 495/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.7005 - val_loss: 51.8184\n",
      "Epoch 496/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.7006 - val_loss: 51.8180\n",
      "Epoch 497/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.7008 - val_loss: 51.8176\n",
      "Epoch 498/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.7010 - val_loss: 51.8172\n",
      "Epoch 499/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 16.7011 - val_loss: 51.8169\n",
      "Epoch 500/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.7013 - val_loss: 51.8169\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.8407, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0015, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0118, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.5775, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 432ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.78130952e+01, 6.77710784e+01, 6.77290616e+01, 6.76870448e+01,\n",
       "        6.76450280e+01, 6.76030112e+01, 6.75609944e+01, 6.75189776e+01,\n",
       "        6.74769608e+01, 6.74349440e+01, 6.73929272e+01, 6.73509104e+01,\n",
       "        6.73088936e+01, 6.72668767e+01, 6.72248599e+01, 6.71828431e+01,\n",
       "        6.71408263e+01, 6.70988095e+01, 6.70567927e+01, 6.70147759e+01,\n",
       "        6.69727591e+01, 6.69307423e+01, 6.68887255e+01, 6.68467087e+01,\n",
       "        6.68046919e+01, 6.67626751e+01, 6.67206583e+01, 6.66886088e+01,\n",
       "        6.66661998e+01, 6.66437909e+01, 6.66213819e+01, 6.65989729e+01,\n",
       "        6.65765640e+01, 6.65541550e+01, 6.65317460e+01, 6.65093371e+01,\n",
       "        6.64869281e+01, 6.64645191e+01, 6.64421102e+01, 6.64197012e+01,\n",
       "        6.63972923e+01, 6.63748833e+01, 6.63524743e+01, 6.63300654e+01,\n",
       "        6.63076564e+01, 6.62852474e+01, 6.62628385e+01, 6.62404295e+01,\n",
       "        6.62180205e+01, 6.61956116e+01, 6.61732026e+01, 6.61507936e+01,\n",
       "        6.61283847e+01, 6.61059757e+01, 6.60835668e+01, 6.60611578e+01,\n",
       "        6.60387488e+01, 6.60163399e+01, 6.59939309e+01, 6.59715219e+01,\n",
       "        6.59491130e+01, 6.59267040e+01, 6.59042951e+01, 6.58977358e+01,\n",
       "        6.58949346e+01, 6.58921335e+01, 6.58893324e+01, 6.58865313e+01,\n",
       "        6.58837302e+01, 6.58809290e+01, 6.58781279e+01, 6.58753268e+01,\n",
       "        6.58725257e+01, 6.58697246e+01, 6.58669234e+01, 6.58641223e+01,\n",
       "        6.58613212e+01, 6.58585201e+01, 6.58557189e+01, 6.58529178e+01,\n",
       "        7.39060822e+01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.45789182e-01, 5.69568798e-02, 3.43369305e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.89334118e-01, 1.14160426e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([64.70739963, 64.69712885, 64.68685808, 64.6765873 , 64.66631653,\n",
       "       64.65604575, 64.64577498, 64.6355042 , 64.62523343, 64.61496265,\n",
       "       64.60469188, 64.5944211 , 64.58415033, 64.57387955, 64.56360878,\n",
       "       64.553338  , 64.54306723, 64.53279645, 64.52252568, 64.5122549 ,\n",
       "       64.50198413, 64.49171335, 64.48144258, 64.4711718 , 64.46090103,\n",
       "       64.45063025, 64.44035948, 64.4300887 , 64.41981793, 64.40954715,\n",
       "       64.39927638, 64.3890056 , 64.37873483, 64.36846405, 64.35819328,\n",
       "       64.3479225 , 64.33765173, 64.32738095, 64.31711018, 64.3068394 ,\n",
       "       64.29656863, 64.28629785, 64.27602708, 64.2657563 , 64.25548553,\n",
       "       64.24521475, 64.23494398, 64.2246732 , 64.21440243, 64.20413165,\n",
       "       64.19386088, 64.1835901 , 64.17331933, 64.16304855, 64.15277778,\n",
       "       64.142507  , 64.13223623, 64.12196545, 64.11169468, 64.1014239 ,\n",
       "       64.09115313, 64.08088235, 64.07061158, 64.0603408 , 64.05007003,\n",
       "       64.03979925, 64.02952848, 64.0192577 , 64.00898693, 63.99912465,\n",
       "       63.99212185, 63.98511905, 63.97811625, 63.97111345, 63.96411064,\n",
       "       63.95710784, 63.95010504, 63.94310224, 63.93609944, 63.92909664,\n",
       "       63.92209384, 63.91509104, 63.90808824, 63.90108543, 63.89408263,\n",
       "       63.88707983, 63.88007703, 63.87307423, 63.86607143, 63.85906863,\n",
       "       63.85206583, 63.84506303, 63.83806022, 63.83105742, 63.82405462,\n",
       "       63.81705182, 63.81004902, 63.80304622, 63.79604342, 63.78904062])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.881631249670967\n",
      "13.997615726018783\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
