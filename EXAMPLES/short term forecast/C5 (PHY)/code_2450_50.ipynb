{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.140000\n",
       "1       88.911429\n",
       "2       88.682857\n",
       "3       88.454286\n",
       "4       88.225714\n",
       "          ...    \n",
       "2495    59.631700\n",
       "2496    59.623775\n",
       "2497    59.615850\n",
       "2498    59.607925\n",
       "2499    59.600000\n",
       "Name: C5, Length: 2500, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c5_interpolated_2450_50.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.140000\n",
       "1       88.911429\n",
       "2       88.682857\n",
       "3       88.454286\n",
       "4       88.225714\n",
       "          ...    \n",
       "2445     0.000000\n",
       "2446     0.685558\n",
       "2447     0.000000\n",
       "2448     0.296425\n",
       "2449     0.159097\n",
       "Name: C5, Length: 2450, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(50)\n",
    "test\n",
    "training_set = training_set.head(2450)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.140000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.911429</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.682857</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.454286</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.225714</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     89.140000  0.000298\n",
       "1     88.911429  0.000298\n",
       "2     88.682857  0.000297\n",
       "3     88.454286  0.000297\n",
       "4     88.225714  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.8407  # Value for C0\n",
    "K0 = -0.0015  # Value for K0\n",
    "K1 = -0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0118    # Value for b\n",
    "c = 2.5775    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-175)  var1(t-174)  var1(t-173)  var1(t-172)  var1(t-171)  \\\n",
      "175    89.140000    88.911429    88.682857    88.454286    88.225714   \n",
      "176    88.911429    88.682857    88.454286    88.225714    87.997143   \n",
      "177    88.682857    88.454286    88.225714    87.997143    87.768571   \n",
      "178    88.454286    88.225714    87.997143    87.768571    87.540000   \n",
      "179    88.225714    87.997143    87.768571    87.540000    87.311429   \n",
      "\n",
      "     var1(t-170)  var1(t-169)  var1(t-168)  var1(t-167)  var1(t-166)  ...  \\\n",
      "175    87.997143    87.768571    87.540000    87.311429    87.095798  ...   \n",
      "176    87.768571    87.540000    87.311429    87.095798    87.039776  ...   \n",
      "177    87.540000    87.311429    87.095798    87.039776    86.983754  ...   \n",
      "178    87.311429    87.095798    87.039776    86.983754    86.927731  ...   \n",
      "179    87.095798    87.039776    86.983754    86.927731    86.871709  ...   \n",
      "\n",
      "     var1(t+45)  var2(t+45)  var1(t+46)  var2(t+46)  var1(t+47)  var2(t+47)  \\\n",
      "175   82.338772    0.000280   82.321032    0.000280   82.303291    0.000280   \n",
      "176   82.321032    0.000280   82.303291    0.000280   82.285551    0.000280   \n",
      "177   82.303291    0.000280   82.285551    0.000280   82.267810    0.000279   \n",
      "178   82.285551    0.000280   82.267810    0.000279   82.250070    0.000279   \n",
      "179   82.267810    0.000279   82.250070    0.000279   82.232330    0.000279   \n",
      "\n",
      "     var1(t+48)  var2(t+48)  var1(t+49)  var2(t+49)  \n",
      "175   82.285551    0.000280   82.267810    0.000279  \n",
      "176   82.267810    0.000279   82.250070    0.000279  \n",
      "177   82.250070    0.000279   82.232330    0.000279  \n",
      "178   82.232330    0.000279   82.214589    0.000279  \n",
      "179   82.214589    0.000279   82.196849    0.000279  \n",
      "\n",
      "[5 rows x 276 columns]\n",
      "Index(['var1(t-175)', 'var1(t-174)', 'var1(t-173)', 'var1(t-172)',\n",
      "       'var1(t-171)', 'var1(t-170)', 'var1(t-169)', 'var1(t-168)',\n",
      "       'var1(t-167)', 'var1(t-166)',\n",
      "       ...\n",
      "       'var1(t+45)', 'var2(t+45)', 'var1(t+46)', 'var2(t+46)', 'var1(t+47)',\n",
      "       'var2(t+47)', 'var1(t+48)', 'var2(t+48)', 'var1(t+49)', 'var2(t+49)'],\n",
      "      dtype='object', length=276)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 175, n_out = 50)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 176):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-150:]\n",
    "trainX = train[:,:-150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-150:]\n",
    "forecastX = forecast[:,:-150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225, 1, 126) (2225, 150) (1, 1, 126)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "28/28 [==============================] - 2s 19ms/step - loss: 5026.5669 - val_loss: 2947.2515\n",
      "Epoch 2/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4900.7686 - val_loss: 2892.3188\n",
      "Epoch 3/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4794.3247 - val_loss: 2842.8772\n",
      "Epoch 4/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4699.7451 - val_loss: 2795.4607\n",
      "Epoch 5/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4606.0488 - val_loss: 2747.1274\n",
      "Epoch 6/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4508.4258 - val_loss: 2698.5000\n",
      "Epoch 7/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4413.1987 - val_loss: 2651.4116\n",
      "Epoch 8/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4320.5552 - val_loss: 2605.5056\n",
      "Epoch 9/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 4229.9434 - val_loss: 2560.6040\n",
      "Epoch 10/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4141.0928 - val_loss: 2516.6182\n",
      "Epoch 11/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 4053.8608 - val_loss: 2473.4966\n",
      "Epoch 12/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3968.1611 - val_loss: 2431.2039\n",
      "Epoch 13/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3883.9312 - val_loss: 2389.7148\n",
      "Epoch 14/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3801.1272 - val_loss: 2349.0088\n",
      "Epoch 15/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3719.7119 - val_loss: 2309.0684\n",
      "Epoch 16/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3639.6553 - val_loss: 2269.8787\n",
      "Epoch 17/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3560.9304 - val_loss: 2231.4260\n",
      "Epoch 18/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3483.5134 - val_loss: 2193.6985\n",
      "Epoch 19/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3407.3826 - val_loss: 2156.6843\n",
      "Epoch 20/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3332.5183 - val_loss: 2120.3733\n",
      "Epoch 21/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3258.9019 - val_loss: 2084.7544\n",
      "Epoch 22/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3186.5146 - val_loss: 2049.8184\n",
      "Epoch 23/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3115.3396 - val_loss: 2015.5557\n",
      "Epoch 24/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3045.3613 - val_loss: 1981.9568\n",
      "Epoch 25/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2976.5637 - val_loss: 1949.0133\n",
      "Epoch 26/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2908.9307 - val_loss: 1916.7162\n",
      "Epoch 27/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2842.4480 - val_loss: 1885.0573\n",
      "Epoch 28/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2777.1013 - val_loss: 1854.0282\n",
      "Epoch 29/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 2712.8765 - val_loss: 1823.6205\n",
      "Epoch 30/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2649.7581 - val_loss: 1793.8263\n",
      "Epoch 31/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2587.7351 - val_loss: 1764.6382\n",
      "Epoch 32/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2526.7922 - val_loss: 1736.0476\n",
      "Epoch 33/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2466.9167 - val_loss: 1708.0480\n",
      "Epoch 34/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2408.0957 - val_loss: 1680.6309\n",
      "Epoch 35/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2350.3167 - val_loss: 1653.7891\n",
      "Epoch 36/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2293.5669 - val_loss: 1627.5156\n",
      "Epoch 37/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2237.8347 - val_loss: 1601.8029\n",
      "Epoch 38/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2183.1062 - val_loss: 1576.6436\n",
      "Epoch 39/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2129.3701 - val_loss: 1552.0309\n",
      "Epoch 40/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2076.6150 - val_loss: 1527.9578\n",
      "Epoch 41/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2024.8284 - val_loss: 1504.4170\n",
      "Epoch 42/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1973.9989 - val_loss: 1481.4023\n",
      "Epoch 43/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1924.1156 - val_loss: 1458.9065\n",
      "Epoch 44/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1875.1660 - val_loss: 1436.9227\n",
      "Epoch 45/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1827.1384 - val_loss: 1415.4441\n",
      "Epoch 46/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1780.0236 - val_loss: 1394.4645\n",
      "Epoch 47/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1733.8087 - val_loss: 1373.9774\n",
      "Epoch 48/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1688.4845 - val_loss: 1353.9761\n",
      "Epoch 49/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1644.0391 - val_loss: 1334.4541\n",
      "Epoch 50/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1600.4617 - val_loss: 1315.4054\n",
      "Epoch 51/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 1557.7422 - val_loss: 1296.8230\n",
      "Epoch 52/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 1515.8687 - val_loss: 1278.7007\n",
      "Epoch 53/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 1474.8326 - val_loss: 1261.0325\n",
      "Epoch 54/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1434.6212 - val_loss: 1243.8121\n",
      "Epoch 55/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1395.2266 - val_loss: 1227.0332\n",
      "Epoch 56/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 1356.6371 - val_loss: 1210.6896\n",
      "Epoch 57/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1318.8427 - val_loss: 1194.7755\n",
      "Epoch 58/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1281.8336 - val_loss: 1179.2847\n",
      "Epoch 59/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1245.5995 - val_loss: 1164.2109\n",
      "Epoch 60/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1210.1307 - val_loss: 1149.5487\n",
      "Epoch 61/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1175.4172 - val_loss: 1135.2914\n",
      "Epoch 62/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1141.4493 - val_loss: 1121.4336\n",
      "Epoch 63/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1108.2175 - val_loss: 1107.9694\n",
      "Epoch 64/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1075.7115 - val_loss: 1094.8928\n",
      "Epoch 65/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1043.9224 - val_loss: 1082.1979\n",
      "Epoch 66/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1012.8402 - val_loss: 1069.8790\n",
      "Epoch 67/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 982.4555 - val_loss: 1057.9303\n",
      "Epoch 68/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 952.7585 - val_loss: 1046.3462\n",
      "Epoch 69/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 923.7407 - val_loss: 1035.1207\n",
      "Epoch 70/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 895.3923 - val_loss: 1024.2483\n",
      "Epoch 71/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 867.7037 - val_loss: 1013.7233\n",
      "Epoch 72/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 840.6660 - val_loss: 1003.5401\n",
      "Epoch 73/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 814.2692 - val_loss: 993.6926\n",
      "Epoch 74/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 788.5051 - val_loss: 984.1758\n",
      "Epoch 75/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 763.3641 - val_loss: 974.9838\n",
      "Epoch 76/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 738.8376 - val_loss: 966.1111\n",
      "Epoch 77/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 714.9161 - val_loss: 957.5522\n",
      "Epoch 78/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 691.5905 - val_loss: 949.3016\n",
      "Epoch 79/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 668.8525 - val_loss: 941.3537\n",
      "Epoch 80/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 646.6928 - val_loss: 933.7028\n",
      "Epoch 81/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 625.1026 - val_loss: 926.3441\n",
      "Epoch 82/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 604.0729 - val_loss: 919.2716\n",
      "Epoch 83/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 583.5952 - val_loss: 912.4801\n",
      "Epoch 84/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 563.6607 - val_loss: 905.9641\n",
      "Epoch 85/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 544.2606 - val_loss: 899.7181\n",
      "Epoch 86/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 525.3861 - val_loss: 893.7370\n",
      "Epoch 87/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 507.0290 - val_loss: 888.0153\n",
      "Epoch 88/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 489.1801 - val_loss: 882.5475\n",
      "Epoch 89/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 471.8307 - val_loss: 877.3284\n",
      "Epoch 90/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 454.9730 - val_loss: 872.3530\n",
      "Epoch 91/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 438.5982 - val_loss: 867.6156\n",
      "Epoch 92/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 422.6979 - val_loss: 863.1111\n",
      "Epoch 93/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 407.2632 - val_loss: 858.8340\n",
      "Epoch 94/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 392.2862 - val_loss: 854.7798\n",
      "Epoch 95/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 377.7584 - val_loss: 850.9423\n",
      "Epoch 96/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 363.6712 - val_loss: 847.3171\n",
      "Epoch 97/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 350.0168 - val_loss: 843.8986\n",
      "Epoch 98/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 336.7864 - val_loss: 840.6819\n",
      "Epoch 99/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 323.9721 - val_loss: 837.6617\n",
      "Epoch 100/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 311.5656 - val_loss: 834.8329\n",
      "Epoch 101/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 299.5586 - val_loss: 832.1904\n",
      "Epoch 102/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 287.9431 - val_loss: 829.7293\n",
      "Epoch 103/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 276.7108 - val_loss: 827.4445\n",
      "Epoch 104/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 265.8543 - val_loss: 825.3310\n",
      "Epoch 105/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 255.3650 - val_loss: 823.3839\n",
      "Epoch 106/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 245.2354 - val_loss: 821.5980\n",
      "Epoch 107/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 235.4573 - val_loss: 819.9686\n",
      "Epoch 108/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 226.0226 - val_loss: 818.4907\n",
      "Epoch 109/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 216.9240 - val_loss: 817.1595\n",
      "Epoch 110/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 208.1534 - val_loss: 815.9702\n",
      "Epoch 111/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 199.7032 - val_loss: 814.9182\n",
      "Epoch 112/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 191.5659 - val_loss: 813.9984\n",
      "Epoch 113/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 183.7333 - val_loss: 813.2061\n",
      "Epoch 114/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 176.1983 - val_loss: 812.5368\n",
      "Epoch 115/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 168.9534 - val_loss: 811.9860\n",
      "Epoch 116/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 161.9913 - val_loss: 811.5488\n",
      "Epoch 117/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 155.3042 - val_loss: 811.2208\n",
      "Epoch 118/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 148.8852 - val_loss: 810.9976\n",
      "Epoch 119/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 142.7267 - val_loss: 810.8745\n",
      "Epoch 120/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 136.8216 - val_loss: 810.8475\n",
      "Epoch 121/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.1627 - val_loss: 810.9118\n",
      "Epoch 122/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 125.7432 - val_loss: 811.0634\n",
      "Epoch 123/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 120.5560 - val_loss: 811.2979\n",
      "Epoch 124/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 115.5940 - val_loss: 811.6113\n",
      "Epoch 125/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 110.8507 - val_loss: 811.9995\n",
      "Epoch 126/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 106.3190 - val_loss: 812.4582\n",
      "Epoch 127/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 101.9927 - val_loss: 812.9836\n",
      "Epoch 128/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 97.8648 - val_loss: 813.5718\n",
      "Epoch 129/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 93.9293 - val_loss: 814.2189\n",
      "Epoch 130/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 90.1794 - val_loss: 814.9211\n",
      "Epoch 131/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 86.6091 - val_loss: 815.6748\n",
      "Epoch 132/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 83.2121 - val_loss: 816.4762\n",
      "Epoch 133/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 79.9824 - val_loss: 817.3218\n",
      "Epoch 134/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 76.9139 - val_loss: 818.2082\n",
      "Epoch 135/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 74.0008 - val_loss: 819.1321\n",
      "Epoch 136/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 71.2371 - val_loss: 820.0900\n",
      "Epoch 137/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 68.6173 - val_loss: 821.0787\n",
      "Epoch 138/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 66.1359 - val_loss: 822.0951\n",
      "Epoch 139/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 63.7875 - val_loss: 823.1362\n",
      "Epoch 140/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 61.5664 - val_loss: 824.1989\n",
      "Epoch 141/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 59.4678 - val_loss: 825.2804\n",
      "Epoch 142/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 57.4863 - val_loss: 826.3779\n",
      "Epoch 143/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 55.6171 - val_loss: 827.4888\n",
      "Epoch 144/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 53.8554 - val_loss: 828.6105\n",
      "Epoch 145/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 52.1963 - val_loss: 829.7405\n",
      "Epoch 146/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 50.6351 - val_loss: 830.8760\n",
      "Epoch 147/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 49.1675 - val_loss: 832.0154\n",
      "Epoch 148/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 47.7890 - val_loss: 833.1562\n",
      "Epoch 149/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 46.4954 - val_loss: 834.2959\n",
      "Epoch 150/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 45.2827 - val_loss: 835.4330\n",
      "Epoch 151/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 44.1467 - val_loss: 836.5654\n",
      "Epoch 152/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 43.0837 - val_loss: 837.6913\n",
      "Epoch 153/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 42.0899 - val_loss: 838.8088\n",
      "Epoch 154/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 41.1617 - val_loss: 839.9166\n",
      "Epoch 155/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 40.2956 - val_loss: 841.0129\n",
      "Epoch 156/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 39.4883 - val_loss: 842.0965\n",
      "Epoch 157/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 38.7363 - val_loss: 843.1658\n",
      "Epoch 158/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 38.0369 - val_loss: 844.2197\n",
      "Epoch 159/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 37.3868 - val_loss: 845.2573\n",
      "Epoch 160/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 36.7832 - val_loss: 846.2771\n",
      "Epoch 161/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 36.2234 - val_loss: 847.2786\n",
      "Epoch 162/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 35.7046 - val_loss: 848.2604\n",
      "Epoch 163/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 35.2245 - val_loss: 849.2219\n",
      "Epoch 164/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 34.7806 - val_loss: 850.1626\n",
      "Epoch 165/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 34.3706 - val_loss: 851.0817\n",
      "Epoch 166/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 33.9923 - val_loss: 851.9786\n",
      "Epoch 167/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 33.6437 - val_loss: 852.8530\n",
      "Epoch 168/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 33.3226 - val_loss: 853.7043\n",
      "Epoch 169/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 33.0275 - val_loss: 854.5325\n",
      "Epoch 170/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 32.7563 - val_loss: 855.3370\n",
      "Epoch 171/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 32.5073 - val_loss: 856.1179\n",
      "Epoch 172/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 32.2791 - val_loss: 856.8746\n",
      "Epoch 173/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 32.0703 - val_loss: 857.6074\n",
      "Epoch 174/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 31.8792 - val_loss: 858.3166\n",
      "Epoch 175/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 31.7046 - val_loss: 859.0019\n",
      "Epoch 176/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 31.5451 - val_loss: 859.6633\n",
      "Epoch 177/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 31.3998 - val_loss: 860.3013\n",
      "Epoch 178/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 31.2674 - val_loss: 860.9158\n",
      "Epoch 179/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 31.1470 - val_loss: 861.5076\n",
      "Epoch 180/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 31.0375 - val_loss: 862.0763\n",
      "Epoch 181/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.9381 - val_loss: 862.6221\n",
      "Epoch 182/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.8480 - val_loss: 863.1462\n",
      "Epoch 183/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.7663 - val_loss: 863.6483\n",
      "Epoch 184/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.6925 - val_loss: 864.1292\n",
      "Epoch 185/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.6257 - val_loss: 864.5890\n",
      "Epoch 186/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.5654 - val_loss: 865.0286\n",
      "Epoch 187/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.5110 - val_loss: 865.4482\n",
      "Epoch 188/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.4619 - val_loss: 865.8483\n",
      "Epoch 189/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.4178 - val_loss: 866.2294\n",
      "Epoch 190/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.3781 - val_loss: 866.5925\n",
      "Epoch 191/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.3425 - val_loss: 866.9375\n",
      "Epoch 192/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.3105 - val_loss: 867.2652\n",
      "Epoch 193/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 30.2818 - val_loss: 867.5763\n",
      "Epoch 194/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 30.2562 - val_loss: 867.8710\n",
      "Epoch 195/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.2332 - val_loss: 868.1503\n",
      "Epoch 196/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.2127 - val_loss: 868.4144\n",
      "Epoch 197/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1946 - val_loss: 868.6642\n",
      "Epoch 198/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1782 - val_loss: 868.9000\n",
      "Epoch 199/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1638 - val_loss: 869.1224\n",
      "Epoch 200/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1510 - val_loss: 869.3321\n",
      "Epoch 201/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1396 - val_loss: 869.5295\n",
      "Epoch 202/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1295 - val_loss: 869.7151\n",
      "Epoch 203/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1207 - val_loss: 869.8896\n",
      "Epoch 204/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1128 - val_loss: 870.0535\n",
      "Epoch 205/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1059 - val_loss: 870.2067\n",
      "Epoch 206/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1000 - val_loss: 870.3505\n",
      "Epoch 207/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0947 - val_loss: 870.4851\n",
      "Epoch 208/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.0900 - val_loss: 870.6107\n",
      "Epoch 209/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0862 - val_loss: 870.7282\n",
      "Epoch 210/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.0828 - val_loss: 870.8376\n",
      "Epoch 211/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0800 - val_loss: 870.9399\n",
      "Epoch 212/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0775 - val_loss: 871.0349\n",
      "Epoch 213/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0755 - val_loss: 871.1230\n",
      "Epoch 214/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0739 - val_loss: 871.2053\n",
      "Epoch 215/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 30.0726 - val_loss: 871.2816\n",
      "Epoch 216/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0715 - val_loss: 871.3521\n",
      "Epoch 217/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0707 - val_loss: 871.4173\n",
      "Epoch 218/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0702 - val_loss: 871.4776\n",
      "Epoch 219/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0698 - val_loss: 871.5331\n",
      "Epoch 220/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0697 - val_loss: 871.5845\n",
      "Epoch 221/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0698 - val_loss: 871.6320\n",
      "Epoch 222/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0699 - val_loss: 871.6755\n",
      "Epoch 223/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.0703 - val_loss: 871.7153\n",
      "Epoch 224/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0708 - val_loss: 871.7520\n",
      "Epoch 225/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0713 - val_loss: 871.7854\n",
      "Epoch 226/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0720 - val_loss: 871.8163\n",
      "Epoch 227/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0727 - val_loss: 871.8441\n",
      "Epoch 228/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0736 - val_loss: 871.8697\n",
      "Epoch 229/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0745 - val_loss: 871.8928\n",
      "Epoch 230/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0755 - val_loss: 871.9139\n",
      "Epoch 231/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0765 - val_loss: 871.9329\n",
      "Epoch 232/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0776 - val_loss: 871.9501\n",
      "Epoch 233/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.0788 - val_loss: 871.9656\n",
      "Epoch 234/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0800 - val_loss: 871.9796\n",
      "Epoch 235/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 30.0811 - val_loss: 871.9922\n",
      "Epoch 236/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0824 - val_loss: 872.0035\n",
      "Epoch 237/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0836 - val_loss: 872.0135\n",
      "Epoch 238/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0850 - val_loss: 872.0225\n",
      "Epoch 239/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0862 - val_loss: 872.0300\n",
      "Epoch 240/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0876 - val_loss: 872.0369\n",
      "Epoch 241/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0890 - val_loss: 872.0433\n",
      "Epoch 242/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0903 - val_loss: 872.0483\n",
      "Epoch 243/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0917 - val_loss: 872.0528\n",
      "Epoch 244/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0931 - val_loss: 872.0568\n",
      "Epoch 245/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0944 - val_loss: 872.0600\n",
      "Epoch 246/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.0958 - val_loss: 872.0625\n",
      "Epoch 247/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0972 - val_loss: 872.0647\n",
      "Epoch 248/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0986 - val_loss: 872.0665\n",
      "Epoch 249/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.0999 - val_loss: 872.0677\n",
      "Epoch 250/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1013 - val_loss: 872.0686\n",
      "Epoch 251/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1027 - val_loss: 872.0692\n",
      "Epoch 252/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 30.1040 - val_loss: 872.0694\n",
      "Epoch 253/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1054 - val_loss: 872.0695\n",
      "Epoch 254/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 30.1067 - val_loss: 872.0690\n",
      "Epoch 255/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1081 - val_loss: 872.0686\n",
      "Epoch 256/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1093 - val_loss: 872.0678\n",
      "Epoch 257/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1106 - val_loss: 872.0668\n",
      "Epoch 258/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1120 - val_loss: 872.0659\n",
      "Epoch 259/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1133 - val_loss: 872.0650\n",
      "Epoch 260/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1145 - val_loss: 872.0638\n",
      "Epoch 261/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1158 - val_loss: 872.0624\n",
      "Epoch 262/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1170 - val_loss: 872.0607\n",
      "Epoch 263/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1182 - val_loss: 872.0594\n",
      "Epoch 264/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1194 - val_loss: 872.0577\n",
      "Epoch 265/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1206 - val_loss: 872.0561\n",
      "Epoch 266/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1218 - val_loss: 872.0544\n",
      "Epoch 267/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1230 - val_loss: 872.0531\n",
      "Epoch 268/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1242 - val_loss: 872.0515\n",
      "Epoch 269/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1252 - val_loss: 872.0498\n",
      "Epoch 270/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 30.1264 - val_loss: 872.0480\n",
      "Epoch 271/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1274 - val_loss: 872.0463\n",
      "Epoch 272/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1285 - val_loss: 872.0443\n",
      "Epoch 273/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1296 - val_loss: 872.0427\n",
      "Epoch 274/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1306 - val_loss: 872.0410\n",
      "Epoch 275/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1316 - val_loss: 872.0392\n",
      "Epoch 276/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1326 - val_loss: 872.0373\n",
      "Epoch 277/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1336 - val_loss: 872.0353\n",
      "Epoch 278/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1345 - val_loss: 872.0334\n",
      "Epoch 279/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1355 - val_loss: 872.0316\n",
      "Epoch 280/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1364 - val_loss: 872.0298\n",
      "Epoch 281/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1373 - val_loss: 872.0280\n",
      "Epoch 282/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1382 - val_loss: 872.0262\n",
      "Epoch 283/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1391 - val_loss: 872.0246\n",
      "Epoch 284/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1400 - val_loss: 872.0228\n",
      "Epoch 285/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1408 - val_loss: 872.0211\n",
      "Epoch 286/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1417 - val_loss: 872.0194\n",
      "Epoch 287/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 30.1425 - val_loss: 872.0178\n",
      "Epoch 288/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1433 - val_loss: 872.0163\n",
      "Epoch 289/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1441 - val_loss: 872.0146\n",
      "Epoch 290/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1448 - val_loss: 872.0131\n",
      "Epoch 291/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1456 - val_loss: 872.0115\n",
      "Epoch 292/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1463 - val_loss: 872.0099\n",
      "Epoch 293/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1471 - val_loss: 872.0085\n",
      "Epoch 294/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1478 - val_loss: 872.0073\n",
      "Epoch 295/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1485 - val_loss: 872.0058\n",
      "Epoch 296/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1491 - val_loss: 872.0042\n",
      "Epoch 297/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1498 - val_loss: 872.0030\n",
      "Epoch 298/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1504 - val_loss: 872.0015\n",
      "Epoch 299/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1509 - val_loss: 872.0000\n",
      "Epoch 300/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1516 - val_loss: 871.9985\n",
      "Epoch 301/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 30.1522 - val_loss: 871.9969\n",
      "Epoch 302/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1529 - val_loss: 871.9956\n",
      "Epoch 303/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1534 - val_loss: 871.9944\n",
      "Epoch 304/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 30.1540 - val_loss: 871.9930\n",
      "Epoch 305/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1546 - val_loss: 871.9919\n",
      "Epoch 306/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1551 - val_loss: 871.9908\n",
      "Epoch 307/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1556 - val_loss: 871.9894\n",
      "Epoch 308/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1561 - val_loss: 871.9885\n",
      "Epoch 309/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1566 - val_loss: 871.9875\n",
      "Epoch 310/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1572 - val_loss: 871.9864\n",
      "Epoch 311/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1576 - val_loss: 871.9854\n",
      "Epoch 312/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1581 - val_loss: 871.9844\n",
      "Epoch 313/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1585 - val_loss: 871.9833\n",
      "Epoch 314/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1590 - val_loss: 871.9825\n",
      "Epoch 315/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1594 - val_loss: 871.9813\n",
      "Epoch 316/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1598 - val_loss: 871.9801\n",
      "Epoch 317/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1603 - val_loss: 871.9792\n",
      "Epoch 318/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1606 - val_loss: 871.9780\n",
      "Epoch 319/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1610 - val_loss: 871.9771\n",
      "Epoch 320/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1614 - val_loss: 871.9761\n",
      "Epoch 321/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 30.1618 - val_loss: 871.9753\n",
      "Epoch 322/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 30.1622 - val_loss: 871.9745\n",
      "Epoch 323/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1625 - val_loss: 871.9733\n",
      "Epoch 324/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1628 - val_loss: 871.9725\n",
      "Epoch 325/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1632 - val_loss: 871.9716\n",
      "Epoch 326/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1635 - val_loss: 871.9706\n",
      "Epoch 327/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1639 - val_loss: 871.9698\n",
      "Epoch 328/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1642 - val_loss: 871.9691\n",
      "Epoch 329/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1645 - val_loss: 871.9684\n",
      "Epoch 330/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1648 - val_loss: 871.9674\n",
      "Epoch 331/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1651 - val_loss: 871.9668\n",
      "Epoch 332/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1654 - val_loss: 871.9660\n",
      "Epoch 333/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1657 - val_loss: 871.9651\n",
      "Epoch 334/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1658 - val_loss: 871.9642\n",
      "Epoch 335/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1662 - val_loss: 871.9637\n",
      "Epoch 336/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1664 - val_loss: 871.9628\n",
      "Epoch 337/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 30.1666 - val_loss: 871.9619\n",
      "Epoch 338/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 30.1669 - val_loss: 871.9612\n",
      "Epoch 339/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1672 - val_loss: 871.9606\n",
      "Epoch 340/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1674 - val_loss: 871.9600\n",
      "Epoch 341/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1676 - val_loss: 871.9593\n",
      "Epoch 342/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1678 - val_loss: 871.9588\n",
      "Epoch 343/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1680 - val_loss: 871.9580\n",
      "Epoch 344/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1682 - val_loss: 871.9572\n",
      "Epoch 345/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1684 - val_loss: 871.9565\n",
      "Epoch 346/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1686 - val_loss: 871.9557\n",
      "Epoch 347/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1688 - val_loss: 871.9553\n",
      "Epoch 348/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1690 - val_loss: 871.9546\n",
      "Epoch 349/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1692 - val_loss: 871.9539\n",
      "Epoch 350/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1694 - val_loss: 871.9536\n",
      "Epoch 351/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1696 - val_loss: 871.9529\n",
      "Epoch 352/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 30.1697 - val_loss: 871.9525\n",
      "Epoch 353/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 30.1699 - val_loss: 871.9520\n",
      "Epoch 354/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1700 - val_loss: 871.9514\n",
      "Epoch 355/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1702 - val_loss: 871.9507\n",
      "Epoch 356/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1704 - val_loss: 871.9502\n",
      "Epoch 357/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1705 - val_loss: 871.9497\n",
      "Epoch 358/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1707 - val_loss: 871.9493\n",
      "Epoch 359/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1708 - val_loss: 871.9488\n",
      "Epoch 360/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1709 - val_loss: 871.9482\n",
      "Epoch 361/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1711 - val_loss: 871.9479\n",
      "Epoch 362/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1712 - val_loss: 871.9474\n",
      "Epoch 363/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1713 - val_loss: 871.9470\n",
      "Epoch 364/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1714 - val_loss: 871.9465\n",
      "Epoch 365/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1716 - val_loss: 871.9462\n",
      "Epoch 366/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1717 - val_loss: 871.9458\n",
      "Epoch 367/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1718 - val_loss: 871.9453\n",
      "Epoch 368/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 30.1719 - val_loss: 871.9445\n",
      "Epoch 369/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1719 - val_loss: 871.9441\n",
      "Epoch 370/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1721 - val_loss: 871.9435\n",
      "Epoch 371/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1722 - val_loss: 871.9432\n",
      "Epoch 372/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1723 - val_loss: 871.9427\n",
      "Epoch 373/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1724 - val_loss: 871.9421\n",
      "Epoch 374/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1725 - val_loss: 871.9417\n",
      "Epoch 375/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1725 - val_loss: 871.9412\n",
      "Epoch 376/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1726 - val_loss: 871.9407\n",
      "Epoch 377/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1728 - val_loss: 871.9404\n",
      "Epoch 378/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1728 - val_loss: 871.9399\n",
      "Epoch 379/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1729 - val_loss: 871.9396\n",
      "Epoch 380/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1730 - val_loss: 871.9392\n",
      "Epoch 381/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1731 - val_loss: 871.9384\n",
      "Epoch 382/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 30.1732 - val_loss: 871.9380\n",
      "Epoch 383/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 30.1732 - val_loss: 871.9377\n",
      "Epoch 384/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1733 - val_loss: 871.9372\n",
      "Epoch 385/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1733 - val_loss: 871.9368\n",
      "Epoch 386/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1734 - val_loss: 871.9365\n",
      "Epoch 387/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1735 - val_loss: 871.9359\n",
      "Epoch 388/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1735 - val_loss: 871.9352\n",
      "Epoch 389/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1736 - val_loss: 871.9348\n",
      "Epoch 390/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1737 - val_loss: 871.9346\n",
      "Epoch 391/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1737 - val_loss: 871.9341\n",
      "Epoch 392/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1738 - val_loss: 871.9336\n",
      "Epoch 393/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1739 - val_loss: 871.9332\n",
      "Epoch 394/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1738 - val_loss: 871.9328\n",
      "Epoch 395/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1739 - val_loss: 871.9323\n",
      "Epoch 396/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1739 - val_loss: 871.9318\n",
      "Epoch 397/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 30.1740 - val_loss: 871.9315\n",
      "Epoch 398/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1740 - val_loss: 871.9312\n",
      "Epoch 399/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1740 - val_loss: 871.9305\n",
      "Epoch 400/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1742 - val_loss: 871.9301\n",
      "Epoch 401/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1743 - val_loss: 871.9299\n",
      "Epoch 402/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1743 - val_loss: 871.9296\n",
      "Epoch 403/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1743 - val_loss: 871.9294\n",
      "Epoch 404/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1743 - val_loss: 871.9287\n",
      "Epoch 405/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1744 - val_loss: 871.9282\n",
      "Epoch 406/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1745 - val_loss: 871.9281\n",
      "Epoch 407/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1745 - val_loss: 871.9279\n",
      "Epoch 408/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1745 - val_loss: 871.9274\n",
      "Epoch 409/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1745 - val_loss: 871.9269\n",
      "Epoch 410/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1745 - val_loss: 871.9263\n",
      "Epoch 411/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 30.1747 - val_loss: 871.9262\n",
      "Epoch 412/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1747 - val_loss: 871.9260\n",
      "Epoch 413/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1747 - val_loss: 871.9255\n",
      "Epoch 414/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1747 - val_loss: 871.9252\n",
      "Epoch 415/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1746 - val_loss: 871.9245\n",
      "Epoch 416/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1747 - val_loss: 871.9239\n",
      "Epoch 417/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1747 - val_loss: 871.9235\n",
      "Epoch 418/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1748 - val_loss: 871.9231\n",
      "Epoch 419/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1749 - val_loss: 871.9227\n",
      "Epoch 420/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1749 - val_loss: 871.9224\n",
      "Epoch 421/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1749 - val_loss: 871.9222\n",
      "Epoch 422/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1749 - val_loss: 871.9220\n",
      "Epoch 423/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1750 - val_loss: 871.9216\n",
      "Epoch 424/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1750 - val_loss: 871.9213\n",
      "Epoch 425/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1750 - val_loss: 871.9209\n",
      "Epoch 426/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1750 - val_loss: 871.9206\n",
      "Epoch 427/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1750 - val_loss: 871.9201\n",
      "Epoch 428/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1750 - val_loss: 871.9197\n",
      "Epoch 429/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1751 - val_loss: 871.9193\n",
      "Epoch 430/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1751 - val_loss: 871.9188\n",
      "Epoch 431/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1751 - val_loss: 871.9183\n",
      "Epoch 432/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 30.1751 - val_loss: 871.9179\n",
      "Epoch 433/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1751 - val_loss: 871.9174\n",
      "Epoch 434/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1752 - val_loss: 871.9171\n",
      "Epoch 435/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1752 - val_loss: 871.9167\n",
      "Epoch 436/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1752 - val_loss: 871.9164\n",
      "Epoch 437/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1752 - val_loss: 871.9161\n",
      "Epoch 438/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1753 - val_loss: 871.9156\n",
      "Epoch 439/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1753 - val_loss: 871.9154\n",
      "Epoch 440/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1753 - val_loss: 871.9150\n",
      "Epoch 441/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1753 - val_loss: 871.9145\n",
      "Epoch 442/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1753 - val_loss: 871.9140\n",
      "Epoch 443/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1753 - val_loss: 871.9136\n",
      "Epoch 444/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1753 - val_loss: 871.9132\n",
      "Epoch 445/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1753 - val_loss: 871.9127\n",
      "Epoch 446/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1753 - val_loss: 871.9124\n",
      "Epoch 447/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1753 - val_loss: 871.9119\n",
      "Epoch 448/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1753 - val_loss: 871.9114\n",
      "Epoch 449/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1754 - val_loss: 871.9110\n",
      "Epoch 450/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1754 - val_loss: 871.9108\n",
      "Epoch 451/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1754 - val_loss: 871.9105\n",
      "Epoch 452/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1754 - val_loss: 871.9100\n",
      "Epoch 453/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 30.1754 - val_loss: 871.9096\n",
      "Epoch 454/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1754 - val_loss: 871.9091\n",
      "Epoch 455/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1754 - val_loss: 871.9089\n",
      "Epoch 456/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1755 - val_loss: 871.9086\n",
      "Epoch 457/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 30.1755 - val_loss: 871.9084\n",
      "Epoch 458/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 30.1755 - val_loss: 871.9078\n",
      "Epoch 459/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1755 - val_loss: 871.9075\n",
      "Epoch 460/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1754 - val_loss: 871.9070\n",
      "Epoch 461/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1755 - val_loss: 871.9064\n",
      "Epoch 462/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1755 - val_loss: 871.9061\n",
      "Epoch 463/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1755 - val_loss: 871.9058\n",
      "Epoch 464/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1755 - val_loss: 871.9056\n",
      "Epoch 465/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1755 - val_loss: 871.9052\n",
      "Epoch 466/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1755 - val_loss: 871.9048\n",
      "Epoch 467/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1755 - val_loss: 871.9043\n",
      "Epoch 468/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1755 - val_loss: 871.9038\n",
      "Epoch 469/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 30.1755 - val_loss: 871.9035\n",
      "Epoch 470/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1755 - val_loss: 871.9030\n",
      "Epoch 471/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1755 - val_loss: 871.9026\n",
      "Epoch 472/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1755 - val_loss: 871.9022\n",
      "Epoch 473/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1755 - val_loss: 871.9017\n",
      "Epoch 474/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1755 - val_loss: 871.9012\n",
      "Epoch 475/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1755 - val_loss: 871.9008\n",
      "Epoch 476/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1755 - val_loss: 871.9003\n",
      "Epoch 477/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1756 - val_loss: 871.9001\n",
      "Epoch 478/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1756 - val_loss: 871.8997\n",
      "Epoch 479/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1756 - val_loss: 871.8994\n",
      "Epoch 480/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1756 - val_loss: 871.8990\n",
      "Epoch 481/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1756 - val_loss: 871.8984\n",
      "Epoch 482/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1756 - val_loss: 871.8979\n",
      "Epoch 483/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1756 - val_loss: 871.8975\n",
      "Epoch 484/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1756 - val_loss: 871.8970\n",
      "Epoch 485/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1756 - val_loss: 871.8964\n",
      "Epoch 486/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1756 - val_loss: 871.8959\n",
      "Epoch 487/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1756 - val_loss: 871.8954\n",
      "Epoch 488/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1756 - val_loss: 871.8949\n",
      "Epoch 489/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 30.1756 - val_loss: 871.8944\n",
      "Epoch 490/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1756 - val_loss: 871.8937\n",
      "Epoch 491/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1756 - val_loss: 871.8931\n",
      "Epoch 492/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1756 - val_loss: 871.8925\n",
      "Epoch 493/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1756 - val_loss: 871.8920\n",
      "Epoch 494/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1756 - val_loss: 871.8914\n",
      "Epoch 495/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1757 - val_loss: 871.8909\n",
      "Epoch 496/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1757 - val_loss: 871.8903\n",
      "Epoch 497/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 30.1757 - val_loss: 871.8898\n",
      "Epoch 498/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1756 - val_loss: 871.8893\n",
      "Epoch 499/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1757 - val_loss: 871.8886\n",
      "Epoch 500/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 30.1757 - val_loss: 871.8881\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.8407, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0015, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0118, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.5775, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 349ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 126)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 126)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 186)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:150:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 7.19799630e+01, 0.00000000e+00, 6.98609244e+01,\n",
       "        6.97096639e+01, 6.94168067e+01, 6.97806256e+01, 6.95587302e+01,\n",
       "        7.21918301e+01, 0.00000000e+00, 3.43769310e-01, 6.61508080e-01,\n",
       "        6.95400560e+01, 0.00000000e+00, 6.96819795e+01, 7.34859477e+01,\n",
       "        0.00000000e+00, 6.95213819e+01, 7.17995565e+01, 7.21036718e+01,\n",
       "        3.09567900e-01, 1.20388903e-01, 0.00000000e+00, 3.13862383e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.35351896e-01,\n",
       "        2.61133390e-02, 0.00000000e+00, 2.82687759e+01, 4.56669629e-01,\n",
       "        0.00000000e+00, 3.60358626e-01, 7.78816402e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 8.05063844e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.99653590e-01, 4.82478768e-01, 0.00000000e+00, 5.54718077e-02,\n",
       "        1.94206059e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([59.98833094, 59.98040582, 59.9724807 , 59.96455558, 59.95663046,\n",
       "       59.94870534, 59.94078021, 59.93285509, 59.92492997, 59.91700485,\n",
       "       59.90907973, 59.90115461, 59.89322949, 59.88530437, 59.87737924,\n",
       "       59.86945412, 59.861529  , 59.85360388, 59.84567876, 59.83775364,\n",
       "       59.82982852, 59.8219034 , 59.81397827, 59.80605315, 59.79812803,\n",
       "       59.79020291, 59.78227779, 59.77435267, 59.76642755, 59.75850243,\n",
       "       59.7505773 , 59.74265218, 59.73472706, 59.72680194, 59.71887682,\n",
       "       59.7109517 , 59.70302658, 59.69510146, 59.68717633, 59.67925121,\n",
       "       59.67132609, 59.66340097, 59.65547585, 59.64755073, 59.63962561,\n",
       "       59.63170049, 59.62377536, 59.61585024, 59.60792512, 59.6       ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.0612001497549\n",
      "46.33150862370092\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
