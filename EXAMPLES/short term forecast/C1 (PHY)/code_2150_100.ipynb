{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.200000\n",
       "1       88.931092\n",
       "2       88.662185\n",
       "3       88.393277\n",
       "4       88.124370\n",
       "          ...    \n",
       "2245    54.421220\n",
       "2246    54.412475\n",
       "2247    54.403730\n",
       "2248    54.394985\n",
       "2249    54.386240\n",
       "Name: C1, Length: 2250, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c1_interpolated_2150_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.200000\n",
       "1       88.931092\n",
       "2       88.662185\n",
       "3       88.393277\n",
       "4       88.124370\n",
       "          ...    \n",
       "2145     0.384851\n",
       "2146     0.352796\n",
       "2147     0.254658\n",
       "2148     0.000000\n",
       "2149     0.018535\n",
       "Name: C1, Length: 2150, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2150)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.931092</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.662185</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.393277</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.124370</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     89.200000  0.000298\n",
       "1     88.931092  0.000298\n",
       "2     88.662185  0.000297\n",
       "3     88.393277  0.000297\n",
       "4     88.124370  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6465  # Value for C0\n",
    "K0 = -0.0029  # Value for K0\n",
    "K1 = -0.0003  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0168    # Value for b\n",
    "c = 2.3581    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    89.200000    88.931092    88.662185    88.393277    88.124370   \n",
      "351    88.931092    88.662185    88.393277    88.124370    87.855462   \n",
      "352    88.662185    88.393277    88.124370    87.855462    87.586555   \n",
      "353    88.393277    88.124370    87.855462    87.586555    87.317647   \n",
      "354    88.124370    87.855462    87.586555    87.317647    87.048739   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.855462    87.586555    87.317647    87.048739    86.794538  ...   \n",
      "351    87.586555    87.317647    87.048739    86.794538    86.721709  ...   \n",
      "352    87.317647    87.048739    86.794538    86.721709    86.648880  ...   \n",
      "353    87.048739    86.794538    86.721709    86.648880    86.576050  ...   \n",
      "354    86.794538    86.721709    86.648880    86.576050    86.503221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   73.989683    0.000263   73.957937    0.000263   73.926190    0.000263   \n",
      "351   73.957937    0.000263   73.926190    0.000263   73.894444    0.000262   \n",
      "352   73.926190    0.000263   73.894444    0.000262   73.862698    0.000262   \n",
      "353   73.894444    0.000262   73.862698    0.000262   73.830952    0.000262   \n",
      "354   73.862698    0.000262   73.830952    0.000262   73.799206    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   73.894444    0.000262   73.862698    0.000262  \n",
      "351   73.862698    0.000262   73.830952    0.000262  \n",
      "352   73.830952    0.000262   73.799206    0.000262  \n",
      "353   73.799206    0.000262   73.767460    0.000262  \n",
      "354   73.767460    0.000262   73.735714    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1700, 1, 251) (1700, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 2s 23ms/step - loss: 4268.0977 - val_loss: 3081.1499\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4161.4292 - val_loss: 3021.9370\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4085.2793 - val_loss: 2963.5012\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4010.3386 - val_loss: 2906.1743\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3936.6614 - val_loss: 2849.8555\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3864.1455 - val_loss: 2794.1809\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 3783.3684 - val_loss: 2730.4199\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3708.2021 - val_loss: 2673.7039\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3635.2100 - val_loss: 2618.4602\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3563.8345 - val_loss: 2564.4482\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3493.8352 - val_loss: 2511.5286\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3425.0764 - val_loss: 2459.6226\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3357.4817 - val_loss: 2408.6797\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3290.9958 - val_loss: 2358.6641\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3225.5815 - val_loss: 2309.5483\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3161.2092 - val_loss: 2261.3103\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3097.8533 - val_loss: 2213.9304\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3035.4944 - val_loss: 2167.3931\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2974.1138 - val_loss: 2121.6833\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2913.6946 - val_loss: 2076.7876\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2854.2229 - val_loss: 2032.6941\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2795.6841 - val_loss: 1989.3903\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2738.0652 - val_loss: 1946.8652\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2681.3538 - val_loss: 1905.1084\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2625.5383 - val_loss: 1864.1094\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2570.6064 - val_loss: 1823.8584\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2516.5479 - val_loss: 1784.3458\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2463.3530 - val_loss: 1745.5627\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2411.0093 - val_loss: 1707.4989\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2359.5078 - val_loss: 1670.1464\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2308.8386 - val_loss: 1633.4957\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2258.9915 - val_loss: 1597.5381\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 2209.9573 - val_loss: 1562.2649\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2161.7263 - val_loss: 1527.6675\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2114.2898 - val_loss: 1493.7354\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2067.6379 - val_loss: 1460.4408\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2014.6301 - val_loss: 1418.4050\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1961.1780 - val_loss: 1382.0768\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1911.5947 - val_loss: 1347.5071\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1863.9819 - val_loss: 1314.2650\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1817.8774 - val_loss: 1282.1066\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1773.0291 - val_loss: 1250.8953\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1729.2941 - val_loss: 1220.5488\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1686.5803 - val_loss: 1191.0087\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1644.8230 - val_loss: 1162.2336\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1603.9749 - val_loss: 1134.1904\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1563.9978 - val_loss: 1106.8522\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1524.8604 - val_loss: 1080.1962\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1486.5363 - val_loss: 1054.2028\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1449.0035 - val_loss: 1028.8551\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1412.2419 - val_loss: 1004.1370\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1376.2334 - val_loss: 980.0345\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1340.9613 - val_loss: 956.5342\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1306.4110 - val_loss: 933.6238\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1272.5681 - val_loss: 911.2916\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1239.4196 - val_loss: 889.5269\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1206.9526 - val_loss: 868.3189\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1175.1558 - val_loss: 847.6575\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1144.0172 - val_loss: 827.5327\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1113.5259 - val_loss: 807.9357\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1083.6721 - val_loss: 788.8560\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1054.4440 - val_loss: 770.2861\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1025.8331 - val_loss: 752.2164\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 997.8292 - val_loss: 734.6391\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 970.4226 - val_loss: 717.5454\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 943.6042 - val_loss: 700.9267\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 917.3650 - val_loss: 684.7755\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 891.6965 - val_loss: 669.0840\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 866.5895 - val_loss: 653.8444\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 842.0358 - val_loss: 639.0485\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 818.0269 - val_loss: 624.6895\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 794.5540 - val_loss: 610.7592\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 771.6096 - val_loss: 597.2509\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 749.1859 - val_loss: 584.1564\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 727.2740 - val_loss: 571.4696\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 705.8669 - val_loss: 559.1826\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 684.9561 - val_loss: 547.2883\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 664.5343 - val_loss: 535.7800\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 644.5937 - val_loss: 524.6503\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 625.1270 - val_loss: 513.8933\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 606.1266 - val_loss: 503.5013\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 587.5854 - val_loss: 493.4674\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 569.4957 - val_loss: 483.7854\n",
      "Epoch 84/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 551.8502 - val_loss: 474.4488\n",
      "Epoch 85/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 534.6420 - val_loss: 465.4503\n",
      "Epoch 86/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 517.8635 - val_loss: 456.7840\n",
      "Epoch 87/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 501.5083 - val_loss: 448.4429\n",
      "Epoch 88/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 485.5688 - val_loss: 440.4209\n",
      "Epoch 89/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 470.0385 - val_loss: 432.7111\n",
      "Epoch 90/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 454.9103 - val_loss: 425.3080\n",
      "Epoch 91/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 440.1775 - val_loss: 418.2045\n",
      "Epoch 92/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 425.8329 - val_loss: 411.3944\n",
      "Epoch 93/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 411.8699 - val_loss: 404.8721\n",
      "Epoch 94/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 398.2821 - val_loss: 398.6305\n",
      "Epoch 95/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 385.0626 - val_loss: 392.6640\n",
      "Epoch 96/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 372.2047 - val_loss: 386.9666\n",
      "Epoch 97/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 359.7021 - val_loss: 381.5319\n",
      "Epoch 98/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 347.5481 - val_loss: 376.3543\n",
      "Epoch 99/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 335.7368 - val_loss: 371.4273\n",
      "Epoch 100/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 324.2609 - val_loss: 366.7452\n",
      "Epoch 101/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 313.1147 - val_loss: 362.3025\n",
      "Epoch 102/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 302.2917 - val_loss: 358.0927\n",
      "Epoch 103/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 291.7857 - val_loss: 354.1106\n",
      "Epoch 104/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 281.5902 - val_loss: 350.3502\n",
      "Epoch 105/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 271.6995 - val_loss: 346.8057\n",
      "Epoch 106/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 262.1070 - val_loss: 343.4717\n",
      "Epoch 107/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 252.8070 - val_loss: 340.3422\n",
      "Epoch 108/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 243.7932 - val_loss: 337.4121\n",
      "Epoch 109/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0600 - val_loss: 334.6755\n",
      "Epoch 110/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 226.6011 - val_loss: 332.1272\n",
      "Epoch 111/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 218.4109 - val_loss: 329.7617\n",
      "Epoch 112/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 210.4835 - val_loss: 327.5738\n",
      "Epoch 113/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 202.8131 - val_loss: 325.5577\n",
      "Epoch 114/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 195.3939 - val_loss: 323.7087\n",
      "Epoch 115/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 188.2202 - val_loss: 322.0212\n",
      "Epoch 116/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 181.2865 - val_loss: 320.4904\n",
      "Epoch 117/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 174.5873 - val_loss: 319.1107\n",
      "Epoch 118/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 168.1170 - val_loss: 317.8776\n",
      "Epoch 119/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 161.8700 - val_loss: 316.7857\n",
      "Epoch 120/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 155.8412 - val_loss: 315.8303\n",
      "Epoch 121/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 150.0251 - val_loss: 315.0065\n",
      "Epoch 122/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 144.4164 - val_loss: 314.3095\n",
      "Epoch 123/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 139.0100 - val_loss: 313.7344\n",
      "Epoch 124/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 133.8005 - val_loss: 313.2767\n",
      "Epoch 125/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 128.7831 - val_loss: 312.9317\n",
      "Epoch 126/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 123.9524 - val_loss: 312.6949\n",
      "Epoch 127/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 119.3037 - val_loss: 312.5617\n",
      "Epoch 128/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 114.8319 - val_loss: 312.5278\n",
      "Epoch 129/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 110.5322 - val_loss: 312.5888\n",
      "Epoch 130/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 106.3999 - val_loss: 312.7404\n",
      "Epoch 131/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 102.4300 - val_loss: 312.9784\n",
      "Epoch 132/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 98.6182 - val_loss: 313.2986\n",
      "Epoch 133/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 94.9596 - val_loss: 313.6969\n",
      "Epoch 134/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 91.4499 - val_loss: 314.1695\n",
      "Epoch 135/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.0845 - val_loss: 314.7123\n",
      "Epoch 136/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 84.8589 - val_loss: 315.3215\n",
      "Epoch 137/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 81.7690 - val_loss: 315.9934\n",
      "Epoch 138/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 78.8105 - val_loss: 316.7242\n",
      "Epoch 139/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 75.9792 - val_loss: 317.5103\n",
      "Epoch 140/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 73.2709 - val_loss: 318.3482\n",
      "Epoch 141/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 70.6816 - val_loss: 319.2345\n",
      "Epoch 142/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 68.2076 - val_loss: 320.1657\n",
      "Epoch 143/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 65.8446 - val_loss: 321.1386\n",
      "Epoch 144/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 63.5890 - val_loss: 322.1500\n",
      "Epoch 145/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 61.4371 - val_loss: 323.1967\n",
      "Epoch 146/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 59.3852 - val_loss: 324.2758\n",
      "Epoch 147/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 57.4296 - val_loss: 325.3842\n",
      "Epoch 148/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 55.5669 - val_loss: 326.5192\n",
      "Epoch 149/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 53.7937 - val_loss: 327.6778\n",
      "Epoch 150/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 52.1066 - val_loss: 328.8575\n",
      "Epoch 151/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 50.5023 - val_loss: 330.0555\n",
      "Epoch 152/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 48.9776 - val_loss: 331.2693\n",
      "Epoch 153/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 47.5295 - val_loss: 332.4966\n",
      "Epoch 154/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 46.1548 - val_loss: 333.7352\n",
      "Epoch 155/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 44.8504 - val_loss: 334.9825\n",
      "Epoch 156/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 43.6138 - val_loss: 336.2364\n",
      "Epoch 157/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 42.4420 - val_loss: 337.4949\n",
      "Epoch 158/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 41.3322 - val_loss: 338.7558\n",
      "Epoch 159/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 40.2818 - val_loss: 340.0175\n",
      "Epoch 160/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 39.2882 - val_loss: 341.2780\n",
      "Epoch 161/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 38.3490 - val_loss: 342.5355\n",
      "Epoch 162/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 37.4616 - val_loss: 343.7881\n",
      "Epoch 163/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 36.6239 - val_loss: 345.0347\n",
      "Epoch 164/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 35.8333 - val_loss: 346.2735\n",
      "Epoch 165/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 35.0878 - val_loss: 347.5033\n",
      "Epoch 166/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 34.3852 - val_loss: 348.7223\n",
      "Epoch 167/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 33.7236 - val_loss: 349.9296\n",
      "Epoch 168/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 33.1009 - val_loss: 351.1239\n",
      "Epoch 169/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.5151 - val_loss: 352.3040\n",
      "Epoch 170/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 31.9645 - val_loss: 353.4691\n",
      "Epoch 171/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 31.4471 - val_loss: 354.6181\n",
      "Epoch 172/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 30.9614 - val_loss: 355.7501\n",
      "Epoch 173/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 30.5056 - val_loss: 356.8644\n",
      "Epoch 174/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 30.0783 - val_loss: 357.9601\n",
      "Epoch 175/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 29.6779 - val_loss: 359.0363\n",
      "Epoch 176/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 29.3029 - val_loss: 360.0928\n",
      "Epoch 177/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 28.9519 - val_loss: 361.1288\n",
      "Epoch 178/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 28.6237 - val_loss: 362.1439\n",
      "Epoch 179/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 28.3169 - val_loss: 363.1377\n",
      "Epoch 180/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 28.0302 - val_loss: 364.1098\n",
      "Epoch 181/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 27.7625 - val_loss: 365.0595\n",
      "Epoch 182/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 27.5129 - val_loss: 365.9873\n",
      "Epoch 183/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 27.2800 - val_loss: 366.8924\n",
      "Epoch 184/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 27.0631 - val_loss: 367.7749\n",
      "Epoch 185/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 26.8609 - val_loss: 368.6342\n",
      "Epoch 186/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 26.6729 - val_loss: 369.4709\n",
      "Epoch 187/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 26.4979 - val_loss: 370.2845\n",
      "Epoch 188/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 26.3352 - val_loss: 371.0754\n",
      "Epoch 189/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 26.1840 - val_loss: 371.8436\n",
      "Epoch 190/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 26.0436 - val_loss: 372.5888\n",
      "Epoch 191/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 25.9132 - val_loss: 373.3112\n",
      "Epoch 192/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 25.7923 - val_loss: 374.0111\n",
      "Epoch 193/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 25.6802 - val_loss: 374.6890\n",
      "Epoch 194/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 25.5763 - val_loss: 375.3447\n",
      "Epoch 195/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 25.4801 - val_loss: 375.9785\n",
      "Epoch 196/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 25.3910 - val_loss: 376.5907\n",
      "Epoch 197/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 25.3085 - val_loss: 377.1819\n",
      "Epoch 198/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 25.2322 - val_loss: 377.7520\n",
      "Epoch 199/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 25.1617 - val_loss: 378.3018\n",
      "Epoch 200/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 25.0965 - val_loss: 378.8311\n",
      "Epoch 201/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 25.0363 - val_loss: 379.3410\n",
      "Epoch 202/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.9806 - val_loss: 379.8311\n",
      "Epoch 203/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.9293 - val_loss: 380.3023\n",
      "Epoch 204/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.8819 - val_loss: 380.7549\n",
      "Epoch 205/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.8382 - val_loss: 381.1892\n",
      "Epoch 206/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.7978 - val_loss: 381.6060\n",
      "Epoch 207/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.7607 - val_loss: 382.0055\n",
      "Epoch 208/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.7263 - val_loss: 382.3880\n",
      "Epoch 209/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 24.6948 - val_loss: 382.7541\n",
      "Epoch 210/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.6657 - val_loss: 383.1043\n",
      "Epoch 211/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.6390 - val_loss: 383.4390\n",
      "Epoch 212/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.6143 - val_loss: 383.7587\n",
      "Epoch 213/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.5916 - val_loss: 384.0637\n",
      "Epoch 214/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.5707 - val_loss: 384.3549\n",
      "Epoch 215/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.5515 - val_loss: 384.6321\n",
      "Epoch 216/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.5338 - val_loss: 384.8963\n",
      "Epoch 217/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.5176 - val_loss: 385.1475\n",
      "Epoch 218/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.5027 - val_loss: 385.3866\n",
      "Epoch 219/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4889 - val_loss: 385.6137\n",
      "Epoch 220/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 24.4763 - val_loss: 385.8296\n",
      "Epoch 221/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4648 - val_loss: 386.0345\n",
      "Epoch 222/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4541 - val_loss: 386.2285\n",
      "Epoch 223/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4444 - val_loss: 386.4126\n",
      "Epoch 224/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4355 - val_loss: 386.5872\n",
      "Epoch 225/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4272 - val_loss: 386.7523\n",
      "Epoch 226/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4197 - val_loss: 386.9085\n",
      "Epoch 227/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4128 - val_loss: 387.0560\n",
      "Epoch 228/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4065 - val_loss: 387.1951\n",
      "Epoch 229/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4008 - val_loss: 387.3264\n",
      "Epoch 230/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3955 - val_loss: 387.4505\n",
      "Epoch 231/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3907 - val_loss: 387.5671\n",
      "Epoch 232/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3863 - val_loss: 387.6770\n",
      "Epoch 233/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3824 - val_loss: 387.7806\n",
      "Epoch 234/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3787 - val_loss: 387.8778\n",
      "Epoch 235/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3755 - val_loss: 387.9691\n",
      "Epoch 236/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3725 - val_loss: 388.0551\n",
      "Epoch 237/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3698 - val_loss: 388.1355\n",
      "Epoch 238/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3673 - val_loss: 388.2108\n",
      "Epoch 239/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3651 - val_loss: 388.2812\n",
      "Epoch 240/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3632 - val_loss: 388.3472\n",
      "Epoch 241/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3615 - val_loss: 388.4090\n",
      "Epoch 242/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3600 - val_loss: 388.4665\n",
      "Epoch 243/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3586 - val_loss: 388.5204\n",
      "Epoch 244/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3574 - val_loss: 388.5706\n",
      "Epoch 245/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 24.3563 - val_loss: 388.6171\n",
      "Epoch 246/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3555 - val_loss: 388.6607\n",
      "Epoch 247/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3547 - val_loss: 388.7010\n",
      "Epoch 248/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3541 - val_loss: 388.7383\n",
      "Epoch 249/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3536 - val_loss: 388.7731\n",
      "Epoch 250/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3532 - val_loss: 388.8056\n",
      "Epoch 251/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3529 - val_loss: 388.8354\n",
      "Epoch 252/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3526 - val_loss: 388.8631\n",
      "Epoch 253/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3525 - val_loss: 388.8883\n",
      "Epoch 254/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3525 - val_loss: 388.9119\n",
      "Epoch 255/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3525 - val_loss: 388.9335\n",
      "Epoch 256/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3526 - val_loss: 388.9535\n",
      "Epoch 257/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3527 - val_loss: 388.9716\n",
      "Epoch 258/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3530 - val_loss: 388.9887\n",
      "Epoch 259/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3533 - val_loss: 389.0042\n",
      "Epoch 260/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3536 - val_loss: 389.0184\n",
      "Epoch 261/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3539 - val_loss: 389.0312\n",
      "Epoch 262/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3544 - val_loss: 389.0430\n",
      "Epoch 263/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3548 - val_loss: 389.0537\n",
      "Epoch 264/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3552 - val_loss: 389.0631\n",
      "Epoch 265/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3558 - val_loss: 389.0719\n",
      "Epoch 266/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3564 - val_loss: 389.0799\n",
      "Epoch 267/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3569 - val_loss: 389.0870\n",
      "Epoch 268/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3575 - val_loss: 389.0935\n",
      "Epoch 269/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3581 - val_loss: 389.0991\n",
      "Epoch 270/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 24.3588 - val_loss: 389.1042\n",
      "Epoch 271/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3594 - val_loss: 389.1086\n",
      "Epoch 272/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3601 - val_loss: 389.1126\n",
      "Epoch 273/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3608 - val_loss: 389.1158\n",
      "Epoch 274/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3616 - val_loss: 389.1189\n",
      "Epoch 275/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3622 - val_loss: 389.1212\n",
      "Epoch 276/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3630 - val_loss: 389.1234\n",
      "Epoch 277/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3637 - val_loss: 389.1250\n",
      "Epoch 278/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3645 - val_loss: 389.1264\n",
      "Epoch 279/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3652 - val_loss: 389.1276\n",
      "Epoch 280/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3660 - val_loss: 389.1284\n",
      "Epoch 281/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3667 - val_loss: 389.1288\n",
      "Epoch 282/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3675 - val_loss: 389.1292\n",
      "Epoch 283/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3683 - val_loss: 389.1292\n",
      "Epoch 284/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3691 - val_loss: 389.1291\n",
      "Epoch 285/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3698 - val_loss: 389.1286\n",
      "Epoch 286/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3706 - val_loss: 389.1281\n",
      "Epoch 287/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3714 - val_loss: 389.1276\n",
      "Epoch 288/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3722 - val_loss: 389.1266\n",
      "Epoch 289/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3730 - val_loss: 389.1260\n",
      "Epoch 290/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3738 - val_loss: 389.1251\n",
      "Epoch 291/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3746 - val_loss: 389.1242\n",
      "Epoch 292/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3753 - val_loss: 389.1231\n",
      "Epoch 293/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3761 - val_loss: 389.1218\n",
      "Epoch 294/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3768 - val_loss: 389.1206\n",
      "Epoch 295/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3776 - val_loss: 389.1195\n",
      "Epoch 296/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 24.3784 - val_loss: 389.1180\n",
      "Epoch 297/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3791 - val_loss: 389.1166\n",
      "Epoch 298/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3799 - val_loss: 389.1152\n",
      "Epoch 299/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3806 - val_loss: 389.1136\n",
      "Epoch 300/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3814 - val_loss: 389.1121\n",
      "Epoch 301/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3821 - val_loss: 389.1104\n",
      "Epoch 302/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3828 - val_loss: 389.1087\n",
      "Epoch 303/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3836 - val_loss: 389.1074\n",
      "Epoch 304/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3843 - val_loss: 389.1060\n",
      "Epoch 305/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3850 - val_loss: 389.1045\n",
      "Epoch 306/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3857 - val_loss: 389.1030\n",
      "Epoch 307/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3863 - val_loss: 389.1010\n",
      "Epoch 308/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3871 - val_loss: 389.0994\n",
      "Epoch 309/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3878 - val_loss: 389.0979\n",
      "Epoch 310/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3885 - val_loss: 389.0962\n",
      "Epoch 311/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3892 - val_loss: 389.0948\n",
      "Epoch 312/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3898 - val_loss: 389.0932\n",
      "Epoch 313/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3904 - val_loss: 389.0914\n",
      "Epoch 314/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3911 - val_loss: 389.0898\n",
      "Epoch 315/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3917 - val_loss: 389.0881\n",
      "Epoch 316/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3924 - val_loss: 389.0866\n",
      "Epoch 317/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3930 - val_loss: 389.0851\n",
      "Epoch 318/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3936 - val_loss: 389.0837\n",
      "Epoch 319/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3942 - val_loss: 389.0820\n",
      "Epoch 320/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 24.3948 - val_loss: 389.0805\n",
      "Epoch 321/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3954 - val_loss: 389.0790\n",
      "Epoch 322/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3960 - val_loss: 389.0773\n",
      "Epoch 323/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3966 - val_loss: 389.0761\n",
      "Epoch 324/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3972 - val_loss: 389.0746\n",
      "Epoch 325/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3977 - val_loss: 389.0730\n",
      "Epoch 326/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3983 - val_loss: 389.0718\n",
      "Epoch 327/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3988 - val_loss: 389.0702\n",
      "Epoch 328/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3994 - val_loss: 389.0688\n",
      "Epoch 329/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.3999 - val_loss: 389.0675\n",
      "Epoch 330/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4004 - val_loss: 389.0659\n",
      "Epoch 331/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4010 - val_loss: 389.0647\n",
      "Epoch 332/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4015 - val_loss: 389.0633\n",
      "Epoch 333/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4019 - val_loss: 389.0620\n",
      "Epoch 334/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4025 - val_loss: 389.0606\n",
      "Epoch 335/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4030 - val_loss: 389.0596\n",
      "Epoch 336/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4034 - val_loss: 389.0583\n",
      "Epoch 337/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4039 - val_loss: 389.0570\n",
      "Epoch 338/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4044 - val_loss: 389.0557\n",
      "Epoch 339/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4048 - val_loss: 389.0546\n",
      "Epoch 340/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4053 - val_loss: 389.0531\n",
      "Epoch 341/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 24.4058 - val_loss: 389.0522\n",
      "Epoch 342/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4062 - val_loss: 389.0511\n",
      "Epoch 343/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4066 - val_loss: 389.0497\n",
      "Epoch 344/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4070 - val_loss: 389.0484\n",
      "Epoch 345/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4075 - val_loss: 389.0471\n",
      "Epoch 346/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4079 - val_loss: 389.0461\n",
      "Epoch 347/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4083 - val_loss: 389.0450\n",
      "Epoch 348/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4087 - val_loss: 389.0442\n",
      "Epoch 349/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4091 - val_loss: 389.0431\n",
      "Epoch 350/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4095 - val_loss: 389.0419\n",
      "Epoch 351/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4099 - val_loss: 389.0411\n",
      "Epoch 352/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4102 - val_loss: 389.0399\n",
      "Epoch 353/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4106 - val_loss: 389.0390\n",
      "Epoch 354/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4109 - val_loss: 389.0380\n",
      "Epoch 355/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4113 - val_loss: 389.0369\n",
      "Epoch 356/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4117 - val_loss: 389.0362\n",
      "Epoch 357/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4120 - val_loss: 389.0354\n",
      "Epoch 358/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 24.4123 - val_loss: 389.0344\n",
      "Epoch 359/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4127 - val_loss: 389.0337\n",
      "Epoch 360/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4130 - val_loss: 389.0330\n",
      "Epoch 361/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4133 - val_loss: 389.0323\n",
      "Epoch 362/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4136 - val_loss: 389.0314\n",
      "Epoch 363/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4140 - val_loss: 389.0306\n",
      "Epoch 364/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4143 - val_loss: 389.0300\n",
      "Epoch 365/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4146 - val_loss: 389.0294\n",
      "Epoch 366/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4148 - val_loss: 389.0284\n",
      "Epoch 367/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4151 - val_loss: 389.0275\n",
      "Epoch 368/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4154 - val_loss: 389.0267\n",
      "Epoch 369/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4157 - val_loss: 389.0260\n",
      "Epoch 370/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4159 - val_loss: 389.0253\n",
      "Epoch 371/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4162 - val_loss: 389.0245\n",
      "Epoch 372/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4164 - val_loss: 389.0235\n",
      "Epoch 373/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4168 - val_loss: 389.0230\n",
      "Epoch 374/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4170 - val_loss: 389.0222\n",
      "Epoch 375/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4173 - val_loss: 389.0219\n",
      "Epoch 376/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4175 - val_loss: 389.0211\n",
      "Epoch 377/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 24.4177 - val_loss: 389.0202\n",
      "Epoch 378/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4180 - val_loss: 389.0193\n",
      "Epoch 379/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4182 - val_loss: 389.0186\n",
      "Epoch 380/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4185 - val_loss: 389.0184\n",
      "Epoch 381/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4187 - val_loss: 389.0177\n",
      "Epoch 382/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4189 - val_loss: 389.0173\n",
      "Epoch 383/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4191 - val_loss: 389.0164\n",
      "Epoch 384/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4193 - val_loss: 389.0162\n",
      "Epoch 385/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4195 - val_loss: 389.0154\n",
      "Epoch 386/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4197 - val_loss: 389.0148\n",
      "Epoch 387/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4199 - val_loss: 389.0145\n",
      "Epoch 388/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4201 - val_loss: 389.0139\n",
      "Epoch 389/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4204 - val_loss: 389.0136\n",
      "Epoch 390/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4205 - val_loss: 389.0130\n",
      "Epoch 391/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4206 - val_loss: 389.0124\n",
      "Epoch 392/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4209 - val_loss: 389.0117\n",
      "Epoch 393/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4210 - val_loss: 389.0113\n",
      "Epoch 394/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4212 - val_loss: 389.0108\n",
      "Epoch 395/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4213 - val_loss: 389.0103\n",
      "Epoch 396/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4216 - val_loss: 389.0097\n",
      "Epoch 397/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4217 - val_loss: 389.0096\n",
      "Epoch 398/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4219 - val_loss: 389.0092\n",
      "Epoch 399/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4220 - val_loss: 389.0089\n",
      "Epoch 400/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4221 - val_loss: 389.0084\n",
      "Epoch 401/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4223 - val_loss: 389.0080\n",
      "Epoch 402/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4225 - val_loss: 389.0072\n",
      "Epoch 403/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 24.4227 - val_loss: 389.0069\n",
      "Epoch 404/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4227 - val_loss: 389.0064\n",
      "Epoch 405/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4230 - val_loss: 389.0063\n",
      "Epoch 406/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4231 - val_loss: 389.0061\n",
      "Epoch 407/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4231 - val_loss: 389.0058\n",
      "Epoch 408/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4233 - val_loss: 389.0052\n",
      "Epoch 409/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4234 - val_loss: 389.0049\n",
      "Epoch 410/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4236 - val_loss: 389.0045\n",
      "Epoch 411/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4237 - val_loss: 389.0042\n",
      "Epoch 412/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4238 - val_loss: 389.0040\n",
      "Epoch 413/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4239 - val_loss: 389.0034\n",
      "Epoch 414/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4240 - val_loss: 389.0030\n",
      "Epoch 415/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4242 - val_loss: 389.0030\n",
      "Epoch 416/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4243 - val_loss: 389.0028\n",
      "Epoch 417/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4244 - val_loss: 389.0027\n",
      "Epoch 418/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4245 - val_loss: 389.0025\n",
      "Epoch 419/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4245 - val_loss: 389.0022\n",
      "Epoch 420/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4247 - val_loss: 389.0021\n",
      "Epoch 421/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4248 - val_loss: 389.0018\n",
      "Epoch 422/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 24.4248 - val_loss: 389.0015\n",
      "Epoch 423/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4249 - val_loss: 389.0012\n",
      "Epoch 424/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4250 - val_loss: 389.0009\n",
      "Epoch 425/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4252 - val_loss: 389.0007\n",
      "Epoch 426/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4252 - val_loss: 389.0005\n",
      "Epoch 427/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4253 - val_loss: 389.0001\n",
      "Epoch 428/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4254 - val_loss: 388.9995\n",
      "Epoch 429/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4255 - val_loss: 388.9995\n",
      "Epoch 430/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4256 - val_loss: 388.9994\n",
      "Epoch 431/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4257 - val_loss: 388.9994\n",
      "Epoch 432/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4257 - val_loss: 388.9992\n",
      "Epoch 433/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4258 - val_loss: 388.9989\n",
      "Epoch 434/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 24.4259 - val_loss: 388.9989\n",
      "Epoch 435/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4260 - val_loss: 388.9985\n",
      "Epoch 436/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4260 - val_loss: 388.9983\n",
      "Epoch 437/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4261 - val_loss: 388.9983\n",
      "Epoch 438/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4261 - val_loss: 388.9980\n",
      "Epoch 439/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4263 - val_loss: 388.9979\n",
      "Epoch 440/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4263 - val_loss: 388.9979\n",
      "Epoch 441/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4263 - val_loss: 388.9973\n",
      "Epoch 442/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4264 - val_loss: 388.9972\n",
      "Epoch 443/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4265 - val_loss: 388.9971\n",
      "Epoch 444/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 24.4266 - val_loss: 388.9971\n",
      "Epoch 445/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4267 - val_loss: 388.9968\n",
      "Epoch 446/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4266 - val_loss: 388.9966\n",
      "Epoch 447/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4267 - val_loss: 388.9962\n",
      "Epoch 448/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4268 - val_loss: 388.9962\n",
      "Epoch 449/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4269 - val_loss: 388.9960\n",
      "Epoch 450/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4269 - val_loss: 388.9960\n",
      "Epoch 451/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4269 - val_loss: 388.9957\n",
      "Epoch 452/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4270 - val_loss: 388.9956\n",
      "Epoch 453/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4271 - val_loss: 388.9957\n",
      "Epoch 454/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4271 - val_loss: 388.9957\n",
      "Epoch 455/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4271 - val_loss: 388.9955\n",
      "Epoch 456/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 24.4272 - val_loss: 388.9952\n",
      "Epoch 457/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4273 - val_loss: 388.9952\n",
      "Epoch 458/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4273 - val_loss: 388.9951\n",
      "Epoch 459/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4273 - val_loss: 388.9951\n",
      "Epoch 460/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4274 - val_loss: 388.9950\n",
      "Epoch 461/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4274 - val_loss: 388.9950\n",
      "Epoch 462/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4274 - val_loss: 388.9948\n",
      "Epoch 463/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 24.4275 - val_loss: 388.9948\n",
      "Epoch 464/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4275 - val_loss: 388.9944\n",
      "Epoch 465/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4276 - val_loss: 388.9944\n",
      "Epoch 466/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4276 - val_loss: 388.9943\n",
      "Epoch 467/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4277 - val_loss: 388.9941\n",
      "Epoch 468/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4277 - val_loss: 388.9941\n",
      "Epoch 469/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4277 - val_loss: 388.9940\n",
      "Epoch 470/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4277 - val_loss: 388.9937\n",
      "Epoch 471/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4278 - val_loss: 388.9934\n",
      "Epoch 472/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4278 - val_loss: 388.9933\n",
      "Epoch 473/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4279 - val_loss: 388.9931\n",
      "Epoch 474/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4279 - val_loss: 388.9931\n",
      "Epoch 475/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4280 - val_loss: 388.9931\n",
      "Epoch 476/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4280 - val_loss: 388.9931\n",
      "Epoch 477/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4280 - val_loss: 388.9931\n",
      "Epoch 478/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4280 - val_loss: 388.9930\n",
      "Epoch 479/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4281 - val_loss: 388.9928\n",
      "Epoch 480/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4281 - val_loss: 388.9928\n",
      "Epoch 481/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 24.4281 - val_loss: 388.9926\n",
      "Epoch 482/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4281 - val_loss: 388.9923\n",
      "Epoch 483/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4282 - val_loss: 388.9922\n",
      "Epoch 484/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4282 - val_loss: 388.9920\n",
      "Epoch 485/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4282 - val_loss: 388.9919\n",
      "Epoch 486/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4283 - val_loss: 388.9919\n",
      "Epoch 487/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4283 - val_loss: 388.9920\n",
      "Epoch 488/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4283 - val_loss: 388.9920\n",
      "Epoch 489/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4283 - val_loss: 388.9919\n",
      "Epoch 490/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4283 - val_loss: 388.9920\n",
      "Epoch 491/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4284 - val_loss: 388.9920\n",
      "Epoch 492/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4284 - val_loss: 388.9920\n",
      "Epoch 493/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4284 - val_loss: 388.9920\n",
      "Epoch 494/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4285 - val_loss: 388.9923\n",
      "Epoch 495/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4285 - val_loss: 388.9926\n",
      "Epoch 496/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4285 - val_loss: 388.9927\n",
      "Epoch 497/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4285 - val_loss: 388.9926\n",
      "Epoch 498/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4284 - val_loss: 388.9923\n",
      "Epoch 499/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.4285 - val_loss: 388.9922\n",
      "Epoch 500/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 24.4285 - val_loss: 388.9923\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6465, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0029, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0003, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0168, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3581, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 359ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[60.3328338 , 60.31994865, 60.30706349, 60.29417834, 60.28129318,\n",
       "        60.26840803, 60.25552288, 60.24263772, 60.22975257, 60.21686741,\n",
       "        60.20398226, 60.19109711, 60.17821195, 62.36757703, 62.35917367,\n",
       "        62.35077031, 62.34236695, 62.33396359, 62.32556022, 62.31715686,\n",
       "        62.3087535 , 62.30035014, 62.24362745, 62.18480392, 62.12598039,\n",
       "        62.06715686, 62.00833333, 61.9495098 , 61.89068627, 61.83186275,\n",
       "        61.77303922, 61.71421569, 61.65539216, 61.59833333, 61.5697619 ,\n",
       "        61.54119048, 61.51261905, 61.48404762, 61.45547619, 61.42690476,\n",
       "         0.27336845,  0.        ,  0.31125778,  0.49588051,  0.45370549,\n",
       "         0.        ,  0.        , 62.0802288 , 62.0214052 , 61.9625817 ,\n",
       "        61.9037582 , 61.8449346 , 61.7861111 , 61.7272876 , 61.668464  ,\n",
       "        61.6096405 , 61.5761111 , 61.5475397 , 61.5189683 , 61.4903968 ,\n",
       "        61.4618254 , 61.433254  , 61.4046825 , 61.3761111 , 61.3475397 ,\n",
       "        61.3189682 , 61.2903968 , 61.2618254 , 61.1892017 , 61.1135714 ,\n",
       "        61.0379412 , 60.9623109 , 60.8866807 , 67.9906693 ,  0.1256559 ,\n",
       "         0.6921699 ,  0.19718473,  0.        ,  0.4667008 ,  0.09258895,\n",
       "        56.39619446,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.17803535,  0.        ,\n",
       "         0.51116228,  0.92358869,  0.29285455,  0.17605332,  0.        ,\n",
       "         0.09130782,  0.0747228 ,  0.75868291,  0.        ,  0.07070377]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([55.25199153, 55.24324657, 55.23450161, 55.22575664, 55.21701168,\n",
       "       55.20826672, 55.19952176, 55.1907768 , 55.18203184, 55.17328688,\n",
       "       55.16454191, 55.15579695, 55.14705199, 55.13830703, 55.12956207,\n",
       "       55.12081711, 55.11207215, 55.10332718, 55.09458222, 55.08583726,\n",
       "       55.0770923 , 55.06834734, 55.05960238, 55.05085742, 55.04211245,\n",
       "       55.03336749, 55.02462253, 55.01587757, 55.00713261, 54.99838765,\n",
       "       54.98964269, 54.98089772, 54.97215276, 54.9634078 , 54.95466284,\n",
       "       54.94591788, 54.93717292, 54.92842796, 54.919683  , 54.91093803,\n",
       "       54.90219307, 54.89344811, 54.88470315, 54.87595819, 54.86721323,\n",
       "       54.85846827, 54.8497233 , 54.84097834, 54.83223338, 54.82348842,\n",
       "       54.81474346, 54.8059985 , 54.79725354, 54.78850857, 54.77976361,\n",
       "       54.77101865, 54.76227369, 54.75352873, 54.74478377, 54.73603881,\n",
       "       54.72729384, 54.71854888, 54.70980392, 54.70105896, 54.692314  ,\n",
       "       54.68356904, 54.67482408, 54.66607911, 54.65733415, 54.64858919,\n",
       "       54.63984423, 54.63109927, 54.62235431, 54.61360935, 54.60486438,\n",
       "       54.59611942, 54.58737446, 54.5786295 , 54.56988454, 54.56113958,\n",
       "       54.55239462, 54.54364965, 54.53490469, 54.52615973, 54.51741477,\n",
       "       54.50866981, 54.49992485, 54.49117989, 54.48243493, 54.47368996,\n",
       "       54.464945  , 54.45620004, 54.44745508, 54.43871012, 54.42996516,\n",
       "       54.4212202 , 54.41247523, 54.40373027, 54.39498531, 54.38624035])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.24945711687055\n",
      "21.843991417753873\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
