{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.200000\n",
       "1       88.931092\n",
       "2       88.662185\n",
       "3       88.393277\n",
       "4       88.124370\n",
       "          ...    \n",
       "2045    56.798016\n",
       "2046    56.782143\n",
       "2047    56.766270\n",
       "2048    56.750397\n",
       "2049    56.734524\n",
       "Name: C1, Length: 2050, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c1_interpolated_1950_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.200000\n",
       "1       88.931092\n",
       "2       88.662185\n",
       "3       88.393277\n",
       "4       88.124370\n",
       "          ...    \n",
       "1945     0.466701\n",
       "1946     0.551191\n",
       "1947     0.000000\n",
       "1948     0.092589\n",
       "1949     0.346674\n",
       "Name: C1, Length: 1950, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1950)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.931092</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.662185</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.393277</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.124370</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     89.200000  0.000298\n",
       "1     88.931092  0.000298\n",
       "2     88.662185  0.000297\n",
       "3     88.393277  0.000297\n",
       "4     88.124370  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6465  # Value for C0\n",
    "K0 = -0.0029  # Value for K0\n",
    "K1 = -0.0003  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0168    # Value for b\n",
    "c = 2.3581    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    89.200000    88.931092    88.662185    88.393277    88.124370   \n",
      "351    88.931092    88.662185    88.393277    88.124370    87.855462   \n",
      "352    88.662185    88.393277    88.124370    87.855462    87.586555   \n",
      "353    88.393277    88.124370    87.855462    87.586555    87.317647   \n",
      "354    88.124370    87.855462    87.586555    87.317647    87.048739   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.855462    87.586555    87.317647    87.048739    86.794538  ...   \n",
      "351    87.586555    87.317647    87.048739    86.794538    86.721709  ...   \n",
      "352    87.317647    87.048739    86.794538    86.721709    86.648880  ...   \n",
      "353    87.048739    86.794538    86.721709    86.648880    86.576050  ...   \n",
      "354    86.794538    86.721709    86.648880    86.576050    86.503221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   73.989683    0.000263   73.957937    0.000263   73.926190    0.000263   \n",
      "351   73.957937    0.000263   73.926190    0.000263   73.894444    0.000262   \n",
      "352   73.926190    0.000263   73.894444    0.000262   73.862698    0.000262   \n",
      "353   73.894444    0.000262   73.862698    0.000262   73.830952    0.000262   \n",
      "354   73.862698    0.000262   73.830952    0.000262   73.799206    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   73.894444    0.000262   73.862698    0.000262  \n",
      "351   73.862698    0.000262   73.830952    0.000262  \n",
      "352   73.830952    0.000262   73.799206    0.000262  \n",
      "353   73.799206    0.000262   73.767460    0.000262  \n",
      "354   73.767460    0.000262   73.735714    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 1, 251) (1500, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "19/19 [==============================] - 3s 35ms/step - loss: 4478.4590 - val_loss: 3543.1663\n",
      "Epoch 2/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 4374.4521 - val_loss: 3473.4392\n",
      "Epoch 3/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 4299.5044 - val_loss: 3414.3718\n",
      "Epoch 4/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 4234.1108 - val_loss: 3343.2993\n",
      "Epoch 5/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 4154.0576 - val_loss: 3283.4492\n",
      "Epoch 6/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 4087.5862 - val_loss: 3224.5188\n",
      "Epoch 7/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 4022.3413 - val_loss: 3166.7847\n",
      "Epoch 8/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 3958.2847 - val_loss: 3110.0784\n",
      "Epoch 9/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3895.2563 - val_loss: 3054.2876\n",
      "Epoch 10/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 3833.1558 - val_loss: 2999.3450\n",
      "Epoch 11/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 3771.9211 - val_loss: 2945.2068\n",
      "Epoch 12/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 3711.5134 - val_loss: 2891.8420\n",
      "Epoch 13/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 3651.9026 - val_loss: 2839.2280\n",
      "Epoch 14/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 3593.0654 - val_loss: 2787.3462\n",
      "Epoch 15/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3534.9846 - val_loss: 2736.1812\n",
      "Epoch 16/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 3477.6453 - val_loss: 2685.7205\n",
      "Epoch 17/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 3421.0344 - val_loss: 2635.9529\n",
      "Epoch 18/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 3365.1401 - val_loss: 2586.8672\n",
      "Epoch 19/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 3309.9524 - val_loss: 2538.4558\n",
      "Epoch 20/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 3255.4622 - val_loss: 2490.7083\n",
      "Epoch 21/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3201.6597 - val_loss: 2443.6169\n",
      "Epoch 22/500\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 3148.5381 - val_loss: 2397.1741\n",
      "Epoch 23/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 3096.0884 - val_loss: 2351.3726\n",
      "Epoch 24/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 3044.3030 - val_loss: 2306.2053\n",
      "Epoch 25/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 2993.1758 - val_loss: 2261.6653\n",
      "Epoch 26/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 2928.5938 - val_loss: 2201.1387\n",
      "Epoch 27/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 2871.4709 - val_loss: 2153.2971\n",
      "Epoch 28/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 2817.5889 - val_loss: 2107.0449\n",
      "Epoch 29/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 2765.2788 - val_loss: 2062.0503\n",
      "Epoch 30/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 2714.2156 - val_loss: 2018.0938\n",
      "Epoch 31/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 2664.2034 - val_loss: 1975.0500\n",
      "Epoch 32/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 2615.1272 - val_loss: 1932.8400\n",
      "Epoch 33/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 2566.9116 - val_loss: 1891.4092\n",
      "Epoch 34/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 2519.5054 - val_loss: 1850.7191\n",
      "Epoch 35/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 2472.8689 - val_loss: 1810.7396\n",
      "Epoch 36/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 2426.9724 - val_loss: 1771.4460\n",
      "Epoch 37/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 2381.7917 - val_loss: 1732.8184\n",
      "Epoch 38/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 2337.3057 - val_loss: 1694.8400\n",
      "Epoch 39/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 2293.4980 - val_loss: 1657.4961\n",
      "Epoch 40/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 2250.3530 - val_loss: 1620.7731\n",
      "Epoch 41/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 2207.8572 - val_loss: 1584.6594\n",
      "Epoch 42/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 2165.9988 - val_loss: 1549.1440\n",
      "Epoch 43/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 2124.7671 - val_loss: 1514.2173\n",
      "Epoch 44/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 2084.1504 - val_loss: 1479.8696\n",
      "Epoch 45/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 2044.1400 - val_loss: 1446.0927\n",
      "Epoch 46/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 2004.7279 - val_loss: 1412.8774\n",
      "Epoch 47/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 1965.9044 - val_loss: 1380.2169\n",
      "Epoch 48/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 1927.6613 - val_loss: 1348.1035\n",
      "Epoch 49/500\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 1889.9927 - val_loss: 1316.5293\n",
      "Epoch 50/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 1852.8892 - val_loss: 1285.4873\n",
      "Epoch 51/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 1816.3446 - val_loss: 1254.9712\n",
      "Epoch 52/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 1780.3521 - val_loss: 1224.9749\n",
      "Epoch 53/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 1744.9052 - val_loss: 1195.4906\n",
      "Epoch 54/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 1709.9969 - val_loss: 1166.5135\n",
      "Epoch 55/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 1675.6215 - val_loss: 1138.0366\n",
      "Epoch 56/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 1641.7725 - val_loss: 1110.0548\n",
      "Epoch 57/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 1608.4442 - val_loss: 1082.5612\n",
      "Epoch 58/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 1575.6299 - val_loss: 1055.5516\n",
      "Epoch 59/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 1543.3252 - val_loss: 1029.0183\n",
      "Epoch 60/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 1511.5231 - val_loss: 1002.9576\n",
      "Epoch 61/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 1480.2190 - val_loss: 977.3631\n",
      "Epoch 62/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 1449.4070 - val_loss: 952.2296\n",
      "Epoch 63/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 1419.0815 - val_loss: 927.5521\n",
      "Epoch 64/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 1389.2379 - val_loss: 903.3246\n",
      "Epoch 65/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 1359.8701 - val_loss: 879.5428\n",
      "Epoch 66/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 1330.9734 - val_loss: 856.2013\n",
      "Epoch 67/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 1302.5428 - val_loss: 833.2946\n",
      "Epoch 68/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 1274.5728 - val_loss: 810.8179\n",
      "Epoch 69/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 1247.0583 - val_loss: 788.7661\n",
      "Epoch 70/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 1219.9946 - val_loss: 767.1344\n",
      "Epoch 71/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 1193.3771 - val_loss: 745.9182\n",
      "Epoch 72/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 1167.2008 - val_loss: 725.1129\n",
      "Epoch 73/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 1141.4604 - val_loss: 704.7130\n",
      "Epoch 74/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 1116.1520 - val_loss: 684.7139\n",
      "Epoch 75/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 1091.2700 - val_loss: 665.1111\n",
      "Epoch 76/500\n",
      "19/19 [==============================] - 0s 10ms/step - loss: 1066.8107 - val_loss: 645.9001\n",
      "Epoch 77/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 1042.7684 - val_loss: 627.0757\n",
      "Epoch 78/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 1019.1387 - val_loss: 608.6335\n",
      "Epoch 79/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 995.9176 - val_loss: 590.5690\n",
      "Epoch 80/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 973.1000 - val_loss: 572.8777\n",
      "Epoch 81/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 950.6818 - val_loss: 555.5547\n",
      "Epoch 82/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 928.6582 - val_loss: 538.5961\n",
      "Epoch 83/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 907.0247 - val_loss: 521.9966\n",
      "Epoch 84/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 885.7772 - val_loss: 505.7527\n",
      "Epoch 85/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 864.9109 - val_loss: 489.8592\n",
      "Epoch 86/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 844.4219 - val_loss: 474.3126\n",
      "Epoch 87/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 824.3056 - val_loss: 459.1075\n",
      "Epoch 88/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 804.5578 - val_loss: 444.2404\n",
      "Epoch 89/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 785.1742 - val_loss: 429.7065\n",
      "Epoch 90/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 766.1505 - val_loss: 415.5016\n",
      "Epoch 91/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 747.4825 - val_loss: 401.6214\n",
      "Epoch 92/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 729.1658 - val_loss: 388.0619\n",
      "Epoch 93/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 711.1967 - val_loss: 374.8187\n",
      "Epoch 94/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 693.5704 - val_loss: 361.8869\n",
      "Epoch 95/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 676.2830 - val_loss: 349.2635\n",
      "Epoch 96/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 659.3303 - val_loss: 336.9436\n",
      "Epoch 97/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 642.7081 - val_loss: 324.9230\n",
      "Epoch 98/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 626.4128 - val_loss: 313.1981\n",
      "Epoch 99/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 610.4399 - val_loss: 301.7643\n",
      "Epoch 100/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 594.7858 - val_loss: 290.6178\n",
      "Epoch 101/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 579.4457 - val_loss: 279.7541\n",
      "Epoch 102/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 564.4164 - val_loss: 269.1697\n",
      "Epoch 103/500\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 549.6936 - val_loss: 258.8600\n",
      "Epoch 104/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 535.2735 - val_loss: 248.8216\n",
      "Epoch 105/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 521.1517 - val_loss: 239.0503\n",
      "Epoch 106/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 507.3248 - val_loss: 229.5417\n",
      "Epoch 107/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 493.7886 - val_loss: 220.2923\n",
      "Epoch 108/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 480.5393 - val_loss: 211.2978\n",
      "Epoch 109/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 467.5731 - val_loss: 202.5546\n",
      "Epoch 110/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 454.8859 - val_loss: 194.0582\n",
      "Epoch 111/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 442.4738 - val_loss: 185.8053\n",
      "Epoch 112/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 430.3335 - val_loss: 177.7922\n",
      "Epoch 113/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 418.4611 - val_loss: 170.0143\n",
      "Epoch 114/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 406.8525 - val_loss: 162.4684\n",
      "Epoch 115/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 395.5040 - val_loss: 155.1503\n",
      "Epoch 116/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 384.4121 - val_loss: 148.0560\n",
      "Epoch 117/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 373.5725 - val_loss: 141.1822\n",
      "Epoch 118/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 362.9820 - val_loss: 134.5247\n",
      "Epoch 119/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 352.6367 - val_loss: 128.0803\n",
      "Epoch 120/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 342.5333 - val_loss: 121.8449\n",
      "Epoch 121/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 332.6678 - val_loss: 115.8147\n",
      "Epoch 122/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 323.0366 - val_loss: 109.9861\n",
      "Epoch 123/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 313.6362 - val_loss: 104.3555\n",
      "Epoch 124/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 304.4631 - val_loss: 98.9191\n",
      "Epoch 125/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 295.5132 - val_loss: 93.6731\n",
      "Epoch 126/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 286.7833 - val_loss: 88.6143\n",
      "Epoch 127/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 278.2698 - val_loss: 83.7389\n",
      "Epoch 128/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 269.9693 - val_loss: 79.0433\n",
      "Epoch 129/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 261.8782 - val_loss: 74.5240\n",
      "Epoch 130/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 253.9930 - val_loss: 70.1773\n",
      "Epoch 131/500\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 246.3102 - val_loss: 65.9997\n",
      "Epoch 132/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 238.8265 - val_loss: 61.9880\n",
      "Epoch 133/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 231.5384 - val_loss: 58.1383\n",
      "Epoch 134/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 224.4424 - val_loss: 54.4475\n",
      "Epoch 135/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 217.5353 - val_loss: 50.9120\n",
      "Epoch 136/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 210.8138 - val_loss: 47.5284\n",
      "Epoch 137/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 204.2742 - val_loss: 44.2933\n",
      "Epoch 138/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 197.9135 - val_loss: 41.2034\n",
      "Epoch 139/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 191.7282 - val_loss: 38.2553\n",
      "Epoch 140/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 185.7152 - val_loss: 35.4455\n",
      "Epoch 141/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 179.8711 - val_loss: 32.7712\n",
      "Epoch 142/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 174.1927 - val_loss: 30.2287\n",
      "Epoch 143/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 168.6770 - val_loss: 27.8149\n",
      "Epoch 144/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 163.3205 - val_loss: 25.5266\n",
      "Epoch 145/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 158.1203 - val_loss: 23.3607\n",
      "Epoch 146/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 153.0731 - val_loss: 21.3140\n",
      "Epoch 147/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 148.1759 - val_loss: 19.3832\n",
      "Epoch 148/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 143.4255 - val_loss: 17.5654\n",
      "Epoch 149/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 138.8190 - val_loss: 15.8573\n",
      "Epoch 150/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 134.3533 - val_loss: 14.2562\n",
      "Epoch 151/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 130.0253 - val_loss: 12.7590\n",
      "Epoch 152/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 125.8323 - val_loss: 11.3624\n",
      "Epoch 153/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 121.7710 - val_loss: 10.0638\n",
      "Epoch 154/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 117.8387 - val_loss: 8.8601\n",
      "Epoch 155/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 114.0325 - val_loss: 7.7485\n",
      "Epoch 156/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 110.3494 - val_loss: 6.7261\n",
      "Epoch 157/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 106.7868 - val_loss: 5.7902\n",
      "Epoch 158/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 103.3417 - val_loss: 4.9378\n",
      "Epoch 159/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 100.0113 - val_loss: 4.1664\n",
      "Epoch 160/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 96.7929 - val_loss: 3.4730\n",
      "Epoch 161/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 93.6839 - val_loss: 2.8552\n",
      "Epoch 162/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 90.6815 - val_loss: 2.3101\n",
      "Epoch 163/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 87.7831 - val_loss: 1.8354\n",
      "Epoch 164/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 84.9862 - val_loss: 1.4283\n",
      "Epoch 165/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 82.2880 - val_loss: 1.0863\n",
      "Epoch 166/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 79.6860 - val_loss: 0.8069\n",
      "Epoch 167/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 77.1776 - val_loss: 0.5877\n",
      "Epoch 168/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 74.7606 - val_loss: 0.4263\n",
      "Epoch 169/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 72.4324 - val_loss: 0.3203\n",
      "Epoch 170/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 70.1904 - val_loss: 0.2673\n",
      "Epoch 171/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 68.0323 - val_loss: 0.2651\n",
      "Epoch 172/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 65.9560 - val_loss: 0.3115\n",
      "Epoch 173/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 63.9589 - val_loss: 0.4041\n",
      "Epoch 174/500\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 62.0389 - val_loss: 0.5408\n",
      "Epoch 175/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 60.1937 - val_loss: 0.7196\n",
      "Epoch 176/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 58.4212 - val_loss: 0.9382\n",
      "Epoch 177/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 56.7191 - val_loss: 1.1948\n",
      "Epoch 178/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 55.0852 - val_loss: 1.4872\n",
      "Epoch 179/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 53.5176 - val_loss: 1.8135\n",
      "Epoch 180/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 52.0142 - val_loss: 2.1718\n",
      "Epoch 181/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 50.5729 - val_loss: 2.5602\n",
      "Epoch 182/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 49.1918 - val_loss: 2.9770\n",
      "Epoch 183/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 47.8688 - val_loss: 3.4204\n",
      "Epoch 184/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 46.6022 - val_loss: 3.8885\n",
      "Epoch 185/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 45.3899 - val_loss: 4.3797\n",
      "Epoch 186/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 44.2304 - val_loss: 4.8924\n",
      "Epoch 187/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 43.1217 - val_loss: 5.4249\n",
      "Epoch 188/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 42.0621 - val_loss: 5.9757\n",
      "Epoch 189/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 41.0500 - val_loss: 6.5433\n",
      "Epoch 190/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 40.0835 - val_loss: 7.1263\n",
      "Epoch 191/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 39.1609 - val_loss: 7.7231\n",
      "Epoch 192/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 38.2809 - val_loss: 8.3324\n",
      "Epoch 193/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 37.4419 - val_loss: 8.9529\n",
      "Epoch 194/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 36.6422 - val_loss: 9.5833\n",
      "Epoch 195/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 35.8804 - val_loss: 10.2223\n",
      "Epoch 196/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 35.1550 - val_loss: 10.8688\n",
      "Epoch 197/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 34.4647 - val_loss: 11.5216\n",
      "Epoch 198/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 33.8080 - val_loss: 12.1795\n",
      "Epoch 199/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 33.1838 - val_loss: 12.8415\n",
      "Epoch 200/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 32.5905 - val_loss: 13.5065\n",
      "Epoch 201/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 32.0270 - val_loss: 14.1736\n",
      "Epoch 202/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 31.4921 - val_loss: 14.8418\n",
      "Epoch 203/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 30.9845 - val_loss: 15.5102\n",
      "Epoch 204/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 30.5031 - val_loss: 16.1780\n",
      "Epoch 205/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 30.0469 - val_loss: 16.8442\n",
      "Epoch 206/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 29.6146 - val_loss: 17.5083\n",
      "Epoch 207/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 29.2052 - val_loss: 18.1694\n",
      "Epoch 208/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 28.8178 - val_loss: 18.8266\n",
      "Epoch 209/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 28.4513 - val_loss: 19.4797\n",
      "Epoch 210/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 28.1048 - val_loss: 20.1276\n",
      "Epoch 211/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 27.7774 - val_loss: 20.7700\n",
      "Epoch 212/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 27.4681 - val_loss: 21.4061\n",
      "Epoch 213/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 27.1763 - val_loss: 22.0355\n",
      "Epoch 214/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 26.9008 - val_loss: 22.6578\n",
      "Epoch 215/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 26.6411 - val_loss: 23.2723\n",
      "Epoch 216/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 26.3964 - val_loss: 23.8789\n",
      "Epoch 217/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 26.1658 - val_loss: 24.4770\n",
      "Epoch 218/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 25.9487 - val_loss: 25.0663\n",
      "Epoch 219/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 25.7445 - val_loss: 25.6461\n",
      "Epoch 220/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 25.5524 - val_loss: 26.2166\n",
      "Epoch 221/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 25.3719 - val_loss: 26.7774\n",
      "Epoch 222/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 25.2023 - val_loss: 27.3281\n",
      "Epoch 223/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 25.0430 - val_loss: 27.8686\n",
      "Epoch 224/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 24.8936 - val_loss: 28.3987\n",
      "Epoch 225/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 24.7535 - val_loss: 28.9182\n",
      "Epoch 226/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 24.6221 - val_loss: 29.4271\n",
      "Epoch 227/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 24.4989 - val_loss: 29.9249\n",
      "Epoch 228/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 24.3836 - val_loss: 30.4119\n",
      "Epoch 229/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 24.2758 - val_loss: 30.8877\n",
      "Epoch 230/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 24.1748 - val_loss: 31.3526\n",
      "Epoch 231/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 24.0804 - val_loss: 31.8062\n",
      "Epoch 232/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 23.9922 - val_loss: 32.2491\n",
      "Epoch 233/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 23.9098 - val_loss: 32.6806\n",
      "Epoch 234/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 23.8329 - val_loss: 33.1009\n",
      "Epoch 235/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 23.7611 - val_loss: 33.5102\n",
      "Epoch 236/500\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 23.6941 - val_loss: 33.9080\n",
      "Epoch 237/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 23.6318 - val_loss: 34.2951\n",
      "Epoch 238/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 23.5736 - val_loss: 34.6712\n",
      "Epoch 239/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 23.5195 - val_loss: 35.0366\n",
      "Epoch 240/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 23.4691 - val_loss: 35.3912\n",
      "Epoch 241/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 23.4222 - val_loss: 35.7350\n",
      "Epoch 242/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 23.3786 - val_loss: 36.0684\n",
      "Epoch 243/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 23.3381 - val_loss: 36.3915\n",
      "Epoch 244/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 23.3004 - val_loss: 36.7043\n",
      "Epoch 245/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 23.2655 - val_loss: 37.0070\n",
      "Epoch 246/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 23.2330 - val_loss: 37.3001\n",
      "Epoch 247/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 23.2029 - val_loss: 37.5830\n",
      "Epoch 248/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 23.1750 - val_loss: 37.8568\n",
      "Epoch 249/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 23.1490 - val_loss: 38.1207\n",
      "Epoch 250/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 23.1251 - val_loss: 38.3756\n",
      "Epoch 251/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 23.1028 - val_loss: 38.6215\n",
      "Epoch 252/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 23.0822 - val_loss: 38.8586\n",
      "Epoch 253/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 23.0631 - val_loss: 39.0870\n",
      "Epoch 254/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 23.0455 - val_loss: 39.3071\n",
      "Epoch 255/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 23.0291 - val_loss: 39.5186\n",
      "Epoch 256/500\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 23.0141 - val_loss: 39.7221\n",
      "Epoch 257/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 23.0001 - val_loss: 39.9177\n",
      "Epoch 258/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.9872 - val_loss: 40.1058\n",
      "Epoch 259/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.9753 - val_loss: 40.2861\n",
      "Epoch 260/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.9644 - val_loss: 40.4596\n",
      "Epoch 261/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.9542 - val_loss: 40.6258\n",
      "Epoch 262/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.9448 - val_loss: 40.7851\n",
      "Epoch 263/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.9362 - val_loss: 40.9375\n",
      "Epoch 264/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.9283 - val_loss: 41.0838\n",
      "Epoch 265/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.9209 - val_loss: 41.2237\n",
      "Epoch 266/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.9142 - val_loss: 41.3577\n",
      "Epoch 267/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 22.9079 - val_loss: 41.4856\n",
      "Epoch 268/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.9022 - val_loss: 41.6080\n",
      "Epoch 269/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8969 - val_loss: 41.7246\n",
      "Epoch 270/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8921 - val_loss: 41.8363\n",
      "Epoch 271/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 22.8877 - val_loss: 41.9427\n",
      "Epoch 272/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 22.8836 - val_loss: 42.0443\n",
      "Epoch 273/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8798 - val_loss: 42.1408\n",
      "Epoch 274/500\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 22.8764 - val_loss: 42.2329\n",
      "Epoch 275/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8733 - val_loss: 42.3206\n",
      "Epoch 276/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8704 - val_loss: 42.4040\n",
      "Epoch 277/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8677 - val_loss: 42.4835\n",
      "Epoch 278/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8653 - val_loss: 42.5587\n",
      "Epoch 279/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8631 - val_loss: 42.6303\n",
      "Epoch 280/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8611 - val_loss: 42.6984\n",
      "Epoch 281/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8593 - val_loss: 42.7629\n",
      "Epoch 282/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8576 - val_loss: 42.8241\n",
      "Epoch 283/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8561 - val_loss: 42.8821\n",
      "Epoch 284/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8548 - val_loss: 42.9370\n",
      "Epoch 285/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8536 - val_loss: 42.9891\n",
      "Epoch 286/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8524 - val_loss: 43.0384\n",
      "Epoch 287/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8515 - val_loss: 43.0849\n",
      "Epoch 288/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8506 - val_loss: 43.1288\n",
      "Epoch 289/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8498 - val_loss: 43.1704\n",
      "Epoch 290/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8491 - val_loss: 43.2095\n",
      "Epoch 291/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8485 - val_loss: 43.2462\n",
      "Epoch 292/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8480 - val_loss: 43.2812\n",
      "Epoch 293/500\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 22.8476 - val_loss: 43.3140\n",
      "Epoch 294/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8472 - val_loss: 43.3450\n",
      "Epoch 295/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 22.8469 - val_loss: 43.3739\n",
      "Epoch 296/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8466 - val_loss: 43.4013\n",
      "Epoch 297/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8464 - val_loss: 43.4269\n",
      "Epoch 298/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8463 - val_loss: 43.4509\n",
      "Epoch 299/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8462 - val_loss: 43.4736\n",
      "Epoch 300/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8462 - val_loss: 43.4951\n",
      "Epoch 301/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8461 - val_loss: 43.5148\n",
      "Epoch 302/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8462 - val_loss: 43.5335\n",
      "Epoch 303/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8462 - val_loss: 43.5508\n",
      "Epoch 304/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8463 - val_loss: 43.5669\n",
      "Epoch 305/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8464 - val_loss: 43.5822\n",
      "Epoch 306/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8466 - val_loss: 43.5965\n",
      "Epoch 307/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8468 - val_loss: 43.6096\n",
      "Epoch 308/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8470 - val_loss: 43.6221\n",
      "Epoch 309/500\n",
      "19/19 [==============================] - 0s 11ms/step - loss: 22.8472 - val_loss: 43.6337\n",
      "Epoch 310/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8475 - val_loss: 43.6445\n",
      "Epoch 311/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8478 - val_loss: 43.6544\n",
      "Epoch 312/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8481 - val_loss: 43.6637\n",
      "Epoch 313/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8483 - val_loss: 43.6722\n",
      "Epoch 314/500\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 22.8487 - val_loss: 43.6801\n",
      "Epoch 315/500\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 22.8490 - val_loss: 43.6873\n",
      "Epoch 316/500\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 22.8494 - val_loss: 43.6941\n",
      "Epoch 317/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 22.8497 - val_loss: 43.7003\n",
      "Epoch 318/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 22.8502 - val_loss: 43.7061\n",
      "Epoch 319/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8506 - val_loss: 43.7113\n",
      "Epoch 320/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8510 - val_loss: 43.7164\n",
      "Epoch 321/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8513 - val_loss: 43.7206\n",
      "Epoch 322/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8518 - val_loss: 43.7246\n",
      "Epoch 323/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8522 - val_loss: 43.7282\n",
      "Epoch 324/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8527 - val_loss: 43.7317\n",
      "Epoch 325/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8531 - val_loss: 43.7349\n",
      "Epoch 326/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8535 - val_loss: 43.7375\n",
      "Epoch 327/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8540 - val_loss: 43.7399\n",
      "Epoch 328/500\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 22.8545 - val_loss: 43.7421\n",
      "Epoch 329/500\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 22.8549 - val_loss: 43.7441\n",
      "Epoch 330/500\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 22.8554 - val_loss: 43.7459\n",
      "Epoch 331/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8558 - val_loss: 43.7475\n",
      "Epoch 332/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8563 - val_loss: 43.7488\n",
      "Epoch 333/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8568 - val_loss: 43.7498\n",
      "Epoch 334/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8573 - val_loss: 43.7508\n",
      "Epoch 335/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 22.8578 - val_loss: 43.7517\n",
      "Epoch 336/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8583 - val_loss: 43.7526\n",
      "Epoch 337/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8587 - val_loss: 43.7531\n",
      "Epoch 338/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8592 - val_loss: 43.7534\n",
      "Epoch 339/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8597 - val_loss: 43.7538\n",
      "Epoch 340/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8602 - val_loss: 43.7543\n",
      "Epoch 341/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8606 - val_loss: 43.7544\n",
      "Epoch 342/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8611 - val_loss: 43.7544\n",
      "Epoch 343/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8616 - val_loss: 43.7545\n",
      "Epoch 344/500\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 22.8621 - val_loss: 43.7543\n",
      "Epoch 345/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8626 - val_loss: 43.7540\n",
      "Epoch 346/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8631 - val_loss: 43.7538\n",
      "Epoch 347/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8635 - val_loss: 43.7533\n",
      "Epoch 348/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8640 - val_loss: 43.7532\n",
      "Epoch 349/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8645 - val_loss: 43.7530\n",
      "Epoch 350/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8649 - val_loss: 43.7524\n",
      "Epoch 351/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8654 - val_loss: 43.7518\n",
      "Epoch 352/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8659 - val_loss: 43.7515\n",
      "Epoch 353/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8663 - val_loss: 43.7510\n",
      "Epoch 354/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8668 - val_loss: 43.7501\n",
      "Epoch 355/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8673 - val_loss: 43.7497\n",
      "Epoch 356/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8677 - val_loss: 43.7491\n",
      "Epoch 357/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8682 - val_loss: 43.7485\n",
      "Epoch 358/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8686 - val_loss: 43.7478\n",
      "Epoch 359/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8691 - val_loss: 43.7474\n",
      "Epoch 360/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8695 - val_loss: 43.7466\n",
      "Epoch 361/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 22.8700 - val_loss: 43.7459\n",
      "Epoch 362/500\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 22.8704 - val_loss: 43.7451\n",
      "Epoch 363/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8708 - val_loss: 43.7443\n",
      "Epoch 364/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8712 - val_loss: 43.7435\n",
      "Epoch 365/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8717 - val_loss: 43.7428\n",
      "Epoch 366/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 22.8721 - val_loss: 43.7422\n",
      "Epoch 367/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8725 - val_loss: 43.7411\n",
      "Epoch 368/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8729 - val_loss: 43.7405\n",
      "Epoch 369/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8734 - val_loss: 43.7397\n",
      "Epoch 370/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8738 - val_loss: 43.7390\n",
      "Epoch 371/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8742 - val_loss: 43.7384\n",
      "Epoch 372/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8746 - val_loss: 43.7374\n",
      "Epoch 373/500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 22.8750 - val_loss: 43.7367\n",
      "Epoch 374/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8754 - val_loss: 43.7359\n",
      "Epoch 375/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8757 - val_loss: 43.7351\n",
      "Epoch 376/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8762 - val_loss: 43.7347\n",
      "Epoch 377/500\n",
      "19/19 [==============================] - 0s 10ms/step - loss: 22.8765 - val_loss: 43.7337\n",
      "Epoch 378/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8769 - val_loss: 43.7331\n",
      "Epoch 379/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8773 - val_loss: 43.7323\n",
      "Epoch 380/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8776 - val_loss: 43.7316\n",
      "Epoch 381/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8780 - val_loss: 43.7310\n",
      "Epoch 382/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8783 - val_loss: 43.7299\n",
      "Epoch 383/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8787 - val_loss: 43.7293\n",
      "Epoch 384/500\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 22.8791 - val_loss: 43.7283\n",
      "Epoch 385/500\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 22.8795 - val_loss: 43.7277\n",
      "Epoch 386/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8798 - val_loss: 43.7271\n",
      "Epoch 387/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8801 - val_loss: 43.7264\n",
      "Epoch 388/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8805 - val_loss: 43.7259\n",
      "Epoch 389/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8808 - val_loss: 43.7253\n",
      "Epoch 390/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8811 - val_loss: 43.7245\n",
      "Epoch 391/500\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 22.8814 - val_loss: 43.7239\n",
      "Epoch 392/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8818 - val_loss: 43.7232\n",
      "Epoch 393/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8821 - val_loss: 43.7225\n",
      "Epoch 394/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8824 - val_loss: 43.7219\n",
      "Epoch 395/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8827 - val_loss: 43.7214\n",
      "Epoch 396/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8830 - val_loss: 43.7207\n",
      "Epoch 397/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8833 - val_loss: 43.7200\n",
      "Epoch 398/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8836 - val_loss: 43.7193\n",
      "Epoch 399/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8839 - val_loss: 43.7187\n",
      "Epoch 400/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8842 - val_loss: 43.7179\n",
      "Epoch 401/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8845 - val_loss: 43.7171\n",
      "Epoch 402/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8848 - val_loss: 43.7167\n",
      "Epoch 403/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8851 - val_loss: 43.7164\n",
      "Epoch 404/500\n",
      "19/19 [==============================] - 0s 10ms/step - loss: 22.8853 - val_loss: 43.7156\n",
      "Epoch 405/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8856 - val_loss: 43.7151\n",
      "Epoch 406/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8859 - val_loss: 43.7146\n",
      "Epoch 407/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8862 - val_loss: 43.7141\n",
      "Epoch 408/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8864 - val_loss: 43.7135\n",
      "Epoch 409/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8867 - val_loss: 43.7131\n",
      "Epoch 410/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8869 - val_loss: 43.7124\n",
      "Epoch 411/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8872 - val_loss: 43.7117\n",
      "Epoch 412/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8875 - val_loss: 43.7113\n",
      "Epoch 413/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8877 - val_loss: 43.7109\n",
      "Epoch 414/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8880 - val_loss: 43.7105\n",
      "Epoch 415/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8882 - val_loss: 43.7098\n",
      "Epoch 416/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8884 - val_loss: 43.7095\n",
      "Epoch 417/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8887 - val_loss: 43.7092\n",
      "Epoch 418/500\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 22.8888 - val_loss: 43.7086\n",
      "Epoch 419/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8891 - val_loss: 43.7082\n",
      "Epoch 420/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8893 - val_loss: 43.7076\n",
      "Epoch 421/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8896 - val_loss: 43.7075\n",
      "Epoch 422/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8897 - val_loss: 43.7069\n",
      "Epoch 423/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8900 - val_loss: 43.7064\n",
      "Epoch 424/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8902 - val_loss: 43.7059\n",
      "Epoch 425/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8904 - val_loss: 43.7057\n",
      "Epoch 426/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8906 - val_loss: 43.7055\n",
      "Epoch 427/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8908 - val_loss: 43.7050\n",
      "Epoch 428/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8910 - val_loss: 43.7044\n",
      "Epoch 429/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8912 - val_loss: 43.7042\n",
      "Epoch 430/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8914 - val_loss: 43.7039\n",
      "Epoch 431/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8915 - val_loss: 43.7036\n",
      "Epoch 432/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8917 - val_loss: 43.7031\n",
      "Epoch 433/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8919 - val_loss: 43.7025\n",
      "Epoch 434/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8921 - val_loss: 43.7023\n",
      "Epoch 435/500\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 22.8923 - val_loss: 43.7022\n",
      "Epoch 436/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8924 - val_loss: 43.7017\n",
      "Epoch 437/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8926 - val_loss: 43.7012\n",
      "Epoch 438/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8928 - val_loss: 43.7008\n",
      "Epoch 439/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8930 - val_loss: 43.7006\n",
      "Epoch 440/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8931 - val_loss: 43.7003\n",
      "Epoch 441/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8933 - val_loss: 43.7002\n",
      "Epoch 442/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8934 - val_loss: 43.6999\n",
      "Epoch 443/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8936 - val_loss: 43.6995\n",
      "Epoch 444/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8937 - val_loss: 43.6992\n",
      "Epoch 445/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8939 - val_loss: 43.6990\n",
      "Epoch 446/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8940 - val_loss: 43.6988\n",
      "Epoch 447/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8942 - val_loss: 43.6984\n",
      "Epoch 448/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8944 - val_loss: 43.6982\n",
      "Epoch 449/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8945 - val_loss: 43.6980\n",
      "Epoch 450/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8946 - val_loss: 43.6976\n",
      "Epoch 451/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8948 - val_loss: 43.6974\n",
      "Epoch 452/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8949 - val_loss: 43.6970\n",
      "Epoch 453/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8950 - val_loss: 43.6967\n",
      "Epoch 454/500\n",
      "19/19 [==============================] - 0s 14ms/step - loss: 22.8952 - val_loss: 43.6966\n",
      "Epoch 455/500\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 22.8953 - val_loss: 43.6964\n",
      "Epoch 456/500\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 22.8954 - val_loss: 43.6963\n",
      "Epoch 457/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8955 - val_loss: 43.6958\n",
      "Epoch 458/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8956 - val_loss: 43.6955\n",
      "Epoch 459/500\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 22.8957 - val_loss: 43.6949\n",
      "Epoch 460/500\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 22.8959 - val_loss: 43.6946\n",
      "Epoch 461/500\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 22.8960 - val_loss: 43.6945\n",
      "Epoch 462/500\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 22.8961 - val_loss: 43.6942\n",
      "Epoch 463/500\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 22.8962 - val_loss: 43.6940\n",
      "Epoch 464/500\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 22.8963 - val_loss: 43.6939\n",
      "Epoch 465/500\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 22.8964 - val_loss: 43.6935\n",
      "Epoch 466/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8965 - val_loss: 43.6933\n",
      "Epoch 467/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8966 - val_loss: 43.6930\n",
      "Epoch 468/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8968 - val_loss: 43.6929\n",
      "Epoch 469/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8969 - val_loss: 43.6928\n",
      "Epoch 470/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8969 - val_loss: 43.6925\n",
      "Epoch 471/500\n",
      "19/19 [==============================] - 0s 11ms/step - loss: 22.8971 - val_loss: 43.6922\n",
      "Epoch 472/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8972 - val_loss: 43.6921\n",
      "Epoch 473/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8973 - val_loss: 43.6919\n",
      "Epoch 474/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8973 - val_loss: 43.6917\n",
      "Epoch 475/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8974 - val_loss: 43.6915\n",
      "Epoch 476/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8976 - val_loss: 43.6914\n",
      "Epoch 477/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8976 - val_loss: 43.6911\n",
      "Epoch 478/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8977 - val_loss: 43.6910\n",
      "Epoch 479/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8978 - val_loss: 43.6909\n",
      "Epoch 480/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8979 - val_loss: 43.6908\n",
      "Epoch 481/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8979 - val_loss: 43.6906\n",
      "Epoch 482/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8980 - val_loss: 43.6903\n",
      "Epoch 483/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8981 - val_loss: 43.6899\n",
      "Epoch 484/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8982 - val_loss: 43.6898\n",
      "Epoch 485/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8982 - val_loss: 43.6895\n",
      "Epoch 486/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8984 - val_loss: 43.6894\n",
      "Epoch 487/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8984 - val_loss: 43.6893\n",
      "Epoch 488/500\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 22.8985 - val_loss: 43.6892\n",
      "Epoch 489/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8985 - val_loss: 43.6889\n",
      "Epoch 490/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8987 - val_loss: 43.6889\n",
      "Epoch 491/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8987 - val_loss: 43.6887\n",
      "Epoch 492/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8987 - val_loss: 43.6884\n",
      "Epoch 493/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8988 - val_loss: 43.6880\n",
      "Epoch 494/500\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 22.8989 - val_loss: 43.6880\n",
      "Epoch 495/500\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 22.8990 - val_loss: 43.6878\n",
      "Epoch 496/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8991 - val_loss: 43.6878\n",
      "Epoch 497/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8991 - val_loss: 43.6876\n",
      "Epoch 498/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8992 - val_loss: 43.6877\n",
      "Epoch 499/500\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 22.8992 - val_loss: 43.6875\n",
      "Epoch 500/500\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 22.8992 - val_loss: 43.6874\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6465, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0029, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0003, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0168, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3581, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 441ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.15348413e+01, 6.15253175e+01, 6.15157936e+01, 6.15062698e+01,\n",
       "        6.14967460e+01, 6.14872222e+01, 6.14776984e+01, 6.14681746e+01,\n",
       "        6.14586508e+01, 6.14491270e+01, 6.14396032e+01, 6.14300794e+01,\n",
       "        6.14205556e+01, 6.14110317e+01, 6.14015079e+01, 6.13919841e+01,\n",
       "        6.13824603e+01, 6.13729365e+01, 6.13634127e+01, 6.13538889e+01,\n",
       "        6.13443651e+01, 6.13348413e+01, 6.13253175e+01, 6.13157937e+01,\n",
       "        6.13062698e+01, 6.12967460e+01, 6.12872222e+01, 6.12776984e+01,\n",
       "        6.12681746e+01, 6.12564286e+01, 6.12312185e+01, 6.12060084e+01,\n",
       "        6.11807983e+01, 6.11555882e+01, 6.11303781e+01, 6.11051681e+01,\n",
       "        6.10799580e+01, 6.10547479e+01, 6.10295378e+01, 6.10043277e+01,\n",
       "        6.09791176e+01, 6.09539076e+01, 6.09286975e+01, 6.09034874e+01,\n",
       "        6.08782773e+01, 6.08530672e+01, 6.08278571e+01, 6.08026471e+01,\n",
       "        6.07774370e+01, 6.07522269e+01, 6.07270168e+01, 6.07018067e+01,\n",
       "        6.06765966e+01, 6.06513865e+01, 6.06261765e+01, 6.06009664e+01,\n",
       "        6.05757563e+01, 6.05505462e+01, 6.05253361e+01, 6.05001260e+01,\n",
       "        6.04749160e+01, 6.04497059e+01, 6.04244958e+01, 6.03992857e+01,\n",
       "        6.03740756e+01, 6.03543091e+01, 6.03414239e+01, 6.03285387e+01,\n",
       "        6.03156536e+01, 6.03027684e+01, 6.02898833e+01, 6.02769981e+01,\n",
       "        6.02641130e+01, 6.02512278e+01, 6.02383427e+01, 6.02254575e+01,\n",
       "        6.02125724e+01, 6.01996872e+01, 6.01868020e+01, 6.01739169e+01,\n",
       "        6.76675415e+01, 0.00000000e+00, 0.00000000e+00, 4.05724868e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.06146836e-01,\n",
       "        8.80491316e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.79979944e-01,\n",
       "        0.00000000e+00, 2.53561914e-01, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58.30595238, 58.29007937, 58.27420635, 58.25833333, 58.24246032,\n",
       "       58.2265873 , 58.21071429, 58.19484127, 58.17896825, 58.16309524,\n",
       "       58.14722222, 58.13134921, 58.11547619, 58.09960317, 58.08373016,\n",
       "       58.06785714, 58.05198413, 58.03611111, 58.0202381 , 58.00436508,\n",
       "       57.98849206, 57.97261905, 57.95674603, 57.94087302, 57.925     ,\n",
       "       57.90912698, 57.89325397, 57.87738095, 57.86150794, 57.84563492,\n",
       "       57.8297619 , 57.81388889, 57.79801587, 57.78214286, 57.76626984,\n",
       "       57.75039683, 57.73452381, 57.71865079, 57.70277778, 57.68690476,\n",
       "       57.67103175, 57.65515873, 57.63928571, 57.6234127 , 57.60753968,\n",
       "       57.59166667, 57.57579365, 57.55992063, 57.54404762, 57.5281746 ,\n",
       "       57.51230159, 57.49642857, 57.48055556, 57.46468254, 57.44880952,\n",
       "       57.43293651, 57.41706349, 57.40119048, 57.38531746, 57.36944444,\n",
       "       57.35357143, 57.33769841, 57.3218254 , 57.30595238, 57.29007937,\n",
       "       57.27420635, 57.25833333, 57.24246032, 57.2265873 , 57.21071429,\n",
       "       57.19484127, 57.17896825, 57.16309524, 57.14722222, 57.13134921,\n",
       "       57.11547619, 57.09960317, 57.08373016, 57.06785714, 57.05198413,\n",
       "       57.03611111, 57.0202381 , 57.00436508, 56.98849206, 56.97261905,\n",
       "       56.95674603, 56.94087302, 56.925     , 56.90912698, 56.89325397,\n",
       "       56.87738095, 56.86150794, 56.84563492, 56.8297619 , 56.81388889,\n",
       "       56.79801587, 56.78214286, 56.76626984, 56.75039683, 56.73452381])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.93551381101665\n",
      "13.48777956116293\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
