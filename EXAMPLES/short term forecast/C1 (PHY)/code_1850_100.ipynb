{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.200000\n",
       "1       88.931092\n",
       "2       88.662185\n",
       "3       88.393277\n",
       "4       88.124370\n",
       "          ...    \n",
       "1945    58.385317\n",
       "1946    58.369444\n",
       "1947    58.353571\n",
       "1948    58.337698\n",
       "1949    58.321825\n",
       "Name: C1, Length: 1950, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c1_interpolated_1850_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.200000\n",
       "1       88.931092\n",
       "2       88.662185\n",
       "3       88.393277\n",
       "4       88.124370\n",
       "          ...    \n",
       "1845     0.000000\n",
       "1846     0.000000\n",
       "1847     0.000000\n",
       "1848     0.000000\n",
       "1849     0.000000\n",
       "Name: C1, Length: 1850, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1850)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.931092</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.662185</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.393277</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.124370</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     89.200000  0.000298\n",
       "1     88.931092  0.000298\n",
       "2     88.662185  0.000297\n",
       "3     88.393277  0.000297\n",
       "4     88.124370  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6465  # Value for C0\n",
    "K0 = -0.0029  # Value for K0\n",
    "K1 = -0.0003  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0168    # Value for b\n",
    "c = 2.3581    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    89.200000    88.931092    88.662185    88.393277    88.124370   \n",
      "351    88.931092    88.662185    88.393277    88.124370    87.855462   \n",
      "352    88.662185    88.393277    88.124370    87.855462    87.586555   \n",
      "353    88.393277    88.124370    87.855462    87.586555    87.317647   \n",
      "354    88.124370    87.855462    87.586555    87.317647    87.048739   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.855462    87.586555    87.317647    87.048739    86.794538  ...   \n",
      "351    87.586555    87.317647    87.048739    86.794538    86.721709  ...   \n",
      "352    87.317647    87.048739    86.794538    86.721709    86.648880  ...   \n",
      "353    87.048739    86.794538    86.721709    86.648880    86.576050  ...   \n",
      "354    86.794538    86.721709    86.648880    86.576050    86.503221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   73.989683    0.000263   73.957937    0.000263   73.926190    0.000263   \n",
      "351   73.957937    0.000263   73.926190    0.000263   73.894444    0.000262   \n",
      "352   73.926190    0.000263   73.894444    0.000262   73.862698    0.000262   \n",
      "353   73.894444    0.000262   73.862698    0.000262   73.830952    0.000262   \n",
      "354   73.862698    0.000262   73.830952    0.000262   73.799206    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   73.894444    0.000262   73.862698    0.000262  \n",
      "351   73.862698    0.000262   73.830952    0.000262  \n",
      "352   73.830952    0.000262   73.799206    0.000262  \n",
      "353   73.799206    0.000262   73.767460    0.000262  \n",
      "354   73.767460    0.000262   73.735714    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 1, 251) (1400, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "18/18 [==============================] - 2s 26ms/step - loss: 4503.6934 - val_loss: 3592.6094\n",
      "Epoch 2/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4405.9185 - val_loss: 3521.0991\n",
      "Epoch 3/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4342.7725 - val_loss: 3472.8711\n",
      "Epoch 4/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4289.4595 - val_loss: 3425.2908\n",
      "Epoch 5/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4236.8799 - val_loss: 3378.3906\n",
      "Epoch 6/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4184.9819 - val_loss: 3332.0933\n",
      "Epoch 7/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4133.6904 - val_loss: 3286.3447\n",
      "Epoch 8/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4082.9568 - val_loss: 3241.1125\n",
      "Epoch 9/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4032.7512 - val_loss: 3196.3750\n",
      "Epoch 10/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3983.0544 - val_loss: 3152.1160\n",
      "Epoch 11/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3933.8508 - val_loss: 3108.3254\n",
      "Epoch 12/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3880.6055 - val_loss: 3046.8218\n",
      "Epoch 13/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3812.6992 - val_loss: 2995.9319\n",
      "Epoch 14/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3756.0840 - val_loss: 2946.2959\n",
      "Epoch 15/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3701.3196 - val_loss: 2898.3486\n",
      "Epoch 16/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3648.1868 - val_loss: 2851.6765\n",
      "Epoch 17/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3596.2957 - val_loss: 2806.0161\n",
      "Epoch 18/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3545.4094 - val_loss: 2761.2097\n",
      "Epoch 19/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3495.3872 - val_loss: 2717.1606\n",
      "Epoch 20/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3446.1372 - val_loss: 2673.8015\n",
      "Epoch 21/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3397.5959 - val_loss: 2631.0859\n",
      "Epoch 22/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3349.7183 - val_loss: 2588.9788\n",
      "Epoch 23/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3302.4685 - val_loss: 2547.4519\n",
      "Epoch 24/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3255.8193 - val_loss: 2506.4836\n",
      "Epoch 25/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3209.7488 - val_loss: 2466.0559\n",
      "Epoch 26/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3164.2388 - val_loss: 2426.1531\n",
      "Epoch 27/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3119.2742 - val_loss: 2386.7625\n",
      "Epoch 28/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3074.8403 - val_loss: 2347.8728\n",
      "Epoch 29/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3030.9268 - val_loss: 2309.4729\n",
      "Epoch 30/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2987.5229 - val_loss: 2271.5554\n",
      "Epoch 31/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2944.6191 - val_loss: 2234.1106\n",
      "Epoch 32/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2902.2070 - val_loss: 2197.1313\n",
      "Epoch 33/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2860.2788 - val_loss: 2160.6118\n",
      "Epoch 34/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2818.8279 - val_loss: 2124.5432\n",
      "Epoch 35/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2777.8474 - val_loss: 2088.9221\n",
      "Epoch 36/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2737.3306 - val_loss: 2053.7415\n",
      "Epoch 37/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2697.2729 - val_loss: 2018.9960\n",
      "Epoch 38/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2657.6675 - val_loss: 1984.6810\n",
      "Epoch 39/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2618.5095 - val_loss: 1950.7910\n",
      "Epoch 40/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2579.7944 - val_loss: 1917.3219\n",
      "Epoch 41/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2541.5171 - val_loss: 1884.2683\n",
      "Epoch 42/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2503.6733 - val_loss: 1851.6270\n",
      "Epoch 43/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2466.2581 - val_loss: 1819.3933\n",
      "Epoch 44/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2429.2673 - val_loss: 1787.5625\n",
      "Epoch 45/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2392.6970 - val_loss: 1756.1317\n",
      "Epoch 46/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2356.5430 - val_loss: 1725.0966\n",
      "Epoch 47/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2320.8010 - val_loss: 1694.4530\n",
      "Epoch 48/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2285.4678 - val_loss: 1664.1973\n",
      "Epoch 49/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2250.5388 - val_loss: 1634.3259\n",
      "Epoch 50/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2216.0117 - val_loss: 1604.8362\n",
      "Epoch 51/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2181.8811 - val_loss: 1575.7233\n",
      "Epoch 52/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2148.1448 - val_loss: 1546.9852\n",
      "Epoch 53/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2114.7986 - val_loss: 1518.6171\n",
      "Epoch 54/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2081.8401 - val_loss: 1490.6169\n",
      "Epoch 55/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2049.2651 - val_loss: 1462.9810\n",
      "Epoch 56/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2017.0706 - val_loss: 1435.7051\n",
      "Epoch 57/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1985.2533 - val_loss: 1408.7876\n",
      "Epoch 58/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1953.8098 - val_loss: 1382.2246\n",
      "Epoch 59/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1922.7373 - val_loss: 1356.0133\n",
      "Epoch 60/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1892.0328 - val_loss: 1330.1500\n",
      "Epoch 61/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1861.6927 - val_loss: 1304.6326\n",
      "Epoch 62/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1831.7145 - val_loss: 1279.4576\n",
      "Epoch 63/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1802.0947 - val_loss: 1254.6224\n",
      "Epoch 64/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1772.8311 - val_loss: 1230.1239\n",
      "Epoch 65/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1743.9199 - val_loss: 1205.9590\n",
      "Epoch 66/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1715.3588 - val_loss: 1182.1249\n",
      "Epoch 67/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1687.1448 - val_loss: 1158.6193\n",
      "Epoch 68/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1659.2749 - val_loss: 1135.4385\n",
      "Epoch 69/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1631.7465 - val_loss: 1112.5802\n",
      "Epoch 70/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1604.5565 - val_loss: 1090.0420\n",
      "Epoch 71/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1577.7021 - val_loss: 1067.8206\n",
      "Epoch 72/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1551.1813 - val_loss: 1045.9133\n",
      "Epoch 73/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1524.9905 - val_loss: 1024.3174\n",
      "Epoch 74/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1499.1276 - val_loss: 1003.0306\n",
      "Epoch 75/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1473.5892 - val_loss: 982.0500\n",
      "Epoch 76/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1448.3734 - val_loss: 961.3724\n",
      "Epoch 77/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1423.4777 - val_loss: 940.9960\n",
      "Epoch 78/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1398.8987 - val_loss: 920.9188\n",
      "Epoch 79/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1374.6346 - val_loss: 901.1370\n",
      "Epoch 80/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1350.6824 - val_loss: 881.6482\n",
      "Epoch 81/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1327.0393 - val_loss: 862.4502\n",
      "Epoch 82/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1303.7032 - val_loss: 843.5398\n",
      "Epoch 83/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1280.6716 - val_loss: 824.9160\n",
      "Epoch 84/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1257.9420 - val_loss: 806.5748\n",
      "Epoch 85/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1235.5114 - val_loss: 788.5136\n",
      "Epoch 86/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1213.3778 - val_loss: 770.7313\n",
      "Epoch 87/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1191.5383 - val_loss: 753.2243\n",
      "Epoch 88/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1169.9911 - val_loss: 735.9904\n",
      "Epoch 89/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1148.7330 - val_loss: 719.0270\n",
      "Epoch 90/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1127.7626 - val_loss: 702.3325\n",
      "Epoch 91/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1107.0765 - val_loss: 685.9034\n",
      "Epoch 92/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1086.6729 - val_loss: 669.7386\n",
      "Epoch 93/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1066.5492 - val_loss: 653.8341\n",
      "Epoch 94/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1046.7029 - val_loss: 638.1885\n",
      "Epoch 95/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1027.1318 - val_loss: 622.7991\n",
      "Epoch 96/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1007.8334 - val_loss: 607.6635\n",
      "Epoch 97/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 988.8057 - val_loss: 592.7798\n",
      "Epoch 98/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 970.0460 - val_loss: 578.1453\n",
      "Epoch 99/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 951.5524 - val_loss: 563.7575\n",
      "Epoch 100/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 933.3221 - val_loss: 549.6147\n",
      "Epoch 101/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 915.3533 - val_loss: 535.7138\n",
      "Epoch 102/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 897.6436 - val_loss: 522.0530\n",
      "Epoch 103/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 880.1905 - val_loss: 508.6298\n",
      "Epoch 104/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 862.9917 - val_loss: 495.4417\n",
      "Epoch 105/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 846.0453 - val_loss: 482.4875\n",
      "Epoch 106/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 829.3489 - val_loss: 469.7638\n",
      "Epoch 107/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 812.9001 - val_loss: 457.2684\n",
      "Epoch 108/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 796.6970 - val_loss: 444.9992\n",
      "Epoch 109/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 780.7370 - val_loss: 432.9542\n",
      "Epoch 110/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 765.0181 - val_loss: 421.1310\n",
      "Epoch 111/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 749.5381 - val_loss: 409.5278\n",
      "Epoch 112/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 734.2950 - val_loss: 398.1411\n",
      "Epoch 113/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 719.2861 - val_loss: 386.9701\n",
      "Epoch 114/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 704.5098 - val_loss: 376.0118\n",
      "Epoch 115/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 689.9635 - val_loss: 365.2641\n",
      "Epoch 116/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 675.6453 - val_loss: 354.7250\n",
      "Epoch 117/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 661.5530 - val_loss: 344.3925\n",
      "Epoch 118/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 647.6845 - val_loss: 334.2635\n",
      "Epoch 119/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 634.0374 - val_loss: 324.3370\n",
      "Epoch 120/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 620.6097 - val_loss: 314.6097\n",
      "Epoch 121/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 607.3994 - val_loss: 305.0804\n",
      "Epoch 122/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 594.4042 - val_loss: 295.7463\n",
      "Epoch 123/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 581.6219 - val_loss: 286.6051\n",
      "Epoch 124/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 569.0505 - val_loss: 277.6553\n",
      "Epoch 125/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 556.6882 - val_loss: 268.8947\n",
      "Epoch 126/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 544.5329 - val_loss: 260.3206\n",
      "Epoch 127/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 532.5820 - val_loss: 251.9311\n",
      "Epoch 128/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 520.8339 - val_loss: 243.7242\n",
      "Epoch 129/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 509.2863 - val_loss: 235.6976\n",
      "Epoch 130/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 497.9372 - val_loss: 227.8492\n",
      "Epoch 131/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 486.7843 - val_loss: 220.1772\n",
      "Epoch 132/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 475.8259 - val_loss: 212.6793\n",
      "Epoch 133/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 465.0598 - val_loss: 205.3532\n",
      "Epoch 134/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 454.4839 - val_loss: 198.1968\n",
      "Epoch 135/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 444.0961 - val_loss: 191.2082\n",
      "Epoch 136/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 433.8945 - val_loss: 184.3855\n",
      "Epoch 137/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 423.8770 - val_loss: 177.7261\n",
      "Epoch 138/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 414.0418 - val_loss: 171.2283\n",
      "Epoch 139/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 404.3864 - val_loss: 164.8897\n",
      "Epoch 140/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 394.9093 - val_loss: 158.7084\n",
      "Epoch 141/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 385.6082 - val_loss: 152.6827\n",
      "Epoch 142/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 376.4810 - val_loss: 146.8097\n",
      "Epoch 143/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 367.5259 - val_loss: 141.0881\n",
      "Epoch 144/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 358.7408 - val_loss: 135.5155\n",
      "Epoch 145/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 350.1239 - val_loss: 130.0900\n",
      "Epoch 146/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 341.6730 - val_loss: 124.8095\n",
      "Epoch 147/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 333.3864 - val_loss: 119.6719\n",
      "Epoch 148/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 325.2618 - val_loss: 114.6755\n",
      "Epoch 149/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 317.2975 - val_loss: 109.8177\n",
      "Epoch 150/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 309.4915 - val_loss: 105.0970\n",
      "Epoch 151/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 301.8417 - val_loss: 100.5110\n",
      "Epoch 152/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 294.3463 - val_loss: 96.0581\n",
      "Epoch 153/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 287.0034 - val_loss: 91.7357\n",
      "Epoch 154/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 279.8107 - val_loss: 87.5425\n",
      "Epoch 155/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 272.7667 - val_loss: 83.4761\n",
      "Epoch 156/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 265.8692 - val_loss: 79.5346\n",
      "Epoch 157/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 259.1164 - val_loss: 75.7161\n",
      "Epoch 158/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 252.5065 - val_loss: 72.0183\n",
      "Epoch 159/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 246.0372 - val_loss: 68.4396\n",
      "Epoch 160/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 239.7070 - val_loss: 64.9781\n",
      "Epoch 161/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 233.5140 - val_loss: 61.6315\n",
      "Epoch 162/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 227.4561 - val_loss: 58.3981\n",
      "Epoch 163/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 221.5315 - val_loss: 55.2758\n",
      "Epoch 164/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 215.7384 - val_loss: 52.2628\n",
      "Epoch 165/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 210.0748 - val_loss: 49.3572\n",
      "Epoch 166/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 204.5388 - val_loss: 46.5568\n",
      "Epoch 167/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 199.1288 - val_loss: 43.8601\n",
      "Epoch 168/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 193.8427 - val_loss: 41.2649\n",
      "Epoch 169/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 188.6787 - val_loss: 38.7694\n",
      "Epoch 170/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 183.6350 - val_loss: 36.3718\n",
      "Epoch 171/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 178.7098 - val_loss: 34.0702\n",
      "Epoch 172/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 173.9015 - val_loss: 31.8627\n",
      "Epoch 173/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 169.2080 - val_loss: 29.7473\n",
      "Epoch 174/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 164.6276 - val_loss: 27.7223\n",
      "Epoch 175/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 160.1582 - val_loss: 25.7858\n",
      "Epoch 176/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 155.7986 - val_loss: 23.9362\n",
      "Epoch 177/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 151.5466 - val_loss: 22.1714\n",
      "Epoch 178/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 147.4007 - val_loss: 20.4896\n",
      "Epoch 179/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 143.3589 - val_loss: 18.8892\n",
      "Epoch 180/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 139.4197 - val_loss: 17.3684\n",
      "Epoch 181/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 135.5812 - val_loss: 15.9252\n",
      "Epoch 182/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 131.8416 - val_loss: 14.5580\n",
      "Epoch 183/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 128.1993 - val_loss: 13.2650\n",
      "Epoch 184/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 124.6526 - val_loss: 12.0444\n",
      "Epoch 185/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 121.1998 - val_loss: 10.8946\n",
      "Epoch 186/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 117.8392 - val_loss: 9.8139\n",
      "Epoch 187/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 114.5691 - val_loss: 8.8003\n",
      "Epoch 188/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 111.3877 - val_loss: 7.8525\n",
      "Epoch 189/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 108.2937 - val_loss: 6.9686\n",
      "Epoch 190/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 105.2852 - val_loss: 6.1469\n",
      "Epoch 191/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 102.3607 - val_loss: 5.3860\n",
      "Epoch 192/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.5185 - val_loss: 4.6840\n",
      "Epoch 193/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 96.7570 - val_loss: 4.0394\n",
      "Epoch 194/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 94.0745 - val_loss: 3.4505\n",
      "Epoch 195/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 91.4695 - val_loss: 2.9157\n",
      "Epoch 196/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 88.9404 - val_loss: 2.4336\n",
      "Epoch 197/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 86.4859 - val_loss: 2.0025\n",
      "Epoch 198/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 84.1040 - val_loss: 1.6208\n",
      "Epoch 199/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 81.7936 - val_loss: 1.2872\n",
      "Epoch 200/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 79.5529 - val_loss: 0.9999\n",
      "Epoch 201/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 77.3805 - val_loss: 0.7576\n",
      "Epoch 202/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 75.2750 - val_loss: 0.5588\n",
      "Epoch 203/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 73.2347 - val_loss: 0.4019\n",
      "Epoch 204/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 71.2584 - val_loss: 0.2857\n",
      "Epoch 205/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 69.3446 - val_loss: 0.2085\n",
      "Epoch 206/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 67.4918 - val_loss: 0.1691\n",
      "Epoch 207/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 65.6985 - val_loss: 0.1660\n",
      "Epoch 208/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 63.9635 - val_loss: 0.1979\n",
      "Epoch 209/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 62.2854 - val_loss: 0.2635\n",
      "Epoch 210/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 60.6626 - val_loss: 0.3613\n",
      "Epoch 211/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 59.0940 - val_loss: 0.4900\n",
      "Epoch 212/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 57.5782 - val_loss: 0.6485\n",
      "Epoch 213/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 56.1138 - val_loss: 0.8354\n",
      "Epoch 214/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 54.6996 - val_loss: 1.0494\n",
      "Epoch 215/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 53.3344 - val_loss: 1.2894\n",
      "Epoch 216/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 52.0168 - val_loss: 1.5541\n",
      "Epoch 217/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 50.7455 - val_loss: 1.8423\n",
      "Epoch 218/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 49.5194 - val_loss: 2.1529\n",
      "Epoch 219/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 48.3372 - val_loss: 2.4847\n",
      "Epoch 220/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 47.1978 - val_loss: 2.8367\n",
      "Epoch 221/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 46.0999 - val_loss: 3.2076\n",
      "Epoch 222/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 45.0425 - val_loss: 3.5965\n",
      "Epoch 223/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 44.0243 - val_loss: 4.0023\n",
      "Epoch 224/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 43.0442 - val_loss: 4.4238\n",
      "Epoch 225/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 42.1013 - val_loss: 4.8603\n",
      "Epoch 226/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 41.1943 - val_loss: 5.3105\n",
      "Epoch 227/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 40.3222 - val_loss: 5.7736\n",
      "Epoch 228/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 39.4840 - val_loss: 6.2487\n",
      "Epoch 229/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 38.6787 - val_loss: 6.7348\n",
      "Epoch 230/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 37.9051 - val_loss: 7.2310\n",
      "Epoch 231/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 37.1624 - val_loss: 7.7364\n",
      "Epoch 232/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 36.4496 - val_loss: 8.2503\n",
      "Epoch 233/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 35.7657 - val_loss: 8.7717\n",
      "Epoch 234/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 35.1098 - val_loss: 9.2998\n",
      "Epoch 235/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 34.4810 - val_loss: 9.8339\n",
      "Epoch 236/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 33.8784 - val_loss: 10.3733\n",
      "Epoch 237/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 33.3011 - val_loss: 10.9170\n",
      "Epoch 238/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 32.7483 - val_loss: 11.4647\n",
      "Epoch 239/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 32.2191 - val_loss: 12.0154\n",
      "Epoch 240/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 31.7126 - val_loss: 12.5685\n",
      "Epoch 241/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 31.2283 - val_loss: 13.1234\n",
      "Epoch 242/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 30.7652 - val_loss: 13.6794\n",
      "Epoch 243/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 30.3226 - val_loss: 14.2359\n",
      "Epoch 244/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 29.8998 - val_loss: 14.7926\n",
      "Epoch 245/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 29.4958 - val_loss: 15.3486\n",
      "Epoch 246/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 29.1103 - val_loss: 15.9034\n",
      "Epoch 247/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.7424 - val_loss: 16.4568\n",
      "Epoch 248/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3914 - val_loss: 17.0079\n",
      "Epoch 249/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.0567 - val_loss: 17.5565\n",
      "Epoch 250/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 27.7377 - val_loss: 18.1022\n",
      "Epoch 251/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 27.4338 - val_loss: 18.6443\n",
      "Epoch 252/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 27.1443 - val_loss: 19.1825\n",
      "Epoch 253/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 26.8688 - val_loss: 19.7165\n",
      "Epoch 254/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 26.6066 - val_loss: 20.2461\n",
      "Epoch 255/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 26.3571 - val_loss: 20.7705\n",
      "Epoch 256/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 26.1200 - val_loss: 21.2895\n",
      "Epoch 257/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 25.8946 - val_loss: 21.8032\n",
      "Epoch 258/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 25.6804 - val_loss: 22.3110\n",
      "Epoch 259/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 25.4770 - val_loss: 22.8126\n",
      "Epoch 260/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 25.2838 - val_loss: 23.3080\n",
      "Epoch 261/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 25.1005 - val_loss: 23.7966\n",
      "Epoch 262/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.9266 - val_loss: 24.2783\n",
      "Epoch 263/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.7618 - val_loss: 24.7533\n",
      "Epoch 264/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 24.6055 - val_loss: 25.2209\n",
      "Epoch 265/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.4574 - val_loss: 25.6812\n",
      "Epoch 266/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.3171 - val_loss: 26.1337\n",
      "Epoch 267/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.1844 - val_loss: 26.5788\n",
      "Epoch 268/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.0587 - val_loss: 27.0161\n",
      "Epoch 269/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 23.9398 - val_loss: 27.4453\n",
      "Epoch 270/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 23.8273 - val_loss: 27.8665\n",
      "Epoch 271/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.7211 - val_loss: 28.2797\n",
      "Epoch 272/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 23.6206 - val_loss: 28.6848\n",
      "Epoch 273/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 23.5257 - val_loss: 29.0816\n",
      "Epoch 274/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 23.4361 - val_loss: 29.4703\n",
      "Epoch 275/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 23.3516 - val_loss: 29.8509\n",
      "Epoch 276/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 23.2717 - val_loss: 30.2227\n",
      "Epoch 277/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 23.1964 - val_loss: 30.5865\n",
      "Epoch 278/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 23.1254 - val_loss: 30.9420\n",
      "Epoch 279/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 23.0585 - val_loss: 31.2893\n",
      "Epoch 280/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.9954 - val_loss: 31.6283\n",
      "Epoch 281/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.9360 - val_loss: 31.9591\n",
      "Epoch 282/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.8800 - val_loss: 32.2815\n",
      "Epoch 283/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.8273 - val_loss: 32.5960\n",
      "Epoch 284/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.7776 - val_loss: 32.9023\n",
      "Epoch 285/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.7310 - val_loss: 33.2006\n",
      "Epoch 286/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.6871 - val_loss: 33.4912\n",
      "Epoch 287/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 22.6457 - val_loss: 33.7737\n",
      "Epoch 288/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.6069 - val_loss: 34.0483\n",
      "Epoch 289/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.5704 - val_loss: 34.3152\n",
      "Epoch 290/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.5362 - val_loss: 34.5745\n",
      "Epoch 291/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.5040 - val_loss: 34.8264\n",
      "Epoch 292/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.4737 - val_loss: 35.0709\n",
      "Epoch 293/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.4453 - val_loss: 35.3079\n",
      "Epoch 294/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.4187 - val_loss: 35.5379\n",
      "Epoch 295/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.3936 - val_loss: 35.7606\n",
      "Epoch 296/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.3701 - val_loss: 35.9766\n",
      "Epoch 297/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.3481 - val_loss: 36.1857\n",
      "Epoch 298/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.3275 - val_loss: 36.3881\n",
      "Epoch 299/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.3080 - val_loss: 36.5838\n",
      "Epoch 300/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.2899 - val_loss: 36.7731\n",
      "Epoch 301/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.2729 - val_loss: 36.9563\n",
      "Epoch 302/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.2569 - val_loss: 37.1329\n",
      "Epoch 303/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.2420 - val_loss: 37.3039\n",
      "Epoch 304/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.2279 - val_loss: 37.4687\n",
      "Epoch 305/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.2147 - val_loss: 37.6277\n",
      "Epoch 306/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 22.2024 - val_loss: 37.7811\n",
      "Epoch 307/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.1909 - val_loss: 37.9288\n",
      "Epoch 308/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.1801 - val_loss: 38.0714\n",
      "Epoch 309/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.1700 - val_loss: 38.2087\n",
      "Epoch 310/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.1605 - val_loss: 38.3409\n",
      "Epoch 311/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.1517 - val_loss: 38.4680\n",
      "Epoch 312/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.1433 - val_loss: 38.5901\n",
      "Epoch 313/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.1356 - val_loss: 38.7076\n",
      "Epoch 314/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.1283 - val_loss: 38.8206\n",
      "Epoch 315/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.1216 - val_loss: 38.9293\n",
      "Epoch 316/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.1151 - val_loss: 39.0336\n",
      "Epoch 317/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.1092 - val_loss: 39.1337\n",
      "Epoch 318/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.1036 - val_loss: 39.2297\n",
      "Epoch 319/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0984 - val_loss: 39.3220\n",
      "Epoch 320/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0935 - val_loss: 39.4102\n",
      "Epoch 321/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0890 - val_loss: 39.4949\n",
      "Epoch 322/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0847 - val_loss: 39.5758\n",
      "Epoch 323/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0807 - val_loss: 39.6534\n",
      "Epoch 324/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0770 - val_loss: 39.7274\n",
      "Epoch 325/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0736 - val_loss: 39.7984\n",
      "Epoch 326/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0703 - val_loss: 39.8663\n",
      "Epoch 327/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 22.0673 - val_loss: 39.9312\n",
      "Epoch 328/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0645 - val_loss: 39.9930\n",
      "Epoch 329/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0619 - val_loss: 40.0522\n",
      "Epoch 330/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.0594 - val_loss: 40.1085\n",
      "Epoch 331/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.0572 - val_loss: 40.1624\n",
      "Epoch 332/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0550 - val_loss: 40.2137\n",
      "Epoch 333/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0531 - val_loss: 40.2627\n",
      "Epoch 334/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0513 - val_loss: 40.3091\n",
      "Epoch 335/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0496 - val_loss: 40.3536\n",
      "Epoch 336/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0479 - val_loss: 40.3955\n",
      "Epoch 337/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.0465 - val_loss: 40.4358\n",
      "Epoch 338/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0452 - val_loss: 40.4741\n",
      "Epoch 339/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0439 - val_loss: 40.5104\n",
      "Epoch 340/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0428 - val_loss: 40.5449\n",
      "Epoch 341/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0417 - val_loss: 40.5775\n",
      "Epoch 342/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0408 - val_loss: 40.6088\n",
      "Epoch 343/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0399 - val_loss: 40.6381\n",
      "Epoch 344/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.0391 - val_loss: 40.6660\n",
      "Epoch 345/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 22.0383 - val_loss: 40.6928\n",
      "Epoch 346/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0376 - val_loss: 40.7177\n",
      "Epoch 347/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0370 - val_loss: 40.7417\n",
      "Epoch 348/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0364 - val_loss: 40.7642\n",
      "Epoch 349/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0359 - val_loss: 40.7854\n",
      "Epoch 350/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.0355 - val_loss: 40.8056\n",
      "Epoch 351/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0351 - val_loss: 40.8244\n",
      "Epoch 352/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0347 - val_loss: 40.8425\n",
      "Epoch 353/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0345 - val_loss: 40.8594\n",
      "Epoch 354/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0342 - val_loss: 40.8756\n",
      "Epoch 355/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0340 - val_loss: 40.8908\n",
      "Epoch 356/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0337 - val_loss: 40.9049\n",
      "Epoch 357/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0336 - val_loss: 40.9181\n",
      "Epoch 358/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0335 - val_loss: 40.9308\n",
      "Epoch 359/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0333 - val_loss: 40.9426\n",
      "Epoch 360/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0333 - val_loss: 40.9539\n",
      "Epoch 361/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0333 - val_loss: 40.9645\n",
      "Epoch 362/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0333 - val_loss: 40.9742\n",
      "Epoch 363/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0332 - val_loss: 40.9833\n",
      "Epoch 364/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 22.0333 - val_loss: 40.9920\n",
      "Epoch 365/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0334 - val_loss: 41.0002\n",
      "Epoch 366/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0334 - val_loss: 41.0078\n",
      "Epoch 367/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0335 - val_loss: 41.0149\n",
      "Epoch 368/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0336 - val_loss: 41.0214\n",
      "Epoch 369/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0338 - val_loss: 41.0279\n",
      "Epoch 370/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.0339 - val_loss: 41.0336\n",
      "Epoch 371/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0341 - val_loss: 41.0394\n",
      "Epoch 372/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0342 - val_loss: 41.0444\n",
      "Epoch 373/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0344 - val_loss: 41.0490\n",
      "Epoch 374/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0346 - val_loss: 41.0533\n",
      "Epoch 375/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0348 - val_loss: 41.0574\n",
      "Epoch 376/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0350 - val_loss: 41.0611\n",
      "Epoch 377/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0353 - val_loss: 41.0648\n",
      "Epoch 378/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0355 - val_loss: 41.0680\n",
      "Epoch 379/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0357 - val_loss: 41.0708\n",
      "Epoch 380/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0360 - val_loss: 41.0735\n",
      "Epoch 381/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 22.0362 - val_loss: 41.0761\n",
      "Epoch 382/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0364 - val_loss: 41.0781\n",
      "Epoch 383/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0368 - val_loss: 41.0805\n",
      "Epoch 384/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0370 - val_loss: 41.0823\n",
      "Epoch 385/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0373 - val_loss: 41.0840\n",
      "Epoch 386/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0375 - val_loss: 41.0856\n",
      "Epoch 387/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0379 - val_loss: 41.0871\n",
      "Epoch 388/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0381 - val_loss: 41.0885\n",
      "Epoch 389/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0384 - val_loss: 41.0897\n",
      "Epoch 390/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0387 - val_loss: 41.0908\n",
      "Epoch 391/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0390 - val_loss: 41.0917\n",
      "Epoch 392/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0393 - val_loss: 41.0929\n",
      "Epoch 393/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0396 - val_loss: 41.0934\n",
      "Epoch 394/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0399 - val_loss: 41.0941\n",
      "Epoch 395/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0402 - val_loss: 41.0945\n",
      "Epoch 396/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0405 - val_loss: 41.0953\n",
      "Epoch 397/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0408 - val_loss: 41.0958\n",
      "Epoch 398/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 22.0411 - val_loss: 41.0961\n",
      "Epoch 399/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0414 - val_loss: 41.0963\n",
      "Epoch 400/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0417 - val_loss: 41.0965\n",
      "Epoch 401/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0420 - val_loss: 41.0968\n",
      "Epoch 402/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0423 - val_loss: 41.0969\n",
      "Epoch 403/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.0426 - val_loss: 41.0970\n",
      "Epoch 404/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0429 - val_loss: 41.0973\n",
      "Epoch 405/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0432 - val_loss: 41.0974\n",
      "Epoch 406/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0435 - val_loss: 41.0972\n",
      "Epoch 407/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0438 - val_loss: 41.0969\n",
      "Epoch 408/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0441 - val_loss: 41.0970\n",
      "Epoch 409/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.0444 - val_loss: 41.0967\n",
      "Epoch 410/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0447 - val_loss: 41.0965\n",
      "Epoch 411/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0450 - val_loss: 41.0962\n",
      "Epoch 412/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0453 - val_loss: 41.0960\n",
      "Epoch 413/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0456 - val_loss: 41.0958\n",
      "Epoch 414/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0459 - val_loss: 41.0955\n",
      "Epoch 415/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 22.0462 - val_loss: 41.0953\n",
      "Epoch 416/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0464 - val_loss: 41.0948\n",
      "Epoch 417/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0467 - val_loss: 41.0944\n",
      "Epoch 418/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0470 - val_loss: 41.0941\n",
      "Epoch 419/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0473 - val_loss: 41.0937\n",
      "Epoch 420/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0476 - val_loss: 41.0934\n",
      "Epoch 421/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0478 - val_loss: 41.0931\n",
      "Epoch 422/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0481 - val_loss: 41.0929\n",
      "Epoch 423/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0484 - val_loss: 41.0924\n",
      "Epoch 424/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0486 - val_loss: 41.0920\n",
      "Epoch 425/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0489 - val_loss: 41.0916\n",
      "Epoch 426/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0491 - val_loss: 41.0912\n",
      "Epoch 427/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0494 - val_loss: 41.0908\n",
      "Epoch 428/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0497 - val_loss: 41.0903\n",
      "Epoch 429/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 22.0499 - val_loss: 41.0898\n",
      "Epoch 430/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.0502 - val_loss: 41.0895\n",
      "Epoch 431/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0504 - val_loss: 41.0890\n",
      "Epoch 432/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0507 - val_loss: 41.0887\n",
      "Epoch 433/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 22.0510 - val_loss: 41.0884\n",
      "Epoch 434/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 22.0512 - val_loss: 41.0882\n",
      "Epoch 435/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 22.0514 - val_loss: 41.0874\n",
      "Epoch 436/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0517 - val_loss: 41.0871\n",
      "Epoch 437/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0518 - val_loss: 41.0867\n",
      "Epoch 438/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0521 - val_loss: 41.0863\n",
      "Epoch 439/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0524 - val_loss: 41.0860\n",
      "Epoch 440/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0525 - val_loss: 41.0856\n",
      "Epoch 441/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0528 - val_loss: 41.0853\n",
      "Epoch 442/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0530 - val_loss: 41.0847\n",
      "Epoch 443/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0532 - val_loss: 41.0843\n",
      "Epoch 444/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0534 - val_loss: 41.0840\n",
      "Epoch 445/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0537 - val_loss: 41.0839\n",
      "Epoch 446/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0538 - val_loss: 41.0830\n",
      "Epoch 447/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0541 - val_loss: 41.0827\n",
      "Epoch 448/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0543 - val_loss: 41.0824\n",
      "Epoch 449/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0544 - val_loss: 41.0819\n",
      "Epoch 450/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0547 - val_loss: 41.0814\n",
      "Epoch 451/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 22.0549 - val_loss: 41.0811\n",
      "Epoch 452/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0551 - val_loss: 41.0808\n",
      "Epoch 453/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0552 - val_loss: 41.0806\n",
      "Epoch 454/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0555 - val_loss: 41.0802\n",
      "Epoch 455/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0556 - val_loss: 41.0796\n",
      "Epoch 456/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 22.0558 - val_loss: 41.0793\n",
      "Epoch 457/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0560 - val_loss: 41.0792\n",
      "Epoch 458/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0562 - val_loss: 41.0786\n",
      "Epoch 459/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0564 - val_loss: 41.0782\n",
      "Epoch 460/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0565 - val_loss: 41.0778\n",
      "Epoch 461/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0567 - val_loss: 41.0777\n",
      "Epoch 462/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0569 - val_loss: 41.0773\n",
      "Epoch 463/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 22.0571 - val_loss: 41.0770\n",
      "Epoch 464/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 22.0572 - val_loss: 41.0767\n",
      "Epoch 465/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0574 - val_loss: 41.0763\n",
      "Epoch 466/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0575 - val_loss: 41.0762\n",
      "Epoch 467/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0577 - val_loss: 41.0758\n",
      "Epoch 468/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 22.0578 - val_loss: 41.0752\n",
      "Epoch 469/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0580 - val_loss: 41.0749\n",
      "Epoch 470/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0582 - val_loss: 41.0748\n",
      "Epoch 471/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0584 - val_loss: 41.0746\n",
      "Epoch 472/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0585 - val_loss: 41.0745\n",
      "Epoch 473/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0586 - val_loss: 41.0741\n",
      "Epoch 474/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0588 - val_loss: 41.0736\n",
      "Epoch 475/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0589 - val_loss: 41.0735\n",
      "Epoch 476/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0591 - val_loss: 41.0733\n",
      "Epoch 477/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0592 - val_loss: 41.0731\n",
      "Epoch 478/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0593 - val_loss: 41.0729\n",
      "Epoch 479/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0594 - val_loss: 41.0724\n",
      "Epoch 480/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 22.0596 - val_loss: 41.0722\n",
      "Epoch 481/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0597 - val_loss: 41.0719\n",
      "Epoch 482/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0599 - val_loss: 41.0715\n",
      "Epoch 483/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0600 - val_loss: 41.0714\n",
      "Epoch 484/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0601 - val_loss: 41.0709\n",
      "Epoch 485/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 22.0603 - val_loss: 41.0706\n",
      "Epoch 486/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0604 - val_loss: 41.0704\n",
      "Epoch 487/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 22.0605 - val_loss: 41.0701\n",
      "Epoch 488/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 22.0606 - val_loss: 41.0700\n",
      "Epoch 489/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 22.0607 - val_loss: 41.0699\n",
      "Epoch 490/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 22.0608 - val_loss: 41.0698\n",
      "Epoch 491/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 22.0609 - val_loss: 41.0694\n",
      "Epoch 492/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 22.0611 - val_loss: 41.0692\n",
      "Epoch 493/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0612 - val_loss: 41.0690\n",
      "Epoch 494/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0613 - val_loss: 41.0687\n",
      "Epoch 495/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0614 - val_loss: 41.0686\n",
      "Epoch 496/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0615 - val_loss: 41.0685\n",
      "Epoch 497/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.0616 - val_loss: 41.0684\n",
      "Epoch 498/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.0616 - val_loss: 41.0679\n",
      "Epoch 499/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0618 - val_loss: 41.0676\n",
      "Epoch 500/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.0619 - val_loss: 41.0674\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6465, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0029, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0003, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0168, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3581, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 373ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.21194444e+01, 6.20998366e+01, 6.20802288e+01, 6.20606209e+01,\n",
       "        6.20410131e+01, 6.20214052e+01, 6.20017974e+01, 6.19821895e+01,\n",
       "        6.19625817e+01, 6.19429739e+01, 6.19233660e+01, 6.19037582e+01,\n",
       "        6.18841503e+01, 6.18645425e+01, 6.18449346e+01, 6.18253268e+01,\n",
       "        6.18057189e+01, 6.17861111e+01, 6.17665033e+01, 6.17468954e+01,\n",
       "        6.17272876e+01, 6.17076797e+01, 6.16880719e+01, 6.16684640e+01,\n",
       "        6.16488562e+01, 6.16292484e+01, 6.16096405e+01, 6.15951587e+01,\n",
       "        6.15856349e+01, 6.15761111e+01, 6.15665873e+01, 6.15570635e+01,\n",
       "        6.15475397e+01, 6.15380159e+01, 6.15284921e+01, 6.15189683e+01,\n",
       "        6.15094444e+01, 6.14999206e+01, 6.14903968e+01, 6.14808730e+01,\n",
       "        6.14713492e+01, 6.14618254e+01, 6.14523016e+01, 6.14427778e+01,\n",
       "        6.14332540e+01, 6.14237302e+01, 6.14142064e+01, 6.14046825e+01,\n",
       "        6.13951587e+01, 6.13856349e+01, 6.13761111e+01, 6.13665873e+01,\n",
       "        6.13570635e+01, 6.13475397e+01, 6.13380159e+01, 6.13284921e+01,\n",
       "        6.13189682e+01, 6.13094444e+01, 6.12999206e+01, 6.12903968e+01,\n",
       "        6.12808730e+01, 6.12713492e+01, 6.12618254e+01, 6.12396218e+01,\n",
       "        6.12144118e+01, 6.11892017e+01, 6.11639916e+01, 6.11387815e+01,\n",
       "        6.11135714e+01, 6.10883613e+01, 6.10631513e+01, 6.10379412e+01,\n",
       "        6.10127311e+01, 6.09875210e+01, 6.09623109e+01, 6.09371008e+01,\n",
       "        6.09118908e+01, 6.08866807e+01, 6.08614706e+01, 6.08362605e+01,\n",
       "        6.79906693e+01, 0.00000000e+00, 0.00000000e+00, 1.25655919e-01,\n",
       "        0.00000000e+00, 1.39610022e-01, 6.92169905e-01, 0.00000000e+00,\n",
       "        9.62131262e-01, 1.97184727e-01, 1.10394910e-01, 1.37028396e-01,\n",
       "        0.00000000e+00, 3.58253643e-02, 2.83495396e-01, 4.66700792e-01,\n",
       "        5.51191390e-01, 0.00000000e+00, 9.25889537e-02, 3.46674263e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([59.50739963, 59.49712885, 59.48685808, 59.4765873 , 59.46631653,\n",
       "       59.45604575, 59.44577498, 59.4355042 , 59.42523343, 59.41496265,\n",
       "       59.40469188, 59.3944211 , 59.38415033, 59.37387955, 59.36360878,\n",
       "       59.353338  , 59.34306723, 59.33279645, 59.32252568, 59.3122549 ,\n",
       "       59.30198413, 59.29171335, 59.28144258, 59.2711718 , 59.26090103,\n",
       "       59.25063025, 59.24035948, 59.2300887 , 59.21981793, 59.20954715,\n",
       "       59.19927638, 59.1890056 , 59.17873483, 59.16846405, 59.15819328,\n",
       "       59.1479225 , 59.13765173, 59.12738095, 59.11711018, 59.1068394 ,\n",
       "       59.09656863, 59.08629785, 59.07602708, 59.0657563 , 59.05548553,\n",
       "       59.04521475, 59.03494398, 59.0246732 , 59.01440243, 59.00413165,\n",
       "       58.99386088, 58.9835901 , 58.97331933, 58.96304855, 58.95277778,\n",
       "       58.942507  , 58.93223623, 58.92196545, 58.91169468, 58.9014239 ,\n",
       "       58.89115313, 58.88088235, 58.87061158, 58.8603408 , 58.85007003,\n",
       "       58.83979925, 58.82952848, 58.8192577 , 58.80898693, 58.79801587,\n",
       "       58.78214286, 58.76626984, 58.75039683, 58.73452381, 58.71865079,\n",
       "       58.70277778, 58.68690476, 58.67103175, 58.65515873, 58.63928571,\n",
       "       58.6234127 , 58.60753968, 58.59166667, 58.57579365, 58.55992063,\n",
       "       58.54404762, 58.5281746 , 58.51230159, 58.49642857, 58.48055556,\n",
       "       58.46468254, 58.44880952, 58.43293651, 58.41706349, 58.40119048,\n",
       "       58.38531746, 58.36944444, 58.35357143, 58.33769841, 58.3218254 ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.497039600326186\n",
      "13.076484399399389\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
