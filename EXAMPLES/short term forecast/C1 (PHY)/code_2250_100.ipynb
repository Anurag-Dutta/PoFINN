{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.200000\n",
       "1       88.931092\n",
       "2       88.662185\n",
       "3       88.393277\n",
       "4       88.124370\n",
       "          ...    \n",
       "2345    53.546724\n",
       "2346    53.537979\n",
       "2347    53.529234\n",
       "2348    53.520489\n",
       "2349    53.511744\n",
       "Name: C1, Length: 2350, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c1_interpolated_2250_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.200000\n",
       "1       88.931092\n",
       "2       88.662185\n",
       "3       88.393277\n",
       "4       88.124370\n",
       "          ...    \n",
       "2245     0.091308\n",
       "2246     0.074723\n",
       "2247     0.758683\n",
       "2248     0.000000\n",
       "2249     0.070704\n",
       "Name: C1, Length: 2250, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2250)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.931092</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.662185</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.393277</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.124370</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     89.200000  0.000298\n",
       "1     88.931092  0.000298\n",
       "2     88.662185  0.000297\n",
       "3     88.393277  0.000297\n",
       "4     88.124370  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6465  # Value for C0\n",
    "K0 = -0.0029  # Value for K0\n",
    "K1 = -0.0003  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0168    # Value for b\n",
    "c = 2.3581    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    89.200000    88.931092    88.662185    88.393277    88.124370   \n",
      "351    88.931092    88.662185    88.393277    88.124370    87.855462   \n",
      "352    88.662185    88.393277    88.124370    87.855462    87.586555   \n",
      "353    88.393277    88.124370    87.855462    87.586555    87.317647   \n",
      "354    88.124370    87.855462    87.586555    87.317647    87.048739   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.855462    87.586555    87.317647    87.048739    86.794538  ...   \n",
      "351    87.586555    87.317647    87.048739    86.794538    86.721709  ...   \n",
      "352    87.317647    87.048739    86.794538    86.721709    86.648880  ...   \n",
      "353    87.048739    86.794538    86.721709    86.648880    86.576050  ...   \n",
      "354    86.794538    86.721709    86.648880    86.576050    86.503221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   73.989683    0.000263   73.957937    0.000263   73.926190    0.000263   \n",
      "351   73.957937    0.000263   73.926190    0.000263   73.894444    0.000262   \n",
      "352   73.926190    0.000263   73.894444    0.000262   73.862698    0.000262   \n",
      "353   73.894444    0.000262   73.862698    0.000262   73.830952    0.000262   \n",
      "354   73.862698    0.000262   73.830952    0.000262   73.799206    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   73.894444    0.000262   73.862698    0.000262  \n",
      "351   73.862698    0.000262   73.830952    0.000262  \n",
      "352   73.830952    0.000262   73.799206    0.000262  \n",
      "353   73.799206    0.000262   73.767460    0.000262  \n",
      "354   73.767460    0.000262   73.735714    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 1, 251) (1800, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "23/23 [==============================] - 3s 22ms/step - loss: 4282.6406 - val_loss: 2978.0815\n",
      "Epoch 2/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4158.5244 - val_loss: 2911.6650\n",
      "Epoch 3/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4075.3618 - val_loss: 2850.6963\n",
      "Epoch 4/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3988.4878 - val_loss: 2794.3784\n",
      "Epoch 5/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3908.8042 - val_loss: 2739.7419\n",
      "Epoch 6/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3831.3003 - val_loss: 2686.5491\n",
      "Epoch 7/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3755.5647 - val_loss: 2634.5520\n",
      "Epoch 8/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3681.3284 - val_loss: 2583.6169\n",
      "Epoch 9/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3608.4407 - val_loss: 2533.6650\n",
      "Epoch 10/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3536.8127 - val_loss: 2484.6453\n",
      "Epoch 11/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3466.3838 - val_loss: 2436.5195\n",
      "Epoch 12/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3397.1077 - val_loss: 2389.2554\n",
      "Epoch 13/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3328.0874 - val_loss: 2335.4597\n",
      "Epoch 14/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3244.5369 - val_loss: 2286.8977\n",
      "Epoch 15/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3172.9243 - val_loss: 2239.7273\n",
      "Epoch 16/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3103.6101 - val_loss: 2193.9065\n",
      "Epoch 17/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3036.1809 - val_loss: 2149.2075\n",
      "Epoch 18/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2970.3052 - val_loss: 2105.5078\n",
      "Epoch 19/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2905.8057 - val_loss: 2062.7358\n",
      "Epoch 20/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2842.5747 - val_loss: 2020.8445\n",
      "Epoch 21/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2780.5400 - val_loss: 1979.7986\n",
      "Epoch 22/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2719.6487 - val_loss: 1939.5714\n",
      "Epoch 23/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2659.8616 - val_loss: 1900.1410\n",
      "Epoch 24/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2601.1443 - val_loss: 1861.4880\n",
      "Epoch 25/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2543.4695 - val_loss: 1823.5958\n",
      "Epoch 26/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2486.8130 - val_loss: 1786.4490\n",
      "Epoch 27/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 2431.1536 - val_loss: 1750.0337\n",
      "Epoch 28/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2376.4727 - val_loss: 1714.3330\n",
      "Epoch 29/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2319.2690 - val_loss: 1673.8048\n",
      "Epoch 30/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 2258.5371 - val_loss: 1635.2266\n",
      "Epoch 31/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2200.9895 - val_loss: 1598.4502\n",
      "Epoch 32/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2145.7029 - val_loss: 1563.0338\n",
      "Epoch 33/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2092.1477 - val_loss: 1528.7267\n",
      "Epoch 34/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2040.0361 - val_loss: 1495.3934\n",
      "Epoch 35/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1989.2106 - val_loss: 1462.9503\n",
      "Epoch 36/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1939.5674 - val_loss: 1431.3413\n",
      "Epoch 37/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1891.0353 - val_loss: 1400.5242\n",
      "Epoch 38/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1843.5605 - val_loss: 1370.4659\n",
      "Epoch 39/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1797.1012 - val_loss: 1341.1405\n",
      "Epoch 40/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1751.6206 - val_loss: 1312.5248\n",
      "Epoch 41/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1707.0909 - val_loss: 1284.5990\n",
      "Epoch 42/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1663.4855 - val_loss: 1257.3459\n",
      "Epoch 43/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1620.7811 - val_loss: 1230.7493\n",
      "Epoch 44/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1578.9572 - val_loss: 1204.7949\n",
      "Epoch 45/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1537.9954 - val_loss: 1179.4695\n",
      "Epoch 46/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1497.8771 - val_loss: 1154.7601\n",
      "Epoch 47/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1458.5873 - val_loss: 1130.6550\n",
      "Epoch 48/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1420.1093 - val_loss: 1107.1427\n",
      "Epoch 49/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1382.4290 - val_loss: 1084.2123\n",
      "Epoch 50/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1345.5326 - val_loss: 1061.8536\n",
      "Epoch 51/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1309.4064 - val_loss: 1040.0564\n",
      "Epoch 52/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1274.0375 - val_loss: 1018.8114\n",
      "Epoch 53/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1239.4137 - val_loss: 998.1085\n",
      "Epoch 54/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1205.5227 - val_loss: 977.9386\n",
      "Epoch 55/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1172.3530 - val_loss: 958.2930\n",
      "Epoch 56/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1139.8929 - val_loss: 939.1626\n",
      "Epoch 57/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1108.1320 - val_loss: 920.5392\n",
      "Epoch 58/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1077.0587 - val_loss: 902.4141\n",
      "Epoch 59/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1046.6627 - val_loss: 884.7786\n",
      "Epoch 60/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1016.9336 - val_loss: 867.6256\n",
      "Epoch 61/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 987.8610 - val_loss: 850.9460\n",
      "Epoch 62/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 959.4344 - val_loss: 834.7325\n",
      "Epoch 63/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 931.6451 - val_loss: 818.9772\n",
      "Epoch 64/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 904.4823 - val_loss: 803.6721\n",
      "Epoch 65/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 877.9365 - val_loss: 788.8094\n",
      "Epoch 66/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 851.9985 - val_loss: 774.3823\n",
      "Epoch 67/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 826.6588 - val_loss: 760.3830\n",
      "Epoch 68/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 801.9083 - val_loss: 746.8040\n",
      "Epoch 69/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 777.7374 - val_loss: 733.6378\n",
      "Epoch 70/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 754.1376 - val_loss: 720.8778\n",
      "Epoch 71/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 731.0997 - val_loss: 708.5162\n",
      "Epoch 72/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 708.6146 - val_loss: 696.5463\n",
      "Epoch 73/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 686.6736 - val_loss: 684.9609\n",
      "Epoch 74/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 665.2686 - val_loss: 673.7529\n",
      "Epoch 75/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 644.3901 - val_loss: 662.9157\n",
      "Epoch 76/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 624.0304 - val_loss: 652.4424\n",
      "Epoch 77/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 604.1807 - val_loss: 642.3258\n",
      "Epoch 78/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 584.8327 - val_loss: 632.5597\n",
      "Epoch 79/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 565.9781 - val_loss: 623.1369\n",
      "Epoch 80/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 547.6087 - val_loss: 614.0511\n",
      "Epoch 81/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 529.7161 - val_loss: 605.2953\n",
      "Epoch 82/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 512.2925 - val_loss: 596.8635\n",
      "Epoch 83/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 495.3295 - val_loss: 588.7485\n",
      "Epoch 84/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 478.8194 - val_loss: 580.9443\n",
      "Epoch 85/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 462.7543 - val_loss: 573.4444\n",
      "Epoch 86/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 447.1263 - val_loss: 566.2426\n",
      "Epoch 87/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 431.9276 - val_loss: 559.3320\n",
      "Epoch 88/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 417.1502 - val_loss: 552.7068\n",
      "Epoch 89/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 402.7869 - val_loss: 546.3608\n",
      "Epoch 90/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 388.8297 - val_loss: 540.2874\n",
      "Epoch 91/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 375.2711 - val_loss: 534.4807\n",
      "Epoch 92/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 362.1038 - val_loss: 528.9344\n",
      "Epoch 93/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 349.3203 - val_loss: 523.6427\n",
      "Epoch 94/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 336.9128 - val_loss: 518.5994\n",
      "Epoch 95/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 324.8745 - val_loss: 513.7988\n",
      "Epoch 96/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 313.1979 - val_loss: 509.2346\n",
      "Epoch 97/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 301.8756 - val_loss: 504.9009\n",
      "Epoch 98/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 290.9007 - val_loss: 500.7924\n",
      "Epoch 99/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 280.2658 - val_loss: 496.9026\n",
      "Epoch 100/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 269.9639 - val_loss: 493.2261\n",
      "Epoch 101/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 259.9882 - val_loss: 489.7573\n",
      "Epoch 102/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 250.3314 - val_loss: 486.4904\n",
      "Epoch 103/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 240.9871 - val_loss: 483.4197\n",
      "Epoch 104/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 231.9482 - val_loss: 480.5399\n",
      "Epoch 105/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 223.2078 - val_loss: 477.8454\n",
      "Epoch 106/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 214.7594 - val_loss: 475.3306\n",
      "Epoch 107/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 206.5962 - val_loss: 472.9903\n",
      "Epoch 108/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 198.7116 - val_loss: 470.8190\n",
      "Epoch 109/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 191.0991 - val_loss: 468.8116\n",
      "Epoch 110/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 183.7522 - val_loss: 466.9627\n",
      "Epoch 111/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 176.6645 - val_loss: 465.2672\n",
      "Epoch 112/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 169.8298 - val_loss: 463.7199\n",
      "Epoch 113/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 163.2416 - val_loss: 462.3158\n",
      "Epoch 114/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 156.8939 - val_loss: 461.0500\n",
      "Epoch 115/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 150.7803 - val_loss: 459.9174\n",
      "Epoch 116/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 144.8949 - val_loss: 458.9131\n",
      "Epoch 117/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 139.2317 - val_loss: 458.0326\n",
      "Epoch 118/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 133.7847 - val_loss: 457.2708\n",
      "Epoch 119/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 128.5481 - val_loss: 456.6231\n",
      "Epoch 120/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 123.5160 - val_loss: 456.0851\n",
      "Epoch 121/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 118.6827 - val_loss: 455.6520\n",
      "Epoch 122/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 114.0427 - val_loss: 455.3195\n",
      "Epoch 123/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 109.5903 - val_loss: 455.0830\n",
      "Epoch 124/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 105.3200 - val_loss: 454.9384\n",
      "Epoch 125/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 101.2265 - val_loss: 454.8813\n",
      "Epoch 126/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 97.3044 - val_loss: 454.9075\n",
      "Epoch 127/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 93.5483 - val_loss: 455.0130\n",
      "Epoch 128/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 89.9533 - val_loss: 455.1938\n",
      "Epoch 129/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 86.5142 - val_loss: 455.4457\n",
      "Epoch 130/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 83.2259 - val_loss: 455.7652\n",
      "Epoch 131/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 80.0835 - val_loss: 456.1482\n",
      "Epoch 132/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 77.0822 - val_loss: 456.5912\n",
      "Epoch 133/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 74.2173 - val_loss: 457.0904\n",
      "Epoch 134/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 71.4841 - val_loss: 457.6424\n",
      "Epoch 135/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 68.8780 - val_loss: 458.2437\n",
      "Epoch 136/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 66.3946 - val_loss: 458.8909\n",
      "Epoch 137/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 64.0295 - val_loss: 459.5807\n",
      "Epoch 138/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 61.7783 - val_loss: 460.3101\n",
      "Epoch 139/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 59.6369 - val_loss: 461.0757\n",
      "Epoch 140/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 57.6010 - val_loss: 461.8747\n",
      "Epoch 141/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 55.6668 - val_loss: 462.7039\n",
      "Epoch 142/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 53.8302 - val_loss: 463.5609\n",
      "Epoch 143/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 52.0874 - val_loss: 464.4425\n",
      "Epoch 144/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 50.4347 - val_loss: 465.3463\n",
      "Epoch 145/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 48.8683 - val_loss: 466.2697\n",
      "Epoch 146/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 47.3848 - val_loss: 467.2100\n",
      "Epoch 147/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 45.9808 - val_loss: 468.1650\n",
      "Epoch 148/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 44.6527 - val_loss: 469.1325\n",
      "Epoch 149/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 43.3973 - val_loss: 470.1101\n",
      "Epoch 150/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 42.2114 - val_loss: 471.0957\n",
      "Epoch 151/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 41.0919 - val_loss: 472.0873\n",
      "Epoch 152/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 40.0359 - val_loss: 473.0830\n",
      "Epoch 153/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 39.0404 - val_loss: 474.0809\n",
      "Epoch 154/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 38.1026 - val_loss: 475.0791\n",
      "Epoch 155/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 37.2198 - val_loss: 476.0761\n",
      "Epoch 156/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 36.3892 - val_loss: 477.0703\n",
      "Epoch 157/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 35.6085 - val_loss: 478.0599\n",
      "Epoch 158/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 34.8751 - val_loss: 479.0438\n",
      "Epoch 159/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 34.1866 - val_loss: 480.0206\n",
      "Epoch 160/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.5407 - val_loss: 480.9889\n",
      "Epoch 161/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 32.9353 - val_loss: 481.9476\n",
      "Epoch 162/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 32.3682 - val_loss: 482.8953\n",
      "Epoch 163/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 31.8374 - val_loss: 483.8314\n",
      "Epoch 164/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 31.3408 - val_loss: 484.7549\n",
      "Epoch 165/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 30.8767 - val_loss: 485.6647\n",
      "Epoch 166/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 30.4433 - val_loss: 486.5602\n",
      "Epoch 167/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 30.0387 - val_loss: 487.4404\n",
      "Epoch 168/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 29.6613 - val_loss: 488.3047\n",
      "Epoch 169/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 29.3097 - val_loss: 489.1528\n",
      "Epoch 170/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 28.9822 - val_loss: 489.9837\n",
      "Epoch 171/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 28.6774 - val_loss: 490.7971\n",
      "Epoch 172/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 28.3941 - val_loss: 491.5927\n",
      "Epoch 173/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 28.1307 - val_loss: 492.3700\n",
      "Epoch 174/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 27.8861 - val_loss: 493.1285\n",
      "Epoch 175/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 27.6593 - val_loss: 493.8683\n",
      "Epoch 176/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 27.4489 - val_loss: 494.5891\n",
      "Epoch 177/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 27.2540 - val_loss: 495.2907\n",
      "Epoch 178/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 27.0734 - val_loss: 495.9730\n",
      "Epoch 179/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 26.9064 - val_loss: 496.6359\n",
      "Epoch 180/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 26.7519 - val_loss: 497.2793\n",
      "Epoch 181/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 26.6092 - val_loss: 497.9035\n",
      "Epoch 182/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 26.4774 - val_loss: 498.5085\n",
      "Epoch 183/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 26.3558 - val_loss: 499.0940\n",
      "Epoch 184/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 26.2437 - val_loss: 499.6606\n",
      "Epoch 185/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.1403 - val_loss: 500.2083\n",
      "Epoch 186/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 26.0451 - val_loss: 500.7373\n",
      "Epoch 187/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.9575 - val_loss: 501.2478\n",
      "Epoch 188/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.8768 - val_loss: 501.7399\n",
      "Epoch 189/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.8027 - val_loss: 502.2142\n",
      "Epoch 190/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.7346 - val_loss: 502.6707\n",
      "Epoch 191/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.6721 - val_loss: 503.1098\n",
      "Epoch 192/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.6147 - val_loss: 503.5318\n",
      "Epoch 193/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.5620 - val_loss: 503.9371\n",
      "Epoch 194/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.5137 - val_loss: 504.3259\n",
      "Epoch 195/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 25.4695 - val_loss: 504.6988\n",
      "Epoch 196/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.4289 - val_loss: 505.0559\n",
      "Epoch 197/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.3918 - val_loss: 505.3978\n",
      "Epoch 198/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.3579 - val_loss: 505.7249\n",
      "Epoch 199/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.3268 - val_loss: 506.0374\n",
      "Epoch 200/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.2985 - val_loss: 506.3360\n",
      "Epoch 201/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.2725 - val_loss: 506.6211\n",
      "Epoch 202/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.2488 - val_loss: 506.8928\n",
      "Epoch 203/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.2272 - val_loss: 507.1519\n",
      "Epoch 204/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.2074 - val_loss: 507.3984\n",
      "Epoch 205/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1894 - val_loss: 507.6330\n",
      "Epoch 206/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1729 - val_loss: 507.8560\n",
      "Epoch 207/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.1579 - val_loss: 508.0678\n",
      "Epoch 208/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1443 - val_loss: 508.2689\n",
      "Epoch 209/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1319 - val_loss: 508.4595\n",
      "Epoch 210/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.1206 - val_loss: 508.6403\n",
      "Epoch 211/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.1102 - val_loss: 508.8115\n",
      "Epoch 212/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1008 - val_loss: 508.9734\n",
      "Epoch 213/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0923 - val_loss: 509.1263\n",
      "Epoch 214/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.0846 - val_loss: 509.2708\n",
      "Epoch 215/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.0776 - val_loss: 509.4070\n",
      "Epoch 216/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0713 - val_loss: 509.5358\n",
      "Epoch 217/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0655 - val_loss: 509.6571\n",
      "Epoch 218/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0603 - val_loss: 509.7712\n",
      "Epoch 219/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.0557 - val_loss: 509.8786\n",
      "Epoch 220/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0514 - val_loss: 509.9797\n",
      "Epoch 221/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0476 - val_loss: 510.0746\n",
      "Epoch 222/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0442 - val_loss: 510.1637\n",
      "Epoch 223/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 25.0411 - val_loss: 510.2472\n",
      "Epoch 224/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0384 - val_loss: 510.3254\n",
      "Epoch 225/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0360 - val_loss: 510.3987\n",
      "Epoch 226/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0338 - val_loss: 510.4672\n",
      "Epoch 227/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.0319 - val_loss: 510.5311\n",
      "Epoch 228/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0302 - val_loss: 510.5908\n",
      "Epoch 229/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0288 - val_loss: 510.6465\n",
      "Epoch 230/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0275 - val_loss: 510.6984\n",
      "Epoch 231/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0264 - val_loss: 510.7468\n",
      "Epoch 232/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0255 - val_loss: 510.7920\n",
      "Epoch 233/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0247 - val_loss: 510.8338\n",
      "Epoch 234/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0241 - val_loss: 510.8726\n",
      "Epoch 235/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0236 - val_loss: 510.9086\n",
      "Epoch 236/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0232 - val_loss: 510.9418\n",
      "Epoch 237/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0229 - val_loss: 510.9727\n",
      "Epoch 238/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0227 - val_loss: 511.0009\n",
      "Epoch 239/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0227 - val_loss: 511.0273\n",
      "Epoch 240/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.0227 - val_loss: 511.0514\n",
      "Epoch 241/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.0228 - val_loss: 511.0737\n",
      "Epoch 242/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0230 - val_loss: 511.0945\n",
      "Epoch 243/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0233 - val_loss: 511.1134\n",
      "Epoch 244/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0235 - val_loss: 511.1306\n",
      "Epoch 245/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0239 - val_loss: 511.1465\n",
      "Epoch 246/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 25.0243 - val_loss: 511.1608\n",
      "Epoch 247/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0248 - val_loss: 511.1740\n",
      "Epoch 248/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0253 - val_loss: 511.1860\n",
      "Epoch 249/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.0258 - val_loss: 511.1968\n",
      "Epoch 250/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0264 - val_loss: 511.2067\n",
      "Epoch 251/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0270 - val_loss: 511.2155\n",
      "Epoch 252/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0277 - val_loss: 511.2236\n",
      "Epoch 253/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0283 - val_loss: 511.2309\n",
      "Epoch 254/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0290 - val_loss: 511.2374\n",
      "Epoch 255/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0297 - val_loss: 511.2430\n",
      "Epoch 256/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0305 - val_loss: 511.2480\n",
      "Epoch 257/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.0313 - val_loss: 511.2526\n",
      "Epoch 258/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0320 - val_loss: 511.2565\n",
      "Epoch 259/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0329 - val_loss: 511.2599\n",
      "Epoch 260/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0337 - val_loss: 511.2630\n",
      "Epoch 261/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.0345 - val_loss: 511.2654\n",
      "Epoch 262/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0353 - val_loss: 511.2677\n",
      "Epoch 263/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0361 - val_loss: 511.2694\n",
      "Epoch 264/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0369 - val_loss: 511.2708\n",
      "Epoch 265/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0378 - val_loss: 511.2718\n",
      "Epoch 266/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0387 - val_loss: 511.2726\n",
      "Epoch 267/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0396 - val_loss: 511.2733\n",
      "Epoch 268/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0404 - val_loss: 511.2738\n",
      "Epoch 269/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.0413 - val_loss: 511.2740\n",
      "Epoch 270/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0422 - val_loss: 511.2742\n",
      "Epoch 271/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0430 - val_loss: 511.2740\n",
      "Epoch 272/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0439 - val_loss: 511.2736\n",
      "Epoch 273/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0447 - val_loss: 511.2731\n",
      "Epoch 274/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 25.0456 - val_loss: 511.2721\n",
      "Epoch 275/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0465 - val_loss: 511.2716\n",
      "Epoch 276/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0474 - val_loss: 511.2707\n",
      "Epoch 277/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0482 - val_loss: 511.2699\n",
      "Epoch 278/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0491 - val_loss: 511.2688\n",
      "Epoch 279/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0500 - val_loss: 511.2679\n",
      "Epoch 280/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0508 - val_loss: 511.2665\n",
      "Epoch 281/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0516 - val_loss: 511.2653\n",
      "Epoch 282/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0524 - val_loss: 511.2639\n",
      "Epoch 283/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0533 - val_loss: 511.2626\n",
      "Epoch 284/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0542 - val_loss: 511.2614\n",
      "Epoch 285/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0550 - val_loss: 511.2602\n",
      "Epoch 286/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0558 - val_loss: 511.2587\n",
      "Epoch 287/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0566 - val_loss: 511.2573\n",
      "Epoch 288/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0573 - val_loss: 511.2560\n",
      "Epoch 289/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0581 - val_loss: 511.2545\n",
      "Epoch 290/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0590 - val_loss: 511.2532\n",
      "Epoch 291/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0597 - val_loss: 511.2517\n",
      "Epoch 292/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0605 - val_loss: 511.2503\n",
      "Epoch 293/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0612 - val_loss: 511.2488\n",
      "Epoch 294/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0620 - val_loss: 511.2472\n",
      "Epoch 295/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0627 - val_loss: 511.2458\n",
      "Epoch 296/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 25.0635 - val_loss: 511.2444\n",
      "Epoch 297/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0642 - val_loss: 511.2429\n",
      "Epoch 298/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0649 - val_loss: 511.2413\n",
      "Epoch 299/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0656 - val_loss: 511.2399\n",
      "Epoch 300/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0663 - val_loss: 511.2383\n",
      "Epoch 301/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0670 - val_loss: 511.2369\n",
      "Epoch 302/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0677 - val_loss: 511.2354\n",
      "Epoch 303/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0684 - val_loss: 511.2339\n",
      "Epoch 304/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0691 - val_loss: 511.2325\n",
      "Epoch 305/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0697 - val_loss: 511.2311\n",
      "Epoch 306/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0703 - val_loss: 511.2297\n",
      "Epoch 307/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0710 - val_loss: 511.2283\n",
      "Epoch 308/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0717 - val_loss: 511.2270\n",
      "Epoch 309/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0723 - val_loss: 511.2256\n",
      "Epoch 310/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0729 - val_loss: 511.2243\n",
      "Epoch 311/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0735 - val_loss: 511.2230\n",
      "Epoch 312/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0741 - val_loss: 511.2216\n",
      "Epoch 313/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0747 - val_loss: 511.2204\n",
      "Epoch 314/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0753 - val_loss: 511.2192\n",
      "Epoch 315/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0758 - val_loss: 511.2178\n",
      "Epoch 316/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0764 - val_loss: 511.2166\n",
      "Epoch 317/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0770 - val_loss: 511.2153\n",
      "Epoch 318/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 25.0775 - val_loss: 511.2141\n",
      "Epoch 319/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0781 - val_loss: 511.2130\n",
      "Epoch 320/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0786 - val_loss: 511.2120\n",
      "Epoch 321/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0791 - val_loss: 511.2108\n",
      "Epoch 322/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0796 - val_loss: 511.2097\n",
      "Epoch 323/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0801 - val_loss: 511.2085\n",
      "Epoch 324/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0806 - val_loss: 511.2076\n",
      "Epoch 325/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.0811 - val_loss: 511.2065\n",
      "Epoch 326/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0816 - val_loss: 511.2053\n",
      "Epoch 327/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0821 - val_loss: 511.2043\n",
      "Epoch 328/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0825 - val_loss: 511.2033\n",
      "Epoch 329/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0830 - val_loss: 511.2024\n",
      "Epoch 330/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0834 - val_loss: 511.2013\n",
      "Epoch 331/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0839 - val_loss: 511.2003\n",
      "Epoch 332/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0843 - val_loss: 511.1992\n",
      "Epoch 333/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0848 - val_loss: 511.1984\n",
      "Epoch 334/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0851 - val_loss: 511.1974\n",
      "Epoch 335/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0856 - val_loss: 511.1966\n",
      "Epoch 336/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0860 - val_loss: 511.1957\n",
      "Epoch 337/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.0864 - val_loss: 511.1948\n",
      "Epoch 338/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0868 - val_loss: 511.1939\n",
      "Epoch 339/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0871 - val_loss: 511.1929\n",
      "Epoch 340/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0875 - val_loss: 511.1922\n",
      "Epoch 341/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 25.0879 - val_loss: 511.1912\n",
      "Epoch 342/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0882 - val_loss: 511.1905\n",
      "Epoch 343/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0886 - val_loss: 511.1897\n",
      "Epoch 344/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0890 - val_loss: 511.1890\n",
      "Epoch 345/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0893 - val_loss: 511.1884\n",
      "Epoch 346/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0897 - val_loss: 511.1875\n",
      "Epoch 347/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0900 - val_loss: 511.1869\n",
      "Epoch 348/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0903 - val_loss: 511.1861\n",
      "Epoch 349/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.0906 - val_loss: 511.1853\n",
      "Epoch 350/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0909 - val_loss: 511.1845\n",
      "Epoch 351/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0913 - val_loss: 511.1839\n",
      "Epoch 352/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0915 - val_loss: 511.1832\n",
      "Epoch 353/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0918 - val_loss: 511.1826\n",
      "Epoch 354/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0921 - val_loss: 511.1818\n",
      "Epoch 355/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0924 - val_loss: 511.1812\n",
      "Epoch 356/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 25.0927 - val_loss: 511.1805\n",
      "Epoch 357/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0930 - val_loss: 511.1800\n",
      "Epoch 358/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0932 - val_loss: 511.1793\n",
      "Epoch 359/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0935 - val_loss: 511.1786\n",
      "Epoch 360/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0938 - val_loss: 511.1780\n",
      "Epoch 361/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0940 - val_loss: 511.1775\n",
      "Epoch 362/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0943 - val_loss: 511.1770\n",
      "Epoch 363/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0945 - val_loss: 511.1765\n",
      "Epoch 364/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0948 - val_loss: 511.1761\n",
      "Epoch 365/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0950 - val_loss: 511.1755\n",
      "Epoch 366/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0952 - val_loss: 511.1749\n",
      "Epoch 367/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0954 - val_loss: 511.1745\n",
      "Epoch 368/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0956 - val_loss: 511.1739\n",
      "Epoch 369/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0958 - val_loss: 511.1732\n",
      "Epoch 370/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0961 - val_loss: 511.1728\n",
      "Epoch 371/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0963 - val_loss: 511.1724\n",
      "Epoch 372/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0965 - val_loss: 511.1720\n",
      "Epoch 373/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0967 - val_loss: 511.1715\n",
      "Epoch 374/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0969 - val_loss: 511.1711\n",
      "Epoch 375/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.0971 - val_loss: 511.1707\n",
      "Epoch 376/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0973 - val_loss: 511.1702\n",
      "Epoch 377/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0974 - val_loss: 511.1697\n",
      "Epoch 378/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 25.0977 - val_loss: 511.1694\n",
      "Epoch 379/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0978 - val_loss: 511.1691\n",
      "Epoch 380/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0980 - val_loss: 511.1689\n",
      "Epoch 381/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0981 - val_loss: 511.1682\n",
      "Epoch 382/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0983 - val_loss: 511.1677\n",
      "Epoch 383/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0985 - val_loss: 511.1672\n",
      "Epoch 384/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0987 - val_loss: 511.1671\n",
      "Epoch 385/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0988 - val_loss: 511.1667\n",
      "Epoch 386/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0989 - val_loss: 511.1662\n",
      "Epoch 387/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0991 - val_loss: 511.1658\n",
      "Epoch 388/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0993 - val_loss: 511.1656\n",
      "Epoch 389/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0994 - val_loss: 511.1653\n",
      "Epoch 390/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0995 - val_loss: 511.1649\n",
      "Epoch 391/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0996 - val_loss: 511.1645\n",
      "Epoch 392/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.0998 - val_loss: 511.1640\n",
      "Epoch 393/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0999 - val_loss: 511.1636\n",
      "Epoch 394/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1001 - val_loss: 511.1634\n",
      "Epoch 395/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.1002 - val_loss: 511.1631\n",
      "Epoch 396/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1003 - val_loss: 511.1630\n",
      "Epoch 397/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1004 - val_loss: 511.1627\n",
      "Epoch 398/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1005 - val_loss: 511.1623\n",
      "Epoch 399/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1007 - val_loss: 511.1620\n",
      "Epoch 400/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.1008 - val_loss: 511.1619\n",
      "Epoch 401/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1009 - val_loss: 511.1617\n",
      "Epoch 402/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 25.1009 - val_loss: 511.1614\n",
      "Epoch 403/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.1011 - val_loss: 511.1613\n",
      "Epoch 404/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.1012 - val_loss: 511.1610\n",
      "Epoch 405/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.1013 - val_loss: 511.1606\n",
      "Epoch 406/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1014 - val_loss: 511.1603\n",
      "Epoch 407/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1015 - val_loss: 511.1602\n",
      "Epoch 408/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1016 - val_loss: 511.1601\n",
      "Epoch 409/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1017 - val_loss: 511.1600\n",
      "Epoch 410/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1018 - val_loss: 511.1598\n",
      "Epoch 411/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1019 - val_loss: 511.1597\n",
      "Epoch 412/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1019 - val_loss: 511.1594\n",
      "Epoch 413/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1020 - val_loss: 511.1590\n",
      "Epoch 414/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1021 - val_loss: 511.1586\n",
      "Epoch 415/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.1022 - val_loss: 511.1585\n",
      "Epoch 416/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1023 - val_loss: 511.1584\n",
      "Epoch 417/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1024 - val_loss: 511.1583\n",
      "Epoch 418/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1024 - val_loss: 511.1581\n",
      "Epoch 419/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1025 - val_loss: 511.1580\n",
      "Epoch 420/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1025 - val_loss: 511.1577\n",
      "Epoch 421/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1027 - val_loss: 511.1576\n",
      "Epoch 422/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1027 - val_loss: 511.1575\n",
      "Epoch 423/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.1027 - val_loss: 511.1571\n",
      "Epoch 424/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1029 - val_loss: 511.1571\n",
      "Epoch 425/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1029 - val_loss: 511.1569\n",
      "Epoch 426/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1029 - val_loss: 511.1567\n",
      "Epoch 427/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.1031 - val_loss: 511.1569\n",
      "Epoch 428/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1031 - val_loss: 511.1567\n",
      "Epoch 429/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1031 - val_loss: 511.1566\n",
      "Epoch 430/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1032 - val_loss: 511.1566\n",
      "Epoch 431/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1033 - val_loss: 511.1564\n",
      "Epoch 432/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1033 - val_loss: 511.1563\n",
      "Epoch 433/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1033 - val_loss: 511.1561\n",
      "Epoch 434/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1034 - val_loss: 511.1560\n",
      "Epoch 435/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1035 - val_loss: 511.1558\n",
      "Epoch 436/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1035 - val_loss: 511.1556\n",
      "Epoch 437/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1035 - val_loss: 511.1554\n",
      "Epoch 438/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.1036 - val_loss: 511.1553\n",
      "Epoch 439/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1037 - val_loss: 511.1551\n",
      "Epoch 440/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1037 - val_loss: 511.1550\n",
      "Epoch 441/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1037 - val_loss: 511.1548\n",
      "Epoch 442/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1038 - val_loss: 511.1548\n",
      "Epoch 443/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 25.1039 - val_loss: 511.1549\n",
      "Epoch 444/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.1039 - val_loss: 511.1549\n",
      "Epoch 445/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1039 - val_loss: 511.1547\n",
      "Epoch 446/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1040 - val_loss: 511.1547\n",
      "Epoch 447/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1040 - val_loss: 511.1546\n",
      "Epoch 448/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1041 - val_loss: 511.1545\n",
      "Epoch 449/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1041 - val_loss: 511.1543\n",
      "Epoch 450/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1041 - val_loss: 511.1543\n",
      "Epoch 451/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1041 - val_loss: 511.1542\n",
      "Epoch 452/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1042 - val_loss: 511.1539\n",
      "Epoch 453/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1042 - val_loss: 511.1539\n",
      "Epoch 454/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1042 - val_loss: 511.1536\n",
      "Epoch 455/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.1042 - val_loss: 511.1535\n",
      "Epoch 456/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1043 - val_loss: 511.1532\n",
      "Epoch 457/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1043 - val_loss: 511.1531\n",
      "Epoch 458/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1044 - val_loss: 511.1532\n",
      "Epoch 459/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1044 - val_loss: 511.1532\n",
      "Epoch 460/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1044 - val_loss: 511.1531\n",
      "Epoch 461/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1045 - val_loss: 511.1530\n",
      "Epoch 462/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1044 - val_loss: 511.1527\n",
      "Epoch 463/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1045 - val_loss: 511.1525\n",
      "Epoch 464/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 25.1046 - val_loss: 511.1526\n",
      "Epoch 465/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1046 - val_loss: 511.1527\n",
      "Epoch 466/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1047 - val_loss: 511.1530\n",
      "Epoch 467/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1047 - val_loss: 511.1530\n",
      "Epoch 468/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1047 - val_loss: 511.1530\n",
      "Epoch 469/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1046 - val_loss: 511.1528\n",
      "Epoch 470/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1047 - val_loss: 511.1525\n",
      "Epoch 471/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1047 - val_loss: 511.1525\n",
      "Epoch 472/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1048 - val_loss: 511.1524\n",
      "Epoch 473/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1048 - val_loss: 511.1525\n",
      "Epoch 474/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1048 - val_loss: 511.1526\n",
      "Epoch 475/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1048 - val_loss: 511.1527\n",
      "Epoch 476/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1048 - val_loss: 511.1524\n",
      "Epoch 477/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1048 - val_loss: 511.1522\n",
      "Epoch 478/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1048 - val_loss: 511.1519\n",
      "Epoch 479/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1049 - val_loss: 511.1518\n",
      "Epoch 480/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1049 - val_loss: 511.1516\n",
      "Epoch 481/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1049 - val_loss: 511.1516\n",
      "Epoch 482/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1050 - val_loss: 511.1518\n",
      "Epoch 483/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1050 - val_loss: 511.1518\n",
      "Epoch 484/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1050 - val_loss: 511.1518\n",
      "Epoch 485/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1050 - val_loss: 511.1517\n",
      "Epoch 486/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1050 - val_loss: 511.1515\n",
      "Epoch 487/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 25.1050 - val_loss: 511.1513\n",
      "Epoch 488/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.1050 - val_loss: 511.1511\n",
      "Epoch 489/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1051 - val_loss: 511.1511\n",
      "Epoch 490/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1051 - val_loss: 511.1511\n",
      "Epoch 491/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1051 - val_loss: 511.1511\n",
      "Epoch 492/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1051 - val_loss: 511.1510\n",
      "Epoch 493/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1051 - val_loss: 511.1510\n",
      "Epoch 494/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1051 - val_loss: 511.1509\n",
      "Epoch 495/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1051 - val_loss: 511.1508\n",
      "Epoch 496/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1051 - val_loss: 511.1506\n",
      "Epoch 497/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1051 - val_loss: 511.1506\n",
      "Epoch 498/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.1052 - val_loss: 511.1506\n",
      "Epoch 499/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1052 - val_loss: 511.1506\n",
      "Epoch 500/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1052 - val_loss: 511.1507\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6465, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0029, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0003, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0168, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3581, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 323ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.15888095e+01, 6.15602381e+01, 6.15316667e+01, 6.15030952e+01,\n",
       "        6.14745238e+01, 6.14459524e+01, 6.14173810e+01, 5.02780260e-01,\n",
       "        0.00000000e+00, 1.44994810e-01, 6.78518890e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.21194444e+01, 6.20606209e+01, 6.20017974e+01,\n",
       "        6.19429739e+01, 6.18841503e+01, 6.18253268e+01, 6.17665033e+01,\n",
       "        6.17076797e+01, 6.16488562e+01, 6.15951587e+01, 6.15665873e+01,\n",
       "        6.15380159e+01, 6.15094444e+01, 6.14808730e+01, 6.14523016e+01,\n",
       "        6.14237302e+01, 6.13951587e+01, 6.13665873e+01, 6.13380159e+01,\n",
       "        6.13094444e+01, 6.12808730e+01, 6.12396218e+01, 6.11639916e+01,\n",
       "        6.10883613e+01, 6.10127311e+01, 6.09371008e+01, 6.08614706e+01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.10394910e-01,\n",
       "        3.58253600e-02, 5.51191400e-01, 3.46674300e-01, 6.15157936e+01,\n",
       "        6.14872222e+01, 6.14586508e+01, 6.14300794e+01, 6.14015079e+01,\n",
       "        6.13729365e+01, 6.13443651e+01, 6.13157937e+01, 6.12872222e+01,\n",
       "        6.12564286e+01, 6.11807983e+01, 6.11051681e+01, 6.10295378e+01,\n",
       "        6.09539076e+01, 6.08782773e+01, 6.08026471e+01, 6.07270168e+01,\n",
       "        6.06513865e+01, 6.05757563e+01, 6.05001260e+01, 6.04244958e+01,\n",
       "        6.03543091e+01, 6.03156536e+01, 6.02769981e+01, 6.02383427e+01,\n",
       "        6.01996872e+01, 6.76675415e+01, 4.05724870e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.79979944e-01, 0.00000000e+00,\n",
       "        4.14460144e+01, 0.00000000e+00, 3.25977623e-01, 1.08057189e+00,\n",
       "        1.18194818e-01, 1.09603655e+00, 0.00000000e+00, 7.41210505e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.90133864e-01, 7.93430448e-01,\n",
       "        8.43455434e-01, 2.68061459e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.85567343e-03]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([54.37749539, 54.36875043, 54.36000547, 54.3512605 , 54.34251554,\n",
       "       54.33377058, 54.32502562, 54.31628066, 54.3075357 , 54.29879074,\n",
       "       54.29004577, 54.28130081, 54.27255585, 54.26381089, 54.25506593,\n",
       "       54.24632097, 54.23757601, 54.22883104, 54.22008608, 54.21134112,\n",
       "       54.20259616, 54.1938512 , 54.18510624, 54.17636128, 54.16761631,\n",
       "       54.15887135, 54.15012639, 54.14138143, 54.13263647, 54.12389151,\n",
       "       54.11514655, 54.10640159, 54.09765662, 54.08891166, 54.0801667 ,\n",
       "       54.07142174, 54.06267678, 54.05393182, 54.04518686, 54.03644189,\n",
       "       54.02769693, 54.01895197, 54.01020701, 54.00146205, 53.99271709,\n",
       "       53.98397213, 53.97522716, 53.9664822 , 53.95773724, 53.94899228,\n",
       "       53.94024732, 53.93150236, 53.9227574 , 53.91401243, 53.90526747,\n",
       "       53.89652251, 53.88777755, 53.87903259, 53.87028763, 53.86154267,\n",
       "       53.8527977 , 53.84405274, 53.83530778, 53.82656282, 53.81781786,\n",
       "       53.8090729 , 53.80032794, 53.79158297, 53.78283801, 53.77409305,\n",
       "       53.76534809, 53.75660313, 53.74785817, 53.73911321, 53.73036824,\n",
       "       53.72162328, 53.71287832, 53.70413336, 53.6953884 , 53.68664344,\n",
       "       53.67789848, 53.66915352, 53.66040855, 53.65166359, 53.64291863,\n",
       "       53.63417367, 53.62542871, 53.61668375, 53.60793879, 53.59919382,\n",
       "       53.59044886, 53.5817039 , 53.57295894, 53.56421398, 53.55546902,\n",
       "       53.54672406, 53.53797909, 53.52923413, 53.52048917, 53.51174421])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.56418618782538\n",
      "24.953880383444968\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
