{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.200000\n",
       "1       88.931092\n",
       "2       88.662185\n",
       "3       88.393277\n",
       "4       88.124370\n",
       "          ...    \n",
       "2495    52.234980\n",
       "2496    52.226235\n",
       "2497    52.217490\n",
       "2498    52.208745\n",
       "2499    52.200000\n",
       "Name: C1, Length: 2500, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c1_interpolated_2450_50.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.200000\n",
       "1       88.931092\n",
       "2       88.662185\n",
       "3       88.393277\n",
       "4       88.124370\n",
       "          ...    \n",
       "2445     0.000000\n",
       "2446     0.000000\n",
       "2447     0.000000\n",
       "2448     0.000000\n",
       "2449     0.046890\n",
       "Name: C1, Length: 2450, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(50)\n",
    "test\n",
    "training_set = training_set.head(2450)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.931092</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.662185</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.393277</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.124370</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     89.200000  0.000298\n",
       "1     88.931092  0.000298\n",
       "2     88.662185  0.000297\n",
       "3     88.393277  0.000297\n",
       "4     88.124370  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAAsTAAALEwEAmpwYAAAmxklEQVR4nO3deXhc1WH38e/RvliSJUuW5N3gfQm2cSAswYBJvCQNJJCF0oQU+rhNSUPStAWS522TrumbNwmhadpSQkuSvhBeTEpCCIkBG0JqbGTjDRsveJWtzbZkyZa1n/ePGY21jDT33tnujH6f5+HRLPfOPceDfvfo3HPPMdZaREQk9WUkuwAiIhIbCnQRkTShQBcRSRMKdBGRNKFAFxFJE1mJPFh5ebmdMWNGIg8pIpLytm3bdtpaWxFpu4QG+owZM6ipqUnkIUVEUp4x5piT7dTlIiKSJhToIiJpQoEuIpImFOgiImlCgS4ikiYU6CIiaUKBLiKSJlIi0J/bcZIfv+FoGKaIyJiVEoH+4p56/u21d5NdDBERX0uJQL9yeiknzl6ksbUj2UUREfGtlAj0ZdNLAdh+vDnJJRER8a+UCPSFk4rJycpg2zEFuojISFIi0HOzMnnP5BIFuojIKFIi0CHQj7775DmOn2lPdlFERHwpZQL99943nYKcLNb9qIb2rp5kF0dExHdSJtCnlhXwyJ1L2d/Qxl88swtrbbKLJCLiKykT6AAr5lTw56vm8vyuOu56bAuHm84nu0giIr6RUoEO8LkVl/N3H13E7pPnWP3wb3j4pQN09vQmu1giIkmXcoFujOGuq6fz8pdXsGpRFQ+/dJA13/0Nm989k+yiiYgkVcoFer+JRXn8051LeeKeq+jptdz572/w1z/fS3dvX7KLJiKSFCkb6P1WzKng11+6gc9eO4PHf3uEOx99gwZNESAiY1DKBzpAXnYmX/vIQh65cyl761r50COv88ZhdcGIyNiSFoHe7yNXTOK/77uO4vws7npsCy/tbUh2kUREEiatAh1gTmURz913HQsnFfOFp95iz8lzyS6SiEhCpF2gAxTlZfPYZ5ZTkp/NvU+8Sf059amLSPpzFOjGmC8ZY942xuwxxjxpjMkzxsw0xmwxxhwyxvzEGJMT78K6MbE4j8c/+17Od/Rw7xNvcqFT0wWISHqLGOjGmMnAF4Dl1tpFQCbwKeAfge9Ya2cBzcC98SyoF/Ori/ne7y5jX10r9z/1Fr19mi5ARNKX0y6XLCDfGJMFFAB1wM3AM8H3nwBui3npYuCmeRP5q99ZyEv7Gvn4v/4PP3nzOG0d3ckulohIzEUMdGvtSeD/AMcJBPk5YBvQYq3t78eoBSbHq5DRuvvaGfzNrQtpae/mgfW7ee/fvcT9T73Fqwea1GoXkbSRFWkDY0wpcCswE2gB/h+w2ukBjDHrgHUA06ZN81TIWPj0NTP4vfdNZ8eJFtZvr+XnO+t4bscpKotzuW3pZG5fNoU5lUVJK5+ISLRMpGlojTEfB1Zba+8NPv8McA3wcaDKWttjjLkG+Jq1dtVon7V8+XJbU1MTm5JHqbOnl1f2NbJ+ey0b9wda6osnl3D7ssl8ZMlkygp9dY1XRMYwY8w2a+3yiNs5CPSrgceB9wIXgf8EaoAbgPXW2qeMMf8K7LLWfn+0z/JToA90+nwnz+04xbPba3n7VCtZGYab5k3k9mVTuHneRHKy0nJ0p4ikiJgFevDDvg58EugB3gL+gECf+VNAWfC137PWdo72OX4N9IHeqW9l/bZa/nvHKZraOhlfkM1HrpjE7cum8J4pJRhjkl1EERljYhrosZIKgd6vp7eP3xw6zfpttfx6bwNdPX3MmjiOjy2bzEeXTqa6JD/ZRRSRMUKBHkPnLnbzwu461m+rpeZYM8bA9bPK+diyyaxaWEVBTsRryyIininQ4+To6Qs8+9ZJnt1eS23zRQpzMlm7uJqPLZvC1TPLyMhQl4yIxJYCPc76+ixbj57l2e21vLC7nvOdPUwpzefD75nEBxZUsnTqeIW7iMSEAj2BLnb18qu361m/vZbN756hp89SPi6XW+ZP5AMLKrluVjl52ZnJLqaIpCgFepKca+9m04FGfr23gVf3N3G+s4f87EzeP7ucDyyoZOX8So1xFxFXFOg+0NnTy5bDZ9mwt4GX9jVQd66DDAPLp5fxwYWVfPK9UynKy052MUXE5xToPmOtZc/JVjbsa2DD3gb21bUyoTCHL35gDne+dypZmbp5SUTCU6D73M4TLfzdL/ax9ehZLq8o5Ctr53PzvIm6cUlEhnEa6GoWJskVU8fzkz98H//26Svps3DvEzXc9dgW3j6lJfNExBsFehIZY1i1sIpff+kGvvY7C9hX18qH/+l1vvz0Ti2bJyKuqcvFR85d7Ob7Gw/xH789SkYGrHv/ZaxbcTnjcnUnqshYpi6XFFSSn81Da+fz8pdX8IEFVTzyyiFu/OYmntx6nJ7evmQXT0R8ToHuQ1PLCvinO5fy0z++lukTCnjo2d186JHX2bS/MdlFExEfU5eLz1lreXFPPd948R2OnWlnQXUxK+dP5Ma5E1kydTyZml5AJO1p2GKa6erp48mtx3l+1ym2HWumz0JpQTYr5lRw07yJrJhTwfgC3YEqko4U6Gmspb2L1w6eZtM7jWw60MTZC11kGFg6rZSb503kxrkVLKgu1ph2kTShQB8jevssu2pb2Li/iY3vNLL7ZGAce2VxLjfNnchN8yZy3axyjZQRSWEK9DGqsa2DV/c3sXF/I785cJq2zh6yMw1XzSxj5bxKVi2qYvJ4rbYkkkoU6EJ3bx81R5vZtL+RV95p5GDjeQCumFLCqkVVrFlUzczywiSXUkQiUaDLMEdOX+DFPfW8uKeOnbWBrpl5VUWsWljFmsVVzK0sUr+7RHTibDtTywqSXYyQsxe6yM3KoDCNuxUV6DKqky0X+dWeel7cU8+bx85iLcwsL2T1oipWL6ziPVNKFO4yzM92nuILT77Fj+69ivfPrnC0z9YjZykrzGbWxKK4lGnGg7+guiSPzQ+tdLR9S3sX//PuGdYuro5LeeLBaaCn7ylNRjV5fD73XD+Te66fSWNbBxv2NvDinnoefe0w/7LpXSaV5DG/upjJpflMKc1nSmkBk8cHHpcV5ijsx6gdx1sA2F/f5jjQP/FvmwE4+o0POdp+76lWvr/pEA9/conjaaXrXMx99Lkfb2fz4TNsfuhmqksiX0+y1vLA+l3cvmwKV182wfFxkkGBLkwsyuOuq6dz19XTaWnv4qV9jbzyTgNHTrez9ehZ2jp6Bm2fn50ZCvpAyBcEHgdfqxiXq8BPU33Bv+gz4vj9fv7J7RxuusAXb5ntqlX/9y/s4ytr50fc7kRzOwA9vc56J3r7LE/X1PLMtloO/4Ozk9KXn97J+u21jk9isaJAl0HGF+Rwx5VTuOPKKaHXzl3s5mTzRWqb2znZcpHaAY93nGihpb170GfkZmUwa+I45lYVMa+qiLlVxcyrKmJikYI+1dlQoMfzIIEfbv9fefS1w44C3YY+31VxXJ3E1m+vdbxtLCnQJaKS/GxK8rNZMKk47PvnO3sGBf6xM+0caGjj9YOneXb7ydB24wuymVPZH/KBn3Mqi7QMXwrpC6ZbRhwTPd5/Bbj9/ET8VRIrCnSJ2rjcLOYGQ3qo5gtdvFPfxoGGNt6pb2N/fSvPbj/J+c5L3TgzJhSwcn4laxdXsXRqaVzDQqLTH27x/IZCJ404HSRUB6ctdJct+mRSoEtclRbmcM3lE7jm8ksXk6y11DZfZH99G/sb2th2rJkfbT7GD14/QmVxLqsWVrF6URVXzSjTWqs+09/9EM+us0snjfgcw4ZOGO5a6Ap0kTCMMUwtK2BqWQG3LKgEoK2jm1feaeSXu+t5uuYEP9x8jLLCHD64oJI1i6u55rIJ5GQp3JPNJqD7Id4t4j63feguTwDJpEAXXyjKy+bWJZO5dclk2rt6eHV/Ey/sqefnO0/x1JsnKM7L4pYFlaxZVM37Z5eTl52Z7CKPSX3BdVbi2SsWOmnE6SBuT0rqQxeJQkFOFmsWV7NmcTUd3b28fvA0v9xTz4a99Ty7/SSFOZncOHcicyqLBoyTz6eqOE9dNHGWiO6HRPWhOw/0wM8UyHMFuvhbXnYmtyyo5JYFlXT1LGbz4TO8uKeOTfub+MXuukHbZmYYqorzLoX8kDHy1SX56raJUiL70OM3ysXd9jYBF4JjRYEuKSMnK4MVcypYMSdwh2JHdy915zoCwyWbA+PjA+Pk23nj3TPUtXYwcGYLY6CyKC/Uop9WVsD0CYXMKC9gWlkh5eN0B2wkieh+CLWI4/T5bqc7udSn7///NxTokrLysjOZWV444oyRXT191J/roLalPRD2odBv582jzfxs56lBrbXCnEymTyhk+oRg0Ad/TinNpyAnk5ysDLIzM8jJzEjLoZU9vX1kZphRg6s/3OLbsxU50a21EcrZ3zU0fBu301f1JeJmqhhRoEvaysnKYNqEAqZNCD8zYFdPH7XN7Rw7087RMxc4dqadY2cusL++jZf2NdA9yq3hmRmGnMwMsjMNOVmBkM8OBn52ZgY5WRkU52VRVZxHdUkelSWBn1XF+VSX5DG+INtXLb7a5nZWfutVrIWywhwmjMth4aRi1iyq5tpZE8jNClyE7u271ELffryZE2fbuXHuREryY3dz2KUWevh/n4tdvdzwzY1YC9/42OKw29z572+w40QL6264nC/dMpsTZy9ytr2LJVPHhwJ6oMbWDn5z8DQfXTp52Mm6z+Uol499/7eOtosHBbqMWTlZGVxWMY7LKsYNe6+3z3IqeNdrbXM7nT19dPf2hX529/bR1dNHd6+lq7eP7p6+wM/ePrp6Aq+du9jNgYYmmto6h/Xb5mZlBAK+JI/qknyqSvKoKg48ryjKpSQ/m+K8wB26iej3P9XSQWdPH2sXV1GQk8Xp8528sLuep2tqKcrNYuX8iaxeVM3ZC12hff7m+b28dbyF7EzDtZeXs3pRFdddXs7UsvwRWsaB+w8qi/NGrVO4wB2oub2LprZOAP7ox9vCbrPjRAsd3X088vJBTp/v5NiZC/z20Bm+/Ykrhn0X++vbWL+9lkdfO8zG/Y1855NLyM7M4EJnD4W5WSO29lvauyjMzSJ7yJ8r24MTmCWDAl0kjMyMS2Plo9XT20fT+U7qznVQf64j+PNi6PnWI2dpbOsY8S+C/OzMQMDnZ4WmYSjOy6Y4+LgkP5tJ4/OYUV7I9LJC8nPcD+ns6gmMR/zstTO5amYZAJ09vfz20Gl+ubueX+9t4L93nAptn5lh6O2zLKgu5v2zy/nlnnoeenY3AEV5WSycVMzCSSVcP6s8tM/G/Y3c85815GRmMLeqiEWTi1k6tZQ1i6sGTf8wMM8PNbaRm5U56HvoL+tf37qQ53YEFk0fKjszg0+9dxoFOZl8f9O7odf/9OmdA45jOd/Zw6qHXwu99vyuOvbWtfKHN1zGA+t3c8XU8Xz9IwuBS10uW4+c5WBjG3/7/D4udveGphJuvtBFdpIvujsKdGPMeOAxYBGBDq57gP3AT4AZwFHgE9ba4f+yImNcVmYG1SX5o07V2tdnOXOhi/pzHZw+38m5i920dnRzrj3482Lgv9aLPZxq6WDfxTZaO7qHzYQJgfVkp08oZOaEQqaXFzAjeF1gxoTCEReB6O4NhGR25qVWaG5WJjfPq+TmeZX8fW8f248189jrR9iwt4GKcbmhYz20dj4PrpnHO/VtvHW8hbdPnWPPqVZ+/Ebg7t9+/ZO43bZ0EqdaOnhhdz1Pbj3B13/+NrdfOYXPXDOdWROLQi10i+XTP9hKfWsHK+dV8tlrZ3DdrAmhspYW5PDje69m/l++GLY+OVkZ/MXqefz23TPsPNHC1LJ8Pn7lVL694UBou/6TQ7/v/e5Svvmr/TywPnBy2nmihbsf3wpcGrb4l8/t4Z36ttA+6364jZ/edy1/9dzbNLQ6n8Y3Hpy20L8LvGitvcMYkwMUAF8BXrbWfsMY8yDwIPBAnMopktYyMgwVRblUFOW62q+3z9J6sZuTLRc5cvoCx85c4OiZdo6evsDL7zRy+nznoO0rinKZEQz3aWUFTBiXS2lBdmh5wpG6QrIzM7j6sgn09Fk27G0Y1v1gjGF+dTHzqy9N4Nbd28cLu+u4/6kdFAz4q+G+m2YxfUIh1lp21p7jh5uP8tTWwN3B188qH3SSutDZw8zyQt463sxL+xqYNXFcqNWfk5VBfk7gwviR0xeAwLWA53fV0dXTR06wK2RqaT47T7SQm5XJF1bOZtuxZl490BS2nrfMr6S0IIe7HtsCwLobLuPR1w4H6hjs059ZXhgK9Huum8nzu07xhz/aRkd3Lw2tnWE/N1EiBroxpgS4AfgsgLW2C+gyxtwK3Bjc7AlgEwp0kYTKzDCUFuZQWpjDosklw94/39kTCPnT/Rd+A49fPdBEY9vw8Cl2MfNlpNEi2ZkZ3LpkMt975RCzK4dfpzDGsGTqeJZMXcJX1s7nJ2+e4MdvHBu23Yo5FTyweh6/2FXHE5uP8p//c3TEsj6zrZaHXzoIMOKF2pXzJ44Y6AB52ZdOaivmVJCTmcH3Nh6iMDdwUhp481pFUS7fv2sZd/xrYBGPnKyMYa3+RHLSQp8JNAH/YYy5AtgG3A9UWmv77+yoByrD7WyMWQesA5g2bVrUBRYR58blZrFwUgkLJw0P+47uXlrauzl7oYuW9q7QdQMnvCxdOdou5eNyue+mWXxuxeV87edv88PNg4M9LzuT26+cwu1XTqHu3EWOnm7n6mBf/0B9wSuez/zRNcOmew43RsVJPf5s1Vy+t/EQqxdVhX1/+Ywy5lcXs6+ulRVzKvhfH1rADd/cGPFz48FJoGcBy4A/sdZuMcZ8l0D3Soi11hpjwv7LWGsfBR6FwJqiUZZXRGIkLzuTqpJMqkryHO8zNBSdDr10mv8ZGYY5lcFpmO2lO1MHGno9YniZAiEbsUxh9h2q/31jLg1bHHgS6K/+wIEuIw2TTQQnl2RrgVpr7Zbg82cIBHyDMaYaIPizMT5FFJFUNjTzI02L63Z4fqRzxbD+fncf73mfZIgY6NbaeuCEMWZu8KWVwF7gZ8DdwdfuBp6LSwlFZExzMy96bLoAUiW+h3M6yuVPgP8KjnA5DPw+gZPB08aYe4FjwCfiU0QR8SMbo/hMlHAt/1jVoP+kk+xTgaNAt9buAJaHeWtlTEsjIinBbRDaEfrDY34gnIeqtWG6gxzs7OfTmOYSFRHnhvWHO9ll6Jh1Z4eKXet56Asu2tEDNg13YTfZLfKhFOgi4ivRrCXqZjRl/1QFw44f5vB+mkhtNAp0EfHEw1B0T+KVpS/ta4jZZ/kl7xXoIuKalzD3cjOSFyO1psO1/IcWKZq/DgLHjmr3qCnQRcQx7/3h3sLcyV4Rbw6KImTDBvyQVbD8RIEuInHlNfScNujtoMeRd/KawakwTFOBLiK+MvzO0gQf3+FrfqRAFxFP3HaJJ6p9O1L4hr+xKLb3lkbbBx8tBbqIuHYpCOMbYLG4kOqmhE5uNBp4Ekh2gA+lQBcRx2LRHx7rC6kDPy6eA2kSNUwzGgp0EfGVcNPhjsZtzkb6vHDDHv02mmUkCnQR8cR1g9WPNyLFbH4BE/bY/eufJooCXUR8KxZ56+a2/WF/HUT87NHfn/3VXzo+diwo0EXEvWDSeploK1XmRRmJn/vSFegi4pin1X48Brin6QUcbON1ZIqPczxEgS4inrgdUuh41MqwoYPuAjjaFY7C31gUYdk8x0eMLwW6iPhWLLs3nJwYou0OSnZvkgJdRFyL5rYiP9zK73jfMAmtPnQRSQuJ7A/383H8SoEuInHlfXbD6PYfSeQbi8K/PtrJItldLf0U6CKSEE5bz1HPjzLC7uFedtyid1ikZM/tokAXkYRy05qNyWyI3taEDjyPNDmXX5rmQQp0EXHNuryxKFFSYRGKeFKgi4hj3gPcfdCGxrlH6vP2UJrRPy/8J452svDLeU2BLiKeOO4TD6ad40h3mY5DPzfSAheDptv1MEVvbDaMDwW6iPhWLIYhurlQ6Xb5O7+0zPsp0EXEMy+jOuI6EiQBXeh+HuuuQBcR18b6xceh/DLaRYEuIo55vknIelhUOnRN1OXkXC5uHApXphH3T4FzmAJdRDzxOntixO0HHSN6oYuhHs5GkfZ12+cebwp0EfEsmpCMh2hPAE66TvzcUFegi4ikCQW6iLjmuj8c7xdS3XfZJL7jI9ldLf0U6CLiWKQLimH3iSbuYrnARbAcA8sT+ePdbJv80S4KdBFJqIg360QRiuGWxYtFxFoLj752mMa2jkEnMr+0zPs5DnRjTKYx5i1jzPPB5zONMVuMMYeMMT8xxuTEr5gi4kc+GX4dd509fQA88MyuJJdkdG5a6PcD+wY8/0fgO9baWUAzcG8sCyYi/pXIkR5uzxnRnmRG27+7N3zN/XJicxToxpgpwIeAx4LPDXAz8ExwkyeA2+JQPhHxFa9L0FlPNxbF8o7UsHObx/g+/mTnutMW+sPAXwB9wecTgBZrbU/weS0wOdyOxph1xpgaY0xNU1NTNGUVER9xvNjPsFUjImzvpTAxPP6o+w57P9kRPljEQDfGfBhotNZu83IAa+2j1trl1trlFRUVXj5CRHwq2UuuDZWYibP8e2tRloNtrgM+YoxZC+QBxcB3gfHGmKxgK30KcDJ+xRQRcSYZpxi/nNgittCttQ9Za6dYa2cAnwJesdbeBWwE7ghudjfwXNxKKSK+4qXv2Wu7Nla9Gk4XiY4mnJPdAxPNOPQHgD81xhwi0Kf+g9gUSUT8avCNRc4jeuCWTgPT4v5CajhRBXSk9/3RMA9x0uUSYq3dBGwKPj4MXBX7IolIOknksENHd3N6//jAMfzbha47RUUkCj5roUKSRp745N9BgS4iieGxaZvoC45+60ZxQ4EuIq5Z3F3k9NpNEcsFLrwkdaTWfhRD3ONCgS4ijnkKrCGh6DRXvZwEnOzjt5uBYkmBLiKexSMa/Z634c4ZfimyAl1EEiJR49CjDVe/n1BGo0AXEfcSNHQvlpNnOb2xKNI+o26vBS5EJFUMCiwXWTv4xiL3+zg/TpgFLuKZsT5rzivQRcQzJy1S163cOPdIR39jUYJPGi4o0EUkvfgkXJNBgS4iCeFpGCKxy+ewC1yE66KJ49wv8aZAFxHXbBRrCcXzwmH4k0b8Zk9MdoAPpUAXEce8Bpin6XZ9OguWP0sVoEAXEc+cBLzrceTRLvIc588Pf0x/tNUV6CKSEJ5b3AnuovHLiBUvFOgiklBu8jImC1wY98cN7Rthr2gWoI4HBbqIuGatf/u4xzIFuog4NrRF6qR7wvOFVI/7RZzyNspWtJ+7aRToIpIQngM6pqUYzM8zJ3qhQBeRtHZpfQsvC1xE+mwz6vNEU6CLiCeJmg7XjbHer69AFxHX3OZmvFYfCife/dlhu2l80k+jQBcRx4Z3MTjYJ5h2TgM63nOKJ7tbJJ4U6CLiU4EzQKzyPfwCF7GdCjfZLXUFuogkVDxbyG67Q6LtcU92gA+lQBcR1yyxuYszHuKdsWFb9T7pxlGgi4hjw28schZkXibb9esJw88U6CISV/2R7zSfh24f69bvoGVRYzx/erIp0EUkseI6Dj3c4QIHDP/HxOinGbcLXCT7VKBAFxFPvK9ZFF/xHvYY/pgJP2RYCnQRcW2s35HpVwp0EfHMacPUy3S7/ZvHd6qA4a/5pbXthQJdROLK7ZS7/e97GhkTZp9LC1wMP3CkI7hd4CLZJwMFuohImlCgi4gnfu1Gj/+NRXE+QBQiBroxZqoxZqMxZq8x5m1jzP3B18uMMRuMMQeDP0vjX1wR8YNQpjlMT7+GoF/L5ZWTFnoP8GVr7QLgfcB9xpgFwIPAy9ba2cDLweciksa89BF7maERBlwUdXGs8OPQBz8YOKwxbJ/7wMcu65vsKQAiBrq1ts5auz34uA3YB0wGbgWeCG72BHBbnMooImNIfyimQus52QE+lKs+dGPMDGApsAWotNbWBd+qBypjWzQR8TO/Bm4yRpok42amcBwHujFmHLAe+KK1tnXgezYwwDT8rAjGrDPG1BhjapqamqIqrIj4w6XuEOdB5scTgKeVlHx6hyw4DHRjTDaBMP8va+2zwZcbjDHVwfergcZw+1prH7XWLrfWLq+oqIhFmUUkSbx2MUQzpjzaxm9/6zn8Ahcjb+/l2MluqDsZ5WKAHwD7rLXfHvDWz4C7g4/vBp6LffFEJOW5nHI3dGORp9az6+JExx89LSFZDra5Dvg0sNsYsyP42leAbwBPG2PuBY4Bn4hLCUVEXEnC5FwJP2J4EQPdWvs6I5d3ZWyLIyLpyo99z+m28IbuFBURD2K7gPNo4jmCZNRx6ylIgS4ijnnN1oHB6fbGIq/HGSoWN0UNfz/6Y8SSAl1E4sptxkWbiU5nc4ylZAd5PwW6iHgyVhe58HO1Fegi4lm086wkmx/LFA0Fuoi45jYIB27utHvC26gYdwtchDOwfJG7b/y1TLQCXUQc83RhMQYXUl0dL+L7sQ9dn3ShK9BFxJt49VYk8gKjH8fGR0OBLiIJ4cfoHJOTc4mIhBOPsd39+mPTzTHC3yhkXH3OwPK5HnKpcegikmpct1E93STkrSWcyFDtP1TKzYcuIuLlgqL7feIbjgOz17+dJ94o0EXEE9dDF9MkPf1cDwW6iHjmqcXueBy6+2OEX7BitO1HHrceaV8/UqCLiGt+bqWOdAKI5xwuob702B/CFQW6iDjm+SYhn8477uPzkicKdBHxxGlIh5aUc7l9vCS7FR1PCnQR8Sy+4et9YWnH20fcfHgF/dyqV6CLiK9Fe9K41L8dhzlchi2AHfNDuKJAFxHX/Hz7u7tQjU09kh3k/RToIuKY19yK9XJy0fBL+MaDAl1EPHEauKGLok637/981yVyz9NJw79/nCjQRcS7RMyPHu2qSKGx4k4n53KzwMWQ0sWjn94NBbqIpBVXJ4CkHDV+FOgi4lo0S9DF6xiiQBcRF7zMVOi2GyL+U9Fe+nwtcCEiEhK/6XT7J85ytcBFuOO5PEEM3N4fHSnOKdBFJK24CXCvi2hcOlb4n8miQBcR1/zb6TC2KdBFxAVvTVBrrevWsE4a7inQRcQTtzcWjfR82PYuP3+g8ItEj3z8SIcI133j59E3CnQR8SwRfcaxulnHaTh7OZoWuBARkZhSoIuIa15Gh/i5qyJdKNBFxLHBvRbOE3rglk67JTwtW+dgn4HHj7R9uLL6+bykQBcRz+LRZxw6adghz93uH3ph0I/BRpnMK5yDjW1DtjWD9on/Xa6jiyrQjTGrjTH7jTGHjDEPxqpQIuJv9z+1g9Pnuxxv/9bxFr614YCrY/z5M7scbXfk9IXQ42e3n3R1jN99bIur7RtaO9l2rHnY6+1dvSPu83+3HKe7t8/VcbzyHOjGmEzgn4E1wALgTmPMglgVTET8x0swHTvT7ulYJ1suAu773mubLw563nwhcOLZfPgMAFuPnHX8WV0R6tvZHQjy3xw8DUBDa8ewbb7y091875VDUd+V6kQ0LfSrgEPW2sPW2i7gKeDW2BRLRPyofFyu632Onx0c6JkZ7rol9tW1uj7mQHXnAiHb1RMI54oi53WYUpo/6vtbhpwcunvDh/Z3Xz4YKkc8RRPok4ETA57XBl8bxBizzhhTY4ypaWpqiuJwIpJs5eNy+era+cyYUMCH31PNbUuH/coP81e/c+kP97++dWHEfual00q5aW5F6Pkf33T5qNs//MklocezJ47jb29bNOj9v//oYnIyM/iXu5YB8Afvv4yh55Sq4jzuunoaAJVFeVw/q5yPLZ1MQU4WAH/54Ut1WFBdzI3B8j24Zh4A37zjPQB87sbLQscc6DPXTE/MmH2vfwYYY+4AVltr/yD4/NPA1dbaz4+0z/Lly21NTY2n44mIjFXGmG3W2uWRtoumhX4SmDrg+ZTgayIikgTRBPqbwGxjzExjTA7wKeBnsSmWiIi4leV1R2ttjzHm88CvgEzgcWvt2zErmYiIuOI50AGstS8AL8SoLCIiEgXdKSoikiYU6CIiaUKBLiKSJhToIiJpwvONRZ4OZkwTcMzj7uXA6RgWJ1Wo3mPLWK03jN26O6n3dGttRYRtEhvo0TDG1Di5UyrdqN5jy1itN4zdusey3upyERFJEwp0EZE0kUqB/miyC5AkqvfYMlbrDWO37jGrd8r0oYuIyOhSqYUuIiKjUKCLiKSJlAj0dF+M2hhz1Biz2xizwxhTE3ytzBizwRhzMPizNPi6McY8Evy32GWMWZbc0jtnjHncGNNojNkz4DXX9TTG3B3c/qAx5u5k1MWNEer9NWPMyeB3vsMYs3bAew8F673fGLNqwOsp9XtgjJlqjNlojNlrjHnbGHN/8PW0/s5HqXf8v3Nrra//IzA177vAZUAOsBNYkOxyxbiOR4HyIa/9b+DB4OMHgX8MPl4L/BIwwPuALckuv4t63gAsA/Z4rSdQBhwO/iwNPi5Ndt081PtrwJ+F2XZB8P/xXGBm8P/9zFT8PQCqgWXBx0XAgWD90vo7H6Xecf/OU6GFPlYXo74VeCL4+AngtgGv/9AGvAGMN8ZUJ6F8rllrXwOGLrnutp6rgA3W2rPW2mZgA7A67oWPwgj1HsmtwFPW2k5r7RHgEIHfgZT7PbDW1llrtwcftwH7CKw7nNbf+Sj1HknMvvNUCHRHi1GnOAv82hizzRizLvhapbW2Lvi4HqgMPk63fw+39Uyn+n8+2LXweH+3A2lab2PMDGApsIUx9J0PqTfE+TtPhUAfC6631i4D1gD3GWNuGPimDfxdlvbjS8dKPYP+BbgcWALUAd9KamniyBgzDlgPfNFa2zrwvXT+zsPUO+7feSoEetovRm2tPRn82Qj8lMCfWg39XSnBn43BzdPt38NtPdOi/tbaBmttr7W2D/h3At85pFm9jTHZBELtv6y1zwZfTvvvPFy9E/Gdp0Kgp/Vi1MaYQmNMUf9j4IPAHgJ17L+afzfwXPDxz4DPBEcEvA84N+DP11Tktp6/Aj5ojCkN/sn6weBrKWXIdY+PEvjOIVDvTxljco0xM4HZwFZS8PfAGGOAHwD7rLXfHvBWWn/nI9U7Id95sq8IO7xqvJbAleJ3ga8muzwxrttlBK5e7wTe7q8fMAF4GTgIvASUBV83wD8H/y12A8uTXQcXdX2SwJ+a3QT6A+/1Uk/gHgIXjg4Bv5/senms94+C9doV/CWtHrD9V4P13g+sGfB6Sv0eANcT6E7ZBewI/rc23b/zUeod9+9ct/6LiKSJVOhyERERBxToIiJpQoEuIpImFOgiImlCgS4ikiYU6CIiaUKBLiKSJv4/0nLxWZQNCIYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6465  # Value for C0\n",
    "K0 = -0.0029  # Value for K0\n",
    "K1 = -0.0003  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0168    # Value for b\n",
    "c = 2.3581    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-175)  var1(t-174)  var1(t-173)  var1(t-172)  var1(t-171)  \\\n",
      "175    89.200000    88.931092    88.662185    88.393277    88.124370   \n",
      "176    88.931092    88.662185    88.393277    88.124370    87.855462   \n",
      "177    88.662185    88.393277    88.124370    87.855462    87.586555   \n",
      "178    88.393277    88.124370    87.855462    87.586555    87.317647   \n",
      "179    88.124370    87.855462    87.586555    87.317647    87.048739   \n",
      "\n",
      "     var1(t-170)  var1(t-169)  var1(t-168)  var1(t-167)  var1(t-166)  ...  \\\n",
      "175    87.855462    87.586555    87.317647    87.048739    86.794538  ...   \n",
      "176    87.586555    87.317647    87.048739    86.794538    86.721709  ...   \n",
      "177    87.317647    87.048739    86.794538    86.721709    86.648880  ...   \n",
      "178    87.048739    86.794538    86.721709    86.648880    86.576050  ...   \n",
      "179    86.794538    86.721709    86.648880    86.576050    86.503221  ...   \n",
      "\n",
      "     var1(t+45)  var2(t+45)  var1(t+46)  var2(t+46)  var1(t+47)  var2(t+47)  \\\n",
      "175   78.583777    0.000280   78.562302    0.000280   78.540826    0.000280   \n",
      "176   78.562302    0.000280   78.540826    0.000280   78.519351    0.000280   \n",
      "177   78.540826    0.000280   78.519351    0.000280   78.497876    0.000279   \n",
      "178   78.519351    0.000280   78.497876    0.000279   78.476401    0.000279   \n",
      "179   78.497876    0.000279   78.476401    0.000279   78.454925    0.000279   \n",
      "\n",
      "     var1(t+48)  var2(t+48)  var1(t+49)  var2(t+49)  \n",
      "175   78.519351    0.000280   78.497876    0.000279  \n",
      "176   78.497876    0.000279   78.476401    0.000279  \n",
      "177   78.476401    0.000279   78.454925    0.000279  \n",
      "178   78.454925    0.000279   78.433450    0.000279  \n",
      "179   78.433450    0.000279   78.411975    0.000279  \n",
      "\n",
      "[5 rows x 276 columns]\n",
      "Index(['var1(t-175)', 'var1(t-174)', 'var1(t-173)', 'var1(t-172)',\n",
      "       'var1(t-171)', 'var1(t-170)', 'var1(t-169)', 'var1(t-168)',\n",
      "       'var1(t-167)', 'var1(t-166)',\n",
      "       ...\n",
      "       'var1(t+45)', 'var2(t+45)', 'var1(t+46)', 'var2(t+46)', 'var1(t+47)',\n",
      "       'var2(t+47)', 'var1(t+48)', 'var2(t+48)', 'var1(t+49)', 'var2(t+49)'],\n",
      "      dtype='object', length=276)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 175, n_out = 50)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 176):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-150:]\n",
    "trainX = train[:,:-150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-150:]\n",
    "forecastX = forecast[:,:-150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225, 1, 126) (2225, 150) (1, 1, 126)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "28/28 [==============================] - 2s 19ms/step - loss: 4364.5957 - val_loss: 2532.7007\n",
      "Epoch 2/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 4164.0156 - val_loss: 2373.4609\n",
      "Epoch 3/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3967.9651 - val_loss: 2280.8306\n",
      "Epoch 4/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3859.2769 - val_loss: 2221.7009\n",
      "Epoch 5/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 3751.7634 - val_loss: 2156.3752\n",
      "Epoch 6/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3655.7480 - val_loss: 2101.1191\n",
      "Epoch 7/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 3565.5090 - val_loss: 2047.5573\n",
      "Epoch 8/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3464.7078 - val_loss: 1987.6932\n",
      "Epoch 9/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3369.4507 - val_loss: 1931.9950\n",
      "Epoch 10/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3275.8459 - val_loss: 1879.4387\n",
      "Epoch 11/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3186.7703 - val_loss: 1829.6025\n",
      "Epoch 12/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3101.1265 - val_loss: 1781.8928\n",
      "Epoch 13/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 3018.2092 - val_loss: 1736.0082\n",
      "Epoch 14/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2937.6455 - val_loss: 1691.7723\n",
      "Epoch 15/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2859.2085 - val_loss: 1649.0707\n",
      "Epoch 16/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2782.7427 - val_loss: 1607.8096\n",
      "Epoch 17/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2708.1338 - val_loss: 1567.9230\n",
      "Epoch 18/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2635.2937 - val_loss: 1529.3571\n",
      "Epoch 19/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2564.1509 - val_loss: 1492.0654\n",
      "Epoch 20/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2494.6433 - val_loss: 1456.0084\n",
      "Epoch 21/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2426.7197 - val_loss: 1421.1497\n",
      "Epoch 22/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2360.3357 - val_loss: 1387.4576\n",
      "Epoch 23/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2295.4507 - val_loss: 1354.9019\n",
      "Epoch 24/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2232.0266 - val_loss: 1323.4553\n",
      "Epoch 25/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2170.0310 - val_loss: 1293.0922\n",
      "Epoch 26/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2109.4312 - val_loss: 1263.7885\n",
      "Epoch 27/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 2050.1987 - val_loss: 1235.5211\n",
      "Epoch 28/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1992.3058 - val_loss: 1208.2673\n",
      "Epoch 29/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1935.7260 - val_loss: 1182.0067\n",
      "Epoch 30/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1880.4346 - val_loss: 1156.7180\n",
      "Epoch 31/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1826.4076 - val_loss: 1132.3817\n",
      "Epoch 32/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1773.6216 - val_loss: 1108.9786\n",
      "Epoch 33/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1722.0551 - val_loss: 1086.4896\n",
      "Epoch 34/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1671.6857 - val_loss: 1064.8965\n",
      "Epoch 35/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1622.4927 - val_loss: 1044.1810\n",
      "Epoch 36/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 1574.4562 - val_loss: 1024.3259\n",
      "Epoch 37/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 1527.5560 - val_loss: 1005.3136\n",
      "Epoch 38/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1481.7726 - val_loss: 987.1271\n",
      "Epoch 39/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1437.0870 - val_loss: 969.7501\n",
      "Epoch 40/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1393.4813 - val_loss: 953.1657\n",
      "Epoch 41/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1350.9360 - val_loss: 937.3579\n",
      "Epoch 42/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 1309.4343 - val_loss: 922.3105\n",
      "Epoch 43/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1268.9581 - val_loss: 908.0078\n",
      "Epoch 44/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1229.4904 - val_loss: 894.4340\n",
      "Epoch 45/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1191.0134 - val_loss: 881.5737\n",
      "Epoch 46/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1153.5110 - val_loss: 869.4119\n",
      "Epoch 47/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1116.9666 - val_loss: 857.9332\n",
      "Epoch 48/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1081.3635 - val_loss: 847.1227\n",
      "Epoch 49/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1046.6855 - val_loss: 836.9655\n",
      "Epoch 50/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1012.9172 - val_loss: 827.4469\n",
      "Epoch 51/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 980.0424 - val_loss: 818.5524\n",
      "Epoch 52/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 948.0452 - val_loss: 810.2678\n",
      "Epoch 53/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 916.9108 - val_loss: 802.5784\n",
      "Epoch 54/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 886.6237 - val_loss: 795.4703\n",
      "Epoch 55/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 857.1688 - val_loss: 788.9292\n",
      "Epoch 56/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 828.5314 - val_loss: 782.9412\n",
      "Epoch 57/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 800.6965 - val_loss: 777.4924\n",
      "Epoch 58/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 773.6495 - val_loss: 772.5690\n",
      "Epoch 59/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 747.3756 - val_loss: 768.1573\n",
      "Epoch 60/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 721.8608 - val_loss: 764.2439\n",
      "Epoch 61/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 697.0905 - val_loss: 760.8152\n",
      "Epoch 62/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 673.0510 - val_loss: 757.8578\n",
      "Epoch 63/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 649.7281 - val_loss: 755.3582\n",
      "Epoch 64/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 627.1077 - val_loss: 753.3036\n",
      "Epoch 65/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 605.1763 - val_loss: 751.6805\n",
      "Epoch 66/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 583.9202 - val_loss: 750.4764\n",
      "Epoch 67/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 563.3260 - val_loss: 749.6777\n",
      "Epoch 68/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 543.3799 - val_loss: 749.2720\n",
      "Epoch 69/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 524.0689 - val_loss: 749.2465\n",
      "Epoch 70/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 505.3796 - val_loss: 749.5883\n",
      "Epoch 71/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 487.2991 - val_loss: 750.2851\n",
      "Epoch 72/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 469.8141 - val_loss: 751.3245\n",
      "Epoch 73/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 452.9122 - val_loss: 752.6935\n",
      "Epoch 74/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 436.5802 - val_loss: 754.3805\n",
      "Epoch 75/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 420.8055 - val_loss: 756.3728\n",
      "Epoch 76/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 405.5758 - val_loss: 758.6587\n",
      "Epoch 77/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 390.8782 - val_loss: 761.2258\n",
      "Epoch 78/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 376.7007 - val_loss: 764.0626\n",
      "Epoch 79/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 363.0310 - val_loss: 767.1569\n",
      "Epoch 80/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 349.8568 - val_loss: 770.4971\n",
      "Epoch 81/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 337.1663 - val_loss: 774.0717\n",
      "Epoch 82/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 324.9474 - val_loss: 777.8690\n",
      "Epoch 83/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 313.1882 - val_loss: 781.8779\n",
      "Epoch 84/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 301.8772 - val_loss: 786.0871\n",
      "Epoch 85/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 291.0027 - val_loss: 790.4851\n",
      "Epoch 86/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 280.5534 - val_loss: 795.0611\n",
      "Epoch 87/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 270.5178 - val_loss: 799.8040\n",
      "Epoch 88/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 260.8846 - val_loss: 804.7031\n",
      "Epoch 89/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 251.6430 - val_loss: 809.7475\n",
      "Epoch 90/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 242.7816 - val_loss: 814.9271\n",
      "Epoch 91/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 234.2898 - val_loss: 820.2306\n",
      "Epoch 92/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 226.1569 - val_loss: 825.6482\n",
      "Epoch 93/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 218.3722 - val_loss: 831.1694\n",
      "Epoch 94/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 210.9252 - val_loss: 836.7841\n",
      "Epoch 95/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 203.8056 - val_loss: 842.4822\n",
      "Epoch 96/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 197.0033 - val_loss: 848.2531\n",
      "Epoch 97/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 190.5083 - val_loss: 854.0875\n",
      "Epoch 98/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 184.3105 - val_loss: 859.9743\n",
      "Epoch 99/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 178.4002 - val_loss: 865.9040\n",
      "Epoch 100/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 172.7679 - val_loss: 871.8658\n",
      "Epoch 101/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 167.4040 - val_loss: 877.8488\n",
      "Epoch 102/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 162.2993 - val_loss: 883.8409\n",
      "Epoch 103/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 157.4446 - val_loss: 889.8291\n",
      "Epoch 104/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 152.8310 - val_loss: 895.7977\n",
      "Epoch 105/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 148.4495 - val_loss: 901.7266\n",
      "Epoch 106/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 144.2917 - val_loss: 907.5855\n",
      "Epoch 107/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 140.3489 - val_loss: 913.3214\n",
      "Epoch 108/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 136.6128 - val_loss: 918.8151\n",
      "Epoch 109/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 133.0754 - val_loss: 923.4933\n",
      "Epoch 110/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 129.6719 - val_loss: 910.8503\n",
      "Epoch 111/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 128.6668 - val_loss: 946.4035\n",
      "Epoch 112/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 123.5597 - val_loss: 952.4554\n",
      "Epoch 113/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 120.7397 - val_loss: 958.4548\n",
      "Epoch 114/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 118.0807 - val_loss: 964.3983\n",
      "Epoch 115/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 115.5750 - val_loss: 970.2804\n",
      "Epoch 116/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 113.2157 - val_loss: 976.0861\n",
      "Epoch 117/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 110.9895 - val_loss: 981.7086\n",
      "Epoch 118/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 108.8963 - val_loss: 987.2015\n",
      "Epoch 119/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 106.9432 - val_loss: 992.8607\n",
      "Epoch 120/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 105.0908 - val_loss: 998.1522\n",
      "Epoch 121/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 103.3548 - val_loss: 1003.3163\n",
      "Epoch 122/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 101.7164 - val_loss: 1007.4006\n",
      "Epoch 123/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 100.7874 - val_loss: 1014.4460\n",
      "Epoch 124/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 98.8860 - val_loss: 1020.9735\n",
      "Epoch 125/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 97.2437 - val_loss: 1027.0870\n",
      "Epoch 126/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 95.7928 - val_loss: 1032.8872\n",
      "Epoch 127/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 94.4921 - val_loss: 1038.5223\n",
      "Epoch 128/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 93.3173 - val_loss: 1043.8672\n",
      "Epoch 129/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 92.2519 - val_loss: 1049.0076\n",
      "Epoch 130/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 91.2832 - val_loss: 1053.9578\n",
      "Epoch 131/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 90.4011 - val_loss: 1058.7279\n",
      "Epoch 132/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 89.5971 - val_loss: 1063.3259\n",
      "Epoch 133/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 88.8640 - val_loss: 1067.7579\n",
      "Epoch 134/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 88.1954 - val_loss: 1072.0297\n",
      "Epoch 135/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 87.5857 - val_loss: 1076.1458\n",
      "Epoch 136/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 87.0298 - val_loss: 1080.1102\n",
      "Epoch 137/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 86.5230 - val_loss: 1083.9281\n",
      "Epoch 138/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 86.0613 - val_loss: 1087.6014\n",
      "Epoch 139/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6408 - val_loss: 1091.1350\n",
      "Epoch 140/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.2581 - val_loss: 1094.5314\n",
      "Epoch 141/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 84.9100 - val_loss: 1097.7938\n",
      "Epoch 142/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 84.5936 - val_loss: 1100.9263\n",
      "Epoch 143/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 84.3062 - val_loss: 1103.9315\n",
      "Epoch 144/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 84.0452 - val_loss: 1106.8126\n",
      "Epoch 145/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 83.8085 - val_loss: 1109.5730\n",
      "Epoch 146/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.5939 - val_loss: 1112.2162\n",
      "Epoch 147/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.3995 - val_loss: 1114.7448\n",
      "Epoch 148/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 83.2236 - val_loss: 1117.1621\n",
      "Epoch 149/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 83.0645 - val_loss: 1119.4727\n",
      "Epoch 150/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.9206 - val_loss: 1121.6776\n",
      "Epoch 151/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.7907 - val_loss: 1123.7797\n",
      "Epoch 152/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.6735 - val_loss: 1125.7845\n",
      "Epoch 153/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.5678 - val_loss: 1127.6938\n",
      "Epoch 154/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.4726 - val_loss: 1129.5106\n",
      "Epoch 155/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.3868 - val_loss: 1131.2382\n",
      "Epoch 156/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 82.3097 - val_loss: 1132.8795\n",
      "Epoch 157/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 82.2403 - val_loss: 1134.4371\n",
      "Epoch 158/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.1781 - val_loss: 1135.9152\n",
      "Epoch 159/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 82.1221 - val_loss: 1137.3163\n",
      "Epoch 160/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 82.0719 - val_loss: 1138.6429\n",
      "Epoch 161/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.0269 - val_loss: 1139.8981\n",
      "Epoch 162/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9867 - val_loss: 1141.0851\n",
      "Epoch 163/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.9506 - val_loss: 1142.2051\n",
      "Epoch 164/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.9184 - val_loss: 1143.2622\n",
      "Epoch 165/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8895 - val_loss: 1144.2588\n",
      "Epoch 166/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8637 - val_loss: 1145.1970\n",
      "Epoch 167/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.8407 - val_loss: 1146.0795\n",
      "Epoch 168/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.8203 - val_loss: 1146.9094\n",
      "Epoch 169/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.8020 - val_loss: 1147.6880\n",
      "Epoch 170/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7857 - val_loss: 1148.4186\n",
      "Epoch 171/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7712 - val_loss: 1149.1018\n",
      "Epoch 172/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7584 - val_loss: 1149.7405\n",
      "Epoch 173/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7470 - val_loss: 1150.3367\n",
      "Epoch 174/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7369 - val_loss: 1150.8921\n",
      "Epoch 175/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7280 - val_loss: 1151.4087\n",
      "Epoch 176/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7201 - val_loss: 1151.8882\n",
      "Epoch 177/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7132 - val_loss: 1152.3319\n",
      "Epoch 178/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7071 - val_loss: 1152.7404\n",
      "Epoch 179/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7017 - val_loss: 1153.1151\n",
      "Epoch 180/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.6970 - val_loss: 1153.4574\n",
      "Epoch 181/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.6929 - val_loss: 1153.7670\n",
      "Epoch 182/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 81.6894 - val_loss: 1154.0436\n",
      "Epoch 183/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.6863 - val_loss: 1154.2872\n",
      "Epoch 184/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.6837 - val_loss: 1154.4941\n",
      "Epoch 185/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.6815 - val_loss: 1154.6609\n",
      "Epoch 186/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.6796 - val_loss: 1154.7767\n",
      "Epoch 187/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.6780 - val_loss: 1154.8214\n",
      "Epoch 188/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.6766 - val_loss: 1154.7393\n",
      "Epoch 189/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.6755 - val_loss: 1154.3130\n",
      "Epoch 190/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.6638 - val_loss: 1148.0422\n",
      "Epoch 191/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.5951 - val_loss: 1140.4102\n",
      "Epoch 192/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 84.6392 - val_loss: 1156.1279\n",
      "Epoch 193/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 74.2812 - val_loss: 1088.1273\n",
      "Epoch 194/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6862 - val_loss: 1092.8329\n",
      "Epoch 195/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.2034 - val_loss: 1097.7690\n",
      "Epoch 196/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 84.7193 - val_loss: 1102.4031\n",
      "Epoch 197/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 84.2990 - val_loss: 1106.7214\n",
      "Epoch 198/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.9375 - val_loss: 1110.7389\n",
      "Epoch 199/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.6266 - val_loss: 1114.4729\n",
      "Epoch 200/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 83.3595 - val_loss: 1117.9392\n",
      "Epoch 201/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 83.1300 - val_loss: 1121.1544\n",
      "Epoch 202/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.9326 - val_loss: 1124.1322\n",
      "Epoch 203/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.7631 - val_loss: 1126.8870\n",
      "Epoch 204/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 82.6175 - val_loss: 1129.4332\n",
      "Epoch 205/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 82.4922 - val_loss: 1131.7819\n",
      "Epoch 206/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.3846 - val_loss: 1133.9431\n",
      "Epoch 207/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.2920 - val_loss: 1135.9215\n",
      "Epoch 208/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 82.2124 - val_loss: 1137.6793\n",
      "Epoch 209/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 82.1420 - val_loss: 1119.4667\n",
      "Epoch 210/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 83.8116 - val_loss: 1142.1313\n",
      "Epoch 211/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.0302 - val_loss: 1143.5823\n",
      "Epoch 212/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9861 - val_loss: 1144.8998\n",
      "Epoch 213/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.9487 - val_loss: 1146.0731\n",
      "Epoch 214/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.9164 - val_loss: 1146.0573\n",
      "Epoch 215/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.5753 - val_loss: 1089.9662\n",
      "Epoch 216/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 86.7470 - val_loss: 1084.0774\n",
      "Epoch 217/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 86.2086 - val_loss: 1090.0448\n",
      "Epoch 218/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.5439 - val_loss: 1095.6603\n",
      "Epoch 219/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 84.9692 - val_loss: 1100.8594\n",
      "Epoch 220/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 84.4817 - val_loss: 1105.6631\n",
      "Epoch 221/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 84.0687 - val_loss: 1110.0970\n",
      "Epoch 222/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.7187 - val_loss: 1114.1860\n",
      "Epoch 223/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 83.4220 - val_loss: 1117.9532\n",
      "Epoch 224/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 83.1703 - val_loss: 1121.4203\n",
      "Epoch 225/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 82.9569 - val_loss: 1124.6096\n",
      "Epoch 226/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 82.7756 - val_loss: 1127.5400\n",
      "Epoch 227/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 82.6217 - val_loss: 1130.2303\n",
      "Epoch 228/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.4898 - val_loss: 1132.6593\n",
      "Epoch 229/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 82.3741 - val_loss: 1134.6250\n",
      "Epoch 230/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.2734 - val_loss: 1136.3186\n",
      "Epoch 231/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.2066 - val_loss: 1138.9611\n",
      "Epoch 232/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.1353 - val_loss: 1140.6995\n",
      "Epoch 233/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 82.0765 - val_loss: 1142.2885\n",
      "Epoch 234/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 82.0264 - val_loss: 1143.7408\n",
      "Epoch 235/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9834 - val_loss: 1145.0663\n",
      "Epoch 236/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 81.9467 - val_loss: 1146.2747\n",
      "Epoch 237/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.9151 - val_loss: 1147.3741\n",
      "Epoch 238/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.8880 - val_loss: 1148.3673\n",
      "Epoch 239/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8647 - val_loss: 1149.2322\n",
      "Epoch 240/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.8446 - val_loss: 1149.5216\n",
      "Epoch 241/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7392 - val_loss: 1108.0094\n",
      "Epoch 242/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 86.4535 - val_loss: 1083.0396\n",
      "Epoch 243/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.9387 - val_loss: 1089.0675\n",
      "Epoch 244/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.2888 - val_loss: 1094.7094\n",
      "Epoch 245/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 84.7317 - val_loss: 1099.8992\n",
      "Epoch 246/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 84.2634 - val_loss: 1104.6636\n",
      "Epoch 247/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 83.8702 - val_loss: 1109.0339\n",
      "Epoch 248/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 83.5397 - val_loss: 1113.0389\n",
      "Epoch 249/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.2579 - val_loss: 1116.1270\n",
      "Epoch 250/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 83.0644 - val_loss: 1120.0670\n",
      "Epoch 251/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 82.8306 - val_loss: 1123.1396\n",
      "Epoch 252/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 82.6645 - val_loss: 1125.9465\n",
      "Epoch 253/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.5243 - val_loss: 1128.5079\n",
      "Epoch 254/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.4058 - val_loss: 1130.8397\n",
      "Epoch 255/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 82.3055 - val_loss: 1132.8784\n",
      "Epoch 256/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 82.1310 - val_loss: 1133.2203\n",
      "Epoch 257/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.3141 - val_loss: 1135.7870\n",
      "Epoch 258/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 82.1764 - val_loss: 1137.7399\n",
      "Epoch 259/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 82.0996 - val_loss: 1139.4420\n",
      "Epoch 260/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.0406 - val_loss: 1140.9526\n",
      "Epoch 261/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 81.9927 - val_loss: 1142.2998\n",
      "Epoch 262/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9534 - val_loss: 1143.5059\n",
      "Epoch 263/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.9204 - val_loss: 1144.5856\n",
      "Epoch 264/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8929 - val_loss: 1145.5549\n",
      "Epoch 265/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8695 - val_loss: 1146.4236\n",
      "Epoch 266/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8496 - val_loss: 1147.1981\n",
      "Epoch 267/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.8326 - val_loss: 1147.8710\n",
      "Epoch 268/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.8176 - val_loss: 1148.4230\n",
      "Epoch 269/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.8049 - val_loss: 1148.9060\n",
      "Epoch 270/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7897 - val_loss: 1148.7424\n",
      "Epoch 271/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.3541 - val_loss: 1143.4883\n",
      "Epoch 272/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9892 - val_loss: 1144.8501\n",
      "Epoch 273/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9247 - val_loss: 1145.8693\n",
      "Epoch 274/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.8921 - val_loss: 1146.7490\n",
      "Epoch 275/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.8678 - val_loss: 1147.5210\n",
      "Epoch 276/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8485 - val_loss: 1148.2025\n",
      "Epoch 277/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.8327 - val_loss: 1148.8063\n",
      "Epoch 278/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.8194 - val_loss: 1149.3417\n",
      "Epoch 279/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.8083 - val_loss: 1149.8175\n",
      "Epoch 280/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7987 - val_loss: 1150.2400\n",
      "Epoch 281/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7906 - val_loss: 1150.6151\n",
      "Epoch 282/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7837 - val_loss: 1150.9485\n",
      "Epoch 283/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7776 - val_loss: 1151.2440\n",
      "Epoch 284/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7724 - val_loss: 1151.5063\n",
      "Epoch 285/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7678 - val_loss: 1151.7390\n",
      "Epoch 286/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7638 - val_loss: 1151.9443\n",
      "Epoch 287/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7605 - val_loss: 1152.1260\n",
      "Epoch 288/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 81.7574 - val_loss: 1152.2861\n",
      "Epoch 289/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7547 - val_loss: 1152.4265\n",
      "Epoch 290/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7524 - val_loss: 1152.5492\n",
      "Epoch 291/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7503 - val_loss: 1152.6561\n",
      "Epoch 292/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7485 - val_loss: 1152.7484\n",
      "Epoch 293/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7469 - val_loss: 1152.8282\n",
      "Epoch 294/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7455 - val_loss: 1152.8959\n",
      "Epoch 295/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7443 - val_loss: 1152.9528\n",
      "Epoch 296/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7432 - val_loss: 1152.9996\n",
      "Epoch 297/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7422 - val_loss: 1153.0363\n",
      "Epoch 298/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7413 - val_loss: 1153.0646\n",
      "Epoch 299/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7405 - val_loss: 1153.0852\n",
      "Epoch 300/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7399 - val_loss: 1153.0981\n",
      "Epoch 301/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7393 - val_loss: 1153.1041\n",
      "Epoch 302/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7388 - val_loss: 1153.1028\n",
      "Epoch 303/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7383 - val_loss: 1153.0946\n",
      "Epoch 304/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7379 - val_loss: 1153.0795\n",
      "Epoch 305/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7375 - val_loss: 1153.0571\n",
      "Epoch 306/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7372 - val_loss: 1153.0273\n",
      "Epoch 307/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7369 - val_loss: 1152.9894\n",
      "Epoch 308/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7367 - val_loss: 1152.9430\n",
      "Epoch 309/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7365 - val_loss: 1152.8865\n",
      "Epoch 310/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7363 - val_loss: 1152.8182\n",
      "Epoch 311/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7362 - val_loss: 1152.7352\n",
      "Epoch 312/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7360 - val_loss: 1152.6311\n",
      "Epoch 313/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7359 - val_loss: 1152.4988\n",
      "Epoch 314/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 81.7357 - val_loss: 1152.3203\n",
      "Epoch 315/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7357 - val_loss: 1152.0492\n",
      "Epoch 316/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7356 - val_loss: 1151.4622\n",
      "Epoch 317/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7253 - val_loss: 1129.8328\n",
      "Epoch 318/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 84.0669 - val_loss: 1085.8190\n",
      "Epoch 319/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.7118 - val_loss: 1084.9125\n",
      "Epoch 320/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 87.1031 - val_loss: 1097.2714\n",
      "Epoch 321/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 84.0688 - val_loss: 1054.7148\n",
      "Epoch 322/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 93.3316 - val_loss: 1058.8762\n",
      "Epoch 323/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 91.5976 - val_loss: 1068.9675\n",
      "Epoch 324/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 89.8965 - val_loss: 1078.3071\n",
      "Epoch 325/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 88.4860 - val_loss: 1086.8499\n",
      "Epoch 326/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 87.3270 - val_loss: 1094.6508\n",
      "Epoch 327/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 86.3739 - val_loss: 1101.7656\n",
      "Epoch 328/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.5894 - val_loss: 1108.2501\n",
      "Epoch 329/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 84.9425 - val_loss: 1114.1552\n",
      "Epoch 330/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 84.4085 - val_loss: 1119.5280\n",
      "Epoch 331/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.9672 - val_loss: 1124.4136\n",
      "Epoch 332/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 83.6018 - val_loss: 1128.8523\n",
      "Epoch 333/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 83.2991 - val_loss: 1132.8844\n",
      "Epoch 334/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.0477 - val_loss: 1136.5435\n",
      "Epoch 335/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.8387 - val_loss: 1139.8628\n",
      "Epoch 336/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 82.6647 - val_loss: 1142.8728\n",
      "Epoch 337/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 82.5195 - val_loss: 1145.6013\n",
      "Epoch 338/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 82.3981 - val_loss: 1148.0726\n",
      "Epoch 339/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 82.2965 - val_loss: 1150.3112\n",
      "Epoch 340/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 82.2113 - val_loss: 1152.3379\n",
      "Epoch 341/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.1397 - val_loss: 1154.1732\n",
      "Epoch 342/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.0792 - val_loss: 1155.8337\n",
      "Epoch 343/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 82.0282 - val_loss: 1157.3356\n",
      "Epoch 344/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.9850 - val_loss: 1158.6941\n",
      "Epoch 345/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9483 - val_loss: 1159.9219\n",
      "Epoch 346/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.9171 - val_loss: 1161.0319\n",
      "Epoch 347/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8905 - val_loss: 1162.0355\n",
      "Epoch 348/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8678 - val_loss: 1162.9425\n",
      "Epoch 349/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8483 - val_loss: 1163.7621\n",
      "Epoch 350/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8315 - val_loss: 1164.5016\n",
      "Epoch 351/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8171 - val_loss: 1165.1696\n",
      "Epoch 352/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8047 - val_loss: 1165.7721\n",
      "Epoch 353/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7939 - val_loss: 1166.3170\n",
      "Epoch 354/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7845 - val_loss: 1166.8090\n",
      "Epoch 355/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7764 - val_loss: 1167.2526\n",
      "Epoch 356/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7693 - val_loss: 1167.6533\n",
      "Epoch 357/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7631 - val_loss: 1168.0144\n",
      "Epoch 358/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7576 - val_loss: 1168.3394\n",
      "Epoch 359/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7530 - val_loss: 1168.6332\n",
      "Epoch 360/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7488 - val_loss: 1168.8979\n",
      "Epoch 361/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7452 - val_loss: 1169.1368\n",
      "Epoch 362/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7420 - val_loss: 1169.3524\n",
      "Epoch 363/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7390 - val_loss: 1169.5459\n",
      "Epoch 364/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7365 - val_loss: 1169.7201\n",
      "Epoch 365/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7343 - val_loss: 1169.8773\n",
      "Epoch 366/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7324 - val_loss: 1170.0189\n",
      "Epoch 367/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7305 - val_loss: 1170.1454\n",
      "Epoch 368/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7290 - val_loss: 1170.2595\n",
      "Epoch 369/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7277 - val_loss: 1170.3622\n",
      "Epoch 370/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7264 - val_loss: 1170.4541\n",
      "Epoch 371/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7253 - val_loss: 1170.5360\n",
      "Epoch 372/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7243 - val_loss: 1170.6095\n",
      "Epoch 373/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7235 - val_loss: 1170.6754\n",
      "Epoch 374/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7228 - val_loss: 1170.7349\n",
      "Epoch 375/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7221 - val_loss: 1170.7871\n",
      "Epoch 376/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7214 - val_loss: 1170.8333\n",
      "Epoch 377/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7209 - val_loss: 1170.8739\n",
      "Epoch 378/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7204 - val_loss: 1170.9094\n",
      "Epoch 379/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7200 - val_loss: 1170.9403\n",
      "Epoch 380/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7197 - val_loss: 1170.9670\n",
      "Epoch 381/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7193 - val_loss: 1170.9904\n",
      "Epoch 382/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7190 - val_loss: 1171.0105\n",
      "Epoch 383/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7188 - val_loss: 1171.0262\n",
      "Epoch 384/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 81.7185 - val_loss: 1171.0392\n",
      "Epoch 385/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7182 - val_loss: 1171.0481\n",
      "Epoch 386/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7181 - val_loss: 1171.0543\n",
      "Epoch 387/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7179 - val_loss: 1171.0571\n",
      "Epoch 388/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7178 - val_loss: 1171.0585\n",
      "Epoch 389/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7177 - val_loss: 1171.0559\n",
      "Epoch 390/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7176 - val_loss: 1171.0524\n",
      "Epoch 391/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7175 - val_loss: 1171.0441\n",
      "Epoch 392/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7175 - val_loss: 1171.0345\n",
      "Epoch 393/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7169 - val_loss: 1167.5474\n",
      "Epoch 394/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7268 - val_loss: 1173.2667\n",
      "Epoch 395/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7172 - val_loss: 1173.2736\n",
      "Epoch 396/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7172 - val_loss: 1173.2803\n",
      "Epoch 397/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7171 - val_loss: 1173.2861\n",
      "Epoch 398/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7171 - val_loss: 1173.2909\n",
      "Epoch 399/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7170 - val_loss: 1173.2949\n",
      "Epoch 400/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7170 - val_loss: 1173.2987\n",
      "Epoch 401/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7170 - val_loss: 1173.3026\n",
      "Epoch 402/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7170 - val_loss: 1173.3059\n",
      "Epoch 403/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7170 - val_loss: 1173.3083\n",
      "Epoch 404/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7170 - val_loss: 1173.3123\n",
      "Epoch 405/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7170 - val_loss: 1173.3141\n",
      "Epoch 406/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 81.7170 - val_loss: 1173.3173\n",
      "Epoch 407/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7170 - val_loss: 1173.3185\n",
      "Epoch 408/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7170 - val_loss: 1173.3203\n",
      "Epoch 409/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7169 - val_loss: 1173.3215\n",
      "Epoch 410/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7170 - val_loss: 1173.3224\n",
      "Epoch 411/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7170 - val_loss: 1173.3234\n",
      "Epoch 412/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7170 - val_loss: 1173.3243\n",
      "Epoch 413/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7171 - val_loss: 1173.3263\n",
      "Epoch 414/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7170 - val_loss: 1173.3275\n",
      "Epoch 415/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7170 - val_loss: 1173.3280\n",
      "Epoch 416/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7170 - val_loss: 1173.3280\n",
      "Epoch 417/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7170 - val_loss: 1173.3281\n",
      "Epoch 418/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7170 - val_loss: 1173.3281\n",
      "Epoch 419/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7171 - val_loss: 1173.3285\n",
      "Epoch 420/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7171 - val_loss: 1173.3287\n",
      "Epoch 421/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7171 - val_loss: 1173.3287\n",
      "Epoch 422/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7171 - val_loss: 1173.3290\n",
      "Epoch 423/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7172 - val_loss: 1173.3297\n",
      "Epoch 424/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7172 - val_loss: 1173.3308\n",
      "Epoch 425/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7173 - val_loss: 1173.3319\n",
      "Epoch 426/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7172 - val_loss: 1173.3323\n",
      "Epoch 427/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7172 - val_loss: 1173.3324\n",
      "Epoch 428/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 81.7172 - val_loss: 1173.3325\n",
      "Epoch 429/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7172 - val_loss: 1173.3324\n",
      "Epoch 430/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7172 - val_loss: 1173.3322\n",
      "Epoch 431/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7172 - val_loss: 1173.3320\n",
      "Epoch 432/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7172 - val_loss: 1173.3318\n",
      "Epoch 433/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7172 - val_loss: 1173.3318\n",
      "Epoch 434/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7172 - val_loss: 1173.3314\n",
      "Epoch 435/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7172 - val_loss: 1173.3313\n",
      "Epoch 436/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7172 - val_loss: 1173.3309\n",
      "Epoch 437/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7172 - val_loss: 1173.3303\n",
      "Epoch 438/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7173 - val_loss: 1173.3298\n",
      "Epoch 439/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7173 - val_loss: 1173.3298\n",
      "Epoch 440/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7174 - val_loss: 1173.3295\n",
      "Epoch 441/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7174 - val_loss: 1173.3295\n",
      "Epoch 442/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7174 - val_loss: 1173.3295\n",
      "Epoch 443/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7174 - val_loss: 1173.3292\n",
      "Epoch 444/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7174 - val_loss: 1173.3291\n",
      "Epoch 445/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7174 - val_loss: 1173.3287\n",
      "Epoch 446/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7174 - val_loss: 1173.3281\n",
      "Epoch 447/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7174 - val_loss: 1173.3278\n",
      "Epoch 448/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7175 - val_loss: 1173.3278\n",
      "Epoch 449/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 81.7175 - val_loss: 1173.3279\n",
      "Epoch 450/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7175 - val_loss: 1173.3286\n",
      "Epoch 451/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7174 - val_loss: 1173.3290\n",
      "Epoch 452/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7174 - val_loss: 1173.3295\n",
      "Epoch 453/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7174 - val_loss: 1173.3292\n",
      "Epoch 454/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7174 - val_loss: 1173.3292\n",
      "Epoch 455/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7174 - val_loss: 1173.3291\n",
      "Epoch 456/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7174 - val_loss: 1173.3291\n",
      "Epoch 457/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7174 - val_loss: 1173.3291\n",
      "Epoch 458/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7174 - val_loss: 1173.3287\n",
      "Epoch 459/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7175 - val_loss: 1173.3287\n",
      "Epoch 460/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7175 - val_loss: 1173.3290\n",
      "Epoch 461/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7175 - val_loss: 1173.3289\n",
      "Epoch 462/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7175 - val_loss: 1173.3287\n",
      "Epoch 463/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7175 - val_loss: 1173.3290\n",
      "Epoch 464/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7175 - val_loss: 1173.3290\n",
      "Epoch 465/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7175 - val_loss: 1173.3290\n",
      "Epoch 466/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7175 - val_loss: 1173.3289\n",
      "Epoch 467/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7175 - val_loss: 1173.3290\n",
      "Epoch 468/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 81.7175 - val_loss: 1173.3290\n",
      "Epoch 469/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7176 - val_loss: 1173.3290\n",
      "Epoch 470/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7175 - val_loss: 1173.3290\n",
      "Epoch 471/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7176 - val_loss: 1173.3287\n",
      "Epoch 472/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7176 - val_loss: 1173.3287\n",
      "Epoch 473/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7176 - val_loss: 1173.3289\n",
      "Epoch 474/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7176 - val_loss: 1173.3287\n",
      "Epoch 475/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7176 - val_loss: 1173.3290\n",
      "Epoch 476/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7176 - val_loss: 1173.3291\n",
      "Epoch 477/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7176 - val_loss: 1173.3290\n",
      "Epoch 478/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7176 - val_loss: 1173.3290\n",
      "Epoch 479/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7176 - val_loss: 1173.3287\n",
      "Epoch 480/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7176 - val_loss: 1173.3286\n",
      "Epoch 481/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7176 - val_loss: 1173.3285\n",
      "Epoch 482/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7176 - val_loss: 1173.3282\n",
      "Epoch 483/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7176 - val_loss: 1173.3279\n",
      "Epoch 484/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7176 - val_loss: 1173.3276\n",
      "Epoch 485/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7176 - val_loss: 1173.3275\n",
      "Epoch 486/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7176 - val_loss: 1173.3270\n",
      "Epoch 487/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7176 - val_loss: 1173.3270\n",
      "Epoch 488/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 81.7176 - val_loss: 1173.3267\n",
      "Epoch 489/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7176 - val_loss: 1173.3263\n",
      "Epoch 490/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7176 - val_loss: 1173.3258\n",
      "Epoch 491/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7176 - val_loss: 1173.3257\n",
      "Epoch 492/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7176 - val_loss: 1173.3256\n",
      "Epoch 493/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7176 - val_loss: 1173.3254\n",
      "Epoch 494/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7176 - val_loss: 1173.3252\n",
      "Epoch 495/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7176 - val_loss: 1173.3251\n",
      "Epoch 496/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7176 - val_loss: 1173.3250\n",
      "Epoch 497/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7176 - val_loss: 1173.3248\n",
      "Epoch 498/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7176 - val_loss: 1173.3247\n",
      "Epoch 499/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7177 - val_loss: 1173.3247\n",
      "Epoch 500/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7177 - val_loss: 1173.3247\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6465, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0029, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0003, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0168, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3581, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 327ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 126)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 126)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 186)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:150:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 6.20606209e+01, 6.18841503e+01, 6.17076797e+01,\n",
       "        6.15658730e+01, 6.14808730e+01, 6.13951587e+01, 6.13094444e+01,\n",
       "        6.11639916e+01, 6.09371008e+01, 0.00000000e+00, 3.58253600e-02,\n",
       "        6.15157936e+01, 6.14300794e+01, 6.13443651e+01, 6.12564286e+01,\n",
       "        6.10295378e+01, 6.08026471e+01, 6.05757563e+01, 6.03543091e+01,\n",
       "        6.02383427e+01, 4.05724870e-02, 0.00000000e+00, 4.14460144e+01,\n",
       "        1.08057189e+00, 0.00000000e+00, 0.00000000e+00, 8.43455434e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.84537201e+01, 0.00000000e+00,\n",
       "        4.94801223e-01, 1.34578526e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.15052140e-01, 0.00000000e+00, 3.78040612e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.39704034e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.17390811e+00, 0.00000000e+00, 0.00000000e+00, 1.02042949e+00,\n",
       "        0.00000000e+00, 3.47454429e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([52.62850311, 52.61975815, 52.61101319, 52.60226822, 52.59352326,\n",
       "       52.5847783 , 52.57603334, 52.56728838, 52.55854342, 52.54979846,\n",
       "       52.54105349, 52.53230853, 52.52356357, 52.51481861, 52.50607365,\n",
       "       52.49732869, 52.48858373, 52.47983876, 52.4710938 , 52.46234884,\n",
       "       52.45360388, 52.44485892, 52.43611396, 52.427369  , 52.41862403,\n",
       "       52.40987907, 52.40113411, 52.39238915, 52.38364419, 52.37489923,\n",
       "       52.36615427, 52.35740931, 52.34866434, 52.33991938, 52.33117442,\n",
       "       52.32242946, 52.3136845 , 52.30493954, 52.29619458, 52.28744961,\n",
       "       52.27870465, 52.26995969, 52.26121473, 52.25246977, 52.24372481,\n",
       "       52.23497985, 52.22623488, 52.21748992, 52.20874496, 52.2       ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.754111530963826\n",
      "34.70680170153181\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
