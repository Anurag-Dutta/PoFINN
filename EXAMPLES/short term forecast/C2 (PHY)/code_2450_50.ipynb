{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "2495    52.162308\n",
       "2496    52.146731\n",
       "2497    52.131154\n",
       "2498    52.115577\n",
       "2499    52.100000\n",
       "Name: C2, Length: 2500, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c2_interpolated_2450_50.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "2445     0.488552\n",
       "2446     0.000000\n",
       "2447     0.000000\n",
       "2448     0.000000\n",
       "2449     0.033698\n",
       "Name: C2, Length: 2450, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(50)\n",
    "test\n",
    "training_set = training_set.head(2450)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.500000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.275910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.051821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.827731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.603641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     90.500000  0.000298\n",
       "1     90.275910  0.000298\n",
       "2     90.051821  0.000297\n",
       "3     89.827731  0.000297\n",
       "4     89.603641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD4CAYAAADo30HgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAAsTAAALEwEAmpwYAAAzAUlEQVR4nO3deXxU1d348c83eyArJIRAAmHfZQsgyKLsrmjdtYpLH9THpS0/rfr41LZan0rVulRqRUHRVtGiKIqKbLKILGHfSWRNAiQsCVsIJDm/P+YGJmGSmclMMpPM9/16zWvm3nvunXMZcr/3LPccMcaglFJKlQvydQaUUkr5Fw0MSimlKtDAoJRSqgINDEoppSrQwKCUUqqCEF9noCYSEhJMWlqar7OhlFL1ypo1aw4bYxKdpauXgSEtLY2MjAxfZ0MppeoVEdnrSjqtSlJKKVWBBgallFIVaGBQSilVgQYGpZRSFWhgUEopVYEGBqWUUhVoYFBKKVVBQAWGWeuy+dcKl7rxKqVUwAqowPDNpoMaGJRSygmvBAYRGSsiO0QkS0SecrB9qIisFZESEbmp0rZSEVlvvWZ7Iz9VSYwOJ/9EcW1+hVJK1XseD4khIsHAZGAUkA2sFpHZxpitdsn2AfcAjzs4RJExppen+XBFs+hwjp4+y7nSMkKDA6qwpJRSLvPG1bE/kGWM2WWMOQvMAMbZJzDG7DHGbATKvPB9NZYYHY4xcOTkWV9mQyml/Jo3AkNLYL/dcra1zlURIpIhIitE5PqqEonIBCtdRn5+fo0ymhgVDqDVSUopVQ1/qE9pbYxJB+4AXhORdo4SGWOmGGPSjTHpiYlOR411KDHaCgwnz9Q0r0op1eB5IzDkAKl2yynWOpcYY3Ks913AD0BvL+TJoWYxEYCWGJRSqjreCAyrgQ4i0kZEwoDbAJd6F4lIvIiEW58TgMuArdXvVXMJUWEA5B3XwKCUUlXxODAYY0qAR4C5wDbgU2PMFhF5TkSuAxCRfiKSDdwMvC0iW6zduwAZIrIBWAS8WKk3k1eFhwQTGxlK/kkNDEopVRWvzOBmjPkG+KbSumftPq/GVsVUeb/lQA9v5MFVzfRZBqWUqpY/ND7XqcTocPI0MCilVJUCMjBoiUEppaoWcIGhvCrJGOPrrCillF8KuMCQEt+IonOlZB8r8nVWlFLKLwVcYBjW0fZw3Lyth3ycE6WU8k8BFxjSEhrTMSlKA4NSSlUh4AIDwKiuSazac5SC0zqYnlJKVRaQgWF01+aUlhkWbs/zdVaUUsrvBGRg6NEylqSYcK1OUkopBwIyMAQFCSO7JLF4Zz5nzpX6OjtKKeVXAjIwAIzp1pzTZ0v5aOU+X2dFKaX8SsAGhsHtE7iiUyIvfrudLbmFvs6OUkr5jYANDEFBwss39yS+cSiPfrSOU8Ulvs6SUkr5hYANDABNo8J57dbe7D5yime/3OJ8B6WUCgABHRgABrZryqPDO/DZ2mw+X5vt6+wopZTPBXxgAHhseHv6pzXhf7/YzK78k77OjlJK+ZRXAoOIjBWRHSKSJSJPOdg+VETWikiJiNxUadt4Ecm0XuO9kR93hQQH8frtvQgLCeKRj9ZRXKJdWJVSgcvjwCAiwcBk4EqgK3C7iHStlGwfcA/wUaV9mwB/AAYA/YE/iEi8p3mqieTYSF6+qSdbDxznua9qbXZRpZTye94oMfQHsowxu4wxZ4EZwDj7BMaYPcaYjUBZpX3HAPOMMUeNMceAecBYL+SpRkZ2TeKBoW3598p9/Cdjv6+yoZRSPuWNwNASsL+KZlvrvLqviEwQkQwRycjPz69RRl3xxJhODGrXlGe+2MzmHH2+QSkVeOpN47MxZooxJt0Yk56YmFhr3xMSHMTfb+9NQuMwHvhwDUdP6QisSqnA4o3AkAOk2i2nWOtqe99a0zQqnH/e1Zf8k8U89vE6ysp0GlClVODwRmBYDXQQkTYiEgbcBsx2cd+5wGgRibcanUdb63zukpQ4fn91F5ZlHWZZ1mFfZ0cppeqMx4HBGFMCPILtgr4N+NQYs0VEnhOR6wBEpJ+IZAM3A2+LyBZr36PA89iCy2rgOWudX7ilXyoJUWF8uGKvr7OilFJ1JsQbBzHGfAN8U2nds3afV2OrJnK07zRgmjfy4W3hIcHckp7KPxf/TG5BES3iIn2dJaWUqnX1pvHZV+4Y0AoDfLxKh+dWSgUGDQxOpMQ3YninZny8aj9nSyo/hqGUUg2PBgYX/HJgaw6fLOb7rQd9nRWllKp1GhhcMKxDIqlNIvnwJ22EVko1fBoYXBAUJNw5oDUrdx8l89AJX2dHKaVqlQYGF93cN4Ww4CD+pV1XlVINnAYGFzWNCufqS5L5fG2OTgOqlGrQvPIcQ6D45aWtmbUuh6vfWEqfVvFckhJLz9Q4uiTHEBEa7OvsKaWUV2hgcEOfVnH83w09WLj9EEuzDvP5OtuwTiFBQufkaC5JiaNnSiy9UuPpmBSFiPg4x0op5T4xpv4NEJeenm4yMjJ8mgdjDAePn2HD/gI2ZBeyMbuAjdmFnDhjq2a6sU8Kf/lFD8JCtLZOKeUfRGSNMSbdWTotMdSQiJAcG0lybCRjuycDUFZm2H3kFDPXZPPWDz9z6PgZ3vplH6IjQn2cW6WUcp3eznpRUJDQLjGKJ8d25q83XcKKXUe4+Z8/cbDwjK+zppRSLtPAUEtuSU9l2j392H/0NDf840e2Hzzu6ywppZRLNDDUoqEdE/n0wYGUlhlufusnluu8DkqpekADQy3r1iKWWQ9fRnJcBOPfW8Wsddm+zpJSSlXLK4FBRMaKyA4RyRKRpxxsDxeRT6ztK0UkzVqfJiJFIrLeev3TG/nxNy3jIvnPg4Po2zqe336ygcmLsqiPvcGUUoHB48AgIsHAZOBKoCtwu4h0rZTsfuCYMaY98CowyW7bz8aYXtbrQU/z469iI0OZfl9/xvVqwUtzd/A/szZTUqrDeCul/I83Sgz9gSxjzC5jzFlgBjCuUppxwHTr80xghATg01/hIcG8eksvHrq8HR+v2seED9fo8BpKKb/jjcDQEthvt5xtrXOYxpojuhBoam1rIyLrRGSxiAyp6ktEZIKIZIhIRn5+vhey7RtBQcKTYzvz/PXd+WFHHrdNWUH+iWJfZ0sppc7zdePzAaCVMaY3MBH4SERiHCU0xkwxxqQbY9ITExPrNJO14a5LWzPlrnSy8k5ywz9+JCvvpK+zpJRSgHcCQw6QarecYq1zmEZEQoBY4IgxptgYcwTAGLMG+Bno6IU81QsjuyYxY8KlFJ0t5ca3lrNy1xFfZ0kppbwSGFYDHUSkjYiEAbcBsyulmQ2Mtz7fBCw0xhgRSbQarxGRtkAHYJcX8lRv9EyNY9Z/X0bTxmHc+e5Kpi7brT2WlFI+5XFgsNoMHgHmAtuAT40xW0TkORG5zko2FWgqIlnYqozKu7QOBTaKyHpsjdIPGmOOepqn+qZV00bMevgyLu/UjOe/3sojH63jpDZKK6V8REdX9SNlZYa3l+zipbnbSUtozNu/7EuHpGhfZ0sp1UC4OrqqrxuflZ2gIOGhy9vxr18N4HjROcZN/pEv11durlFKqdqlgcEPDWqXwNePDqFLcgy/nrGeP87ewtkSfRhOKVU3NDD4qeaxEcyYcCn3XdaG95fv4dYpP3GgsMjX2VJKBQANDH4sNDiIZ6/typt39GbnwRNc/cYylmXqCK1KqdqlgaEeuOaSFnz5yGCaNg7jrmkreXNhJmVl9a/TgFKqftDAUE+0bxbFFw9fxrWXtODl73fyqw8yKDx9ztfZUko1QBoY6pHG4SG8flsv/nRdN5Zm5nPNm0vZnFPo62wppRoYDQz1jIgwflAaMyYM5FyJ4RdvLeeT1ft8nS2lVAOigaGe6ts6njmPDaZfWjxPfraJJ/6zgaKzpb7OllKqAdDAUI81jQrng/sG8MgV7fnPmmyun6yjtCqlPKeBoZ4LDhIeH9OJ6ff1J/9kMde9uUyfllZKeUQDQwMxrGMicx4bTLcWtqeln/58E2fOadWSUsp9GhgakOTYSD76r0t5YFhbPl61jxv+sZzdh0/5OltKqXpGA0MDExocxNNXdmHq+HQOFBZx7d+X8fXGXF9nSylVj2hgaKBGdElizmND6JAUxSMfreP3X2ymuESrlpRSzmlgaMBaxkXyyYSB/GpwGz5csZcb31rOviOnfZ0tpZSf80pgEJGxIrJDRLJE5CkH28NF5BNr+0oRSbPb9rS1foeIjPFGftQFYSFB/O81XZlyV1/2HTnN1W8s5bvNB3ydLaWUHwvx9ADWnM2TgVFANrBaRGYbY7baJbsfOGaMaS8itwGTgFtFpCu2OaK7AS2A+SLS0RijdR5eNrpbc+Ykx/DIR2t58F9raZvYmBGdmzG8cxLpafGEBmvhUSll43FgAPoDWcaYXQAiMgMYB9gHhnHAH63PM4E3RUSs9TOMMcXAbmtO6P7AT17Il6oktUkjPn1wIJ+u3s+8bXlMX76Xd5buJjoihGEdExnZJYlhHROJbxzm66wqVa9k7DnKun0F/NfQtr7Oild4IzC0BPbbLWcDA6pKY4wpEZFCoKm1fkWlfVs6+hIRmQBMAGjVqpUXsh2YwkOCuWtgGncNTONkcQnLMg+zYNshFu3I4+uNBwgS23AbI7okMaJzM9o3i8IWw5XyvnlbDxEeEsTQjokupX9p7naKz5Xxv9d09Xpe9hw+xao9R7mye3OiI0Jd2mfBtkMcOXWW383cCOD1wHCgsIizJWW0btrYq8d1xhuBoU4YY6YAUwDS09N1MgIviAoPYWz35ozt3pyyMsPGnEIWbjvE/G15vPjtdl78djupTSIZ0TmJ4Z2bMaBtE8JDgn2dbdWAvLkwk7hGYS4Hhk05xyksqp3h5tfsPcbvZm7kqw25fHh/5Xtbx+6fnlEreSk38C8LAdjz4tW1+j2VeSMw5ACpdssp1jpHabJFJASIBY64uK+qA0FBQq/UOHqlxjFxdCcOFBaxcHseC7bl8fGqfby/fA8tYiN44Rc9uKJTM19nVzUQxSVlhIW41r515lwpS3bm07ppo1rJS6mx3W8u9cNZEoe//AMLH7+8zr7PGy2Oq4EOItJGRMKwNSbPrpRmNjDe+nwTsNAYY6z1t1m9ltoAHYBVXsiT8lBybCR3DmjNtHv6sf7Z0Uy5qy+Nw0O4973VTPxkPcdOnfV1FlUDcLbU9cAwY5VtePm9R05zsrjEpX1OFZfQ9uk5vLt0l9O0xvhvRURBLZWSquJxYDDGlACPAHOBbcCnxpgtIvKciFxnJZsKNLUalycCT1n7bgE+xdZQ/R3wsPZI8j+RYcGM7tacrx8bzGPD2zN7Qy4j/7aYrzfm+vUfk/J/Z0vK+GbTAbbkOp9wqsRuOtv8E8UuHb+k1FBmYMWuo07TlpY5P96iHXms2HXEpe+uqYw9R30+da9X+igaY74xxnQ0xrQzxrxgrXvWGDPb+nzGGHOzMaa9MaZ/eQ8ma9sL1n6djDHfeiM/qnaEhwQzcXQnvnp0MC3iInnko3U88OEaDh0/4+usqXrqXGkZxsDVbyxzmjbIrhNEkIv9Ic6V2a7287cd4viZ6u+6N+wvcHq8e99bzW1TLvSXaZvQmOgI92vkT58tYeehExfNobI5p5Cb/vkTk77b7vYxvUk7ryu3dUmOYdZ/D+LpKzuzeGc+I/+2mE9X79fSg3Lb2RIXbtMtq3ZfuOsPcrGnXEnphf+Txeeq/65PMi50rnz2y818+NMep8ffdfgUJ85cqNaat/UQK10oUczbeojRry5h6rJd9HthPpuybSWmc1ax5Vxpxb+lo6fOknnohNPjeosGBlUjIcFBPDCsHd/+eghdkmP43WcbuWvqKvYf1SE3lOvcCQwbsgvOf3a1B/U5u/qhs67UFVm+33KITTWYT33Sd9uZ7kJA+fdKW3vJ0VPnyD9RfL7hu7y6bHjnizt4zFyT7XZ+akoDg/JI28QoZvzXpTx/fXfW7TvG6FeX8N6Puyn1cR2p8n8ni0s45cZ0tMFB9lVJLpYY7P4fuhOEiktKa9Q1+8jJ4ovu9h2JCLUd+/RZW2mj/NTK8+ioQf7tJc4b0L1FA4PyWFCQcNelrfl+4jAGtG3Cn77ays3/XE5WXt0VfVX986T1UJirQmoSGOxLDG4FhjLCXewtZe/Y6XPM23rIabryY5f3rio/n/JSTWiwbx8q1cCgvKZlXCTv3dOPV2/tya7Dp7jq9WW8Pj+TUy52LVSB42xJGev2HTu/nBgd7nSfiiUGF7/HLjC4M+x8cUkZ4aEVL48HC73XyaL8fHumxAEXqsbOWCWo8hKFr2hgUF4lItzQO4V5vx3GqK5JvDp/J4MnLWTyoixOOOkVogJH9rHT5NpdaIe78NBkSNCFy5Wrw7TMWHWhQdmdEkN8o9CLhsV478fdLu/vTKPQYBqHBZMSHwlcCHrlT3XHRl48JMe1PVt47fud0cCgakVidDiT7+zDrP8eRO9W8bw0dweXvbiQ1+bvpPC0BohAl1fpOQSD83p5+3p3V0sMGXsvlErcCQyJ0RFMGOL9AfGyj50mK+8EZcZWfVTeBFJelRQSHERKfORFgSEqPIS/397b6/mpSr0ZK0nVT71bxTPtnn5syi7k7wszeW1+JlOX7mb8oDTuH9xGR3INUGfOVazWcaWns30vIVfbGOyrMYvd6JW0/eBxgipFH290p3jl+52s3nOU7GNFwIVhOMq/6qa+KdzUN+Wi/eq6xUFLDKpO9EiJZcrd6Xzz2BCGdExg8g9ZDJ60kL98u43DJ117ilU1XO5edF0NDN1axJz/7E6JIdjB8cuf07m0bROXj1PZ0szD54OC/TFdPZ+6ooFB1amuLWL4x519mfuboYzoksSUJbsYPGkhf/56K3n6BHXAqBwI3H02Uly4cp05V8q3mw+eX26X6PrQ1ZVLC3Ahj1Xldf7EYU6PW/kmqMzVwFDHcUMDg/KJjknRvHF7b+ZPHMZVPZJ5b/keBv91EX+cvYUDhUXOD6DqN1N50b3I4Mp10v4Zhh+fGk77ZtHVpg+zm8UwxFFgcPJ9NRkao7x2y1lg0KokFVDaJUbxt1t6sfD/DeOGXi3514q9DPvrDzwzaxPZx/Qp6obK3UBQmStVL/ZDtLSMi3SaPr7xhQZfx1VJ1nsV+9fkoc7zJQYnV+K6nixLA4PyC62bNmbSTZew6PHLuTk9hU8z9nP5Sz/w5MyN7D1yytfZU152UXWMu1VJLlwn3b1M2+fJUVWSswOX1WCsMFfbGOq6CUIDg/IrqU0a8cINPVj8xBXcOaAVs9bnMPyVxUz8dD278k/6OnvKSypfQ/1hABX7PDiuSjIV3yudhDWQK3GNXJsWFLiou2pV6roqSburKr/UIi6SP43rzsNXtOftJbv498q9fLEuh5v6pvDrkR1dqhpQ/qv8kjqqaxKLd+a7PTJvsAsPMrh7A++sxFC58blyzVGpMbx/bz86JFXfllFhn7KK3VWrUq+qkkSkiYjME5FM6z2+inTjrTSZIjLebv0PIrJDRNZbL50zUlXQLCaC31/TlaW/G869l7Xhi/W5XPHSD/zpqy3azbUeKw8Evx7RgeTYCLdKDE+M6eTaAHceFEMctTFUPmzlYFZaZri8UzO3bloGt0/gjdt7E+PgSWd79a3x+SlggTGmA7DAWq5ARJoAfwAGAP2BP1QKIHcaY3pZrzwP86MaqMTocH5/TVd+ePxybujdkunL9zDsr4v42/c7nE7AovyP/SVVcO/uvnNz1+7I3W/gvpDeUYmkPBCUv1cuMdRkPpK0hMZc17OF07GR6lsbwzhguvV5OnC9gzRjgHnGmKPGmGPAPGCsh9+rAlSLuEgm3XQJ8yYO4/LOzXhjYRZD/7qItxf/fNHTtMp/lV9DRWzVJK5cUsu7kw7rmOjWd1ySEutWeoCXbr7k4u0XLVcqMbgRGP7nqs5MHZ/ucvq6LjN4GhiSjDEHrM8HgSQHaVoC++2Ws6115d6zqpF+L9VUpInIBBHJEJGM/Px8D7Ot6rt2iVFMvqMPXz86mF6pcfzl2+0Me2kRs9Zl60xy9Yyrl7yEqDBu6ptCSLB7l61f9G7pPBEVL/yD2iU4TVf5v5mjBuuqdGsRy4guji6XjvldiUFE5ovIZgevcfbpjO2v0d2/yDuNMT2AIdbrrqoSGmOmGGPSjTHpiYmu3TGohq97y1jev7c/nz4wkOTYSH77yQbufX81OQX6kJx/s10qxAoLrgRzg3v3zd6+Pajc+Gyf5eev7+70ATpHx3KV37UxGGNGGmO6O3h9CRwSkWQA691RG0EOkGq3nGKtwxhT/n4C+AhbG4RSbuvfpgmfPTSIP1zblVW7jzL6b4uZvnwPZTqTnF+yr0pCXLuIG+PenXN5sHG1R4+z4HShu6qN/XMLtX3h9rsSgxOzgfJeRuOBLx2kmQuMFpF4q9F5NDBXREJEJAFAREKBa4DNHuZHBbDgIOHey9ow9zdD6dM6nj/M3sLNb/+kM8n5ofJLqoh1UXUhMhjM+RKGu9/hTvoqt1eqQ/LklsPTJ79rm6eB4UVglIhkAiOtZUQkXUTeBTDGHAWeB1Zbr+esdeHYAsRGYD22UsQ7HuZHKVKbNOKD+/rzys09+Tn/JFe9voy/L8h0a3RNVbvOlxgQq/HZhaqkGj6X4GoocXZ8U/m9Dtuy3AmI3uDRA27GmCPACAfrM4Bf2S1PA6ZVSnMK6OvJ9ytVFRHhxr4pDO2YyJ++2sIr83YyZ9MBJt14CT1T43ydvYBXHgjKSwyuXmPdqkrCvr7Kc5XzaF9L6e5X+Hv/CB0SQzVoidHhvHlHH965O51jp89ywz9+5IU5Wyk6q11bfcn+bl7EtQuloXbr2p2XAMqfY6iw2CBpYFABYVTXJOZNHMat/VrxztLdjHltCcuzDvs6WwHL3fp/KL8gu1VkwJ09XL3Ol5dEajJonrvfVa6uG591rCQVMGIiQvnLL3pwXc8WPP35Ru54dyUD2zalTWJjWsZFkhIfSYs42yspOtzt/vLKdRfuzgXBtTYGMG5WJVnf4KXIcFF3Vbtt7rYB+PuzNhoYVMAZ2K4p3/1mKG8uzGJJZj7fbT7I0VNnK6QJEmgeE3E+ULSIi6RlXMXlmIiQOh/crCFyuSrJuPkcg10DtzdcPIief1/cPaGBQQWkiNBgHh/TicfHdAJsk8YfKCwip+AMuQVF5BYUkWO9r99fwHebD3K20mTyUeEhtIiLIDn24sDRMi6SpJgIwkK01FGd8rjqWnnBPfYN3K6ld+14VT357A5/DykaGJQCGoeH0L5ZdJVPr5aVGQ6fKibXQeDILTjD5pxCjlQqdYhAs+hwWsRFkhLfiF6pcQxun0DHpKiAL2lUbHyWWumV5C6nD7hVM2hebf+cOh+DUn4oKEhoFh1Bs+gIelXR3fXMudLzgcI+cBwoPMPavcf4akMuYAsWg9snMLhDAoPbJ9AsJqIOz6R2nSstY+aabK7qnkxsNRPWXLibPz8ohtNjG+PmA24uND5nHjpB28QogoMcD+S3POswyXGRtElofNHzCx7d9buwsy+HddHAoJSXRIQG0zYxiraJUQ635xYUsSzzMEuzDvPDznw+X5cD2IaRLg8UA9o0JTLMhbkG/NTW3OM8/fkm3l78M++OT6+yBFYX3VWdNT4XFp1j1KtLuKpHc/5xZ1+Hebjj3ZUA7Hnx6ouPb/8cg+vZctllLy6shaO6RgODUnWkRVwkt/RL5ZZ+qZSVGbYeOM7SzMMsy8rngxV7eXfZbsKCg+jbOp7BHRIY2iGRbi1iqp9/2M+UWPNb7j9WxPWTl/PG7b0Y3vniUUQrjJXkIvcbnysO1FdZsTVM+zebDlJWZqrtGbUxu8Crjc/+PiSGBgalfCAoSOjeMpbuLWN56PJ2FJ0tZfWeoyzNzGdp5mFemruDl+buICkmnCfGdOYXvVvWiwBRfq38yw09mP7THu6fnsHbv+zL6G7NK6az3m1DYrg6iJ5xq23GrkesQ/ZPLq/cfdRhiSE8JIjikjI+zdh/8ZzPLufEc/Vqak+llHdEhgUztGMiz1zdle9+M5TVz4zktVt7kRwbyeP/2cAv3lrOhv0Fvs6mU+UX25bxkcx8cBCdkqJ57uutF02idGHkU1twcHXYbe/m9cIRl//s+GHHxuG2e+e1ewvOZ+B8icEuspRft8vKDPuPnnb63f7e01UDg1J+KDE6nOt7t+Tzhwbxys09ySkoYtzkH/ndzA3kn/Dfua7L7C74kWHBPHtNV7KPFTHtx90V0tlfF0OChfyTxc6DQw0vplXda9sHhs05hQ4PX55mx6ETFLkwQ+AbCzO5/OUf6v2QKxoYlPJjQUG2wQAX/r9hPDC0LbPW5TD85R94d+kuzpX632ixlR8qG9Q+gZFdkpi8MIu8E2fsElrpxDbD2uac48zdcsjp8d0fQqPqahj7OLQp57jDwFRWZkhtEklpmWFTTiEAeSdsQcxRG0O3FrGUlhm25Ba6lDd/pYFBqXogOiKUp6/qwnfWXBN/nrONsa8tYclO/5rmtvziat8c8szVXThbWsYrc3c63Of2/q3olBTNC99cXOVU4di49xTz+S6xVebV9t6jZSyHTxZzrtRBYDDQOzUegOxjtu6jhUXn2H+0qFKvJNu39LTml96QXX1g8HcaGJSqR9olRvH+vf2YOj6d0jLD3dNW8V8fZLDviPN67bpQXu1u31DeJqEx4wem8ema/efvpO2fYwgJDuLZa7uy/+jFVU72bI3PrufFWc+n8jv+HtbFvKo0STHhdG5esdvtxpwChyWGZjERNI+JYGN2QfV5q3ar72lgUKqeERFGdEli7m+H8uTYzvyYdZiRry7m5bk7OH22xKd5K7/gV+5A9eiIDsRFhvLcV1sxxlz08Nll7RMY1TWJNxdmcej4GRwxuNld1XqvKjCUb+/WIqba4BEkQuumjQBoGRdJWEgQm7Idt0kAXJISy6ZALjGISBMRmScimdZ7fBXpvhORAhH5utL6NiKyUkSyROQTEQnzJD9KBZLwkGAeurwdix6/nKt7JPPmoiyGv7yY/2TsJ7egyCcjeF7oqFPxShsbGcrE0Z1Yufso324+6PCi/cxVXSgpNUz6drvDvNuv8kb7Svkdf1R4CO2qeCixzNgCcXJsJAChwUKX5Bg2ZhdWbCcQWLHrCKeKS+iZGseuw6coLDpX5Xfbn19JadlF51t5ub7N+fwUsMAY0wFYYC078hJwl4P1k4BXjTHtgWPA/R7mR6mAkxQTwau39mLmgwNJiA7jiZkbGfTiQvo8P487313B/32zjS/W5ZB56AQltdxgXeagjaHc7f1S6dw8mkc/Xsf05XuAim0GaQmNuX9IGz5fl8Pd01ax42DFubqNNez2Tz8f4YqXf2DOxgPVBj9nD7hdaA8RLqmiOqmszBAcBD1Tbdv3HDlNz5RY1u8vqNA7bO+RU9z+zgpeX5BJz5Q4AJZlHmbbgePc+NZycqsY3uLQ8TNc/cYyZlvDpZz/Xh/XNXkaGMYB063P04HrHSUyxiwAKvzKYusqMByY6Wx/pZRz6WlN+PLhwXz20CCeH9eNMd2ac+JMCe8v38NvPlnPqFeX0P2Pcxk3+Uee/nwT/1qxl7X7jnm1a6X9xbaykOAg/v2rAfxyQCuy8k5a6yqm+3+jOvLsNV3ZsL+AK19fwjOzNnHk5IULsIgQHhpEVHgID3+0llvfXsHmHMfVNk6rkuzaIO67rI3DNOVVSVd2Tz6/7u6BaZSUlfGXb7edX5fWtDG3pqcyddlu4hqF0qFZFC9+t43Q4CA25RQy6bvtDvMWERJMSLAw6dvtFX4HXw/p7emTz0nGmAPW54PAxc++V60pUGCMKa8UzQZaVpVYRCYAEwBatWpVg6wq1fAFBwl9W8fTt/WFWt1zpWXsyj/FltxCtuYeZ0vucb7ZdICPV+0DbHf3bRIa0ys1nqsvac6QDomE1nCSImcNvk2jwvnTuO7ce1kbth88QUJUeIXtIcFB3De4DTf0bsnrCzL5cMVeZq/P5Y/XdTs/JEafVvHMeWwIn6zezyvf7+DaN5dxU58U/ueqLsQ3vlAbbX9tXbQ9j64tYkiyG7DwfEO52J5C//rRwVzz92UALN6Zz8cr952vSooIDWbJE1dQWHSO9s2ieHBYO/6+MKtC3p8c25m4RmG0btqIP43rxh3vrOTbTQeYMKQtby7K4u6BrSvk7Tcz1pGZd5I/XNuNW97+iXeW7uKxER0oKS3jyMmLR+qtS04Dg4jMB5o72PSM/YIxxohIrYU5Y8wUYApAenq6vzfqK+U3QoOD6NQ8mk7No/lFH9s6Yww5BUXnA8XWA8eZv+0Qn63NJr5RKFf1SGZcr5akt453aygO+4ttddISGpOW0LjK7fGNw/jjdd345aWt+J9Zm3l85gbbhd46bHCQcMeAVlzTM5nJC7OY9uNufsw6zJt39qFPq/KgaMvMiTMl/P6LzTSLieCTCZfS1ApGlau9ure8UJ2UW1DE91sPVtjeymqABnj4ivZ8uT6XfXZPOcc3DuOpKzsDMKhdAkM6JPDZ2mzmPDaETzP288r3FbvrdmsRyxfrc2keE8FVPZrzz8U/M35gGje/vZxWTRrhS05vC4wxI40x3R28vgQOiUgygPWe58Z3HwHiRKQ8OKUAOe6egFLKfSJCSnwjRndrzm9HdeSdu9NZ/cxIpo5PZ0iHRD5fm8Mtb//E4EkL+cs329iSW+hSY7b9k8/e0L5ZNB/c159+aU1sx63UXhBjPd/x2UODCAoSbn37J6Yt210hrzGRoUy5O539R09z19RVFJ4+VyGvjvo63d6/FVPH9yMqPITmDoZFjwgN5vnru59fdvQQ3ehuzdlz5DS5BUXce1kblv98xG6rYWx32/32d1sO8MgVHTh9tpQZq/fRL60J87e5cyn1Pk/bGGYD463P44EvXd3R2H65RcBNNdlfKeVdYSFBjOiSxBu39ybjf0fy+m296Jwcw9Rlu7n6jWWMenUJf1+Qyd4jp6o8hren0wTbRXjq+HSu7N6cAW2bOExzSUoccx4dwrCOzXju6608/NFajp8psfICl7ZtypS708nKO8n491ZxsrjkfF6rKhBd0bkZa34/ktv6O666HtYxkS8evqzKfI/qksSlbZtw6mwpt/dPJSK04uU2tUkj+qXFc7K4lK4tYhjYtinTl+9hTDdHFTR1y9PA8CIwSkQygZHWMiKSLiLvlicSkaXAf4ARIpItImOsTU8CE0UkC1ubw1QP86OU8oLG4SGM69WSaff0Y/UzI3nhhu40aRzGK/N2MuylHxg3+UemLdtdcZgL7BqfvfyEVHREKG/9si9XdGpWZZrYRqG8c3dfnr6yM3O3HOL+91cDF0ovwzom8uYdvdmUU8h976/mtNXYW7naa8LQtuc/h4dUPzdGk0ZV97BvHhvBjAkD6ZUaR1yjMG7sk3J+W3lQ+vSBgUwc1RGA+wa3IbfwDMdOn71oSlhvBlpXeNT4bIw5AoxwsD4D+JXd8pAq9t8F9PckD0qp2hXfOIw7B7TmzgGtyS0o4uuNuXy5Ppfnvt7Kn+dsJb11E4Z1SmRYx0RKq+mVVBdEhAeGtaN3q3ge/XitbZ3dRXV0t+a8emsvfj1jHb+bucGWV7trsKMJeVz6XhfS3HtZGv9eue+i/JYb3rkZrZs2YvryPYzqmsScjQd4YkwnsvJOsmbvsRrlq6Z0PgallMtaxEUyYWg7JgxtR1beCWavz2Xhjrzz80eU3+n6euaI/m2aMOexIbz/4x4urVT9dF3PFpwrKeN3n20E6u5uvH2zaP5wbVf+9NVWh09NBwcJE0d15MSZEuIbhTFn4wF2Hz7lkwcVNTAopWqkfbNoJo7uxMTRncg/UcySnfks3plPbkERLeIifZ09EqLCeXxMJ4fbbuybQlJMBC/N3U7bxKp7Rznj7kxsg9olVLt9XC9bj/3SMsN/X96Om9NTeX3+Tv/rrqqUUs4kRodzY98Ubuyb4jyxnxjcIYHBHQZ75VjuXridFQKCg4Tfje1c8wx5SAfRU0qpGnK3lsdHTS9u08CglFIecrvE4EYVlC+e5tXAoJRSdaSmBYa6LmhoYFBKqRqq6d28Tu2plFINnKtdXrWNQSmllEPuFBh8UbrQwKCUUjXk/sNnNSsyOBqkrzZpYFBKKQ+5/xyDfzcyaGBQSqk6UpMbf+2uqpRS9UhdXbS1u6pSSjVQ5Rd4P69J0sCglFI15f6QGPWjv6pHgUFEmojIPBHJtN7jq0j3nYgUiMjXlda/LyK7RWS99erlSX6UUsoX3L3guzUkhg+KF56WGJ4CFhhjOgALrGVHXgLuqmLbE8aYXtZrvYf5UUopv1Xj8kIdFzQ8DQzjgOnW5+nA9Y4SGWMWACc8/C6llPIzNbubb+htDEnGmAPW54NAUg2O8YKIbBSRV0Uk3MP8KKVUnXP1hr6+dFd1OlGPiMwHmjvY9Iz9gjHGiIi75/A0toASBkwBngSeqyIfE4AJAK1atXLza5RSyveiwkO4Y0Ar0hLcmzWurpusnQYGY8zIqraJyCERSTbGHBCRZCDPnS+3K20Ui8h7wOPVpJ2CLXiQnp7u5wUxpVQgcLdKqGlUOP93Q4/ayYwXeVqVNBsYb30eD3zpzs5WMEFsTfrXA5s9zI9SStW5etIL1WWeBoYXgVEikgmMtJYRkXQRebc8kYgsBf4DjBCRbBEZY236t4hsAjYBCcCfPcyPUko1LD6oH3FalVQdY8wRYISD9RnAr+yWh1Sx/3BPvl8ppXypzobE0NFVlVKqfnF1op76QgODUkrVkL8/j1BTGhiUUspDtVnT487wGd6igUEppfycDrutlFL1hC/u5uuCBgallPJQbd7R+6IdQwODUkr5ubp+gE4Dg1JK1ZD2SlJKKeWQDomhlFKqzmgbg1JK1SN1ddGu6yerNTAopZTHGlZdkgYGpZSqobp4jkGffFZKqXqothuftbuqUkopn9LAoJRSNZQQFc5Dl7ejjZtzOPs7jwKDiDQRkXkikmm9xztI00tEfhKRLSKyUURutdvWRkRWikiWiHwiImGe5EcppepSUkwET47tTMek6Fr7jvrYXfUpYIExpgOwwFqu7DRwtzGmGzAWeE1E4qxtk4BXjTHtgWPA/R7mRymllIc8DQzjgOnW5+nA9ZUTGGN2GmMyrc+5QB6QKLa56oYDM6vbXymlVN3yNDAkGWMOWJ8PAknVJRaR/kAY8DPQFCgwxpRYm7OBltXsO0FEMkQkIz8/38NsK6WUqkqIswQiMh9o7mDTM/YLxhgjIlXWholIMvAhMN4YU+bu5NbGmCnAFID09PQGOnSVUkpV5IuLndPAYIwZWdU2ETkkIsnGmAPWhT+vinQxwBzgGWPMCmv1ESBOREKsUkMKkOP2GSilVAPn7o20pzytSpoNjLc+jwe+rJzA6mk0C/jAGFPenoAxxgCLgJuq218ppVTd8jQwvAiMEpFMYKS1jIiki8i7VppbgKHAPSKy3nr1srY9CUwUkSxsbQ5TPcyPUko1KL7oruq0Kqk6xpgjwAgH6zOAX1mf/wX8q4r9dwH9PcmDUko1dHU9RJ8++ayUUqoCDQxKKaUq0MCglFJ+TYfdVkopVYkOu62UUsqnNDAopZQfq4+jqyqllKplWpWklFLKpzQwKKWUqkADg1JK+TFfjK6qgUEppfyc1PGgGBoYlFJKVaCBQSmlVAUaGJRSyo8ZHzzIoIFBKaX8XL16jkFEmojIPBHJtN7jHaTpJSI/icgWEdkoIrfabXtfRHY7mMBHKaWUj3haYngKWGCM6QAssJYrOw3cbYzpBowFXhOROLvtTxhjelmv9R7mRymlGpT0tCYMapdQp9/p0QxuwDjgcuvzdOAHbNN1nmeM2Wn3OVdE8oBEoMDD71ZKqQbv4Sva1/l3elpiSDLGHLA+HwSSqkssIv2BMOBnu9UvWFVMr4pIuIf5UUop5SGnJQYRmQ80d7DpGfsFY4wRkSqbz0UkGfgQGG+MKbNWP40toIQBU7CVNp6rYv8JwASAVq1aOcu2UkqpGnIaGIwxI6vaJiKHRCTZGHPAuvDnVZEuBpgDPGOMWWF37PLSRrGIvAc8Xk0+pmALHqSnp/viKXGllAoInlYlzQbGW5/HA19WTiAiYcAs4ANjzMxK25KtdwGuBzZ7mB+llFIe8jQwvAiMEpFMYKS1jIiki8i7VppbgKHAPQ66pf5bRDYBm4AE4M8e5kcppZSHxBdP1XkqPT3dZGRk+DobSilVr4jIGmNMurN0+uSzUkqpCjQwKKWUqqBeViWJSD6wt4a7JwCHvZid+kLPO7AE6nlD4J67K+fd2hiT6OxA9TIweEJEMlypY2to9LwDS6CeNwTuuXvzvLUqSSmlVAUaGJRSSlUQiIFhiq8z4CN63oElUM8bAvfcvXbeAdfGoJRSqnqBWGJQSilVDQ0MSimlKgiowCAiY0Vkh4hkiYij2ebqNRHZIyKbrPGoMqx1DqdfFZs3rH+LjSLSx7e5d52ITBORPBHZbLfO7fMUkfFW+kwRGe/ou/xJFef9RxHJsRuH7Cq7bU9b571DRMbYra9Xfwcikioii0RkqzVF8K+t9Q36N6/mvGv/NzfGBMQLCMY2QVBbbPM/bAC6+jpfXj7HPUBCpXV/BZ6yPj8FTLI+XwV8CwhwKbDS1/l34zyHAn2AzTU9T6AJsMt6j7c+x/v63Gpw3n8EHneQtqv1fzwcaGP93w+uj38HQDLQx/ocDey0zq9B/+bVnHet/+aBVGLoD2QZY3YZY84CM7BNTdrQjcM27SrW+/V26z8wNiuAuPJh0P2dMWYJcLTSanfPcwwwzxhz1BhzDJiHbU5yv1XFeVdlHDDDGFNsjNkNZGH7G6h3fwfGmAPGmLXW5xPANqAlDfw3r+a8q+K13zyQAkNLYL/dcjbV/yPXRwb4XkTWWDPeQdXTrza0fw93z7Mhnf8jVpXJtPLqFBroeYtIGtAbWEkA/eaVzhtq+TcPpMAQCAYbY/oAVwIPi8hQ+43GVt5s8P2TA+U8LW8B7YBewAHgFZ/mphaJSBTwGfAbY8xx+20N+Td3cN61/psHUmDIAVLtllOsdQ2GMSbHes/DNmtef+CQXJgpz3761Yb27+HueTaI8zfGHDLGlBrbPOrvYPvNoYGdt4iEYrs4/tsY87m1usH/5o7Ouy5+80AKDKuBDiLSRmzTjd6GbWrSBkFEGotIdPlnYDS2qVKrmn51NnC31YPjUqDQrlheH7l7nnOB0SISbxXFR1vr6pVK7UI3cGF63NnAbSISLiJtgA7AKurh34GICDAV2GaM+Zvdpgb9m1d13nXym/u65b0uX9h6K+zE1kL/jK/z4+Vza4utt8EGYEv5+QFNgQVAJjAfaGKtF2Cy9W+xCUj39Tm4ca4fYytCn8NWX3p/Tc4TuA9bA10WcK+vz6uG5/2hdV4brT/2ZLv0z1jnvQO40m59vfo7AAZjqybaCKy3Xlc19N+8mvOu9d9ch8RQSilVQSBVJSmllHKBBgallFIVaGBQSilVgQYGpZRSFWhgUEopVYEGBqWUUhVoYFBKKVXB/wfGkVSUuSm5wAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.1552  # Value for C0\n",
    "K0 = -0.0026  # Value for K0\n",
    "K1 = -0.0004  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0102    # Value for b\n",
    "c = 2.8734    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-175)  var1(t-174)  var1(t-173)  var1(t-172)  var1(t-171)  \\\n",
      "175    90.500000    90.275910    90.051821    89.827731    89.603641   \n",
      "176    90.275910    90.051821    89.827731    89.603641    89.379552   \n",
      "177    90.051821    89.827731    89.603641    89.379552    89.155462   \n",
      "178    89.827731    89.603641    89.379552    89.155462    88.931373   \n",
      "179    89.603641    89.379552    89.155462    88.931373    88.707283   \n",
      "\n",
      "     var1(t-170)  var1(t-169)  var1(t-168)  var1(t-167)  var1(t-166)  ...  \\\n",
      "175    89.379552    89.155462    88.931373    88.707283    88.494958  ...   \n",
      "176    89.155462    88.931373    88.707283    88.494958    88.427731  ...   \n",
      "177    88.931373    88.707283    88.494958    88.427731    88.360504  ...   \n",
      "178    88.707283    88.494958    88.427731    88.360504    88.293277  ...   \n",
      "179    88.494958    88.427731    88.360504    88.293277    88.226050  ...   \n",
      "\n",
      "     var1(t+45)  var2(t+45)  var1(t+46)  var2(t+46)  var1(t+47)  var2(t+47)  \\\n",
      "175   83.025023    0.000280   83.006349    0.000280   82.987675    0.000280   \n",
      "176   83.006349    0.000280   82.987675    0.000280   82.969001    0.000280   \n",
      "177   82.987675    0.000280   82.969001    0.000280   82.950327    0.000279   \n",
      "178   82.969001    0.000280   82.950327    0.000279   82.931653    0.000279   \n",
      "179   82.950327    0.000279   82.931653    0.000279   82.912979    0.000279   \n",
      "\n",
      "     var1(t+48)  var2(t+48)  var1(t+49)  var2(t+49)  \n",
      "175   82.969001    0.000280   82.950327    0.000279  \n",
      "176   82.950327    0.000279   82.931653    0.000279  \n",
      "177   82.931653    0.000279   82.912979    0.000279  \n",
      "178   82.912979    0.000279   82.894304    0.000279  \n",
      "179   82.894304    0.000279   82.875630    0.000279  \n",
      "\n",
      "[5 rows x 276 columns]\n",
      "Index(['var1(t-175)', 'var1(t-174)', 'var1(t-173)', 'var1(t-172)',\n",
      "       'var1(t-171)', 'var1(t-170)', 'var1(t-169)', 'var1(t-168)',\n",
      "       'var1(t-167)', 'var1(t-166)',\n",
      "       ...\n",
      "       'var1(t+45)', 'var2(t+45)', 'var1(t+46)', 'var2(t+46)', 'var1(t+47)',\n",
      "       'var2(t+47)', 'var1(t+48)', 'var2(t+48)', 'var1(t+49)', 'var2(t+49)'],\n",
      "      dtype='object', length=276)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 175, n_out = 50)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 176):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-150:]\n",
    "trainX = train[:,:-150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-150:]\n",
    "forecastX = forecast[:,:-150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225, 1, 126) (2225, 150) (1, 1, 126)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "28/28 [==============================] - 2s 21ms/step - loss: 5211.6626 - val_loss: 3192.6479\n",
      "Epoch 2/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 5008.7451 - val_loss: 3023.6243\n",
      "Epoch 3/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 4865.9888 - val_loss: 2939.9946\n",
      "Epoch 4/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4744.9819 - val_loss: 2865.9280\n",
      "Epoch 5/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4631.5015 - val_loss: 2798.2791\n",
      "Epoch 6/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4523.8086 - val_loss: 2733.3848\n",
      "Epoch 7/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 4419.6421 - val_loss: 2670.6548\n",
      "Epoch 8/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 4318.2695 - val_loss: 2609.7893\n",
      "Epoch 9/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 4219.3052 - val_loss: 2550.6101\n",
      "Epoch 10/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 4122.5156 - val_loss: 2492.9846\n",
      "Epoch 11/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 4027.7385 - val_loss: 2436.7607\n",
      "Epoch 12/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3934.8582 - val_loss: 2381.8135\n",
      "Epoch 13/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3843.8196 - val_loss: 2328.3335\n",
      "Epoch 14/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3754.4868 - val_loss: 2276.0200\n",
      "Epoch 15/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3667.1997 - val_loss: 2224.4492\n",
      "Epoch 16/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3581.0029 - val_loss: 2174.7935\n",
      "Epoch 17/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3496.7112 - val_loss: 2126.0020\n",
      "Epoch 18/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 3400.6802 - val_loss: 2069.8394\n",
      "Epoch 19/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3306.0095 - val_loss: 2020.2377\n",
      "Epoch 20/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3220.0315 - val_loss: 1973.0140\n",
      "Epoch 21/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 3137.0957 - val_loss: 1927.6373\n",
      "Epoch 22/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3056.5437 - val_loss: 1883.8492\n",
      "Epoch 23/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2978.0522 - val_loss: 1841.5090\n",
      "Epoch 24/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2901.4373 - val_loss: 1800.5261\n",
      "Epoch 25/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2826.5796 - val_loss: 1760.8367\n",
      "Epoch 26/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2753.3894 - val_loss: 1722.3906\n",
      "Epoch 27/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2681.8035 - val_loss: 1685.1459\n",
      "Epoch 28/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 2611.7654 - val_loss: 1649.0688\n",
      "Epoch 29/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2543.2302 - val_loss: 1614.1281\n",
      "Epoch 30/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2476.1584 - val_loss: 1580.2974\n",
      "Epoch 31/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2410.5156 - val_loss: 1547.5496\n",
      "Epoch 32/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2346.2690 - val_loss: 1515.8635\n",
      "Epoch 33/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2283.3896 - val_loss: 1485.2147\n",
      "Epoch 34/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2221.8506 - val_loss: 1455.5814\n",
      "Epoch 35/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2161.6265 - val_loss: 1426.9463\n",
      "Epoch 36/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2102.6936 - val_loss: 1399.2892\n",
      "Epoch 37/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 2045.0286 - val_loss: 1372.5918\n",
      "Epoch 38/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1988.6095 - val_loss: 1346.8348\n",
      "Epoch 39/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 1933.4160 - val_loss: 1322.0006\n",
      "Epoch 40/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1879.4274 - val_loss: 1298.0730\n",
      "Epoch 41/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1826.6235 - val_loss: 1275.0352\n",
      "Epoch 42/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1774.9860 - val_loss: 1252.8716\n",
      "Epoch 43/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1724.4954 - val_loss: 1231.5664\n",
      "Epoch 44/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 1675.1342 - val_loss: 1211.1051\n",
      "Epoch 45/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1626.8843 - val_loss: 1191.4696\n",
      "Epoch 46/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1579.7285 - val_loss: 1172.6448\n",
      "Epoch 47/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1533.6494 - val_loss: 1154.6151\n",
      "Epoch 48/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1488.6304 - val_loss: 1137.3660\n",
      "Epoch 49/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1444.6548 - val_loss: 1120.8822\n",
      "Epoch 50/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1401.7063 - val_loss: 1105.1494\n",
      "Epoch 51/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1359.7697 - val_loss: 1090.1532\n",
      "Epoch 52/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1318.8281 - val_loss: 1075.8794\n",
      "Epoch 53/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1278.8668 - val_loss: 1062.3135\n",
      "Epoch 54/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1239.8698 - val_loss: 1049.4418\n",
      "Epoch 55/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1201.8219 - val_loss: 1037.2500\n",
      "Epoch 56/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1164.7085 - val_loss: 1025.7245\n",
      "Epoch 57/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1128.5146 - val_loss: 1014.8517\n",
      "Epoch 58/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1093.2253 - val_loss: 1004.6177\n",
      "Epoch 59/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1058.8264 - val_loss: 995.0095\n",
      "Epoch 60/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1025.3031 - val_loss: 986.0134\n",
      "Epoch 61/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 992.6416 - val_loss: 977.6158\n",
      "Epoch 62/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 960.8271 - val_loss: 969.8041\n",
      "Epoch 63/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 929.8459 - val_loss: 962.5648\n",
      "Epoch 64/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 899.6842 - val_loss: 955.8846\n",
      "Epoch 65/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 870.3284 - val_loss: 949.7511\n",
      "Epoch 66/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 841.7645 - val_loss: 944.1510\n",
      "Epoch 67/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 813.9789 - val_loss: 939.0717\n",
      "Epoch 68/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 786.9587 - val_loss: 934.5001\n",
      "Epoch 69/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 760.6897 - val_loss: 930.4239\n",
      "Epoch 70/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 735.1592 - val_loss: 926.8303\n",
      "Epoch 71/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 710.3539 - val_loss: 923.7066\n",
      "Epoch 72/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 686.2608 - val_loss: 921.0407\n",
      "Epoch 73/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 662.8669 - val_loss: 918.8199\n",
      "Epoch 74/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 640.1591 - val_loss: 917.0320\n",
      "Epoch 75/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 618.1249 - val_loss: 915.6647\n",
      "Epoch 76/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 596.7516 - val_loss: 914.7057\n",
      "Epoch 77/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 576.0262 - val_loss: 914.1428\n",
      "Epoch 78/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 555.9367 - val_loss: 913.9642\n",
      "Epoch 79/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 536.4703 - val_loss: 914.1577\n",
      "Epoch 80/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 517.6146 - val_loss: 914.7113\n",
      "Epoch 81/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 499.3574 - val_loss: 915.6133\n",
      "Epoch 82/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 481.6866 - val_loss: 916.8519\n",
      "Epoch 83/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 464.5898 - val_loss: 918.4153\n",
      "Epoch 84/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 448.0553 - val_loss: 920.2917\n",
      "Epoch 85/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 432.0710 - val_loss: 922.4700\n",
      "Epoch 86/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 416.6249 - val_loss: 924.9382\n",
      "Epoch 87/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 401.7055 - val_loss: 927.6852\n",
      "Epoch 88/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 387.3008 - val_loss: 930.6996\n",
      "Epoch 89/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 373.3991 - val_loss: 933.9698\n",
      "Epoch 90/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 359.9892 - val_loss: 937.4855\n",
      "Epoch 91/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 347.0595 - val_loss: 941.2348\n",
      "Epoch 92/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 334.5986 - val_loss: 945.2072\n",
      "Epoch 93/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 322.5953 - val_loss: 949.3916\n",
      "Epoch 94/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 311.0383 - val_loss: 953.7772\n",
      "Epoch 95/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 299.9167 - val_loss: 958.3536\n",
      "Epoch 96/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 289.2195 - val_loss: 963.1101\n",
      "Epoch 97/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 278.9358 - val_loss: 968.0361\n",
      "Epoch 98/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 269.0548 - val_loss: 973.1218\n",
      "Epoch 99/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 259.5655 - val_loss: 978.3565\n",
      "Epoch 100/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 250.4577 - val_loss: 983.7305\n",
      "Epoch 101/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 241.7208 - val_loss: 989.2333\n",
      "Epoch 102/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 233.3445 - val_loss: 994.8558\n",
      "Epoch 103/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 225.3185 - val_loss: 1000.5881\n",
      "Epoch 104/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 217.6327 - val_loss: 1006.4202\n",
      "Epoch 105/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 210.2770 - val_loss: 1012.3434\n",
      "Epoch 106/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 203.2414 - val_loss: 1018.3481\n",
      "Epoch 107/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 196.5165 - val_loss: 1024.4257\n",
      "Epoch 108/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 190.0921 - val_loss: 1030.5665\n",
      "Epoch 109/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 183.9593 - val_loss: 1036.7622\n",
      "Epoch 110/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 178.1084 - val_loss: 1043.0044\n",
      "Epoch 111/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 172.5301 - val_loss: 1049.2844\n",
      "Epoch 112/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 167.2154 - val_loss: 1055.5944\n",
      "Epoch 113/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 162.1553 - val_loss: 1061.9261\n",
      "Epoch 114/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 157.3408 - val_loss: 1068.2717\n",
      "Epoch 115/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 152.7634 - val_loss: 1074.6240\n",
      "Epoch 116/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 148.4145 - val_loss: 1080.9752\n",
      "Epoch 117/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 144.2857 - val_loss: 1087.3180\n",
      "Epoch 118/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 140.3690 - val_loss: 1093.6456\n",
      "Epoch 119/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 136.6560 - val_loss: 1099.9517\n",
      "Epoch 120/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 133.1389 - val_loss: 1106.2286\n",
      "Epoch 121/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 129.8101 - val_loss: 1112.4712\n",
      "Epoch 122/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 126.6618 - val_loss: 1118.6729\n",
      "Epoch 123/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 123.6868 - val_loss: 1124.8275\n",
      "Epoch 124/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 120.8777 - val_loss: 1130.9298\n",
      "Epoch 125/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 118.2276 - val_loss: 1136.9742\n",
      "Epoch 126/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 115.7294 - val_loss: 1142.9558\n",
      "Epoch 127/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 113.3766 - val_loss: 1148.8699\n",
      "Epoch 128/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 111.1624 - val_loss: 1154.7112\n",
      "Epoch 129/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 109.0807 - val_loss: 1160.4760\n",
      "Epoch 130/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 107.1250 - val_loss: 1166.1592\n",
      "Epoch 131/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 105.2897 - val_loss: 1171.7574\n",
      "Epoch 132/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 103.5686 - val_loss: 1177.2673\n",
      "Epoch 133/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 101.9564 - val_loss: 1182.6851\n",
      "Epoch 134/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 100.4473 - val_loss: 1188.0076\n",
      "Epoch 135/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 99.0362 - val_loss: 1193.2316\n",
      "Epoch 136/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 97.7179 - val_loss: 1198.3551\n",
      "Epoch 137/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 96.4876 - val_loss: 1203.3750\n",
      "Epoch 138/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 95.3404 - val_loss: 1208.2896\n",
      "Epoch 139/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 94.2718 - val_loss: 1213.0967\n",
      "Epoch 140/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 93.2774 - val_loss: 1217.7947\n",
      "Epoch 141/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 92.3530 - val_loss: 1222.3812\n",
      "Epoch 142/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 91.4945 - val_loss: 1226.8567\n",
      "Epoch 143/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 90.6979 - val_loss: 1231.2194\n",
      "Epoch 144/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 89.9595 - val_loss: 1235.4675\n",
      "Epoch 145/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 89.2759 - val_loss: 1239.6011\n",
      "Epoch 146/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 88.6435 - val_loss: 1243.6200\n",
      "Epoch 147/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 88.0593 - val_loss: 1247.5233\n",
      "Epoch 148/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 87.5200 - val_loss: 1251.3118\n",
      "Epoch 149/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 87.0227 - val_loss: 1254.9847\n",
      "Epoch 150/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 86.5646 - val_loss: 1258.5424\n",
      "Epoch 151/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 86.1431 - val_loss: 1261.9854\n",
      "Epoch 152/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.7558 - val_loss: 1265.3145\n",
      "Epoch 153/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.4000 - val_loss: 1268.5297\n",
      "Epoch 154/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.0737 - val_loss: 1271.6318\n",
      "Epoch 155/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 84.7747 - val_loss: 1274.6226\n",
      "Epoch 156/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 84.5010 - val_loss: 1277.5006\n",
      "Epoch 157/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 84.2507 - val_loss: 1280.2670\n",
      "Epoch 158/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 84.0222 - val_loss: 1282.9215\n",
      "Epoch 159/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.8137 - val_loss: 1285.4652\n",
      "Epoch 160/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.6236 - val_loss: 1287.8936\n",
      "Epoch 161/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 83.4505 - val_loss: 1290.2031\n",
      "Epoch 162/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.2931 - val_loss: 1292.3823\n",
      "Epoch 163/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 83.1500 - val_loss: 1294.3990\n",
      "Epoch 164/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.0199 - val_loss: 1296.1062\n",
      "Epoch 165/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.8684 - val_loss: 1258.8871\n",
      "Epoch 166/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.1387 - val_loss: 1302.7139\n",
      "Epoch 167/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 82.6976 - val_loss: 1304.5598\n",
      "Epoch 168/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.6103 - val_loss: 1306.3208\n",
      "Epoch 169/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.5315 - val_loss: 1307.9990\n",
      "Epoch 170/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.4605 - val_loss: 1309.5988\n",
      "Epoch 171/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 82.3964 - val_loss: 1311.1207\n",
      "Epoch 172/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.3388 - val_loss: 1312.5699\n",
      "Epoch 173/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 82.2869 - val_loss: 1313.9469\n",
      "Epoch 174/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 82.2402 - val_loss: 1315.2546\n",
      "Epoch 175/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 82.1983 - val_loss: 1316.4950\n",
      "Epoch 176/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 82.1606 - val_loss: 1317.6713\n",
      "Epoch 177/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 82.1269 - val_loss: 1318.7848\n",
      "Epoch 178/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 82.0966 - val_loss: 1319.8392\n",
      "Epoch 179/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 82.0696 - val_loss: 1320.8359\n",
      "Epoch 180/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 82.0454 - val_loss: 1321.7771\n",
      "Epoch 181/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 82.0238 - val_loss: 1322.6663\n",
      "Epoch 182/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.0045 - val_loss: 1323.5039\n",
      "Epoch 183/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9873 - val_loss: 1324.2932\n",
      "Epoch 184/500\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 81.9720 - val_loss: 1325.0365\n",
      "Epoch 185/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.9584 - val_loss: 1325.7357\n",
      "Epoch 186/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.9463 - val_loss: 1326.3921\n",
      "Epoch 187/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.9356 - val_loss: 1327.0088\n",
      "Epoch 188/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.9260 - val_loss: 1327.5872\n",
      "Epoch 189/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9176 - val_loss: 1328.1287\n",
      "Epoch 190/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9102 - val_loss: 1328.6362\n",
      "Epoch 191/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9037 - val_loss: 1329.1104\n",
      "Epoch 192/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8979 - val_loss: 1329.5541\n",
      "Epoch 193/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8929 - val_loss: 1329.9684\n",
      "Epoch 194/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8885 - val_loss: 1330.3545\n",
      "Epoch 195/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8846 - val_loss: 1330.7140\n",
      "Epoch 196/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8813 - val_loss: 1331.0482\n",
      "Epoch 197/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8785 - val_loss: 1331.3597\n",
      "Epoch 198/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8760 - val_loss: 1331.6486\n",
      "Epoch 199/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8739 - val_loss: 1331.9160\n",
      "Epoch 200/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8722 - val_loss: 1332.1644\n",
      "Epoch 201/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8707 - val_loss: 1332.3938\n",
      "Epoch 202/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8696 - val_loss: 1332.6064\n",
      "Epoch 203/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8687 - val_loss: 1332.8025\n",
      "Epoch 204/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8679 - val_loss: 1332.9838\n",
      "Epoch 205/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8674 - val_loss: 1333.1499\n",
      "Epoch 206/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8670 - val_loss: 1333.3031\n",
      "Epoch 207/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8668 - val_loss: 1333.4435\n",
      "Epoch 208/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8668 - val_loss: 1333.5724\n",
      "Epoch 209/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8668 - val_loss: 1333.6899\n",
      "Epoch 210/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8670 - val_loss: 1333.7976\n",
      "Epoch 211/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 81.8673 - val_loss: 1333.8954\n",
      "Epoch 212/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8676 - val_loss: 1333.9840\n",
      "Epoch 213/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8681 - val_loss: 1334.0652\n",
      "Epoch 214/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8687 - val_loss: 1334.1388\n",
      "Epoch 215/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8692 - val_loss: 1334.2050\n",
      "Epoch 216/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8699 - val_loss: 1334.2640\n",
      "Epoch 217/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8706 - val_loss: 1334.3179\n",
      "Epoch 218/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8714 - val_loss: 1334.3651\n",
      "Epoch 219/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8722 - val_loss: 1334.4077\n",
      "Epoch 220/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8730 - val_loss: 1334.4452\n",
      "Epoch 221/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8739 - val_loss: 1334.4781\n",
      "Epoch 222/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8747 - val_loss: 1334.5063\n",
      "Epoch 223/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8756 - val_loss: 1334.5309\n",
      "Epoch 224/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8766 - val_loss: 1334.5516\n",
      "Epoch 225/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8775 - val_loss: 1334.5692\n",
      "Epoch 226/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8785 - val_loss: 1334.5834\n",
      "Epoch 227/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8795 - val_loss: 1334.5951\n",
      "Epoch 228/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8804 - val_loss: 1334.6039\n",
      "Epoch 229/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8814 - val_loss: 1334.6104\n",
      "Epoch 230/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8824 - val_loss: 1334.6146\n",
      "Epoch 231/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8835 - val_loss: 1334.6169\n",
      "Epoch 232/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8844 - val_loss: 1334.6165\n",
      "Epoch 233/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8854 - val_loss: 1334.6146\n",
      "Epoch 234/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8865 - val_loss: 1334.6113\n",
      "Epoch 235/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8874 - val_loss: 1334.6069\n",
      "Epoch 236/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8884 - val_loss: 1334.5991\n",
      "Epoch 237/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 81.8895 - val_loss: 1334.5917\n",
      "Epoch 238/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 81.8904 - val_loss: 1334.5823\n",
      "Epoch 239/500\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 81.8913 - val_loss: 1334.5713\n",
      "Epoch 240/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8924 - val_loss: 1334.5597\n",
      "Epoch 241/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8933 - val_loss: 1334.5468\n",
      "Epoch 242/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8942 - val_loss: 1334.5320\n",
      "Epoch 243/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8951 - val_loss: 1334.5171\n",
      "Epoch 244/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8955 - val_loss: 1334.2129\n",
      "Epoch 245/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.0831 - val_loss: 1334.8630\n",
      "Epoch 246/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 81.9391 - val_loss: 1334.8541\n",
      "Epoch 247/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.9267 - val_loss: 1334.7959\n",
      "Epoch 248/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9229 - val_loss: 1334.7361\n",
      "Epoch 249/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9210 - val_loss: 1334.6735\n",
      "Epoch 250/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.9201 - val_loss: 1334.6146\n",
      "Epoch 251/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.9196 - val_loss: 1334.5573\n",
      "Epoch 252/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.9195 - val_loss: 1334.5032\n",
      "Epoch 253/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9195 - val_loss: 1334.4542\n",
      "Epoch 254/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9198 - val_loss: 1334.4065\n",
      "Epoch 255/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.9200 - val_loss: 1334.3567\n",
      "Epoch 256/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.9203 - val_loss: 1334.3107\n",
      "Epoch 257/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9208 - val_loss: 1334.2631\n",
      "Epoch 258/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.9212 - val_loss: 1334.2184\n",
      "Epoch 259/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.9217 - val_loss: 1334.1713\n",
      "Epoch 260/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.9222 - val_loss: 1334.1267\n",
      "Epoch 261/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9227 - val_loss: 1334.0833\n",
      "Epoch 262/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.9231 - val_loss: 1334.0376\n",
      "Epoch 263/500\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 81.9236 - val_loss: 1333.9868\n",
      "Epoch 264/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 81.9241 - val_loss: 1333.9323\n",
      "Epoch 265/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.9246 - val_loss: 1333.8735\n",
      "Epoch 266/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.9251 - val_loss: 1333.8190\n",
      "Epoch 267/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9249 - val_loss: 1333.6313\n",
      "Epoch 268/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8754 - val_loss: 1330.1589\n",
      "Epoch 269/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 84.5610 - val_loss: 1260.9979\n",
      "Epoch 270/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 85.7242 - val_loss: 1267.8779\n",
      "Epoch 271/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.0433 - val_loss: 1274.2096\n",
      "Epoch 272/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 84.4772 - val_loss: 1279.9180\n",
      "Epoch 273/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 84.0178 - val_loss: 1285.0374\n",
      "Epoch 274/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.6452 - val_loss: 1289.6195\n",
      "Epoch 275/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.3423 - val_loss: 1293.6991\n",
      "Epoch 276/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 83.0959 - val_loss: 1297.2753\n",
      "Epoch 277/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 82.8948 - val_loss: 1299.9283\n",
      "Epoch 278/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.5667 - val_loss: 1280.2035\n",
      "Epoch 279/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 89.2864 - val_loss: 1238.8372\n",
      "Epoch 280/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 88.1895 - val_loss: 1247.5247\n",
      "Epoch 281/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 87.0490 - val_loss: 1255.5045\n",
      "Epoch 282/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 86.1115 - val_loss: 1262.7163\n",
      "Epoch 283/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 85.3511 - val_loss: 1269.2214\n",
      "Epoch 284/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 84.7341 - val_loss: 1275.0795\n",
      "Epoch 285/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 84.2328 - val_loss: 1280.3467\n",
      "Epoch 286/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 83.8250 - val_loss: 1285.0726\n",
      "Epoch 287/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 83.4927 - val_loss: 1289.3041\n",
      "Epoch 288/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 83.2214 - val_loss: 1293.0750\n",
      "Epoch 289/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.9997 - val_loss: 1296.4259\n",
      "Epoch 290/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.8182 - val_loss: 1299.3809\n",
      "Epoch 291/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.6693 - val_loss: 1301.4073\n",
      "Epoch 292/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.6126 - val_loss: 1304.2104\n",
      "Epoch 293/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.4456 - val_loss: 1306.0175\n",
      "Epoch 294/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.3623 - val_loss: 1307.2743\n",
      "Epoch 295/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.2933 - val_loss: 1307.2573\n",
      "Epoch 296/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.2319 - val_loss: 1287.5430\n",
      "Epoch 297/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.4186 - val_loss: 1332.6620\n",
      "Epoch 298/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.1482 - val_loss: 1333.9993\n",
      "Epoch 299/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.1149 - val_loss: 1335.1937\n",
      "Epoch 300/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.0872 - val_loss: 1336.2628\n",
      "Epoch 301/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.0639 - val_loss: 1337.2191\n",
      "Epoch 302/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.0443 - val_loss: 1338.0740\n",
      "Epoch 303/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.0276 - val_loss: 1338.8389\n",
      "Epoch 304/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.0135 - val_loss: 1339.5225\n",
      "Epoch 305/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.0014 - val_loss: 1340.1333\n",
      "Epoch 306/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9912 - val_loss: 1340.6782\n",
      "Epoch 307/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9824 - val_loss: 1341.1649\n",
      "Epoch 308/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9749 - val_loss: 1341.5997\n",
      "Epoch 309/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9685 - val_loss: 1341.9879\n",
      "Epoch 310/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9629 - val_loss: 1342.3336\n",
      "Epoch 311/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 81.9582 - val_loss: 1342.6433\n",
      "Epoch 312/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.9540 - val_loss: 1342.9186\n",
      "Epoch 313/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.9504 - val_loss: 1343.1638\n",
      "Epoch 314/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9474 - val_loss: 1343.3843\n",
      "Epoch 315/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9446 - val_loss: 1343.5806\n",
      "Epoch 316/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9423 - val_loss: 1343.7548\n",
      "Epoch 317/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9402 - val_loss: 1343.9097\n",
      "Epoch 318/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9385 - val_loss: 1344.0483\n",
      "Epoch 319/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9369 - val_loss: 1344.1709\n",
      "Epoch 320/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9356 - val_loss: 1344.2802\n",
      "Epoch 321/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9345 - val_loss: 1344.3777\n",
      "Epoch 322/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9335 - val_loss: 1344.4646\n",
      "Epoch 323/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9326 - val_loss: 1344.5421\n",
      "Epoch 324/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.9318 - val_loss: 1344.6107\n",
      "Epoch 325/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9312 - val_loss: 1344.6719\n",
      "Epoch 326/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9307 - val_loss: 1344.7269\n",
      "Epoch 327/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9301 - val_loss: 1344.7751\n",
      "Epoch 328/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9297 - val_loss: 1344.8179\n",
      "Epoch 329/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9294 - val_loss: 1344.8561\n",
      "Epoch 330/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9290 - val_loss: 1344.8892\n",
      "Epoch 331/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9288 - val_loss: 1344.9191\n",
      "Epoch 332/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9286 - val_loss: 1344.9453\n",
      "Epoch 333/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9284 - val_loss: 1344.9684\n",
      "Epoch 334/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 81.9283 - val_loss: 1344.9894\n",
      "Epoch 335/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9281 - val_loss: 1345.0073\n",
      "Epoch 336/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.9280 - val_loss: 1345.0229\n",
      "Epoch 337/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9280 - val_loss: 1345.0365\n",
      "Epoch 338/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9279 - val_loss: 1345.0494\n",
      "Epoch 339/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9278 - val_loss: 1345.0597\n",
      "Epoch 340/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9278 - val_loss: 1345.0692\n",
      "Epoch 341/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9278 - val_loss: 1345.0767\n",
      "Epoch 342/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9278 - val_loss: 1345.0836\n",
      "Epoch 343/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9279 - val_loss: 1345.0901\n",
      "Epoch 344/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9279 - val_loss: 1345.0958\n",
      "Epoch 345/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9279 - val_loss: 1345.1000\n",
      "Epoch 346/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9280 - val_loss: 1345.1036\n",
      "Epoch 347/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9279 - val_loss: 1345.1064\n",
      "Epoch 348/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9280 - val_loss: 1345.1091\n",
      "Epoch 349/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9280 - val_loss: 1345.1110\n",
      "Epoch 350/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9281 - val_loss: 1345.1132\n",
      "Epoch 351/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9281 - val_loss: 1345.1141\n",
      "Epoch 352/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9282 - val_loss: 1345.1149\n",
      "Epoch 353/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9282 - val_loss: 1345.1162\n",
      "Epoch 354/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9283 - val_loss: 1345.1162\n",
      "Epoch 355/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9283 - val_loss: 1345.1155\n",
      "Epoch 356/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9284 - val_loss: 1345.1156\n",
      "Epoch 357/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 81.9285 - val_loss: 1345.1160\n",
      "Epoch 358/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9285 - val_loss: 1345.1147\n",
      "Epoch 359/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9285 - val_loss: 1345.1136\n",
      "Epoch 360/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9286 - val_loss: 1345.1127\n",
      "Epoch 361/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9286 - val_loss: 1345.1108\n",
      "Epoch 362/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9287 - val_loss: 1345.1091\n",
      "Epoch 363/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9288 - val_loss: 1345.1080\n",
      "Epoch 364/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9288 - val_loss: 1345.1064\n",
      "Epoch 365/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9288 - val_loss: 1345.1046\n",
      "Epoch 366/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9289 - val_loss: 1345.1027\n",
      "Epoch 367/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9289 - val_loss: 1345.1003\n",
      "Epoch 368/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9290 - val_loss: 1345.0978\n",
      "Epoch 369/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9290 - val_loss: 1345.0953\n",
      "Epoch 370/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9291 - val_loss: 1345.0931\n",
      "Epoch 371/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9292 - val_loss: 1345.0898\n",
      "Epoch 372/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9292 - val_loss: 1345.0870\n",
      "Epoch 373/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.9292 - val_loss: 1345.0841\n",
      "Epoch 374/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9293 - val_loss: 1345.0814\n",
      "Epoch 375/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9293 - val_loss: 1345.0781\n",
      "Epoch 376/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9294 - val_loss: 1345.0748\n",
      "Epoch 377/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9294 - val_loss: 1345.0710\n",
      "Epoch 378/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9294 - val_loss: 1345.0663\n",
      "Epoch 379/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9295 - val_loss: 1345.0624\n",
      "Epoch 380/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 81.9296 - val_loss: 1345.0588\n",
      "Epoch 381/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9296 - val_loss: 1345.0553\n",
      "Epoch 382/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9296 - val_loss: 1345.0513\n",
      "Epoch 383/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9296 - val_loss: 1345.0466\n",
      "Epoch 384/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9296 - val_loss: 1345.0414\n",
      "Epoch 385/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9297 - val_loss: 1345.0365\n",
      "Epoch 386/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9297 - val_loss: 1345.0306\n",
      "Epoch 387/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9297 - val_loss: 1345.0242\n",
      "Epoch 388/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9297 - val_loss: 1345.0171\n",
      "Epoch 389/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9298 - val_loss: 1345.0110\n",
      "Epoch 390/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9298 - val_loss: 1345.0028\n",
      "Epoch 391/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9299 - val_loss: 1344.9944\n",
      "Epoch 392/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9299 - val_loss: 1344.9860\n",
      "Epoch 393/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9299 - val_loss: 1344.9764\n",
      "Epoch 394/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9299 - val_loss: 1344.9662\n",
      "Epoch 395/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9300 - val_loss: 1344.9554\n",
      "Epoch 396/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9300 - val_loss: 1344.9437\n",
      "Epoch 397/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9300 - val_loss: 1344.9302\n",
      "Epoch 398/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9301 - val_loss: 1344.9154\n",
      "Epoch 399/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9301 - val_loss: 1344.8997\n",
      "Epoch 400/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9301 - val_loss: 1344.8821\n",
      "Epoch 401/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9301 - val_loss: 1344.8605\n",
      "Epoch 402/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 81.9301 - val_loss: 1344.8359\n",
      "Epoch 403/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9301 - val_loss: 1344.8065\n",
      "Epoch 404/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9301 - val_loss: 1344.7725\n",
      "Epoch 405/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9302 - val_loss: 1344.7313\n",
      "Epoch 406/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9302 - val_loss: 1344.6809\n",
      "Epoch 407/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9302 - val_loss: 1344.6173\n",
      "Epoch 408/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9302 - val_loss: 1344.5330\n",
      "Epoch 409/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9303 - val_loss: 1344.4189\n",
      "Epoch 410/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9302 - val_loss: 1344.2515\n",
      "Epoch 411/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9303 - val_loss: 1343.9724\n",
      "Epoch 412/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9303 - val_loss: 1343.2783\n",
      "Epoch 413/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9109 - val_loss: 1322.6274\n",
      "Epoch 414/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 86.0517 - val_loss: 1345.9017\n",
      "Epoch 415/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9267 - val_loss: 1345.8723\n",
      "Epoch 416/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9232 - val_loss: 1345.7993\n",
      "Epoch 417/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9237 - val_loss: 1345.7314\n",
      "Epoch 418/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9245 - val_loss: 1345.6707\n",
      "Epoch 419/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9251 - val_loss: 1345.6163\n",
      "Epoch 420/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9257 - val_loss: 1345.5665\n",
      "Epoch 421/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9261 - val_loss: 1345.5209\n",
      "Epoch 422/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 81.9266 - val_loss: 1345.4788\n",
      "Epoch 423/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9271 - val_loss: 1345.4397\n",
      "Epoch 424/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 81.9274 - val_loss: 1345.4023\n",
      "Epoch 425/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9278 - val_loss: 1345.3641\n",
      "Epoch 426/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9280 - val_loss: 1345.3175\n",
      "Epoch 427/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9283 - val_loss: 1345.2319\n",
      "Epoch 428/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9285 - val_loss: 1344.1438\n",
      "Epoch 429/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.0053 - val_loss: 1343.8418\n",
      "Epoch 430/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.2119 - val_loss: 1346.0809\n",
      "Epoch 431/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9277 - val_loss: 1346.0502\n",
      "Epoch 432/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9265 - val_loss: 1345.9994\n",
      "Epoch 433/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9269 - val_loss: 1345.9456\n",
      "Epoch 434/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9270 - val_loss: 1342.2740\n",
      "Epoch 435/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.9153 - val_loss: 1346.2502\n",
      "Epoch 436/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9270 - val_loss: 1346.2191\n",
      "Epoch 437/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9264 - val_loss: 1346.1792\n",
      "Epoch 438/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9268 - val_loss: 1346.1438\n",
      "Epoch 439/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9271 - val_loss: 1346.1113\n",
      "Epoch 440/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9276 - val_loss: 1346.0840\n",
      "Epoch 441/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9279 - val_loss: 1346.0582\n",
      "Epoch 442/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9282 - val_loss: 1346.0358\n",
      "Epoch 443/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 81.9284 - val_loss: 1346.0160\n",
      "Epoch 444/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9286 - val_loss: 1345.9979\n",
      "Epoch 445/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.9289 - val_loss: 1345.9822\n",
      "Epoch 446/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9290 - val_loss: 1345.9679\n",
      "Epoch 447/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9292 - val_loss: 1345.9552\n",
      "Epoch 448/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9293 - val_loss: 1345.9434\n",
      "Epoch 449/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9294 - val_loss: 1345.9335\n",
      "Epoch 450/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9296 - val_loss: 1345.9242\n",
      "Epoch 451/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9297 - val_loss: 1345.9156\n",
      "Epoch 452/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9298 - val_loss: 1345.9078\n",
      "Epoch 453/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9299 - val_loss: 1345.9017\n",
      "Epoch 454/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9300 - val_loss: 1345.8953\n",
      "Epoch 455/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9300 - val_loss: 1345.8899\n",
      "Epoch 456/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9301 - val_loss: 1345.8843\n",
      "Epoch 457/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9301 - val_loss: 1345.8794\n",
      "Epoch 458/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9301 - val_loss: 1345.8756\n",
      "Epoch 459/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9303 - val_loss: 1345.8713\n",
      "Epoch 460/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9303 - val_loss: 1345.8685\n",
      "Epoch 461/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9303 - val_loss: 1345.8651\n",
      "Epoch 462/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 81.9304 - val_loss: 1345.8627\n",
      "Epoch 463/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9305 - val_loss: 1345.8607\n",
      "Epoch 464/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9305 - val_loss: 1345.8589\n",
      "Epoch 465/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9305 - val_loss: 1345.8571\n",
      "Epoch 466/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9305 - val_loss: 1345.8545\n",
      "Epoch 467/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9305 - val_loss: 1345.8528\n",
      "Epoch 468/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9305 - val_loss: 1345.8510\n",
      "Epoch 469/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9306 - val_loss: 1345.8492\n",
      "Epoch 470/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9306 - val_loss: 1345.8469\n",
      "Epoch 471/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9306 - val_loss: 1345.8452\n",
      "Epoch 472/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9306 - val_loss: 1345.8431\n",
      "Epoch 473/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9306 - val_loss: 1345.8412\n",
      "Epoch 474/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9306 - val_loss: 1345.8390\n",
      "Epoch 475/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9306 - val_loss: 1345.8367\n",
      "Epoch 476/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9306 - val_loss: 1345.8346\n",
      "Epoch 477/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9306 - val_loss: 1345.8323\n",
      "Epoch 478/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9306 - val_loss: 1345.8300\n",
      "Epoch 479/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9307 - val_loss: 1345.8273\n",
      "Epoch 480/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9307 - val_loss: 1345.8247\n",
      "Epoch 481/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9307 - val_loss: 1345.8217\n",
      "Epoch 482/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 81.9307 - val_loss: 1345.8179\n",
      "Epoch 483/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9307 - val_loss: 1345.8116\n",
      "Epoch 484/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9307 - val_loss: 1345.7809\n",
      "Epoch 485/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9171 - val_loss: 1343.5933\n",
      "Epoch 486/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.1602 - val_loss: 1344.9941\n",
      "Epoch 487/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9864 - val_loss: 1345.1621\n",
      "Epoch 488/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9711 - val_loss: 1345.2517\n",
      "Epoch 489/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9649 - val_loss: 1345.3252\n",
      "Epoch 490/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9609 - val_loss: 1345.3873\n",
      "Epoch 491/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.9580 - val_loss: 1345.4418\n",
      "Epoch 492/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.9557 - val_loss: 1345.4891\n",
      "Epoch 493/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9539 - val_loss: 1345.5314\n",
      "Epoch 494/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 81.9524 - val_loss: 1345.5679\n",
      "Epoch 495/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9511 - val_loss: 1345.5999\n",
      "Epoch 496/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.9500 - val_loss: 1345.6279\n",
      "Epoch 497/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9491 - val_loss: 1345.6539\n",
      "Epoch 498/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9481 - val_loss: 1345.6748\n",
      "Epoch 499/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 81.9475 - val_loss: 1345.6941\n",
      "Epoch 500/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9469 - val_loss: 1345.7124\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.1552, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0026, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0004, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0102, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.8734, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 355ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 126)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 126)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 186)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:150:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , 69.3528478 , 68.9494865 , 68.5461251 , 68.2508637 ,\n",
       "        68.1248133 , 67.9987628 , 67.8727124 , 67.693324  , 67.4412232 ,\n",
       "         0.95224452,  0.        , 68.1761671 , 68.0501167 , 67.9240663 ,\n",
       "        67.7960317 , 67.6200606 , 67.4401344 , 67.2602082 , 67.080282  ,\n",
       "        66.9003558 , 74.5072708 ,  0.        , 62.54023361,  0.        ,\n",
       "         0.52089834,  0.92265719,  0.94453633,  0.20587075,  0.23263276,\n",
       "        53.12518311,  0.        ,  0.        ,  0.71475446,  0.09016362,\n",
       "         0.        ,  0.29408324,  0.28857082,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.25247416,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.64641154]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([52.86327116, 52.8476942 , 52.83211724, 52.81654027, 52.80096331,\n",
       "       52.78538635, 52.76980939, 52.75423242, 52.73865546, 52.7230785 ,\n",
       "       52.70750154, 52.69192457, 52.67634761, 52.66077065, 52.64519369,\n",
       "       52.62961672, 52.61403976, 52.5984628 , 52.58288584, 52.56730887,\n",
       "       52.55173191, 52.53615495, 52.52057799, 52.50500102, 52.48942406,\n",
       "       52.4738471 , 52.45827014, 52.44269317, 52.42711621, 52.41153925,\n",
       "       52.39596229, 52.38038532, 52.36480836, 52.3492314 , 52.33365444,\n",
       "       52.31807747, 52.30250051, 52.28692355, 52.27134659, 52.25576962,\n",
       "       52.24019266, 52.2246157 , 52.20903874, 52.19346177, 52.17788481,\n",
       "       52.16230785, 52.14673089, 52.13115392, 52.11557696, 52.1       ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.887717078686876\n",
      "36.372056877776814\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
