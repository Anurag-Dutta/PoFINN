{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "2245    56.056548\n",
       "2246    56.040972\n",
       "2247    56.025395\n",
       "2248    56.009818\n",
       "2249    55.994241\n",
       "Name: C2, Length: 2250, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c2_interpolated_2150_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "2145     0.000000\n",
       "2146     0.387256\n",
       "2147     0.224838\n",
       "2148     0.000000\n",
       "2149     0.000000\n",
       "Name: C2, Length: 2150, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2150)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.500000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.275910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.051821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.827731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.603641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     90.500000  0.000298\n",
       "1     90.275910  0.000298\n",
       "2     90.051821  0.000297\n",
       "3     89.827731  0.000297\n",
       "4     89.603641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.1552  # Value for C0\n",
    "K0 = -0.0026  # Value for K0\n",
    "K1 = -0.0004  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0102    # Value for b\n",
    "c = 2.8734    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    90.500000    90.275910    90.051821    89.827731    89.603641   \n",
      "351    90.275910    90.051821    89.827731    89.603641    89.379552   \n",
      "352    90.051821    89.827731    89.603641    89.379552    89.155462   \n",
      "353    89.827731    89.603641    89.379552    89.155462    88.931373   \n",
      "354    89.603641    89.379552    89.155462    88.931373    88.707283   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    89.379552    89.155462    88.931373    88.707283    88.494958  ...   \n",
      "351    89.155462    88.931373    88.707283    88.494958    88.427731  ...   \n",
      "352    88.931373    88.707283    88.494958    88.427731    88.360504  ...   \n",
      "353    88.707283    88.494958    88.427731    88.360504    88.293277  ...   \n",
      "354    88.494958    88.427731    88.360504    88.293277    88.226050  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   79.071008    0.000263   79.054202    0.000263   79.037395    0.000263   \n",
      "351   79.054202    0.000263   79.037395    0.000263   79.020588    0.000262   \n",
      "352   79.037395    0.000263   79.020588    0.000262   79.003782    0.000262   \n",
      "353   79.020588    0.000262   79.003782    0.000262   78.986975    0.000262   \n",
      "354   79.003782    0.000262   78.986975    0.000262   78.970168    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   79.020588    0.000262   79.003782    0.000262  \n",
      "351   79.003782    0.000262   78.986975    0.000262  \n",
      "352   78.986975    0.000262   78.970168    0.000262  \n",
      "353   78.970168    0.000262   78.953361    0.000262  \n",
      "354   78.953361    0.000262   78.936555    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1700, 1, 251) (1700, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 2s 22ms/step - loss: 5368.3560 - val_loss: 3988.6619\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5292.3984 - val_loss: 3940.3328\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 5224.1050 - val_loss: 3885.2712\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5154.6406 - val_loss: 3830.1812\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5085.8140 - val_loss: 3778.0342\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5002.5610 - val_loss: 3709.0886\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 4925.4629 - val_loss: 3652.1809\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4853.1235 - val_loss: 3597.2107\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4782.9229 - val_loss: 3543.6401\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4713.5732 - val_loss: 3487.9126\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4639.9258 - val_loss: 3431.2268\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4567.7363 - val_loss: 3376.3662\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4497.6836 - val_loss: 3323.0378\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4429.2837 - val_loss: 3270.9050\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4362.2144 - val_loss: 3219.7842\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4296.2900 - val_loss: 3169.5605\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4229.8276 - val_loss: 3116.0933\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4158.4873 - val_loss: 3062.1687\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 4088.9111 - val_loss: 3010.1746\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4021.5691 - val_loss: 2959.6663\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3955.9067 - val_loss: 2910.3345\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3891.6011 - val_loss: 2862.0098\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3828.4717 - val_loss: 2814.5901\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3766.4043 - val_loss: 2768.0073\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3705.3218 - val_loss: 2722.2119\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3645.1663 - val_loss: 2677.1663\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3585.8938 - val_loss: 2632.8413\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3527.4680 - val_loss: 2589.2117\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3469.8618 - val_loss: 2546.2571\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3413.0481 - val_loss: 2503.9600\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3357.0081 - val_loss: 2462.3042\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3301.7207 - val_loss: 2421.2771\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3247.1716 - val_loss: 2380.8650\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3193.3442 - val_loss: 2341.0569\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3140.2266 - val_loss: 2301.8425\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3087.8052 - val_loss: 2263.2126\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3036.0696 - val_loss: 2225.1575\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2985.0085 - val_loss: 2187.6687\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2934.6130 - val_loss: 2150.7390\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2884.8716 - val_loss: 2114.3599\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2835.7776 - val_loss: 2078.5251\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2787.3213 - val_loss: 2043.2267\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2739.4944 - val_loss: 2008.4583\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2692.2908 - val_loss: 1974.2144\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2645.7019 - val_loss: 1940.4872\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2599.7197 - val_loss: 1907.2721\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2554.3391 - val_loss: 1874.5623\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2509.5525 - val_loss: 1842.3531\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2465.3535 - val_loss: 1810.6384\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2421.7354 - val_loss: 1779.4132\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2378.6929 - val_loss: 1748.6713\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2336.2190 - val_loss: 1718.4084\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2294.3088 - val_loss: 1688.6191\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2252.9565 - val_loss: 1659.2993\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2212.1550 - val_loss: 1630.4429\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2171.9011 - val_loss: 1602.0455\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 2132.1885 - val_loss: 1574.1031\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2093.0107 - val_loss: 1546.6107\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2054.3643 - val_loss: 1519.5634\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2016.2430 - val_loss: 1492.9570\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1978.6421 - val_loss: 1466.7864\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1941.5566 - val_loss: 1441.0486\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1904.9822 - val_loss: 1415.7383\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1868.9131 - val_loss: 1390.8510\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1833.3448 - val_loss: 1366.3832\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1798.2732 - val_loss: 1342.3306\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1763.6934 - val_loss: 1318.6885\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1729.6002 - val_loss: 1295.4535\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1695.9899 - val_loss: 1272.6211\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1662.8579 - val_loss: 1250.1875\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1630.1987 - val_loss: 1228.1489\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1598.0094 - val_loss: 1206.5011\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1566.2852 - val_loss: 1185.2401\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1535.0211 - val_loss: 1164.3622\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1504.2136 - val_loss: 1143.8639\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1473.8584 - val_loss: 1123.7406\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1443.9509 - val_loss: 1103.9893\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1414.4874 - val_loss: 1084.6055\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1385.4634 - val_loss: 1065.5861\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1356.8752 - val_loss: 1046.9270\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1328.7186 - val_loss: 1028.6246\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1300.9891 - val_loss: 1010.6755\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1273.6831 - val_loss: 993.0754\n",
      "Epoch 84/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1246.7970 - val_loss: 975.8214\n",
      "Epoch 85/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1220.3259 - val_loss: 958.9095\n",
      "Epoch 86/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1194.2667 - val_loss: 942.3362\n",
      "Epoch 87/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1168.6155 - val_loss: 926.0981\n",
      "Epoch 88/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1143.3682 - val_loss: 910.1913\n",
      "Epoch 89/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1118.5208 - val_loss: 894.6128\n",
      "Epoch 90/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1094.0695 - val_loss: 879.3583\n",
      "Epoch 91/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1070.0110 - val_loss: 864.4249\n",
      "Epoch 92/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1046.3409 - val_loss: 849.8090\n",
      "Epoch 93/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1023.0561 - val_loss: 835.5074\n",
      "Epoch 94/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1000.1524 - val_loss: 821.5161\n",
      "Epoch 95/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 977.6260 - val_loss: 807.8320\n",
      "Epoch 96/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 955.4735 - val_loss: 794.4517\n",
      "Epoch 97/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 933.6908 - val_loss: 781.3710\n",
      "Epoch 98/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 912.2745 - val_loss: 768.5878\n",
      "Epoch 99/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 891.2213 - val_loss: 756.0980\n",
      "Epoch 100/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 870.5270 - val_loss: 743.8982\n",
      "Epoch 101/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 850.1880 - val_loss: 731.9850\n",
      "Epoch 102/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 830.2014 - val_loss: 720.3553\n",
      "Epoch 103/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 810.5628 - val_loss: 709.0056\n",
      "Epoch 104/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 791.2690 - val_loss: 697.9322\n",
      "Epoch 105/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 772.3163 - val_loss: 687.1322\n",
      "Epoch 106/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 753.7012 - val_loss: 676.6031\n",
      "Epoch 107/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 735.4204 - val_loss: 666.3400\n",
      "Epoch 108/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 717.4702 - val_loss: 656.3401\n",
      "Epoch 109/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 699.8469 - val_loss: 646.6007\n",
      "Epoch 110/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 682.5473 - val_loss: 637.1182\n",
      "Epoch 111/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 665.5679 - val_loss: 627.8888\n",
      "Epoch 112/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 648.9051 - val_loss: 618.9103\n",
      "Epoch 113/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 632.5554 - val_loss: 610.1781\n",
      "Epoch 114/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 616.5152 - val_loss: 601.6901\n",
      "Epoch 115/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 600.7815 - val_loss: 593.4426\n",
      "Epoch 116/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 585.3509 - val_loss: 585.4317\n",
      "Epoch 117/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 570.2193 - val_loss: 577.6557\n",
      "Epoch 118/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 555.3840 - val_loss: 570.1099\n",
      "Epoch 119/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 540.8409 - val_loss: 562.7916\n",
      "Epoch 120/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 526.5870 - val_loss: 555.6976\n",
      "Epoch 121/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 512.6191 - val_loss: 548.8248\n",
      "Epoch 122/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 498.9333 - val_loss: 542.1700\n",
      "Epoch 123/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 485.5270 - val_loss: 535.7300\n",
      "Epoch 124/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 472.3964 - val_loss: 529.5012\n",
      "Epoch 125/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 459.5379 - val_loss: 523.4807\n",
      "Epoch 126/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 446.9484 - val_loss: 517.6653\n",
      "Epoch 127/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 434.6249 - val_loss: 512.0518\n",
      "Epoch 128/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 422.5633 - val_loss: 506.6369\n",
      "Epoch 129/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 410.7603 - val_loss: 501.4174\n",
      "Epoch 130/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 399.2134 - val_loss: 496.3905\n",
      "Epoch 131/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 387.9188 - val_loss: 491.5529\n",
      "Epoch 132/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 376.8736 - val_loss: 486.9016\n",
      "Epoch 133/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 366.0738 - val_loss: 482.4325\n",
      "Epoch 134/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 355.5164 - val_loss: 478.1435\n",
      "Epoch 135/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 345.1981 - val_loss: 474.0309\n",
      "Epoch 136/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 335.1158 - val_loss: 470.0921\n",
      "Epoch 137/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 325.2665 - val_loss: 466.3235\n",
      "Epoch 138/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 315.6461 - val_loss: 462.7221\n",
      "Epoch 139/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 306.2524 - val_loss: 459.2849\n",
      "Epoch 140/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 297.0814 - val_loss: 456.0086\n",
      "Epoch 141/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 288.1299 - val_loss: 452.8904\n",
      "Epoch 142/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 279.3950 - val_loss: 449.9268\n",
      "Epoch 143/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 270.8734 - val_loss: 447.1150\n",
      "Epoch 144/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 262.5614 - val_loss: 444.4516\n",
      "Epoch 145/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 254.4563 - val_loss: 441.9340\n",
      "Epoch 146/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 246.5547 - val_loss: 439.5589\n",
      "Epoch 147/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 238.8537 - val_loss: 437.3231\n",
      "Epoch 148/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 231.3498 - val_loss: 435.2239\n",
      "Epoch 149/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 224.0403 - val_loss: 433.2581\n",
      "Epoch 150/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 216.9216 - val_loss: 431.4226\n",
      "Epoch 151/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 209.9906 - val_loss: 429.7143\n",
      "Epoch 152/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 203.2446 - val_loss: 428.1307\n",
      "Epoch 153/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 196.6800 - val_loss: 426.6683\n",
      "Epoch 154/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 190.2939 - val_loss: 425.3245\n",
      "Epoch 155/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 184.0832 - val_loss: 424.0959\n",
      "Epoch 156/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 178.0448 - val_loss: 422.9799\n",
      "Epoch 157/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 172.1756 - val_loss: 421.9735\n",
      "Epoch 158/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 166.4726 - val_loss: 421.0737\n",
      "Epoch 159/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 160.9325 - val_loss: 420.2777\n",
      "Epoch 160/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 155.5529 - val_loss: 419.5826\n",
      "Epoch 161/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 150.3301 - val_loss: 418.9854\n",
      "Epoch 162/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 145.2614 - val_loss: 418.4834\n",
      "Epoch 163/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 140.3436 - val_loss: 418.0736\n",
      "Epoch 164/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 135.5740 - val_loss: 417.7534\n",
      "Epoch 165/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 130.9496 - val_loss: 417.5198\n",
      "Epoch 166/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 126.4676 - val_loss: 417.3702\n",
      "Epoch 167/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 122.1250 - val_loss: 417.3017\n",
      "Epoch 168/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 117.9187 - val_loss: 417.3117\n",
      "Epoch 169/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 113.8460 - val_loss: 417.3973\n",
      "Epoch 170/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 109.9040 - val_loss: 417.5560\n",
      "Epoch 171/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 106.0897 - val_loss: 417.7850\n",
      "Epoch 172/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 102.4005 - val_loss: 418.0818\n",
      "Epoch 173/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 98.8336 - val_loss: 418.4436\n",
      "Epoch 174/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 95.3860 - val_loss: 418.8680\n",
      "Epoch 175/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.0552 - val_loss: 419.3522\n",
      "Epoch 176/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8384 - val_loss: 419.8940\n",
      "Epoch 177/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 85.7329 - val_loss: 420.4906\n",
      "Epoch 178/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 82.7361 - val_loss: 421.1397\n",
      "Epoch 179/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 79.8451 - val_loss: 421.8387\n",
      "Epoch 180/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 77.0574 - val_loss: 422.5853\n",
      "Epoch 181/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 74.3704 - val_loss: 423.3773\n",
      "Epoch 182/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 71.7815 - val_loss: 424.2120\n",
      "Epoch 183/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 69.2881 - val_loss: 425.0873\n",
      "Epoch 184/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 66.8879 - val_loss: 426.0008\n",
      "Epoch 185/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 64.5783 - val_loss: 426.9505\n",
      "Epoch 186/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 62.3566 - val_loss: 427.9338\n",
      "Epoch 187/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 60.2208 - val_loss: 428.9488\n",
      "Epoch 188/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 58.1682 - val_loss: 429.9933\n",
      "Epoch 189/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 56.1966 - val_loss: 431.0652\n",
      "Epoch 190/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 54.3036 - val_loss: 432.1626\n",
      "Epoch 191/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 52.4869 - val_loss: 433.2833\n",
      "Epoch 192/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 50.7440 - val_loss: 434.4254\n",
      "Epoch 193/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 49.0730 - val_loss: 435.5869\n",
      "Epoch 194/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 47.4717 - val_loss: 436.7661\n",
      "Epoch 195/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 45.9376 - val_loss: 437.9608\n",
      "Epoch 196/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 44.4689 - val_loss: 439.1696\n",
      "Epoch 197/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 43.0635 - val_loss: 440.3904\n",
      "Epoch 198/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 41.7192 - val_loss: 441.6216\n",
      "Epoch 199/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 40.4341 - val_loss: 442.8615\n",
      "Epoch 200/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 39.2061 - val_loss: 444.1089\n",
      "Epoch 201/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 38.0332 - val_loss: 445.3615\n",
      "Epoch 202/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.9137 - val_loss: 446.6183\n",
      "Epoch 203/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.8456 - val_loss: 447.8776\n",
      "Epoch 204/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.8270 - val_loss: 449.1382\n",
      "Epoch 205/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 33.8562 - val_loss: 450.3983\n",
      "Epoch 206/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.9314 - val_loss: 451.6569\n",
      "Epoch 207/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.0509 - val_loss: 452.9128\n",
      "Epoch 208/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.2131 - val_loss: 454.1643\n",
      "Epoch 209/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 30.4163 - val_loss: 455.4106\n",
      "Epoch 210/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 29.6590 - val_loss: 456.6503\n",
      "Epoch 211/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 28.9396 - val_loss: 457.8823\n",
      "Epoch 212/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 28.2565 - val_loss: 459.1057\n",
      "Epoch 213/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 27.6083 - val_loss: 460.3194\n",
      "Epoch 214/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 26.9935 - val_loss: 461.5223\n",
      "Epoch 215/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 26.4108 - val_loss: 462.7139\n",
      "Epoch 216/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 25.8588 - val_loss: 463.8929\n",
      "Epoch 217/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 25.3361 - val_loss: 465.0589\n",
      "Epoch 218/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 24.8415 - val_loss: 466.2108\n",
      "Epoch 219/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 24.3737 - val_loss: 467.3482\n",
      "Epoch 220/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.9315 - val_loss: 468.4698\n",
      "Epoch 221/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.5139 - val_loss: 469.5758\n",
      "Epoch 222/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.1195 - val_loss: 470.6650\n",
      "Epoch 223/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.7474 - val_loss: 471.7368\n",
      "Epoch 224/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.3966 - val_loss: 472.7911\n",
      "Epoch 225/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.0659 - val_loss: 473.8272\n",
      "Epoch 226/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.7545 - val_loss: 474.8449\n",
      "Epoch 227/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.4612 - val_loss: 475.8437\n",
      "Epoch 228/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.1853 - val_loss: 476.8229\n",
      "Epoch 229/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.9259 - val_loss: 477.7828\n",
      "Epoch 230/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 20.6821 - val_loss: 478.7229\n",
      "Epoch 231/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 20.4531 - val_loss: 479.6427\n",
      "Epoch 232/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 20.2382 - val_loss: 480.5425\n",
      "Epoch 233/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.0365 - val_loss: 481.4218\n",
      "Epoch 234/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.8474 - val_loss: 482.2806\n",
      "Epoch 235/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.6702 - val_loss: 483.1189\n",
      "Epoch 236/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.5042 - val_loss: 483.9365\n",
      "Epoch 237/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 19.3488 - val_loss: 484.7338\n",
      "Epoch 238/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.2034 - val_loss: 485.5101\n",
      "Epoch 239/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.0674 - val_loss: 486.2658\n",
      "Epoch 240/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.9403 - val_loss: 487.0011\n",
      "Epoch 241/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.8216 - val_loss: 487.7155\n",
      "Epoch 242/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7108 - val_loss: 488.4098\n",
      "Epoch 243/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.6073 - val_loss: 489.0840\n",
      "Epoch 244/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.5108 - val_loss: 489.7384\n",
      "Epoch 245/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.4209 - val_loss: 490.3725\n",
      "Epoch 246/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.3370 - val_loss: 490.9875\n",
      "Epoch 247/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.2589 - val_loss: 491.5825\n",
      "Epoch 248/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.1862 - val_loss: 492.1590\n",
      "Epoch 249/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.1185 - val_loss: 492.7163\n",
      "Epoch 250/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.0555 - val_loss: 493.2548\n",
      "Epoch 251/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.9970 - val_loss: 493.7753\n",
      "Epoch 252/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.9426 - val_loss: 494.2777\n",
      "Epoch 253/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.8920 - val_loss: 494.7621\n",
      "Epoch 254/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.8450 - val_loss: 495.2292\n",
      "Epoch 255/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.8014 - val_loss: 495.6795\n",
      "Epoch 256/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.7608 - val_loss: 496.1127\n",
      "Epoch 257/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.7233 - val_loss: 496.5293\n",
      "Epoch 258/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.6885 - val_loss: 496.9306\n",
      "Epoch 259/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.6562 - val_loss: 497.3162\n",
      "Epoch 260/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.6262 - val_loss: 497.6862\n",
      "Epoch 261/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.5984 - val_loss: 498.0414\n",
      "Epoch 262/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.5727 - val_loss: 498.3821\n",
      "Epoch 263/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.5488 - val_loss: 498.7086\n",
      "Epoch 264/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.5268 - val_loss: 499.0215\n",
      "Epoch 265/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 17.5063 - val_loss: 499.3214\n",
      "Epoch 266/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4874 - val_loss: 499.6077\n",
      "Epoch 267/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4699 - val_loss: 499.8818\n",
      "Epoch 268/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4536 - val_loss: 500.1436\n",
      "Epoch 269/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4387 - val_loss: 500.3936\n",
      "Epoch 270/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4248 - val_loss: 500.6324\n",
      "Epoch 271/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4119 - val_loss: 500.8598\n",
      "Epoch 272/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.4001 - val_loss: 501.0767\n",
      "Epoch 273/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3891 - val_loss: 501.2833\n",
      "Epoch 274/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3790 - val_loss: 501.4798\n",
      "Epoch 275/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3696 - val_loss: 501.6666\n",
      "Epoch 276/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3610 - val_loss: 501.8444\n",
      "Epoch 277/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3529 - val_loss: 502.0131\n",
      "Epoch 278/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3456 - val_loss: 502.1735\n",
      "Epoch 279/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3388 - val_loss: 502.3254\n",
      "Epoch 280/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3325 - val_loss: 502.4696\n",
      "Epoch 281/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3267 - val_loss: 502.6059\n",
      "Epoch 282/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3214 - val_loss: 502.7357\n",
      "Epoch 283/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3165 - val_loss: 502.8579\n",
      "Epoch 284/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3120 - val_loss: 502.9735\n",
      "Epoch 285/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3078 - val_loss: 503.0826\n",
      "Epoch 286/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3040 - val_loss: 503.1858\n",
      "Epoch 287/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3005 - val_loss: 503.2831\n",
      "Epoch 288/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2973 - val_loss: 503.3750\n",
      "Epoch 289/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2943 - val_loss: 503.4612\n",
      "Epoch 290/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2917 - val_loss: 503.5427\n",
      "Epoch 291/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2892 - val_loss: 503.6190\n",
      "Epoch 292/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 17.2870 - val_loss: 503.6910\n",
      "Epoch 293/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.2849 - val_loss: 503.7585\n",
      "Epoch 294/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.2831 - val_loss: 503.8219\n",
      "Epoch 295/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2814 - val_loss: 503.8811\n",
      "Epoch 296/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2799 - val_loss: 503.9370\n",
      "Epoch 297/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.2786 - val_loss: 503.9890\n",
      "Epoch 298/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.2774 - val_loss: 504.0378\n",
      "Epoch 299/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2763 - val_loss: 504.0836\n",
      "Epoch 300/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2753 - val_loss: 504.1260\n",
      "Epoch 301/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.2745 - val_loss: 504.1660\n",
      "Epoch 302/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2737 - val_loss: 504.2032\n",
      "Epoch 303/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.2731 - val_loss: 504.2374\n",
      "Epoch 304/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2725 - val_loss: 504.2696\n",
      "Epoch 305/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2721 - val_loss: 504.2995\n",
      "Epoch 306/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2717 - val_loss: 504.3273\n",
      "Epoch 307/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.2714 - val_loss: 504.3534\n",
      "Epoch 308/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2712 - val_loss: 504.3773\n",
      "Epoch 309/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.2710 - val_loss: 504.3993\n",
      "Epoch 310/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2709 - val_loss: 504.4199\n",
      "Epoch 311/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.2708 - val_loss: 504.4386\n",
      "Epoch 312/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2708 - val_loss: 504.4559\n",
      "Epoch 313/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2709 - val_loss: 504.4717\n",
      "Epoch 314/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2710 - val_loss: 504.4864\n",
      "Epoch 315/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.2711 - val_loss: 504.5002\n",
      "Epoch 316/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2713 - val_loss: 504.5126\n",
      "Epoch 317/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 17.2715 - val_loss: 504.5240\n",
      "Epoch 318/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.2717 - val_loss: 504.5344\n",
      "Epoch 319/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.2720 - val_loss: 504.5439\n",
      "Epoch 320/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.2723 - val_loss: 504.5527\n",
      "Epoch 321/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.2726 - val_loss: 504.5604\n",
      "Epoch 322/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2729 - val_loss: 504.5674\n",
      "Epoch 323/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2733 - val_loss: 504.5739\n",
      "Epoch 324/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.2738 - val_loss: 504.5799\n",
      "Epoch 325/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2741 - val_loss: 504.5854\n",
      "Epoch 326/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2745 - val_loss: 504.5898\n",
      "Epoch 327/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2750 - val_loss: 504.5938\n",
      "Epoch 328/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2755 - val_loss: 504.5974\n",
      "Epoch 329/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2760 - val_loss: 504.6009\n",
      "Epoch 330/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2764 - val_loss: 504.6037\n",
      "Epoch 331/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2769 - val_loss: 504.6062\n",
      "Epoch 332/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2774 - val_loss: 504.6084\n",
      "Epoch 333/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2779 - val_loss: 504.6100\n",
      "Epoch 334/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2785 - val_loss: 504.6118\n",
      "Epoch 335/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2790 - val_loss: 504.6131\n",
      "Epoch 336/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2795 - val_loss: 504.6143\n",
      "Epoch 337/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.2800 - val_loss: 504.6153\n",
      "Epoch 338/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2805 - val_loss: 504.6157\n",
      "Epoch 339/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 17.2811 - val_loss: 504.6161\n",
      "Epoch 340/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2816 - val_loss: 504.6163\n",
      "Epoch 341/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2822 - val_loss: 504.6162\n",
      "Epoch 342/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2827 - val_loss: 504.6159\n",
      "Epoch 343/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2833 - val_loss: 504.6159\n",
      "Epoch 344/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.2838 - val_loss: 504.6155\n",
      "Epoch 345/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.2843 - val_loss: 504.6153\n",
      "Epoch 346/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2849 - val_loss: 504.6148\n",
      "Epoch 347/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2854 - val_loss: 504.6141\n",
      "Epoch 348/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2860 - val_loss: 504.6136\n",
      "Epoch 349/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2865 - val_loss: 504.6126\n",
      "Epoch 350/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.2871 - val_loss: 504.6118\n",
      "Epoch 351/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.2876 - val_loss: 504.6110\n",
      "Epoch 352/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2881 - val_loss: 504.6098\n",
      "Epoch 353/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.2887 - val_loss: 504.6091\n",
      "Epoch 354/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.2892 - val_loss: 504.6083\n",
      "Epoch 355/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2897 - val_loss: 504.6070\n",
      "Epoch 356/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2903 - val_loss: 504.6060\n",
      "Epoch 357/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2907 - val_loss: 504.6048\n",
      "Epoch 358/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2913 - val_loss: 504.6038\n",
      "Epoch 359/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2918 - val_loss: 504.6024\n",
      "Epoch 360/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2923 - val_loss: 504.6013\n",
      "Epoch 361/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2928 - val_loss: 504.6001\n",
      "Epoch 362/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 17.2933 - val_loss: 504.5989\n",
      "Epoch 363/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.2938 - val_loss: 504.5977\n",
      "Epoch 364/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2943 - val_loss: 504.5965\n",
      "Epoch 365/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2948 - val_loss: 504.5955\n",
      "Epoch 366/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2952 - val_loss: 504.5941\n",
      "Epoch 367/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2957 - val_loss: 504.5929\n",
      "Epoch 368/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.2962 - val_loss: 504.5918\n",
      "Epoch 369/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.2966 - val_loss: 504.5905\n",
      "Epoch 370/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2971 - val_loss: 504.5891\n",
      "Epoch 371/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2976 - val_loss: 504.5878\n",
      "Epoch 372/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2980 - val_loss: 504.5865\n",
      "Epoch 373/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2985 - val_loss: 504.5852\n",
      "Epoch 374/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2989 - val_loss: 504.5841\n",
      "Epoch 375/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2994 - val_loss: 504.5831\n",
      "Epoch 376/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2998 - val_loss: 504.5817\n",
      "Epoch 377/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3002 - val_loss: 504.5806\n",
      "Epoch 378/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3006 - val_loss: 504.5797\n",
      "Epoch 379/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3010 - val_loss: 504.5784\n",
      "Epoch 380/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3015 - val_loss: 504.5771\n",
      "Epoch 381/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3018 - val_loss: 504.5760\n",
      "Epoch 382/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3022 - val_loss: 504.5750\n",
      "Epoch 383/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3026 - val_loss: 504.5737\n",
      "Epoch 384/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3030 - val_loss: 504.5727\n",
      "Epoch 385/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3034 - val_loss: 504.5715\n",
      "Epoch 386/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 17.3038 - val_loss: 504.5705\n",
      "Epoch 387/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3042 - val_loss: 504.5694\n",
      "Epoch 388/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3045 - val_loss: 504.5685\n",
      "Epoch 389/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3049 - val_loss: 504.5675\n",
      "Epoch 390/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3052 - val_loss: 504.5665\n",
      "Epoch 391/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3056 - val_loss: 504.5657\n",
      "Epoch 392/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3059 - val_loss: 504.5648\n",
      "Epoch 393/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3063 - val_loss: 504.5638\n",
      "Epoch 394/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3066 - val_loss: 504.5631\n",
      "Epoch 395/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3069 - val_loss: 504.5620\n",
      "Epoch 396/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3072 - val_loss: 504.5611\n",
      "Epoch 397/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3076 - val_loss: 504.5600\n",
      "Epoch 398/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3079 - val_loss: 504.5591\n",
      "Epoch 399/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3081 - val_loss: 504.5579\n",
      "Epoch 400/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3085 - val_loss: 504.5573\n",
      "Epoch 401/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3088 - val_loss: 504.5561\n",
      "Epoch 402/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3091 - val_loss: 504.5552\n",
      "Epoch 403/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3094 - val_loss: 504.5548\n",
      "Epoch 404/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3096 - val_loss: 504.5536\n",
      "Epoch 405/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3100 - val_loss: 504.5528\n",
      "Epoch 406/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3102 - val_loss: 504.5523\n",
      "Epoch 407/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3105 - val_loss: 504.5514\n",
      "Epoch 408/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3108 - val_loss: 504.5505\n",
      "Epoch 409/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.3111 - val_loss: 504.5499\n",
      "Epoch 410/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3113 - val_loss: 504.5493\n",
      "Epoch 411/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3115 - val_loss: 504.5485\n",
      "Epoch 412/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3118 - val_loss: 504.5475\n",
      "Epoch 413/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3120 - val_loss: 504.5469\n",
      "Epoch 414/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3123 - val_loss: 504.5462\n",
      "Epoch 415/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3125 - val_loss: 504.5455\n",
      "Epoch 416/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3127 - val_loss: 504.5448\n",
      "Epoch 417/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3130 - val_loss: 504.5441\n",
      "Epoch 418/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3132 - val_loss: 504.5434\n",
      "Epoch 419/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3134 - val_loss: 504.5428\n",
      "Epoch 420/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3136 - val_loss: 504.5424\n",
      "Epoch 421/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3138 - val_loss: 504.5418\n",
      "Epoch 422/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3141 - val_loss: 504.5409\n",
      "Epoch 423/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3143 - val_loss: 504.5404\n",
      "Epoch 424/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3145 - val_loss: 504.5399\n",
      "Epoch 425/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3146 - val_loss: 504.5391\n",
      "Epoch 426/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3149 - val_loss: 504.5386\n",
      "Epoch 427/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3151 - val_loss: 504.5384\n",
      "Epoch 428/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3152 - val_loss: 504.5376\n",
      "Epoch 429/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3154 - val_loss: 504.5374\n",
      "Epoch 430/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3156 - val_loss: 504.5371\n",
      "Epoch 431/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 17.3157 - val_loss: 504.5364\n",
      "Epoch 432/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3159 - val_loss: 504.5359\n",
      "Epoch 433/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3161 - val_loss: 504.5357\n",
      "Epoch 434/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3163 - val_loss: 504.5353\n",
      "Epoch 435/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3164 - val_loss: 504.5345\n",
      "Epoch 436/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3166 - val_loss: 504.5341\n",
      "Epoch 437/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3168 - val_loss: 504.5337\n",
      "Epoch 438/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3169 - val_loss: 504.5333\n",
      "Epoch 439/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3171 - val_loss: 504.5327\n",
      "Epoch 440/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3172 - val_loss: 504.5323\n",
      "Epoch 441/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3174 - val_loss: 504.5319\n",
      "Epoch 442/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3175 - val_loss: 504.5313\n",
      "Epoch 443/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3177 - val_loss: 504.5310\n",
      "Epoch 444/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3178 - val_loss: 504.5307\n",
      "Epoch 445/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3179 - val_loss: 504.5300\n",
      "Epoch 446/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3181 - val_loss: 504.5296\n",
      "Epoch 447/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3182 - val_loss: 504.5292\n",
      "Epoch 448/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3183 - val_loss: 504.5286\n",
      "Epoch 449/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3184 - val_loss: 504.5281\n",
      "Epoch 450/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 17.3186 - val_loss: 504.5277\n",
      "Epoch 451/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3187 - val_loss: 504.5277\n",
      "Epoch 452/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3188 - val_loss: 504.5274\n",
      "Epoch 453/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3189 - val_loss: 504.5271\n",
      "Epoch 454/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3191 - val_loss: 504.5269\n",
      "Epoch 455/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3192 - val_loss: 504.5266\n",
      "Epoch 456/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3193 - val_loss: 504.5261\n",
      "Epoch 457/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3194 - val_loss: 504.5256\n",
      "Epoch 458/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3195 - val_loss: 504.5254\n",
      "Epoch 459/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3196 - val_loss: 504.5252\n",
      "Epoch 460/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.3197 - val_loss: 504.5247\n",
      "Epoch 461/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3198 - val_loss: 504.5244\n",
      "Epoch 462/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3199 - val_loss: 504.5240\n",
      "Epoch 463/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3200 - val_loss: 504.5236\n",
      "Epoch 464/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3201 - val_loss: 504.5234\n",
      "Epoch 465/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3202 - val_loss: 504.5232\n",
      "Epoch 466/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3203 - val_loss: 504.5231\n",
      "Epoch 467/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3204 - val_loss: 504.5229\n",
      "Epoch 468/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3204 - val_loss: 504.5226\n",
      "Epoch 469/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 17.3205 - val_loss: 504.5225\n",
      "Epoch 470/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3206 - val_loss: 504.5224\n",
      "Epoch 471/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3207 - val_loss: 504.5219\n",
      "Epoch 472/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3208 - val_loss: 504.5215\n",
      "Epoch 473/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3209 - val_loss: 504.5212\n",
      "Epoch 474/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3209 - val_loss: 504.5211\n",
      "Epoch 475/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3210 - val_loss: 504.5208\n",
      "Epoch 476/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3211 - val_loss: 504.5204\n",
      "Epoch 477/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3211 - val_loss: 504.5204\n",
      "Epoch 478/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3212 - val_loss: 504.5202\n",
      "Epoch 479/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3213 - val_loss: 504.5199\n",
      "Epoch 480/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3213 - val_loss: 504.5196\n",
      "Epoch 481/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3214 - val_loss: 504.5196\n",
      "Epoch 482/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3215 - val_loss: 504.5195\n",
      "Epoch 483/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3215 - val_loss: 504.5195\n",
      "Epoch 484/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3216 - val_loss: 504.5193\n",
      "Epoch 485/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3217 - val_loss: 504.5193\n",
      "Epoch 486/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3217 - val_loss: 504.5190\n",
      "Epoch 487/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3218 - val_loss: 504.5189\n",
      "Epoch 488/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3218 - val_loss: 504.5188\n",
      "Epoch 489/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 17.3219 - val_loss: 504.5187\n",
      "Epoch 490/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3219 - val_loss: 504.5186\n",
      "Epoch 491/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3220 - val_loss: 504.5186\n",
      "Epoch 492/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3220 - val_loss: 504.5184\n",
      "Epoch 493/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3221 - val_loss: 504.5180\n",
      "Epoch 494/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3221 - val_loss: 504.5179\n",
      "Epoch 495/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3222 - val_loss: 504.5180\n",
      "Epoch 496/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3222 - val_loss: 504.5177\n",
      "Epoch 497/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3223 - val_loss: 504.5175\n",
      "Epoch 498/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3223 - val_loss: 504.5169\n",
      "Epoch 499/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.3224 - val_loss: 504.5167\n",
      "Epoch 500/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.3224 - val_loss: 504.5165\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.1552, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0026, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0004, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0102, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.8734, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 336ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.67527544e+01, 6.67303455e+01, 6.67079365e+01, 6.66855275e+01,\n",
       "        6.66631186e+01, 6.66407096e+01, 6.66183006e+01, 6.65958917e+01,\n",
       "        6.65734827e+01, 6.65510738e+01, 6.65286648e+01, 6.65062558e+01,\n",
       "        6.64838469e+01, 7.03730392e+01, 7.03142157e+01, 7.02553922e+01,\n",
       "        7.01965686e+01, 7.01377451e+01, 7.00789216e+01, 7.00200980e+01,\n",
       "        6.99612745e+01, 6.99024510e+01, 6.97711485e+01, 6.96366947e+01,\n",
       "        6.95022409e+01, 6.93677871e+01, 6.92333333e+01, 6.90988796e+01,\n",
       "        6.89644258e+01, 6.88299720e+01, 6.86955182e+01, 6.85610644e+01,\n",
       "        6.84266106e+01, 6.82975490e+01, 6.82555322e+01, 6.82135154e+01,\n",
       "        6.81714986e+01, 6.81294818e+01, 6.80874650e+01, 6.80454482e+01,\n",
       "        4.03590024e-01, 0.00000000e+00, 0.00000000e+00, 3.69284540e-01,\n",
       "        0.00000000e+00, 4.68337446e-01, 1.54416636e-01, 6.93976657e+01,\n",
       "        6.92632119e+01, 6.91287582e+01, 6.90043044e+01, 6.88598506e+01,\n",
       "        6.87253968e+01, 6.85909430e+01, 6.84564893e+01, 6.83220355e+01,\n",
       "        6.82648693e+01, 6.82228525e+01, 6.81808357e+01, 6.81388189e+01,\n",
       "        6.80968020e+01, 6.80547852e+01, 6.80127684e+01, 6.79707516e+01,\n",
       "        6.79287348e+01, 6.78867180e+01, 6.78447012e+01, 6.78026844e+01,\n",
       "        6.77213352e+01, 6.76373016e+01, 6.75532680e+01, 6.74692344e+01,\n",
       "        6.73852007e+01, 7.48104630e+01, 2.29810938e-01, 4.26735163e-01,\n",
       "        2.73587630e-02, 1.30296558e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.76988487e+01, 2.33703539e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.21428639e-01, 0.00000000e+00, 4.45347309e-01, 1.01357067e+00,\n",
       "        0.00000000e+00, 8.13373566e-01, 0.00000000e+00, 3.25780332e-01,\n",
       "        5.31821966e-01, 3.53163481e-03, 5.06379366e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.75386542e-01, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([57.53635991, 57.52078295, 57.50520598, 57.48962902, 57.47405206,\n",
       "       57.4584751 , 57.44289813, 57.42732117, 57.41174421, 57.39616725,\n",
       "       57.38059028, 57.36501332, 57.34943636, 57.3338594 , 57.31828243,\n",
       "       57.30270547, 57.28712851, 57.27155155, 57.25597458, 57.24039762,\n",
       "       57.22482066, 57.2092437 , 57.19366673, 57.17808977, 57.16251281,\n",
       "       57.14693585, 57.13135889, 57.11578192, 57.10020496, 57.084628  ,\n",
       "       57.06905104, 57.05347407, 57.03789711, 57.02232015, 57.00674319,\n",
       "       56.99116622, 56.97558926, 56.9600123 , 56.94443534, 56.92885837,\n",
       "       56.91328141, 56.89770445, 56.88212749, 56.86655052, 56.85097356,\n",
       "       56.8353966 , 56.81981964, 56.80424267, 56.78866571, 56.77308875,\n",
       "       56.75751179, 56.74193482, 56.72635786, 56.7107809 , 56.69520394,\n",
       "       56.67962697, 56.66405001, 56.64847305, 56.63289609, 56.61731912,\n",
       "       56.60174216, 56.5861652 , 56.57058824, 56.55501127, 56.53943431,\n",
       "       56.52385735, 56.50828039, 56.49270342, 56.47712646, 56.4615495 ,\n",
       "       56.44597254, 56.43039557, 56.41481861, 56.39924165, 56.38366469,\n",
       "       56.36808772, 56.35251076, 56.3369338 , 56.32135684, 56.30577987,\n",
       "       56.29020291, 56.27462595, 56.25904899, 56.24347202, 56.22789506,\n",
       "       56.2123181 , 56.19674114, 56.18116417, 56.16558721, 56.15001025,\n",
       "       56.13443329, 56.11885632, 56.10327936, 56.0877024 , 56.07212544,\n",
       "       56.05654847, 56.04097151, 56.02539455, 56.00981759, 55.99424062])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.125118712649034\n",
      "25.633620679867228\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
