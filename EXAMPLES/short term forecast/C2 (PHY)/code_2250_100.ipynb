{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "2345    54.498852\n",
       "2346    54.483275\n",
       "2347    54.467698\n",
       "2348    54.452121\n",
       "2349    54.436544\n",
       "Name: C2, Length: 2350, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c2_interpolated_2250_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "2245     0.531822\n",
       "2246     0.003532\n",
       "2247     0.506379\n",
       "2248     0.000000\n",
       "2249     0.000000\n",
       "Name: C2, Length: 2250, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2250)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.500000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.275910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.051821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.827731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.603641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     90.500000  0.000298\n",
       "1     90.275910  0.000298\n",
       "2     90.051821  0.000297\n",
       "3     89.827731  0.000297\n",
       "4     89.603641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.1552  # Value for C0\n",
    "K0 = -0.0026  # Value for K0\n",
    "K1 = -0.0004  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0102    # Value for b\n",
    "c = 2.8734    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    90.500000    90.275910    90.051821    89.827731    89.603641   \n",
      "351    90.275910    90.051821    89.827731    89.603641    89.379552   \n",
      "352    90.051821    89.827731    89.603641    89.379552    89.155462   \n",
      "353    89.827731    89.603641    89.379552    89.155462    88.931373   \n",
      "354    89.603641    89.379552    89.155462    88.931373    88.707283   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    89.379552    89.155462    88.931373    88.707283    88.494958  ...   \n",
      "351    89.155462    88.931373    88.707283    88.494958    88.427731  ...   \n",
      "352    88.931373    88.707283    88.494958    88.427731    88.360504  ...   \n",
      "353    88.707283    88.494958    88.427731    88.360504    88.293277  ...   \n",
      "354    88.494958    88.427731    88.360504    88.293277    88.226050  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   79.071008    0.000263   79.054202    0.000263   79.037395    0.000263   \n",
      "351   79.054202    0.000263   79.037395    0.000263   79.020588    0.000262   \n",
      "352   79.037395    0.000263   79.020588    0.000262   79.003782    0.000262   \n",
      "353   79.020588    0.000262   79.003782    0.000262   78.986975    0.000262   \n",
      "354   79.003782    0.000262   78.986975    0.000262   78.970168    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   79.020588    0.000262   79.003782    0.000262  \n",
      "351   79.003782    0.000262   78.986975    0.000262  \n",
      "352   78.986975    0.000262   78.970168    0.000262  \n",
      "353   78.970168    0.000262   78.953361    0.000262  \n",
      "354   78.953361    0.000262   78.936555    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 1, 251) (1800, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "23/23 [==============================] - 3s 24ms/step - loss: 5340.7793 - val_loss: 3840.2896\n",
      "Epoch 2/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 5261.7617 - val_loss: 3792.8479\n",
      "Epoch 3/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 5197.8726 - val_loss: 3741.7583\n",
      "Epoch 4/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 5123.0498 - val_loss: 3693.2600\n",
      "Epoch 5/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 5055.4575 - val_loss: 3637.0239\n",
      "Epoch 6/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4975.3213 - val_loss: 3586.5068\n",
      "Epoch 7/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4905.7925 - val_loss: 3537.4451\n",
      "Epoch 8/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4837.9536 - val_loss: 3489.5764\n",
      "Epoch 9/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4771.4585 - val_loss: 3442.6250\n",
      "Epoch 10/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4706.0332 - val_loss: 3396.4468\n",
      "Epoch 11/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4641.5337 - val_loss: 3350.9604\n",
      "Epoch 12/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4577.8730 - val_loss: 3306.1152\n",
      "Epoch 13/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4514.9961 - val_loss: 3261.8757\n",
      "Epoch 14/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4452.8657 - val_loss: 3218.2195\n",
      "Epoch 15/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4391.4517 - val_loss: 3175.1252\n",
      "Epoch 16/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4330.7339 - val_loss: 3132.5781\n",
      "Epoch 17/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4270.6904 - val_loss: 3090.5659\n",
      "Epoch 18/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4211.3115 - val_loss: 3049.0781\n",
      "Epoch 19/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4152.5811 - val_loss: 3008.1050\n",
      "Epoch 20/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4094.4893 - val_loss: 2967.6382\n",
      "Epoch 21/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4037.0249 - val_loss: 2927.6704\n",
      "Epoch 22/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3980.1809 - val_loss: 2888.1951\n",
      "Epoch 23/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3923.9465 - val_loss: 2849.2056\n",
      "Epoch 24/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3868.3167 - val_loss: 2810.6958\n",
      "Epoch 25/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3813.2834 - val_loss: 2772.6609\n",
      "Epoch 26/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3758.8389 - val_loss: 2735.0952\n",
      "Epoch 27/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3704.9785 - val_loss: 2697.9934\n",
      "Epoch 28/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3651.6951 - val_loss: 2661.3518\n",
      "Epoch 29/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3598.9827 - val_loss: 2625.1646\n",
      "Epoch 30/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3546.8372 - val_loss: 2589.4275\n",
      "Epoch 31/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3495.2517 - val_loss: 2554.1367\n",
      "Epoch 32/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3444.2214 - val_loss: 2519.2878\n",
      "Epoch 33/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3393.7419 - val_loss: 2484.8767\n",
      "Epoch 34/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3343.8069 - val_loss: 2450.8984\n",
      "Epoch 35/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3294.4128 - val_loss: 2417.3513\n",
      "Epoch 36/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3245.5549 - val_loss: 2384.2288\n",
      "Epoch 37/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3197.2275 - val_loss: 2351.5291\n",
      "Epoch 38/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3149.4270 - val_loss: 2319.2480\n",
      "Epoch 39/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3102.1494 - val_loss: 2287.3818\n",
      "Epoch 40/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3055.3892 - val_loss: 2255.9268\n",
      "Epoch 41/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3009.1421 - val_loss: 2224.8794\n",
      "Epoch 42/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2963.4045 - val_loss: 2194.2366\n",
      "Epoch 43/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2918.1721 - val_loss: 2163.9944\n",
      "Epoch 44/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2873.4419 - val_loss: 2134.1501\n",
      "Epoch 45/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2829.2085 - val_loss: 2104.7000\n",
      "Epoch 46/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2785.4678 - val_loss: 2075.6409\n",
      "Epoch 47/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2742.2168 - val_loss: 2046.9692\n",
      "Epoch 48/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2699.4514 - val_loss: 2018.6821\n",
      "Epoch 49/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2657.1675 - val_loss: 1990.7764\n",
      "Epoch 50/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2615.3608 - val_loss: 1963.2489\n",
      "Epoch 51/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 2574.0291 - val_loss: 1936.0967\n",
      "Epoch 52/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2533.1675 - val_loss: 1909.3163\n",
      "Epoch 53/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2492.7725 - val_loss: 1882.9052\n",
      "Epoch 54/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2452.8411 - val_loss: 1856.8597\n",
      "Epoch 55/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2413.3682 - val_loss: 1831.1775\n",
      "Epoch 56/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2374.3530 - val_loss: 1805.8553\n",
      "Epoch 57/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2335.7900 - val_loss: 1780.8903\n",
      "Epoch 58/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2297.6758 - val_loss: 1756.2795\n",
      "Epoch 59/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2260.0073 - val_loss: 1732.0200\n",
      "Epoch 60/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2222.7815 - val_loss: 1708.1093\n",
      "Epoch 61/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2185.9944 - val_loss: 1684.5437\n",
      "Epoch 62/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2149.6428 - val_loss: 1661.3214\n",
      "Epoch 63/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2113.7231 - val_loss: 1638.4388\n",
      "Epoch 64/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2078.2332 - val_loss: 1615.8938\n",
      "Epoch 65/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2043.1686 - val_loss: 1593.6835\n",
      "Epoch 66/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2008.5260 - val_loss: 1571.8046\n",
      "Epoch 67/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1974.3031 - val_loss: 1550.2546\n",
      "Epoch 68/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1940.4967 - val_loss: 1529.0320\n",
      "Epoch 69/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1907.1031 - val_loss: 1508.1324\n",
      "Epoch 70/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1874.1200 - val_loss: 1487.5546\n",
      "Epoch 71/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1841.5425 - val_loss: 1467.2947\n",
      "Epoch 72/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1809.3691 - val_loss: 1447.3503\n",
      "Epoch 73/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1777.5962 - val_loss: 1427.7200\n",
      "Epoch 74/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1746.2212 - val_loss: 1408.4000\n",
      "Epoch 75/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1715.2408 - val_loss: 1389.3884\n",
      "Epoch 76/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1684.6516 - val_loss: 1370.6821\n",
      "Epoch 77/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1654.4515 - val_loss: 1352.2791\n",
      "Epoch 78/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1624.6368 - val_loss: 1334.1763\n",
      "Epoch 79/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1595.2051 - val_loss: 1316.3716\n",
      "Epoch 80/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1566.1526 - val_loss: 1298.8622\n",
      "Epoch 81/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1537.4777 - val_loss: 1281.6462\n",
      "Epoch 82/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1509.1764 - val_loss: 1264.7206\n",
      "Epoch 83/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1481.2467 - val_loss: 1248.0828\n",
      "Epoch 84/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1453.6847 - val_loss: 1231.7310\n",
      "Epoch 85/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1426.4890 - val_loss: 1215.6621\n",
      "Epoch 86/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1399.6552 - val_loss: 1199.8735\n",
      "Epoch 87/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1373.1815 - val_loss: 1184.3640\n",
      "Epoch 88/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1347.0649 - val_loss: 1169.1300\n",
      "Epoch 89/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1321.3027 - val_loss: 1154.1694\n",
      "Epoch 90/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1295.8920 - val_loss: 1139.4797\n",
      "Epoch 91/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1270.8298 - val_loss: 1125.0591\n",
      "Epoch 92/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1246.1140 - val_loss: 1110.9047\n",
      "Epoch 93/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1221.7412 - val_loss: 1097.0140\n",
      "Epoch 94/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1197.7092 - val_loss: 1083.3851\n",
      "Epoch 95/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1174.0151 - val_loss: 1070.0155\n",
      "Epoch 96/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1150.6564 - val_loss: 1056.9028\n",
      "Epoch 97/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1127.6299 - val_loss: 1044.0443\n",
      "Epoch 98/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1104.9336 - val_loss: 1031.4381\n",
      "Epoch 99/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1082.5642 - val_loss: 1019.0821\n",
      "Epoch 100/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1060.5198 - val_loss: 1006.9733\n",
      "Epoch 101/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1038.7974 - val_loss: 995.1100\n",
      "Epoch 102/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1017.3943 - val_loss: 983.4893\n",
      "Epoch 103/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 996.3078 - val_loss: 972.1094\n",
      "Epoch 104/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 975.5353 - val_loss: 960.9682\n",
      "Epoch 105/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 955.0750 - val_loss: 950.0627\n",
      "Epoch 106/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 934.9233 - val_loss: 939.3910\n",
      "Epoch 107/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 915.0782 - val_loss: 928.9509\n",
      "Epoch 108/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 895.5366 - val_loss: 918.7397\n",
      "Epoch 109/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 876.2965 - val_loss: 908.7553\n",
      "Epoch 110/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 857.3550 - val_loss: 898.9957\n",
      "Epoch 111/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 838.7098 - val_loss: 889.4584\n",
      "Epoch 112/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 820.3577 - val_loss: 880.1412\n",
      "Epoch 113/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 802.2969 - val_loss: 871.0417\n",
      "Epoch 114/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 784.5247 - val_loss: 862.1578\n",
      "Epoch 115/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 767.0385 - val_loss: 853.4871\n",
      "Epoch 116/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 749.8355 - val_loss: 845.0274\n",
      "Epoch 117/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 732.9137 - val_loss: 836.7761\n",
      "Epoch 118/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 716.2700 - val_loss: 828.7314\n",
      "Epoch 119/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 699.9025 - val_loss: 820.8909\n",
      "Epoch 120/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 683.8080 - val_loss: 813.2523\n",
      "Epoch 121/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 667.9846 - val_loss: 805.8137\n",
      "Epoch 122/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 652.4295 - val_loss: 798.5723\n",
      "Epoch 123/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 637.1409 - val_loss: 791.5259\n",
      "Epoch 124/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 622.1152 - val_loss: 784.6727\n",
      "Epoch 125/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 607.3505 - val_loss: 778.0099\n",
      "Epoch 126/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 592.8443 - val_loss: 771.5356\n",
      "Epoch 127/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 578.5943 - val_loss: 765.2474\n",
      "Epoch 128/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 564.5977 - val_loss: 759.1429\n",
      "Epoch 129/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 550.8520 - val_loss: 753.2202\n",
      "Epoch 130/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 537.3547 - val_loss: 747.4767\n",
      "Epoch 131/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 524.1036 - val_loss: 741.9104\n",
      "Epoch 132/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 511.0959 - val_loss: 736.5189\n",
      "Epoch 133/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 498.3293 - val_loss: 731.2997\n",
      "Epoch 134/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 485.8012 - val_loss: 726.2508\n",
      "Epoch 135/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 473.5092 - val_loss: 721.3700\n",
      "Epoch 136/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 461.4510 - val_loss: 716.6552\n",
      "Epoch 137/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 449.6242 - val_loss: 712.1036\n",
      "Epoch 138/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 438.0259 - val_loss: 707.7132\n",
      "Epoch 139/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 426.6538 - val_loss: 703.4818\n",
      "Epoch 140/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 415.5053 - val_loss: 699.4069\n",
      "Epoch 141/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 404.5780 - val_loss: 695.4866\n",
      "Epoch 142/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 393.8701 - val_loss: 691.7184\n",
      "Epoch 143/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 383.3785 - val_loss: 688.1000\n",
      "Epoch 144/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 373.1008 - val_loss: 684.6293\n",
      "Epoch 145/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 363.0349 - val_loss: 681.3038\n",
      "Epoch 146/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 353.1776 - val_loss: 678.1214\n",
      "Epoch 147/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 343.5270 - val_loss: 675.0798\n",
      "Epoch 148/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 334.0806 - val_loss: 672.1765\n",
      "Epoch 149/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 324.8357 - val_loss: 669.4094\n",
      "Epoch 150/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 315.7900 - val_loss: 666.7765\n",
      "Epoch 151/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 306.9411 - val_loss: 664.2748\n",
      "Epoch 152/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 298.2864 - val_loss: 661.9027\n",
      "Epoch 153/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 289.8235 - val_loss: 659.6575\n",
      "Epoch 154/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 281.5499 - val_loss: 657.5372\n",
      "Epoch 155/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 273.4634 - val_loss: 655.5394\n",
      "Epoch 156/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 265.5612 - val_loss: 653.6619\n",
      "Epoch 157/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 257.8410 - val_loss: 651.9021\n",
      "Epoch 158/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 250.3002 - val_loss: 650.2581\n",
      "Epoch 159/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 242.9366 - val_loss: 648.7274\n",
      "Epoch 160/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 235.7475 - val_loss: 647.3077\n",
      "Epoch 161/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 228.7306 - val_loss: 645.9969\n",
      "Epoch 162/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 221.8835 - val_loss: 644.7927\n",
      "Epoch 163/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 215.2035 - val_loss: 643.6928\n",
      "Epoch 164/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 208.6886 - val_loss: 642.6948\n",
      "Epoch 165/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 202.3359 - val_loss: 641.7965\n",
      "Epoch 166/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 196.1432 - val_loss: 640.9957\n",
      "Epoch 167/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 190.1082 - val_loss: 640.2902\n",
      "Epoch 168/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 184.2283 - val_loss: 639.6775\n",
      "Epoch 169/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 178.5011 - val_loss: 639.1556\n",
      "Epoch 170/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 172.9241 - val_loss: 638.7220\n",
      "Epoch 171/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 167.4951 - val_loss: 638.3747\n",
      "Epoch 172/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 162.2114 - val_loss: 638.1113\n",
      "Epoch 173/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 157.0709 - val_loss: 637.9296\n",
      "Epoch 174/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 152.0708 - val_loss: 637.8275\n",
      "Epoch 175/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 147.2090 - val_loss: 637.8027\n",
      "Epoch 176/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 142.4832 - val_loss: 637.8529\n",
      "Epoch 177/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 137.8906 - val_loss: 637.9759\n",
      "Epoch 178/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 133.4293 - val_loss: 638.1697\n",
      "Epoch 179/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 129.0967 - val_loss: 638.4319\n",
      "Epoch 180/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 124.8902 - val_loss: 638.7603\n",
      "Epoch 181/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 120.8080 - val_loss: 639.1530\n",
      "Epoch 182/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 116.8473 - val_loss: 639.6077\n",
      "Epoch 183/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 113.0058 - val_loss: 640.1223\n",
      "Epoch 184/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 109.2814 - val_loss: 640.6946\n",
      "Epoch 185/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 105.6716 - val_loss: 641.3223\n",
      "Epoch 186/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 102.1743 - val_loss: 642.0037\n",
      "Epoch 187/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 98.7871 - val_loss: 642.7365\n",
      "Epoch 188/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 95.5076 - val_loss: 643.5186\n",
      "Epoch 189/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 92.3337 - val_loss: 644.3480\n",
      "Epoch 190/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 89.2630 - val_loss: 645.2226\n",
      "Epoch 191/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 86.2933 - val_loss: 646.1404\n",
      "Epoch 192/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 83.4224 - val_loss: 647.0994\n",
      "Epoch 193/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 80.6483 - val_loss: 648.0978\n",
      "Epoch 194/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 77.9684 - val_loss: 649.1334\n",
      "Epoch 195/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 75.3807 - val_loss: 650.2043\n",
      "Epoch 196/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 72.8831 - val_loss: 651.3085\n",
      "Epoch 197/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 70.4733 - val_loss: 652.4443\n",
      "Epoch 198/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 68.1493 - val_loss: 653.6097\n",
      "Epoch 199/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 65.9090 - val_loss: 654.8028\n",
      "Epoch 200/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 63.7502 - val_loss: 656.0219\n",
      "Epoch 201/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 61.6708 - val_loss: 657.2651\n",
      "Epoch 202/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 59.6689 - val_loss: 658.5308\n",
      "Epoch 203/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 57.7423 - val_loss: 659.8170\n",
      "Epoch 204/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 55.8892 - val_loss: 661.1220\n",
      "Epoch 205/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 54.1074 - val_loss: 662.4442\n",
      "Epoch 206/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 52.3952 - val_loss: 663.7819\n",
      "Epoch 207/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 50.7504 - val_loss: 665.1335\n",
      "Epoch 208/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 49.1712 - val_loss: 666.4974\n",
      "Epoch 209/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 47.6556 - val_loss: 667.8718\n",
      "Epoch 210/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 46.2019 - val_loss: 669.2556\n",
      "Epoch 211/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 44.8081 - val_loss: 670.6469\n",
      "Epoch 212/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 43.4725 - val_loss: 672.0444\n",
      "Epoch 213/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 42.1933 - val_loss: 673.4465\n",
      "Epoch 214/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 40.9687 - val_loss: 674.8519\n",
      "Epoch 215/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 39.7970 - val_loss: 676.2593\n",
      "Epoch 216/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 38.6764 - val_loss: 677.6672\n",
      "Epoch 217/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 37.6053 - val_loss: 679.0746\n",
      "Epoch 218/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 36.5821 - val_loss: 680.4797\n",
      "Epoch 219/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 35.6051 - val_loss: 681.8816\n",
      "Epoch 220/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 34.6728 - val_loss: 683.2794\n",
      "Epoch 221/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 33.7836 - val_loss: 684.6712\n",
      "Epoch 222/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 32.9359 - val_loss: 686.0565\n",
      "Epoch 223/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 32.1283 - val_loss: 687.4343\n",
      "Epoch 224/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 31.3592 - val_loss: 688.8032\n",
      "Epoch 225/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 30.6274 - val_loss: 690.1622\n",
      "Epoch 226/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 29.9313 - val_loss: 691.5104\n",
      "Epoch 227/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 29.2696 - val_loss: 692.8470\n",
      "Epoch 228/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.6411 - val_loss: 694.1712\n",
      "Epoch 229/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 28.0442 - val_loss: 695.4819\n",
      "Epoch 230/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.4778 - val_loss: 696.7784\n",
      "Epoch 231/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.9407 - val_loss: 698.0600\n",
      "Epoch 232/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 26.4316 - val_loss: 699.3260\n",
      "Epoch 233/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.9494 - val_loss: 700.5757\n",
      "Epoch 234/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.4929 - val_loss: 701.8082\n",
      "Epoch 235/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.0610 - val_loss: 703.0236\n",
      "Epoch 236/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.6526 - val_loss: 704.2206\n",
      "Epoch 237/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.2666 - val_loss: 705.3989\n",
      "Epoch 238/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 23.9023 - val_loss: 706.5582\n",
      "Epoch 239/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 23.5584 - val_loss: 707.6979\n",
      "Epoch 240/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 23.2340 - val_loss: 708.8177\n",
      "Epoch 241/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 22.9282 - val_loss: 709.9169\n",
      "Epoch 242/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 22.6402 - val_loss: 710.9960\n",
      "Epoch 243/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.3689 - val_loss: 712.0536\n",
      "Epoch 244/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 22.1138 - val_loss: 713.0902\n",
      "Epoch 245/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.8739 - val_loss: 714.1055\n",
      "Epoch 246/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.6485 - val_loss: 715.0991\n",
      "Epoch 247/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.4368 - val_loss: 716.0710\n",
      "Epoch 248/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.2380 - val_loss: 717.0206\n",
      "Epoch 249/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.0516 - val_loss: 717.9485\n",
      "Epoch 250/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.8770 - val_loss: 718.8541\n",
      "Epoch 251/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.7133 - val_loss: 719.7381\n",
      "Epoch 252/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.5601 - val_loss: 720.5995\n",
      "Epoch 253/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.4168 - val_loss: 721.4391\n",
      "Epoch 254/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.2827 - val_loss: 722.2565\n",
      "Epoch 255/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1574 - val_loss: 723.0524\n",
      "Epoch 256/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.0404 - val_loss: 723.8264\n",
      "Epoch 257/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.9312 - val_loss: 724.5786\n",
      "Epoch 258/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.8293 - val_loss: 725.3094\n",
      "Epoch 259/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.7343 - val_loss: 726.0184\n",
      "Epoch 260/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.6458 - val_loss: 726.7067\n",
      "Epoch 261/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.5633 - val_loss: 727.3740\n",
      "Epoch 262/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.4866 - val_loss: 728.0206\n",
      "Epoch 263/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.4152 - val_loss: 728.6467\n",
      "Epoch 264/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 19.3488 - val_loss: 729.2527\n",
      "Epoch 265/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2872 - val_loss: 729.8392\n",
      "Epoch 266/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2299 - val_loss: 730.4056\n",
      "Epoch 267/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1767 - val_loss: 730.9529\n",
      "Epoch 268/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1273 - val_loss: 731.4817\n",
      "Epoch 269/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.0816 - val_loss: 731.9911\n",
      "Epoch 270/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 19.0391 - val_loss: 732.4829\n",
      "Epoch 271/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 18.9998 - val_loss: 732.9566\n",
      "Epoch 272/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.9634 - val_loss: 733.4130\n",
      "Epoch 273/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.9297 - val_loss: 733.8517\n",
      "Epoch 274/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.8985 - val_loss: 734.2742\n",
      "Epoch 275/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.8696 - val_loss: 734.6803\n",
      "Epoch 276/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 18.8429 - val_loss: 735.0703\n",
      "Epoch 277/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 18.8182 - val_loss: 735.4445\n",
      "Epoch 278/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.7954 - val_loss: 735.8037\n",
      "Epoch 279/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.7744 - val_loss: 736.1479\n",
      "Epoch 280/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.7550 - val_loss: 736.4780\n",
      "Epoch 281/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.7370 - val_loss: 736.7940\n",
      "Epoch 282/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.7205 - val_loss: 737.0964\n",
      "Epoch 283/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.7052 - val_loss: 737.3857\n",
      "Epoch 284/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.6912 - val_loss: 737.6622\n",
      "Epoch 285/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.6782 - val_loss: 737.9262\n",
      "Epoch 286/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.6662 - val_loss: 738.1783\n",
      "Epoch 287/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.6552 - val_loss: 738.4191\n",
      "Epoch 288/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.6450 - val_loss: 738.6484\n",
      "Epoch 289/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.6357 - val_loss: 738.8671\n",
      "Epoch 290/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.6271 - val_loss: 739.0750\n",
      "Epoch 291/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.6192 - val_loss: 739.2731\n",
      "Epoch 292/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.6119 - val_loss: 739.4613\n",
      "Epoch 293/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.6052 - val_loss: 739.6404\n",
      "Epoch 294/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5991 - val_loss: 739.8102\n",
      "Epoch 295/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 18.5934 - val_loss: 739.9716\n",
      "Epoch 296/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5882 - val_loss: 740.1246\n",
      "Epoch 297/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5835 - val_loss: 740.2700\n",
      "Epoch 298/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 18.5792 - val_loss: 740.4077\n",
      "Epoch 299/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5752 - val_loss: 740.5378\n",
      "Epoch 300/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 18.5715 - val_loss: 740.6614\n",
      "Epoch 301/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5681 - val_loss: 740.7775\n",
      "Epoch 302/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5651 - val_loss: 740.8877\n",
      "Epoch 303/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5623 - val_loss: 740.9915\n",
      "Epoch 304/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 18.5598 - val_loss: 741.0896\n",
      "Epoch 305/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 18.5574 - val_loss: 741.1817\n",
      "Epoch 306/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5554 - val_loss: 741.2689\n",
      "Epoch 307/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5535 - val_loss: 741.3507\n",
      "Epoch 308/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5517 - val_loss: 741.4278\n",
      "Epoch 309/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5502 - val_loss: 741.5004\n",
      "Epoch 310/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5488 - val_loss: 741.5686\n",
      "Epoch 311/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5476 - val_loss: 741.6324\n",
      "Epoch 312/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5464 - val_loss: 741.6928\n",
      "Epoch 313/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5454 - val_loss: 741.7490\n",
      "Epoch 314/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5446 - val_loss: 741.8018\n",
      "Epoch 315/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5438 - val_loss: 741.8507\n",
      "Epoch 316/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5431 - val_loss: 741.8970\n",
      "Epoch 317/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5426 - val_loss: 741.9403\n",
      "Epoch 318/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5421 - val_loss: 741.9807\n",
      "Epoch 319/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 18.5416 - val_loss: 742.0182\n",
      "Epoch 320/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5413 - val_loss: 742.0530\n",
      "Epoch 321/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5410 - val_loss: 742.0856\n",
      "Epoch 322/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5408 - val_loss: 742.1159\n",
      "Epoch 323/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 18.5407 - val_loss: 742.1441\n",
      "Epoch 324/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5406 - val_loss: 742.1703\n",
      "Epoch 325/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 18.5406 - val_loss: 742.1951\n",
      "Epoch 326/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5405 - val_loss: 742.2175\n",
      "Epoch 327/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 18.5406 - val_loss: 742.2383\n",
      "Epoch 328/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5406 - val_loss: 742.2578\n",
      "Epoch 329/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5408 - val_loss: 742.2755\n",
      "Epoch 330/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5410 - val_loss: 742.2922\n",
      "Epoch 331/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5412 - val_loss: 742.3077\n",
      "Epoch 332/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 18.5413 - val_loss: 742.3217\n",
      "Epoch 333/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5416 - val_loss: 742.3347\n",
      "Epoch 334/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 18.5418 - val_loss: 742.3464\n",
      "Epoch 335/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 18.5421 - val_loss: 742.3573\n",
      "Epoch 336/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5424 - val_loss: 742.3672\n",
      "Epoch 337/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5428 - val_loss: 742.3764\n",
      "Epoch 338/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5431 - val_loss: 742.3844\n",
      "Epoch 339/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5435 - val_loss: 742.3918\n",
      "Epoch 340/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 18.5439 - val_loss: 742.3990\n",
      "Epoch 341/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5442 - val_loss: 742.4052\n",
      "Epoch 342/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 18.5447 - val_loss: 742.4108\n",
      "Epoch 343/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 18.5450 - val_loss: 742.4155\n",
      "Epoch 344/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5455 - val_loss: 742.4203\n",
      "Epoch 345/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5459 - val_loss: 742.4243\n",
      "Epoch 346/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5463 - val_loss: 742.4276\n",
      "Epoch 347/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5468 - val_loss: 742.4311\n",
      "Epoch 348/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5473 - val_loss: 742.4340\n",
      "Epoch 349/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5477 - val_loss: 742.4366\n",
      "Epoch 350/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5482 - val_loss: 742.4391\n",
      "Epoch 351/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5486 - val_loss: 742.4407\n",
      "Epoch 352/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5490 - val_loss: 742.4424\n",
      "Epoch 353/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5495 - val_loss: 742.4432\n",
      "Epoch 354/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5500 - val_loss: 742.4446\n",
      "Epoch 355/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5505 - val_loss: 742.4455\n",
      "Epoch 356/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5509 - val_loss: 742.4464\n",
      "Epoch 357/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5514 - val_loss: 742.4469\n",
      "Epoch 358/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5518 - val_loss: 742.4472\n",
      "Epoch 359/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 18.5523 - val_loss: 742.4475\n",
      "Epoch 360/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5528 - val_loss: 742.4478\n",
      "Epoch 361/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5532 - val_loss: 742.4478\n",
      "Epoch 362/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 18.5537 - val_loss: 742.4476\n",
      "Epoch 363/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5541 - val_loss: 742.4474\n",
      "Epoch 364/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 18.5546 - val_loss: 742.4469\n",
      "Epoch 365/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5551 - val_loss: 742.4465\n",
      "Epoch 366/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5555 - val_loss: 742.4461\n",
      "Epoch 367/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5559 - val_loss: 742.4453\n",
      "Epoch 368/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5564 - val_loss: 742.4448\n",
      "Epoch 369/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5568 - val_loss: 742.4443\n",
      "Epoch 370/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5573 - val_loss: 742.4434\n",
      "Epoch 371/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5577 - val_loss: 742.4427\n",
      "Epoch 372/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5581 - val_loss: 742.4423\n",
      "Epoch 373/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5585 - val_loss: 742.4414\n",
      "Epoch 374/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5589 - val_loss: 742.4404\n",
      "Epoch 375/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5594 - val_loss: 742.4394\n",
      "Epoch 376/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5598 - val_loss: 742.4387\n",
      "Epoch 377/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5602 - val_loss: 742.4377\n",
      "Epoch 378/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5605 - val_loss: 742.4368\n",
      "Epoch 379/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5609 - val_loss: 742.4359\n",
      "Epoch 380/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5614 - val_loss: 742.4352\n",
      "Epoch 381/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5617 - val_loss: 742.4343\n",
      "Epoch 382/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5621 - val_loss: 742.4334\n",
      "Epoch 383/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 18.5625 - val_loss: 742.4325\n",
      "Epoch 384/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5628 - val_loss: 742.4313\n",
      "Epoch 385/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5632 - val_loss: 742.4304\n",
      "Epoch 386/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 18.5635 - val_loss: 742.4290\n",
      "Epoch 387/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5639 - val_loss: 742.4281\n",
      "Epoch 388/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5642 - val_loss: 742.4270\n",
      "Epoch 389/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5646 - val_loss: 742.4264\n",
      "Epoch 390/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5650 - val_loss: 742.4255\n",
      "Epoch 391/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5653 - val_loss: 742.4245\n",
      "Epoch 392/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5656 - val_loss: 742.4236\n",
      "Epoch 393/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 18.5659 - val_loss: 742.4227\n",
      "Epoch 394/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5662 - val_loss: 742.4216\n",
      "Epoch 395/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5665 - val_loss: 742.4207\n",
      "Epoch 396/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5669 - val_loss: 742.4198\n",
      "Epoch 397/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5672 - val_loss: 742.4190\n",
      "Epoch 398/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5674 - val_loss: 742.4182\n",
      "Epoch 399/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5677 - val_loss: 742.4170\n",
      "Epoch 400/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 18.5681 - val_loss: 742.4163\n",
      "Epoch 401/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 18.5683 - val_loss: 742.4155\n",
      "Epoch 402/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5686 - val_loss: 742.4147\n",
      "Epoch 403/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5689 - val_loss: 742.4139\n",
      "Epoch 404/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5692 - val_loss: 742.4130\n",
      "Epoch 405/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5694 - val_loss: 742.4122\n",
      "Epoch 406/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5697 - val_loss: 742.4116\n",
      "Epoch 407/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5699 - val_loss: 742.4108\n",
      "Epoch 408/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5702 - val_loss: 742.4097\n",
      "Epoch 409/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5704 - val_loss: 742.4091\n",
      "Epoch 410/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 18.5707 - val_loss: 742.4083\n",
      "Epoch 411/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5709 - val_loss: 742.4075\n",
      "Epoch 412/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5711 - val_loss: 742.4069\n",
      "Epoch 413/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5714 - val_loss: 742.4064\n",
      "Epoch 414/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5716 - val_loss: 742.4055\n",
      "Epoch 415/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 18.5718 - val_loss: 742.4045\n",
      "Epoch 416/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 18.5720 - val_loss: 742.4036\n",
      "Epoch 417/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5723 - val_loss: 742.4031\n",
      "Epoch 418/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5725 - val_loss: 742.4030\n",
      "Epoch 419/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5727 - val_loss: 742.4021\n",
      "Epoch 420/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5729 - val_loss: 742.4014\n",
      "Epoch 421/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 18.5730 - val_loss: 742.4008\n",
      "Epoch 422/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5733 - val_loss: 742.4001\n",
      "Epoch 423/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5735 - val_loss: 742.3997\n",
      "Epoch 424/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5736 - val_loss: 742.3992\n",
      "Epoch 425/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5738 - val_loss: 742.3984\n",
      "Epoch 426/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5740 - val_loss: 742.3978\n",
      "Epoch 427/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5742 - val_loss: 742.3970\n",
      "Epoch 428/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5744 - val_loss: 742.3967\n",
      "Epoch 429/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5745 - val_loss: 742.3960\n",
      "Epoch 430/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5747 - val_loss: 742.3954\n",
      "Epoch 431/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 18.5749 - val_loss: 742.3951\n",
      "Epoch 432/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 18.5750 - val_loss: 742.3947\n",
      "Epoch 433/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5752 - val_loss: 742.3943\n",
      "Epoch 434/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5753 - val_loss: 742.3939\n",
      "Epoch 435/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5755 - val_loss: 742.3932\n",
      "Epoch 436/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5757 - val_loss: 742.3929\n",
      "Epoch 437/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 18.5758 - val_loss: 742.3925\n",
      "Epoch 438/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 18.5759 - val_loss: 742.3919\n",
      "Epoch 439/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5761 - val_loss: 742.3915\n",
      "Epoch 440/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5762 - val_loss: 742.3912\n",
      "Epoch 441/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5763 - val_loss: 742.3910\n",
      "Epoch 442/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 18.5765 - val_loss: 742.3908\n",
      "Epoch 443/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5766 - val_loss: 742.3901\n",
      "Epoch 444/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5767 - val_loss: 742.3898\n",
      "Epoch 445/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5768 - val_loss: 742.3892\n",
      "Epoch 446/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5769 - val_loss: 742.3889\n",
      "Epoch 447/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5771 - val_loss: 742.3887\n",
      "Epoch 448/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5772 - val_loss: 742.3882\n",
      "Epoch 449/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5773 - val_loss: 742.3878\n",
      "Epoch 450/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 18.5774 - val_loss: 742.3875\n",
      "Epoch 451/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5775 - val_loss: 742.3871\n",
      "Epoch 452/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 18.5776 - val_loss: 742.3870\n",
      "Epoch 453/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5777 - val_loss: 742.3865\n",
      "Epoch 454/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5778 - val_loss: 742.3860\n",
      "Epoch 455/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5779 - val_loss: 742.3856\n",
      "Epoch 456/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5780 - val_loss: 742.3851\n",
      "Epoch 457/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5782 - val_loss: 742.3851\n",
      "Epoch 458/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5782 - val_loss: 742.3847\n",
      "Epoch 459/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5783 - val_loss: 742.3845\n",
      "Epoch 460/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 18.5784 - val_loss: 742.3840\n",
      "Epoch 461/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5785 - val_loss: 742.3836\n",
      "Epoch 462/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5786 - val_loss: 742.3835\n",
      "Epoch 463/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5786 - val_loss: 742.3831\n",
      "Epoch 464/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5788 - val_loss: 742.3828\n",
      "Epoch 465/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5788 - val_loss: 742.3828\n",
      "Epoch 466/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5789 - val_loss: 742.3826\n",
      "Epoch 467/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5790 - val_loss: 742.3824\n",
      "Epoch 468/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 18.5791 - val_loss: 742.3821\n",
      "Epoch 469/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5792 - val_loss: 742.3820\n",
      "Epoch 470/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5792 - val_loss: 742.3820\n",
      "Epoch 471/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5793 - val_loss: 742.3820\n",
      "Epoch 472/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 18.5793 - val_loss: 742.3818\n",
      "Epoch 473/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 18.5794 - val_loss: 742.3814\n",
      "Epoch 474/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5795 - val_loss: 742.3814\n",
      "Epoch 475/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5795 - val_loss: 742.3810\n",
      "Epoch 476/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5796 - val_loss: 742.3807\n",
      "Epoch 477/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 18.5797 - val_loss: 742.3804\n",
      "Epoch 478/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5797 - val_loss: 742.3800\n",
      "Epoch 479/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5798 - val_loss: 742.3796\n",
      "Epoch 480/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5798 - val_loss: 742.3793\n",
      "Epoch 481/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5799 - val_loss: 742.3793\n",
      "Epoch 482/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5800 - val_loss: 742.3793\n",
      "Epoch 483/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5801 - val_loss: 742.3793\n",
      "Epoch 484/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5801 - val_loss: 742.3791\n",
      "Epoch 485/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5801 - val_loss: 742.3789\n",
      "Epoch 486/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5802 - val_loss: 742.3787\n",
      "Epoch 487/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5802 - val_loss: 742.3785\n",
      "Epoch 488/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 18.5803 - val_loss: 742.3785\n",
      "Epoch 489/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5803 - val_loss: 742.3785\n",
      "Epoch 490/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 18.5804 - val_loss: 742.3781\n",
      "Epoch 491/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5804 - val_loss: 742.3778\n",
      "Epoch 492/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5805 - val_loss: 742.3776\n",
      "Epoch 493/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5805 - val_loss: 742.3774\n",
      "Epoch 494/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5805 - val_loss: 742.3773\n",
      "Epoch 495/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5806 - val_loss: 742.3770\n",
      "Epoch 496/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5806 - val_loss: 742.3770\n",
      "Epoch 497/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5806 - val_loss: 742.3766\n",
      "Epoch 498/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5807 - val_loss: 742.3764\n",
      "Epoch 499/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5807 - val_loss: 742.3762\n",
      "Epoch 500/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 18.5808 - val_loss: 742.3764\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.1552, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0026, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0004, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0102, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.8734, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 324ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[68.2835434 , 68.2415266 , 68.1995098 , 68.157493  , 68.1154762 ,\n",
       "        68.0734594 , 68.0314426 ,  0.        ,  0.32545432,  0.78533208,\n",
       "         0.23574361,  0.        ,  0.        , 69.4873016 , 69.3528478 ,\n",
       "        69.218394  , 69.0839402 , 68.9494865 , 68.8150327 , 68.6805789 ,\n",
       "        68.5461251 , 68.4116713 , 68.2928805 , 68.2508637 , 68.2088469 ,\n",
       "        68.1668301 , 68.1248133 , 68.0827965 , 68.0407797 , 67.9987628 ,\n",
       "        67.956746  , 67.9147292 , 67.8727124 , 67.8306956 , 67.7773576 ,\n",
       "        67.693324  , 67.6092904 , 67.525568  , 67.4412232 , 67.3571895 ,\n",
       "         0.        ,  0.95224452,  0.        ,  0.17251213,  0.        ,\n",
       "         0.30021507,  0.        , 68.1761671 , 68.1341503 , 68.0921335 ,\n",
       "        68.0501167 , 68.0080999 , 67.9660831 , 67.9240663 , 67.8820495 ,\n",
       "        67.8400327 , 67.7960317 , 67.7400103 , 67.680036  , 67.6200606 ,\n",
       "        67.5600852 , 67.5001098 , 67.4401344 , 67.380159  , 67.3201836 ,\n",
       "        67.2602082 , 67.2002328 , 67.1402574 , 67.080282  , 67.0203066 ,\n",
       "        66.9603312 , 66.9003558 , 66.8403804 , 66.780405  , 74.5072708 ,\n",
       "         0.93779647,  0.49107257,  0.        ,  0.        ,  0.54438782,\n",
       "        62.54023361,  0.        ,  0.        ,  0.        ,  0.65842271,\n",
       "         0.        ,  0.52089834,  0.        ,  0.        ,  0.92265719,\n",
       "         0.        ,  0.51473701,  0.94453633,  0.24442138,  0.        ,\n",
       "         0.20587075,  0.3232398 ,  0.        ,  0.23263276,  0.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([55.97866366, 55.9630867 , 55.94750974, 55.93193277, 55.91635581,\n",
       "       55.90077885, 55.88520189, 55.86962492, 55.85404796, 55.838471  ,\n",
       "       55.82289404, 55.80731707, 55.79174011, 55.77616315, 55.76058619,\n",
       "       55.74500922, 55.72943226, 55.7138553 , 55.69827834, 55.68270137,\n",
       "       55.66712441, 55.65154745, 55.63597049, 55.62039352, 55.60481656,\n",
       "       55.5892396 , 55.57366264, 55.55808567, 55.54250871, 55.52693175,\n",
       "       55.51135479, 55.49577782, 55.48020086, 55.4646239 , 55.44904694,\n",
       "       55.43346997, 55.41789301, 55.40231605, 55.38673909, 55.37116212,\n",
       "       55.35558516, 55.3400082 , 55.32443124, 55.30885427, 55.29327731,\n",
       "       55.27770035, 55.26212339, 55.24654642, 55.23096946, 55.2153925 ,\n",
       "       55.19981554, 55.18423857, 55.16866161, 55.15308465, 55.13750769,\n",
       "       55.12193072, 55.10635376, 55.0907768 , 55.07519984, 55.05962287,\n",
       "       55.04404591, 55.02846895, 55.01289199, 54.99731502, 54.98173806,\n",
       "       54.9661611 , 54.95058414, 54.93500717, 54.91943021, 54.90385325,\n",
       "       54.88827629, 54.87269932, 54.85712236, 54.8415454 , 54.82596844,\n",
       "       54.81039147, 54.79481451, 54.77923755, 54.76366059, 54.74808362,\n",
       "       54.73250666, 54.7169297 , 54.70135274, 54.68577577, 54.67019881,\n",
       "       54.65462185, 54.63904489, 54.62346792, 54.60789096, 54.592314  ,\n",
       "       54.57673704, 54.56116007, 54.54558311, 54.53000615, 54.51442919,\n",
       "       54.49885222, 54.48327526, 54.4676983 , 54.45212134, 54.43654437])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.74851178544926\n",
      "28.16061168306453\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
