{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "1845    65.224440\n",
       "1846    65.201097\n",
       "1847    65.177754\n",
       "1848    65.154412\n",
       "1849    65.131069\n",
       "Name: C2, Length: 1850, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c2_interpolated_1750_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "1745    66.498786\n",
       "1746    66.491317\n",
       "1747    66.483847\n",
       "1748    66.476377\n",
       "1749    66.468908\n",
       "Name: C2, Length: 1750, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1750)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.500000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.275910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.051821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.827731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.603641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     90.500000  0.000298\n",
       "1     90.275910  0.000298\n",
       "2     90.051821  0.000297\n",
       "3     89.827731  0.000297\n",
       "4     89.603641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.1552  # Value for C0\n",
    "K0 = -0.0026  # Value for K0\n",
    "K1 = -0.0004  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0102    # Value for b\n",
    "c = 2.8734    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    90.500000    90.275910    90.051821    89.827731    89.603641   \n",
      "351    90.275910    90.051821    89.827731    89.603641    89.379552   \n",
      "352    90.051821    89.827731    89.603641    89.379552    89.155462   \n",
      "353    89.827731    89.603641    89.379552    89.155462    88.931373   \n",
      "354    89.603641    89.379552    89.155462    88.931373    88.707283   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    89.379552    89.155462    88.931373    88.707283    88.494958  ...   \n",
      "351    89.155462    88.931373    88.707283    88.494958    88.427731  ...   \n",
      "352    88.931373    88.707283    88.494958    88.427731    88.360504  ...   \n",
      "353    88.707283    88.494958    88.427731    88.360504    88.293277  ...   \n",
      "354    88.494958    88.427731    88.360504    88.293277    88.226050  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   79.071008    0.000263   79.054202    0.000263   79.037395    0.000263   \n",
      "351   79.054202    0.000263   79.037395    0.000263   79.020588    0.000262   \n",
      "352   79.037395    0.000263   79.020588    0.000262   79.003782    0.000262   \n",
      "353   79.020588    0.000262   79.003782    0.000262   78.986975    0.000262   \n",
      "354   79.003782    0.000262   78.986975    0.000262   78.970168    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   79.020588    0.000262   79.003782    0.000262  \n",
      "351   79.003782    0.000262   78.986975    0.000262  \n",
      "352   78.986975    0.000262   78.970168    0.000262  \n",
      "353   78.970168    0.000262   78.953361    0.000262  \n",
      "354   78.953361    0.000262   78.936555    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1300, 1, 251) (1300, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "17/17 [==============================] - 3s 31ms/step - loss: 5599.8677 - val_loss: 4651.8198\n",
      "Epoch 2/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 5483.4668 - val_loss: 4568.7700\n",
      "Epoch 3/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 5410.1787 - val_loss: 4515.4111\n",
      "Epoch 4/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 5351.3799 - val_loss: 4462.3486\n",
      "Epoch 5/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 5293.8862 - val_loss: 4410.1665\n",
      "Epoch 6/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 5237.4155 - val_loss: 4358.9141\n",
      "Epoch 7/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 5181.8496 - val_loss: 4308.4229\n",
      "Epoch 8/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 5127.0278 - val_loss: 4258.5781\n",
      "Epoch 9/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 5072.8462 - val_loss: 4209.3062\n",
      "Epoch 10/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 5019.2417 - val_loss: 4160.5625\n",
      "Epoch 11/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4966.1714 - val_loss: 4112.3145\n",
      "Epoch 12/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 4913.6055 - val_loss: 4064.5398\n",
      "Epoch 13/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4861.5239 - val_loss: 4017.2222\n",
      "Epoch 14/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4809.9092 - val_loss: 3970.3481\n",
      "Epoch 15/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 4758.7480 - val_loss: 3923.9067\n",
      "Epoch 16/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4708.0312 - val_loss: 3877.8894\n",
      "Epoch 17/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 4657.7500 - val_loss: 3832.2878\n",
      "Epoch 18/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 4607.8945 - val_loss: 3787.0964\n",
      "Epoch 19/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4558.4604 - val_loss: 3742.3083\n",
      "Epoch 20/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4509.4414 - val_loss: 3697.9192\n",
      "Epoch 21/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4460.8311 - val_loss: 3653.9241\n",
      "Epoch 22/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4412.6265 - val_loss: 3610.3176\n",
      "Epoch 23/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4364.8218 - val_loss: 3567.0964\n",
      "Epoch 24/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4317.4126 - val_loss: 3524.2578\n",
      "Epoch 25/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4270.3960 - val_loss: 3481.7971\n",
      "Epoch 26/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 4223.7676 - val_loss: 3439.7107\n",
      "Epoch 27/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4177.5254 - val_loss: 3397.9949\n",
      "Epoch 28/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4131.6641 - val_loss: 3356.6484\n",
      "Epoch 29/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 4086.1812 - val_loss: 3315.6665\n",
      "Epoch 30/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4031.9402 - val_loss: 3259.3472\n",
      "Epoch 31/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3976.1736 - val_loss: 3213.4539\n",
      "Epoch 32/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3926.0662 - val_loss: 3168.9592\n",
      "Epoch 33/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3877.3813 - val_loss: 3125.6140\n",
      "Epoch 34/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3829.8008 - val_loss: 3083.1594\n",
      "Epoch 35/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 3783.0935 - val_loss: 3041.4404\n",
      "Epoch 36/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3737.1189 - val_loss: 3000.3577\n",
      "Epoch 37/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3691.7891 - val_loss: 2959.8511\n",
      "Epoch 38/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3647.0447 - val_loss: 2919.8748\n",
      "Epoch 39/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3602.8445 - val_loss: 2880.3989\n",
      "Epoch 40/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3559.1567 - val_loss: 2841.3982\n",
      "Epoch 41/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3515.9578 - val_loss: 2802.8511\n",
      "Epoch 42/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3473.2271 - val_loss: 2764.7439\n",
      "Epoch 43/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3430.9500 - val_loss: 2727.0623\n",
      "Epoch 44/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3389.1123 - val_loss: 2689.7969\n",
      "Epoch 45/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3347.7036 - val_loss: 2652.9353\n",
      "Epoch 46/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3306.7119 - val_loss: 2616.4707\n",
      "Epoch 47/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3266.1313 - val_loss: 2580.3953\n",
      "Epoch 48/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3225.9521 - val_loss: 2544.7019\n",
      "Epoch 49/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 3186.1675 - val_loss: 2509.3843\n",
      "Epoch 50/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3146.7727 - val_loss: 2474.4380\n",
      "Epoch 51/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3107.7607 - val_loss: 2439.8555\n",
      "Epoch 52/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3069.1260 - val_loss: 2405.6350\n",
      "Epoch 53/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3030.8647 - val_loss: 2371.7703\n",
      "Epoch 54/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2992.9712 - val_loss: 2338.2571\n",
      "Epoch 55/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2955.4409 - val_loss: 2305.0906\n",
      "Epoch 56/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2918.2715 - val_loss: 2272.2693\n",
      "Epoch 57/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2881.4575 - val_loss: 2239.7881\n",
      "Epoch 58/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2844.9954 - val_loss: 2207.6436\n",
      "Epoch 59/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2808.8823 - val_loss: 2175.8335\n",
      "Epoch 60/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2773.1145 - val_loss: 2144.3525\n",
      "Epoch 61/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2737.6887 - val_loss: 2113.2002\n",
      "Epoch 62/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2702.6016 - val_loss: 2082.3726\n",
      "Epoch 63/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2667.8513 - val_loss: 2051.8657\n",
      "Epoch 64/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2633.4331 - val_loss: 2021.6776\n",
      "Epoch 65/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2599.3457 - val_loss: 1991.8068\n",
      "Epoch 66/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2565.5852 - val_loss: 1962.2482\n",
      "Epoch 67/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2532.1497 - val_loss: 1933.0012\n",
      "Epoch 68/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2499.0371 - val_loss: 1904.0629\n",
      "Epoch 69/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2466.2432 - val_loss: 1875.4301\n",
      "Epoch 70/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2433.7668 - val_loss: 1847.1007\n",
      "Epoch 71/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2401.6047 - val_loss: 1819.0730\n",
      "Epoch 72/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2369.7556 - val_loss: 1791.3439\n",
      "Epoch 73/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2338.2161 - val_loss: 1763.9117\n",
      "Epoch 74/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2306.9844 - val_loss: 1736.7732\n",
      "Epoch 75/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2276.0574 - val_loss: 1709.9277\n",
      "Epoch 76/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2245.4341 - val_loss: 1683.3717\n",
      "Epoch 77/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2215.1118 - val_loss: 1657.1042\n",
      "Epoch 78/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2185.0879 - val_loss: 1631.1221\n",
      "Epoch 79/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2155.3604 - val_loss: 1605.4229\n",
      "Epoch 80/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2125.9282 - val_loss: 1580.0060\n",
      "Epoch 81/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 2096.7881 - val_loss: 1554.8685\n",
      "Epoch 82/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2067.9382 - val_loss: 1530.0094\n",
      "Epoch 83/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2039.3774 - val_loss: 1505.4246\n",
      "Epoch 84/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2011.1024 - val_loss: 1481.1140\n",
      "Epoch 85/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1983.1123 - val_loss: 1457.0752\n",
      "Epoch 86/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1955.4047 - val_loss: 1433.3066\n",
      "Epoch 87/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1927.9779 - val_loss: 1409.8059\n",
      "Epoch 88/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1900.8298 - val_loss: 1386.5707\n",
      "Epoch 89/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1873.9584 - val_loss: 1363.6000\n",
      "Epoch 90/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1847.3622 - val_loss: 1340.8917\n",
      "Epoch 91/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1821.0383 - val_loss: 1318.4438\n",
      "Epoch 92/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1794.9862 - val_loss: 1296.2537\n",
      "Epoch 93/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1769.2029 - val_loss: 1274.3212\n",
      "Epoch 94/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1743.6879 - val_loss: 1252.6433\n",
      "Epoch 95/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1718.4386 - val_loss: 1231.2191\n",
      "Epoch 96/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1693.4532 - val_loss: 1210.0458\n",
      "Epoch 97/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1668.7300 - val_loss: 1189.1226\n",
      "Epoch 98/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1644.2675 - val_loss: 1168.4481\n",
      "Epoch 99/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1620.0637 - val_loss: 1148.0189\n",
      "Epoch 100/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1596.1172 - val_loss: 1127.8345\n",
      "Epoch 101/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1572.4258 - val_loss: 1107.8928\n",
      "Epoch 102/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1548.9879 - val_loss: 1088.1929\n",
      "Epoch 103/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1525.8026 - val_loss: 1068.7317\n",
      "Epoch 104/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1502.8669 - val_loss: 1049.5092\n",
      "Epoch 105/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 1480.1805 - val_loss: 1030.5221\n",
      "Epoch 106/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1457.7408 - val_loss: 1011.7695\n",
      "Epoch 107/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1435.5465 - val_loss: 993.2500\n",
      "Epoch 108/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1413.5958 - val_loss: 974.9615\n",
      "Epoch 109/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1391.8872 - val_loss: 956.9021\n",
      "Epoch 110/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1370.4188 - val_loss: 939.0716\n",
      "Epoch 111/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1349.1893 - val_loss: 921.4667\n",
      "Epoch 112/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1328.1974 - val_loss: 904.0860\n",
      "Epoch 113/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1307.4407 - val_loss: 886.9297\n",
      "Epoch 114/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1286.9186 - val_loss: 869.9938\n",
      "Epoch 115/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1266.6285 - val_loss: 853.2784\n",
      "Epoch 116/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1246.5693 - val_loss: 836.7812\n",
      "Epoch 117/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1226.7393 - val_loss: 820.5006\n",
      "Epoch 118/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1207.1375 - val_loss: 804.4354\n",
      "Epoch 119/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1187.7616 - val_loss: 788.5829\n",
      "Epoch 120/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1168.6102 - val_loss: 772.9431\n",
      "Epoch 121/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1149.6823 - val_loss: 757.5138\n",
      "Epoch 122/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1130.9760 - val_loss: 742.2932\n",
      "Epoch 123/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1112.4894 - val_loss: 727.2803\n",
      "Epoch 124/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1094.2216 - val_loss: 712.4732\n",
      "Epoch 125/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1076.1708 - val_loss: 697.8704\n",
      "Epoch 126/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1058.3356 - val_loss: 683.4703\n",
      "Epoch 127/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1040.7145 - val_loss: 669.2718\n",
      "Epoch 128/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1023.3059 - val_loss: 655.2728\n",
      "Epoch 129/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1006.1085 - val_loss: 641.4725\n",
      "Epoch 130/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 989.1205 - val_loss: 627.8683\n",
      "Epoch 131/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 972.3406 - val_loss: 614.4599\n",
      "Epoch 132/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 955.7676 - val_loss: 601.2449\n",
      "Epoch 133/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 939.3996 - val_loss: 588.2224\n",
      "Epoch 134/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 923.2349 - val_loss: 575.3909\n",
      "Epoch 135/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 907.2729 - val_loss: 562.7481\n",
      "Epoch 136/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 891.5115 - val_loss: 550.2933\n",
      "Epoch 137/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 875.9492 - val_loss: 538.0250\n",
      "Epoch 138/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 860.5850 - val_loss: 525.9411\n",
      "Epoch 139/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 845.4174 - val_loss: 514.0410\n",
      "Epoch 140/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 830.4443 - val_loss: 502.3226\n",
      "Epoch 141/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 815.6650 - val_loss: 490.7846\n",
      "Epoch 142/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 801.0779 - val_loss: 479.4257\n",
      "Epoch 143/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 786.6815 - val_loss: 468.2441\n",
      "Epoch 144/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 772.4741 - val_loss: 457.2387\n",
      "Epoch 145/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 758.4547 - val_loss: 446.4077\n",
      "Epoch 146/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 744.6215 - val_loss: 435.7502\n",
      "Epoch 147/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 730.9738 - val_loss: 425.2644\n",
      "Epoch 148/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 717.5093 - val_loss: 414.9484\n",
      "Epoch 149/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 704.2272 - val_loss: 404.8015\n",
      "Epoch 150/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 691.1254 - val_loss: 394.8216\n",
      "Epoch 151/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 678.2031 - val_loss: 385.0080\n",
      "Epoch 152/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 665.4590 - val_loss: 375.3586\n",
      "Epoch 153/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 652.8915 - val_loss: 365.8725\n",
      "Epoch 154/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 640.4987 - val_loss: 356.5471\n",
      "Epoch 155/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 628.2793 - val_loss: 347.3825\n",
      "Epoch 156/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 616.2324 - val_loss: 338.3766\n",
      "Epoch 157/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 604.3569 - val_loss: 329.5280\n",
      "Epoch 158/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 592.6511 - val_loss: 320.8351\n",
      "Epoch 159/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 581.1132 - val_loss: 312.2965\n",
      "Epoch 160/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 569.7421 - val_loss: 303.9111\n",
      "Epoch 161/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 558.5364 - val_loss: 295.6772\n",
      "Epoch 162/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 547.4948 - val_loss: 287.5933\n",
      "Epoch 163/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 536.6159 - val_loss: 279.6585\n",
      "Epoch 164/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 525.8983 - val_loss: 271.8709\n",
      "Epoch 165/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 515.3406 - val_loss: 264.2293\n",
      "Epoch 166/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 504.9414 - val_loss: 256.7316\n",
      "Epoch 167/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 494.6989 - val_loss: 249.3774\n",
      "Epoch 168/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 484.6127 - val_loss: 242.1646\n",
      "Epoch 169/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 474.6810 - val_loss: 235.0922\n",
      "Epoch 170/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 464.9022 - val_loss: 228.1584\n",
      "Epoch 171/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 455.2749 - val_loss: 221.3622\n",
      "Epoch 172/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 445.7981 - val_loss: 214.7018\n",
      "Epoch 173/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 436.4702 - val_loss: 208.1759\n",
      "Epoch 174/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 427.2901 - val_loss: 201.7835\n",
      "Epoch 175/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 418.2562 - val_loss: 195.5227\n",
      "Epoch 176/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 409.3672 - val_loss: 189.3926\n",
      "Epoch 177/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 400.6217 - val_loss: 183.3907\n",
      "Epoch 178/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 392.0182 - val_loss: 177.5167\n",
      "Epoch 179/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 383.5556 - val_loss: 171.7691\n",
      "Epoch 180/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 375.2327 - val_loss: 166.1461\n",
      "Epoch 181/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 367.0479 - val_loss: 160.6465\n",
      "Epoch 182/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 358.9999 - val_loss: 155.2686\n",
      "Epoch 183/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 351.0873 - val_loss: 150.0116\n",
      "Epoch 184/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 343.3089 - val_loss: 144.8736\n",
      "Epoch 185/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 335.6633 - val_loss: 139.8537\n",
      "Epoch 186/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 328.1491 - val_loss: 134.9496\n",
      "Epoch 187/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 320.7648 - val_loss: 130.1609\n",
      "Epoch 188/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 313.5092 - val_loss: 125.4856\n",
      "Epoch 189/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 306.3814 - val_loss: 120.9228\n",
      "Epoch 190/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 299.3795 - val_loss: 116.4706\n",
      "Epoch 191/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 292.5022 - val_loss: 112.1278\n",
      "Epoch 192/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 285.7484 - val_loss: 107.8932\n",
      "Epoch 193/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 279.1169 - val_loss: 103.7654\n",
      "Epoch 194/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 272.6063 - val_loss: 99.7428\n",
      "Epoch 195/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 266.2148 - val_loss: 95.8240\n",
      "Epoch 196/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 259.9413 - val_loss: 92.0078\n",
      "Epoch 197/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 253.7847 - val_loss: 88.2927\n",
      "Epoch 198/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 247.7437 - val_loss: 84.6776\n",
      "Epoch 199/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 241.8169 - val_loss: 81.1612\n",
      "Epoch 200/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 236.0031 - val_loss: 77.7414\n",
      "Epoch 201/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 230.3006 - val_loss: 74.4176\n",
      "Epoch 202/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 224.7085 - val_loss: 71.1881\n",
      "Epoch 203/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 219.2251 - val_loss: 68.0516\n",
      "Epoch 204/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 213.8497 - val_loss: 65.0065\n",
      "Epoch 205/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 208.5802 - val_loss: 62.0519\n",
      "Epoch 206/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 203.4158 - val_loss: 59.1862\n",
      "Epoch 207/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 198.3552 - val_loss: 56.4081\n",
      "Epoch 208/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 193.3969 - val_loss: 53.7163\n",
      "Epoch 209/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 188.5399 - val_loss: 51.1092\n",
      "Epoch 210/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 183.7827 - val_loss: 48.5859\n",
      "Epoch 211/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 179.1241 - val_loss: 46.1446\n",
      "Epoch 212/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 174.5627 - val_loss: 43.7844\n",
      "Epoch 213/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 170.0974 - val_loss: 41.5038\n",
      "Epoch 214/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 165.7267 - val_loss: 39.3010\n",
      "Epoch 215/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 161.4493 - val_loss: 37.1755\n",
      "Epoch 216/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 157.2642 - val_loss: 35.1253\n",
      "Epoch 217/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 153.1700 - val_loss: 33.1497\n",
      "Epoch 218/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 149.1654 - val_loss: 31.2469\n",
      "Epoch 219/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 145.2492 - val_loss: 29.4158\n",
      "Epoch 220/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 141.4203 - val_loss: 27.6554\n",
      "Epoch 221/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 137.6774 - val_loss: 25.9638\n",
      "Epoch 222/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 134.0190 - val_loss: 24.3402\n",
      "Epoch 223/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 130.4439 - val_loss: 22.7829\n",
      "Epoch 224/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 126.9508 - val_loss: 21.2909\n",
      "Epoch 225/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 123.5388 - val_loss: 19.8630\n",
      "Epoch 226/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 120.2066 - val_loss: 18.4976\n",
      "Epoch 227/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 116.9527 - val_loss: 17.1938\n",
      "Epoch 228/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 113.7762 - val_loss: 15.9502\n",
      "Epoch 229/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 110.6758 - val_loss: 14.7655\n",
      "Epoch 230/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 107.6501 - val_loss: 13.6386\n",
      "Epoch 231/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 104.6980 - val_loss: 12.5681\n",
      "Epoch 232/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 101.8184 - val_loss: 11.5529\n",
      "Epoch 233/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 99.0102 - val_loss: 10.5918\n",
      "Epoch 234/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 96.2721 - val_loss: 9.6834\n",
      "Epoch 235/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 93.6029 - val_loss: 8.8266\n",
      "Epoch 236/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 91.0012 - val_loss: 8.0203\n",
      "Epoch 237/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 88.4663 - val_loss: 7.2632\n",
      "Epoch 238/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 85.9966 - val_loss: 6.5541\n",
      "Epoch 239/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 83.5912 - val_loss: 5.8920\n",
      "Epoch 240/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 81.2491 - val_loss: 5.2755\n",
      "Epoch 241/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 78.9688 - val_loss: 4.7036\n",
      "Epoch 242/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 76.7494 - val_loss: 4.1752\n",
      "Epoch 243/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 74.5897 - val_loss: 3.6889\n",
      "Epoch 244/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 72.4887 - val_loss: 3.2439\n",
      "Epoch 245/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 70.4451 - val_loss: 2.8389\n",
      "Epoch 246/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 68.4581 - val_loss: 2.4728\n",
      "Epoch 247/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 66.5263 - val_loss: 2.1445\n",
      "Epoch 248/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 64.6489 - val_loss: 1.8530\n",
      "Epoch 249/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 62.8246 - val_loss: 1.5972\n",
      "Epoch 250/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 61.0525 - val_loss: 1.3759\n",
      "Epoch 251/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 59.3314 - val_loss: 1.1882\n",
      "Epoch 252/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 57.6602 - val_loss: 1.0330\n",
      "Epoch 253/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 56.0381 - val_loss: 0.9092\n",
      "Epoch 254/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 54.4639 - val_loss: 0.8159\n",
      "Epoch 255/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 52.9366 - val_loss: 0.7520\n",
      "Epoch 256/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 51.4553 - val_loss: 0.7165\n",
      "Epoch 257/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 50.0188 - val_loss: 0.7085\n",
      "Epoch 258/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 48.6262 - val_loss: 0.7270\n",
      "Epoch 259/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 47.2766 - val_loss: 0.7710\n",
      "Epoch 260/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 45.9690 - val_loss: 0.8396\n",
      "Epoch 261/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 44.7025 - val_loss: 0.9318\n",
      "Epoch 262/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 43.4761 - val_loss: 1.0467\n",
      "Epoch 263/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 42.2889 - val_loss: 1.1834\n",
      "Epoch 264/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 41.1398 - val_loss: 1.3411\n",
      "Epoch 265/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 40.0281 - val_loss: 1.5187\n",
      "Epoch 266/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 38.9528 - val_loss: 1.7156\n",
      "Epoch 267/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 37.9130 - val_loss: 1.9307\n",
      "Epoch 268/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 36.9079 - val_loss: 2.1633\n",
      "Epoch 269/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 35.9365 - val_loss: 2.4126\n",
      "Epoch 270/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 34.9981 - val_loss: 2.6776\n",
      "Epoch 271/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 34.0917 - val_loss: 2.9578\n",
      "Epoch 272/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 33.2166 - val_loss: 3.2521\n",
      "Epoch 273/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 32.3719 - val_loss: 3.5599\n",
      "Epoch 274/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 31.5569 - val_loss: 3.8806\n",
      "Epoch 275/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 30.7707 - val_loss: 4.2131\n",
      "Epoch 276/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 30.0125 - val_loss: 4.5570\n",
      "Epoch 277/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 29.2816 - val_loss: 4.9114\n",
      "Epoch 278/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 28.5772 - val_loss: 5.2757\n",
      "Epoch 279/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.8986 - val_loss: 5.6492\n",
      "Epoch 280/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.2451 - val_loss: 6.0313\n",
      "Epoch 281/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 26.6157 - val_loss: 6.4211\n",
      "Epoch 282/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 26.0101 - val_loss: 6.8183\n",
      "Epoch 283/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 25.4274 - val_loss: 7.2221\n",
      "Epoch 284/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 24.8668 - val_loss: 7.6321\n",
      "Epoch 285/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 24.3278 - val_loss: 8.0474\n",
      "Epoch 286/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 23.8097 - val_loss: 8.4676\n",
      "Epoch 287/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 23.3120 - val_loss: 8.8922\n",
      "Epoch 288/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 22.8338 - val_loss: 9.3204\n",
      "Epoch 289/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 22.3746 - val_loss: 9.7522\n",
      "Epoch 290/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 21.9338 - val_loss: 10.1865\n",
      "Epoch 291/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 21.5108 - val_loss: 10.6232\n",
      "Epoch 292/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 21.1051 - val_loss: 11.0617\n",
      "Epoch 293/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 20.7160 - val_loss: 11.5014\n",
      "Epoch 294/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 20.3431 - val_loss: 11.9421\n",
      "Epoch 295/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 19.9856 - val_loss: 12.3832\n",
      "Epoch 296/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 19.6433 - val_loss: 12.8244\n",
      "Epoch 297/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 19.3154 - val_loss: 13.2654\n",
      "Epoch 298/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 19.0014 - val_loss: 13.7054\n",
      "Epoch 299/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.7011 - val_loss: 14.1445\n",
      "Epoch 300/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4137 - val_loss: 14.5820\n",
      "Epoch 301/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.1390 - val_loss: 15.0178\n",
      "Epoch 302/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 17.8762 - val_loss: 15.4514\n",
      "Epoch 303/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.6252 - val_loss: 15.8827\n",
      "Epoch 304/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.3854 - val_loss: 16.3110\n",
      "Epoch 305/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.1564 - val_loss: 16.7364\n",
      "Epoch 306/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 16.9378 - val_loss: 17.1585\n",
      "Epoch 307/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.7291 - val_loss: 17.5770\n",
      "Epoch 308/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.5302 - val_loss: 17.9917\n",
      "Epoch 309/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.3404 - val_loss: 18.4023\n",
      "Epoch 310/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.1594 - val_loss: 18.8088\n",
      "Epoch 311/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 15.9870 - val_loss: 19.2109\n",
      "Epoch 312/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 15.8227 - val_loss: 19.6081\n",
      "Epoch 313/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 15.6664 - val_loss: 20.0006\n",
      "Epoch 314/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 15.5174 - val_loss: 20.3881\n",
      "Epoch 315/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 15.3757 - val_loss: 20.7705\n",
      "Epoch 316/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 15.2409 - val_loss: 21.1476\n",
      "Epoch 317/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 15.1126 - val_loss: 21.5191\n",
      "Epoch 318/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 14.9908 - val_loss: 21.8851\n",
      "Epoch 319/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 14.8749 - val_loss: 22.2456\n",
      "Epoch 320/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 14.7648 - val_loss: 22.6000\n",
      "Epoch 321/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 14.6603 - val_loss: 22.9486\n",
      "Epoch 322/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 14.5611 - val_loss: 23.2915\n",
      "Epoch 323/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 14.4669 - val_loss: 23.6280\n",
      "Epoch 324/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 14.3775 - val_loss: 23.9585\n",
      "Epoch 325/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 14.2927 - val_loss: 24.2830\n",
      "Epoch 326/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 14.2123 - val_loss: 24.6013\n",
      "Epoch 327/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 14.1361 - val_loss: 24.9134\n",
      "Epoch 328/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 14.0639 - val_loss: 25.2190\n",
      "Epoch 329/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.9955 - val_loss: 25.5184\n",
      "Epoch 330/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.9307 - val_loss: 25.8114\n",
      "Epoch 331/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.8694 - val_loss: 26.0984\n",
      "Epoch 332/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.8113 - val_loss: 26.3790\n",
      "Epoch 333/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.7563 - val_loss: 26.6533\n",
      "Epoch 334/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.7043 - val_loss: 26.9212\n",
      "Epoch 335/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 13.6551 - val_loss: 27.1831\n",
      "Epoch 336/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.6086 - val_loss: 27.4384\n",
      "Epoch 337/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 13.5646 - val_loss: 27.6879\n",
      "Epoch 338/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 13.5231 - val_loss: 27.9311\n",
      "Epoch 339/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.4838 - val_loss: 28.1682\n",
      "Epoch 340/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 13.4467 - val_loss: 28.3993\n",
      "Epoch 341/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.4117 - val_loss: 28.6242\n",
      "Epoch 342/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.3786 - val_loss: 28.8433\n",
      "Epoch 343/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.3474 - val_loss: 29.0566\n",
      "Epoch 344/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.3179 - val_loss: 29.2638\n",
      "Epoch 345/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.2901 - val_loss: 29.4656\n",
      "Epoch 346/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.2638 - val_loss: 29.6615\n",
      "Epoch 347/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.2390 - val_loss: 29.8518\n",
      "Epoch 348/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.2157 - val_loss: 30.0366\n",
      "Epoch 349/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.1936 - val_loss: 30.2162\n",
      "Epoch 350/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 13.1729 - val_loss: 30.3901\n",
      "Epoch 351/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.1533 - val_loss: 30.5587\n",
      "Epoch 352/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.1348 - val_loss: 30.7224\n",
      "Epoch 353/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.1174 - val_loss: 30.8806\n",
      "Epoch 354/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.1010 - val_loss: 31.0341\n",
      "Epoch 355/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.0856 - val_loss: 31.1828\n",
      "Epoch 356/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 13.0710 - val_loss: 31.3263\n",
      "Epoch 357/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 13.0573 - val_loss: 31.4656\n",
      "Epoch 358/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.0444 - val_loss: 31.5998\n",
      "Epoch 359/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 13.0322 - val_loss: 31.7299\n",
      "Epoch 360/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.0207 - val_loss: 31.8551\n",
      "Epoch 361/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.0100 - val_loss: 31.9762\n",
      "Epoch 362/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.9998 - val_loss: 32.0933\n",
      "Epoch 363/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.9903 - val_loss: 32.2060\n",
      "Epoch 364/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.9812 - val_loss: 32.3149\n",
      "Epoch 365/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.9728 - val_loss: 32.4194\n",
      "Epoch 366/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.9648 - val_loss: 32.5206\n",
      "Epoch 367/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.9573 - val_loss: 32.6178\n",
      "Epoch 368/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.9502 - val_loss: 32.7115\n",
      "Epoch 369/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.9435 - val_loss: 32.8016\n",
      "Epoch 370/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 12.9373 - val_loss: 32.8882\n",
      "Epoch 371/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 12.9313 - val_loss: 32.9715\n",
      "Epoch 372/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 12.9258 - val_loss: 33.0515\n",
      "Epoch 373/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.9206 - val_loss: 33.1286\n",
      "Epoch 374/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.9157 - val_loss: 33.2023\n",
      "Epoch 375/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.9111 - val_loss: 33.2731\n",
      "Epoch 376/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.9067 - val_loss: 33.3412\n",
      "Epoch 377/500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 12.9026 - val_loss: 33.4065\n",
      "Epoch 378/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 12.8988 - val_loss: 33.4689\n",
      "Epoch 379/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8952 - val_loss: 33.5289\n",
      "Epoch 380/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 12.8918 - val_loss: 33.5862\n",
      "Epoch 381/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8886 - val_loss: 33.6412\n",
      "Epoch 382/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8856 - val_loss: 33.6936\n",
      "Epoch 383/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8828 - val_loss: 33.7439\n",
      "Epoch 384/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8802 - val_loss: 33.7919\n",
      "Epoch 385/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8777 - val_loss: 33.8378\n",
      "Epoch 386/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8754 - val_loss: 33.8817\n",
      "Epoch 387/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8732 - val_loss: 33.9235\n",
      "Epoch 388/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8711 - val_loss: 33.9635\n",
      "Epoch 389/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8692 - val_loss: 34.0016\n",
      "Epoch 390/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 12.8674 - val_loss: 34.0381\n",
      "Epoch 391/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 12.8657 - val_loss: 34.0726\n",
      "Epoch 392/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8642 - val_loss: 34.1056\n",
      "Epoch 393/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8627 - val_loss: 34.1371\n",
      "Epoch 394/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8614 - val_loss: 34.1670\n",
      "Epoch 395/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8601 - val_loss: 34.1954\n",
      "Epoch 396/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8589 - val_loss: 34.2225\n",
      "Epoch 397/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 12.8578 - val_loss: 34.2481\n",
      "Epoch 398/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8568 - val_loss: 34.2726\n",
      "Epoch 399/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8558 - val_loss: 34.2955\n",
      "Epoch 400/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8549 - val_loss: 34.3174\n",
      "Epoch 401/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8541 - val_loss: 34.3384\n",
      "Epoch 402/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8533 - val_loss: 34.3579\n",
      "Epoch 403/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8527 - val_loss: 34.3769\n",
      "Epoch 404/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8520 - val_loss: 34.3946\n",
      "Epoch 405/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8514 - val_loss: 34.4113\n",
      "Epoch 406/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 12.8509 - val_loss: 34.4272\n",
      "Epoch 407/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8504 - val_loss: 34.4421\n",
      "Epoch 408/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8499 - val_loss: 34.4562\n",
      "Epoch 409/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8496 - val_loss: 34.4697\n",
      "Epoch 410/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8492 - val_loss: 34.4824\n",
      "Epoch 411/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8489 - val_loss: 34.4943\n",
      "Epoch 412/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8486 - val_loss: 34.5055\n",
      "Epoch 413/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8483 - val_loss: 34.5162\n",
      "Epoch 414/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8481 - val_loss: 34.5261\n",
      "Epoch 415/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 12.8479 - val_loss: 34.5354\n",
      "Epoch 416/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8477 - val_loss: 34.5443\n",
      "Epoch 417/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8476 - val_loss: 34.5525\n",
      "Epoch 418/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8475 - val_loss: 34.5604\n",
      "Epoch 419/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 12.8474 - val_loss: 34.5677\n",
      "Epoch 420/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8473 - val_loss: 34.5745\n",
      "Epoch 421/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8473 - val_loss: 34.5809\n",
      "Epoch 422/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8472 - val_loss: 34.5870\n",
      "Epoch 423/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8472 - val_loss: 34.5927\n",
      "Epoch 424/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8472 - val_loss: 34.5980\n",
      "Epoch 425/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8472 - val_loss: 34.6028\n",
      "Epoch 426/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 12.8473 - val_loss: 34.6075\n",
      "Epoch 427/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 12.8474 - val_loss: 34.6116\n",
      "Epoch 428/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 12.8474 - val_loss: 34.6155\n",
      "Epoch 429/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 12.8475 - val_loss: 34.6192\n",
      "Epoch 430/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8476 - val_loss: 34.6226\n",
      "Epoch 431/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8477 - val_loss: 34.6257\n",
      "Epoch 432/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 12.8479 - val_loss: 34.6285\n",
      "Epoch 433/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8480 - val_loss: 34.6314\n",
      "Epoch 434/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8481 - val_loss: 34.6339\n",
      "Epoch 435/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8483 - val_loss: 34.6361\n",
      "Epoch 436/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8485 - val_loss: 34.6381\n",
      "Epoch 437/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8487 - val_loss: 34.6402\n",
      "Epoch 438/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8488 - val_loss: 34.6420\n",
      "Epoch 439/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 12.8490 - val_loss: 34.6435\n",
      "Epoch 440/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8492 - val_loss: 34.6451\n",
      "Epoch 441/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8494 - val_loss: 34.6466\n",
      "Epoch 442/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8496 - val_loss: 34.6479\n",
      "Epoch 443/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8498 - val_loss: 34.6490\n",
      "Epoch 444/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8500 - val_loss: 34.6500\n",
      "Epoch 445/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 12.8502 - val_loss: 34.6509\n",
      "Epoch 446/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8504 - val_loss: 34.6518\n",
      "Epoch 447/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8506 - val_loss: 34.6524\n",
      "Epoch 448/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8509 - val_loss: 34.6531\n",
      "Epoch 449/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8511 - val_loss: 34.6538\n",
      "Epoch 450/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8513 - val_loss: 34.6541\n",
      "Epoch 451/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8516 - val_loss: 34.6547\n",
      "Epoch 452/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8518 - val_loss: 34.6549\n",
      "Epoch 453/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 12.8521 - val_loss: 34.6552\n",
      "Epoch 454/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 12.8523 - val_loss: 34.6554\n",
      "Epoch 455/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8525 - val_loss: 34.6556\n",
      "Epoch 456/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8528 - val_loss: 34.6558\n",
      "Epoch 457/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8530 - val_loss: 34.6560\n",
      "Epoch 458/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 12.8533 - val_loss: 34.6560\n",
      "Epoch 459/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8535 - val_loss: 34.6560\n",
      "Epoch 460/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8538 - val_loss: 34.6561\n",
      "Epoch 461/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 12.8541 - val_loss: 34.6561\n",
      "Epoch 462/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8543 - val_loss: 34.6560\n",
      "Epoch 463/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8545 - val_loss: 34.6557\n",
      "Epoch 464/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8548 - val_loss: 34.6556\n",
      "Epoch 465/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8551 - val_loss: 34.6555\n",
      "Epoch 466/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8553 - val_loss: 34.6554\n",
      "Epoch 467/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8555 - val_loss: 34.6552\n",
      "Epoch 468/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8558 - val_loss: 34.6548\n",
      "Epoch 469/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8560 - val_loss: 34.6546\n",
      "Epoch 470/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8563 - val_loss: 34.6542\n",
      "Epoch 471/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8565 - val_loss: 34.6539\n",
      "Epoch 472/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8568 - val_loss: 34.6538\n",
      "Epoch 473/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8570 - val_loss: 34.6532\n",
      "Epoch 474/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8573 - val_loss: 34.6530\n",
      "Epoch 475/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 12.8575 - val_loss: 34.6527\n",
      "Epoch 476/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8578 - val_loss: 34.6523\n",
      "Epoch 477/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8580 - val_loss: 34.6519\n",
      "Epoch 478/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8583 - val_loss: 34.6514\n",
      "Epoch 479/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 12.8585 - val_loss: 34.6512\n",
      "Epoch 480/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 12.8587 - val_loss: 34.6507\n",
      "Epoch 481/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8590 - val_loss: 34.6505\n",
      "Epoch 482/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8592 - val_loss: 34.6501\n",
      "Epoch 483/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8594 - val_loss: 34.6498\n",
      "Epoch 484/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8597 - val_loss: 34.6492\n",
      "Epoch 485/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8599 - val_loss: 34.6488\n",
      "Epoch 486/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8602 - val_loss: 34.6485\n",
      "Epoch 487/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8604 - val_loss: 34.6482\n",
      "Epoch 488/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8607 - val_loss: 34.6479\n",
      "Epoch 489/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 12.8609 - val_loss: 34.6474\n",
      "Epoch 490/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8611 - val_loss: 34.6472\n",
      "Epoch 491/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 12.8613 - val_loss: 34.6466\n",
      "Epoch 492/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8615 - val_loss: 34.6462\n",
      "Epoch 493/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8618 - val_loss: 34.6460\n",
      "Epoch 494/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8620 - val_loss: 34.6458\n",
      "Epoch 495/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8622 - val_loss: 34.6453\n",
      "Epoch 496/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8624 - val_loss: 34.6449\n",
      "Epoch 497/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 12.8626 - val_loss: 34.6446\n",
      "Epoch 498/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8628 - val_loss: 34.6441\n",
      "Epoch 499/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8631 - val_loss: 34.6436\n",
      "Epoch 500/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 12.8633 - val_loss: 34.6434\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.1552, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0026, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0004, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0102, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.8734, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 325ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.03730392e+01, 7.03534314e+01, 7.03338235e+01, 7.03142157e+01,\n",
       "        7.02946078e+01, 7.02750000e+01, 7.02553922e+01, 7.02357843e+01,\n",
       "        7.02161765e+01, 7.01965686e+01, 7.01769608e+01, 7.01573529e+01,\n",
       "        7.01377451e+01, 7.01181373e+01, 7.00985294e+01, 7.00789216e+01,\n",
       "        7.00593137e+01, 7.00397059e+01, 7.00200980e+01, 7.00004902e+01,\n",
       "        6.99808824e+01, 6.99612745e+01, 6.99416667e+01, 6.99220588e+01,\n",
       "        6.99024510e+01, 6.98607843e+01, 6.98159664e+01, 6.97711485e+01,\n",
       "        6.97263305e+01, 6.96815126e+01, 6.96366947e+01, 6.95918767e+01,\n",
       "        6.95470588e+01, 6.95022409e+01, 6.94574230e+01, 6.94126050e+01,\n",
       "        6.93677871e+01, 6.93229692e+01, 6.92781513e+01, 6.92333333e+01,\n",
       "        6.91885154e+01, 6.91436975e+01, 6.90988796e+01, 6.90540616e+01,\n",
       "        6.90092437e+01, 6.89644258e+01, 6.89196078e+01, 6.88747899e+01,\n",
       "        6.88299720e+01, 6.87851541e+01, 6.87403361e+01, 6.86955182e+01,\n",
       "        6.86507003e+01, 6.86058824e+01, 6.85610644e+01, 6.85162465e+01,\n",
       "        6.84714286e+01, 6.84266106e+01, 6.83817927e+01, 6.83369748e+01,\n",
       "        6.82975490e+01, 6.82835434e+01, 6.82695378e+01, 6.82555322e+01,\n",
       "        6.82415266e+01, 6.82275210e+01, 6.82135154e+01, 6.81995098e+01,\n",
       "        6.81855042e+01, 6.81714986e+01, 6.81574930e+01, 6.81434874e+01,\n",
       "        6.81294818e+01, 6.81154762e+01, 6.81014706e+01, 6.80874650e+01,\n",
       "        6.80734594e+01, 6.80594538e+01, 6.80454482e+01, 6.80314426e+01,\n",
       "        7.51025543e+01, 4.03590024e-01, 0.00000000e+00, 1.93370014e-01,\n",
       "        0.00000000e+00, 3.25454324e-01, 5.84132373e-01, 0.00000000e+00,\n",
       "        7.85332084e-01, 4.75012898e-01, 3.69284540e-01, 2.35743612e-01,\n",
       "        3.61841954e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.68337446e-01, 0.00000000e+00, 3.34053993e-01, 1.54416636e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([66.46143791, 66.45396825, 66.4464986 , 66.43902894, 66.43155929,\n",
       "       66.42408964, 66.41661998, 66.40915033, 66.40168067, 66.39421102,\n",
       "       66.38674136, 66.37927171, 66.37180205, 66.3643324 , 66.35686275,\n",
       "       66.34939309, 66.34192344, 66.33445378, 66.32698413, 66.31951447,\n",
       "       66.31204482, 66.30457516, 66.29710551, 66.28963585, 66.2821662 ,\n",
       "       66.27469655, 66.26722689, 66.25975724, 66.25228758, 66.24481793,\n",
       "       66.23734827, 66.22987862, 66.22240896, 66.21493931, 66.20746965,\n",
       "       66.2       , 66.19253035, 66.18506069, 66.17759104, 66.17012138,\n",
       "       66.16265173, 66.15518207, 66.14771242, 66.14024276, 66.13277311,\n",
       "       66.12530345, 66.1178338 , 66.11036415, 66.10289449, 66.09542484,\n",
       "       66.08795518, 66.08048553, 66.07301587, 66.06554622, 66.05807656,\n",
       "       66.05060691, 66.04313725, 66.0356676 , 66.02819795, 66.02072829,\n",
       "       66.01325864, 66.00578898, 65.9947479 , 65.97140523, 65.94806256,\n",
       "       65.92471989, 65.90137722, 65.87803455, 65.85469188, 65.83134921,\n",
       "       65.80800654, 65.78466387, 65.7613212 , 65.73797852, 65.71463585,\n",
       "       65.69129318, 65.66795051, 65.64460784, 65.62126517, 65.5979225 ,\n",
       "       65.57457983, 65.55123716, 65.52789449, 65.50455182, 65.48120915,\n",
       "       65.45786648, 65.43452381, 65.41118114, 65.38783847, 65.3644958 ,\n",
       "       65.34115313, 65.31781046, 65.29446779, 65.27112512, 65.24778245,\n",
       "       65.22443978, 65.20109711, 65.17775444, 65.15441176, 65.13106909])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.533692122684368\n",
      "14.910212323001744\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
