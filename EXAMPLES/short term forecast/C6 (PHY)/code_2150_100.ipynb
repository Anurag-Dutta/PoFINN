{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       84.600000\n",
       "1       84.431933\n",
       "2       84.263866\n",
       "3       84.095798\n",
       "4       83.927731\n",
       "          ...    \n",
       "2245    47.698873\n",
       "2246    47.689035\n",
       "2247    47.679197\n",
       "2248    47.669358\n",
       "2249    47.659520\n",
       "Name: C6, Length: 2250, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c6_interpolated_2150_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       84.600000\n",
       "1       84.431933\n",
       "2       84.263866\n",
       "3       84.095798\n",
       "4       83.927731\n",
       "          ...    \n",
       "2145     0.000000\n",
       "2146     0.000000\n",
       "2147     0.187130\n",
       "2148     0.000000\n",
       "2149     0.492844\n",
       "Name: C6, Length: 2150, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2150)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.431933</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84.263866</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84.095798</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83.927731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     84.600000  0.000298\n",
       "1     84.431933  0.000298\n",
       "2     84.263866  0.000297\n",
       "3     84.095798  0.000297\n",
       "4     83.927731  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 82.7524  # Value for C0\n",
    "K0 = -0.0031  # Value for K0\n",
    "K1 = -0.0003  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0171    # Value for b\n",
    "c = 3.0230    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    84.600000    84.431933    84.263866    84.095798    83.927731   \n",
      "351    84.431933    84.263866    84.095798    83.927731    83.759664   \n",
      "352    84.263866    84.095798    83.927731    83.759664    83.591597   \n",
      "353    84.095798    83.927731    83.759664    83.591597    83.423529   \n",
      "354    83.927731    83.759664    83.591597    83.423529    83.255462   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    83.759664    83.591597    83.423529    83.255462    83.092437  ...   \n",
      "351    83.591597    83.423529    83.255462    83.092437    82.991597  ...   \n",
      "352    83.423529    83.255462    83.092437    82.991597    82.890756  ...   \n",
      "353    83.255462    83.092437    82.991597    82.890756    82.789916  ...   \n",
      "354    83.092437    82.991597    82.890756    82.789916    82.689076  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   70.003688    0.000263   69.960738    0.000263   69.917787    0.000263   \n",
      "351   69.960738    0.000263   69.917787    0.000263   69.874837    0.000262   \n",
      "352   69.917787    0.000263   69.874837    0.000262   69.831886    0.000262   \n",
      "353   69.874837    0.000262   69.831886    0.000262   69.788936    0.000262   \n",
      "354   69.831886    0.000262   69.788936    0.000262   69.745985    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   69.874837    0.000262   69.831886    0.000262  \n",
      "351   69.831886    0.000262   69.788936    0.000262  \n",
      "352   69.788936    0.000262   69.745985    0.000262  \n",
      "353   69.745985    0.000262   69.703035    0.000262  \n",
      "354   69.703035    0.000262   69.660084    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1700, 1, 251) (1700, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 2s 23ms/step - loss: 3681.2158 - val_loss: 2396.6455\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3595.2334 - val_loss: 2355.5984\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3532.0625 - val_loss: 2306.7468\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3466.6157 - val_loss: 2264.8828\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3405.9006 - val_loss: 2216.5461\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3336.9673 - val_loss: 2173.5923\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3275.9702 - val_loss: 2131.7988\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3216.4861 - val_loss: 2091.0852\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3158.2625 - val_loss: 2051.2690\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3101.1069 - val_loss: 2012.2500\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3044.9114 - val_loss: 1973.9695\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2989.6113 - val_loss: 1936.3896\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2935.1633 - val_loss: 1899.4838\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2881.5354 - val_loss: 1863.2311\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2828.7051 - val_loss: 1827.6145\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2776.6506 - val_loss: 1792.6206\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2725.3572 - val_loss: 1758.2373\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2674.8091 - val_loss: 1724.4539\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2624.9951 - val_loss: 1691.2601\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2575.9026 - val_loss: 1658.6478\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2527.5208 - val_loss: 1626.6077\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2479.8408 - val_loss: 1595.1326\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2432.8525 - val_loss: 1564.2145\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2386.5474 - val_loss: 1533.8462\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2340.9175 - val_loss: 1504.0210\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2295.9531 - val_loss: 1474.7319\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2251.6479 - val_loss: 1445.9725\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2207.9944 - val_loss: 1417.7361\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2164.9836 - val_loss: 1390.0166\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2122.6101 - val_loss: 1362.8081\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2080.8657 - val_loss: 1336.1041\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2039.7432 - val_loss: 1309.8990\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1999.2368 - val_loss: 1284.1873\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1959.3394 - val_loss: 1258.9626\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1920.0439 - val_loss: 1234.2194\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1881.3452 - val_loss: 1209.9525\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1843.2354 - val_loss: 1186.1565\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1805.7097 - val_loss: 1162.8257\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1768.7610 - val_loss: 1139.9545\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1732.3829 - val_loss: 1117.5374\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1696.5702 - val_loss: 1095.5701\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1661.3164 - val_loss: 1074.0466\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1626.6156 - val_loss: 1052.9620\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1592.4623 - val_loss: 1032.3110\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1558.8502 - val_loss: 1012.0889\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1525.7743 - val_loss: 992.2903\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1493.2283 - val_loss: 972.9104\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1461.2067 - val_loss: 953.9444\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1429.7041 - val_loss: 935.3870\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1398.7150 - val_loss: 917.2335\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1368.2333 - val_loss: 899.4793\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1338.2544 - val_loss: 882.1193\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1308.7729 - val_loss: 865.1493\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1279.7831 - val_loss: 848.5638\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1251.2792 - val_loss: 832.3583\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1223.2568 - val_loss: 816.5285\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1195.7103 - val_loss: 801.0693\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1168.6345 - val_loss: 785.9769\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1142.0248 - val_loss: 771.2454\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1115.8755 - val_loss: 756.8717\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1090.1821 - val_loss: 742.8505\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1064.9388 - val_loss: 729.1772\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1040.1410 - val_loss: 715.8475\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1015.7834 - val_loss: 702.8569\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 991.8614 - val_loss: 690.2012\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 968.3703 - val_loss: 677.8760\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 945.3052 - val_loss: 665.8764\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 922.6608 - val_loss: 654.1985\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 900.4324 - val_loss: 642.8380\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 878.6152 - val_loss: 631.7903\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 857.2044 - val_loss: 621.0511\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 836.1957 - val_loss: 610.6167\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 815.5841 - val_loss: 600.4819\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 795.3647 - val_loss: 590.6433\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 775.5332 - val_loss: 581.0964\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 756.0848 - val_loss: 571.8369\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 737.0146 - val_loss: 562.8604\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 718.3185 - val_loss: 554.1634\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 699.9919 - val_loss: 545.7413\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 682.0302 - val_loss: 537.5899\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 664.4285 - val_loss: 529.7054\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 647.1829 - val_loss: 522.0835\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 630.2885 - val_loss: 514.7201\n",
      "Epoch 84/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 613.7412 - val_loss: 507.6113\n",
      "Epoch 85/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 597.5364 - val_loss: 500.7528\n",
      "Epoch 86/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 581.6694 - val_loss: 494.1408\n",
      "Epoch 87/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 566.1360 - val_loss: 487.7712\n",
      "Epoch 88/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 550.9321 - val_loss: 481.6401\n",
      "Epoch 89/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 536.0530 - val_loss: 475.7433\n",
      "Epoch 90/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 521.4946 - val_loss: 470.0770\n",
      "Epoch 91/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 507.2525 - val_loss: 464.6373\n",
      "Epoch 92/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 493.3227 - val_loss: 459.4200\n",
      "Epoch 93/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 479.7006 - val_loss: 454.4214\n",
      "Epoch 94/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 466.3820 - val_loss: 449.6377\n",
      "Epoch 95/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 453.3627 - val_loss: 445.0647\n",
      "Epoch 96/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 440.6383 - val_loss: 440.6986\n",
      "Epoch 97/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 428.2050 - val_loss: 436.5358\n",
      "Epoch 98/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 416.0584 - val_loss: 432.5720\n",
      "Epoch 99/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 404.1942 - val_loss: 428.8036\n",
      "Epoch 100/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 392.6087 - val_loss: 425.2270\n",
      "Epoch 101/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 381.2975 - val_loss: 421.8380\n",
      "Epoch 102/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 370.2564 - val_loss: 418.6330\n",
      "Epoch 103/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 359.4814 - val_loss: 415.6083\n",
      "Epoch 104/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 348.9684 - val_loss: 412.7600\n",
      "Epoch 105/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 338.7133 - val_loss: 410.0843\n",
      "Epoch 106/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 328.7123 - val_loss: 407.5778\n",
      "Epoch 107/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 318.9613 - val_loss: 405.2363\n",
      "Epoch 108/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 309.4562 - val_loss: 403.0566\n",
      "Epoch 109/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 300.1931 - val_loss: 401.0347\n",
      "Epoch 110/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 291.1682 - val_loss: 399.1671\n",
      "Epoch 111/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 282.3773 - val_loss: 397.4502\n",
      "Epoch 112/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 273.8167 - val_loss: 395.8801\n",
      "Epoch 113/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 265.4822 - val_loss: 394.4534\n",
      "Epoch 114/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 257.3703 - val_loss: 393.1665\n",
      "Epoch 115/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 249.4770 - val_loss: 392.0160\n",
      "Epoch 116/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 241.7985 - val_loss: 390.9980\n",
      "Epoch 117/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 234.3308 - val_loss: 390.1092\n",
      "Epoch 118/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 227.0702 - val_loss: 389.3460\n",
      "Epoch 119/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 220.0130 - val_loss: 388.7050\n",
      "Epoch 120/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.1556 - val_loss: 388.1827\n",
      "Epoch 121/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 206.4939 - val_loss: 387.7757\n",
      "Epoch 122/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 200.0242 - val_loss: 387.4806\n",
      "Epoch 123/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 193.7432 - val_loss: 387.2939\n",
      "Epoch 124/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 187.6471 - val_loss: 387.2123\n",
      "Epoch 125/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 181.7321 - val_loss: 387.2324\n",
      "Epoch 126/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 175.9946 - val_loss: 387.3510\n",
      "Epoch 127/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 170.4312 - val_loss: 387.5647\n",
      "Epoch 128/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 165.0383 - val_loss: 387.8703\n",
      "Epoch 129/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 159.8121 - val_loss: 388.2645\n",
      "Epoch 130/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 154.7493 - val_loss: 388.7442\n",
      "Epoch 131/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 149.8466 - val_loss: 389.3062\n",
      "Epoch 132/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 145.1002 - val_loss: 389.9472\n",
      "Epoch 133/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 140.5069 - val_loss: 390.6643\n",
      "Epoch 134/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 136.0631 - val_loss: 391.4543\n",
      "Epoch 135/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 131.7658 - val_loss: 392.3142\n",
      "Epoch 136/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 127.6113 - val_loss: 393.2410\n",
      "Epoch 137/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 123.5964 - val_loss: 394.2316\n",
      "Epoch 138/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 119.7178 - val_loss: 395.2832\n",
      "Epoch 139/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 115.9722 - val_loss: 396.3929\n",
      "Epoch 140/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 112.3565 - val_loss: 397.5577\n",
      "Epoch 141/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 108.8674 - val_loss: 398.7748\n",
      "Epoch 142/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 105.5019 - val_loss: 400.0415\n",
      "Epoch 143/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 102.2568 - val_loss: 401.3548\n",
      "Epoch 144/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 99.1291 - val_loss: 402.7125\n",
      "Epoch 145/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 96.1156 - val_loss: 404.1114\n",
      "Epoch 146/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 93.2134 - val_loss: 405.5490\n",
      "Epoch 147/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 90.4195 - val_loss: 407.0228\n",
      "Epoch 148/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 87.7309 - val_loss: 408.5304\n",
      "Epoch 149/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 85.1447 - val_loss: 410.0691\n",
      "Epoch 150/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 82.6582 - val_loss: 411.6366\n",
      "Epoch 151/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 80.2684 - val_loss: 413.2302\n",
      "Epoch 152/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 77.9727 - val_loss: 414.8478\n",
      "Epoch 153/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 75.7682 - val_loss: 416.4871\n",
      "Epoch 154/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 73.6524 - val_loss: 418.1457\n",
      "Epoch 155/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 71.6224 - val_loss: 419.8215\n",
      "Epoch 156/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 69.6757 - val_loss: 421.5124\n",
      "Epoch 157/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 67.8097 - val_loss: 423.2160\n",
      "Epoch 158/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 66.0219 - val_loss: 424.9308\n",
      "Epoch 159/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 64.3097 - val_loss: 426.6542\n",
      "Epoch 160/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 62.6709 - val_loss: 428.3846\n",
      "Epoch 161/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 61.1029 - val_loss: 430.1199\n",
      "Epoch 162/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 59.6035 - val_loss: 431.8585\n",
      "Epoch 163/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 58.1703 - val_loss: 433.5984\n",
      "Epoch 164/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 56.8010 - val_loss: 435.3379\n",
      "Epoch 165/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 55.4935 - val_loss: 437.0754\n",
      "Epoch 166/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 54.2455 - val_loss: 438.8092\n",
      "Epoch 167/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 53.0549 - val_loss: 440.5379\n",
      "Epoch 168/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 51.9196 - val_loss: 442.2597\n",
      "Epoch 169/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 50.8377 - val_loss: 443.9735\n",
      "Epoch 170/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 49.8071 - val_loss: 445.6778\n",
      "Epoch 171/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 48.8259 - val_loss: 447.3711\n",
      "Epoch 172/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 47.8922 - val_loss: 449.0520\n",
      "Epoch 173/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 47.0042 - val_loss: 450.7197\n",
      "Epoch 174/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 46.1600 - val_loss: 452.3725\n",
      "Epoch 175/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 45.3580 - val_loss: 454.0098\n",
      "Epoch 176/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 44.5963 - val_loss: 455.6302\n",
      "Epoch 177/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 43.8734 - val_loss: 457.2327\n",
      "Epoch 178/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 43.1877 - val_loss: 458.8167\n",
      "Epoch 179/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 42.5375 - val_loss: 460.3810\n",
      "Epoch 180/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 41.9213 - val_loss: 461.9245\n",
      "Epoch 181/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 41.3378 - val_loss: 463.4470\n",
      "Epoch 182/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 40.7853 - val_loss: 464.9475\n",
      "Epoch 183/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 40.2626 - val_loss: 466.4253\n",
      "Epoch 184/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 39.7683 - val_loss: 467.8799\n",
      "Epoch 185/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 39.3012 - val_loss: 469.3106\n",
      "Epoch 186/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.8598 - val_loss: 470.7166\n",
      "Epoch 187/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 38.4431 - val_loss: 472.0979\n",
      "Epoch 188/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 38.0499 - val_loss: 473.4541\n",
      "Epoch 189/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 37.6790 - val_loss: 474.7842\n",
      "Epoch 190/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 37.3294 - val_loss: 476.0885\n",
      "Epoch 191/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 37.0000 - val_loss: 477.3665\n",
      "Epoch 192/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 36.6897 - val_loss: 478.6178\n",
      "Epoch 193/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 36.3977 - val_loss: 479.8422\n",
      "Epoch 194/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 36.1230 - val_loss: 481.0397\n",
      "Epoch 195/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 35.8648 - val_loss: 482.2101\n",
      "Epoch 196/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 35.6220 - val_loss: 483.3534\n",
      "Epoch 197/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 35.3940 - val_loss: 484.4693\n",
      "Epoch 198/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 35.1799 - val_loss: 485.5582\n",
      "Epoch 199/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 34.9790 - val_loss: 486.6198\n",
      "Epoch 200/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 34.7905 - val_loss: 487.6545\n",
      "Epoch 201/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 34.6138 - val_loss: 488.6619\n",
      "Epoch 202/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 34.4483 - val_loss: 489.6424\n",
      "Epoch 203/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 34.2932 - val_loss: 490.5966\n",
      "Epoch 204/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.1479 - val_loss: 491.5240\n",
      "Epoch 205/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 34.0120 - val_loss: 492.4252\n",
      "Epoch 206/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 33.8848 - val_loss: 493.3004\n",
      "Epoch 207/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 33.7658 - val_loss: 494.1495\n",
      "Epoch 208/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 33.6547 - val_loss: 494.9733\n",
      "Epoch 209/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 33.5508 - val_loss: 495.7713\n",
      "Epoch 210/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 33.4538 - val_loss: 496.5449\n",
      "Epoch 211/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.3633 - val_loss: 497.2936\n",
      "Epoch 212/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 33.2788 - val_loss: 498.0182\n",
      "Epoch 213/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 33.1999 - val_loss: 498.7192\n",
      "Epoch 214/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 33.1264 - val_loss: 499.3966\n",
      "Epoch 215/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 33.0578 - val_loss: 500.0510\n",
      "Epoch 216/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.9939 - val_loss: 500.6832\n",
      "Epoch 217/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.9343 - val_loss: 501.2931\n",
      "Epoch 218/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.8788 - val_loss: 501.8813\n",
      "Epoch 219/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.8271 - val_loss: 502.4480\n",
      "Epoch 220/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.7790 - val_loss: 502.9940\n",
      "Epoch 221/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.7341 - val_loss: 503.5196\n",
      "Epoch 222/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.6925 - val_loss: 504.0256\n",
      "Epoch 223/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.6537 - val_loss: 504.5124\n",
      "Epoch 224/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.6176 - val_loss: 504.9799\n",
      "Epoch 225/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.5841 - val_loss: 505.4291\n",
      "Epoch 226/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.5529 - val_loss: 505.8606\n",
      "Epoch 227/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.5239 - val_loss: 506.2750\n",
      "Epoch 228/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.4968 - val_loss: 506.6720\n",
      "Epoch 229/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.4718 - val_loss: 507.0522\n",
      "Epoch 230/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.4485 - val_loss: 507.4165\n",
      "Epoch 231/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.4269 - val_loss: 507.7662\n",
      "Epoch 232/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.4068 - val_loss: 508.1005\n",
      "Epoch 233/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.3881 - val_loss: 508.4203\n",
      "Epoch 234/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.3708 - val_loss: 508.7260\n",
      "Epoch 235/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.3546 - val_loss: 509.0181\n",
      "Epoch 236/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.3397 - val_loss: 509.2970\n",
      "Epoch 237/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.3258 - val_loss: 509.5634\n",
      "Epoch 238/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.3129 - val_loss: 509.8171\n",
      "Epoch 239/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.3009 - val_loss: 510.0591\n",
      "Epoch 240/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2899 - val_loss: 510.2899\n",
      "Epoch 241/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2795 - val_loss: 510.5093\n",
      "Epoch 242/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2700 - val_loss: 510.7182\n",
      "Epoch 243/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.2612 - val_loss: 510.9170\n",
      "Epoch 244/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.2530 - val_loss: 511.1061\n",
      "Epoch 245/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2454 - val_loss: 511.2856\n",
      "Epoch 246/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2384 - val_loss: 511.4564\n",
      "Epoch 247/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2319 - val_loss: 511.6184\n",
      "Epoch 248/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.2259 - val_loss: 511.7722\n",
      "Epoch 249/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2203 - val_loss: 511.9177\n",
      "Epoch 250/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2151 - val_loss: 512.0559\n",
      "Epoch 251/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2104 - val_loss: 512.1865\n",
      "Epoch 252/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2060 - val_loss: 512.3104\n",
      "Epoch 253/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2020 - val_loss: 512.4274\n",
      "Epoch 254/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1982 - val_loss: 512.5378\n",
      "Epoch 255/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1949 - val_loss: 512.6426\n",
      "Epoch 256/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1917 - val_loss: 512.7416\n",
      "Epoch 257/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1888 - val_loss: 512.8348\n",
      "Epoch 258/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1861 - val_loss: 512.9229\n",
      "Epoch 259/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1837 - val_loss: 513.0059\n",
      "Epoch 260/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1815 - val_loss: 513.0843\n",
      "Epoch 261/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1795 - val_loss: 513.1579\n",
      "Epoch 262/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1777 - val_loss: 513.2273\n",
      "Epoch 263/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1759 - val_loss: 513.2921\n",
      "Epoch 264/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1745 - val_loss: 513.3535\n",
      "Epoch 265/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1731 - val_loss: 513.4110\n",
      "Epoch 266/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1719 - val_loss: 513.4647\n",
      "Epoch 267/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1709 - val_loss: 513.5155\n",
      "Epoch 268/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1699 - val_loss: 513.5630\n",
      "Epoch 269/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.1691 - val_loss: 513.6076\n",
      "Epoch 270/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1683 - val_loss: 513.6495\n",
      "Epoch 271/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1677 - val_loss: 513.6885\n",
      "Epoch 272/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1671 - val_loss: 513.7252\n",
      "Epoch 273/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1667 - val_loss: 513.7595\n",
      "Epoch 274/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1663 - val_loss: 513.7913\n",
      "Epoch 275/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1659 - val_loss: 513.8210\n",
      "Epoch 276/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1657 - val_loss: 513.8488\n",
      "Epoch 277/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1656 - val_loss: 513.8747\n",
      "Epoch 278/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1655 - val_loss: 513.8991\n",
      "Epoch 279/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1653 - val_loss: 513.9217\n",
      "Epoch 280/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1653 - val_loss: 513.9426\n",
      "Epoch 281/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1654 - val_loss: 513.9620\n",
      "Epoch 282/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1654 - val_loss: 513.9799\n",
      "Epoch 283/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1656 - val_loss: 513.9965\n",
      "Epoch 284/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1657 - val_loss: 514.0119\n",
      "Epoch 285/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1659 - val_loss: 514.0258\n",
      "Epoch 286/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1661 - val_loss: 514.0389\n",
      "Epoch 287/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 32.1664 - val_loss: 514.0511\n",
      "Epoch 288/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1667 - val_loss: 514.0624\n",
      "Epoch 289/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1670 - val_loss: 514.0726\n",
      "Epoch 290/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1673 - val_loss: 514.0819\n",
      "Epoch 291/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1677 - val_loss: 514.0905\n",
      "Epoch 292/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1681 - val_loss: 514.0986\n",
      "Epoch 293/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1686 - val_loss: 514.1061\n",
      "Epoch 294/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1689 - val_loss: 514.1128\n",
      "Epoch 295/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1693 - val_loss: 514.1187\n",
      "Epoch 296/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1697 - val_loss: 514.1241\n",
      "Epoch 297/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1702 - val_loss: 514.1293\n",
      "Epoch 298/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1707 - val_loss: 514.1342\n",
      "Epoch 299/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1711 - val_loss: 514.1384\n",
      "Epoch 300/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1716 - val_loss: 514.1419\n",
      "Epoch 301/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1720 - val_loss: 514.1453\n",
      "Epoch 302/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1725 - val_loss: 514.1483\n",
      "Epoch 303/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1730 - val_loss: 514.1509\n",
      "Epoch 304/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.1735 - val_loss: 514.1534\n",
      "Epoch 305/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1740 - val_loss: 514.1552\n",
      "Epoch 306/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1745 - val_loss: 514.1569\n",
      "Epoch 307/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.1750 - val_loss: 514.1583\n",
      "Epoch 308/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1755 - val_loss: 514.1597\n",
      "Epoch 309/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1760 - val_loss: 514.1609\n",
      "Epoch 310/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1765 - val_loss: 514.1616\n",
      "Epoch 311/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1770 - val_loss: 514.1625\n",
      "Epoch 312/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1775 - val_loss: 514.1629\n",
      "Epoch 313/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1780 - val_loss: 514.1633\n",
      "Epoch 314/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1785 - val_loss: 514.1636\n",
      "Epoch 315/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1790 - val_loss: 514.1639\n",
      "Epoch 316/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1795 - val_loss: 514.1639\n",
      "Epoch 317/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1799 - val_loss: 514.1637\n",
      "Epoch 318/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.1805 - val_loss: 514.1635\n",
      "Epoch 319/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1809 - val_loss: 514.1630\n",
      "Epoch 320/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1814 - val_loss: 514.1627\n",
      "Epoch 321/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1819 - val_loss: 514.1621\n",
      "Epoch 322/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.1824 - val_loss: 514.1619\n",
      "Epoch 323/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1828 - val_loss: 514.1613\n",
      "Epoch 324/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1833 - val_loss: 514.1608\n",
      "Epoch 325/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1837 - val_loss: 514.1601\n",
      "Epoch 326/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1842 - val_loss: 514.1594\n",
      "Epoch 327/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1846 - val_loss: 514.1586\n",
      "Epoch 328/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1851 - val_loss: 514.1578\n",
      "Epoch 329/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1855 - val_loss: 514.1571\n",
      "Epoch 330/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1859 - val_loss: 514.1564\n",
      "Epoch 331/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1864 - val_loss: 514.1555\n",
      "Epoch 332/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1868 - val_loss: 514.1545\n",
      "Epoch 333/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1872 - val_loss: 514.1539\n",
      "Epoch 334/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1876 - val_loss: 514.1530\n",
      "Epoch 335/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1880 - val_loss: 514.1520\n",
      "Epoch 336/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.1884 - val_loss: 514.1512\n",
      "Epoch 337/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1888 - val_loss: 514.1504\n",
      "Epoch 338/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1892 - val_loss: 514.1495\n",
      "Epoch 339/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.1895 - val_loss: 514.1485\n",
      "Epoch 340/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1899 - val_loss: 514.1473\n",
      "Epoch 341/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1903 - val_loss: 514.1464\n",
      "Epoch 342/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1907 - val_loss: 514.1457\n",
      "Epoch 343/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1911 - val_loss: 514.1448\n",
      "Epoch 344/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1914 - val_loss: 514.1439\n",
      "Epoch 345/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1917 - val_loss: 514.1426\n",
      "Epoch 346/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1921 - val_loss: 514.1417\n",
      "Epoch 347/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.1924 - val_loss: 514.1407\n",
      "Epoch 348/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1928 - val_loss: 514.1398\n",
      "Epoch 349/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1931 - val_loss: 514.1387\n",
      "Epoch 350/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1935 - val_loss: 514.1380\n",
      "Epoch 351/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1937 - val_loss: 514.1370\n",
      "Epoch 352/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1940 - val_loss: 514.1362\n",
      "Epoch 353/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1943 - val_loss: 514.1351\n",
      "Epoch 354/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1947 - val_loss: 514.1344\n",
      "Epoch 355/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1949 - val_loss: 514.1337\n",
      "Epoch 356/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1952 - val_loss: 514.1331\n",
      "Epoch 357/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1955 - val_loss: 514.1324\n",
      "Epoch 358/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1958 - val_loss: 514.1315\n",
      "Epoch 359/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1961 - val_loss: 514.1305\n",
      "Epoch 360/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1963 - val_loss: 514.1299\n",
      "Epoch 361/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1966 - val_loss: 514.1292\n",
      "Epoch 362/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1968 - val_loss: 514.1281\n",
      "Epoch 363/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 32.1971 - val_loss: 514.1277\n",
      "Epoch 364/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1973 - val_loss: 514.1270\n",
      "Epoch 365/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.1976 - val_loss: 514.1257\n",
      "Epoch 366/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 32.1978 - val_loss: 514.1255\n",
      "Epoch 367/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 32.1981 - val_loss: 514.1247\n",
      "Epoch 368/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.1983 - val_loss: 514.1239\n",
      "Epoch 369/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1985 - val_loss: 514.1234\n",
      "Epoch 370/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1987 - val_loss: 514.1227\n",
      "Epoch 371/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1990 - val_loss: 514.1218\n",
      "Epoch 372/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1992 - val_loss: 514.1210\n",
      "Epoch 373/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1994 - val_loss: 514.1203\n",
      "Epoch 374/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.1996 - val_loss: 514.1199\n",
      "Epoch 375/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.1998 - val_loss: 514.1196\n",
      "Epoch 376/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 32.2000 - val_loss: 514.1190\n",
      "Epoch 377/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2002 - val_loss: 514.1181\n",
      "Epoch 378/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2004 - val_loss: 514.1175\n",
      "Epoch 379/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2006 - val_loss: 514.1167\n",
      "Epoch 380/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.2008 - val_loss: 514.1161\n",
      "Epoch 381/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.2010 - val_loss: 514.1154\n",
      "Epoch 382/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 32.2012 - val_loss: 514.1151\n",
      "Epoch 383/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2013 - val_loss: 514.1144\n",
      "Epoch 384/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2015 - val_loss: 514.1139\n",
      "Epoch 385/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2017 - val_loss: 514.1134\n",
      "Epoch 386/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2018 - val_loss: 514.1129\n",
      "Epoch 387/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2020 - val_loss: 514.1123\n",
      "Epoch 388/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2021 - val_loss: 514.1117\n",
      "Epoch 389/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2023 - val_loss: 514.1112\n",
      "Epoch 390/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2025 - val_loss: 514.1109\n",
      "Epoch 391/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.2026 - val_loss: 514.1104\n",
      "Epoch 392/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2028 - val_loss: 514.1100\n",
      "Epoch 393/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2029 - val_loss: 514.1097\n",
      "Epoch 394/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2030 - val_loss: 514.1092\n",
      "Epoch 395/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2032 - val_loss: 514.1089\n",
      "Epoch 396/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2033 - val_loss: 514.1089\n",
      "Epoch 397/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2034 - val_loss: 514.1083\n",
      "Epoch 398/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2035 - val_loss: 514.1076\n",
      "Epoch 399/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2037 - val_loss: 514.1071\n",
      "Epoch 400/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2038 - val_loss: 514.1064\n",
      "Epoch 401/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.2040 - val_loss: 514.1060\n",
      "Epoch 402/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2041 - val_loss: 514.1056\n",
      "Epoch 403/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2042 - val_loss: 514.1055\n",
      "Epoch 404/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2043 - val_loss: 514.1055\n",
      "Epoch 405/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.2044 - val_loss: 514.1051\n",
      "Epoch 406/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.2045 - val_loss: 514.1044\n",
      "Epoch 407/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.2046 - val_loss: 514.1039\n",
      "Epoch 408/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.2048 - val_loss: 514.1037\n",
      "Epoch 409/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.2048 - val_loss: 514.1033\n",
      "Epoch 410/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.2049 - val_loss: 514.1032\n",
      "Epoch 411/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.2049 - val_loss: 514.1026\n",
      "Epoch 412/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.2051 - val_loss: 514.1022\n",
      "Epoch 413/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.2052 - val_loss: 514.1017\n",
      "Epoch 414/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.2053 - val_loss: 514.1015\n",
      "Epoch 415/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.2054 - val_loss: 514.1010\n",
      "Epoch 416/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.2055 - val_loss: 514.1010\n",
      "Epoch 417/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2056 - val_loss: 514.1007\n",
      "Epoch 418/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2057 - val_loss: 514.1006\n",
      "Epoch 419/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 32.2057 - val_loss: 514.1005\n",
      "Epoch 420/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2058 - val_loss: 514.1002\n",
      "Epoch 421/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2059 - val_loss: 514.0998\n",
      "Epoch 422/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2060 - val_loss: 514.0996\n",
      "Epoch 423/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2060 - val_loss: 514.0992\n",
      "Epoch 424/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2062 - val_loss: 514.0992\n",
      "Epoch 425/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2062 - val_loss: 514.0989\n",
      "Epoch 426/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2063 - val_loss: 514.0984\n",
      "Epoch 427/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2063 - val_loss: 514.0981\n",
      "Epoch 428/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2064 - val_loss: 514.0977\n",
      "Epoch 429/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2064 - val_loss: 514.0974\n",
      "Epoch 430/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2066 - val_loss: 514.0975\n",
      "Epoch 431/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.2066 - val_loss: 514.0973\n",
      "Epoch 432/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2067 - val_loss: 514.0972\n",
      "Epoch 433/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 32.2067 - val_loss: 514.0969\n",
      "Epoch 434/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.2068 - val_loss: 514.0968\n",
      "Epoch 435/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.2068 - val_loss: 514.0961\n",
      "Epoch 436/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.2070 - val_loss: 514.0962\n",
      "Epoch 437/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.2070 - val_loss: 514.0955\n",
      "Epoch 438/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.2071 - val_loss: 514.0958\n",
      "Epoch 439/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.2071 - val_loss: 514.0958\n",
      "Epoch 440/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.2071 - val_loss: 514.0957\n",
      "Epoch 441/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.2072 - val_loss: 514.0953\n",
      "Epoch 442/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.2072 - val_loss: 514.0951\n",
      "Epoch 443/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.2072 - val_loss: 514.0948\n",
      "Epoch 444/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2073 - val_loss: 514.0946\n",
      "Epoch 445/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.2074 - val_loss: 514.0946\n",
      "Epoch 446/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 32.2074 - val_loss: 514.0944\n",
      "Epoch 447/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.2075 - val_loss: 514.0941\n",
      "Epoch 448/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.2076 - val_loss: 514.0939\n",
      "Epoch 449/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.2076 - val_loss: 514.0936\n",
      "Epoch 450/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.2076 - val_loss: 514.0936\n",
      "Epoch 451/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.2076 - val_loss: 514.0930\n",
      "Epoch 452/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.2077 - val_loss: 514.0929\n",
      "Epoch 453/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.2078 - val_loss: 514.0928\n",
      "Epoch 454/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.2078 - val_loss: 514.0927\n",
      "Epoch 455/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2078 - val_loss: 514.0926\n",
      "Epoch 456/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.2079 - val_loss: 514.0923\n",
      "Epoch 457/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.2079 - val_loss: 514.0919\n",
      "Epoch 458/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 32.2080 - val_loss: 514.0918\n",
      "Epoch 459/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2080 - val_loss: 514.0922\n",
      "Epoch 460/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.2080 - val_loss: 514.0920\n",
      "Epoch 461/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.2081 - val_loss: 514.0920\n",
      "Epoch 462/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2081 - val_loss: 514.0920\n",
      "Epoch 463/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2081 - val_loss: 514.0918\n",
      "Epoch 464/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2081 - val_loss: 514.0911\n",
      "Epoch 465/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2082 - val_loss: 514.0908\n",
      "Epoch 466/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2082 - val_loss: 514.0906\n",
      "Epoch 467/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2082 - val_loss: 514.0904\n",
      "Epoch 468/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2082 - val_loss: 514.0901\n",
      "Epoch 469/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2083 - val_loss: 514.0899\n",
      "Epoch 470/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2083 - val_loss: 514.0898\n",
      "Epoch 471/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 32.2084 - val_loss: 514.0895\n",
      "Epoch 472/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.2084 - val_loss: 514.0896\n",
      "Epoch 473/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.2084 - val_loss: 514.0898\n",
      "Epoch 474/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2084 - val_loss: 514.0899\n",
      "Epoch 475/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2085 - val_loss: 514.0898\n",
      "Epoch 476/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2085 - val_loss: 514.0897\n",
      "Epoch 477/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2085 - val_loss: 514.0897\n",
      "Epoch 478/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2085 - val_loss: 514.0897\n",
      "Epoch 479/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2085 - val_loss: 514.0895\n",
      "Epoch 480/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2086 - val_loss: 514.0895\n",
      "Epoch 481/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2086 - val_loss: 514.0892\n",
      "Epoch 482/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2086 - val_loss: 514.0894\n",
      "Epoch 483/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2086 - val_loss: 514.0892\n",
      "Epoch 484/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 32.2087 - val_loss: 514.0889\n",
      "Epoch 485/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2087 - val_loss: 514.0886\n",
      "Epoch 486/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2087 - val_loss: 514.0884\n",
      "Epoch 487/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2087 - val_loss: 514.0883\n",
      "Epoch 488/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2087 - val_loss: 514.0882\n",
      "Epoch 489/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2087 - val_loss: 514.0883\n",
      "Epoch 490/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2087 - val_loss: 514.0878\n",
      "Epoch 491/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2088 - val_loss: 514.0873\n",
      "Epoch 492/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2088 - val_loss: 514.0870\n",
      "Epoch 493/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2089 - val_loss: 514.0870\n",
      "Epoch 494/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2089 - val_loss: 514.0867\n",
      "Epoch 495/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2089 - val_loss: 514.0864\n",
      "Epoch 496/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.2089 - val_loss: 514.0864\n",
      "Epoch 497/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.2089 - val_loss: 514.0862\n",
      "Epoch 498/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2089 - val_loss: 514.0862\n",
      "Epoch 499/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.2090 - val_loss: 514.0863\n",
      "Epoch 500/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.2090 - val_loss: 514.0862\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(82.7524, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0031, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0003, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0171, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(3.0230, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 360ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.42468487e+01, 5.42216387e+01, 5.41964286e+01, 5.41712185e+01,\n",
       "        5.41460084e+01, 5.41207983e+01, 5.40955882e+01, 5.40703782e+01,\n",
       "        5.40451681e+01, 5.40199580e+01, 5.39947479e+01, 5.39695378e+01,\n",
       "        5.39443277e+01, 5.94712418e+01, 5.89670402e+01, 5.84628385e+01,\n",
       "        0.00000000e+00, 2.19980810e-01, 6.08063259e+01, 6.01808590e+01,\n",
       "        5.96766573e+01, 5.91724557e+01, 5.86682540e+01, 5.81910131e+01,\n",
       "        5.80649626e+01, 5.79399122e+01, 5.77812862e+01, 0.00000000e+00,\n",
       "        1.39091580e-01, 5.88736695e+01, 5.83694678e+01, 5.81163165e+01,\n",
       "        5.79902661e+01, 5.78642157e+01, 5.77381653e+01, 5.76121148e+01,\n",
       "        5.74860644e+01, 5.73600140e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00199604e+00, 0.00000000e+00, 2.29751810e-01,\n",
       "        1.30122000e-03, 0.00000000e+00, 0.00000000e+00, 5.63546452e+01,\n",
       "        5.62622082e+01, 5.61697712e+01, 5.60773343e+01, 5.59848973e+01,\n",
       "        5.58924603e+01, 5.58000233e+01, 5.57067586e+01, 5.56165149e+01,\n",
       "        5.55508170e+01, 5.54919935e+01, 5.54331699e+01, 5.53743464e+01,\n",
       "        5.53155229e+01, 5.52566993e+01, 5.51978758e+01, 5.51390523e+01,\n",
       "        5.50802288e+01, 5.50214052e+01, 5.49625817e+01, 5.49037582e+01,\n",
       "        5.48452801e+01, 5.47862381e+01, 5.47271961e+01, 5.46681539e+01,\n",
       "        5.46091121e+01, 6.32139549e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.46544823e-01, 1.96369037e-01,\n",
       "        4.03765259e+01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.62735981e-01, 0.00000000e+00, 2.74267465e-01, 0.00000000e+00,\n",
       "        7.67220497e-01, 0.00000000e+00, 9.04992759e-01, 3.46218407e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.05208224e-02, 2.34764963e-01,\n",
       "        1.13982892e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([48.63349047, 48.62365239, 48.61381431, 48.60397622, 48.59413814,\n",
       "       48.58430006, 48.57446198, 48.5646239 , 48.55478582, 48.54494774,\n",
       "       48.53510965, 48.52527157, 48.51543349, 48.50559541, 48.49575733,\n",
       "       48.48591925, 48.47608116, 48.46624308, 48.456405  , 48.44656692,\n",
       "       48.43672884, 48.42689076, 48.41705267, 48.40721459, 48.39737651,\n",
       "       48.38753843, 48.37770035, 48.36786227, 48.35802419, 48.3481861 ,\n",
       "       48.33834802, 48.32850994, 48.31867186, 48.30883378, 48.2989957 ,\n",
       "       48.28915761, 48.27931953, 48.26948145, 48.25964337, 48.24980529,\n",
       "       48.23996721, 48.23012912, 48.22029104, 48.21045296, 48.20061488,\n",
       "       48.1907768 , 48.18093872, 48.17110064, 48.16126255, 48.15142447,\n",
       "       48.14158639, 48.13174831, 48.12191023, 48.11207215, 48.10223406,\n",
       "       48.09239598, 48.0825579 , 48.07271982, 48.06288174, 48.05304366,\n",
       "       48.04320557, 48.03336749, 48.02352941, 48.01369133, 48.00385325,\n",
       "       47.99401517, 47.98417709, 47.974339  , 47.96450092, 47.95466284,\n",
       "       47.94482476, 47.93498668, 47.9251486 , 47.91531051, 47.90547243,\n",
       "       47.89563435, 47.88579627, 47.87595819, 47.86612011, 47.85628203,\n",
       "       47.84644394, 47.83660586, 47.82676778, 47.8169297 , 47.80709162,\n",
       "       47.79725354, 47.78741545, 47.77757737, 47.76773929, 47.75790121,\n",
       "       47.74806313, 47.73822505, 47.72838696, 47.71854888, 47.7087108 ,\n",
       "       47.69887272, 47.68903464, 47.67919656, 47.66935848, 47.65952039])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.17461042658146\n",
      "23.142266066857964\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
