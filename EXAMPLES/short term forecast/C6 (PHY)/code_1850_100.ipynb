{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       84.600000\n",
       "1       84.431933\n",
       "2       84.263866\n",
       "3       84.095798\n",
       "4       83.927731\n",
       "          ...    \n",
       "1945    51.346300\n",
       "1946    51.332761\n",
       "1947    51.319223\n",
       "1948    51.305684\n",
       "1949    51.292145\n",
       "Name: C6, Length: 1950, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c6_interpolated_1850_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       84.600000\n",
       "1       84.431933\n",
       "2       84.263866\n",
       "3       84.095798\n",
       "4       83.927731\n",
       "          ...    \n",
       "1845     0.000000\n",
       "1846     0.000000\n",
       "1847     0.231329\n",
       "1848     0.000000\n",
       "1849     0.000000\n",
       "Name: C6, Length: 1850, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1850)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.431933</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84.263866</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84.095798</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83.927731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     84.600000  0.000298\n",
       "1     84.431933  0.000298\n",
       "2     84.263866  0.000297\n",
       "3     84.095798  0.000297\n",
       "4     83.927731  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 82.7524  # Value for C0\n",
    "K0 = -0.0031  # Value for K0\n",
    "K1 = -0.0003  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0171    # Value for b\n",
    "c = 3.0230    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    84.600000    84.431933    84.263866    84.095798    83.927731   \n",
      "351    84.431933    84.263866    84.095798    83.927731    83.759664   \n",
      "352    84.263866    84.095798    83.927731    83.759664    83.591597   \n",
      "353    84.095798    83.927731    83.759664    83.591597    83.423529   \n",
      "354    83.927731    83.759664    83.591597    83.423529    83.255462   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    83.759664    83.591597    83.423529    83.255462    83.092437  ...   \n",
      "351    83.591597    83.423529    83.255462    83.092437    82.991597  ...   \n",
      "352    83.423529    83.255462    83.092437    82.991597    82.890756  ...   \n",
      "353    83.255462    83.092437    82.991597    82.890756    82.789916  ...   \n",
      "354    83.092437    82.991597    82.890756    82.789916    82.689076  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   70.003688    0.000263   69.960738    0.000263   69.917787    0.000263   \n",
      "351   69.960738    0.000263   69.917787    0.000263   69.874837    0.000262   \n",
      "352   69.917787    0.000263   69.874837    0.000262   69.831886    0.000262   \n",
      "353   69.874837    0.000262   69.831886    0.000262   69.788936    0.000262   \n",
      "354   69.831886    0.000262   69.788936    0.000262   69.745985    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   69.874837    0.000262   69.831886    0.000262  \n",
      "351   69.831886    0.000262   69.788936    0.000262  \n",
      "352   69.788936    0.000262   69.745985    0.000262  \n",
      "353   69.745985    0.000262   69.703035    0.000262  \n",
      "354   69.703035    0.000262   69.660084    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 1, 251) (1400, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "18/18 [==============================] - 2s 27ms/step - loss: 3830.2629 - val_loss: 2859.0535\n",
      "Epoch 2/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3758.0491 - val_loss: 2797.8845\n",
      "Epoch 3/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3681.6758 - val_loss: 2735.2168\n",
      "Epoch 4/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3619.4807 - val_loss: 2683.2966\n",
      "Epoch 5/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3560.1582 - val_loss: 2632.2808\n",
      "Epoch 6/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3501.7856 - val_loss: 2582.1160\n",
      "Epoch 7/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3444.2842 - val_loss: 2532.7351\n",
      "Epoch 8/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3387.5916 - val_loss: 2484.0952\n",
      "Epoch 9/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3331.6672 - val_loss: 2436.1694\n",
      "Epoch 10/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3276.4856 - val_loss: 2388.9377\n",
      "Epoch 11/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3222.0281 - val_loss: 2342.3867\n",
      "Epoch 12/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3168.2788 - val_loss: 2296.5027\n",
      "Epoch 13/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3115.2283 - val_loss: 2251.2773\n",
      "Epoch 14/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3062.8640 - val_loss: 2206.7000\n",
      "Epoch 15/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3011.1785 - val_loss: 2162.7632\n",
      "Epoch 16/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2960.1621 - val_loss: 2119.4592\n",
      "Epoch 17/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2909.8076 - val_loss: 2076.7803\n",
      "Epoch 18/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2860.1079 - val_loss: 2034.7206\n",
      "Epoch 19/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2811.0552 - val_loss: 1993.2723\n",
      "Epoch 20/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2762.6440 - val_loss: 1952.4302\n",
      "Epoch 21/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2714.8672 - val_loss: 1912.1870\n",
      "Epoch 22/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2667.7178 - val_loss: 1872.5364\n",
      "Epoch 23/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2621.1907 - val_loss: 1833.4733\n",
      "Epoch 24/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2575.2788 - val_loss: 1794.9913\n",
      "Epoch 25/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 2529.9775 - val_loss: 1757.0848\n",
      "Epoch 26/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2485.2798 - val_loss: 1719.7473\n",
      "Epoch 27/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2441.1807 - val_loss: 1682.9742\n",
      "Epoch 28/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2397.6733 - val_loss: 1646.7590\n",
      "Epoch 29/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2354.7532 - val_loss: 1611.0970\n",
      "Epoch 30/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2312.4148 - val_loss: 1575.9823\n",
      "Epoch 31/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2270.6521 - val_loss: 1541.4095\n",
      "Epoch 32/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2229.4600 - val_loss: 1507.3733\n",
      "Epoch 33/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2188.8323 - val_loss: 1473.8682\n",
      "Epoch 34/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2148.7644 - val_loss: 1440.8894\n",
      "Epoch 35/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2109.2520 - val_loss: 1408.4314\n",
      "Epoch 36/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2070.2883 - val_loss: 1376.4889\n",
      "Epoch 37/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2031.8691 - val_loss: 1345.0574\n",
      "Epoch 38/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1993.9889 - val_loss: 1314.1309\n",
      "Epoch 39/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1956.6425 - val_loss: 1283.7046\n",
      "Epoch 40/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1919.8246 - val_loss: 1253.7734\n",
      "Epoch 41/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1883.5304 - val_loss: 1224.3331\n",
      "Epoch 42/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1847.7554 - val_loss: 1195.3778\n",
      "Epoch 43/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1812.4945 - val_loss: 1166.9033\n",
      "Epoch 44/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1777.7419 - val_loss: 1138.9044\n",
      "Epoch 45/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1743.4945 - val_loss: 1111.3762\n",
      "Epoch 46/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1709.7461 - val_loss: 1084.3140\n",
      "Epoch 47/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1676.4918 - val_loss: 1057.7128\n",
      "Epoch 48/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1643.7279 - val_loss: 1031.5682\n",
      "Epoch 49/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1611.4485 - val_loss: 1005.8757\n",
      "Epoch 50/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1579.6499 - val_loss: 980.6298\n",
      "Epoch 51/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1548.3269 - val_loss: 955.8262\n",
      "Epoch 52/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1517.4746 - val_loss: 931.4604\n",
      "Epoch 53/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1487.0891 - val_loss: 907.5277\n",
      "Epoch 54/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1455.2408 - val_loss: 878.3923\n",
      "Epoch 55/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1418.4166 - val_loss: 851.4153\n",
      "Epoch 56/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1384.8507 - val_loss: 825.6793\n",
      "Epoch 57/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1352.7194 - val_loss: 801.0314\n",
      "Epoch 58/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1321.7385 - val_loss: 777.2454\n",
      "Epoch 59/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1291.6786 - val_loss: 754.1862\n",
      "Epoch 60/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1262.4037 - val_loss: 731.7714\n",
      "Epoch 61/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1233.8286 - val_loss: 709.9469\n",
      "Epoch 62/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1205.8964 - val_loss: 688.6735\n",
      "Epoch 63/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1178.5664 - val_loss: 667.9229\n",
      "Epoch 64/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1151.8068 - val_loss: 647.6715\n",
      "Epoch 65/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1125.5931 - val_loss: 627.9014\n",
      "Epoch 66/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1099.9052 - val_loss: 608.5958\n",
      "Epoch 67/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1074.7249 - val_loss: 589.7413\n",
      "Epoch 68/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1050.0378 - val_loss: 571.3257\n",
      "Epoch 69/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1025.8304 - val_loss: 553.3376\n",
      "Epoch 70/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1002.0913 - val_loss: 535.7677\n",
      "Epoch 71/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 978.8098 - val_loss: 518.6066\n",
      "Epoch 72/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 955.9763 - val_loss: 501.8460\n",
      "Epoch 73/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 933.5814 - val_loss: 485.4774\n",
      "Epoch 74/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 911.6167 - val_loss: 469.4941\n",
      "Epoch 75/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 890.0745 - val_loss: 453.8884\n",
      "Epoch 76/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 868.9470 - val_loss: 438.6530\n",
      "Epoch 77/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 848.2275 - val_loss: 423.7821\n",
      "Epoch 78/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 827.9084 - val_loss: 409.2693\n",
      "Epoch 79/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 807.9837 - val_loss: 395.1078\n",
      "Epoch 80/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 788.4468 - val_loss: 381.2922\n",
      "Epoch 81/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 769.2919 - val_loss: 367.8169\n",
      "Epoch 82/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 750.5131 - val_loss: 354.6760\n",
      "Epoch 83/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 732.1040 - val_loss: 341.8639\n",
      "Epoch 84/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 714.0594 - val_loss: 329.3753\n",
      "Epoch 85/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 696.3740 - val_loss: 317.2056\n",
      "Epoch 86/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 679.0422 - val_loss: 305.3481\n",
      "Epoch 87/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 662.0589 - val_loss: 293.7994\n",
      "Epoch 88/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 645.4191 - val_loss: 282.5535\n",
      "Epoch 89/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 629.1172 - val_loss: 271.6056\n",
      "Epoch 90/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 613.1486 - val_loss: 260.9511\n",
      "Epoch 91/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 597.5083 - val_loss: 250.5856\n",
      "Epoch 92/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 582.1919 - val_loss: 240.5034\n",
      "Epoch 93/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 567.1942 - val_loss: 230.7005\n",
      "Epoch 94/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 552.5107 - val_loss: 221.1717\n",
      "Epoch 95/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 538.1370 - val_loss: 211.9141\n",
      "Epoch 96/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 524.0684 - val_loss: 202.9215\n",
      "Epoch 97/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 510.3003 - val_loss: 194.1896\n",
      "Epoch 98/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 496.8281 - val_loss: 185.7145\n",
      "Epoch 99/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 483.6480 - val_loss: 177.4921\n",
      "Epoch 100/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 470.7550 - val_loss: 169.5171\n",
      "Epoch 101/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 458.1450 - val_loss: 161.7866\n",
      "Epoch 102/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 445.8140 - val_loss: 154.2947\n",
      "Epoch 103/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 433.7575 - val_loss: 147.0385\n",
      "Epoch 104/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 421.9716 - val_loss: 140.0132\n",
      "Epoch 105/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 410.4519 - val_loss: 133.2149\n",
      "Epoch 106/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 399.1943 - val_loss: 126.6395\n",
      "Epoch 107/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 388.1951 - val_loss: 120.2832\n",
      "Epoch 108/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 377.4498 - val_loss: 114.1413\n",
      "Epoch 109/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 366.9548 - val_loss: 108.2102\n",
      "Epoch 110/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 356.7060 - val_loss: 102.4861\n",
      "Epoch 111/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 346.6996 - val_loss: 96.9649\n",
      "Epoch 112/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 336.9314 - val_loss: 91.6424\n",
      "Epoch 113/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 327.3977 - val_loss: 86.5148\n",
      "Epoch 114/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 318.0947 - val_loss: 81.5789\n",
      "Epoch 115/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 309.0186 - val_loss: 76.8299\n",
      "Epoch 116/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 300.1656 - val_loss: 72.2647\n",
      "Epoch 117/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 291.5318 - val_loss: 67.8791\n",
      "Epoch 118/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 283.1136 - val_loss: 63.6700\n",
      "Epoch 119/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 274.9075 - val_loss: 59.6328\n",
      "Epoch 120/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 266.9096 - val_loss: 55.7649\n",
      "Epoch 121/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 259.1166 - val_loss: 52.0618\n",
      "Epoch 122/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 251.5246 - val_loss: 48.5204\n",
      "Epoch 123/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 244.1303 - val_loss: 45.1365\n",
      "Epoch 124/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 236.9296 - val_loss: 41.9073\n",
      "Epoch 125/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 229.9197 - val_loss: 38.8288\n",
      "Epoch 126/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 223.0968 - val_loss: 35.8977\n",
      "Epoch 127/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 216.4575 - val_loss: 33.1101\n",
      "Epoch 128/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 209.9981 - val_loss: 30.4632\n",
      "Epoch 129/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 203.7155 - val_loss: 27.9536\n",
      "Epoch 130/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 197.6065 - val_loss: 25.5772\n",
      "Epoch 131/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 191.6674 - val_loss: 23.3314\n",
      "Epoch 132/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 185.8951 - val_loss: 21.2126\n",
      "Epoch 133/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 180.2861 - val_loss: 19.2174\n",
      "Epoch 134/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 174.8376 - val_loss: 17.3428\n",
      "Epoch 135/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 169.5459 - val_loss: 15.5852\n",
      "Epoch 136/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 164.4079 - val_loss: 13.9420\n",
      "Epoch 137/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 159.4208 - val_loss: 12.4096\n",
      "Epoch 138/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 154.5812 - val_loss: 10.9850\n",
      "Epoch 139/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 149.8859 - val_loss: 9.6652\n",
      "Epoch 140/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 145.3322 - val_loss: 8.4469\n",
      "Epoch 141/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 140.9165 - val_loss: 7.3275\n",
      "Epoch 142/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 136.6365 - val_loss: 6.3037\n",
      "Epoch 143/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 132.4885 - val_loss: 5.3726\n",
      "Epoch 144/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 128.4700 - val_loss: 4.5314\n",
      "Epoch 145/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 124.5779 - val_loss: 3.7771\n",
      "Epoch 146/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 120.8095 - val_loss: 3.1070\n",
      "Epoch 147/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 117.1618 - val_loss: 2.5181\n",
      "Epoch 148/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 113.6321 - val_loss: 2.0080\n",
      "Epoch 149/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 110.2175 - val_loss: 1.5735\n",
      "Epoch 150/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 106.9154 - val_loss: 1.2123\n",
      "Epoch 151/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 103.7228 - val_loss: 0.9215\n",
      "Epoch 152/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.6373 - val_loss: 0.6987\n",
      "Epoch 153/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 97.6563 - val_loss: 0.5412\n",
      "Epoch 154/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 94.7768 - val_loss: 0.4465\n",
      "Epoch 155/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 91.9966 - val_loss: 0.4121\n",
      "Epoch 156/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 89.3131 - val_loss: 0.4355\n",
      "Epoch 157/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 86.7238 - val_loss: 0.5145\n",
      "Epoch 158/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 84.2260 - val_loss: 0.6464\n",
      "Epoch 159/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 81.8175 - val_loss: 0.8292\n",
      "Epoch 160/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 79.4958 - val_loss: 1.0604\n",
      "Epoch 161/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 77.2586 - val_loss: 1.3378\n",
      "Epoch 162/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 75.1034 - val_loss: 1.6594\n",
      "Epoch 163/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 73.0281 - val_loss: 2.0227\n",
      "Epoch 164/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 71.0304 - val_loss: 2.4258\n",
      "Epoch 165/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 69.1081 - val_loss: 2.8666\n",
      "Epoch 166/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 67.2589 - val_loss: 3.3430\n",
      "Epoch 167/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 65.4808 - val_loss: 3.8531\n",
      "Epoch 168/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 63.7715 - val_loss: 4.3950\n",
      "Epoch 169/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 62.1292 - val_loss: 4.9667\n",
      "Epoch 170/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 60.5516 - val_loss: 5.5664\n",
      "Epoch 171/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 59.0370 - val_loss: 6.1922\n",
      "Epoch 172/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 57.5831 - val_loss: 6.8425\n",
      "Epoch 173/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 56.1884 - val_loss: 7.5154\n",
      "Epoch 174/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 54.8507 - val_loss: 8.2092\n",
      "Epoch 175/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 53.5683 - val_loss: 8.9226\n",
      "Epoch 176/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 52.3392 - val_loss: 9.6537\n",
      "Epoch 177/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 51.1619 - val_loss: 10.4009\n",
      "Epoch 178/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 50.0345 - val_loss: 11.1630\n",
      "Epoch 179/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 48.9553 - val_loss: 11.9384\n",
      "Epoch 180/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 47.9228 - val_loss: 12.7257\n",
      "Epoch 181/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 46.9352 - val_loss: 13.5236\n",
      "Epoch 182/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 45.9909 - val_loss: 14.3305\n",
      "Epoch 183/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 45.0886 - val_loss: 15.1454\n",
      "Epoch 184/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 44.2267 - val_loss: 15.9670\n",
      "Epoch 185/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 43.4036 - val_loss: 16.7940\n",
      "Epoch 186/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 42.6180 - val_loss: 17.6254\n",
      "Epoch 187/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 41.8685 - val_loss: 18.4600\n",
      "Epoch 188/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 41.1537 - val_loss: 19.2968\n",
      "Epoch 189/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 40.4723 - val_loss: 20.1347\n",
      "Epoch 190/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 39.8229 - val_loss: 20.9729\n",
      "Epoch 191/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 39.2044 - val_loss: 21.8103\n",
      "Epoch 192/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 38.6155 - val_loss: 22.6459\n",
      "Epoch 193/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 38.0550 - val_loss: 23.4790\n",
      "Epoch 194/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 37.5219 - val_loss: 24.3088\n",
      "Epoch 195/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 37.0149 - val_loss: 25.1344\n",
      "Epoch 196/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 36.5331 - val_loss: 25.9552\n",
      "Epoch 197/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 36.0753 - val_loss: 26.7701\n",
      "Epoch 198/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 35.6406 - val_loss: 27.5790\n",
      "Epoch 199/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 35.2279 - val_loss: 28.3811\n",
      "Epoch 200/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 34.8363 - val_loss: 29.1755\n",
      "Epoch 201/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 34.4649 - val_loss: 29.9620\n",
      "Epoch 202/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 34.1127 - val_loss: 30.7397\n",
      "Epoch 203/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 33.7791 - val_loss: 31.5083\n",
      "Epoch 204/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 33.4630 - val_loss: 32.2676\n",
      "Epoch 205/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 33.1638 - val_loss: 33.0169\n",
      "Epoch 206/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 32.8805 - val_loss: 33.7557\n",
      "Epoch 207/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 32.6125 - val_loss: 34.4839\n",
      "Epoch 208/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 32.3591 - val_loss: 35.2009\n",
      "Epoch 209/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 32.1196 - val_loss: 35.9066\n",
      "Epoch 210/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 31.8933 - val_loss: 36.6008\n",
      "Epoch 211/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 31.6796 - val_loss: 37.2830\n",
      "Epoch 212/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 31.4778 - val_loss: 37.9531\n",
      "Epoch 213/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 31.2873 - val_loss: 38.6109\n",
      "Epoch 214/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 31.1077 - val_loss: 39.2562\n",
      "Epoch 215/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 30.9383 - val_loss: 39.8890\n",
      "Epoch 216/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 30.7786 - val_loss: 40.5090\n",
      "Epoch 217/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 30.6282 - val_loss: 41.1161\n",
      "Epoch 218/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 30.4865 - val_loss: 41.7102\n",
      "Epoch 219/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 30.3531 - val_loss: 42.2914\n",
      "Epoch 220/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 30.2276 - val_loss: 42.8593\n",
      "Epoch 221/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 30.1094 - val_loss: 43.4142\n",
      "Epoch 222/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 29.9984 - val_loss: 43.9563\n",
      "Epoch 223/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 29.8940 - val_loss: 44.4851\n",
      "Epoch 224/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 29.7959 - val_loss: 45.0010\n",
      "Epoch 225/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 29.7037 - val_loss: 45.5039\n",
      "Epoch 226/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 29.6172 - val_loss: 45.9938\n",
      "Epoch 227/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 29.5360 - val_loss: 46.4709\n",
      "Epoch 228/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 29.4597 - val_loss: 46.9350\n",
      "Epoch 229/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 29.3882 - val_loss: 47.3869\n",
      "Epoch 230/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 29.3211 - val_loss: 47.8261\n",
      "Epoch 231/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 29.2583 - val_loss: 48.2529\n",
      "Epoch 232/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 29.1993 - val_loss: 48.6674\n",
      "Epoch 233/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 29.1441 - val_loss: 49.0700\n",
      "Epoch 234/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 29.0923 - val_loss: 49.4607\n",
      "Epoch 235/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 29.0438 - val_loss: 49.8395\n",
      "Epoch 236/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.9984 - val_loss: 50.2067\n",
      "Epoch 237/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 28.9559 - val_loss: 50.5622\n",
      "Epoch 238/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.9162 - val_loss: 50.9067\n",
      "Epoch 239/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.8790 - val_loss: 51.2403\n",
      "Epoch 240/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.8441 - val_loss: 51.5627\n",
      "Epoch 241/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.8116 - val_loss: 51.8745\n",
      "Epoch 242/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 28.7812 - val_loss: 52.1758\n",
      "Epoch 243/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.7528 - val_loss: 52.4671\n",
      "Epoch 244/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.7262 - val_loss: 52.7483\n",
      "Epoch 245/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.7014 - val_loss: 53.0199\n",
      "Epoch 246/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.6781 - val_loss: 53.2816\n",
      "Epoch 247/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.6564 - val_loss: 53.5340\n",
      "Epoch 248/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.6362 - val_loss: 53.7776\n",
      "Epoch 249/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.6172 - val_loss: 54.0120\n",
      "Epoch 250/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.5995 - val_loss: 54.2377\n",
      "Epoch 251/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.5831 - val_loss: 54.4548\n",
      "Epoch 252/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.5676 - val_loss: 54.6640\n",
      "Epoch 253/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.5533 - val_loss: 54.8648\n",
      "Epoch 254/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.5398 - val_loss: 55.0579\n",
      "Epoch 255/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.5273 - val_loss: 55.2433\n",
      "Epoch 256/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.5156 - val_loss: 55.4214\n",
      "Epoch 257/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 28.5047 - val_loss: 55.5923\n",
      "Epoch 258/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4945 - val_loss: 55.7562\n",
      "Epoch 259/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4851 - val_loss: 55.9133\n",
      "Epoch 260/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4762 - val_loss: 56.0641\n",
      "Epoch 261/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4679 - val_loss: 56.2083\n",
      "Epoch 262/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4603 - val_loss: 56.3465\n",
      "Epoch 263/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4531 - val_loss: 56.4788\n",
      "Epoch 264/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4465 - val_loss: 56.6054\n",
      "Epoch 265/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4402 - val_loss: 56.7267\n",
      "Epoch 266/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4344 - val_loss: 56.8422\n",
      "Epoch 267/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4290 - val_loss: 56.9526\n",
      "Epoch 268/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 28.4240 - val_loss: 57.0581\n",
      "Epoch 269/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4193 - val_loss: 57.1586\n",
      "Epoch 270/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4150 - val_loss: 57.2547\n",
      "Epoch 271/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4109 - val_loss: 57.3461\n",
      "Epoch 272/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4072 - val_loss: 57.4336\n",
      "Epoch 273/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4037 - val_loss: 57.5168\n",
      "Epoch 274/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4004 - val_loss: 57.5960\n",
      "Epoch 275/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3975 - val_loss: 57.6717\n",
      "Epoch 276/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 28.3947 - val_loss: 57.7435\n",
      "Epoch 277/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3920 - val_loss: 57.8117\n",
      "Epoch 278/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3897 - val_loss: 57.8765\n",
      "Epoch 279/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3875 - val_loss: 57.9383\n",
      "Epoch 280/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3854 - val_loss: 57.9965\n",
      "Epoch 281/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3836 - val_loss: 58.0524\n",
      "Epoch 282/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3818 - val_loss: 58.1048\n",
      "Epoch 283/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3802 - val_loss: 58.1547\n",
      "Epoch 284/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3788 - val_loss: 58.2022\n",
      "Epoch 285/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3774 - val_loss: 58.2470\n",
      "Epoch 286/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3762 - val_loss: 58.2895\n",
      "Epoch 287/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3751 - val_loss: 58.3296\n",
      "Epoch 288/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3741 - val_loss: 58.3677\n",
      "Epoch 289/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3732 - val_loss: 58.4036\n",
      "Epoch 290/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 28.3724 - val_loss: 58.4377\n",
      "Epoch 291/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3716 - val_loss: 58.4697\n",
      "Epoch 292/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3710 - val_loss: 58.4999\n",
      "Epoch 293/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3704 - val_loss: 58.5288\n",
      "Epoch 294/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3698 - val_loss: 58.5554\n",
      "Epoch 295/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3694 - val_loss: 58.5809\n",
      "Epoch 296/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3690 - val_loss: 58.6045\n",
      "Epoch 297/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3687 - val_loss: 58.6271\n",
      "Epoch 298/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3684 - val_loss: 58.6481\n",
      "Epoch 299/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3681 - val_loss: 58.6679\n",
      "Epoch 300/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3680 - val_loss: 58.6864\n",
      "Epoch 301/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3679 - val_loss: 58.7040\n",
      "Epoch 302/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3678 - val_loss: 58.7203\n",
      "Epoch 303/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3678 - val_loss: 58.7362\n",
      "Epoch 304/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3677 - val_loss: 58.7506\n",
      "Epoch 305/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3678 - val_loss: 58.7642\n",
      "Epoch 306/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3679 - val_loss: 58.7768\n",
      "Epoch 307/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 28.3679 - val_loss: 58.7886\n",
      "Epoch 308/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3680 - val_loss: 58.7996\n",
      "Epoch 309/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3682 - val_loss: 58.8098\n",
      "Epoch 310/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3684 - val_loss: 58.8195\n",
      "Epoch 311/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3685 - val_loss: 58.8284\n",
      "Epoch 312/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3687 - val_loss: 58.8368\n",
      "Epoch 313/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3690 - val_loss: 58.8446\n",
      "Epoch 314/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3692 - val_loss: 58.8517\n",
      "Epoch 315/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3695 - val_loss: 58.8584\n",
      "Epoch 316/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3698 - val_loss: 58.8649\n",
      "Epoch 317/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3701 - val_loss: 58.8707\n",
      "Epoch 318/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3704 - val_loss: 58.8761\n",
      "Epoch 319/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3707 - val_loss: 58.8808\n",
      "Epoch 320/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3711 - val_loss: 58.8853\n",
      "Epoch 321/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 28.3714 - val_loss: 58.8894\n",
      "Epoch 322/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3719 - val_loss: 58.8936\n",
      "Epoch 323/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3722 - val_loss: 58.8971\n",
      "Epoch 324/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3725 - val_loss: 58.9003\n",
      "Epoch 325/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3729 - val_loss: 58.9030\n",
      "Epoch 326/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3734 - val_loss: 58.9059\n",
      "Epoch 327/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3737 - val_loss: 58.9083\n",
      "Epoch 328/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3741 - val_loss: 58.9104\n",
      "Epoch 329/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3746 - val_loss: 58.9125\n",
      "Epoch 330/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3750 - val_loss: 58.9145\n",
      "Epoch 331/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3754 - val_loss: 58.9160\n",
      "Epoch 332/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3759 - val_loss: 58.9176\n",
      "Epoch 333/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3762 - val_loss: 58.9189\n",
      "Epoch 334/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3767 - val_loss: 58.9200\n",
      "Epoch 335/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 28.3771 - val_loss: 58.9210\n",
      "Epoch 336/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 28.3776 - val_loss: 58.9216\n",
      "Epoch 337/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3780 - val_loss: 58.9224\n",
      "Epoch 338/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3784 - val_loss: 58.9231\n",
      "Epoch 339/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3789 - val_loss: 58.9235\n",
      "Epoch 340/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3793 - val_loss: 58.9238\n",
      "Epoch 341/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3798 - val_loss: 58.9245\n",
      "Epoch 342/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3802 - val_loss: 58.9247\n",
      "Epoch 343/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3806 - val_loss: 58.9248\n",
      "Epoch 344/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3811 - val_loss: 58.9249\n",
      "Epoch 345/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3815 - val_loss: 58.9249\n",
      "Epoch 346/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3820 - val_loss: 58.9248\n",
      "Epoch 347/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3824 - val_loss: 58.9247\n",
      "Epoch 348/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3828 - val_loss: 58.9245\n",
      "Epoch 349/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3833 - val_loss: 58.9245\n",
      "Epoch 350/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3836 - val_loss: 58.9239\n",
      "Epoch 351/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3842 - val_loss: 58.9237\n",
      "Epoch 352/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3846 - val_loss: 58.9235\n",
      "Epoch 353/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 28.3850 - val_loss: 58.9231\n",
      "Epoch 354/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3854 - val_loss: 58.9228\n",
      "Epoch 355/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3859 - val_loss: 58.9225\n",
      "Epoch 356/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3863 - val_loss: 58.9220\n",
      "Epoch 357/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3867 - val_loss: 58.9215\n",
      "Epoch 358/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3871 - val_loss: 58.9211\n",
      "Epoch 359/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3875 - val_loss: 58.9206\n",
      "Epoch 360/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3879 - val_loss: 58.9202\n",
      "Epoch 361/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3883 - val_loss: 58.9195\n",
      "Epoch 362/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3887 - val_loss: 58.9189\n",
      "Epoch 363/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3891 - val_loss: 58.9185\n",
      "Epoch 364/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3895 - val_loss: 58.9181\n",
      "Epoch 365/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3899 - val_loss: 58.9176\n",
      "Epoch 366/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 28.3903 - val_loss: 58.9170\n",
      "Epoch 367/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3907 - val_loss: 58.9166\n",
      "Epoch 368/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3910 - val_loss: 58.9159\n",
      "Epoch 369/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3914 - val_loss: 58.9153\n",
      "Epoch 370/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3918 - val_loss: 58.9146\n",
      "Epoch 371/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3922 - val_loss: 58.9139\n",
      "Epoch 372/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3925 - val_loss: 58.9133\n",
      "Epoch 373/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3929 - val_loss: 58.9128\n",
      "Epoch 374/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3933 - val_loss: 58.9122\n",
      "Epoch 375/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3936 - val_loss: 58.9116\n",
      "Epoch 376/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3939 - val_loss: 58.9109\n",
      "Epoch 377/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3943 - val_loss: 58.9103\n",
      "Epoch 378/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 28.3947 - val_loss: 58.9098\n",
      "Epoch 379/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 28.3950 - val_loss: 58.9093\n",
      "Epoch 380/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3953 - val_loss: 58.9087\n",
      "Epoch 381/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3956 - val_loss: 58.9080\n",
      "Epoch 382/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3960 - val_loss: 58.9075\n",
      "Epoch 383/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3963 - val_loss: 58.9068\n",
      "Epoch 384/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3966 - val_loss: 58.9062\n",
      "Epoch 385/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3969 - val_loss: 58.9053\n",
      "Epoch 386/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3973 - val_loss: 58.9049\n",
      "Epoch 387/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3976 - val_loss: 58.9044\n",
      "Epoch 388/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3979 - val_loss: 58.9035\n",
      "Epoch 389/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3982 - val_loss: 58.9030\n",
      "Epoch 390/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.3985 - val_loss: 58.9027\n",
      "Epoch 391/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3988 - val_loss: 58.9019\n",
      "Epoch 392/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3991 - val_loss: 58.9014\n",
      "Epoch 393/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 28.3994 - val_loss: 58.9007\n",
      "Epoch 394/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3997 - val_loss: 58.9000\n",
      "Epoch 395/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4000 - val_loss: 58.8997\n",
      "Epoch 396/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4002 - val_loss: 58.8992\n",
      "Epoch 397/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4005 - val_loss: 58.8986\n",
      "Epoch 398/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4008 - val_loss: 58.8979\n",
      "Epoch 399/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4011 - val_loss: 58.8975\n",
      "Epoch 400/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4013 - val_loss: 58.8969\n",
      "Epoch 401/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4016 - val_loss: 58.8964\n",
      "Epoch 402/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4018 - val_loss: 58.8957\n",
      "Epoch 403/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4022 - val_loss: 58.8954\n",
      "Epoch 404/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4023 - val_loss: 58.8950\n",
      "Epoch 405/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4026 - val_loss: 58.8947\n",
      "Epoch 406/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4029 - val_loss: 58.8941\n",
      "Epoch 407/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4031 - val_loss: 58.8935\n",
      "Epoch 408/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4033 - val_loss: 58.8932\n",
      "Epoch 409/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 28.4036 - val_loss: 58.8929\n",
      "Epoch 410/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4038 - val_loss: 58.8923\n",
      "Epoch 411/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4040 - val_loss: 58.8920\n",
      "Epoch 412/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4042 - val_loss: 58.8914\n",
      "Epoch 413/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4045 - val_loss: 58.8911\n",
      "Epoch 414/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4047 - val_loss: 58.8909\n",
      "Epoch 415/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4048 - val_loss: 58.8902\n",
      "Epoch 416/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4051 - val_loss: 58.8900\n",
      "Epoch 417/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4054 - val_loss: 58.8898\n",
      "Epoch 418/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4055 - val_loss: 58.8891\n",
      "Epoch 419/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4057 - val_loss: 58.8888\n",
      "Epoch 420/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4060 - val_loss: 58.8884\n",
      "Epoch 421/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4061 - val_loss: 58.8880\n",
      "Epoch 422/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4063 - val_loss: 58.8874\n",
      "Epoch 423/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4065 - val_loss: 58.8870\n",
      "Epoch 424/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4067 - val_loss: 58.8866\n",
      "Epoch 425/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4069 - val_loss: 58.8863\n",
      "Epoch 426/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4071 - val_loss: 58.8861\n",
      "Epoch 427/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 28.4073 - val_loss: 58.8857\n",
      "Epoch 428/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4075 - val_loss: 58.8856\n",
      "Epoch 429/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4076 - val_loss: 58.8852\n",
      "Epoch 430/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4078 - val_loss: 58.8848\n",
      "Epoch 431/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4080 - val_loss: 58.8847\n",
      "Epoch 432/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4081 - val_loss: 58.8845\n",
      "Epoch 433/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4083 - val_loss: 58.8839\n",
      "Epoch 434/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4084 - val_loss: 58.8836\n",
      "Epoch 435/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4086 - val_loss: 58.8835\n",
      "Epoch 436/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4088 - val_loss: 58.8831\n",
      "Epoch 437/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4089 - val_loss: 58.8827\n",
      "Epoch 438/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4091 - val_loss: 58.8825\n",
      "Epoch 439/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4092 - val_loss: 58.8820\n",
      "Epoch 440/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4094 - val_loss: 58.8815\n",
      "Epoch 441/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 28.4095 - val_loss: 58.8814\n",
      "Epoch 442/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4097 - val_loss: 58.8813\n",
      "Epoch 443/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4098 - val_loss: 58.8812\n",
      "Epoch 444/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4099 - val_loss: 58.8806\n",
      "Epoch 445/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4101 - val_loss: 58.8800\n",
      "Epoch 446/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4102 - val_loss: 58.8796\n",
      "Epoch 447/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4104 - val_loss: 58.8796\n",
      "Epoch 448/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4105 - val_loss: 58.8793\n",
      "Epoch 449/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4107 - val_loss: 58.8792\n",
      "Epoch 450/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4108 - val_loss: 58.8791\n",
      "Epoch 451/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4109 - val_loss: 58.8788\n",
      "Epoch 452/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4110 - val_loss: 58.8784\n",
      "Epoch 453/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4111 - val_loss: 58.8782\n",
      "Epoch 454/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4113 - val_loss: 58.8780\n",
      "Epoch 455/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4113 - val_loss: 58.8775\n",
      "Epoch 456/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 28.4115 - val_loss: 58.8773\n",
      "Epoch 457/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4116 - val_loss: 58.8771\n",
      "Epoch 458/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4118 - val_loss: 58.8771\n",
      "Epoch 459/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4118 - val_loss: 58.8771\n",
      "Epoch 460/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4119 - val_loss: 58.8767\n",
      "Epoch 461/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4120 - val_loss: 58.8763\n",
      "Epoch 462/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4121 - val_loss: 58.8761\n",
      "Epoch 463/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4123 - val_loss: 58.8760\n",
      "Epoch 464/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4124 - val_loss: 58.8760\n",
      "Epoch 465/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4125 - val_loss: 58.8758\n",
      "Epoch 466/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4126 - val_loss: 58.8755\n",
      "Epoch 467/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4127 - val_loss: 58.8754\n",
      "Epoch 468/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4127 - val_loss: 58.8751\n",
      "Epoch 469/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4128 - val_loss: 58.8749\n",
      "Epoch 470/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4129 - val_loss: 58.8747\n",
      "Epoch 471/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4130 - val_loss: 58.8742\n",
      "Epoch 472/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4131 - val_loss: 58.8741\n",
      "Epoch 473/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4132 - val_loss: 58.8740\n",
      "Epoch 474/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 28.4134 - val_loss: 58.8740\n",
      "Epoch 475/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4134 - val_loss: 58.8739\n",
      "Epoch 476/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4135 - val_loss: 58.8737\n",
      "Epoch 477/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4136 - val_loss: 58.8736\n",
      "Epoch 478/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4136 - val_loss: 58.8733\n",
      "Epoch 479/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4137 - val_loss: 58.8731\n",
      "Epoch 480/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4138 - val_loss: 58.8728\n",
      "Epoch 481/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4139 - val_loss: 58.8727\n",
      "Epoch 482/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4140 - val_loss: 58.8727\n",
      "Epoch 483/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4141 - val_loss: 58.8727\n",
      "Epoch 484/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4141 - val_loss: 58.8727\n",
      "Epoch 485/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4142 - val_loss: 58.8727\n",
      "Epoch 486/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4142 - val_loss: 58.8726\n",
      "Epoch 487/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4143 - val_loss: 58.8719\n",
      "Epoch 488/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 28.4144 - val_loss: 58.8717\n",
      "Epoch 489/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4145 - val_loss: 58.8717\n",
      "Epoch 490/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4145 - val_loss: 58.8716\n",
      "Epoch 491/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 28.4146 - val_loss: 58.8716\n",
      "Epoch 492/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 28.4146 - val_loss: 58.8714\n",
      "Epoch 493/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 28.4146 - val_loss: 58.8709\n",
      "Epoch 494/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4148 - val_loss: 58.8707\n",
      "Epoch 495/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4149 - val_loss: 58.8707\n",
      "Epoch 496/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4149 - val_loss: 58.8707\n",
      "Epoch 497/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 28.4150 - val_loss: 58.8707\n",
      "Epoch 498/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4150 - val_loss: 58.8708\n",
      "Epoch 499/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4150 - val_loss: 58.8706\n",
      "Epoch 500/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4151 - val_loss: 58.8705\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(82.7524, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0031, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0003, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0171, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(3.0230, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 434ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.64162698e+01, 5.63854575e+01, 5.63546452e+01, 5.63238329e+01,\n",
       "        5.62930205e+01, 5.62622082e+01, 5.62313959e+01, 5.62005836e+01,\n",
       "        5.61697712e+01, 5.61389589e+01, 5.61081466e+01, 5.60773343e+01,\n",
       "        5.60465219e+01, 5.60157096e+01, 5.59848973e+01, 5.59540850e+01,\n",
       "        5.59232726e+01, 5.58924603e+01, 5.58616480e+01, 5.58308357e+01,\n",
       "        5.58000233e+01, 5.57692110e+01, 5.57383987e+01, 5.57075864e+01,\n",
       "        5.56767740e+01, 5.56459617e+01, 5.56151494e+01, 5.55900327e+01,\n",
       "        5.55704248e+01, 5.55508170e+01, 5.55312092e+01, 5.55116013e+01,\n",
       "        5.54919935e+01, 5.54723856e+01, 5.54527778e+01, 5.54331699e+01,\n",
       "        5.54135621e+01, 5.53939543e+01, 5.53743464e+01, 5.53547386e+01,\n",
       "        5.53351307e+01, 5.53155229e+01, 5.52959150e+01, 5.52763072e+01,\n",
       "        5.52566993e+01, 5.52370915e+01, 5.52174837e+01, 5.51978758e+01,\n",
       "        5.51782680e+01, 5.51586601e+01, 5.51390523e+01, 5.51194444e+01,\n",
       "        5.50998366e+01, 5.50802288e+01, 5.50606209e+01, 5.50410131e+01,\n",
       "        5.50214052e+01, 5.50017974e+01, 5.49821895e+01, 5.49625817e+01,\n",
       "        5.49429739e+01, 5.49233660e+01, 5.49037582e+01, 5.48864146e+01,\n",
       "        5.48696078e+01, 5.48528011e+01, 5.48359944e+01, 5.48191877e+01,\n",
       "        5.48023809e+01, 5.47855742e+01, 5.47687675e+01, 5.47519608e+01,\n",
       "        5.47351541e+01, 5.47183473e+01, 5.47015406e+01, 5.46847339e+01,\n",
       "        5.46679272e+01, 5.46511205e+01, 5.46343137e+01, 5.46175070e+01,\n",
       "        6.32139549e+01, 3.24703380e-02, 1.09459259e-01, 0.00000000e+00,\n",
       "        6.81859016e-01, 2.14646727e-01, 0.00000000e+00, 7.94346452e-01,\n",
       "        3.88949066e-02, 0.00000000e+00, 3.02115470e-01, 1.76237631e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.46544823e-01,\n",
       "        0.00000000e+00, 1.30328488e+00, 1.96369037e-01, 3.17359865e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([52.79325397, 52.77738095, 52.76150794, 52.74563492, 52.7297619 ,\n",
       "       52.71388889, 52.69801587, 52.68214286, 52.66626984, 52.65039683,\n",
       "       52.63452381, 52.61865079, 52.60277778, 52.58690476, 52.57103175,\n",
       "       52.55515873, 52.53928571, 52.5234127 , 52.50753968, 52.49166667,\n",
       "       52.47579365, 52.45992063, 52.44404762, 52.4281746 , 52.41230159,\n",
       "       52.39642857, 52.38055556, 52.36468254, 52.34880952, 52.33293651,\n",
       "       52.31706349, 52.30119048, 52.28531746, 52.26944444, 52.25357143,\n",
       "       52.23769841, 52.2218254 , 52.20595238, 52.19007937, 52.17420635,\n",
       "       52.15833333, 52.14246032, 52.1265873 , 52.11071429, 52.09484127,\n",
       "       52.07896825, 52.06309524, 52.04722222, 52.03134921, 52.01547619,\n",
       "       51.99960317, 51.98373016, 51.96785714, 51.95198413, 51.93611111,\n",
       "       51.9202381 , 51.90436508, 51.88849206, 51.87261905, 51.85674603,\n",
       "       51.84087302, 51.825     , 51.80912698, 51.79325397, 51.77738095,\n",
       "       51.76150794, 51.74563492, 51.7297619 , 51.71388889, 51.69830766,\n",
       "       51.68476891, 51.67123016, 51.65769141, 51.64415266, 51.63061391,\n",
       "       51.61707516, 51.60353641, 51.58999767, 51.57645892, 51.56292017,\n",
       "       51.54938142, 51.53584267, 51.52230392, 51.50876517, 51.49522642,\n",
       "       51.48168768, 51.46814893, 51.45461018, 51.44107143, 51.42753268,\n",
       "       51.41399393, 51.40045518, 51.38691643, 51.37337768, 51.35983894,\n",
       "       51.34630019, 51.33276144, 51.31922269, 51.30568394, 51.29214519])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.491921441466044\n",
      "12.411932778103045\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
