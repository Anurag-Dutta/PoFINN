{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       84.600000\n",
       "1       84.431933\n",
       "2       84.263866\n",
       "3       84.095798\n",
       "4       83.927731\n",
       "          ...    \n",
       "2345    46.715065\n",
       "2346    46.705226\n",
       "2347    46.695388\n",
       "2348    46.685550\n",
       "2349    46.675712\n",
       "Name: C6, Length: 2350, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c6_interpolated_2250_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       84.600000\n",
       "1       84.431933\n",
       "2       84.263866\n",
       "3       84.095798\n",
       "4       83.927731\n",
       "          ...    \n",
       "2245     0.234765\n",
       "2246     1.139829\n",
       "2247     0.000000\n",
       "2248     0.000000\n",
       "2249     0.000000\n",
       "Name: C6, Length: 2250, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2250)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.431933</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84.263866</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84.095798</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83.927731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     84.600000  0.000298\n",
       "1     84.431933  0.000298\n",
       "2     84.263866  0.000297\n",
       "3     84.095798  0.000297\n",
       "4     83.927731  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 82.7524  # Value for C0\n",
    "K0 = -0.0031  # Value for K0\n",
    "K1 = -0.0003  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0171    # Value for b\n",
    "c = 3.0230    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    84.600000    84.431933    84.263866    84.095798    83.927731   \n",
      "351    84.431933    84.263866    84.095798    83.927731    83.759664   \n",
      "352    84.263866    84.095798    83.927731    83.759664    83.591597   \n",
      "353    84.095798    83.927731    83.759664    83.591597    83.423529   \n",
      "354    83.927731    83.759664    83.591597    83.423529    83.255462   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    83.759664    83.591597    83.423529    83.255462    83.092437  ...   \n",
      "351    83.591597    83.423529    83.255462    83.092437    82.991597  ...   \n",
      "352    83.423529    83.255462    83.092437    82.991597    82.890756  ...   \n",
      "353    83.255462    83.092437    82.991597    82.890756    82.789916  ...   \n",
      "354    83.092437    82.991597    82.890756    82.789916    82.689076  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   70.003688    0.000263   69.960738    0.000263   69.917787    0.000263   \n",
      "351   69.960738    0.000263   69.917787    0.000263   69.874837    0.000262   \n",
      "352   69.917787    0.000263   69.874837    0.000262   69.831886    0.000262   \n",
      "353   69.874837    0.000262   69.831886    0.000262   69.788936    0.000262   \n",
      "354   69.831886    0.000262   69.788936    0.000262   69.745985    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   69.874837    0.000262   69.831886    0.000262  \n",
      "351   69.831886    0.000262   69.788936    0.000262  \n",
      "352   69.788936    0.000262   69.745985    0.000262  \n",
      "353   69.745985    0.000262   69.703035    0.000262  \n",
      "354   69.703035    0.000262   69.660084    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 1, 251) (1800, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "23/23 [==============================] - 2s 23ms/step - loss: 3567.4382 - val_loss: 2248.7131\n",
      "Epoch 2/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3480.2556 - val_loss: 2206.2390\n",
      "Epoch 3/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3414.1389 - val_loss: 2165.2090\n",
      "Epoch 4/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3349.0979 - val_loss: 2125.0488\n",
      "Epoch 5/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3285.1482 - val_loss: 2085.6416\n",
      "Epoch 6/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 3222.2146 - val_loss: 2046.9417\n",
      "Epoch 7/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3160.2522 - val_loss: 2008.9052\n",
      "Epoch 8/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3096.6851 - val_loss: 1966.7102\n",
      "Epoch 9/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3029.6519 - val_loss: 1926.7710\n",
      "Epoch 10/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2966.2053 - val_loss: 1888.2075\n",
      "Epoch 11/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2904.5210 - val_loss: 1850.7483\n",
      "Epoch 12/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2844.2476 - val_loss: 1814.2123\n",
      "Epoch 13/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2785.1890 - val_loss: 1778.5040\n",
      "Epoch 14/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2727.2380 - val_loss: 1743.5667\n",
      "Epoch 15/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2670.3267 - val_loss: 1709.3619\n",
      "Epoch 16/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2614.4102 - val_loss: 1675.8594\n",
      "Epoch 17/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2558.8086 - val_loss: 1641.9049\n",
      "Epoch 18/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2499.4888 - val_loss: 1607.6373\n",
      "Epoch 19/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2441.0964 - val_loss: 1574.5760\n",
      "Epoch 20/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2384.8037 - val_loss: 1542.5066\n",
      "Epoch 21/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2330.1255 - val_loss: 1511.2896\n",
      "Epoch 22/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2276.7954 - val_loss: 1480.8494\n",
      "Epoch 23/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2224.6633 - val_loss: 1451.1367\n",
      "Epoch 24/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2173.6367 - val_loss: 1422.1189\n",
      "Epoch 25/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2123.6497 - val_loss: 1393.7701\n",
      "Epoch 26/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2074.6562 - val_loss: 1366.0696\n",
      "Epoch 27/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2026.6188 - val_loss: 1339.0002\n",
      "Epoch 28/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1979.5062 - val_loss: 1312.5470\n",
      "Epoch 29/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1933.2915 - val_loss: 1286.6959\n",
      "Epoch 30/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1887.9536 - val_loss: 1261.4349\n",
      "Epoch 31/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1843.4720 - val_loss: 1236.7529\n",
      "Epoch 32/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1799.8285 - val_loss: 1212.6392\n",
      "Epoch 33/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1757.0061 - val_loss: 1189.0835\n",
      "Epoch 34/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1714.9902 - val_loss: 1166.0767\n",
      "Epoch 35/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1673.7665 - val_loss: 1143.6095\n",
      "Epoch 36/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1633.3208 - val_loss: 1121.6732\n",
      "Epoch 37/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1593.6411 - val_loss: 1100.2592\n",
      "Epoch 38/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1554.7144 - val_loss: 1079.3594\n",
      "Epoch 39/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1516.5295 - val_loss: 1058.9656\n",
      "Epoch 40/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1479.0747 - val_loss: 1039.0704\n",
      "Epoch 41/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1442.3397 - val_loss: 1019.6663\n",
      "Epoch 42/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1406.3132 - val_loss: 1000.7455\n",
      "Epoch 43/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1370.9857 - val_loss: 982.3005\n",
      "Epoch 44/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1336.3462 - val_loss: 964.3244\n",
      "Epoch 45/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1302.3854 - val_loss: 946.8099\n",
      "Epoch 46/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1269.0936 - val_loss: 929.7502\n",
      "Epoch 47/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1236.4618 - val_loss: 913.1387\n",
      "Epoch 48/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1204.4800 - val_loss: 896.9679\n",
      "Epoch 49/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1173.1393 - val_loss: 881.2313\n",
      "Epoch 50/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1142.4308 - val_loss: 865.9229\n",
      "Epoch 51/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1112.3461 - val_loss: 851.0354\n",
      "Epoch 52/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1082.8763 - val_loss: 836.5629\n",
      "Epoch 53/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1054.0134 - val_loss: 822.4988\n",
      "Epoch 54/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1025.7482 - val_loss: 808.8364\n",
      "Epoch 55/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 998.0722 - val_loss: 795.5698\n",
      "Epoch 56/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 970.9776 - val_loss: 782.6924\n",
      "Epoch 57/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 944.4564 - val_loss: 770.1982\n",
      "Epoch 58/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 918.5001 - val_loss: 758.0814\n",
      "Epoch 59/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 893.1011 - val_loss: 746.3353\n",
      "Epoch 60/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 868.2513 - val_loss: 734.9540\n",
      "Epoch 61/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 843.9428 - val_loss: 723.9320\n",
      "Epoch 62/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 820.1686 - val_loss: 713.2630\n",
      "Epoch 63/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 796.9202 - val_loss: 702.9410\n",
      "Epoch 64/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 774.1900 - val_loss: 692.9601\n",
      "Epoch 65/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 751.9710 - val_loss: 683.3146\n",
      "Epoch 66/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 730.2553 - val_loss: 673.9988\n",
      "Epoch 67/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 709.0358 - val_loss: 665.0070\n",
      "Epoch 68/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 688.3052 - val_loss: 656.3331\n",
      "Epoch 69/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 668.0558 - val_loss: 647.9718\n",
      "Epoch 70/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 648.2807 - val_loss: 639.9173\n",
      "Epoch 71/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 628.9725 - val_loss: 632.1640\n",
      "Epoch 72/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 610.1243 - val_loss: 624.7064\n",
      "Epoch 73/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 591.7287 - val_loss: 617.5388\n",
      "Epoch 74/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 573.7789 - val_loss: 610.6558\n",
      "Epoch 75/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 556.2679 - val_loss: 604.0519\n",
      "Epoch 76/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 539.1887 - val_loss: 597.7218\n",
      "Epoch 77/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 522.5348 - val_loss: 591.6599\n",
      "Epoch 78/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 506.2986 - val_loss: 585.8609\n",
      "Epoch 79/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 490.4739 - val_loss: 580.3193\n",
      "Epoch 80/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 475.0536 - val_loss: 575.0300\n",
      "Epoch 81/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 460.0314 - val_loss: 569.9876\n",
      "Epoch 82/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 445.4006 - val_loss: 565.1870\n",
      "Epoch 83/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 431.1544 - val_loss: 560.6228\n",
      "Epoch 84/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 417.2863 - val_loss: 556.2899\n",
      "Epoch 85/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 403.7897 - val_loss: 552.1832\n",
      "Epoch 86/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 390.6582 - val_loss: 548.2973\n",
      "Epoch 87/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 377.8853 - val_loss: 544.6273\n",
      "Epoch 88/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 365.4645 - val_loss: 541.1682\n",
      "Epoch 89/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 353.3898 - val_loss: 537.9148\n",
      "Epoch 90/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 341.6546 - val_loss: 534.8622\n",
      "Epoch 91/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 330.2527 - val_loss: 532.0053\n",
      "Epoch 92/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 319.1778 - val_loss: 529.3394\n",
      "Epoch 93/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 308.4237 - val_loss: 526.8593\n",
      "Epoch 94/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 297.9843 - val_loss: 524.5604\n",
      "Epoch 95/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 287.8534 - val_loss: 522.4377\n",
      "Epoch 96/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 278.0252 - val_loss: 520.4865\n",
      "Epoch 97/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 268.4935 - val_loss: 518.7019\n",
      "Epoch 98/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 259.2524 - val_loss: 517.0793\n",
      "Epoch 99/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 250.2959 - val_loss: 515.6140\n",
      "Epoch 100/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 241.6180 - val_loss: 514.3012\n",
      "Epoch 101/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 233.2133 - val_loss: 513.1365\n",
      "Epoch 102/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 225.0756 - val_loss: 512.1152\n",
      "Epoch 103/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 217.1994 - val_loss: 511.2328\n",
      "Epoch 104/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 209.5787 - val_loss: 510.4849\n",
      "Epoch 105/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 202.2083 - val_loss: 509.8668\n",
      "Epoch 106/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 195.0824 - val_loss: 509.3743\n",
      "Epoch 107/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 188.1953 - val_loss: 509.0028\n",
      "Epoch 108/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 181.5417 - val_loss: 508.7483\n",
      "Epoch 109/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 175.1159 - val_loss: 508.6063\n",
      "Epoch 110/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 168.9126 - val_loss: 508.5727\n",
      "Epoch 111/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 162.9265 - val_loss: 508.6432\n",
      "Epoch 112/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 157.1523 - val_loss: 508.8138\n",
      "Epoch 113/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 151.5848 - val_loss: 509.0803\n",
      "Epoch 114/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 146.2189 - val_loss: 509.4387\n",
      "Epoch 115/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 141.0492 - val_loss: 509.8851\n",
      "Epoch 116/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 136.0708 - val_loss: 510.4154\n",
      "Epoch 117/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 131.2788 - val_loss: 511.0259\n",
      "Epoch 118/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 126.6679 - val_loss: 511.7128\n",
      "Epoch 119/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 122.2334 - val_loss: 512.4723\n",
      "Epoch 120/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 117.9706 - val_loss: 513.3005\n",
      "Epoch 121/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 113.8747 - val_loss: 514.1940\n",
      "Epoch 122/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 109.9408 - val_loss: 515.1491\n",
      "Epoch 123/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 106.1644 - val_loss: 516.1624\n",
      "Epoch 124/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 102.5408 - val_loss: 517.2304\n",
      "Epoch 125/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 99.0655 - val_loss: 518.3495\n",
      "Epoch 126/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 95.7342 - val_loss: 519.5167\n",
      "Epoch 127/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 92.5424 - val_loss: 520.7286\n",
      "Epoch 128/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 89.4857 - val_loss: 521.9821\n",
      "Epoch 129/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 86.5601 - val_loss: 523.2737\n",
      "Epoch 130/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 83.7612 - val_loss: 524.6006\n",
      "Epoch 131/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 81.0850 - val_loss: 525.9600\n",
      "Epoch 132/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 78.5273 - val_loss: 527.3488\n",
      "Epoch 133/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 76.0844 - val_loss: 528.7642\n",
      "Epoch 134/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 73.7520 - val_loss: 530.2032\n",
      "Epoch 135/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 71.5268 - val_loss: 531.6634\n",
      "Epoch 136/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 69.4048 - val_loss: 533.1420\n",
      "Epoch 137/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 67.3824 - val_loss: 534.6365\n",
      "Epoch 138/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 65.4559 - val_loss: 536.1445\n",
      "Epoch 139/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 63.6219 - val_loss: 537.6636\n",
      "Epoch 140/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 61.8767 - val_loss: 539.1914\n",
      "Epoch 141/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 60.2173 - val_loss: 540.7256\n",
      "Epoch 142/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 58.6403 - val_loss: 542.2640\n",
      "Epoch 143/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 57.1424 - val_loss: 543.8046\n",
      "Epoch 144/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 55.7206 - val_loss: 545.3455\n",
      "Epoch 145/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 54.3717 - val_loss: 546.8843\n",
      "Epoch 146/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 53.0930 - val_loss: 548.4199\n",
      "Epoch 147/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 51.8812 - val_loss: 549.9498\n",
      "Epoch 148/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 50.7338 - val_loss: 551.4727\n",
      "Epoch 149/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 49.6479 - val_loss: 552.9869\n",
      "Epoch 150/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 48.6210 - val_loss: 554.4908\n",
      "Epoch 151/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 47.6503 - val_loss: 555.9827\n",
      "Epoch 152/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 46.7335 - val_loss: 557.4617\n",
      "Epoch 153/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 45.8680 - val_loss: 558.9261\n",
      "Epoch 154/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 45.0515 - val_loss: 560.3746\n",
      "Epoch 155/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 44.2817 - val_loss: 561.8066\n",
      "Epoch 156/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 43.5565 - val_loss: 563.2201\n",
      "Epoch 157/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 42.8737 - val_loss: 564.6146\n",
      "Epoch 158/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 42.2312 - val_loss: 565.9891\n",
      "Epoch 159/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 41.6271 - val_loss: 567.3428\n",
      "Epoch 160/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 41.0593 - val_loss: 568.6747\n",
      "Epoch 161/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 40.5262 - val_loss: 569.9842\n",
      "Epoch 162/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 40.0258 - val_loss: 571.2704\n",
      "Epoch 163/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 39.5565 - val_loss: 572.5328\n",
      "Epoch 164/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 39.1167 - val_loss: 573.7708\n",
      "Epoch 165/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 38.7048 - val_loss: 574.9841\n",
      "Epoch 166/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 38.3191 - val_loss: 576.1722\n",
      "Epoch 167/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 37.9584 - val_loss: 577.3345\n",
      "Epoch 168/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 37.6212 - val_loss: 578.4708\n",
      "Epoch 169/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 37.3062 - val_loss: 579.5808\n",
      "Epoch 170/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 37.0121 - val_loss: 580.6646\n",
      "Epoch 171/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 36.7376 - val_loss: 581.7214\n",
      "Epoch 172/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 36.4819 - val_loss: 582.7516\n",
      "Epoch 173/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 36.2435 - val_loss: 583.7551\n",
      "Epoch 174/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 36.0215 - val_loss: 584.7322\n",
      "Epoch 175/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 35.8148 - val_loss: 585.6820\n",
      "Epoch 176/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 35.6227 - val_loss: 586.6052\n",
      "Epoch 177/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 35.4440 - val_loss: 587.5019\n",
      "Epoch 178/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 35.2781 - val_loss: 588.3719\n",
      "Epoch 179/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 35.1241 - val_loss: 589.2159\n",
      "Epoch 180/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 34.9811 - val_loss: 590.0336\n",
      "Epoch 181/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 34.8486 - val_loss: 590.8258\n",
      "Epoch 182/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 34.7258 - val_loss: 591.5923\n",
      "Epoch 183/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 34.6120 - val_loss: 592.3337\n",
      "Epoch 184/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 34.5066 - val_loss: 593.0502\n",
      "Epoch 185/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 34.4092 - val_loss: 593.7422\n",
      "Epoch 186/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 34.3190 - val_loss: 594.4101\n",
      "Epoch 187/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 34.2357 - val_loss: 595.0544\n",
      "Epoch 188/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 34.1587 - val_loss: 595.6754\n",
      "Epoch 189/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 34.0876 - val_loss: 596.2734\n",
      "Epoch 190/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 34.0220 - val_loss: 596.8489\n",
      "Epoch 191/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.9615 - val_loss: 597.4029\n",
      "Epoch 192/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.9056 - val_loss: 597.9349\n",
      "Epoch 193/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.8541 - val_loss: 598.4464\n",
      "Epoch 194/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.8067 - val_loss: 598.9371\n",
      "Epoch 195/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.7631 - val_loss: 599.4078\n",
      "Epoch 196/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.7229 - val_loss: 599.8593\n",
      "Epoch 197/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.6859 - val_loss: 600.2919\n",
      "Epoch 198/500\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 33.6518 - val_loss: 600.7060\n",
      "Epoch 199/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 33.6205 - val_loss: 601.1023\n",
      "Epoch 200/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.5917 - val_loss: 601.4811\n",
      "Epoch 201/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 33.5652 - val_loss: 601.8433\n",
      "Epoch 202/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 33.5409 - val_loss: 602.1889\n",
      "Epoch 203/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 33.5186 - val_loss: 602.5189\n",
      "Epoch 204/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.4981 - val_loss: 602.8338\n",
      "Epoch 205/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.4792 - val_loss: 603.1335\n",
      "Epoch 206/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.4620 - val_loss: 603.4194\n",
      "Epoch 207/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 33.4461 - val_loss: 603.6914\n",
      "Epoch 208/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.4316 - val_loss: 603.9500\n",
      "Epoch 209/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.4183 - val_loss: 604.1959\n",
      "Epoch 210/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 33.4061 - val_loss: 604.4294\n",
      "Epoch 211/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3949 - val_loss: 604.6513\n",
      "Epoch 212/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3847 - val_loss: 604.8618\n",
      "Epoch 213/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3753 - val_loss: 605.0611\n",
      "Epoch 214/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3667 - val_loss: 605.2502\n",
      "Epoch 215/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3590 - val_loss: 605.4294\n",
      "Epoch 216/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3518 - val_loss: 605.5989\n",
      "Epoch 217/500\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 33.3453 - val_loss: 605.7594\n",
      "Epoch 218/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3393 - val_loss: 605.9107\n",
      "Epoch 219/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3339 - val_loss: 606.0538\n",
      "Epoch 220/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3290 - val_loss: 606.1889\n",
      "Epoch 221/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3245 - val_loss: 606.3162\n",
      "Epoch 222/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3204 - val_loss: 606.4362\n",
      "Epoch 223/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3167 - val_loss: 606.5491\n",
      "Epoch 224/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3134 - val_loss: 606.6555\n",
      "Epoch 225/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3104 - val_loss: 606.7554\n",
      "Epoch 226/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3077 - val_loss: 606.8496\n",
      "Epoch 227/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3052 - val_loss: 606.9379\n",
      "Epoch 228/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3030 - val_loss: 607.0208\n",
      "Epoch 229/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3011 - val_loss: 607.0985\n",
      "Epoch 230/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.2994 - val_loss: 607.1713\n",
      "Epoch 231/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 33.2979 - val_loss: 607.2399\n",
      "Epoch 232/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 33.2965 - val_loss: 607.3038\n",
      "Epoch 233/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.2954 - val_loss: 607.3636\n",
      "Epoch 234/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 33.2943 - val_loss: 607.4197\n",
      "Epoch 235/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.2935 - val_loss: 607.4716\n",
      "Epoch 236/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 33.2928 - val_loss: 607.5204\n",
      "Epoch 237/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.2922 - val_loss: 607.5657\n",
      "Epoch 238/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 33.2917 - val_loss: 607.6082\n",
      "Epoch 239/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.2913 - val_loss: 607.6476\n",
      "Epoch 240/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.2910 - val_loss: 607.6843\n",
      "Epoch 241/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.2908 - val_loss: 607.7185\n",
      "Epoch 242/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.2907 - val_loss: 607.7501\n",
      "Epoch 243/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.2907 - val_loss: 607.7795\n",
      "Epoch 244/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.2907 - val_loss: 607.8066\n",
      "Epoch 245/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.2908 - val_loss: 607.8317\n",
      "Epoch 246/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.2910 - val_loss: 607.8549\n",
      "Epoch 247/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.2912 - val_loss: 607.8763\n",
      "Epoch 248/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.2915 - val_loss: 607.8959\n",
      "Epoch 249/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.2919 - val_loss: 607.9142\n",
      "Epoch 250/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.2922 - val_loss: 607.9311\n",
      "Epoch 251/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.2927 - val_loss: 607.9465\n",
      "Epoch 252/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.2931 - val_loss: 607.9609\n",
      "Epoch 253/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 33.2936 - val_loss: 607.9738\n",
      "Epoch 254/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.2941 - val_loss: 607.9857\n",
      "Epoch 255/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.2946 - val_loss: 607.9966\n",
      "Epoch 256/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.2952 - val_loss: 608.0065\n",
      "Epoch 257/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.2957 - val_loss: 608.0156\n",
      "Epoch 258/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 33.2964 - val_loss: 608.0239\n",
      "Epoch 259/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.2970 - val_loss: 608.0314\n",
      "Epoch 260/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.2976 - val_loss: 608.0381\n",
      "Epoch 261/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.2983 - val_loss: 608.0443\n",
      "Epoch 262/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.2989 - val_loss: 608.0497\n",
      "Epoch 263/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.2996 - val_loss: 608.0546\n",
      "Epoch 264/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3003 - val_loss: 608.0589\n",
      "Epoch 265/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3010 - val_loss: 608.0626\n",
      "Epoch 266/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3017 - val_loss: 608.0660\n",
      "Epoch 267/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3024 - val_loss: 608.0690\n",
      "Epoch 268/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3032 - val_loss: 608.0717\n",
      "Epoch 269/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3039 - val_loss: 608.0741\n",
      "Epoch 270/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3045 - val_loss: 608.0756\n",
      "Epoch 271/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3053 - val_loss: 608.0775\n",
      "Epoch 272/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 33.3060 - val_loss: 608.0788\n",
      "Epoch 273/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3068 - val_loss: 608.0798\n",
      "Epoch 274/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3075 - val_loss: 608.0809\n",
      "Epoch 275/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3082 - val_loss: 608.0817\n",
      "Epoch 276/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3089 - val_loss: 608.0823\n",
      "Epoch 277/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3097 - val_loss: 608.0828\n",
      "Epoch 278/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3104 - val_loss: 608.0830\n",
      "Epoch 279/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3110 - val_loss: 608.0830\n",
      "Epoch 280/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3117 - val_loss: 608.0828\n",
      "Epoch 281/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3124 - val_loss: 608.0824\n",
      "Epoch 282/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3131 - val_loss: 608.0820\n",
      "Epoch 283/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3138 - val_loss: 608.0817\n",
      "Epoch 284/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3145 - val_loss: 608.0810\n",
      "Epoch 285/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3152 - val_loss: 608.0804\n",
      "Epoch 286/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3158 - val_loss: 608.0797\n",
      "Epoch 287/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3165 - val_loss: 608.0791\n",
      "Epoch 288/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3171 - val_loss: 608.0782\n",
      "Epoch 289/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3178 - val_loss: 608.0775\n",
      "Epoch 290/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 33.3185 - val_loss: 608.0767\n",
      "Epoch 291/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 33.3190 - val_loss: 608.0755\n",
      "Epoch 292/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3198 - val_loss: 608.0749\n",
      "Epoch 293/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3203 - val_loss: 608.0739\n",
      "Epoch 294/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3209 - val_loss: 608.0728\n",
      "Epoch 295/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3215 - val_loss: 608.0715\n",
      "Epoch 296/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3221 - val_loss: 608.0706\n",
      "Epoch 297/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3227 - val_loss: 608.0692\n",
      "Epoch 298/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3233 - val_loss: 608.0682\n",
      "Epoch 299/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3238 - val_loss: 608.0670\n",
      "Epoch 300/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3244 - val_loss: 608.0659\n",
      "Epoch 301/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3249 - val_loss: 608.0645\n",
      "Epoch 302/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3255 - val_loss: 608.0637\n",
      "Epoch 303/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3260 - val_loss: 608.0623\n",
      "Epoch 304/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3266 - val_loss: 608.0613\n",
      "Epoch 305/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3271 - val_loss: 608.0601\n",
      "Epoch 306/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3276 - val_loss: 608.0591\n",
      "Epoch 307/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3281 - val_loss: 608.0579\n",
      "Epoch 308/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 33.3286 - val_loss: 608.0568\n",
      "Epoch 309/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3291 - val_loss: 608.0554\n",
      "Epoch 310/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3297 - val_loss: 608.0545\n",
      "Epoch 311/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3301 - val_loss: 608.0535\n",
      "Epoch 312/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3305 - val_loss: 608.0522\n",
      "Epoch 313/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3310 - val_loss: 608.0513\n",
      "Epoch 314/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3314 - val_loss: 608.0500\n",
      "Epoch 315/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3319 - val_loss: 608.0490\n",
      "Epoch 316/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3323 - val_loss: 608.0479\n",
      "Epoch 317/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3327 - val_loss: 608.0469\n",
      "Epoch 318/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3332 - val_loss: 608.0458\n",
      "Epoch 319/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3336 - val_loss: 608.0450\n",
      "Epoch 320/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3340 - val_loss: 608.0438\n",
      "Epoch 321/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3344 - val_loss: 608.0430\n",
      "Epoch 322/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3348 - val_loss: 608.0424\n",
      "Epoch 323/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3352 - val_loss: 608.0413\n",
      "Epoch 324/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3355 - val_loss: 608.0405\n",
      "Epoch 325/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 33.3359 - val_loss: 608.0393\n",
      "Epoch 326/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3363 - val_loss: 608.0385\n",
      "Epoch 327/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3367 - val_loss: 608.0375\n",
      "Epoch 328/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 33.3370 - val_loss: 608.0366\n",
      "Epoch 329/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3374 - val_loss: 608.0359\n",
      "Epoch 330/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3377 - val_loss: 608.0350\n",
      "Epoch 331/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3380 - val_loss: 608.0342\n",
      "Epoch 332/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 33.3384 - val_loss: 608.0338\n",
      "Epoch 333/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 33.3386 - val_loss: 608.0329\n",
      "Epoch 334/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3390 - val_loss: 608.0322\n",
      "Epoch 335/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3393 - val_loss: 608.0314\n",
      "Epoch 336/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3396 - val_loss: 608.0308\n",
      "Epoch 337/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3399 - val_loss: 608.0300\n",
      "Epoch 338/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3402 - val_loss: 608.0294\n",
      "Epoch 339/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3405 - val_loss: 608.0287\n",
      "Epoch 340/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3407 - val_loss: 608.0278\n",
      "Epoch 341/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 33.3410 - val_loss: 608.0272\n",
      "Epoch 342/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3413 - val_loss: 608.0267\n",
      "Epoch 343/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3415 - val_loss: 608.0259\n",
      "Epoch 344/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3418 - val_loss: 608.0253\n",
      "Epoch 345/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3421 - val_loss: 608.0248\n",
      "Epoch 346/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3423 - val_loss: 608.0242\n",
      "Epoch 347/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3426 - val_loss: 608.0235\n",
      "Epoch 348/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3428 - val_loss: 608.0229\n",
      "Epoch 349/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3430 - val_loss: 608.0225\n",
      "Epoch 350/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3433 - val_loss: 608.0220\n",
      "Epoch 351/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3435 - val_loss: 608.0217\n",
      "Epoch 352/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3437 - val_loss: 608.0208\n",
      "Epoch 353/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3439 - val_loss: 608.0203\n",
      "Epoch 354/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3441 - val_loss: 608.0199\n",
      "Epoch 355/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3443 - val_loss: 608.0193\n",
      "Epoch 356/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 33.3445 - val_loss: 608.0186\n",
      "Epoch 357/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 33.3447 - val_loss: 608.0179\n",
      "Epoch 358/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3450 - val_loss: 608.0176\n",
      "Epoch 359/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3451 - val_loss: 608.0173\n",
      "Epoch 360/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3453 - val_loss: 608.0168\n",
      "Epoch 361/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3455 - val_loss: 608.0162\n",
      "Epoch 362/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3456 - val_loss: 608.0156\n",
      "Epoch 363/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3459 - val_loss: 608.0153\n",
      "Epoch 364/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3460 - val_loss: 608.0151\n",
      "Epoch 365/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3462 - val_loss: 608.0148\n",
      "Epoch 366/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3463 - val_loss: 608.0144\n",
      "Epoch 367/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3465 - val_loss: 608.0139\n",
      "Epoch 368/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3467 - val_loss: 608.0134\n",
      "Epoch 369/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3468 - val_loss: 608.0130\n",
      "Epoch 370/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 33.3470 - val_loss: 608.0126\n",
      "Epoch 371/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3471 - val_loss: 608.0123\n",
      "Epoch 372/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 33.3472 - val_loss: 608.0118\n",
      "Epoch 373/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3474 - val_loss: 608.0113\n",
      "Epoch 374/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3475 - val_loss: 608.0110\n",
      "Epoch 375/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3476 - val_loss: 608.0107\n",
      "Epoch 376/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3478 - val_loss: 608.0103\n",
      "Epoch 377/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3479 - val_loss: 608.0099\n",
      "Epoch 378/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3480 - val_loss: 608.0098\n",
      "Epoch 379/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 33.3482 - val_loss: 608.0094\n",
      "Epoch 380/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3483 - val_loss: 608.0091\n",
      "Epoch 381/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3484 - val_loss: 608.0086\n",
      "Epoch 382/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3486 - val_loss: 608.0085\n",
      "Epoch 383/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 33.3487 - val_loss: 608.0085\n",
      "Epoch 384/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3487 - val_loss: 608.0082\n",
      "Epoch 385/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3489 - val_loss: 608.0081\n",
      "Epoch 386/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3489 - val_loss: 608.0078\n",
      "Epoch 387/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3491 - val_loss: 608.0078\n",
      "Epoch 388/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3491 - val_loss: 608.0072\n",
      "Epoch 389/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 33.3493 - val_loss: 608.0068\n",
      "Epoch 390/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3494 - val_loss: 608.0064\n",
      "Epoch 391/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 33.3495 - val_loss: 608.0062\n",
      "Epoch 392/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 33.3496 - val_loss: 608.0061\n",
      "Epoch 393/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3496 - val_loss: 608.0059\n",
      "Epoch 394/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3498 - val_loss: 608.0059\n",
      "Epoch 395/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3498 - val_loss: 608.0057\n",
      "Epoch 396/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3499 - val_loss: 608.0056\n",
      "Epoch 397/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3500 - val_loss: 608.0054\n",
      "Epoch 398/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3500 - val_loss: 608.0049\n",
      "Epoch 399/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3502 - val_loss: 608.0047\n",
      "Epoch 400/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3502 - val_loss: 608.0045\n",
      "Epoch 401/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3503 - val_loss: 608.0043\n",
      "Epoch 402/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3504 - val_loss: 608.0041\n",
      "Epoch 403/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3504 - val_loss: 608.0038\n",
      "Epoch 404/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 33.3505 - val_loss: 608.0038\n",
      "Epoch 405/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 33.3506 - val_loss: 608.0034\n",
      "Epoch 406/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 33.3507 - val_loss: 608.0034\n",
      "Epoch 407/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3507 - val_loss: 608.0031\n",
      "Epoch 408/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3508 - val_loss: 608.0028\n",
      "Epoch 409/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3509 - val_loss: 608.0026\n",
      "Epoch 410/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3509 - val_loss: 608.0025\n",
      "Epoch 411/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 33.3510 - val_loss: 608.0025\n",
      "Epoch 412/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3511 - val_loss: 608.0023\n",
      "Epoch 413/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3511 - val_loss: 608.0023\n",
      "Epoch 414/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3512 - val_loss: 608.0023\n",
      "Epoch 415/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3512 - val_loss: 608.0022\n",
      "Epoch 416/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3512 - val_loss: 608.0018\n",
      "Epoch 417/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3513 - val_loss: 608.0016\n",
      "Epoch 418/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3514 - val_loss: 608.0016\n",
      "Epoch 419/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 33.3514 - val_loss: 608.0015\n",
      "Epoch 420/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3515 - val_loss: 608.0010\n",
      "Epoch 421/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3516 - val_loss: 608.0012\n",
      "Epoch 422/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3516 - val_loss: 608.0011\n",
      "Epoch 423/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3516 - val_loss: 608.0009\n",
      "Epoch 424/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3516 - val_loss: 608.0004\n",
      "Epoch 425/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3517 - val_loss: 608.0004\n",
      "Epoch 426/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3518 - val_loss: 608.0004\n",
      "Epoch 427/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3518 - val_loss: 608.0002\n",
      "Epoch 428/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3518 - val_loss: 608.0001\n",
      "Epoch 429/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3520 - val_loss: 608.0004\n",
      "Epoch 430/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3520 - val_loss: 608.0006\n",
      "Epoch 431/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3519 - val_loss: 608.0003\n",
      "Epoch 432/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3519 - val_loss: 607.9999\n",
      "Epoch 433/500\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 33.3520 - val_loss: 607.9998\n",
      "Epoch 434/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3521 - val_loss: 607.9996\n",
      "Epoch 435/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3521 - val_loss: 607.9995\n",
      "Epoch 436/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3521 - val_loss: 607.9995\n",
      "Epoch 437/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3522 - val_loss: 607.9993\n",
      "Epoch 438/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3522 - val_loss: 607.9991\n",
      "Epoch 439/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 33.3523 - val_loss: 607.9991\n",
      "Epoch 440/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3523 - val_loss: 607.9991\n",
      "Epoch 441/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3523 - val_loss: 607.9990\n",
      "Epoch 442/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3524 - val_loss: 607.9990\n",
      "Epoch 443/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3523 - val_loss: 607.9987\n",
      "Epoch 444/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3524 - val_loss: 607.9985\n",
      "Epoch 445/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3525 - val_loss: 607.9986\n",
      "Epoch 446/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3525 - val_loss: 607.9987\n",
      "Epoch 447/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3525 - val_loss: 607.9988\n",
      "Epoch 448/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3525 - val_loss: 607.9989\n",
      "Epoch 449/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 33.3525 - val_loss: 607.9988\n",
      "Epoch 450/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3525 - val_loss: 607.9984\n",
      "Epoch 451/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3525 - val_loss: 607.9983\n",
      "Epoch 452/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3526 - val_loss: 607.9983\n",
      "Epoch 453/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3526 - val_loss: 607.9981\n",
      "Epoch 454/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3527 - val_loss: 607.9982\n",
      "Epoch 455/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3527 - val_loss: 607.9983\n",
      "Epoch 456/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3527 - val_loss: 607.9982\n",
      "Epoch 457/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3527 - val_loss: 607.9982\n",
      "Epoch 458/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3527 - val_loss: 607.9980\n",
      "Epoch 459/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3528 - val_loss: 607.9981\n",
      "Epoch 460/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3528 - val_loss: 607.9981\n",
      "Epoch 461/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3528 - val_loss: 607.9981\n",
      "Epoch 462/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 33.3528 - val_loss: 607.9980\n",
      "Epoch 463/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3528 - val_loss: 607.9980\n",
      "Epoch 464/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3528 - val_loss: 607.9979\n",
      "Epoch 465/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3528 - val_loss: 607.9979\n",
      "Epoch 466/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3529 - val_loss: 607.9977\n",
      "Epoch 467/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3529 - val_loss: 607.9976\n",
      "Epoch 468/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3528 - val_loss: 607.9974\n",
      "Epoch 469/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3529 - val_loss: 607.9971\n",
      "Epoch 470/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3530 - val_loss: 607.9971\n",
      "Epoch 471/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3530 - val_loss: 607.9969\n",
      "Epoch 472/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3530 - val_loss: 607.9966\n",
      "Epoch 473/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3530 - val_loss: 607.9965\n",
      "Epoch 474/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3531 - val_loss: 607.9966\n",
      "Epoch 475/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 33.3531 - val_loss: 607.9968\n",
      "Epoch 476/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3531 - val_loss: 607.9971\n",
      "Epoch 477/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3531 - val_loss: 607.9972\n",
      "Epoch 478/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3531 - val_loss: 607.9972\n",
      "Epoch 479/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3531 - val_loss: 607.9969\n",
      "Epoch 480/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3531 - val_loss: 607.9966\n",
      "Epoch 481/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3531 - val_loss: 607.9965\n",
      "Epoch 482/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3531 - val_loss: 607.9966\n",
      "Epoch 483/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3531 - val_loss: 607.9966\n",
      "Epoch 484/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3531 - val_loss: 607.9965\n",
      "Epoch 485/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3531 - val_loss: 607.9965\n",
      "Epoch 486/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3531 - val_loss: 607.9965\n",
      "Epoch 487/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3532 - val_loss: 607.9963\n",
      "Epoch 488/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3532 - val_loss: 607.9962\n",
      "Epoch 489/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3532 - val_loss: 607.9960\n",
      "Epoch 490/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3532 - val_loss: 607.9959\n",
      "Epoch 491/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3532 - val_loss: 607.9959\n",
      "Epoch 492/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3532 - val_loss: 607.9958\n",
      "Epoch 493/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.3532 - val_loss: 607.9958\n",
      "Epoch 494/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3533 - val_loss: 607.9959\n",
      "Epoch 495/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3533 - val_loss: 607.9959\n",
      "Epoch 496/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 33.3533 - val_loss: 607.9960\n",
      "Epoch 497/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3533 - val_loss: 607.9960\n",
      "Epoch 498/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3533 - val_loss: 607.9962\n",
      "Epoch 499/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3533 - val_loss: 607.9963\n",
      "Epoch 500/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.3533 - val_loss: 607.9966\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(82.7524, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0031, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0003, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0171, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(3.0230, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 388ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.78221989e+01, 5.76961485e+01, 5.75700980e+01, 5.74440476e+01,\n",
       "        6.55019760e+01, 0.00000000e+00, 0.00000000e+00, 2.25151930e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.62066305e-01,\n",
       "        2.31329113e-01, 5.64162698e+01, 5.63238329e+01, 5.62313959e+01,\n",
       "        5.61389589e+01, 5.60465219e+01, 5.59540850e+01, 5.58616480e+01,\n",
       "        5.57692110e+01, 5.56767740e+01, 5.55900327e+01, 5.55309292e+01,\n",
       "        5.54723856e+01, 5.54135621e+01, 5.53547386e+01, 5.52959150e+01,\n",
       "        5.52370915e+01, 5.51782680e+01, 5.51194444e+01, 5.50606209e+01,\n",
       "        5.50017974e+01, 5.49429739e+01, 5.48841460e+01, 5.48255944e+01,\n",
       "        5.47665574e+01, 5.47075154e+01, 5.46484734e+01, 5.45894314e+01,\n",
       "        3.24703380e-02, 6.81859016e-01, 7.94346452e-01, 3.02115470e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.17359865e-01, 5.54266340e+01,\n",
       "        5.53678105e+01, 5.53089869e+01, 5.52501634e+01, 5.51901339e+01,\n",
       "        5.51325163e+01, 5.50736928e+01, 5.50148693e+01, 5.49556046e+01,\n",
       "        5.48976191e+01, 5.48471989e+01, 5.47967787e+01, 5.47463585e+01,\n",
       "        5.46959384e+01, 5.46455182e+01, 5.45950980e+01, 5.45446779e+01,\n",
       "        5.44942577e+01, 5.44438375e+01, 5.43934174e+01, 5.43429972e+01,\n",
       "        5.42888655e+01, 5.42132353e+01, 5.41376050e+01, 5.40619748e+01,\n",
       "        5.39863445e+01, 6.28420219e+01, 3.81504953e-01, 4.23922650e-02,\n",
       "        7.58284330e-01, 6.48473263e-01, 7.46107101e-01, 5.60725451e-01,\n",
       "        4.21908684e+01, 4.58209902e-01, 0.00000000e+00, 1.27891511e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.59206975e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.34957731e-01, 7.26839840e-01,\n",
       "        0.00000000e+00, 6.96135312e-02, 0.00000000e+00, 1.17829227e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([47.64968231, 47.63984423, 47.63000615, 47.62016807, 47.61032999,\n",
       "       47.6004919 , 47.59065382, 47.58081574, 47.57097766, 47.56113958,\n",
       "       47.5513015 , 47.54146341, 47.53162533, 47.52178725, 47.51194917,\n",
       "       47.50211109, 47.49227301, 47.48243493, 47.47259684, 47.46275876,\n",
       "       47.45292068, 47.4430826 , 47.43324452, 47.42340644, 47.41356835,\n",
       "       47.40373027, 47.39389219, 47.38405411, 47.37421603, 47.36437795,\n",
       "       47.35453986, 47.34470178, 47.3348637 , 47.32502562, 47.31518754,\n",
       "       47.30534946, 47.29551138, 47.28567329, 47.27583521, 47.26599713,\n",
       "       47.25615905, 47.24632097, 47.23648289, 47.2266448 , 47.21680672,\n",
       "       47.20696864, 47.19713056, 47.18729248, 47.1774544 , 47.16761631,\n",
       "       47.15777823, 47.14794015, 47.13810207, 47.12826399, 47.11842591,\n",
       "       47.10858783, 47.09874974, 47.08891166, 47.07907358, 47.0692355 ,\n",
       "       47.05939742, 47.04955934, 47.03972125, 47.02988317, 47.02004509,\n",
       "       47.01020701, 47.00036893, 46.99053085, 46.98069276, 46.97085468,\n",
       "       46.9610166 , 46.95117852, 46.94134044, 46.93150236, 46.92166428,\n",
       "       46.91182619, 46.90198811, 46.89215003, 46.88231195, 46.87247387,\n",
       "       46.86263579, 46.8527977 , 46.84295962, 46.83312154, 46.82328346,\n",
       "       46.81344538, 46.8036073 , 46.79376922, 46.78393113, 46.77409305,\n",
       "       46.76425497, 46.75441689, 46.74457881, 46.73474073, 46.72490264,\n",
       "       46.71506456, 46.70522648, 46.6953884 , 46.68555032, 46.67571224])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.296372192297145\n",
      "23.650287609561456\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
