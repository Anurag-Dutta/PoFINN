{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       84.600000\n",
       "1       84.431933\n",
       "2       84.263866\n",
       "3       84.095798\n",
       "4       83.927731\n",
       "          ...    \n",
       "2445    45.731256\n",
       "2446    45.721418\n",
       "2447    45.711580\n",
       "2448    45.701742\n",
       "2449    45.691904\n",
       "Name: C6, Length: 2450, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c6_interpolated_2350_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       84.600000\n",
       "1       84.431933\n",
       "2       84.263866\n",
       "3       84.095798\n",
       "4       83.927731\n",
       "          ...    \n",
       "2345     0.634958\n",
       "2346     0.726840\n",
       "2347     0.000000\n",
       "2348     0.069614\n",
       "2349     0.000000\n",
       "Name: C6, Length: 2350, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2350)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.431933</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84.263866</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84.095798</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83.927731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     84.600000  0.000298\n",
       "1     84.431933  0.000298\n",
       "2     84.263866  0.000297\n",
       "3     84.095798  0.000297\n",
       "4     83.927731  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 82.7524  # Value for C0\n",
    "K0 = -0.0031  # Value for K0\n",
    "K1 = -0.0003  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0171    # Value for b\n",
    "c = 3.0230    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    84.600000    84.431933    84.263866    84.095798    83.927731   \n",
      "351    84.431933    84.263866    84.095798    83.927731    83.759664   \n",
      "352    84.263866    84.095798    83.927731    83.759664    83.591597   \n",
      "353    84.095798    83.927731    83.759664    83.591597    83.423529   \n",
      "354    83.927731    83.759664    83.591597    83.423529    83.255462   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    83.759664    83.591597    83.423529    83.255462    83.092437  ...   \n",
      "351    83.591597    83.423529    83.255462    83.092437    82.991597  ...   \n",
      "352    83.423529    83.255462    83.092437    82.991597    82.890756  ...   \n",
      "353    83.255462    83.092437    82.991597    82.890756    82.789916  ...   \n",
      "354    83.092437    82.991597    82.890756    82.789916    82.689076  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   70.003688    0.000263   69.960738    0.000263   69.917787    0.000263   \n",
      "351   69.960738    0.000263   69.917787    0.000263   69.874837    0.000262   \n",
      "352   69.917787    0.000263   69.874837    0.000262   69.831886    0.000262   \n",
      "353   69.874837    0.000262   69.831886    0.000262   69.788936    0.000262   \n",
      "354   69.831886    0.000262   69.788936    0.000262   69.745985    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   69.874837    0.000262   69.831886    0.000262  \n",
      "351   69.831886    0.000262   69.788936    0.000262  \n",
      "352   69.788936    0.000262   69.745985    0.000262  \n",
      "353   69.745985    0.000262   69.703035    0.000262  \n",
      "354   69.703035    0.000262   69.660084    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1900, 1, 251) (1900, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "24/24 [==============================] - 3s 23ms/step - loss: 3555.0020 - val_loss: 2110.9880\n",
      "Epoch 2/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3403.3806 - val_loss: 2022.7549\n",
      "Epoch 3/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3306.5171 - val_loss: 1972.0671\n",
      "Epoch 4/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 3231.6724 - val_loss: 1928.9847\n",
      "Epoch 5/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3159.0010 - val_loss: 1887.2360\n",
      "Epoch 6/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 3088.0947 - val_loss: 1846.6201\n",
      "Epoch 7/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3018.7039 - val_loss: 1806.6543\n",
      "Epoch 8/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2938.4534 - val_loss: 1752.9729\n",
      "Epoch 9/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2852.6865 - val_loss: 1707.0426\n",
      "Epoch 10/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2775.3911 - val_loss: 1663.8035\n",
      "Epoch 11/500\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 2701.6726 - val_loss: 1622.6500\n",
      "Epoch 12/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 2630.6323 - val_loss: 1583.1375\n",
      "Epoch 13/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 2561.7485 - val_loss: 1545.0306\n",
      "Epoch 14/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2494.7366 - val_loss: 1508.1888\n",
      "Epoch 15/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2429.4216 - val_loss: 1472.5190\n",
      "Epoch 16/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2365.6821 - val_loss: 1437.9537\n",
      "Epoch 17/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2303.4312 - val_loss: 1404.4414\n",
      "Epoch 18/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2242.6016 - val_loss: 1371.9386\n",
      "Epoch 19/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2182.9504 - val_loss: 1339.5133\n",
      "Epoch 20/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2120.4126 - val_loss: 1305.7570\n",
      "Epoch 21/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2058.0576 - val_loss: 1273.7622\n",
      "Epoch 22/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1998.3196 - val_loss: 1243.2448\n",
      "Epoch 23/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 1940.6528 - val_loss: 1213.9666\n",
      "Epoch 24/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1884.7277 - val_loss: 1185.8022\n",
      "Epoch 25/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1830.3628 - val_loss: 1158.6902\n",
      "Epoch 26/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1777.4431 - val_loss: 1132.5413\n",
      "Epoch 27/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1725.8864 - val_loss: 1107.2169\n",
      "Epoch 28/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1675.6315 - val_loss: 1082.8486\n",
      "Epoch 29/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1626.6277 - val_loss: 1059.3625\n",
      "Epoch 30/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 1578.8330 - val_loss: 1036.7169\n",
      "Epoch 31/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1532.2125 - val_loss: 1014.8906\n",
      "Epoch 32/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1486.7324 - val_loss: 993.8890\n",
      "Epoch 33/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1442.3646 - val_loss: 973.5673\n",
      "Epoch 34/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1399.0815 - val_loss: 954.0439\n",
      "Epoch 35/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1356.8595 - val_loss: 935.2997\n",
      "Epoch 36/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1315.6743 - val_loss: 917.8125\n",
      "Epoch 37/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 1275.5043 - val_loss: 899.4589\n",
      "Epoch 38/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1236.3287 - val_loss: 882.7808\n",
      "Epoch 39/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 1198.1274 - val_loss: 866.7731\n",
      "Epoch 40/500\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 1160.8815 - val_loss: 851.4205\n",
      "Epoch 41/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 1124.5720 - val_loss: 836.7087\n",
      "Epoch 42/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 1089.1809 - val_loss: 822.6234\n",
      "Epoch 43/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1054.6914 - val_loss: 809.1506\n",
      "Epoch 44/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1021.0857 - val_loss: 796.2770\n",
      "Epoch 45/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 988.3475 - val_loss: 783.9892\n",
      "Epoch 46/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 956.4609 - val_loss: 772.2739\n",
      "Epoch 47/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 925.4101 - val_loss: 761.1182\n",
      "Epoch 48/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 895.1787 - val_loss: 750.5095\n",
      "Epoch 49/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 865.7521 - val_loss: 740.4352\n",
      "Epoch 50/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 837.1152 - val_loss: 730.8828\n",
      "Epoch 51/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 809.2530 - val_loss: 721.8403\n",
      "Epoch 52/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 782.1510 - val_loss: 713.2955\n",
      "Epoch 53/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 755.7950 - val_loss: 705.2365\n",
      "Epoch 54/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 730.1707 - val_loss: 697.6514\n",
      "Epoch 55/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 705.2642 - val_loss: 690.5288\n",
      "Epoch 56/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 681.0620 - val_loss: 683.8570\n",
      "Epoch 57/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 657.5500 - val_loss: 677.6245\n",
      "Epoch 58/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 634.7153 - val_loss: 671.8203\n",
      "Epoch 59/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 612.5444 - val_loss: 666.4330\n",
      "Epoch 60/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 591.0242 - val_loss: 661.4517\n",
      "Epoch 61/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 570.1418 - val_loss: 656.8651\n",
      "Epoch 62/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 549.8842 - val_loss: 652.6627\n",
      "Epoch 63/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 530.2390 - val_loss: 648.8336\n",
      "Epoch 64/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 511.1933 - val_loss: 645.3673\n",
      "Epoch 65/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 492.7350 - val_loss: 642.2531\n",
      "Epoch 66/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 474.8515 - val_loss: 639.4805\n",
      "Epoch 67/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 457.5310 - val_loss: 637.0394\n",
      "Epoch 68/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 440.7614 - val_loss: 634.9193\n",
      "Epoch 69/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 424.5307 - val_loss: 633.1103\n",
      "Epoch 70/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 408.8269 - val_loss: 631.6022\n",
      "Epoch 71/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 393.6383 - val_loss: 630.3851\n",
      "Epoch 72/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 378.9537 - val_loss: 629.4492\n",
      "Epoch 73/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 364.7616 - val_loss: 628.7847\n",
      "Epoch 74/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 351.0507 - val_loss: 628.3821\n",
      "Epoch 75/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 337.8094 - val_loss: 628.2318\n",
      "Epoch 76/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 325.0271 - val_loss: 628.3243\n",
      "Epoch 77/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 312.6926 - val_loss: 628.6505\n",
      "Epoch 78/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 300.7952 - val_loss: 629.2009\n",
      "Epoch 79/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 289.3239 - val_loss: 629.9666\n",
      "Epoch 80/500\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 278.2684 - val_loss: 630.9387\n",
      "Epoch 81/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 267.6179 - val_loss: 632.1082\n",
      "Epoch 82/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 257.3623 - val_loss: 633.4662\n",
      "Epoch 83/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 247.4913 - val_loss: 635.0042\n",
      "Epoch 84/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 237.9947 - val_loss: 636.7137\n",
      "Epoch 85/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 228.8626 - val_loss: 638.5864\n",
      "Epoch 86/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 220.0848 - val_loss: 640.6138\n",
      "Epoch 87/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 211.6519 - val_loss: 642.7879\n",
      "Epoch 88/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 203.5540 - val_loss: 645.1005\n",
      "Epoch 89/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 195.7818 - val_loss: 647.5438\n",
      "Epoch 90/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 188.3258 - val_loss: 650.1100\n",
      "Epoch 91/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 181.1770 - val_loss: 652.7915\n",
      "Epoch 92/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 174.3259 - val_loss: 655.5808\n",
      "Epoch 93/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 167.7638 - val_loss: 658.4706\n",
      "Epoch 94/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 161.4816 - val_loss: 661.4536\n",
      "Epoch 95/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 155.4709 - val_loss: 664.5226\n",
      "Epoch 96/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 149.7229 - val_loss: 667.6708\n",
      "Epoch 97/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 144.2291 - val_loss: 670.8914\n",
      "Epoch 98/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 138.9815 - val_loss: 674.1778\n",
      "Epoch 99/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 133.9717 - val_loss: 677.5236\n",
      "Epoch 100/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 129.1918 - val_loss: 680.9221\n",
      "Epoch 101/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 124.6339 - val_loss: 684.3674\n",
      "Epoch 102/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 120.2902 - val_loss: 687.8536\n",
      "Epoch 103/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 116.1532 - val_loss: 691.3746\n",
      "Epoch 104/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 112.2155 - val_loss: 694.9247\n",
      "Epoch 105/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 108.4699 - val_loss: 698.4987\n",
      "Epoch 106/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 104.9091 - val_loss: 702.0911\n",
      "Epoch 107/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 101.5260 - val_loss: 705.6965\n",
      "Epoch 108/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 98.3142 - val_loss: 709.3099\n",
      "Epoch 109/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 95.2669 - val_loss: 712.9268\n",
      "Epoch 110/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 92.3775 - val_loss: 716.5421\n",
      "Epoch 111/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 89.6395 - val_loss: 720.1517\n",
      "Epoch 112/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 87.0470 - val_loss: 723.7509\n",
      "Epoch 113/500\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 84.5937 - val_loss: 727.3357\n",
      "Epoch 114/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 82.2740 - val_loss: 730.9020\n",
      "Epoch 115/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 80.0819 - val_loss: 734.4460\n",
      "Epoch 116/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 78.0121 - val_loss: 737.9645\n",
      "Epoch 117/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 76.0589 - val_loss: 741.4536\n",
      "Epoch 118/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 74.2171 - val_loss: 744.9100\n",
      "Epoch 119/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 72.4818 - val_loss: 748.3303\n",
      "Epoch 120/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 70.8479 - val_loss: 751.7125\n",
      "Epoch 121/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 69.3106 - val_loss: 755.0533\n",
      "Epoch 122/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 67.8653 - val_loss: 758.3496\n",
      "Epoch 123/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 66.5076 - val_loss: 761.5994\n",
      "Epoch 124/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 65.2331 - val_loss: 764.8008\n",
      "Epoch 125/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 64.0375 - val_loss: 767.9512\n",
      "Epoch 126/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 62.9169 - val_loss: 771.0490\n",
      "Epoch 127/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 61.8673 - val_loss: 774.0920\n",
      "Epoch 128/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 60.8851 - val_loss: 777.0788\n",
      "Epoch 129/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 59.9665 - val_loss: 780.0081\n",
      "Epoch 130/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 59.1083 - val_loss: 782.8784\n",
      "Epoch 131/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 58.3069 - val_loss: 785.6889\n",
      "Epoch 132/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 57.5593 - val_loss: 788.4379\n",
      "Epoch 133/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 56.8624 - val_loss: 791.1250\n",
      "Epoch 134/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 56.2132 - val_loss: 793.7493\n",
      "Epoch 135/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 55.6091 - val_loss: 796.3105\n",
      "Epoch 136/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 55.0472 - val_loss: 798.8071\n",
      "Epoch 137/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 54.5251 - val_loss: 801.2404\n",
      "Epoch 138/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 54.0403 - val_loss: 803.6089\n",
      "Epoch 139/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 53.5905 - val_loss: 805.9131\n",
      "Epoch 140/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 53.1735 - val_loss: 808.1529\n",
      "Epoch 141/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 52.7873 - val_loss: 810.3278\n",
      "Epoch 142/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 52.4298 - val_loss: 812.4388\n",
      "Epoch 143/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 52.0991 - val_loss: 814.4860\n",
      "Epoch 144/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 51.7935 - val_loss: 816.4699\n",
      "Epoch 145/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 51.5113 - val_loss: 818.3911\n",
      "Epoch 146/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 51.2509 - val_loss: 820.2493\n",
      "Epoch 147/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 51.0108 - val_loss: 822.0461\n",
      "Epoch 148/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 50.7896 - val_loss: 823.7820\n",
      "Epoch 149/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 50.5858 - val_loss: 825.4576\n",
      "Epoch 150/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 50.3985 - val_loss: 827.0741\n",
      "Epoch 151/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 50.2261 - val_loss: 828.6319\n",
      "Epoch 152/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 50.0679 - val_loss: 830.1326\n",
      "Epoch 153/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 49.9225 - val_loss: 831.5770\n",
      "Epoch 154/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 49.7892 - val_loss: 832.9661\n",
      "Epoch 155/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 49.6669 - val_loss: 834.3011\n",
      "Epoch 156/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 49.5549 - val_loss: 835.5831\n",
      "Epoch 157/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 49.4524 - val_loss: 836.8137\n",
      "Epoch 158/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 49.3584 - val_loss: 837.9931\n",
      "Epoch 159/500\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 49.2726 - val_loss: 839.1238\n",
      "Epoch 160/500\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 49.1941 - val_loss: 840.2062\n",
      "Epoch 161/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 49.1224 - val_loss: 841.2416\n",
      "Epoch 162/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 49.0569 - val_loss: 842.2321\n",
      "Epoch 163/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.9971 - val_loss: 843.1780\n",
      "Epoch 164/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.9426 - val_loss: 844.0814\n",
      "Epoch 165/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 48.8930 - val_loss: 844.9429\n",
      "Epoch 166/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 48.8477 - val_loss: 845.7643\n",
      "Epoch 167/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 48.8064 - val_loss: 846.5468\n",
      "Epoch 168/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.7689 - val_loss: 847.2916\n",
      "Epoch 169/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.7347 - val_loss: 848.0002\n",
      "Epoch 170/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.7036 - val_loss: 848.6738\n",
      "Epoch 171/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.6754 - val_loss: 849.3137\n",
      "Epoch 172/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.6497 - val_loss: 849.9209\n",
      "Epoch 173/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.6263 - val_loss: 850.4967\n",
      "Epoch 174/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.6051 - val_loss: 851.0426\n",
      "Epoch 175/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.5859 - val_loss: 851.5595\n",
      "Epoch 176/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.5685 - val_loss: 852.0487\n",
      "Epoch 177/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.5526 - val_loss: 852.5114\n",
      "Epoch 178/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.5382 - val_loss: 852.9484\n",
      "Epoch 179/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 48.5253 - val_loss: 853.3610\n",
      "Epoch 180/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 48.5135 - val_loss: 853.7507\n",
      "Epoch 181/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.5028 - val_loss: 854.1177\n",
      "Epoch 182/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.4932 - val_loss: 854.4636\n",
      "Epoch 183/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.4845 - val_loss: 854.7895\n",
      "Epoch 184/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 48.4767 - val_loss: 855.0956\n",
      "Epoch 185/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.4696 - val_loss: 855.3839\n",
      "Epoch 186/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.4632 - val_loss: 855.6544\n",
      "Epoch 187/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.4575 - val_loss: 855.9086\n",
      "Epoch 188/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 48.4523 - val_loss: 856.1464\n",
      "Epoch 189/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.4478 - val_loss: 856.3702\n",
      "Epoch 190/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4436 - val_loss: 856.5790\n",
      "Epoch 191/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4399 - val_loss: 856.7747\n",
      "Epoch 192/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 48.4366 - val_loss: 856.9572\n",
      "Epoch 193/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.4338 - val_loss: 857.1281\n",
      "Epoch 194/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.4312 - val_loss: 857.2878\n",
      "Epoch 195/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4289 - val_loss: 857.4363\n",
      "Epoch 196/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4269 - val_loss: 857.5750\n",
      "Epoch 197/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4253 - val_loss: 857.7041\n",
      "Epoch 198/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 48.4238 - val_loss: 857.8244\n",
      "Epoch 199/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.4225 - val_loss: 857.9363\n",
      "Epoch 200/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4215 - val_loss: 858.0395\n",
      "Epoch 201/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4206 - val_loss: 858.1356\n",
      "Epoch 202/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4199 - val_loss: 858.2246\n",
      "Epoch 203/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4194 - val_loss: 858.3073\n",
      "Epoch 204/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4189 - val_loss: 858.3839\n",
      "Epoch 205/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4187 - val_loss: 858.4543\n",
      "Epoch 206/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4185 - val_loss: 858.5199\n",
      "Epoch 207/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4185 - val_loss: 858.5800\n",
      "Epoch 208/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4185 - val_loss: 858.6357\n",
      "Epoch 209/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4186 - val_loss: 858.6867\n",
      "Epoch 210/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4189 - val_loss: 858.7334\n",
      "Epoch 211/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4192 - val_loss: 858.7761\n",
      "Epoch 212/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4196 - val_loss: 858.8157\n",
      "Epoch 213/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4200 - val_loss: 858.8519\n",
      "Epoch 214/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4205 - val_loss: 858.8847\n",
      "Epoch 215/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4210 - val_loss: 858.9149\n",
      "Epoch 216/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4216 - val_loss: 858.9424\n",
      "Epoch 217/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4223 - val_loss: 858.9676\n",
      "Epoch 218/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4229 - val_loss: 858.9903\n",
      "Epoch 219/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4236 - val_loss: 859.0107\n",
      "Epoch 220/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4244 - val_loss: 859.0296\n",
      "Epoch 221/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 48.4251 - val_loss: 859.0462\n",
      "Epoch 222/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.4259 - val_loss: 859.0611\n",
      "Epoch 223/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4268 - val_loss: 859.0746\n",
      "Epoch 224/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4276 - val_loss: 859.0869\n",
      "Epoch 225/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4285 - val_loss: 859.0975\n",
      "Epoch 226/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4293 - val_loss: 859.1069\n",
      "Epoch 227/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4302 - val_loss: 859.1153\n",
      "Epoch 228/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4311 - val_loss: 859.1227\n",
      "Epoch 229/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4320 - val_loss: 859.1293\n",
      "Epoch 230/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4329 - val_loss: 859.1349\n",
      "Epoch 231/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4338 - val_loss: 859.1396\n",
      "Epoch 232/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4348 - val_loss: 859.1437\n",
      "Epoch 233/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4358 - val_loss: 859.1471\n",
      "Epoch 234/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4366 - val_loss: 859.1501\n",
      "Epoch 235/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4376 - val_loss: 859.1523\n",
      "Epoch 236/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4385 - val_loss: 859.1542\n",
      "Epoch 237/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.4394 - val_loss: 859.1555\n",
      "Epoch 238/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4403 - val_loss: 859.1563\n",
      "Epoch 239/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4413 - val_loss: 859.1570\n",
      "Epoch 240/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4422 - val_loss: 859.1570\n",
      "Epoch 241/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4431 - val_loss: 859.1569\n",
      "Epoch 242/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4440 - val_loss: 859.1566\n",
      "Epoch 243/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 48.4449 - val_loss: 859.1558\n",
      "Epoch 244/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.4458 - val_loss: 859.1551\n",
      "Epoch 245/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4467 - val_loss: 859.1538\n",
      "Epoch 246/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4476 - val_loss: 859.1527\n",
      "Epoch 247/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4485 - val_loss: 859.1511\n",
      "Epoch 248/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4494 - val_loss: 859.1498\n",
      "Epoch 249/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4502 - val_loss: 859.1479\n",
      "Epoch 250/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4510 - val_loss: 859.1460\n",
      "Epoch 251/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4519 - val_loss: 859.1441\n",
      "Epoch 252/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4527 - val_loss: 859.1420\n",
      "Epoch 253/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4536 - val_loss: 859.1400\n",
      "Epoch 254/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4544 - val_loss: 859.1379\n",
      "Epoch 255/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4552 - val_loss: 859.1357\n",
      "Epoch 256/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4560 - val_loss: 859.1333\n",
      "Epoch 257/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 48.4567 - val_loss: 859.1309\n",
      "Epoch 258/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.4575 - val_loss: 859.1288\n",
      "Epoch 259/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 48.4583 - val_loss: 859.1263\n",
      "Epoch 260/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.4590 - val_loss: 859.1240\n",
      "Epoch 261/500\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 48.4598 - val_loss: 859.1217\n",
      "Epoch 262/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 48.4605 - val_loss: 859.1194\n",
      "Epoch 263/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4612 - val_loss: 859.1169\n",
      "Epoch 264/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4619 - val_loss: 859.1147\n",
      "Epoch 265/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4627 - val_loss: 859.1124\n",
      "Epoch 266/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4633 - val_loss: 859.1099\n",
      "Epoch 267/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4640 - val_loss: 859.1078\n",
      "Epoch 268/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4647 - val_loss: 859.1056\n",
      "Epoch 269/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4653 - val_loss: 859.1034\n",
      "Epoch 270/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4660 - val_loss: 859.1008\n",
      "Epoch 271/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4666 - val_loss: 859.0989\n",
      "Epoch 272/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4672 - val_loss: 859.0964\n",
      "Epoch 273/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4678 - val_loss: 859.0944\n",
      "Epoch 274/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4685 - val_loss: 859.0921\n",
      "Epoch 275/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4690 - val_loss: 859.0901\n",
      "Epoch 276/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4696 - val_loss: 859.0876\n",
      "Epoch 277/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4702 - val_loss: 859.0859\n",
      "Epoch 278/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4708 - val_loss: 859.0839\n",
      "Epoch 279/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4713 - val_loss: 859.0821\n",
      "Epoch 280/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4719 - val_loss: 859.0798\n",
      "Epoch 281/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4724 - val_loss: 859.0781\n",
      "Epoch 282/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4729 - val_loss: 859.0763\n",
      "Epoch 283/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 48.4734 - val_loss: 859.0743\n",
      "Epoch 284/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.4739 - val_loss: 859.0724\n",
      "Epoch 285/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4744 - val_loss: 859.0705\n",
      "Epoch 286/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.4749 - val_loss: 859.0690\n",
      "Epoch 287/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4754 - val_loss: 859.0670\n",
      "Epoch 288/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4759 - val_loss: 859.0652\n",
      "Epoch 289/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4763 - val_loss: 859.0634\n",
      "Epoch 290/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4768 - val_loss: 859.0618\n",
      "Epoch 291/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4772 - val_loss: 859.0602\n",
      "Epoch 292/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4777 - val_loss: 859.0583\n",
      "Epoch 293/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4781 - val_loss: 859.0571\n",
      "Epoch 294/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4785 - val_loss: 859.0549\n",
      "Epoch 295/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4789 - val_loss: 859.0538\n",
      "Epoch 296/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4793 - val_loss: 859.0523\n",
      "Epoch 297/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4797 - val_loss: 859.0502\n",
      "Epoch 298/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4801 - val_loss: 859.0493\n",
      "Epoch 299/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4804 - val_loss: 859.0475\n",
      "Epoch 300/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4809 - val_loss: 859.0461\n",
      "Epoch 301/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 48.4813 - val_loss: 859.0450\n",
      "Epoch 302/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4816 - val_loss: 859.0434\n",
      "Epoch 303/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4819 - val_loss: 859.0421\n",
      "Epoch 304/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4823 - val_loss: 859.0410\n",
      "Epoch 305/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4826 - val_loss: 859.0396\n",
      "Epoch 306/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4829 - val_loss: 859.0384\n",
      "Epoch 307/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4832 - val_loss: 859.0372\n",
      "Epoch 308/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4836 - val_loss: 859.0358\n",
      "Epoch 309/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4839 - val_loss: 859.0347\n",
      "Epoch 310/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4842 - val_loss: 859.0334\n",
      "Epoch 311/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4845 - val_loss: 859.0324\n",
      "Epoch 312/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4848 - val_loss: 859.0316\n",
      "Epoch 313/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4850 - val_loss: 859.0298\n",
      "Epoch 314/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4854 - val_loss: 859.0289\n",
      "Epoch 315/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4856 - val_loss: 859.0283\n",
      "Epoch 316/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4858 - val_loss: 859.0272\n",
      "Epoch 317/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4861 - val_loss: 859.0259\n",
      "Epoch 318/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4864 - val_loss: 859.0250\n",
      "Epoch 319/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4866 - val_loss: 859.0241\n",
      "Epoch 320/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.4869 - val_loss: 859.0230\n",
      "Epoch 321/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 48.4871 - val_loss: 859.0223\n",
      "Epoch 322/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4873 - val_loss: 859.0218\n",
      "Epoch 323/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4876 - val_loss: 859.0208\n",
      "Epoch 324/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4878 - val_loss: 859.0198\n",
      "Epoch 325/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4880 - val_loss: 859.0189\n",
      "Epoch 326/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4882 - val_loss: 859.0182\n",
      "Epoch 327/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4885 - val_loss: 859.0173\n",
      "Epoch 328/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4886 - val_loss: 859.0164\n",
      "Epoch 329/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4889 - val_loss: 859.0158\n",
      "Epoch 330/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4890 - val_loss: 859.0148\n",
      "Epoch 331/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4893 - val_loss: 859.0145\n",
      "Epoch 332/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4894 - val_loss: 859.0137\n",
      "Epoch 333/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4896 - val_loss: 859.0132\n",
      "Epoch 334/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4898 - val_loss: 859.0121\n",
      "Epoch 335/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4900 - val_loss: 859.0112\n",
      "Epoch 336/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4901 - val_loss: 859.0105\n",
      "Epoch 337/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4903 - val_loss: 859.0099\n",
      "Epoch 338/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4905 - val_loss: 859.0094\n",
      "Epoch 339/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4906 - val_loss: 859.0085\n",
      "Epoch 340/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4907 - val_loss: 859.0076\n",
      "Epoch 341/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4910 - val_loss: 859.0074\n",
      "Epoch 342/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4911 - val_loss: 859.0070\n",
      "Epoch 343/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4912 - val_loss: 859.0062\n",
      "Epoch 344/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 48.4914 - val_loss: 859.0057\n",
      "Epoch 345/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4915 - val_loss: 859.0050\n",
      "Epoch 346/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4916 - val_loss: 859.0042\n",
      "Epoch 347/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4918 - val_loss: 859.0036\n",
      "Epoch 348/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4919 - val_loss: 859.0031\n",
      "Epoch 349/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4921 - val_loss: 859.0029\n",
      "Epoch 350/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4922 - val_loss: 859.0023\n",
      "Epoch 351/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4923 - val_loss: 859.0020\n",
      "Epoch 352/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4924 - val_loss: 859.0016\n",
      "Epoch 353/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4926 - val_loss: 859.0012\n",
      "Epoch 354/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4926 - val_loss: 859.0005\n",
      "Epoch 355/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4927 - val_loss: 859.0002\n",
      "Epoch 356/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4929 - val_loss: 858.9997\n",
      "Epoch 357/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4930 - val_loss: 858.9993\n",
      "Epoch 358/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4931 - val_loss: 858.9990\n",
      "Epoch 359/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4932 - val_loss: 858.9985\n",
      "Epoch 360/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.4933 - val_loss: 858.9981\n",
      "Epoch 361/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4934 - val_loss: 858.9979\n",
      "Epoch 362/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4935 - val_loss: 858.9977\n",
      "Epoch 363/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4935 - val_loss: 858.9971\n",
      "Epoch 364/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4936 - val_loss: 858.9965\n",
      "Epoch 365/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4938 - val_loss: 858.9962\n",
      "Epoch 366/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4938 - val_loss: 858.9962\n",
      "Epoch 367/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 48.4939 - val_loss: 858.9959\n",
      "Epoch 368/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4940 - val_loss: 858.9955\n",
      "Epoch 369/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4941 - val_loss: 858.9951\n",
      "Epoch 370/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4942 - val_loss: 858.9950\n",
      "Epoch 371/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4942 - val_loss: 858.9945\n",
      "Epoch 372/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4943 - val_loss: 858.9943\n",
      "Epoch 373/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4944 - val_loss: 858.9940\n",
      "Epoch 374/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4944 - val_loss: 858.9938\n",
      "Epoch 375/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4945 - val_loss: 858.9931\n",
      "Epoch 376/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4946 - val_loss: 858.9926\n",
      "Epoch 377/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4947 - val_loss: 858.9924\n",
      "Epoch 378/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4948 - val_loss: 858.9925\n",
      "Epoch 379/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4948 - val_loss: 858.9920\n",
      "Epoch 380/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4948 - val_loss: 858.9915\n",
      "Epoch 381/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4949 - val_loss: 858.9911\n",
      "Epoch 382/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4950 - val_loss: 858.9908\n",
      "Epoch 383/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4950 - val_loss: 858.9905\n",
      "Epoch 384/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4951 - val_loss: 858.9901\n",
      "Epoch 385/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4952 - val_loss: 858.9900\n",
      "Epoch 386/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4952 - val_loss: 858.9900\n",
      "Epoch 387/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4953 - val_loss: 858.9898\n",
      "Epoch 388/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4953 - val_loss: 858.9895\n",
      "Epoch 389/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 48.4953 - val_loss: 858.9895\n",
      "Epoch 390/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4954 - val_loss: 858.9892\n",
      "Epoch 391/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4954 - val_loss: 858.9891\n",
      "Epoch 392/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4954 - val_loss: 858.9890\n",
      "Epoch 393/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4955 - val_loss: 858.9887\n",
      "Epoch 394/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4956 - val_loss: 858.9881\n",
      "Epoch 395/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4956 - val_loss: 858.9879\n",
      "Epoch 396/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.4957 - val_loss: 858.9877\n",
      "Epoch 397/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4957 - val_loss: 858.9874\n",
      "Epoch 398/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4958 - val_loss: 858.9874\n",
      "Epoch 399/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4958 - val_loss: 858.9872\n",
      "Epoch 400/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4959 - val_loss: 858.9872\n",
      "Epoch 401/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4958 - val_loss: 858.9867\n",
      "Epoch 402/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4959 - val_loss: 858.9865\n",
      "Epoch 403/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4959 - val_loss: 858.9863\n",
      "Epoch 404/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4960 - val_loss: 858.9863\n",
      "Epoch 405/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4960 - val_loss: 858.9863\n",
      "Epoch 406/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4961 - val_loss: 858.9861\n",
      "Epoch 407/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4961 - val_loss: 858.9858\n",
      "Epoch 408/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4961 - val_loss: 858.9855\n",
      "Epoch 409/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4961 - val_loss: 858.9852\n",
      "Epoch 410/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 48.4962 - val_loss: 858.9852\n",
      "Epoch 411/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4962 - val_loss: 858.9853\n",
      "Epoch 412/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4963 - val_loss: 858.9852\n",
      "Epoch 413/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4963 - val_loss: 858.9847\n",
      "Epoch 414/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4963 - val_loss: 858.9844\n",
      "Epoch 415/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4963 - val_loss: 858.9846\n",
      "Epoch 416/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4963 - val_loss: 858.9841\n",
      "Epoch 417/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4964 - val_loss: 858.9839\n",
      "Epoch 418/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4964 - val_loss: 858.9839\n",
      "Epoch 419/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4964 - val_loss: 858.9836\n",
      "Epoch 420/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4965 - val_loss: 858.9836\n",
      "Epoch 421/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4965 - val_loss: 858.9834\n",
      "Epoch 422/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4965 - val_loss: 858.9833\n",
      "Epoch 423/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4965 - val_loss: 858.9832\n",
      "Epoch 424/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.4965 - val_loss: 858.9830\n",
      "Epoch 425/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4966 - val_loss: 858.9830\n",
      "Epoch 426/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4966 - val_loss: 858.9830\n",
      "Epoch 427/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4967 - val_loss: 858.9833\n",
      "Epoch 428/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4967 - val_loss: 858.9832\n",
      "Epoch 429/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4967 - val_loss: 858.9833\n",
      "Epoch 430/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 48.4967 - val_loss: 858.9833\n",
      "Epoch 431/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4967 - val_loss: 858.9832\n",
      "Epoch 432/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4967 - val_loss: 858.9830\n",
      "Epoch 433/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4967 - val_loss: 858.9825\n",
      "Epoch 434/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4967 - val_loss: 858.9823\n",
      "Epoch 435/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4967 - val_loss: 858.9822\n",
      "Epoch 436/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4968 - val_loss: 858.9820\n",
      "Epoch 437/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4968 - val_loss: 858.9821\n",
      "Epoch 438/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4968 - val_loss: 858.9820\n",
      "Epoch 439/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4969 - val_loss: 858.9821\n",
      "Epoch 440/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4969 - val_loss: 858.9823\n",
      "Epoch 441/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4969 - val_loss: 858.9825\n",
      "Epoch 442/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4969 - val_loss: 858.9829\n",
      "Epoch 443/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.4969 - val_loss: 858.9828\n",
      "Epoch 444/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4969 - val_loss: 858.9830\n",
      "Epoch 445/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4969 - val_loss: 858.9830\n",
      "Epoch 446/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4969 - val_loss: 858.9832\n",
      "Epoch 447/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4969 - val_loss: 858.9829\n",
      "Epoch 448/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4969 - val_loss: 858.9823\n",
      "Epoch 449/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4969 - val_loss: 858.9819\n",
      "Epoch 450/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4969 - val_loss: 858.9816\n",
      "Epoch 451/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4969 - val_loss: 858.9811\n",
      "Epoch 452/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4970 - val_loss: 858.9809\n",
      "Epoch 453/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4970 - val_loss: 858.9810\n",
      "Epoch 454/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4970 - val_loss: 858.9809\n",
      "Epoch 455/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4971 - val_loss: 858.9811\n",
      "Epoch 456/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4971 - val_loss: 858.9813\n",
      "Epoch 457/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4971 - val_loss: 858.9813\n",
      "Epoch 458/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4971 - val_loss: 858.9815\n",
      "Epoch 459/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4971 - val_loss: 858.9817\n",
      "Epoch 460/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4971 - val_loss: 858.9818\n",
      "Epoch 461/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 48.4971 - val_loss: 858.9818\n",
      "Epoch 462/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4971 - val_loss: 858.9820\n",
      "Epoch 463/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.4971 - val_loss: 858.9824\n",
      "Epoch 464/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4971 - val_loss: 858.9825\n",
      "Epoch 465/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4971 - val_loss: 858.9825\n",
      "Epoch 466/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4971 - val_loss: 858.9825\n",
      "Epoch 467/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4971 - val_loss: 858.9823\n",
      "Epoch 468/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4971 - val_loss: 858.9822\n",
      "Epoch 469/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4971 - val_loss: 858.9818\n",
      "Epoch 470/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4971 - val_loss: 858.9814\n",
      "Epoch 471/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4971 - val_loss: 858.9811\n",
      "Epoch 472/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4971 - val_loss: 858.9809\n",
      "Epoch 473/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4971 - val_loss: 858.9806\n",
      "Epoch 474/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4972 - val_loss: 858.9805\n",
      "Epoch 475/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4972 - val_loss: 858.9802\n",
      "Epoch 476/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4972 - val_loss: 858.9799\n",
      "Epoch 477/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4972 - val_loss: 858.9798\n",
      "Epoch 478/500\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 48.4972 - val_loss: 858.9796\n",
      "Epoch 479/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4972 - val_loss: 858.9794\n",
      "Epoch 480/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4972 - val_loss: 858.9792\n",
      "Epoch 481/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4972 - val_loss: 858.9792\n",
      "Epoch 482/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4972 - val_loss: 858.9794\n",
      "Epoch 483/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4973 - val_loss: 858.9796\n",
      "Epoch 484/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4973 - val_loss: 858.9794\n",
      "Epoch 485/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4972 - val_loss: 858.9794\n",
      "Epoch 486/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4973 - val_loss: 858.9794\n",
      "Epoch 487/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4973 - val_loss: 858.9798\n",
      "Epoch 488/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4973 - val_loss: 858.9798\n",
      "Epoch 489/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4973 - val_loss: 858.9798\n",
      "Epoch 490/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4973 - val_loss: 858.9800\n",
      "Epoch 491/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4973 - val_loss: 858.9800\n",
      "Epoch 492/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4973 - val_loss: 858.9800\n",
      "Epoch 493/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4973 - val_loss: 858.9800\n",
      "Epoch 494/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4973 - val_loss: 858.9799\n",
      "Epoch 495/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 48.4973 - val_loss: 858.9799\n",
      "Epoch 496/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4973 - val_loss: 858.9796\n",
      "Epoch 497/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4973 - val_loss: 858.9796\n",
      "Epoch 498/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4973 - val_loss: 858.9791\n",
      "Epoch 499/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 48.4973 - val_loss: 858.9791\n",
      "Epoch 500/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.4973 - val_loss: 858.9790\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(82.7524, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0031, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0003, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0171, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(3.0230, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 389ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.49233660e+01, 5.48649608e+01, 5.48059188e+01, 5.47468768e+01,\n",
       "        5.46878347e+01, 5.46287927e+01, 5.45697507e+01, 1.09459259e-01,\n",
       "        2.14646727e-01, 3.88949070e-02, 1.76237631e+00, 0.00000000e+00,\n",
       "        1.30328488e+00, 5.54658497e+01, 5.54070261e+01, 5.53482026e+01,\n",
       "        5.52893791e+01, 5.52305556e+01, 5.51717320e+01, 5.51129085e+01,\n",
       "        5.50540850e+01, 5.49952614e+01, 5.49356438e+01, 5.48808123e+01,\n",
       "        5.48303922e+01, 5.47799720e+01, 5.47295518e+01, 5.46791317e+01,\n",
       "        5.46287115e+01, 5.45782913e+01, 5.45278712e+01, 5.44774510e+01,\n",
       "        5.44270308e+01, 5.43766106e+01, 5.43261905e+01, 5.42636555e+01,\n",
       "        5.41880252e+01, 5.41123950e+01, 5.40367647e+01, 5.39611345e+01,\n",
       "        3.37765515e-01, 2.86526829e-01, 4.75062817e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.99287319e-01, 5.47911765e+01,\n",
       "        5.47407563e+01, 5.46903361e+01, 5.46399160e+01, 5.45894958e+01,\n",
       "        5.45390756e+01, 5.44886555e+01, 5.44382353e+01, 5.43878151e+01,\n",
       "        5.43373950e+01, 5.42804622e+01, 5.42048319e+01, 5.41292017e+01,\n",
       "        5.40535714e+01, 5.39779412e+01, 5.91351074e+01, 0.00000000e+00,\n",
       "        5.98447246e+01, 5.83321195e+01, 5.78348786e+01, 5.90417367e+01,\n",
       "        5.80322829e+01, 5.76541316e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.91867260e-01, 4.81827469e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.48017654e+01, 0.00000000e+00, 0.00000000e+00, 2.90124685e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.16637623e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.33734280e-01, 1.39452100e+00,\n",
       "        7.93281555e-01, 1.28464162e-01, 0.00000000e+00, 7.63168156e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46.66587415, 46.65603607, 46.64619799, 46.63635991, 46.62652183,\n",
       "       46.61668375, 46.60684567, 46.59700758, 46.5871695 , 46.57733142,\n",
       "       46.56749334, 46.55765526, 46.54781718, 46.53797909, 46.52814101,\n",
       "       46.51830293, 46.50846485, 46.49862677, 46.48878869, 46.4789506 ,\n",
       "       46.46911252, 46.45927444, 46.44943636, 46.43959828, 46.4297602 ,\n",
       "       46.41992212, 46.41008403, 46.40024595, 46.39040787, 46.38056979,\n",
       "       46.37073171, 46.36089363, 46.35105554, 46.34121746, 46.33137938,\n",
       "       46.3215413 , 46.31170322, 46.30186514, 46.29202705, 46.28218897,\n",
       "       46.27235089, 46.26251281, 46.25267473, 46.24283665, 46.23299857,\n",
       "       46.22316048, 46.2133224 , 46.20348432, 46.19364624, 46.18380816,\n",
       "       46.17397008, 46.16413199, 46.15429391, 46.14445583, 46.13461775,\n",
       "       46.12477967, 46.11494159, 46.1051035 , 46.09526542, 46.08542734,\n",
       "       46.07558926, 46.06575118, 46.0559131 , 46.04607502, 46.03623693,\n",
       "       46.02639885, 46.01656077, 46.00672269, 45.99688461, 45.98704653,\n",
       "       45.97720844, 45.96737036, 45.95753228, 45.9476942 , 45.93785612,\n",
       "       45.92801804, 45.91817995, 45.90834187, 45.89850379, 45.88866571,\n",
       "       45.87882763, 45.86898955, 45.85915147, 45.84931338, 45.8394753 ,\n",
       "       45.82963722, 45.81979914, 45.80996106, 45.80012298, 45.79028489,\n",
       "       45.78044681, 45.77060873, 45.76077065, 45.75093257, 45.74109449,\n",
       "       45.73125641, 45.72141832, 45.71158024, 45.70174216, 45.69190408])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.418703223886208\n",
      "24.26843624009467\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
