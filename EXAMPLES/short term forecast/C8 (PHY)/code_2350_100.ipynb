{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "2445    55.786985\n",
       "2446    55.777967\n",
       "2447    55.768949\n",
       "2448    55.759930\n",
       "2449    55.750912\n",
       "Name: C8, Length: 2450, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c8_interpolated_2350_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "2345     0.489992\n",
       "2346     0.362192\n",
       "2347     0.000000\n",
       "2348     0.000000\n",
       "2349     0.167189\n",
       "Name: C8, Length: 2350, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2350)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.987115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.774230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.561345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.348459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.200000  0.000298\n",
       "1     87.987115  0.000298\n",
       "2     87.774230  0.000297\n",
       "3     87.561345  0.000297\n",
       "4     87.348459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 85.7336  # Value for C0\n",
    "K0 = -0.0020  # Value for K0\n",
    "K1 = 0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0125    # Value for b\n",
    "c = 2.3008    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.200000    87.987115    87.774230    87.561345    87.348459   \n",
      "351    87.987115    87.774230    87.561345    87.348459    87.135574   \n",
      "352    87.774230    87.561345    87.348459    87.135574    86.922689   \n",
      "353    87.561345    87.348459    87.135574    86.922689    86.709804   \n",
      "354    87.348459    87.135574    86.922689    86.709804    86.496919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.135574    86.922689    86.709804    86.496919    86.294538  ...   \n",
      "351    86.922689    86.709804    86.496919    86.294538    86.221709  ...   \n",
      "352    86.709804    86.496919    86.294538    86.221709    86.148880  ...   \n",
      "353    86.496919    86.294538    86.221709    86.148880    86.076050  ...   \n",
      "354    86.294538    86.221709    86.148880    86.076050    86.003221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   76.094351    0.000263   76.058870    0.000263   76.023389    0.000263   \n",
      "351   76.058870    0.000263   76.023389    0.000263   75.987909    0.000262   \n",
      "352   76.023389    0.000263   75.987909    0.000262   75.952428    0.000262   \n",
      "353   75.987909    0.000262   75.952428    0.000262   75.916947    0.000262   \n",
      "354   75.952428    0.000262   75.916947    0.000262   75.881466    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   75.987909    0.000262   75.952428    0.000262  \n",
      "351   75.952428    0.000262   75.916947    0.000262  \n",
      "352   75.916947    0.000262   75.881466    0.000262  \n",
      "353   75.881466    0.000262   75.845985    0.000262  \n",
      "354   75.845985    0.000262   75.810504    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1900, 1, 251) (1900, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "24/24 [==============================] - 3s 29ms/step - loss: 4469.5762 - val_loss: 2818.1438\n",
      "Epoch 2/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 4364.7798 - val_loss: 2746.2996\n",
      "Epoch 3/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 4278.3149 - val_loss: 2696.4255\n",
      "Epoch 4/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 4197.3140 - val_loss: 2649.8855\n",
      "Epoch 5/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 4118.4443 - val_loss: 2604.8904\n",
      "Epoch 6/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 4041.2683 - val_loss: 2560.9358\n",
      "Epoch 7/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3965.5234 - val_loss: 2517.8914\n",
      "Epoch 8/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3891.0708 - val_loss: 2475.6943\n",
      "Epoch 9/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3817.8267 - val_loss: 2434.3057\n",
      "Epoch 10/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 3745.7361 - val_loss: 2393.6963\n",
      "Epoch 11/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 3674.7583 - val_loss: 2353.8459\n",
      "Epoch 12/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3604.8618 - val_loss: 2314.7356\n",
      "Epoch 13/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 3536.0234 - val_loss: 2276.3511\n",
      "Epoch 14/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3468.2195 - val_loss: 2238.6790\n",
      "Epoch 15/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3401.4316 - val_loss: 2201.7070\n",
      "Epoch 16/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3335.6438 - val_loss: 2165.4248\n",
      "Epoch 17/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3270.8411 - val_loss: 2129.8218\n",
      "Epoch 18/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3207.0085 - val_loss: 2094.8884\n",
      "Epoch 19/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3144.1335 - val_loss: 2060.6152\n",
      "Epoch 20/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3082.2017 - val_loss: 2026.9943\n",
      "Epoch 21/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 3021.2026 - val_loss: 1994.0156\n",
      "Epoch 22/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 2961.1240 - val_loss: 1961.6722\n",
      "Epoch 23/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2901.9563 - val_loss: 1929.9556\n",
      "Epoch 24/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2843.6858 - val_loss: 1898.8578\n",
      "Epoch 25/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 2786.3037 - val_loss: 1868.3716\n",
      "Epoch 26/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2729.7998 - val_loss: 1838.4895\n",
      "Epoch 27/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 2674.1641 - val_loss: 1809.2036\n",
      "Epoch 28/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 2619.3857 - val_loss: 1780.5070\n",
      "Epoch 29/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2565.4561 - val_loss: 1752.3925\n",
      "Epoch 30/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2512.3655 - val_loss: 1724.8531\n",
      "Epoch 31/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2460.1045 - val_loss: 1697.8820\n",
      "Epoch 32/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2408.6638 - val_loss: 1671.4720\n",
      "Epoch 33/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2358.0339 - val_loss: 1645.6168\n",
      "Epoch 34/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2308.2068 - val_loss: 1620.3091\n",
      "Epoch 35/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2259.1736 - val_loss: 1595.5430\n",
      "Epoch 36/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2210.9250 - val_loss: 1571.3114\n",
      "Epoch 37/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2163.4524 - val_loss: 1547.6083\n",
      "Epoch 38/500\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 2116.7478 - val_loss: 1524.4270\n",
      "Epoch 39/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 2070.8032 - val_loss: 1501.7612\n",
      "Epoch 40/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2025.6090 - val_loss: 1479.6047\n",
      "Epoch 41/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1981.1581 - val_loss: 1457.9515\n",
      "Epoch 42/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1937.4424 - val_loss: 1436.7949\n",
      "Epoch 43/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1894.4526 - val_loss: 1416.1292\n",
      "Epoch 44/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1852.1824 - val_loss: 1395.9480\n",
      "Epoch 45/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1810.6226 - val_loss: 1376.2454\n",
      "Epoch 46/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1769.7657 - val_loss: 1357.0160\n",
      "Epoch 47/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1729.6050 - val_loss: 1338.2528\n",
      "Epoch 48/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 1690.1315 - val_loss: 1319.9507\n",
      "Epoch 49/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1651.3376 - val_loss: 1302.1038\n",
      "Epoch 50/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1613.2166 - val_loss: 1284.7056\n",
      "Epoch 51/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 1575.7603 - val_loss: 1267.7513\n",
      "Epoch 52/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1538.9622 - val_loss: 1251.2349\n",
      "Epoch 53/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1502.8136 - val_loss: 1235.1503\n",
      "Epoch 54/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1467.3081 - val_loss: 1219.4918\n",
      "Epoch 55/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1432.4384 - val_loss: 1204.2544\n",
      "Epoch 56/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1398.1967 - val_loss: 1189.4321\n",
      "Epoch 57/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1364.5763 - val_loss: 1175.0197\n",
      "Epoch 58/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1331.5704 - val_loss: 1161.0112\n",
      "Epoch 59/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1299.1715 - val_loss: 1147.4014\n",
      "Epoch 60/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1267.3728 - val_loss: 1134.1849\n",
      "Epoch 61/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1236.1670 - val_loss: 1121.3558\n",
      "Epoch 62/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1205.5475 - val_loss: 1108.9094\n",
      "Epoch 63/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1175.5073 - val_loss: 1096.8398\n",
      "Epoch 64/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1146.0399 - val_loss: 1085.1421\n",
      "Epoch 65/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 1117.1384 - val_loss: 1073.8107\n",
      "Epoch 66/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 1088.7961 - val_loss: 1062.8403\n",
      "Epoch 67/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1061.0060 - val_loss: 1052.2260\n",
      "Epoch 68/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1033.7617 - val_loss: 1041.9618\n",
      "Epoch 69/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1007.0563 - val_loss: 1032.0432\n",
      "Epoch 70/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 980.8832 - val_loss: 1022.4644\n",
      "Epoch 71/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 955.2363 - val_loss: 1013.2207\n",
      "Epoch 72/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 930.1089 - val_loss: 1004.3070\n",
      "Epoch 73/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 905.4946 - val_loss: 995.7178\n",
      "Epoch 74/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 881.3868 - val_loss: 987.4481\n",
      "Epoch 75/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 857.7789 - val_loss: 979.4929\n",
      "Epoch 76/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 834.6650 - val_loss: 971.8470\n",
      "Epoch 77/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 812.0385 - val_loss: 964.5056\n",
      "Epoch 78/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 789.8932 - val_loss: 957.4634\n",
      "Epoch 79/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 768.2225 - val_loss: 950.7154\n",
      "Epoch 80/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 747.0210 - val_loss: 944.2569\n",
      "Epoch 81/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 726.2813 - val_loss: 938.0828\n",
      "Epoch 82/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 705.9984 - val_loss: 932.1879\n",
      "Epoch 83/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 686.1655 - val_loss: 926.5674\n",
      "Epoch 84/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 666.7762 - val_loss: 921.2164\n",
      "Epoch 85/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 647.8252 - val_loss: 916.1300\n",
      "Epoch 86/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 629.3059 - val_loss: 911.3033\n",
      "Epoch 87/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 611.2121 - val_loss: 906.7313\n",
      "Epoch 88/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 593.5380 - val_loss: 902.4094\n",
      "Epoch 89/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 576.2779 - val_loss: 898.3326\n",
      "Epoch 90/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 559.4252 - val_loss: 894.4960\n",
      "Epoch 91/500\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 542.9747 - val_loss: 890.8949\n",
      "Epoch 92/500\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 526.9200 - val_loss: 887.5245\n",
      "Epoch 93/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 511.2553 - val_loss: 884.3801\n",
      "Epoch 94/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 495.9746 - val_loss: 881.4567\n",
      "Epoch 95/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 481.0724 - val_loss: 878.7497\n",
      "Epoch 96/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 466.5425 - val_loss: 876.2543\n",
      "Epoch 97/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 452.3794 - val_loss: 873.9659\n",
      "Epoch 98/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 438.5773 - val_loss: 871.8798\n",
      "Epoch 99/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 425.1305 - val_loss: 869.9912\n",
      "Epoch 100/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 412.0329 - val_loss: 868.2957\n",
      "Epoch 101/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 399.2793 - val_loss: 866.7883\n",
      "Epoch 102/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 386.8638 - val_loss: 865.4645\n",
      "Epoch 103/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 374.7807 - val_loss: 864.3199\n",
      "Epoch 104/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 363.0244 - val_loss: 863.3497\n",
      "Epoch 105/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 351.5895 - val_loss: 862.5495\n",
      "Epoch 106/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 340.4702 - val_loss: 861.9146\n",
      "Epoch 107/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 329.6612 - val_loss: 861.4404\n",
      "Epoch 108/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 319.1568 - val_loss: 861.1227\n",
      "Epoch 109/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 308.9516 - val_loss: 860.9567\n",
      "Epoch 110/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 299.0399 - val_loss: 860.9382\n",
      "Epoch 111/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 289.4166 - val_loss: 861.0627\n",
      "Epoch 112/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 280.0760 - val_loss: 861.3257\n",
      "Epoch 113/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 271.0130 - val_loss: 861.7228\n",
      "Epoch 114/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 262.2223 - val_loss: 862.2498\n",
      "Epoch 115/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 253.6986 - val_loss: 862.9023\n",
      "Epoch 116/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 245.4363 - val_loss: 863.6760\n",
      "Epoch 117/500\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 237.4303 - val_loss: 864.5666\n",
      "Epoch 118/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 229.6752 - val_loss: 865.5698\n",
      "Epoch 119/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 222.1662 - val_loss: 866.6816\n",
      "Epoch 120/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 214.8978 - val_loss: 867.8975\n",
      "Epoch 121/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 207.8651 - val_loss: 869.2136\n",
      "Epoch 122/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 201.0630 - val_loss: 870.6257\n",
      "Epoch 123/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 194.4863 - val_loss: 872.1298\n",
      "Epoch 124/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 188.1302 - val_loss: 873.7216\n",
      "Epoch 125/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 181.9896 - val_loss: 875.3977\n",
      "Epoch 126/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 176.0591 - val_loss: 877.1534\n",
      "Epoch 127/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 170.3344 - val_loss: 878.9852\n",
      "Epoch 128/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 164.8108 - val_loss: 880.8892\n",
      "Epoch 129/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 159.4830 - val_loss: 882.8613\n",
      "Epoch 130/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 154.3463 - val_loss: 884.8980\n",
      "Epoch 131/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 149.3961 - val_loss: 886.9955\n",
      "Epoch 132/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 144.6277 - val_loss: 889.1501\n",
      "Epoch 133/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 140.0365 - val_loss: 891.3582\n",
      "Epoch 134/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 135.6176 - val_loss: 893.6158\n",
      "Epoch 135/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 131.3668 - val_loss: 895.9200\n",
      "Epoch 136/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 127.2797 - val_loss: 898.2667\n",
      "Epoch 137/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 123.3516 - val_loss: 900.6527\n",
      "Epoch 138/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 119.5782 - val_loss: 903.0748\n",
      "Epoch 139/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 115.9551 - val_loss: 905.5294\n",
      "Epoch 140/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 112.4782 - val_loss: 908.0135\n",
      "Epoch 141/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 109.1430 - val_loss: 910.5237\n",
      "Epoch 142/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 105.9454 - val_loss: 913.0568\n",
      "Epoch 143/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 102.8815 - val_loss: 915.6097\n",
      "Epoch 144/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 99.9471 - val_loss: 918.1796\n",
      "Epoch 145/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 97.1381 - val_loss: 920.7635\n",
      "Epoch 146/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 94.4507 - val_loss: 923.3582\n",
      "Epoch 147/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 91.8809 - val_loss: 925.9614\n",
      "Epoch 148/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 89.4250 - val_loss: 928.5698\n",
      "Epoch 149/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 87.0793 - val_loss: 931.1814\n",
      "Epoch 150/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 84.8397 - val_loss: 933.7931\n",
      "Epoch 151/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 82.7032 - val_loss: 936.4023\n",
      "Epoch 152/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 80.6658 - val_loss: 939.0067\n",
      "Epoch 153/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 78.7242 - val_loss: 941.6041\n",
      "Epoch 154/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 76.8750 - val_loss: 944.1920\n",
      "Epoch 155/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 75.1148 - val_loss: 946.7681\n",
      "Epoch 156/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 73.4402 - val_loss: 949.3304\n",
      "Epoch 157/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 71.8482 - val_loss: 951.8767\n",
      "Epoch 158/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 70.3356 - val_loss: 954.4048\n",
      "Epoch 159/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 68.8993 - val_loss: 956.9135\n",
      "Epoch 160/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 67.5362 - val_loss: 959.3999\n",
      "Epoch 161/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 66.2436 - val_loss: 961.8633\n",
      "Epoch 162/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 65.0184 - val_loss: 964.3017\n",
      "Epoch 163/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 63.8579 - val_loss: 966.7126\n",
      "Epoch 164/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 62.7596 - val_loss: 969.0956\n",
      "Epoch 165/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 61.7206 - val_loss: 971.4486\n",
      "Epoch 166/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 60.7384 - val_loss: 973.7703\n",
      "Epoch 167/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 59.8106 - val_loss: 976.0590\n",
      "Epoch 168/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 58.9346 - val_loss: 978.3134\n",
      "Epoch 169/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 58.1082 - val_loss: 980.5286\n",
      "Epoch 170/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 57.3291 - val_loss: 982.4707\n",
      "Epoch 171/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 56.5950 - val_loss: 984.9625\n",
      "Epoch 172/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 55.9039 - val_loss: 987.0765\n",
      "Epoch 173/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 55.2537 - val_loss: 989.1522\n",
      "Epoch 174/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 54.6424 - val_loss: 991.1887\n",
      "Epoch 175/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 54.0680 - val_loss: 993.1859\n",
      "Epoch 176/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 53.5286 - val_loss: 995.1432\n",
      "Epoch 177/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 53.0225 - val_loss: 997.0599\n",
      "Epoch 178/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 52.5480 - val_loss: 998.9346\n",
      "Epoch 179/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 52.1035 - val_loss: 1000.7682\n",
      "Epoch 180/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 51.6872 - val_loss: 1002.5599\n",
      "Epoch 181/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 51.2978 - val_loss: 1004.3097\n",
      "Epoch 182/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 50.9337 - val_loss: 1006.0173\n",
      "Epoch 183/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 50.5935 - val_loss: 1007.6823\n",
      "Epoch 184/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 50.2758 - val_loss: 1009.3054\n",
      "Epoch 185/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 49.9794 - val_loss: 1010.8859\n",
      "Epoch 186/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 49.7031 - val_loss: 1012.4245\n",
      "Epoch 187/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 49.4455 - val_loss: 1013.9209\n",
      "Epoch 188/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 49.2058 - val_loss: 1015.3755\n",
      "Epoch 189/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.9826 - val_loss: 1016.7881\n",
      "Epoch 190/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.7752 - val_loss: 1018.1593\n",
      "Epoch 191/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.5824 - val_loss: 1019.4896\n",
      "Epoch 192/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.4034 - val_loss: 1020.7793\n",
      "Epoch 193/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 48.2372 - val_loss: 1022.0283\n",
      "Epoch 194/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 48.0832 - val_loss: 1023.2379\n",
      "Epoch 195/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 47.9404 - val_loss: 1024.4081\n",
      "Epoch 196/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 47.8081 - val_loss: 1025.5399\n",
      "Epoch 197/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 47.6856 - val_loss: 1026.6332\n",
      "Epoch 198/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 47.5723 - val_loss: 1027.6891\n",
      "Epoch 199/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 47.4676 - val_loss: 1028.7074\n",
      "Epoch 200/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 47.3708 - val_loss: 1029.6899\n",
      "Epoch 201/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 47.2814 - val_loss: 1030.6372\n",
      "Epoch 202/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 47.1989 - val_loss: 1031.5487\n",
      "Epoch 203/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 47.1227 - val_loss: 1032.4258\n",
      "Epoch 204/500\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 47.0526 - val_loss: 1033.2698\n",
      "Epoch 205/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.9879 - val_loss: 1034.0808\n",
      "Epoch 206/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.9284 - val_loss: 1034.8599\n",
      "Epoch 207/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.8735 - val_loss: 1035.6078\n",
      "Epoch 208/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.8231 - val_loss: 1036.3250\n",
      "Epoch 209/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.7767 - val_loss: 1037.0128\n",
      "Epoch 210/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.7340 - val_loss: 1037.6714\n",
      "Epoch 211/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.6947 - val_loss: 1038.3022\n",
      "Epoch 212/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.6587 - val_loss: 1038.9054\n",
      "Epoch 213/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.6256 - val_loss: 1039.4823\n",
      "Epoch 214/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 46.5953 - val_loss: 1040.0336\n",
      "Epoch 215/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.5674 - val_loss: 1040.5598\n",
      "Epoch 216/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.5418 - val_loss: 1041.0621\n",
      "Epoch 217/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.5183 - val_loss: 1041.5410\n",
      "Epoch 218/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.4968 - val_loss: 1041.9971\n",
      "Epoch 219/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.4772 - val_loss: 1042.4320\n",
      "Epoch 220/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.4591 - val_loss: 1042.8456\n",
      "Epoch 221/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.4426 - val_loss: 1043.2388\n",
      "Epoch 222/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.4274 - val_loss: 1043.6128\n",
      "Epoch 223/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.4135 - val_loss: 1043.9674\n",
      "Epoch 224/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.4009 - val_loss: 1044.3041\n",
      "Epoch 225/500\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 46.3893 - val_loss: 1044.6239\n",
      "Epoch 226/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 46.3787 - val_loss: 1044.9270\n",
      "Epoch 227/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3690 - val_loss: 1045.2141\n",
      "Epoch 228/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3601 - val_loss: 1045.4851\n",
      "Epoch 229/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 46.3520 - val_loss: 1045.7421\n",
      "Epoch 230/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3447 - val_loss: 1045.9847\n",
      "Epoch 231/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3380 - val_loss: 1046.2146\n",
      "Epoch 232/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 46.3318 - val_loss: 1046.4313\n",
      "Epoch 233/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3263 - val_loss: 1046.6356\n",
      "Epoch 234/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3211 - val_loss: 1046.8279\n",
      "Epoch 235/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 46.3166 - val_loss: 1047.0096\n",
      "Epoch 236/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3124 - val_loss: 1047.1805\n",
      "Epoch 237/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3085 - val_loss: 1047.3413\n",
      "Epoch 238/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3051 - val_loss: 1047.4924\n",
      "Epoch 239/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3020 - val_loss: 1047.6339\n",
      "Epoch 240/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2992 - val_loss: 1047.7675\n",
      "Epoch 241/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2966 - val_loss: 1047.8922\n",
      "Epoch 242/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.2944 - val_loss: 1048.0096\n",
      "Epoch 243/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.2924 - val_loss: 1048.1195\n",
      "Epoch 244/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.2905 - val_loss: 1048.2220\n",
      "Epoch 245/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.2889 - val_loss: 1048.3182\n",
      "Epoch 246/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 46.2875 - val_loss: 1048.4081\n",
      "Epoch 247/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.2862 - val_loss: 1048.4916\n",
      "Epoch 248/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.2851 - val_loss: 1048.5695\n",
      "Epoch 249/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.2842 - val_loss: 1048.6425\n",
      "Epoch 250/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2835 - val_loss: 1048.7111\n",
      "Epoch 251/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.2827 - val_loss: 1048.7740\n",
      "Epoch 252/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.2821 - val_loss: 1048.8334\n",
      "Epoch 253/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.2816 - val_loss: 1048.8878\n",
      "Epoch 254/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.2813 - val_loss: 1048.9381\n",
      "Epoch 255/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.2811 - val_loss: 1048.9856\n",
      "Epoch 256/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2808 - val_loss: 1049.0291\n",
      "Epoch 257/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.2807 - val_loss: 1049.0699\n",
      "Epoch 258/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.2807 - val_loss: 1049.1075\n",
      "Epoch 259/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2807 - val_loss: 1049.1417\n",
      "Epoch 260/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.2808 - val_loss: 1049.1741\n",
      "Epoch 261/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2809 - val_loss: 1049.2037\n",
      "Epoch 262/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2811 - val_loss: 1049.2310\n",
      "Epoch 263/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2813 - val_loss: 1049.2561\n",
      "Epoch 264/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2816 - val_loss: 1049.2787\n",
      "Epoch 265/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 46.2819 - val_loss: 1049.2998\n",
      "Epoch 266/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 46.2822 - val_loss: 1049.3188\n",
      "Epoch 267/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.2826 - val_loss: 1049.3367\n",
      "Epoch 268/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2830 - val_loss: 1049.3534\n",
      "Epoch 269/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2834 - val_loss: 1049.3679\n",
      "Epoch 270/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2839 - val_loss: 1049.3815\n",
      "Epoch 271/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2844 - val_loss: 1049.3938\n",
      "Epoch 272/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.2849 - val_loss: 1049.4049\n",
      "Epoch 273/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2854 - val_loss: 1049.4149\n",
      "Epoch 274/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2859 - val_loss: 1049.4240\n",
      "Epoch 275/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2865 - val_loss: 1049.4321\n",
      "Epoch 276/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2870 - val_loss: 1049.4391\n",
      "Epoch 277/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2876 - val_loss: 1049.4456\n",
      "Epoch 278/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2882 - val_loss: 1049.4518\n",
      "Epoch 279/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2887 - val_loss: 1049.4568\n",
      "Epoch 280/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2893 - val_loss: 1049.4611\n",
      "Epoch 281/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2899 - val_loss: 1049.4651\n",
      "Epoch 282/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2906 - val_loss: 1049.4685\n",
      "Epoch 283/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2911 - val_loss: 1049.4716\n",
      "Epoch 284/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 46.2918 - val_loss: 1049.4741\n",
      "Epoch 285/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2924 - val_loss: 1049.4764\n",
      "Epoch 286/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2930 - val_loss: 1049.4788\n",
      "Epoch 287/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2936 - val_loss: 1049.4800\n",
      "Epoch 288/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2942 - val_loss: 1049.4814\n",
      "Epoch 289/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.2948 - val_loss: 1049.4822\n",
      "Epoch 290/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2954 - val_loss: 1049.4828\n",
      "Epoch 291/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2961 - val_loss: 1049.4840\n",
      "Epoch 292/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2967 - val_loss: 1049.4844\n",
      "Epoch 293/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2972 - val_loss: 1049.4844\n",
      "Epoch 294/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2978 - val_loss: 1049.4843\n",
      "Epoch 295/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2984 - val_loss: 1049.4839\n",
      "Epoch 296/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2990 - val_loss: 1049.4836\n",
      "Epoch 297/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2996 - val_loss: 1049.4834\n",
      "Epoch 298/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3001 - val_loss: 1049.4825\n",
      "Epoch 299/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3007 - val_loss: 1049.4813\n",
      "Epoch 300/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3013 - val_loss: 1049.4806\n",
      "Epoch 301/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 46.3019 - val_loss: 1049.4800\n",
      "Epoch 302/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3024 - val_loss: 1049.4794\n",
      "Epoch 303/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3030 - val_loss: 1049.4778\n",
      "Epoch 304/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3036 - val_loss: 1049.4769\n",
      "Epoch 305/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3040 - val_loss: 1049.4762\n",
      "Epoch 306/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3046 - val_loss: 1049.4747\n",
      "Epoch 307/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3050 - val_loss: 1049.4734\n",
      "Epoch 308/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3056 - val_loss: 1049.4724\n",
      "Epoch 309/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3061 - val_loss: 1049.4705\n",
      "Epoch 310/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3066 - val_loss: 1049.4696\n",
      "Epoch 311/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3071 - val_loss: 1049.4681\n",
      "Epoch 312/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3076 - val_loss: 1049.4664\n",
      "Epoch 313/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3080 - val_loss: 1049.4651\n",
      "Epoch 314/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3085 - val_loss: 1049.4634\n",
      "Epoch 315/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3090 - val_loss: 1049.4623\n",
      "Epoch 316/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3094 - val_loss: 1049.4602\n",
      "Epoch 317/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3099 - val_loss: 1049.4591\n",
      "Epoch 318/500\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 46.3104 - val_loss: 1049.4580\n",
      "Epoch 319/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 46.3108 - val_loss: 1049.4564\n",
      "Epoch 320/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3112 - val_loss: 1049.4553\n",
      "Epoch 321/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3117 - val_loss: 1049.4536\n",
      "Epoch 322/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3120 - val_loss: 1049.4525\n",
      "Epoch 323/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3124 - val_loss: 1049.4509\n",
      "Epoch 324/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3129 - val_loss: 1049.4497\n",
      "Epoch 325/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3133 - val_loss: 1049.4486\n",
      "Epoch 326/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3136 - val_loss: 1049.4469\n",
      "Epoch 327/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3140 - val_loss: 1049.4459\n",
      "Epoch 328/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3144 - val_loss: 1049.4440\n",
      "Epoch 329/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3148 - val_loss: 1049.4427\n",
      "Epoch 330/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3152 - val_loss: 1049.4421\n",
      "Epoch 331/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3155 - val_loss: 1049.4409\n",
      "Epoch 332/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3159 - val_loss: 1049.4397\n",
      "Epoch 333/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3162 - val_loss: 1049.4386\n",
      "Epoch 334/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 46.3165 - val_loss: 1049.4371\n",
      "Epoch 335/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3169 - val_loss: 1049.4364\n",
      "Epoch 336/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3172 - val_loss: 1049.4354\n",
      "Epoch 337/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 46.3174 - val_loss: 1049.4342\n",
      "Epoch 338/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3178 - val_loss: 1049.4329\n",
      "Epoch 339/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3181 - val_loss: 1049.4318\n",
      "Epoch 340/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3184 - val_loss: 1049.4309\n",
      "Epoch 341/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3187 - val_loss: 1049.4296\n",
      "Epoch 342/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3190 - val_loss: 1049.4285\n",
      "Epoch 343/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3192 - val_loss: 1049.4276\n",
      "Epoch 344/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3195 - val_loss: 1049.4265\n",
      "Epoch 345/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3198 - val_loss: 1049.4252\n",
      "Epoch 346/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3201 - val_loss: 1049.4248\n",
      "Epoch 347/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3203 - val_loss: 1049.4241\n",
      "Epoch 348/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3205 - val_loss: 1049.4230\n",
      "Epoch 349/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 46.3208 - val_loss: 1049.4216\n",
      "Epoch 350/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 46.3211 - val_loss: 1049.4209\n",
      "Epoch 351/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3213 - val_loss: 1049.4202\n",
      "Epoch 352/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3215 - val_loss: 1049.4188\n",
      "Epoch 353/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3218 - val_loss: 1049.4177\n",
      "Epoch 354/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3220 - val_loss: 1049.4172\n",
      "Epoch 355/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3222 - val_loss: 1049.4166\n",
      "Epoch 356/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3224 - val_loss: 1049.4153\n",
      "Epoch 357/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3227 - val_loss: 1049.4144\n",
      "Epoch 358/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3229 - val_loss: 1049.4137\n",
      "Epoch 359/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3231 - val_loss: 1049.4130\n",
      "Epoch 360/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3233 - val_loss: 1049.4124\n",
      "Epoch 361/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3234 - val_loss: 1049.4111\n",
      "Epoch 362/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3236 - val_loss: 1049.4104\n",
      "Epoch 363/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3238 - val_loss: 1049.4099\n",
      "Epoch 364/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3240 - val_loss: 1049.4093\n",
      "Epoch 365/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3242 - val_loss: 1049.4086\n",
      "Epoch 366/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3244 - val_loss: 1049.4081\n",
      "Epoch 367/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3245 - val_loss: 1049.4073\n",
      "Epoch 368/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3247 - val_loss: 1049.4064\n",
      "Epoch 369/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3248 - val_loss: 1049.4059\n",
      "Epoch 370/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 46.3250 - val_loss: 1049.4055\n",
      "Epoch 371/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3252 - val_loss: 1049.4048\n",
      "Epoch 372/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3254 - val_loss: 1049.4044\n",
      "Epoch 373/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3255 - val_loss: 1049.4039\n",
      "Epoch 374/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3256 - val_loss: 1049.4030\n",
      "Epoch 375/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3257 - val_loss: 1049.4019\n",
      "Epoch 376/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3260 - val_loss: 1049.4015\n",
      "Epoch 377/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3260 - val_loss: 1049.4009\n",
      "Epoch 378/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3261 - val_loss: 1049.4001\n",
      "Epoch 379/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3263 - val_loss: 1049.3995\n",
      "Epoch 380/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3264 - val_loss: 1049.3990\n",
      "Epoch 381/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3265 - val_loss: 1049.3978\n",
      "Epoch 382/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3267 - val_loss: 1049.3973\n",
      "Epoch 383/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3268 - val_loss: 1049.3966\n",
      "Epoch 384/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3270 - val_loss: 1049.3965\n",
      "Epoch 385/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3270 - val_loss: 1049.3958\n",
      "Epoch 386/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3271 - val_loss: 1049.3954\n",
      "Epoch 387/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3273 - val_loss: 1049.3950\n",
      "Epoch 388/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3273 - val_loss: 1049.3943\n",
      "Epoch 389/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3275 - val_loss: 1049.3934\n",
      "Epoch 390/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3276 - val_loss: 1049.3929\n",
      "Epoch 391/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3277 - val_loss: 1049.3925\n",
      "Epoch 392/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3278 - val_loss: 1049.3918\n",
      "Epoch 393/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 46.3279 - val_loss: 1049.3915\n",
      "Epoch 394/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3280 - val_loss: 1049.3911\n",
      "Epoch 395/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3281 - val_loss: 1049.3907\n",
      "Epoch 396/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3281 - val_loss: 1049.3901\n",
      "Epoch 397/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3282 - val_loss: 1049.3890\n",
      "Epoch 398/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3283 - val_loss: 1049.3888\n",
      "Epoch 399/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3284 - val_loss: 1049.3882\n",
      "Epoch 400/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3285 - val_loss: 1049.3876\n",
      "Epoch 401/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3286 - val_loss: 1049.3872\n",
      "Epoch 402/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3287 - val_loss: 1049.3870\n",
      "Epoch 403/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3287 - val_loss: 1049.3866\n",
      "Epoch 404/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3288 - val_loss: 1049.3861\n",
      "Epoch 405/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3289 - val_loss: 1049.3862\n",
      "Epoch 406/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3289 - val_loss: 1049.3859\n",
      "Epoch 407/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3290 - val_loss: 1049.3854\n",
      "Epoch 408/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3291 - val_loss: 1049.3850\n",
      "Epoch 409/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3291 - val_loss: 1049.3845\n",
      "Epoch 410/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3292 - val_loss: 1049.3842\n",
      "Epoch 411/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3293 - val_loss: 1049.3832\n",
      "Epoch 412/500\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 46.3294 - val_loss: 1049.3829\n",
      "Epoch 413/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3295 - val_loss: 1049.3824\n",
      "Epoch 414/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 46.3295 - val_loss: 1049.3817\n",
      "Epoch 415/500\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 46.3296 - val_loss: 1049.3815\n",
      "Epoch 416/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 46.3296 - val_loss: 1049.3810\n",
      "Epoch 417/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 46.3296 - val_loss: 1049.3806\n",
      "Epoch 418/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3297 - val_loss: 1049.3805\n",
      "Epoch 419/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3297 - val_loss: 1049.3804\n",
      "Epoch 420/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3297 - val_loss: 1049.3796\n",
      "Epoch 421/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3298 - val_loss: 1049.3795\n",
      "Epoch 422/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3299 - val_loss: 1049.3792\n",
      "Epoch 423/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3299 - val_loss: 1049.3788\n",
      "Epoch 424/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3300 - val_loss: 1049.3784\n",
      "Epoch 425/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3300 - val_loss: 1049.3783\n",
      "Epoch 426/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3301 - val_loss: 1049.3779\n",
      "Epoch 427/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3301 - val_loss: 1049.3774\n",
      "Epoch 428/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3302 - val_loss: 1049.3773\n",
      "Epoch 429/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3303 - val_loss: 1049.3770\n",
      "Epoch 430/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 46.3303 - val_loss: 1049.3767\n",
      "Epoch 431/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3303 - val_loss: 1049.3763\n",
      "Epoch 432/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3304 - val_loss: 1049.3766\n",
      "Epoch 433/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3304 - val_loss: 1049.3765\n",
      "Epoch 434/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3304 - val_loss: 1049.3759\n",
      "Epoch 435/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3304 - val_loss: 1049.3754\n",
      "Epoch 436/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 46.3305 - val_loss: 1049.3745\n",
      "Epoch 437/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3305 - val_loss: 1049.3737\n",
      "Epoch 438/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3306 - val_loss: 1049.3740\n",
      "Epoch 439/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3306 - val_loss: 1049.3740\n",
      "Epoch 440/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3306 - val_loss: 1049.3735\n",
      "Epoch 441/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3306 - val_loss: 1049.3727\n",
      "Epoch 442/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3307 - val_loss: 1049.3724\n",
      "Epoch 443/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3307 - val_loss: 1049.3719\n",
      "Epoch 444/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3307 - val_loss: 1049.3716\n",
      "Epoch 445/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3307 - val_loss: 1049.3711\n",
      "Epoch 446/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3307 - val_loss: 1049.3708\n",
      "Epoch 447/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3309 - val_loss: 1049.3706\n",
      "Epoch 448/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3308 - val_loss: 1049.3706\n",
      "Epoch 449/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3308 - val_loss: 1049.3702\n",
      "Epoch 450/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 46.3309 - val_loss: 1049.3699\n",
      "Epoch 451/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3309 - val_loss: 1049.3696\n",
      "Epoch 452/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3309 - val_loss: 1049.3690\n",
      "Epoch 453/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3309 - val_loss: 1049.3685\n",
      "Epoch 454/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3309 - val_loss: 1049.3683\n",
      "Epoch 455/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3310 - val_loss: 1049.3680\n",
      "Epoch 456/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3310 - val_loss: 1049.3677\n",
      "Epoch 457/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3310 - val_loss: 1049.3673\n",
      "Epoch 458/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3310 - val_loss: 1049.3671\n",
      "Epoch 459/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3310 - val_loss: 1049.3663\n",
      "Epoch 460/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3311 - val_loss: 1049.3656\n",
      "Epoch 461/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3311 - val_loss: 1049.3652\n",
      "Epoch 462/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3311 - val_loss: 1049.3651\n",
      "Epoch 463/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3311 - val_loss: 1049.3650\n",
      "Epoch 464/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3312 - val_loss: 1049.3638\n",
      "Epoch 465/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3312 - val_loss: 1049.3617\n",
      "Epoch 466/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3312 - val_loss: 1049.3524\n",
      "Epoch 467/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.2508 - val_loss: 1048.3083\n",
      "Epoch 468/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.4179 - val_loss: 1048.6013\n",
      "Epoch 469/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3824 - val_loss: 1048.7035\n",
      "Epoch 470/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3730 - val_loss: 1048.7819\n",
      "Epoch 471/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3676 - val_loss: 1048.8489\n",
      "Epoch 472/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 46.3637 - val_loss: 1048.9080\n",
      "Epoch 473/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3606 - val_loss: 1048.9596\n",
      "Epoch 474/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3580 - val_loss: 1049.0051\n",
      "Epoch 475/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3558 - val_loss: 1049.0450\n",
      "Epoch 476/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3540 - val_loss: 1049.0797\n",
      "Epoch 477/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3524 - val_loss: 1049.1110\n",
      "Epoch 478/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3511 - val_loss: 1049.1389\n",
      "Epoch 479/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3498 - val_loss: 1049.1633\n",
      "Epoch 480/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3488 - val_loss: 1049.1855\n",
      "Epoch 481/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3479 - val_loss: 1049.2050\n",
      "Epoch 482/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3471 - val_loss: 1049.2222\n",
      "Epoch 483/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3464 - val_loss: 1049.2380\n",
      "Epoch 484/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3457 - val_loss: 1049.2513\n",
      "Epoch 485/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3451 - val_loss: 1049.2632\n",
      "Epoch 486/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3446 - val_loss: 1049.2739\n",
      "Epoch 487/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3441 - val_loss: 1049.2837\n",
      "Epoch 488/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3437 - val_loss: 1049.2924\n",
      "Epoch 489/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3433 - val_loss: 1049.2998\n",
      "Epoch 490/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3429 - val_loss: 1049.3063\n",
      "Epoch 491/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 46.3426 - val_loss: 1049.3124\n",
      "Epoch 492/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 46.3423 - val_loss: 1049.3171\n",
      "Epoch 493/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3421 - val_loss: 1049.3220\n",
      "Epoch 494/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3419 - val_loss: 1049.3263\n",
      "Epoch 495/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3417 - val_loss: 1049.3300\n",
      "Epoch 496/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3415 - val_loss: 1049.3336\n",
      "Epoch 497/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3413 - val_loss: 1049.3369\n",
      "Epoch 498/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3411 - val_loss: 1049.3391\n",
      "Epoch 499/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3410 - val_loss: 1049.3418\n",
      "Epoch 500/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 46.3408 - val_loss: 1049.3441\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(85.7336, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0020, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0125, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3008, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 377ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.27400560e+01, 6.26746732e+01, 6.26326564e+01, 6.25906396e+01,\n",
       "        6.25486228e+01, 6.25066060e+01, 6.24645892e+01, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.32110770e-01, 1.07955158e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.36700280e+01, 6.35691877e+01, 6.34683473e+01,\n",
       "        6.33675070e+01, 6.32666667e+01, 6.31658263e+01, 6.30649860e+01,\n",
       "        6.29641457e+01, 6.28633053e+01, 6.27624650e+01, 6.26840103e+01,\n",
       "        6.26419935e+01, 6.25999767e+01, 6.25579599e+01, 6.25159430e+01,\n",
       "        6.24739262e+01, 6.24319094e+01, 6.23898926e+01, 6.23478758e+01,\n",
       "        6.23058590e+01, 6.22638422e+01, 6.22218254e+01, 6.21596172e+01,\n",
       "        6.20755836e+01, 6.19915499e+01, 6.19075163e+01, 6.18234827e+01,\n",
       "        1.45537212e-01, 0.00000000e+00, 1.78240418e-01, 1.13204098e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 8.11721530e-02, 6.26093137e+01,\n",
       "        6.25672969e+01, 6.25252801e+01, 6.24832633e+01, 6.24412465e+01,\n",
       "        6.23992297e+01, 6.23572129e+01, 6.23151961e+01, 6.22731793e+01,\n",
       "        6.22311625e+01, 6.21782913e+01, 6.20942577e+01, 6.20102241e+01,\n",
       "        6.19261905e+01, 6.18421569e+01, 6.75214636e+01, 0.00000000e+00,\n",
       "        6.82455532e+01, 6.69976541e+01, 6.61741013e+01, 6.75791920e-01,\n",
       "        6.65910014e+01, 6.58725140e+01, 6.51820252e+01, 0.00000000e+00,\n",
       "        2.49343430e-01, 4.87544937e+01, 9.12700900e-02, 5.44621940e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.20866510e-01, 0.00000000e+00,\n",
       "        5.15854721e+01, 0.00000000e+00, 3.64410549e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.71010250e-01,\n",
       "        8.65074992e-03, 9.17237252e-02, 0.00000000e+00, 2.52014875e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.96508467e-01, 1.60440609e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.81322843e-01, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([56.64371797, 56.63469973, 56.62568149, 56.61666325, 56.60764501,\n",
       "       56.59862677, 56.58960853, 56.58059028, 56.57157204, 56.5625538 ,\n",
       "       56.55353556, 56.54451732, 56.53549908, 56.52648084, 56.51746259,\n",
       "       56.50844435, 56.49942611, 56.49040787, 56.48138963, 56.47237139,\n",
       "       56.46335315, 56.4543349 , 56.44531666, 56.43629842, 56.42728018,\n",
       "       56.41826194, 56.4092437 , 56.40022546, 56.39120721, 56.38218897,\n",
       "       56.37317073, 56.36415249, 56.35513425, 56.34611601, 56.33709777,\n",
       "       56.32807952, 56.31906128, 56.31004304, 56.3010248 , 56.29200656,\n",
       "       56.28298832, 56.27397008, 56.26495183, 56.25593359, 56.24691535,\n",
       "       56.23789711, 56.22887887, 56.21986063, 56.21084239, 56.20182414,\n",
       "       56.1928059 , 56.18378766, 56.17476942, 56.16575118, 56.15673294,\n",
       "       56.1477147 , 56.13869645, 56.12967821, 56.12065997, 56.11164173,\n",
       "       56.10262349, 56.09360525, 56.08458701, 56.07556876, 56.06655052,\n",
       "       56.05753228, 56.04851404, 56.0394958 , 56.03047756, 56.02145932,\n",
       "       56.01244107, 56.00342283, 55.99440459, 55.98538635, 55.97636811,\n",
       "       55.96734987, 55.95833163, 55.94931338, 55.94029514, 55.9312769 ,\n",
       "       55.92225866, 55.91324042, 55.90422218, 55.89520394, 55.88618569,\n",
       "       55.87716745, 55.86814921, 55.85913097, 55.85011273, 55.84109449,\n",
       "       55.83207625, 55.823058  , 55.81403976, 55.80502152, 55.79600328,\n",
       "       55.78698504, 55.7779668 , 55.76894856, 55.75993031, 55.75091207])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.59647197593472\n",
      "27.354082366513843\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
