{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "1945    59.565838\n",
       "1946    59.560703\n",
       "1947    59.555567\n",
       "1948    59.550432\n",
       "1949    59.545296\n",
       "Name: C8, Length: 1950, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c8_interpolated_1850_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "1845     0.000000\n",
       "1846     0.000000\n",
       "1847     0.000000\n",
       "1848     0.000000\n",
       "1849     0.000000\n",
       "Name: C8, Length: 1850, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1850)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.987115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.774230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.561345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.348459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.200000  0.000298\n",
       "1     87.987115  0.000298\n",
       "2     87.774230  0.000297\n",
       "3     87.561345  0.000297\n",
       "4     87.348459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 85.7336  # Value for C0\n",
    "K0 = -0.0020  # Value for K0\n",
    "K1 = 0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0125    # Value for b\n",
    "c = 2.3008    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.200000    87.987115    87.774230    87.561345    87.348459   \n",
      "351    87.987115    87.774230    87.561345    87.348459    87.135574   \n",
      "352    87.774230    87.561345    87.348459    87.135574    86.922689   \n",
      "353    87.561345    87.348459    87.135574    86.922689    86.709804   \n",
      "354    87.348459    87.135574    86.922689    86.709804    86.496919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.135574    86.922689    86.709804    86.496919    86.294538  ...   \n",
      "351    86.922689    86.709804    86.496919    86.294538    86.221709  ...   \n",
      "352    86.709804    86.496919    86.294538    86.221709    86.148880  ...   \n",
      "353    86.496919    86.294538    86.221709    86.148880    86.076050  ...   \n",
      "354    86.294538    86.221709    86.148880    86.076050    86.003221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   76.094351    0.000263   76.058870    0.000263   76.023389    0.000263   \n",
      "351   76.058870    0.000263   76.023389    0.000263   75.987909    0.000262   \n",
      "352   76.023389    0.000263   75.987909    0.000262   75.952428    0.000262   \n",
      "353   75.987909    0.000262   75.952428    0.000262   75.916947    0.000262   \n",
      "354   75.952428    0.000262   75.916947    0.000262   75.881466    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   75.987909    0.000262   75.952428    0.000262  \n",
      "351   75.952428    0.000262   75.916947    0.000262  \n",
      "352   75.916947    0.000262   75.881466    0.000262  \n",
      "353   75.881466    0.000262   75.845985    0.000262  \n",
      "354   75.845985    0.000262   75.810504    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 1, 251) (1400, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "18/18 [==============================] - 2s 34ms/step - loss: 4848.4204 - val_loss: 3856.0813\n",
      "Epoch 2/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4782.9736 - val_loss: 3815.4004\n",
      "Epoch 3/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4737.5459 - val_loss: 3774.6621\n",
      "Epoch 4/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4692.2378 - val_loss: 3734.0447\n",
      "Epoch 5/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 4644.9243 - val_loss: 3690.2468\n",
      "Epoch 6/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 4596.8672 - val_loss: 3647.4827\n",
      "Epoch 7/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4549.4434 - val_loss: 3605.3977\n",
      "Epoch 8/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4502.6992 - val_loss: 3563.8831\n",
      "Epoch 9/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4456.5166 - val_loss: 3522.8528\n",
      "Epoch 10/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4410.8208 - val_loss: 3482.2566\n",
      "Epoch 11/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4365.5674 - val_loss: 3442.0637\n",
      "Epoch 12/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4320.7266 - val_loss: 3402.2537\n",
      "Epoch 13/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4276.2788 - val_loss: 3362.8127\n",
      "Epoch 14/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4232.2119 - val_loss: 3323.7302\n",
      "Epoch 15/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4184.7798 - val_loss: 3269.5259\n",
      "Epoch 16/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4125.4033 - val_loss: 3226.3586\n",
      "Epoch 17/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4077.3335 - val_loss: 3173.0344\n",
      "Epoch 18/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 4015.7839 - val_loss: 3127.4128\n",
      "Epoch 19/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3964.6340 - val_loss: 3082.7554\n",
      "Epoch 20/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3915.0562 - val_loss: 3039.5554\n",
      "Epoch 21/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3866.9023 - val_loss: 2997.4644\n",
      "Epoch 22/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3819.8347 - val_loss: 2956.2495\n",
      "Epoch 23/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 3773.6445 - val_loss: 2915.7722\n",
      "Epoch 24/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3728.2019 - val_loss: 2875.9414\n",
      "Epoch 25/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3683.4226 - val_loss: 2836.6956\n",
      "Epoch 26/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3639.2458 - val_loss: 2797.9900\n",
      "Epoch 27/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3595.6279 - val_loss: 2759.7910\n",
      "Epoch 28/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3552.5361 - val_loss: 2722.0723\n",
      "Epoch 29/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3509.9429 - val_loss: 2684.8115\n",
      "Epoch 30/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3467.8259 - val_loss: 2647.9929\n",
      "Epoch 31/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3426.1682 - val_loss: 2611.6003\n",
      "Epoch 32/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3384.9548 - val_loss: 2575.6216\n",
      "Epoch 33/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3344.1724 - val_loss: 2540.0457\n",
      "Epoch 34/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3303.8098 - val_loss: 2504.8638\n",
      "Epoch 35/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3263.8574 - val_loss: 2470.0674\n",
      "Epoch 36/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3224.3059 - val_loss: 2435.6477\n",
      "Epoch 37/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3185.1484 - val_loss: 2401.6001\n",
      "Epoch 38/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3146.3765 - val_loss: 2367.9158\n",
      "Epoch 39/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3107.9856 - val_loss: 2334.5911\n",
      "Epoch 40/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3069.9683 - val_loss: 2301.6199\n",
      "Epoch 41/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 3032.3193 - val_loss: 2268.9978\n",
      "Epoch 42/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2995.0342 - val_loss: 2236.7192\n",
      "Epoch 43/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2958.1084 - val_loss: 2204.7805\n",
      "Epoch 44/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2921.5364 - val_loss: 2173.1785\n",
      "Epoch 45/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2885.3135 - val_loss: 2141.9075\n",
      "Epoch 46/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2849.4377 - val_loss: 2110.9653\n",
      "Epoch 47/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2813.9041 - val_loss: 2080.3477\n",
      "Epoch 48/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2778.7087 - val_loss: 2050.0518\n",
      "Epoch 49/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2743.8491 - val_loss: 2020.0743\n",
      "Epoch 50/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2709.3210 - val_loss: 1990.4116\n",
      "Epoch 51/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 2675.1213 - val_loss: 1961.0614\n",
      "Epoch 52/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2641.2483 - val_loss: 1932.0199\n",
      "Epoch 53/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2607.6973 - val_loss: 1903.2855\n",
      "Epoch 54/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2574.4656 - val_loss: 1874.8545\n",
      "Epoch 55/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2541.5513 - val_loss: 1846.7249\n",
      "Epoch 56/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2508.9517 - val_loss: 1818.8938\n",
      "Epoch 57/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2476.6628 - val_loss: 1791.3588\n",
      "Epoch 58/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2444.6836 - val_loss: 1764.1172\n",
      "Epoch 59/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2413.0105 - val_loss: 1737.1665\n",
      "Epoch 60/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2381.6418 - val_loss: 1710.5054\n",
      "Epoch 61/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2350.5745 - val_loss: 1684.1301\n",
      "Epoch 62/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2319.8066 - val_loss: 1658.0402\n",
      "Epoch 63/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2289.3362 - val_loss: 1632.2312\n",
      "Epoch 64/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2259.1604 - val_loss: 1606.7025\n",
      "Epoch 65/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2229.2766 - val_loss: 1581.4515\n",
      "Epoch 66/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2199.6836 - val_loss: 1556.4760\n",
      "Epoch 67/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2170.3784 - val_loss: 1531.7745\n",
      "Epoch 68/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2141.3596 - val_loss: 1507.3441\n",
      "Epoch 69/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2112.6250 - val_loss: 1483.1833\n",
      "Epoch 70/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2084.1721 - val_loss: 1459.2899\n",
      "Epoch 71/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2055.9993 - val_loss: 1435.6627\n",
      "Epoch 72/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2028.1049 - val_loss: 1412.2988\n",
      "Epoch 73/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2000.4862 - val_loss: 1389.1960\n",
      "Epoch 74/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1973.1417 - val_loss: 1366.3545\n",
      "Epoch 75/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1946.0695 - val_loss: 1343.7689\n",
      "Epoch 76/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1919.2667 - val_loss: 1321.4406\n",
      "Epoch 77/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1892.7330 - val_loss: 1299.3661\n",
      "Epoch 78/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1866.4652 - val_loss: 1277.5438\n",
      "Epoch 79/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1840.4625 - val_loss: 1255.9722\n",
      "Epoch 80/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1814.7228 - val_loss: 1234.6494\n",
      "Epoch 81/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1789.2440 - val_loss: 1213.5741\n",
      "Epoch 82/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1764.0244 - val_loss: 1192.7430\n",
      "Epoch 83/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1739.0624 - val_loss: 1172.1567\n",
      "Epoch 84/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1714.3563 - val_loss: 1151.8113\n",
      "Epoch 85/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1689.9047 - val_loss: 1131.7067\n",
      "Epoch 86/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1665.7051 - val_loss: 1111.8409\n",
      "Epoch 87/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1641.7565 - val_loss: 1092.2115\n",
      "Epoch 88/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1618.0566 - val_loss: 1072.8171\n",
      "Epoch 89/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1594.6045 - val_loss: 1053.6556\n",
      "Epoch 90/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1571.3976 - val_loss: 1034.7268\n",
      "Epoch 91/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1548.4347 - val_loss: 1016.0276\n",
      "Epoch 92/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1525.7146 - val_loss: 997.5573\n",
      "Epoch 93/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1503.2349 - val_loss: 979.3138\n",
      "Epoch 94/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1480.9948 - val_loss: 961.2955\n",
      "Epoch 95/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1458.9919 - val_loss: 943.5012\n",
      "Epoch 96/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1437.2252 - val_loss: 925.9291\n",
      "Epoch 97/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1415.6932 - val_loss: 908.5773\n",
      "Epoch 98/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1394.3940 - val_loss: 891.4448\n",
      "Epoch 99/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1373.3260 - val_loss: 874.5300\n",
      "Epoch 100/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1352.4877 - val_loss: 857.8311\n",
      "Epoch 101/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1331.8781 - val_loss: 841.3466\n",
      "Epoch 102/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1311.4950 - val_loss: 825.0747\n",
      "Epoch 103/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1291.3369 - val_loss: 809.0141\n",
      "Epoch 104/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1271.4028 - val_loss: 793.1638\n",
      "Epoch 105/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1251.6909 - val_loss: 777.5221\n",
      "Epoch 106/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1232.1998 - val_loss: 762.0865\n",
      "Epoch 107/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1212.9277 - val_loss: 746.8560\n",
      "Epoch 108/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1193.8735 - val_loss: 731.8301\n",
      "Epoch 109/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1175.0355 - val_loss: 717.0070\n",
      "Epoch 110/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1156.4125 - val_loss: 702.3842\n",
      "Epoch 111/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1138.0029 - val_loss: 687.9611\n",
      "Epoch 112/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1119.8052 - val_loss: 673.7361\n",
      "Epoch 113/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1101.8181 - val_loss: 659.7072\n",
      "Epoch 114/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1084.0399 - val_loss: 645.8740\n",
      "Epoch 115/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1066.4697 - val_loss: 632.2338\n",
      "Epoch 116/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1049.1052 - val_loss: 618.7862\n",
      "Epoch 117/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1031.9459 - val_loss: 605.5297\n",
      "Epoch 118/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1014.9903 - val_loss: 592.4622\n",
      "Epoch 119/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 998.2360 - val_loss: 579.5825\n",
      "Epoch 120/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 981.6828 - val_loss: 566.8896\n",
      "Epoch 121/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 965.3287 - val_loss: 554.3817\n",
      "Epoch 122/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 949.1728 - val_loss: 542.0580\n",
      "Epoch 123/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 933.2129 - val_loss: 529.9158\n",
      "Epoch 124/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 917.4482 - val_loss: 517.9551\n",
      "Epoch 125/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 901.8773 - val_loss: 506.1738\n",
      "Epoch 126/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 886.4990 - val_loss: 494.5707\n",
      "Epoch 127/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 871.3115 - val_loss: 483.1441\n",
      "Epoch 128/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 856.3134 - val_loss: 471.8928\n",
      "Epoch 129/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 841.5039 - val_loss: 460.8154\n",
      "Epoch 130/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 826.8812 - val_loss: 449.9106\n",
      "Epoch 131/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 812.4440 - val_loss: 439.1773\n",
      "Epoch 132/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 798.1911 - val_loss: 428.6137\n",
      "Epoch 133/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 784.1213 - val_loss: 418.2184\n",
      "Epoch 134/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 770.2328 - val_loss: 407.9903\n",
      "Epoch 135/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 756.5249 - val_loss: 397.9278\n",
      "Epoch 136/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 742.9956 - val_loss: 388.0299\n",
      "Epoch 137/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 729.6442 - val_loss: 378.2951\n",
      "Epoch 138/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 716.4689 - val_loss: 368.7211\n",
      "Epoch 139/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 703.4686 - val_loss: 359.3082\n",
      "Epoch 140/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 690.6420 - val_loss: 350.0542\n",
      "Epoch 141/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 677.9878 - val_loss: 340.9576\n",
      "Epoch 142/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 665.5048 - val_loss: 332.0172\n",
      "Epoch 143/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 653.1912 - val_loss: 323.2318\n",
      "Epoch 144/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 641.0459 - val_loss: 314.5995\n",
      "Epoch 145/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 629.0680 - val_loss: 306.1199\n",
      "Epoch 146/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 617.2559 - val_loss: 297.7906\n",
      "Epoch 147/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 605.6083 - val_loss: 289.6111\n",
      "Epoch 148/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 594.1237 - val_loss: 281.5800\n",
      "Epoch 149/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 582.8014 - val_loss: 273.6956\n",
      "Epoch 150/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 571.6395 - val_loss: 265.9564\n",
      "Epoch 151/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 560.6370 - val_loss: 258.3617\n",
      "Epoch 152/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 549.7928 - val_loss: 250.9098\n",
      "Epoch 153/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 539.1052 - val_loss: 243.5991\n",
      "Epoch 154/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 528.5734 - val_loss: 236.4284\n",
      "Epoch 155/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 518.1954 - val_loss: 229.3969\n",
      "Epoch 156/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 507.9709 - val_loss: 222.5030\n",
      "Epoch 157/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 497.8977 - val_loss: 215.7450\n",
      "Epoch 158/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 487.9747 - val_loss: 209.1218\n",
      "Epoch 159/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 478.2010 - val_loss: 202.6324\n",
      "Epoch 160/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 468.5755 - val_loss: 196.2748\n",
      "Epoch 161/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 459.0964 - val_loss: 190.0484\n",
      "Epoch 162/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 449.7626 - val_loss: 183.9512\n",
      "Epoch 163/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 440.5726 - val_loss: 177.9821\n",
      "Epoch 164/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 431.5255 - val_loss: 172.1404\n",
      "Epoch 165/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 422.6203 - val_loss: 166.4241\n",
      "Epoch 166/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 413.8552 - val_loss: 160.8320\n",
      "Epoch 167/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 405.2292 - val_loss: 155.3630\n",
      "Epoch 168/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 396.7409 - val_loss: 150.0153\n",
      "Epoch 169/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 388.3889 - val_loss: 144.7880\n",
      "Epoch 170/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 380.1721 - val_loss: 139.6796\n",
      "Epoch 171/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 372.0897 - val_loss: 134.6889\n",
      "Epoch 172/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 364.1396 - val_loss: 129.8144\n",
      "Epoch 173/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 356.3211 - val_loss: 125.0553\n",
      "Epoch 174/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 348.6328 - val_loss: 120.4094\n",
      "Epoch 175/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 341.0736 - val_loss: 115.8765\n",
      "Epoch 176/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 333.6420 - val_loss: 111.4542\n",
      "Epoch 177/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 326.3368 - val_loss: 107.1416\n",
      "Epoch 178/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 319.1567 - val_loss: 102.9377\n",
      "Epoch 179/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 312.1007 - val_loss: 98.8409\n",
      "Epoch 180/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 305.1675 - val_loss: 94.8499\n",
      "Epoch 181/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 298.3557 - val_loss: 90.9634\n",
      "Epoch 182/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 291.6642 - val_loss: 87.1801\n",
      "Epoch 183/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 285.0915 - val_loss: 83.4984\n",
      "Epoch 184/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 278.6367 - val_loss: 79.9178\n",
      "Epoch 185/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 272.2984 - val_loss: 76.4363\n",
      "Epoch 186/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 266.0753 - val_loss: 73.0526\n",
      "Epoch 187/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 259.9661 - val_loss: 69.7657\n",
      "Epoch 188/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 253.9698 - val_loss: 66.5740\n",
      "Epoch 189/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 248.0851 - val_loss: 63.4766\n",
      "Epoch 190/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 242.3106 - val_loss: 60.4718\n",
      "Epoch 191/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 236.6453 - val_loss: 57.5584\n",
      "Epoch 192/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 231.0875 - val_loss: 54.7351\n",
      "Epoch 193/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 225.6364 - val_loss: 52.0009\n",
      "Epoch 194/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 220.2908 - val_loss: 49.3539\n",
      "Epoch 195/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 215.0490 - val_loss: 46.7933\n",
      "Epoch 196/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 209.9103 - val_loss: 44.3179\n",
      "Epoch 197/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 204.8734 - val_loss: 41.9260\n",
      "Epoch 198/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 199.9370 - val_loss: 39.6166\n",
      "Epoch 199/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 195.1000 - val_loss: 37.3882\n",
      "Epoch 200/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 190.3608 - val_loss: 35.2397\n",
      "Epoch 201/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 185.7184 - val_loss: 33.1697\n",
      "Epoch 202/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 181.1716 - val_loss: 31.1768\n",
      "Epoch 203/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 176.7192 - val_loss: 29.2601\n",
      "Epoch 204/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 172.3601 - val_loss: 27.4182\n",
      "Epoch 205/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 168.0930 - val_loss: 25.6496\n",
      "Epoch 206/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 163.9167 - val_loss: 23.9532\n",
      "Epoch 207/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 159.8297 - val_loss: 22.3276\n",
      "Epoch 208/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 155.8312 - val_loss: 20.7718\n",
      "Epoch 209/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 151.9198 - val_loss: 19.2844\n",
      "Epoch 210/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 147.6540 - val_loss: 17.3015\n",
      "Epoch 211/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 142.2335 - val_loss: 15.3944\n",
      "Epoch 212/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 137.1330 - val_loss: 13.7213\n",
      "Epoch 213/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 132.4566 - val_loss: 12.2290\n",
      "Epoch 214/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 128.0947 - val_loss: 10.8805\n",
      "Epoch 215/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 123.9771 - val_loss: 9.6535\n",
      "Epoch 216/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 120.0612 - val_loss: 8.5335\n",
      "Epoch 217/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 116.3197 - val_loss: 7.5102\n",
      "Epoch 218/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 112.7337 - val_loss: 6.5759\n",
      "Epoch 219/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.2891 - val_loss: 5.7241\n",
      "Epoch 220/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 105.9752 - val_loss: 4.9498\n",
      "Epoch 221/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 102.7828 - val_loss: 4.2485\n",
      "Epoch 222/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.7048 - val_loss: 3.6163\n",
      "Epoch 223/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 96.7349 - val_loss: 3.0498\n",
      "Epoch 224/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 93.8678 - val_loss: 2.5458\n",
      "Epoch 225/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 91.0990 - val_loss: 2.1015\n",
      "Epoch 226/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 88.4241 - val_loss: 1.7143\n",
      "Epoch 227/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 85.8395 - val_loss: 1.3816\n",
      "Epoch 228/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 83.3414 - val_loss: 1.1011\n",
      "Epoch 229/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 80.9268 - val_loss: 0.8707\n",
      "Epoch 230/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 78.5928 - val_loss: 0.6883\n",
      "Epoch 231/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 76.3367 - val_loss: 0.5518\n",
      "Epoch 232/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 74.1557 - val_loss: 0.4593\n",
      "Epoch 233/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 72.0476 - val_loss: 0.4091\n",
      "Epoch 234/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 70.0099 - val_loss: 0.3993\n",
      "Epoch 235/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 68.0404 - val_loss: 0.4283\n",
      "Epoch 236/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 66.1370 - val_loss: 0.4944\n",
      "Epoch 237/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 64.2977 - val_loss: 0.5960\n",
      "Epoch 238/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 62.5206 - val_loss: 0.7317\n",
      "Epoch 239/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 60.8038 - val_loss: 0.8998\n",
      "Epoch 240/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 59.1455 - val_loss: 1.0990\n",
      "Epoch 241/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 57.5441 - val_loss: 1.3278\n",
      "Epoch 242/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 55.9979 - val_loss: 1.5848\n",
      "Epoch 243/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 54.5052 - val_loss: 1.8688\n",
      "Epoch 244/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 53.0646 - val_loss: 2.1784\n",
      "Epoch 245/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 51.6744 - val_loss: 2.5124\n",
      "Epoch 246/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 50.3330 - val_loss: 2.8695\n",
      "Epoch 247/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 49.0392 - val_loss: 3.2485\n",
      "Epoch 248/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 47.7917 - val_loss: 3.6482\n",
      "Epoch 249/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 46.5888 - val_loss: 4.0676\n",
      "Epoch 250/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 45.4295 - val_loss: 4.5054\n",
      "Epoch 251/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 44.3123 - val_loss: 4.9606\n",
      "Epoch 252/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 43.2361 - val_loss: 5.4321\n",
      "Epoch 253/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 42.1996 - val_loss: 5.9189\n",
      "Epoch 254/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 41.2016 - val_loss: 6.4201\n",
      "Epoch 255/500\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 40.2408 - val_loss: 6.9346\n",
      "Epoch 256/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 39.3163 - val_loss: 7.4614\n",
      "Epoch 257/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 38.4268 - val_loss: 7.9997\n",
      "Epoch 258/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 37.5714 - val_loss: 8.5485\n",
      "Epoch 259/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 36.7489 - val_loss: 9.1068\n",
      "Epoch 260/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 35.9584 - val_loss: 9.6741\n",
      "Epoch 261/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 35.1986 - val_loss: 10.2493\n",
      "Epoch 262/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 34.4689 - val_loss: 10.8317\n",
      "Epoch 263/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 33.7681 - val_loss: 11.4203\n",
      "Epoch 264/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 33.0952 - val_loss: 12.0146\n",
      "Epoch 265/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 32.4496 - val_loss: 12.6138\n",
      "Epoch 266/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 31.8300 - val_loss: 13.2171\n",
      "Epoch 267/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 31.2359 - val_loss: 13.8239\n",
      "Epoch 268/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 30.6662 - val_loss: 14.4336\n",
      "Epoch 269/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 30.1202 - val_loss: 15.0453\n",
      "Epoch 270/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 29.5970 - val_loss: 15.6586\n",
      "Epoch 271/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 29.0959 - val_loss: 16.2727\n",
      "Epoch 272/500\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 28.6161 - val_loss: 16.8871\n",
      "Epoch 273/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 28.1568 - val_loss: 17.5013\n",
      "Epoch 274/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 27.7174 - val_loss: 18.1148\n",
      "Epoch 275/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 27.2970 - val_loss: 18.7269\n",
      "Epoch 276/500\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 26.8950 - val_loss: 19.3372\n",
      "Epoch 277/500\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 26.5108 - val_loss: 19.9453\n",
      "Epoch 278/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 26.1437 - val_loss: 20.5506\n",
      "Epoch 279/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 25.7931 - val_loss: 21.1527\n",
      "Epoch 280/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 25.4582 - val_loss: 21.7513\n",
      "Epoch 281/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 25.1386 - val_loss: 22.3458\n",
      "Epoch 282/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 24.8336 - val_loss: 22.9360\n",
      "Epoch 283/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 24.5427 - val_loss: 23.5213\n",
      "Epoch 284/500\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 24.2654 - val_loss: 24.1016\n",
      "Epoch 285/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 24.0010 - val_loss: 24.6765\n",
      "Epoch 286/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 23.7491 - val_loss: 25.2456\n",
      "Epoch 287/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 23.5092 - val_loss: 25.8087\n",
      "Epoch 288/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 23.2808 - val_loss: 26.3654\n",
      "Epoch 289/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 23.0633 - val_loss: 26.9156\n",
      "Epoch 290/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 22.8565 - val_loss: 27.4590\n",
      "Epoch 291/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 22.6598 - val_loss: 27.9954\n",
      "Epoch 292/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 22.4728 - val_loss: 28.5245\n",
      "Epoch 293/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 22.2950 - val_loss: 29.0462\n",
      "Epoch 294/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 22.1261 - val_loss: 29.5602\n",
      "Epoch 295/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 21.9657 - val_loss: 30.0667\n",
      "Epoch 296/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 21.8134 - val_loss: 30.5650\n",
      "Epoch 297/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 21.6689 - val_loss: 31.0554\n",
      "Epoch 298/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 21.5318 - val_loss: 31.5377\n",
      "Epoch 299/500\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 21.4017 - val_loss: 32.0116\n",
      "Epoch 300/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 21.2785 - val_loss: 32.4771\n",
      "Epoch 301/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 21.1616 - val_loss: 32.9344\n",
      "Epoch 302/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 21.0508 - val_loss: 33.3831\n",
      "Epoch 303/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 20.9460 - val_loss: 33.8231\n",
      "Epoch 304/500\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 20.8467 - val_loss: 34.2544\n",
      "Epoch 305/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 20.7527 - val_loss: 34.6770\n",
      "Epoch 306/500\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 20.6638 - val_loss: 35.0910\n",
      "Epoch 307/500\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 20.5798 - val_loss: 35.4964\n",
      "Epoch 308/500\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 20.5003 - val_loss: 35.8931\n",
      "Epoch 309/500\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 20.4251 - val_loss: 36.2810\n",
      "Epoch 310/500\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 20.3541 - val_loss: 36.6602\n",
      "Epoch 311/500\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 20.2870 - val_loss: 37.0307\n",
      "Epoch 312/500\n",
      "18/18 [==============================] - 0s 19ms/step - loss: 20.2237 - val_loss: 37.3925\n",
      "Epoch 313/500\n",
      "18/18 [==============================] - 0s 17ms/step - loss: 20.1639 - val_loss: 37.7457\n",
      "Epoch 314/500\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 20.1075 - val_loss: 38.0907\n",
      "Epoch 315/500\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 20.0542 - val_loss: 38.4270\n",
      "Epoch 316/500\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 20.0039 - val_loss: 38.7550\n",
      "Epoch 317/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 19.9566 - val_loss: 39.0746\n",
      "Epoch 318/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 19.9119 - val_loss: 39.3857\n",
      "Epoch 319/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 19.8698 - val_loss: 39.6889\n",
      "Epoch 320/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 19.8301 - val_loss: 39.9839\n",
      "Epoch 321/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.7928 - val_loss: 40.2711\n",
      "Epoch 322/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 19.7575 - val_loss: 40.5504\n",
      "Epoch 323/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.7244 - val_loss: 40.8215\n",
      "Epoch 324/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 19.6932 - val_loss: 41.0851\n",
      "Epoch 325/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.6638 - val_loss: 41.3413\n",
      "Epoch 326/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.6361 - val_loss: 41.5898\n",
      "Epoch 327/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 19.6101 - val_loss: 41.8309\n",
      "Epoch 328/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.5857 - val_loss: 42.0650\n",
      "Epoch 329/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.5627 - val_loss: 42.2916\n",
      "Epoch 330/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 19.5410 - val_loss: 42.5116\n",
      "Epoch 331/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.5207 - val_loss: 42.7245\n",
      "Epoch 332/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.5016 - val_loss: 42.9307\n",
      "Epoch 333/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.4836 - val_loss: 43.1302\n",
      "Epoch 334/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.4667 - val_loss: 43.3235\n",
      "Epoch 335/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 19.4508 - val_loss: 43.5102\n",
      "Epoch 336/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 19.4359 - val_loss: 43.6906\n",
      "Epoch 337/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 19.4219 - val_loss: 43.8651\n",
      "Epoch 338/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.4088 - val_loss: 44.0336\n",
      "Epoch 339/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 19.3964 - val_loss: 44.1965\n",
      "Epoch 340/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.3848 - val_loss: 44.3535\n",
      "Epoch 341/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 19.3739 - val_loss: 44.5051\n",
      "Epoch 342/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.3637 - val_loss: 44.6513\n",
      "Epoch 343/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 19.3541 - val_loss: 44.7922\n",
      "Epoch 344/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.3451 - val_loss: 44.9280\n",
      "Epoch 345/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.3366 - val_loss: 45.0589\n",
      "Epoch 346/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.3286 - val_loss: 45.1846\n",
      "Epoch 347/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.3212 - val_loss: 45.3057\n",
      "Epoch 348/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.3143 - val_loss: 45.4224\n",
      "Epoch 349/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.3077 - val_loss: 45.5348\n",
      "Epoch 350/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.3015 - val_loss: 45.6426\n",
      "Epoch 351/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2957 - val_loss: 45.7463\n",
      "Epoch 352/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2903 - val_loss: 45.8458\n",
      "Epoch 353/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 19.2852 - val_loss: 45.9414\n",
      "Epoch 354/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 19.2805 - val_loss: 46.0333\n",
      "Epoch 355/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2760 - val_loss: 46.1210\n",
      "Epoch 356/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2718 - val_loss: 46.2055\n",
      "Epoch 357/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 19.2679 - val_loss: 46.2865\n",
      "Epoch 358/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 19.2642 - val_loss: 46.3638\n",
      "Epoch 359/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 19.2607 - val_loss: 46.4381\n",
      "Epoch 360/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2575 - val_loss: 46.5092\n",
      "Epoch 361/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 19.2545 - val_loss: 46.5773\n",
      "Epoch 362/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2517 - val_loss: 46.6422\n",
      "Epoch 363/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2490 - val_loss: 46.7046\n",
      "Epoch 364/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 19.2465 - val_loss: 46.7639\n",
      "Epoch 365/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 19.2443 - val_loss: 46.8209\n",
      "Epoch 366/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 19.2420 - val_loss: 46.8751\n",
      "Epoch 367/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2401 - val_loss: 46.9270\n",
      "Epoch 368/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2382 - val_loss: 46.9764\n",
      "Epoch 369/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2364 - val_loss: 47.0236\n",
      "Epoch 370/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2348 - val_loss: 47.0686\n",
      "Epoch 371/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 19.2333 - val_loss: 47.1115\n",
      "Epoch 372/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 19.2318 - val_loss: 47.1524\n",
      "Epoch 373/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2305 - val_loss: 47.1913\n",
      "Epoch 374/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2293 - val_loss: 47.2284\n",
      "Epoch 375/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2281 - val_loss: 47.2636\n",
      "Epoch 376/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2271 - val_loss: 47.2972\n",
      "Epoch 377/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2261 - val_loss: 47.3291\n",
      "Epoch 378/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2252 - val_loss: 47.3592\n",
      "Epoch 379/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2244 - val_loss: 47.3882\n",
      "Epoch 380/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2236 - val_loss: 47.4156\n",
      "Epoch 381/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2229 - val_loss: 47.4415\n",
      "Epoch 382/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2223 - val_loss: 47.4663\n",
      "Epoch 383/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2216 - val_loss: 47.4895\n",
      "Epoch 384/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2211 - val_loss: 47.5119\n",
      "Epoch 385/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 19.2206 - val_loss: 47.5329\n",
      "Epoch 386/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2202 - val_loss: 47.5527\n",
      "Epoch 387/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2198 - val_loss: 47.5714\n",
      "Epoch 388/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2194 - val_loss: 47.5892\n",
      "Epoch 389/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 19.2191 - val_loss: 47.6060\n",
      "Epoch 390/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2188 - val_loss: 47.6219\n",
      "Epoch 391/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 19.2186 - val_loss: 47.6370\n",
      "Epoch 392/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2184 - val_loss: 47.6512\n",
      "Epoch 393/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2182 - val_loss: 47.6647\n",
      "Epoch 394/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2181 - val_loss: 47.6773\n",
      "Epoch 395/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2179 - val_loss: 47.6891\n",
      "Epoch 396/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2178 - val_loss: 47.7005\n",
      "Epoch 397/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2177 - val_loss: 47.7111\n",
      "Epoch 398/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2177 - val_loss: 47.7208\n",
      "Epoch 399/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2176 - val_loss: 47.7301\n",
      "Epoch 400/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2176 - val_loss: 47.7389\n",
      "Epoch 401/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2177 - val_loss: 47.7471\n",
      "Epoch 402/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2177 - val_loss: 47.7550\n",
      "Epoch 403/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2177 - val_loss: 47.7619\n",
      "Epoch 404/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2178 - val_loss: 47.7690\n",
      "Epoch 405/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2178 - val_loss: 47.7752\n",
      "Epoch 406/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2180 - val_loss: 47.7812\n",
      "Epoch 407/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2181 - val_loss: 47.7869\n",
      "Epoch 408/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2181 - val_loss: 47.7919\n",
      "Epoch 409/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 19.2183 - val_loss: 47.7970\n",
      "Epoch 410/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2185 - val_loss: 47.8018\n",
      "Epoch 411/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2186 - val_loss: 47.8059\n",
      "Epoch 412/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2187 - val_loss: 47.8098\n",
      "Epoch 413/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2189 - val_loss: 47.8137\n",
      "Epoch 414/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2191 - val_loss: 47.8172\n",
      "Epoch 415/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2193 - val_loss: 47.8203\n",
      "Epoch 416/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2195 - val_loss: 47.8236\n",
      "Epoch 417/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.2196 - val_loss: 47.8262\n",
      "Epoch 418/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2198 - val_loss: 47.8284\n",
      "Epoch 419/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2201 - val_loss: 47.8309\n",
      "Epoch 420/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2203 - val_loss: 47.8330\n",
      "Epoch 421/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2205 - val_loss: 47.8352\n",
      "Epoch 422/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2207 - val_loss: 47.8370\n",
      "Epoch 423/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2209 - val_loss: 47.8387\n",
      "Epoch 424/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2211 - val_loss: 47.8402\n",
      "Epoch 425/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2214 - val_loss: 47.8417\n",
      "Epoch 426/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2216 - val_loss: 47.8429\n",
      "Epoch 427/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2219 - val_loss: 47.8442\n",
      "Epoch 428/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2221 - val_loss: 47.8451\n",
      "Epoch 429/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2223 - val_loss: 47.8461\n",
      "Epoch 430/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2226 - val_loss: 47.8470\n",
      "Epoch 431/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2228 - val_loss: 47.8476\n",
      "Epoch 432/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2231 - val_loss: 47.8485\n",
      "Epoch 433/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2233 - val_loss: 47.8491\n",
      "Epoch 434/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2236 - val_loss: 47.8496\n",
      "Epoch 435/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2238 - val_loss: 47.8502\n",
      "Epoch 436/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2240 - val_loss: 47.8506\n",
      "Epoch 437/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2243 - val_loss: 47.8509\n",
      "Epoch 438/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2245 - val_loss: 47.8509\n",
      "Epoch 439/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2248 - val_loss: 47.8512\n",
      "Epoch 440/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2250 - val_loss: 47.8513\n",
      "Epoch 441/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2253 - val_loss: 47.8514\n",
      "Epoch 442/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 19.2255 - val_loss: 47.8514\n",
      "Epoch 443/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2257 - val_loss: 47.8516\n",
      "Epoch 444/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2260 - val_loss: 47.8517\n",
      "Epoch 445/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2262 - val_loss: 47.8516\n",
      "Epoch 446/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2265 - val_loss: 47.8515\n",
      "Epoch 447/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2267 - val_loss: 47.8514\n",
      "Epoch 448/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2270 - val_loss: 47.8514\n",
      "Epoch 449/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2272 - val_loss: 47.8512\n",
      "Epoch 450/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2274 - val_loss: 47.8511\n",
      "Epoch 451/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2277 - val_loss: 47.8510\n",
      "Epoch 452/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.2279 - val_loss: 47.8507\n",
      "Epoch 453/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2281 - val_loss: 47.8505\n",
      "Epoch 454/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2284 - val_loss: 47.8504\n",
      "Epoch 455/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2286 - val_loss: 47.8500\n",
      "Epoch 456/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2289 - val_loss: 47.8498\n",
      "Epoch 457/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.2290 - val_loss: 47.8495\n",
      "Epoch 458/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2293 - val_loss: 47.8493\n",
      "Epoch 459/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2295 - val_loss: 47.8490\n",
      "Epoch 460/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2297 - val_loss: 47.8488\n",
      "Epoch 461/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 19.2300 - val_loss: 47.8484\n",
      "Epoch 462/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2302 - val_loss: 47.8481\n",
      "Epoch 463/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2304 - val_loss: 47.8479\n",
      "Epoch 464/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2306 - val_loss: 47.8477\n",
      "Epoch 465/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2308 - val_loss: 47.8472\n",
      "Epoch 466/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2310 - val_loss: 47.8469\n",
      "Epoch 467/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2312 - val_loss: 47.8467\n",
      "Epoch 468/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2314 - val_loss: 47.8464\n",
      "Epoch 469/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2316 - val_loss: 47.8458\n",
      "Epoch 470/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2319 - val_loss: 47.8457\n",
      "Epoch 471/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2321 - val_loss: 47.8455\n",
      "Epoch 472/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2322 - val_loss: 47.8453\n",
      "Epoch 473/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2324 - val_loss: 47.8451\n",
      "Epoch 474/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2326 - val_loss: 47.8447\n",
      "Epoch 475/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2328 - val_loss: 47.8443\n",
      "Epoch 476/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 19.2330 - val_loss: 47.8440\n",
      "Epoch 477/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2332 - val_loss: 47.8436\n",
      "Epoch 478/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2334 - val_loss: 47.8434\n",
      "Epoch 479/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2335 - val_loss: 47.8431\n",
      "Epoch 480/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2337 - val_loss: 47.8427\n",
      "Epoch 481/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2339 - val_loss: 47.8425\n",
      "Epoch 482/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2341 - val_loss: 47.8420\n",
      "Epoch 483/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 19.2343 - val_loss: 47.8418\n",
      "Epoch 484/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2344 - val_loss: 47.8414\n",
      "Epoch 485/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2346 - val_loss: 47.8411\n",
      "Epoch 486/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2347 - val_loss: 47.8408\n",
      "Epoch 487/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2349 - val_loss: 47.8405\n",
      "Epoch 488/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2351 - val_loss: 47.8403\n",
      "Epoch 489/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2353 - val_loss: 47.8397\n",
      "Epoch 490/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 19.2354 - val_loss: 47.8394\n",
      "Epoch 491/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2356 - val_loss: 47.8391\n",
      "Epoch 492/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2357 - val_loss: 47.8390\n",
      "Epoch 493/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2358 - val_loss: 47.8385\n",
      "Epoch 494/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2361 - val_loss: 47.8382\n",
      "Epoch 495/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2362 - val_loss: 47.8381\n",
      "Epoch 496/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2363 - val_loss: 47.8378\n",
      "Epoch 497/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2364 - val_loss: 47.8376\n",
      "Epoch 498/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2366 - val_loss: 47.8372\n",
      "Epoch 499/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2368 - val_loss: 47.8368\n",
      "Epoch 500/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.2369 - val_loss: 47.8366\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(85.7336, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0020, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0125, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3008, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 414ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[64.1968254 , 64.18562092, 64.17441643, 64.16321195, 64.15200747,\n",
       "        64.14080299, 64.12959851, 64.11839402, 64.10718954, 64.09598506,\n",
       "        64.08478058, 64.0735761 , 64.06237162, 64.05116713, 64.03996265,\n",
       "        64.02875817, 64.01755369, 64.00634921, 63.99514472, 63.98394024,\n",
       "        63.97273576, 63.96153128, 63.9503268 , 63.93912232, 63.92791783,\n",
       "        63.91671335, 63.90550887, 63.88291317, 63.84929972, 63.81568627,\n",
       "        63.78207283, 63.74845938, 63.71484594, 63.68123249, 63.64761905,\n",
       "        63.6140056 , 63.58039216, 63.54677871, 63.51316527, 63.47955182,\n",
       "        63.44593838, 63.41232493, 63.37871148, 63.34509804, 63.31148459,\n",
       "        63.27787115, 63.2442577 , 63.21064426, 63.17703081, 63.14341737,\n",
       "        63.10980392, 63.07619048, 63.04257703, 63.00896359, 62.97535014,\n",
       "        62.94173669, 62.90812325, 62.8745098 , 62.84089636, 62.80728291,\n",
       "        62.77366947, 62.74005602, 62.70644258, 62.6886788 , 62.6746732 ,\n",
       "        62.6606676 , 62.646662  , 62.6326564 , 62.61865079, 62.60464519,\n",
       "        62.59063959, 62.57663399, 62.56262838, 62.54862278, 62.53461718,\n",
       "        62.52061158, 62.50660598, 62.49260037, 62.47859477, 62.46458917,\n",
       "        70.40444183,  0.        ,  0.        ,  0.        ,  0.180943  ,\n",
       "         0.        ,  0.        ,  0.25664502,  0.13211077,  0.42765766,\n",
       "         0.09160335,  1.07955158,  0.23033956,  0.18624884,  0.        ,\n",
       "         0.        ,  0.14688107,  0.        ,  0.14389737,  0.42730436]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([60.66463585, 60.65063025, 60.63662465, 60.62261905, 60.60861345,\n",
       "       60.59460784, 60.58060224, 60.56659664, 60.55259104, 60.53858543,\n",
       "       60.52457983, 60.51057423, 60.49656863, 60.48256303, 60.46855742,\n",
       "       60.45455182, 60.44054622, 60.42654062, 60.41253501, 60.39852941,\n",
       "       60.38452381, 60.37051821, 60.35651261, 60.342507  , 60.3285014 ,\n",
       "       60.3144958 , 60.3004902 , 60.28648459, 60.27247899, 60.25847339,\n",
       "       60.24446779, 60.23046218, 60.21645658, 60.20245098, 60.18844538,\n",
       "       60.17443978, 60.16043417, 60.14642857, 60.13242297, 60.11841737,\n",
       "       60.10441176, 60.09040616, 60.07640056, 60.06239496, 60.04838936,\n",
       "       60.03438375, 60.02037815, 60.00637255, 59.99236695, 59.97836134,\n",
       "       59.96435574, 59.95035014, 59.93634454, 59.92233894, 59.90833333,\n",
       "       59.89432773, 59.88032213, 59.86631653, 59.85231092, 59.83830532,\n",
       "       59.82429972, 59.81029412, 59.79628852, 59.78228291, 59.76827731,\n",
       "       59.75427171, 59.74026611, 59.7262605 , 59.7122549 , 59.69935808,\n",
       "       59.69422269, 59.6890873 , 59.68395191, 59.67881653, 59.67368114,\n",
       "       59.66854575, 59.66341036, 59.65827498, 59.65313959, 59.6480042 ,\n",
       "       59.64286881, 59.63773343, 59.63259804, 59.62746265, 59.62232726,\n",
       "       59.61719188, 59.61205649, 59.6069211 , 59.60178571, 59.59665033,\n",
       "       59.59151494, 59.58637955, 59.58124416, 59.57610878, 59.57097339,\n",
       "       59.565838  , 59.56070261, 59.55556723, 59.55043184, 59.54529645])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.088868322995015\n",
      "14.020766444039191\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
