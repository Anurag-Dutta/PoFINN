{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "2345    56.688809\n",
       "2346    56.679791\n",
       "2347    56.670773\n",
       "2348    56.661754\n",
       "2349    56.652736\n",
       "Name: C8, Length: 2350, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c8_interpolated_2250_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "2245     0.000000\n",
       "2246     0.072535\n",
       "2247     0.000000\n",
       "2248     0.240430\n",
       "2249     0.094473\n",
       "Name: C8, Length: 2250, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2250)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.987115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.774230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.561345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.348459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.200000  0.000298\n",
       "1     87.987115  0.000298\n",
       "2     87.774230  0.000297\n",
       "3     87.561345  0.000297\n",
       "4     87.348459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 85.7336  # Value for C0\n",
    "K0 = -0.0020  # Value for K0\n",
    "K1 = 0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0125    # Value for b\n",
    "c = 2.3008    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.200000    87.987115    87.774230    87.561345    87.348459   \n",
      "351    87.987115    87.774230    87.561345    87.348459    87.135574   \n",
      "352    87.774230    87.561345    87.348459    87.135574    86.922689   \n",
      "353    87.561345    87.348459    87.135574    86.922689    86.709804   \n",
      "354    87.348459    87.135574    86.922689    86.709804    86.496919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.135574    86.922689    86.709804    86.496919    86.294538  ...   \n",
      "351    86.922689    86.709804    86.496919    86.294538    86.221709  ...   \n",
      "352    86.709804    86.496919    86.294538    86.221709    86.148880  ...   \n",
      "353    86.496919    86.294538    86.221709    86.148880    86.076050  ...   \n",
      "354    86.294538    86.221709    86.148880    86.076050    86.003221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   76.094351    0.000263   76.058870    0.000263   76.023389    0.000263   \n",
      "351   76.058870    0.000263   76.023389    0.000263   75.987909    0.000262   \n",
      "352   76.023389    0.000263   75.987909    0.000262   75.952428    0.000262   \n",
      "353   75.987909    0.000262   75.952428    0.000262   75.916947    0.000262   \n",
      "354   75.952428    0.000262   75.916947    0.000262   75.881466    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   75.987909    0.000262   75.952428    0.000262  \n",
      "351   75.952428    0.000262   75.916947    0.000262  \n",
      "352   75.916947    0.000262   75.881466    0.000262  \n",
      "353   75.881466    0.000262   75.845985    0.000262  \n",
      "354   75.845985    0.000262   75.810504    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 1, 251) (1800, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "23/23 [==============================] - 2s 22ms/step - loss: 4620.4932 - val_loss: 3053.9990\n",
      "Epoch 2/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4532.7290 - val_loss: 3012.9580\n",
      "Epoch 3/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4469.3838 - val_loss: 2973.4480\n",
      "Epoch 4/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4406.6099 - val_loss: 2934.4080\n",
      "Epoch 5/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4344.4663 - val_loss: 2895.8345\n",
      "Epoch 6/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4282.9619 - val_loss: 2857.7256\n",
      "Epoch 7/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4222.1021 - val_loss: 2820.0776\n",
      "Epoch 8/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4161.5146 - val_loss: 2781.1831\n",
      "Epoch 9/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4096.9692 - val_loss: 2740.5862\n",
      "Epoch 10/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 4032.6470 - val_loss: 2701.1721\n",
      "Epoch 11/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3970.0044 - val_loss: 2662.7827\n",
      "Epoch 12/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3908.6687 - val_loss: 2625.1938\n",
      "Epoch 13/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3848.3840 - val_loss: 2588.2878\n",
      "Epoch 14/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3789.0166 - val_loss: 2552.0007\n",
      "Epoch 15/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3730.4919 - val_loss: 2516.2913\n",
      "Epoch 16/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3672.7581 - val_loss: 2481.1316\n",
      "Epoch 17/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3615.7786 - val_loss: 2446.5015\n",
      "Epoch 18/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3559.5281 - val_loss: 2412.3850\n",
      "Epoch 19/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3503.9858 - val_loss: 2378.7688\n",
      "Epoch 20/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3449.1350 - val_loss: 2345.6421\n",
      "Epoch 21/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3394.9597 - val_loss: 2312.9958\n",
      "Epoch 22/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3341.4490 - val_loss: 2280.8218\n",
      "Epoch 23/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3288.5923 - val_loss: 2249.1123\n",
      "Epoch 24/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3236.3789 - val_loss: 2217.8608\n",
      "Epoch 25/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3184.7991 - val_loss: 2187.0608\n",
      "Epoch 26/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3133.8464 - val_loss: 2156.7068\n",
      "Epoch 27/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3083.5115 - val_loss: 2126.7930\n",
      "Epoch 28/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3033.7876 - val_loss: 2097.3147\n",
      "Epoch 29/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2984.6677 - val_loss: 2068.2659\n",
      "Epoch 30/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2936.1450 - val_loss: 2039.6423\n",
      "Epoch 31/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2888.2131 - val_loss: 2011.4401\n",
      "Epoch 32/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2840.8667 - val_loss: 1983.6538\n",
      "Epoch 33/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2794.0989 - val_loss: 1956.2792\n",
      "Epoch 34/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2747.9041 - val_loss: 1929.3120\n",
      "Epoch 35/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2702.2766 - val_loss: 1902.7484\n",
      "Epoch 36/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2657.2114 - val_loss: 1876.5842\n",
      "Epoch 37/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2612.7026 - val_loss: 1850.8157\n",
      "Epoch 38/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2568.7451 - val_loss: 1825.4382\n",
      "Epoch 39/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2525.3345 - val_loss: 1800.4482\n",
      "Epoch 40/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 2482.4648 - val_loss: 1775.8427\n",
      "Epoch 41/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2440.1313 - val_loss: 1751.6172\n",
      "Epoch 42/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2398.3291 - val_loss: 1727.7681\n",
      "Epoch 43/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2357.0537 - val_loss: 1704.2920\n",
      "Epoch 44/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2316.3000 - val_loss: 1681.1855\n",
      "Epoch 45/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2276.0640 - val_loss: 1658.4448\n",
      "Epoch 46/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2236.3403 - val_loss: 1636.0667\n",
      "Epoch 47/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2197.1248 - val_loss: 1614.0474\n",
      "Epoch 48/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2158.4131 - val_loss: 1592.3839\n",
      "Epoch 49/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2120.2007 - val_loss: 1571.0723\n",
      "Epoch 50/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2082.4829 - val_loss: 1550.1101\n",
      "Epoch 51/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2045.2554 - val_loss: 1529.4930\n",
      "Epoch 52/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2008.5144 - val_loss: 1509.2186\n",
      "Epoch 53/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1972.2549 - val_loss: 1489.2833\n",
      "Epoch 54/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1936.4738 - val_loss: 1469.6842\n",
      "Epoch 55/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1901.1666 - val_loss: 1450.4181\n",
      "Epoch 56/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1866.3282 - val_loss: 1431.4810\n",
      "Epoch 57/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1831.9557 - val_loss: 1412.8710\n",
      "Epoch 58/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1798.0448 - val_loss: 1394.5844\n",
      "Epoch 59/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1764.5916 - val_loss: 1376.6180\n",
      "Epoch 60/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1731.5916 - val_loss: 1358.9692\n",
      "Epoch 61/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1699.0411 - val_loss: 1341.6346\n",
      "Epoch 62/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1666.9368 - val_loss: 1324.6116\n",
      "Epoch 63/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1635.2742 - val_loss: 1307.8969\n",
      "Epoch 64/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1604.0494 - val_loss: 1291.4874\n",
      "Epoch 65/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1573.2589 - val_loss: 1275.3802\n",
      "Epoch 66/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1542.8986 - val_loss: 1259.5730\n",
      "Epoch 67/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1512.9651 - val_loss: 1244.0621\n",
      "Epoch 68/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1483.4543 - val_loss: 1228.8451\n",
      "Epoch 69/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1454.3630 - val_loss: 1213.9191\n",
      "Epoch 70/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1425.6876 - val_loss: 1199.2811\n",
      "Epoch 71/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1397.4240 - val_loss: 1184.9281\n",
      "Epoch 72/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1369.5686 - val_loss: 1170.8574\n",
      "Epoch 73/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1342.1179 - val_loss: 1157.0667\n",
      "Epoch 74/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1315.0687 - val_loss: 1143.5525\n",
      "Epoch 75/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1288.4170 - val_loss: 1130.3124\n",
      "Epoch 76/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1262.1598 - val_loss: 1117.3436\n",
      "Epoch 77/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1236.2933 - val_loss: 1104.6434\n",
      "Epoch 78/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1210.8138 - val_loss: 1092.2089\n",
      "Epoch 79/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1185.7177 - val_loss: 1080.0374\n",
      "Epoch 80/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1161.0024 - val_loss: 1068.1265\n",
      "Epoch 81/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1136.6637 - val_loss: 1056.4730\n",
      "Epoch 82/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1112.6981 - val_loss: 1045.0743\n",
      "Epoch 83/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1089.1027 - val_loss: 1033.9277\n",
      "Epoch 84/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1065.8739 - val_loss: 1023.0312\n",
      "Epoch 85/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1043.0087 - val_loss: 1012.3813\n",
      "Epoch 86/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1020.5032 - val_loss: 1001.9757\n",
      "Epoch 87/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 998.3542 - val_loss: 991.8115\n",
      "Epoch 88/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 976.5586 - val_loss: 981.8865\n",
      "Epoch 89/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 955.1130 - val_loss: 972.1980\n",
      "Epoch 90/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 934.0143 - val_loss: 962.7432\n",
      "Epoch 91/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 913.2590 - val_loss: 953.5195\n",
      "Epoch 92/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 892.8443 - val_loss: 944.5240\n",
      "Epoch 93/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 872.7660 - val_loss: 935.7546\n",
      "Epoch 94/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 853.0217 - val_loss: 927.2087\n",
      "Epoch 95/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 833.6077 - val_loss: 918.8834\n",
      "Epoch 96/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 814.5214 - val_loss: 910.7762\n",
      "Epoch 97/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 795.7590 - val_loss: 902.8847\n",
      "Epoch 98/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 777.3174 - val_loss: 895.2062\n",
      "Epoch 99/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 759.1940 - val_loss: 887.7382\n",
      "Epoch 100/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 741.3853 - val_loss: 880.4780\n",
      "Epoch 101/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 723.8878 - val_loss: 873.4233\n",
      "Epoch 102/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 706.6989 - val_loss: 866.5714\n",
      "Epoch 103/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 689.8153 - val_loss: 859.9198\n",
      "Epoch 104/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 673.2337 - val_loss: 853.4659\n",
      "Epoch 105/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 656.9510 - val_loss: 847.2072\n",
      "Epoch 106/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 640.9641 - val_loss: 841.1410\n",
      "Epoch 107/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 625.2699 - val_loss: 835.2650\n",
      "Epoch 108/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 609.8656 - val_loss: 829.5768\n",
      "Epoch 109/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 594.7480 - val_loss: 824.0736\n",
      "Epoch 110/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 579.9138 - val_loss: 818.7531\n",
      "Epoch 111/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 565.3605 - val_loss: 813.6126\n",
      "Epoch 112/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 551.0844 - val_loss: 808.6497\n",
      "Epoch 113/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 537.0827 - val_loss: 803.8618\n",
      "Epoch 114/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 523.3525 - val_loss: 799.2465\n",
      "Epoch 115/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 509.8904 - val_loss: 794.8013\n",
      "Epoch 116/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 496.6934 - val_loss: 790.5236\n",
      "Epoch 117/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 483.7589 - val_loss: 786.4109\n",
      "Epoch 118/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 471.0836 - val_loss: 782.4608\n",
      "Epoch 119/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 458.6645 - val_loss: 778.6708\n",
      "Epoch 120/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 446.4987 - val_loss: 775.0384\n",
      "Epoch 121/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 434.5833 - val_loss: 771.5610\n",
      "Epoch 122/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 422.9149 - val_loss: 768.2362\n",
      "Epoch 123/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 411.4905 - val_loss: 765.0615\n",
      "Epoch 124/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 400.3076 - val_loss: 762.0345\n",
      "Epoch 125/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 389.3630 - val_loss: 759.1528\n",
      "Epoch 126/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 378.6534 - val_loss: 756.4135\n",
      "Epoch 127/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 368.1761 - val_loss: 753.8147\n",
      "Epoch 128/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 357.9286 - val_loss: 751.3536\n",
      "Epoch 129/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 347.9076 - val_loss: 749.0277\n",
      "Epoch 130/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 338.1098 - val_loss: 746.8347\n",
      "Epoch 131/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 328.5322 - val_loss: 744.7718\n",
      "Epoch 132/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 319.1721 - val_loss: 742.8369\n",
      "Epoch 133/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 310.0271 - val_loss: 741.0274\n",
      "Epoch 134/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 301.0934 - val_loss: 739.3409\n",
      "Epoch 135/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 292.3687 - val_loss: 737.7751\n",
      "Epoch 136/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 283.8498 - val_loss: 736.3273\n",
      "Epoch 137/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 275.5340 - val_loss: 734.9950\n",
      "Epoch 138/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 267.4179 - val_loss: 733.7759\n",
      "Epoch 139/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 259.4991 - val_loss: 732.6675\n",
      "Epoch 140/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 251.7741 - val_loss: 731.6675\n",
      "Epoch 141/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 244.2404 - val_loss: 730.7734\n",
      "Epoch 142/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 236.8951 - val_loss: 729.9828\n",
      "Epoch 143/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 229.7353 - val_loss: 729.2932\n",
      "Epoch 144/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 222.7585 - val_loss: 728.7025\n",
      "Epoch 145/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 215.9615 - val_loss: 728.2077\n",
      "Epoch 146/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 209.3415 - val_loss: 727.8070\n",
      "Epoch 147/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 202.8954 - val_loss: 727.4977\n",
      "Epoch 148/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 196.6203 - val_loss: 727.2773\n",
      "Epoch 149/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 190.5137 - val_loss: 727.1437\n",
      "Epoch 150/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 184.5729 - val_loss: 727.0945\n",
      "Epoch 151/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 178.7944 - val_loss: 727.1272\n",
      "Epoch 152/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 173.1759 - val_loss: 727.2396\n",
      "Epoch 153/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 167.7146 - val_loss: 727.4293\n",
      "Epoch 154/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 162.4076 - val_loss: 727.6939\n",
      "Epoch 155/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 157.2521 - val_loss: 728.0312\n",
      "Epoch 156/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 152.2454 - val_loss: 728.4388\n",
      "Epoch 157/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 147.3847 - val_loss: 728.9146\n",
      "Epoch 158/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 142.6671 - val_loss: 729.4560\n",
      "Epoch 159/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 138.0902 - val_loss: 730.0609\n",
      "Epoch 160/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 133.6510 - val_loss: 730.7272\n",
      "Epoch 161/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 129.3469 - val_loss: 731.4523\n",
      "Epoch 162/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 125.1753 - val_loss: 732.2343\n",
      "Epoch 163/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 121.1336 - val_loss: 733.0709\n",
      "Epoch 164/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 117.2188 - val_loss: 733.9597\n",
      "Epoch 165/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 113.4284 - val_loss: 734.8989\n",
      "Epoch 166/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 109.7597 - val_loss: 735.8859\n",
      "Epoch 167/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 106.2102 - val_loss: 736.9189\n",
      "Epoch 168/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 102.7773 - val_loss: 737.9957\n",
      "Epoch 169/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 99.4583 - val_loss: 739.1144\n",
      "Epoch 170/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 96.2508 - val_loss: 740.2724\n",
      "Epoch 171/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 93.1521 - val_loss: 741.4680\n",
      "Epoch 172/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 90.1599 - val_loss: 742.6992\n",
      "Epoch 173/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 87.2714 - val_loss: 743.9638\n",
      "Epoch 174/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 84.4844 - val_loss: 745.2599\n",
      "Epoch 175/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 81.7961 - val_loss: 746.5856\n",
      "Epoch 176/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 79.2044 - val_loss: 747.9389\n",
      "Epoch 177/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 76.7067 - val_loss: 749.3179\n",
      "Epoch 178/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 74.3007 - val_loss: 750.7206\n",
      "Epoch 179/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 71.9841 - val_loss: 752.1453\n",
      "Epoch 180/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 69.7542 - val_loss: 753.5901\n",
      "Epoch 181/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 67.6091 - val_loss: 755.0533\n",
      "Epoch 182/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 65.5462 - val_loss: 756.5328\n",
      "Epoch 183/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 63.5635 - val_loss: 758.0273\n",
      "Epoch 184/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 61.6585 - val_loss: 759.5351\n",
      "Epoch 185/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 59.8291 - val_loss: 761.0539\n",
      "Epoch 186/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 58.0733 - val_loss: 762.5828\n",
      "Epoch 187/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 56.3886 - val_loss: 764.1198\n",
      "Epoch 188/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 54.7732 - val_loss: 765.6634\n",
      "Epoch 189/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 53.2250 - val_loss: 767.2119\n",
      "Epoch 190/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 51.7417 - val_loss: 768.7642\n",
      "Epoch 191/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 50.3214 - val_loss: 770.3184\n",
      "Epoch 192/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 48.9622 - val_loss: 771.8734\n",
      "Epoch 193/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 47.6618 - val_loss: 773.4276\n",
      "Epoch 194/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 46.4188 - val_loss: 774.9796\n",
      "Epoch 195/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 45.2309 - val_loss: 776.5286\n",
      "Epoch 196/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 44.0963 - val_loss: 778.0726\n",
      "Epoch 197/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 43.0133 - val_loss: 779.6110\n",
      "Epoch 198/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 41.9801 - val_loss: 781.1420\n",
      "Epoch 199/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 40.9948 - val_loss: 782.6649\n",
      "Epoch 200/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 40.0559 - val_loss: 784.1785\n",
      "Epoch 201/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 39.1615 - val_loss: 785.6815\n",
      "Epoch 202/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 38.3100 - val_loss: 787.1735\n",
      "Epoch 203/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 37.4999 - val_loss: 788.6527\n",
      "Epoch 204/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 36.7295 - val_loss: 790.1186\n",
      "Epoch 205/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 35.9973 - val_loss: 791.5699\n",
      "Epoch 206/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 35.3020 - val_loss: 793.0063\n",
      "Epoch 207/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 34.6418 - val_loss: 794.4266\n",
      "Epoch 208/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 34.0156 - val_loss: 795.8302\n",
      "Epoch 209/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.4218 - val_loss: 797.2163\n",
      "Epoch 210/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 32.8589 - val_loss: 798.5842\n",
      "Epoch 211/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 32.3260 - val_loss: 799.9332\n",
      "Epoch 212/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 31.8215 - val_loss: 801.2630\n",
      "Epoch 213/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 31.3443 - val_loss: 802.5726\n",
      "Epoch 214/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 30.8931 - val_loss: 803.8617\n",
      "Epoch 215/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 30.4668 - val_loss: 805.1295\n",
      "Epoch 216/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 30.0643 - val_loss: 806.3759\n",
      "Epoch 217/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 29.6845 - val_loss: 807.6003\n",
      "Epoch 218/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 29.3263 - val_loss: 808.8024\n",
      "Epoch 219/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 28.9887 - val_loss: 809.9819\n",
      "Epoch 220/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 28.6706 - val_loss: 811.1385\n",
      "Epoch 221/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 28.3712 - val_loss: 812.2719\n",
      "Epoch 222/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 28.0896 - val_loss: 813.3816\n",
      "Epoch 223/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 27.8248 - val_loss: 814.4680\n",
      "Epoch 224/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 27.5759 - val_loss: 815.5306\n",
      "Epoch 225/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 27.3422 - val_loss: 816.5693\n",
      "Epoch 226/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 27.1229 - val_loss: 817.5845\n",
      "Epoch 227/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 26.9171 - val_loss: 818.5753\n",
      "Epoch 228/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 26.7242 - val_loss: 819.5420\n",
      "Epoch 229/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 26.5436 - val_loss: 820.4847\n",
      "Epoch 230/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 26.3744 - val_loss: 821.4039\n",
      "Epoch 231/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 26.2160 - val_loss: 822.2989\n",
      "Epoch 232/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 26.0680 - val_loss: 823.1700\n",
      "Epoch 233/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.9296 - val_loss: 824.0177\n",
      "Epoch 234/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.8004 - val_loss: 824.8417\n",
      "Epoch 235/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.6797 - val_loss: 825.6427\n",
      "Epoch 236/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.5671 - val_loss: 826.4202\n",
      "Epoch 237/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.4621 - val_loss: 827.1748\n",
      "Epoch 238/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.3642 - val_loss: 827.9068\n",
      "Epoch 239/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.2730 - val_loss: 828.6161\n",
      "Epoch 240/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1881 - val_loss: 829.3036\n",
      "Epoch 241/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1092 - val_loss: 829.9688\n",
      "Epoch 242/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.0357 - val_loss: 830.6129\n",
      "Epoch 243/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.9675 - val_loss: 831.2355\n",
      "Epoch 244/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.9040 - val_loss: 831.8374\n",
      "Epoch 245/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.8450 - val_loss: 832.4186\n",
      "Epoch 246/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 24.7904 - val_loss: 832.9797\n",
      "Epoch 247/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.7396 - val_loss: 833.5212\n",
      "Epoch 248/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.6925 - val_loss: 834.0429\n",
      "Epoch 249/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.6490 - val_loss: 834.5460\n",
      "Epoch 250/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.6085 - val_loss: 835.0302\n",
      "Epoch 251/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.5711 - val_loss: 835.4965\n",
      "Epoch 252/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.5364 - val_loss: 835.9449\n",
      "Epoch 253/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.5043 - val_loss: 836.3762\n",
      "Epoch 254/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.4746 - val_loss: 836.7902\n",
      "Epoch 255/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.4472 - val_loss: 837.1879\n",
      "Epoch 256/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.4218 - val_loss: 837.5693\n",
      "Epoch 257/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.3984 - val_loss: 837.9355\n",
      "Epoch 258/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.3767 - val_loss: 838.2861\n",
      "Epoch 259/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.3567 - val_loss: 838.6221\n",
      "Epoch 260/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.3382 - val_loss: 838.9442\n",
      "Epoch 261/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.3211 - val_loss: 839.2518\n",
      "Epoch 262/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.3054 - val_loss: 839.5460\n",
      "Epoch 263/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.2909 - val_loss: 839.8273\n",
      "Epoch 264/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 24.2775 - val_loss: 840.0960\n",
      "Epoch 265/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.2652 - val_loss: 840.3525\n",
      "Epoch 266/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.2538 - val_loss: 840.5970\n",
      "Epoch 267/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.2433 - val_loss: 840.8304\n",
      "Epoch 268/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.2336 - val_loss: 841.0522\n",
      "Epoch 269/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.2247 - val_loss: 841.2636\n",
      "Epoch 270/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.2166 - val_loss: 841.4651\n",
      "Epoch 271/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.2091 - val_loss: 841.6568\n",
      "Epoch 272/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.2022 - val_loss: 841.8387\n",
      "Epoch 273/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1958 - val_loss: 842.0115\n",
      "Epoch 274/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1900 - val_loss: 842.1756\n",
      "Epoch 275/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1846 - val_loss: 842.3314\n",
      "Epoch 276/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1797 - val_loss: 842.4790\n",
      "Epoch 277/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1752 - val_loss: 842.6187\n",
      "Epoch 278/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1711 - val_loss: 842.7511\n",
      "Epoch 279/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1672 - val_loss: 842.8762\n",
      "Epoch 280/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1638 - val_loss: 842.9946\n",
      "Epoch 281/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1607 - val_loss: 843.1068\n",
      "Epoch 282/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 24.1578 - val_loss: 843.2128\n",
      "Epoch 283/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1552 - val_loss: 843.3129\n",
      "Epoch 284/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1528 - val_loss: 843.4068\n",
      "Epoch 285/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1506 - val_loss: 843.4955\n",
      "Epoch 286/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1487 - val_loss: 843.5792\n",
      "Epoch 287/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1470 - val_loss: 843.6581\n",
      "Epoch 288/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1454 - val_loss: 843.7324\n",
      "Epoch 289/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1440 - val_loss: 843.8021\n",
      "Epoch 290/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1427 - val_loss: 843.8677\n",
      "Epoch 291/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1416 - val_loss: 843.9291\n",
      "Epoch 292/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1406 - val_loss: 843.9870\n",
      "Epoch 293/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1397 - val_loss: 844.0413\n",
      "Epoch 294/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1389 - val_loss: 844.0919\n",
      "Epoch 295/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1382 - val_loss: 844.1398\n",
      "Epoch 296/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1377 - val_loss: 844.1842\n",
      "Epoch 297/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1372 - val_loss: 844.2260\n",
      "Epoch 298/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1367 - val_loss: 844.2648\n",
      "Epoch 299/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1364 - val_loss: 844.3012\n",
      "Epoch 300/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 24.1362 - val_loss: 844.3351\n",
      "Epoch 301/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1360 - val_loss: 844.3666\n",
      "Epoch 302/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1359 - val_loss: 844.3959\n",
      "Epoch 303/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1358 - val_loss: 844.4235\n",
      "Epoch 304/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1358 - val_loss: 844.4488\n",
      "Epoch 305/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1358 - val_loss: 844.4725\n",
      "Epoch 306/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1360 - val_loss: 844.4946\n",
      "Epoch 307/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1361 - val_loss: 844.5151\n",
      "Epoch 308/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1362 - val_loss: 844.5341\n",
      "Epoch 309/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1363 - val_loss: 844.5516\n",
      "Epoch 310/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1366 - val_loss: 844.5677\n",
      "Epoch 311/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1368 - val_loss: 844.5826\n",
      "Epoch 312/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1371 - val_loss: 844.5964\n",
      "Epoch 313/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1374 - val_loss: 844.6090\n",
      "Epoch 314/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1378 - val_loss: 844.6205\n",
      "Epoch 315/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1381 - val_loss: 844.6314\n",
      "Epoch 316/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 24.1384 - val_loss: 844.6413\n",
      "Epoch 317/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1389 - val_loss: 844.6503\n",
      "Epoch 318/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1392 - val_loss: 844.6585\n",
      "Epoch 319/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1397 - val_loss: 844.6658\n",
      "Epoch 320/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1401 - val_loss: 844.6727\n",
      "Epoch 321/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1405 - val_loss: 844.6790\n",
      "Epoch 322/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1410 - val_loss: 844.6846\n",
      "Epoch 323/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1414 - val_loss: 844.6897\n",
      "Epoch 324/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1419 - val_loss: 844.6943\n",
      "Epoch 325/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1424 - val_loss: 844.6985\n",
      "Epoch 326/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1429 - val_loss: 844.7020\n",
      "Epoch 327/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1434 - val_loss: 844.7054\n",
      "Epoch 328/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1439 - val_loss: 844.7084\n",
      "Epoch 329/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1443 - val_loss: 844.7111\n",
      "Epoch 330/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1448 - val_loss: 844.7134\n",
      "Epoch 331/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1453 - val_loss: 844.7154\n",
      "Epoch 332/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1458 - val_loss: 844.7172\n",
      "Epoch 333/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1464 - val_loss: 844.7189\n",
      "Epoch 334/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1468 - val_loss: 844.7203\n",
      "Epoch 335/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1473 - val_loss: 844.7212\n",
      "Epoch 336/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 24.1478 - val_loss: 844.7220\n",
      "Epoch 337/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1483 - val_loss: 844.7226\n",
      "Epoch 338/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1488 - val_loss: 844.7233\n",
      "Epoch 339/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1493 - val_loss: 844.7236\n",
      "Epoch 340/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1498 - val_loss: 844.7238\n",
      "Epoch 341/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1503 - val_loss: 844.7238\n",
      "Epoch 342/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1507 - val_loss: 844.7237\n",
      "Epoch 343/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1513 - val_loss: 844.7236\n",
      "Epoch 344/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1517 - val_loss: 844.7233\n",
      "Epoch 345/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1522 - val_loss: 844.7233\n",
      "Epoch 346/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1527 - val_loss: 844.7228\n",
      "Epoch 347/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1531 - val_loss: 844.7223\n",
      "Epoch 348/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1536 - val_loss: 844.7221\n",
      "Epoch 349/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1541 - val_loss: 844.7216\n",
      "Epoch 350/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1545 - val_loss: 844.7208\n",
      "Epoch 351/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 24.1550 - val_loss: 844.7202\n",
      "Epoch 352/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1554 - val_loss: 844.7197\n",
      "Epoch 353/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1558 - val_loss: 844.7188\n",
      "Epoch 354/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1562 - val_loss: 844.7181\n",
      "Epoch 355/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1566 - val_loss: 844.7172\n",
      "Epoch 356/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1571 - val_loss: 844.7167\n",
      "Epoch 357/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1575 - val_loss: 844.7156\n",
      "Epoch 358/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1579 - val_loss: 844.7144\n",
      "Epoch 359/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.1583 - val_loss: 844.7136\n",
      "Epoch 360/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1587 - val_loss: 844.7128\n",
      "Epoch 361/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1591 - val_loss: 844.7118\n",
      "Epoch 362/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1595 - val_loss: 844.7109\n",
      "Epoch 363/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1599 - val_loss: 844.7097\n",
      "Epoch 364/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1602 - val_loss: 844.7089\n",
      "Epoch 365/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1606 - val_loss: 844.7078\n",
      "Epoch 366/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 24.1610 - val_loss: 844.7068\n",
      "Epoch 367/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1613 - val_loss: 844.7058\n",
      "Epoch 368/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1617 - val_loss: 844.7051\n",
      "Epoch 369/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1620 - val_loss: 844.7041\n",
      "Epoch 370/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1624 - val_loss: 844.7032\n",
      "Epoch 371/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1627 - val_loss: 844.7025\n",
      "Epoch 372/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1631 - val_loss: 844.7017\n",
      "Epoch 373/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1634 - val_loss: 844.7009\n",
      "Epoch 374/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1637 - val_loss: 844.7001\n",
      "Epoch 375/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1640 - val_loss: 844.6996\n",
      "Epoch 376/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1643 - val_loss: 844.6987\n",
      "Epoch 377/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1646 - val_loss: 844.6979\n",
      "Epoch 378/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1649 - val_loss: 844.6969\n",
      "Epoch 379/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1652 - val_loss: 844.6963\n",
      "Epoch 380/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1655 - val_loss: 844.6957\n",
      "Epoch 381/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1658 - val_loss: 844.6948\n",
      "Epoch 382/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1661 - val_loss: 844.6942\n",
      "Epoch 383/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1663 - val_loss: 844.6934\n",
      "Epoch 384/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 24.1666 - val_loss: 844.6925\n",
      "Epoch 385/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1668 - val_loss: 844.6918\n",
      "Epoch 386/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1672 - val_loss: 844.6912\n",
      "Epoch 387/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1674 - val_loss: 844.6904\n",
      "Epoch 388/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1676 - val_loss: 844.6898\n",
      "Epoch 389/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1679 - val_loss: 844.6891\n",
      "Epoch 390/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 24.1681 - val_loss: 844.6882\n",
      "Epoch 391/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1684 - val_loss: 844.6876\n",
      "Epoch 392/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1686 - val_loss: 844.6870\n",
      "Epoch 393/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1688 - val_loss: 844.6861\n",
      "Epoch 394/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1690 - val_loss: 844.6851\n",
      "Epoch 395/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1693 - val_loss: 844.6847\n",
      "Epoch 396/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1694 - val_loss: 844.6840\n",
      "Epoch 397/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1697 - val_loss: 844.6833\n",
      "Epoch 398/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1699 - val_loss: 844.6828\n",
      "Epoch 399/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1701 - val_loss: 844.6819\n",
      "Epoch 400/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1703 - val_loss: 844.6819\n",
      "Epoch 401/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1705 - val_loss: 844.6811\n",
      "Epoch 402/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1706 - val_loss: 844.6805\n",
      "Epoch 403/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1709 - val_loss: 844.6799\n",
      "Epoch 404/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1711 - val_loss: 844.6797\n",
      "Epoch 405/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1712 - val_loss: 844.6788\n",
      "Epoch 406/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 24.1714 - val_loss: 844.6785\n",
      "Epoch 407/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1715 - val_loss: 844.6775\n",
      "Epoch 408/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1718 - val_loss: 844.6775\n",
      "Epoch 409/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1719 - val_loss: 844.6768\n",
      "Epoch 410/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1721 - val_loss: 844.6764\n",
      "Epoch 411/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1723 - val_loss: 844.6757\n",
      "Epoch 412/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1724 - val_loss: 844.6754\n",
      "Epoch 413/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1725 - val_loss: 844.6747\n",
      "Epoch 414/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1727 - val_loss: 844.6742\n",
      "Epoch 415/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1729 - val_loss: 844.6738\n",
      "Epoch 416/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1730 - val_loss: 844.6737\n",
      "Epoch 417/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1731 - val_loss: 844.6733\n",
      "Epoch 418/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1733 - val_loss: 844.6731\n",
      "Epoch 419/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1734 - val_loss: 844.6722\n",
      "Epoch 420/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1735 - val_loss: 844.6718\n",
      "Epoch 421/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1736 - val_loss: 844.6714\n",
      "Epoch 422/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1738 - val_loss: 844.6713\n",
      "Epoch 423/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1739 - val_loss: 844.6708\n",
      "Epoch 424/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 24.1740 - val_loss: 844.6704\n",
      "Epoch 425/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1742 - val_loss: 844.6704\n",
      "Epoch 426/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1742 - val_loss: 844.6698\n",
      "Epoch 427/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1743 - val_loss: 844.6693\n",
      "Epoch 428/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1745 - val_loss: 844.6688\n",
      "Epoch 429/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1746 - val_loss: 844.6685\n",
      "Epoch 430/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1747 - val_loss: 844.6680\n",
      "Epoch 431/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1748 - val_loss: 844.6680\n",
      "Epoch 432/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1749 - val_loss: 844.6677\n",
      "Epoch 433/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1750 - val_loss: 844.6672\n",
      "Epoch 434/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1751 - val_loss: 844.6670\n",
      "Epoch 435/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1752 - val_loss: 844.6666\n",
      "Epoch 436/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1753 - val_loss: 844.6663\n",
      "Epoch 437/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1754 - val_loss: 844.6660\n",
      "Epoch 438/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1755 - val_loss: 844.6655\n",
      "Epoch 439/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1756 - val_loss: 844.6655\n",
      "Epoch 440/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1757 - val_loss: 844.6652\n",
      "Epoch 441/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1757 - val_loss: 844.6645\n",
      "Epoch 442/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 24.1759 - val_loss: 844.6644\n",
      "Epoch 443/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1759 - val_loss: 844.6644\n",
      "Epoch 444/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1760 - val_loss: 844.6643\n",
      "Epoch 445/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1760 - val_loss: 844.6637\n",
      "Epoch 446/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1761 - val_loss: 844.6633\n",
      "Epoch 447/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1762 - val_loss: 844.6633\n",
      "Epoch 448/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1762 - val_loss: 844.6630\n",
      "Epoch 449/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1764 - val_loss: 844.6627\n",
      "Epoch 450/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1764 - val_loss: 844.6624\n",
      "Epoch 451/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1765 - val_loss: 844.6622\n",
      "Epoch 452/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1765 - val_loss: 844.6620\n",
      "Epoch 453/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1766 - val_loss: 844.6620\n",
      "Epoch 454/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1767 - val_loss: 844.6614\n",
      "Epoch 455/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1768 - val_loss: 844.6614\n",
      "Epoch 456/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1768 - val_loss: 844.6610\n",
      "Epoch 457/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1769 - val_loss: 844.6610\n",
      "Epoch 458/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1769 - val_loss: 844.6608\n",
      "Epoch 459/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 24.1770 - val_loss: 844.6607\n",
      "Epoch 460/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1770 - val_loss: 844.6604\n",
      "Epoch 461/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1771 - val_loss: 844.6601\n",
      "Epoch 462/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1772 - val_loss: 844.6601\n",
      "Epoch 463/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1772 - val_loss: 844.6598\n",
      "Epoch 464/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1772 - val_loss: 844.6597\n",
      "Epoch 465/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1773 - val_loss: 844.6597\n",
      "Epoch 466/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1773 - val_loss: 844.6594\n",
      "Epoch 467/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1774 - val_loss: 844.6595\n",
      "Epoch 468/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 24.1774 - val_loss: 844.6591\n",
      "Epoch 469/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1775 - val_loss: 844.6587\n",
      "Epoch 470/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1775 - val_loss: 844.6587\n",
      "Epoch 471/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1776 - val_loss: 844.6585\n",
      "Epoch 472/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1776 - val_loss: 844.6585\n",
      "Epoch 473/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1777 - val_loss: 844.6584\n",
      "Epoch 474/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1777 - val_loss: 844.6585\n",
      "Epoch 475/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 24.1777 - val_loss: 844.6581\n",
      "Epoch 476/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1778 - val_loss: 844.6581\n",
      "Epoch 477/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1778 - val_loss: 844.6581\n",
      "Epoch 478/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1779 - val_loss: 844.6581\n",
      "Epoch 479/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1779 - val_loss: 844.6577\n",
      "Epoch 480/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 24.1779 - val_loss: 844.6575\n",
      "Epoch 481/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1780 - val_loss: 844.6573\n",
      "Epoch 482/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1780 - val_loss: 844.6573\n",
      "Epoch 483/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1781 - val_loss: 844.6573\n",
      "Epoch 484/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1780 - val_loss: 844.6572\n",
      "Epoch 485/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1781 - val_loss: 844.6569\n",
      "Epoch 486/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1781 - val_loss: 844.6566\n",
      "Epoch 487/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1781 - val_loss: 844.6563\n",
      "Epoch 488/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1782 - val_loss: 844.6562\n",
      "Epoch 489/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1782 - val_loss: 844.6565\n",
      "Epoch 490/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1782 - val_loss: 844.6565\n",
      "Epoch 491/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1783 - val_loss: 844.6564\n",
      "Epoch 492/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1783 - val_loss: 844.6564\n",
      "Epoch 493/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1783 - val_loss: 844.6563\n",
      "Epoch 494/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1783 - val_loss: 844.6562\n",
      "Epoch 495/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1784 - val_loss: 844.6560\n",
      "Epoch 496/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.1784 - val_loss: 844.6560\n",
      "Epoch 497/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1784 - val_loss: 844.6558\n",
      "Epoch 498/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 24.1784 - val_loss: 844.6557\n",
      "Epoch 499/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1785 - val_loss: 844.6556\n",
      "Epoch 500/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1785 - val_loss: 844.6555\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(85.7336, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0020, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0125, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3008, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 351ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.61918417e+01, 6.59523459e+01, 6.57174370e+01, 6.54885462e+01,\n",
       "        6.52586555e+01, 0.00000000e+00, 0.00000000e+00, 5.13586121e+01,\n",
       "        3.59793186e-01, 0.00000000e+00, 2.88133472e-01, 1.43447533e-01,\n",
       "        0.00000000e+00, 6.41968254e+01, 6.41632120e+01, 6.41295985e+01,\n",
       "        6.40959851e+01, 6.40623716e+01, 6.40287582e+01, 6.39951447e+01,\n",
       "        6.39615313e+01, 6.39279178e+01, 6.38829132e+01, 6.37820728e+01,\n",
       "        6.36812325e+01, 6.35803922e+01, 6.34795518e+01, 6.33787115e+01,\n",
       "        6.32778712e+01, 6.31770308e+01, 6.30761905e+01, 6.29753501e+01,\n",
       "        6.28745098e+01, 6.27736695e+01, 6.26886788e+01, 6.26466620e+01,\n",
       "        6.26046452e+01, 6.25626284e+01, 6.25206116e+01, 6.24785948e+01,\n",
       "        0.00000000e+00, 1.80943000e-01, 2.56645020e-01, 9.16033500e-02,\n",
       "        1.86248840e-01, 1.46881070e-01, 4.27304360e-01, 6.36028011e+01,\n",
       "        6.35019608e+01, 6.34011205e+01, 6.33002801e+01, 6.31994398e+01,\n",
       "        6.30985994e+01, 6.29977591e+01, 6.28969188e+01, 6.27960784e+01,\n",
       "        6.26980159e+01, 6.26559991e+01, 6.26139823e+01, 6.25719655e+01,\n",
       "        6.25299487e+01, 6.24879318e+01, 6.24459150e+01, 6.24038982e+01,\n",
       "        6.23618814e+01, 6.23198646e+01, 6.22778478e+01, 6.22358310e+01,\n",
       "        6.21876284e+01, 6.21035948e+01, 6.20195612e+01, 6.19355275e+01,\n",
       "        6.18514939e+01, 7.00675430e+01, 3.55150253e-01, 1.08446628e-01,\n",
       "        0.00000000e+00, 2.78546482e-01, 2.63000011e-01, 1.53639470e-02,\n",
       "        5.85645638e+01, 7.74735451e-01, 0.00000000e+00, 3.60960543e-01,\n",
       "        2.70093322e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.78241694e-01, 0.00000000e+00, 0.00000000e+00, 8.31274509e-01,\n",
       "        0.00000000e+00, 8.50868598e-02, 4.89992410e-01, 3.62192243e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.67188540e-01, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([57.54554212, 57.53652388, 57.52750564, 57.51848739, 57.50946915,\n",
       "       57.50045091, 57.49143267, 57.48241443, 57.47339619, 57.46437795,\n",
       "       57.4553597 , 57.44634146, 57.43732322, 57.42830498, 57.41928674,\n",
       "       57.4102685 , 57.40125026, 57.39223201, 57.38321377, 57.37419553,\n",
       "       57.36517729, 57.35615905, 57.34714081, 57.33812257, 57.32910432,\n",
       "       57.32008608, 57.31106784, 57.3020496 , 57.29303136, 57.28401312,\n",
       "       57.27499488, 57.26597663, 57.25695839, 57.24794015, 57.23892191,\n",
       "       57.22990367, 57.22088543, 57.21186719, 57.20284894, 57.1938307 ,\n",
       "       57.18481246, 57.17579422, 57.16677598, 57.15775774, 57.1487395 ,\n",
       "       57.13972125, 57.13070301, 57.12168477, 57.11266653, 57.10364829,\n",
       "       57.09463005, 57.08561181, 57.07659356, 57.06757532, 57.05855708,\n",
       "       57.04953884, 57.0405206 , 57.03150236, 57.02248412, 57.01346587,\n",
       "       57.00444763, 56.99542939, 56.98641115, 56.97739291, 56.96837467,\n",
       "       56.95935643, 56.95033818, 56.94131994, 56.9323017 , 56.92328346,\n",
       "       56.91426522, 56.90524698, 56.89622874, 56.88721049, 56.87819225,\n",
       "       56.86917401, 56.86015577, 56.85113753, 56.84211929, 56.83310105,\n",
       "       56.8240828 , 56.81506456, 56.80604632, 56.79702808, 56.78800984,\n",
       "       56.7789916 , 56.76997336, 56.76095511, 56.75193687, 56.74291863,\n",
       "       56.73390039, 56.72488215, 56.71586391, 56.70684567, 56.69782742,\n",
       "       56.68880918, 56.67979094, 56.6707727 , 56.66175446, 56.65273622])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.79999731928521\n",
      "25.876124135969707\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
