{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "2245    57.590633\n",
       "2246    57.581615\n",
       "2247    57.572597\n",
       "2248    57.563579\n",
       "2249    57.554560\n",
       "Name: C8, Length: 2250, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c8_interpolated_2150_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "2145     0.120867\n",
       "2146     0.164191\n",
       "2147     0.327420\n",
       "2148     0.000000\n",
       "2149     0.150736\n",
       "Name: C8, Length: 2150, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2150)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.987115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.774230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.561345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.348459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.200000  0.000298\n",
       "1     87.987115  0.000298\n",
       "2     87.774230  0.000297\n",
       "3     87.561345  0.000297\n",
       "4     87.348459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 85.7336  # Value for C0\n",
    "K0 = -0.0020  # Value for K0\n",
    "K1 = 0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0125    # Value for b\n",
    "c = 2.3008    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.200000    87.987115    87.774230    87.561345    87.348459   \n",
      "351    87.987115    87.774230    87.561345    87.348459    87.135574   \n",
      "352    87.774230    87.561345    87.348459    87.135574    86.922689   \n",
      "353    87.561345    87.348459    87.135574    86.922689    86.709804   \n",
      "354    87.348459    87.135574    86.922689    86.709804    86.496919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.135574    86.922689    86.709804    86.496919    86.294538  ...   \n",
      "351    86.922689    86.709804    86.496919    86.294538    86.221709  ...   \n",
      "352    86.709804    86.496919    86.294538    86.221709    86.148880  ...   \n",
      "353    86.496919    86.294538    86.221709    86.148880    86.076050  ...   \n",
      "354    86.294538    86.221709    86.148880    86.076050    86.003221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   76.094351    0.000263   76.058870    0.000263   76.023389    0.000263   \n",
      "351   76.058870    0.000263   76.023389    0.000263   75.987909    0.000262   \n",
      "352   76.023389    0.000263   75.987909    0.000262   75.952428    0.000262   \n",
      "353   75.987909    0.000262   75.952428    0.000262   75.916947    0.000262   \n",
      "354   75.952428    0.000262   75.916947    0.000262   75.881466    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   75.987909    0.000262   75.952428    0.000262  \n",
      "351   75.952428    0.000262   75.916947    0.000262  \n",
      "352   75.916947    0.000262   75.881466    0.000262  \n",
      "353   75.881466    0.000262   75.845985    0.000262  \n",
      "354   75.845985    0.000262   75.810504    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1700, 1, 251) (1700, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 2s 22ms/step - loss: 4766.5322 - val_loss: 3280.0466\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4675.6138 - val_loss: 3230.2405\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4606.6465 - val_loss: 3181.4490\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4538.4692 - val_loss: 3133.3596\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4471.1602 - val_loss: 3085.9238\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 4402.6118 - val_loss: 3035.3945\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 4315.6938 - val_loss: 2973.1797\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4241.3838 - val_loss: 2922.3225\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4169.9990 - val_loss: 2872.9331\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4100.3628 - val_loss: 2824.6895\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4031.1772 - val_loss: 2773.4749\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3955.0015 - val_loss: 2723.8613\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3883.3643 - val_loss: 2675.6602\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3813.8816 - val_loss: 2628.7427\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3746.1230 - val_loss: 2582.8794\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3679.7690 - val_loss: 2537.9424\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3614.6423 - val_loss: 2493.8545\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3550.6316 - val_loss: 2450.5645\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3487.6655 - val_loss: 2408.0347\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3425.6880 - val_loss: 2366.2371\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3364.6592 - val_loss: 2325.1487\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3304.5457 - val_loss: 2284.7502\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3245.3206 - val_loss: 2245.0256\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3186.9617 - val_loss: 2205.9604\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3129.4482 - val_loss: 2167.5422\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3072.7622 - val_loss: 2129.7588\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3016.8887 - val_loss: 2092.6006\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2961.8132 - val_loss: 2056.0566\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2907.5225 - val_loss: 2020.1184\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 2854.0042 - val_loss: 1984.7770\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2801.2473 - val_loss: 1950.0243\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2749.2412 - val_loss: 1915.8528\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2697.9751 - val_loss: 1882.2543\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2647.4397 - val_loss: 1849.2217\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2597.6255 - val_loss: 1816.7483\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2548.5239 - val_loss: 1784.8271\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2500.1262 - val_loss: 1753.4518\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2452.4241 - val_loss: 1722.6155\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2405.4097 - val_loss: 1692.3123\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2359.0740 - val_loss: 1662.5353\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2313.4109 - val_loss: 1633.2792\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2268.4124 - val_loss: 1604.5381\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2224.0703 - val_loss: 1576.3055\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2180.3792 - val_loss: 1548.5763\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2137.3303 - val_loss: 1521.3445\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2094.9177 - val_loss: 1494.6047\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2053.1348 - val_loss: 1468.3517\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2011.9742 - val_loss: 1442.5797\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1971.4302 - val_loss: 1417.2834\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1931.4957 - val_loss: 1392.4576\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1892.1649 - val_loss: 1368.0975\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1853.4313 - val_loss: 1344.1973\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1815.2886 - val_loss: 1320.7523\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1777.7307 - val_loss: 1297.7568\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1740.7511 - val_loss: 1275.2069\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1704.3448 - val_loss: 1253.0969\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1668.5055 - val_loss: 1231.4224\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1633.2275 - val_loss: 1210.1780\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1598.5048 - val_loss: 1189.3589\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1564.3314 - val_loss: 1168.9604\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1530.7021 - val_loss: 1148.9779\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1497.6108 - val_loss: 1129.4066\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1465.0524 - val_loss: 1110.2418\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1433.0206 - val_loss: 1091.4788\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1401.5111 - val_loss: 1073.1135\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1370.5179 - val_loss: 1055.1406\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1340.0363 - val_loss: 1037.5562\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1310.0593 - val_loss: 1020.3546\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1280.5823 - val_loss: 1003.5325\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1251.6006 - val_loss: 987.0849\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1223.1086 - val_loss: 971.0075\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1195.1017 - val_loss: 955.2961\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1167.5742 - val_loss: 939.9462\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1140.5206 - val_loss: 924.9531\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1113.9365 - val_loss: 910.3128\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1087.8168 - val_loss: 896.0207\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1062.1560 - val_loss: 882.0727\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1036.9496 - val_loss: 868.4646\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1012.1923 - val_loss: 855.1918\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 987.8797 - val_loss: 842.2505\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 964.0060 - val_loss: 829.6359\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 940.5672 - val_loss: 817.3445\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 917.5576 - val_loss: 805.3710\n",
      "Epoch 84/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 894.9724 - val_loss: 793.7120\n",
      "Epoch 85/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 872.8076 - val_loss: 782.3637\n",
      "Epoch 86/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 851.0580 - val_loss: 771.3214\n",
      "Epoch 87/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 829.7189 - val_loss: 760.5810\n",
      "Epoch 88/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 808.7855 - val_loss: 750.1385\n",
      "Epoch 89/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 788.2531 - val_loss: 739.9901\n",
      "Epoch 90/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 768.1171 - val_loss: 730.1309\n",
      "Epoch 91/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 748.3728 - val_loss: 720.5577\n",
      "Epoch 92/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 729.0157 - val_loss: 711.2661\n",
      "Epoch 93/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 710.0408 - val_loss: 702.2522\n",
      "Epoch 94/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 691.4437 - val_loss: 693.5116\n",
      "Epoch 95/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 673.2200 - val_loss: 685.0408\n",
      "Epoch 96/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 655.3649 - val_loss: 676.8353\n",
      "Epoch 97/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 637.8737 - val_loss: 668.8917\n",
      "Epoch 98/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 620.7419 - val_loss: 661.2051\n",
      "Epoch 99/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 603.9651 - val_loss: 653.7724\n",
      "Epoch 100/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 587.5391 - val_loss: 646.5895\n",
      "Epoch 101/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 571.4592 - val_loss: 639.6522\n",
      "Epoch 102/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 555.7206 - val_loss: 632.9566\n",
      "Epoch 103/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 540.3192 - val_loss: 626.4993\n",
      "Epoch 104/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 525.2508 - val_loss: 620.2757\n",
      "Epoch 105/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 510.5104 - val_loss: 614.2825\n",
      "Epoch 106/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 496.0945 - val_loss: 608.5154\n",
      "Epoch 107/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 481.9980 - val_loss: 602.9708\n",
      "Epoch 108/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 468.2164 - val_loss: 597.6447\n",
      "Epoch 109/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 454.7458 - val_loss: 592.5334\n",
      "Epoch 110/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 441.5822 - val_loss: 587.6331\n",
      "Epoch 111/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 428.7206 - val_loss: 582.9397\n",
      "Epoch 112/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 416.1569 - val_loss: 578.4496\n",
      "Epoch 113/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 403.8869 - val_loss: 574.1590\n",
      "Epoch 114/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 391.9063 - val_loss: 570.0643\n",
      "Epoch 115/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 380.2108 - val_loss: 566.1613\n",
      "Epoch 116/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 368.7962 - val_loss: 562.4467\n",
      "Epoch 117/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 357.6583 - val_loss: 558.9164\n",
      "Epoch 118/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 346.7929 - val_loss: 555.5669\n",
      "Epoch 119/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 336.1960 - val_loss: 552.3945\n",
      "Epoch 120/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 325.8634 - val_loss: 549.3954\n",
      "Epoch 121/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 315.7907 - val_loss: 546.5660\n",
      "Epoch 122/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 305.9737 - val_loss: 543.9026\n",
      "Epoch 123/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 296.4087 - val_loss: 541.4016\n",
      "Epoch 124/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 287.0915 - val_loss: 539.0590\n",
      "Epoch 125/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 278.0175 - val_loss: 536.8718\n",
      "Epoch 126/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 269.1830 - val_loss: 534.8359\n",
      "Epoch 127/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 260.5844 - val_loss: 532.9479\n",
      "Epoch 128/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 252.2170 - val_loss: 531.2043\n",
      "Epoch 129/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 244.0773 - val_loss: 529.6013\n",
      "Epoch 130/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 236.1608 - val_loss: 528.1357\n",
      "Epoch 131/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 228.4642 - val_loss: 526.8038\n",
      "Epoch 132/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 220.9830 - val_loss: 525.6020\n",
      "Epoch 133/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7131 - val_loss: 524.5269\n",
      "Epoch 134/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 206.6511 - val_loss: 523.5751\n",
      "Epoch 135/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 199.7932 - val_loss: 522.7432\n",
      "Epoch 136/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 193.1353 - val_loss: 522.0276\n",
      "Epoch 137/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 186.6734 - val_loss: 521.4250\n",
      "Epoch 138/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 180.4037 - val_loss: 520.9321\n",
      "Epoch 139/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 174.3224 - val_loss: 520.5453\n",
      "Epoch 140/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 168.4259 - val_loss: 520.2614\n",
      "Epoch 141/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 162.7102 - val_loss: 520.0771\n",
      "Epoch 142/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 157.1718 - val_loss: 519.9889\n",
      "Epoch 143/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 151.8072 - val_loss: 519.9927\n",
      "Epoch 144/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 146.6124 - val_loss: 520.0805\n",
      "Epoch 145/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 139.9211 - val_loss: 519.8427\n",
      "Epoch 146/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 132.8970 - val_loss: 520.0464\n",
      "Epoch 147/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 126.8205 - val_loss: 520.4044\n",
      "Epoch 148/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 121.2971 - val_loss: 520.8849\n",
      "Epoch 149/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 116.1791 - val_loss: 521.4700\n",
      "Epoch 150/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 111.3848 - val_loss: 522.1476\n",
      "Epoch 151/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 106.8658 - val_loss: 522.9088\n",
      "Epoch 152/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 102.5901 - val_loss: 523.7461\n",
      "Epoch 153/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 98.5347 - val_loss: 524.6536\n",
      "Epoch 154/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 94.6816 - val_loss: 525.6257\n",
      "Epoch 155/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 91.0167 - val_loss: 526.6573\n",
      "Epoch 156/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 87.5277 - val_loss: 527.7440\n",
      "Epoch 157/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 84.2045 - val_loss: 528.8814\n",
      "Epoch 158/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 81.0381 - val_loss: 530.0657\n",
      "Epoch 159/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 78.0201 - val_loss: 531.2928\n",
      "Epoch 160/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 75.1437 - val_loss: 532.5593\n",
      "Epoch 161/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 72.4016 - val_loss: 533.8619\n",
      "Epoch 162/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 69.7881 - val_loss: 535.1971\n",
      "Epoch 163/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 67.2974 - val_loss: 536.5620\n",
      "Epoch 164/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 64.9239 - val_loss: 537.9534\n",
      "Epoch 165/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 62.6625 - val_loss: 539.3687\n",
      "Epoch 166/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 60.5084 - val_loss: 540.8047\n",
      "Epoch 167/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 58.4574 - val_loss: 542.2591\n",
      "Epoch 168/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 56.5048 - val_loss: 543.7291\n",
      "Epoch 169/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 54.6465 - val_loss: 545.2123\n",
      "Epoch 170/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 52.8787 - val_loss: 546.7064\n",
      "Epoch 171/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 51.1975 - val_loss: 548.2091\n",
      "Epoch 172/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 49.5994 - val_loss: 549.7183\n",
      "Epoch 173/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 48.0806 - val_loss: 551.2317\n",
      "Epoch 174/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 46.6380 - val_loss: 552.7475\n",
      "Epoch 175/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 45.2681 - val_loss: 554.2637\n",
      "Epoch 176/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 43.9680 - val_loss: 555.7784\n",
      "Epoch 177/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 42.7346 - val_loss: 557.2899\n",
      "Epoch 178/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 41.5652 - val_loss: 558.7966\n",
      "Epoch 179/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 40.4566 - val_loss: 560.2966\n",
      "Epoch 180/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 39.4065 - val_loss: 561.7888\n",
      "Epoch 181/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 38.4121 - val_loss: 563.2713\n",
      "Epoch 182/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 37.4710 - val_loss: 564.7429\n",
      "Epoch 183/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 36.5808 - val_loss: 566.2020\n",
      "Epoch 184/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 35.7391 - val_loss: 567.6481\n",
      "Epoch 185/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 34.9437 - val_loss: 569.0793\n",
      "Epoch 186/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 34.1924 - val_loss: 570.4950\n",
      "Epoch 187/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.4830 - val_loss: 571.8938\n",
      "Epoch 188/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.8138 - val_loss: 573.2748\n",
      "Epoch 189/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.1827 - val_loss: 574.6373\n",
      "Epoch 190/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.5878 - val_loss: 575.9802\n",
      "Epoch 191/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.0275 - val_loss: 577.3028\n",
      "Epoch 192/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 30.4999 - val_loss: 578.6042\n",
      "Epoch 193/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 30.0034 - val_loss: 579.8842\n",
      "Epoch 194/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 29.5364 - val_loss: 581.1420\n",
      "Epoch 195/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 29.0975 - val_loss: 582.3770\n",
      "Epoch 196/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 28.6850 - val_loss: 583.5887\n",
      "Epoch 197/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 28.2977 - val_loss: 584.7767\n",
      "Epoch 198/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 27.9343 - val_loss: 585.9407\n",
      "Epoch 199/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 27.5934 - val_loss: 587.0804\n",
      "Epoch 200/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 27.2737 - val_loss: 588.1953\n",
      "Epoch 201/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 26.9743 - val_loss: 589.2851\n",
      "Epoch 202/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.6938 - val_loss: 590.3503\n",
      "Epoch 203/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.4312 - val_loss: 591.3903\n",
      "Epoch 204/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.1856 - val_loss: 592.4048\n",
      "Epoch 205/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 25.9559 - val_loss: 593.3937\n",
      "Epoch 206/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 25.7413 - val_loss: 594.3574\n",
      "Epoch 207/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 25.5409 - val_loss: 595.2958\n",
      "Epoch 208/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 25.3537 - val_loss: 596.2087\n",
      "Epoch 209/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 25.1791 - val_loss: 597.0968\n",
      "Epoch 210/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 25.0161 - val_loss: 597.9595\n",
      "Epoch 211/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.8642 - val_loss: 598.7972\n",
      "Epoch 212/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 24.7228 - val_loss: 599.6102\n",
      "Epoch 213/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.5910 - val_loss: 600.3988\n",
      "Epoch 214/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 24.4683 - val_loss: 601.1632\n",
      "Epoch 215/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 24.3541 - val_loss: 601.9038\n",
      "Epoch 216/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.2479 - val_loss: 602.6205\n",
      "Epoch 217/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.1492 - val_loss: 603.3143\n",
      "Epoch 218/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.0573 - val_loss: 603.9844\n",
      "Epoch 219/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.9721 - val_loss: 604.6320\n",
      "Epoch 220/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.8929 - val_loss: 605.2576\n",
      "Epoch 221/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.8194 - val_loss: 605.8610\n",
      "Epoch 222/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.7512 - val_loss: 606.4431\n",
      "Epoch 223/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.6879 - val_loss: 607.0038\n",
      "Epoch 224/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.6292 - val_loss: 607.5439\n",
      "Epoch 225/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.5748 - val_loss: 608.0642\n",
      "Epoch 226/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.5243 - val_loss: 608.5646\n",
      "Epoch 227/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.4776 - val_loss: 609.0455\n",
      "Epoch 228/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.4343 - val_loss: 609.5074\n",
      "Epoch 229/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.3942 - val_loss: 609.9508\n",
      "Epoch 230/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.3571 - val_loss: 610.3766\n",
      "Epoch 231/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.3227 - val_loss: 610.7849\n",
      "Epoch 232/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.2909 - val_loss: 611.1761\n",
      "Epoch 233/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.2615 - val_loss: 611.5511\n",
      "Epoch 234/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.2342 - val_loss: 611.9099\n",
      "Epoch 235/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 23.2090 - val_loss: 612.2531\n",
      "Epoch 236/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.1858 - val_loss: 612.5811\n",
      "Epoch 237/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.1642 - val_loss: 612.8944\n",
      "Epoch 238/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.1443 - val_loss: 613.1935\n",
      "Epoch 239/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.1259 - val_loss: 613.4789\n",
      "Epoch 240/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.1089 - val_loss: 613.7513\n",
      "Epoch 241/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.0932 - val_loss: 614.0107\n",
      "Epoch 242/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.0787 - val_loss: 614.2579\n",
      "Epoch 243/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.0653 - val_loss: 614.4929\n",
      "Epoch 244/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.0529 - val_loss: 614.7166\n",
      "Epoch 245/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.0415 - val_loss: 614.9292\n",
      "Epoch 246/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.0310 - val_loss: 615.1312\n",
      "Epoch 247/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.0212 - val_loss: 615.3230\n",
      "Epoch 248/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.0123 - val_loss: 615.5051\n",
      "Epoch 249/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.0040 - val_loss: 615.6778\n",
      "Epoch 250/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9963 - val_loss: 615.8409\n",
      "Epoch 251/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9894 - val_loss: 615.9959\n",
      "Epoch 252/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9829 - val_loss: 616.1428\n",
      "Epoch 253/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9770 - val_loss: 616.2812\n",
      "Epoch 254/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 22.9714 - val_loss: 616.4119\n",
      "Epoch 255/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9664 - val_loss: 616.5358\n",
      "Epoch 256/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9618 - val_loss: 616.6526\n",
      "Epoch 257/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9576 - val_loss: 616.7628\n",
      "Epoch 258/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9537 - val_loss: 616.8665\n",
      "Epoch 259/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9502 - val_loss: 616.9645\n",
      "Epoch 260/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9469 - val_loss: 617.0566\n",
      "Epoch 261/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9440 - val_loss: 617.1433\n",
      "Epoch 262/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9413 - val_loss: 617.2248\n",
      "Epoch 263/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9388 - val_loss: 617.3012\n",
      "Epoch 264/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9366 - val_loss: 617.3730\n",
      "Epoch 265/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9346 - val_loss: 617.4401\n",
      "Epoch 266/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9328 - val_loss: 617.5032\n",
      "Epoch 267/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9311 - val_loss: 617.5620\n",
      "Epoch 268/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9297 - val_loss: 617.6171\n",
      "Epoch 269/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9284 - val_loss: 617.6686\n",
      "Epoch 270/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9273 - val_loss: 617.7167\n",
      "Epoch 271/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9263 - val_loss: 617.7620\n",
      "Epoch 272/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 22.9254 - val_loss: 617.8036\n",
      "Epoch 273/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9247 - val_loss: 617.8428\n",
      "Epoch 274/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9240 - val_loss: 617.8790\n",
      "Epoch 275/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9235 - val_loss: 617.9132\n",
      "Epoch 276/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9230 - val_loss: 617.9446\n",
      "Epoch 277/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9226 - val_loss: 617.9738\n",
      "Epoch 278/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9224 - val_loss: 618.0007\n",
      "Epoch 279/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9222 - val_loss: 618.0261\n",
      "Epoch 280/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9221 - val_loss: 618.0494\n",
      "Epoch 281/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9221 - val_loss: 618.0712\n",
      "Epoch 282/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9220 - val_loss: 618.0908\n",
      "Epoch 283/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9222 - val_loss: 618.1092\n",
      "Epoch 284/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9222 - val_loss: 618.1259\n",
      "Epoch 285/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9224 - val_loss: 618.1415\n",
      "Epoch 286/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9226 - val_loss: 618.1555\n",
      "Epoch 287/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9229 - val_loss: 618.1685\n",
      "Epoch 288/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9233 - val_loss: 618.1807\n",
      "Epoch 289/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 22.9236 - val_loss: 618.1915\n",
      "Epoch 290/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.9239 - val_loss: 618.2015\n",
      "Epoch 291/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9244 - val_loss: 618.2104\n",
      "Epoch 292/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9248 - val_loss: 618.2185\n",
      "Epoch 293/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9253 - val_loss: 618.2262\n",
      "Epoch 294/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9258 - val_loss: 618.2326\n",
      "Epoch 295/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9262 - val_loss: 618.2387\n",
      "Epoch 296/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9268 - val_loss: 618.2443\n",
      "Epoch 297/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9273 - val_loss: 618.2490\n",
      "Epoch 298/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9279 - val_loss: 618.2534\n",
      "Epoch 299/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9285 - val_loss: 618.2573\n",
      "Epoch 300/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9291 - val_loss: 618.2607\n",
      "Epoch 301/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9297 - val_loss: 618.2637\n",
      "Epoch 302/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9303 - val_loss: 618.2660\n",
      "Epoch 303/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9309 - val_loss: 618.2679\n",
      "Epoch 304/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9316 - val_loss: 618.2697\n",
      "Epoch 305/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9322 - val_loss: 618.2711\n",
      "Epoch 306/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9328 - val_loss: 618.2722\n",
      "Epoch 307/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9335 - val_loss: 618.2733\n",
      "Epoch 308/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9342 - val_loss: 618.2738\n",
      "Epoch 309/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 22.9348 - val_loss: 618.2743\n",
      "Epoch 310/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9355 - val_loss: 618.2746\n",
      "Epoch 311/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9362 - val_loss: 618.2747\n",
      "Epoch 312/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9369 - val_loss: 618.2751\n",
      "Epoch 313/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9375 - val_loss: 618.2749\n",
      "Epoch 314/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9382 - val_loss: 618.2747\n",
      "Epoch 315/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9388 - val_loss: 618.2744\n",
      "Epoch 316/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9395 - val_loss: 618.2739\n",
      "Epoch 317/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9401 - val_loss: 618.2731\n",
      "Epoch 318/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9408 - val_loss: 618.2723\n",
      "Epoch 319/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9415 - val_loss: 618.2716\n",
      "Epoch 320/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9421 - val_loss: 618.2707\n",
      "Epoch 321/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9428 - val_loss: 618.2695\n",
      "Epoch 322/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9435 - val_loss: 618.2686\n",
      "Epoch 323/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9441 - val_loss: 618.2676\n",
      "Epoch 324/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9448 - val_loss: 618.2665\n",
      "Epoch 325/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9453 - val_loss: 618.2651\n",
      "Epoch 326/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9460 - val_loss: 618.2637\n",
      "Epoch 327/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 22.9466 - val_loss: 618.2626\n",
      "Epoch 328/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.9473 - val_loss: 618.2611\n",
      "Epoch 329/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9479 - val_loss: 618.2599\n",
      "Epoch 330/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9485 - val_loss: 618.2587\n",
      "Epoch 331/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9491 - val_loss: 618.2571\n",
      "Epoch 332/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9497 - val_loss: 618.2560\n",
      "Epoch 333/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9503 - val_loss: 618.2546\n",
      "Epoch 334/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9509 - val_loss: 618.2533\n",
      "Epoch 335/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9515 - val_loss: 618.2520\n",
      "Epoch 336/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9521 - val_loss: 618.2508\n",
      "Epoch 337/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9527 - val_loss: 618.2495\n",
      "Epoch 338/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9532 - val_loss: 618.2482\n",
      "Epoch 339/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9538 - val_loss: 618.2466\n",
      "Epoch 340/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9543 - val_loss: 618.2453\n",
      "Epoch 341/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9549 - val_loss: 618.2438\n",
      "Epoch 342/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9555 - val_loss: 618.2424\n",
      "Epoch 343/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9560 - val_loss: 618.2411\n",
      "Epoch 344/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 22.9565 - val_loss: 618.2398\n",
      "Epoch 345/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9570 - val_loss: 618.2382\n",
      "Epoch 346/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.9576 - val_loss: 618.2369\n",
      "Epoch 347/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9581 - val_loss: 618.2356\n",
      "Epoch 348/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9586 - val_loss: 618.2344\n",
      "Epoch 349/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9590 - val_loss: 618.2329\n",
      "Epoch 350/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9595 - val_loss: 618.2313\n",
      "Epoch 351/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.9600 - val_loss: 618.2299\n",
      "Epoch 352/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9606 - val_loss: 618.2289\n",
      "Epoch 353/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9610 - val_loss: 618.2278\n",
      "Epoch 354/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9615 - val_loss: 618.2264\n",
      "Epoch 355/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9619 - val_loss: 618.2254\n",
      "Epoch 356/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9623 - val_loss: 618.2239\n",
      "Epoch 357/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9628 - val_loss: 618.2228\n",
      "Epoch 358/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 22.9633 - val_loss: 618.2218\n",
      "Epoch 359/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9637 - val_loss: 618.2207\n",
      "Epoch 360/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9641 - val_loss: 618.2195\n",
      "Epoch 361/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9645 - val_loss: 618.2186\n",
      "Epoch 362/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9649 - val_loss: 618.2175\n",
      "Epoch 363/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9653 - val_loss: 618.2164\n",
      "Epoch 364/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9658 - val_loss: 618.2155\n",
      "Epoch 365/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9661 - val_loss: 618.2142\n",
      "Epoch 366/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.9665 - val_loss: 618.2133\n",
      "Epoch 367/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9669 - val_loss: 618.2120\n",
      "Epoch 368/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.9673 - val_loss: 618.2111\n",
      "Epoch 369/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9677 - val_loss: 618.2099\n",
      "Epoch 370/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9680 - val_loss: 618.2088\n",
      "Epoch 371/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9684 - val_loss: 618.2078\n",
      "Epoch 372/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9687 - val_loss: 618.2066\n",
      "Epoch 373/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9691 - val_loss: 618.2057\n",
      "Epoch 374/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 22.9694 - val_loss: 618.2047\n",
      "Epoch 375/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9698 - val_loss: 618.2039\n",
      "Epoch 376/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9701 - val_loss: 618.2029\n",
      "Epoch 377/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9704 - val_loss: 618.2019\n",
      "Epoch 378/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9707 - val_loss: 618.2011\n",
      "Epoch 379/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.9710 - val_loss: 618.2001\n",
      "Epoch 380/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9713 - val_loss: 618.1992\n",
      "Epoch 381/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.9717 - val_loss: 618.1986\n",
      "Epoch 382/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9720 - val_loss: 618.1981\n",
      "Epoch 383/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.9722 - val_loss: 618.1970\n",
      "Epoch 384/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9725 - val_loss: 618.1964\n",
      "Epoch 385/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9728 - val_loss: 618.1957\n",
      "Epoch 386/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9731 - val_loss: 618.1946\n",
      "Epoch 387/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 22.9734 - val_loss: 618.1939\n",
      "Epoch 388/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9736 - val_loss: 618.1933\n",
      "Epoch 389/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9739 - val_loss: 618.1922\n",
      "Epoch 390/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9741 - val_loss: 618.1915\n",
      "Epoch 391/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9744 - val_loss: 618.1908\n",
      "Epoch 392/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9747 - val_loss: 618.1900\n",
      "Epoch 393/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9749 - val_loss: 618.1891\n",
      "Epoch 394/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9752 - val_loss: 618.1887\n",
      "Epoch 395/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9754 - val_loss: 618.1882\n",
      "Epoch 396/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9756 - val_loss: 618.1873\n",
      "Epoch 397/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9759 - val_loss: 618.1868\n",
      "Epoch 398/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9761 - val_loss: 618.1862\n",
      "Epoch 399/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9763 - val_loss: 618.1857\n",
      "Epoch 400/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9765 - val_loss: 618.1852\n",
      "Epoch 401/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 22.9767 - val_loss: 618.1844\n",
      "Epoch 402/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9769 - val_loss: 618.1838\n",
      "Epoch 403/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.9771 - val_loss: 618.1833\n",
      "Epoch 404/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9773 - val_loss: 618.1826\n",
      "Epoch 405/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9775 - val_loss: 618.1820\n",
      "Epoch 406/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9777 - val_loss: 618.1815\n",
      "Epoch 407/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9779 - val_loss: 618.1814\n",
      "Epoch 408/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9781 - val_loss: 618.1809\n",
      "Epoch 409/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9783 - val_loss: 618.1802\n",
      "Epoch 410/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9784 - val_loss: 618.1796\n",
      "Epoch 411/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9786 - val_loss: 618.1788\n",
      "Epoch 412/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9788 - val_loss: 618.1785\n",
      "Epoch 413/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9790 - val_loss: 618.1780\n",
      "Epoch 414/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9791 - val_loss: 618.1774\n",
      "Epoch 415/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 22.9793 - val_loss: 618.1770\n",
      "Epoch 416/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.9795 - val_loss: 618.1766\n",
      "Epoch 417/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9796 - val_loss: 618.1762\n",
      "Epoch 418/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9797 - val_loss: 618.1758\n",
      "Epoch 419/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9799 - val_loss: 618.1755\n",
      "Epoch 420/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9800 - val_loss: 618.1751\n",
      "Epoch 421/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9802 - val_loss: 618.1747\n",
      "Epoch 422/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9803 - val_loss: 618.1742\n",
      "Epoch 423/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9805 - val_loss: 618.1737\n",
      "Epoch 424/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9806 - val_loss: 618.1732\n",
      "Epoch 425/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9807 - val_loss: 618.1726\n",
      "Epoch 426/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9809 - val_loss: 618.1722\n",
      "Epoch 427/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9810 - val_loss: 618.1718\n",
      "Epoch 428/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 22.9811 - val_loss: 618.1712\n",
      "Epoch 429/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9813 - val_loss: 618.1710\n",
      "Epoch 430/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.9814 - val_loss: 618.1707\n",
      "Epoch 431/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9815 - val_loss: 618.1705\n",
      "Epoch 432/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9816 - val_loss: 618.1701\n",
      "Epoch 433/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9817 - val_loss: 618.1697\n",
      "Epoch 434/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9818 - val_loss: 618.1693\n",
      "Epoch 435/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9819 - val_loss: 618.1689\n",
      "Epoch 436/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9820 - val_loss: 618.1683\n",
      "Epoch 437/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9822 - val_loss: 618.1683\n",
      "Epoch 438/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9822 - val_loss: 618.1679\n",
      "Epoch 439/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9823 - val_loss: 618.1675\n",
      "Epoch 440/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9825 - val_loss: 618.1674\n",
      "Epoch 441/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 22.9825 - val_loss: 618.1671\n",
      "Epoch 442/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 22.9826 - val_loss: 618.1667\n",
      "Epoch 443/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 22.9827 - val_loss: 618.1666\n",
      "Epoch 444/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.9828 - val_loss: 618.1661\n",
      "Epoch 445/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9829 - val_loss: 618.1659\n",
      "Epoch 446/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9829 - val_loss: 618.1653\n",
      "Epoch 447/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.9831 - val_loss: 618.1653\n",
      "Epoch 448/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9831 - val_loss: 618.1649\n",
      "Epoch 449/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.9832 - val_loss: 618.1646\n",
      "Epoch 450/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9833 - val_loss: 618.1644\n",
      "Epoch 451/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9834 - val_loss: 618.1642\n",
      "Epoch 452/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9834 - val_loss: 618.1637\n",
      "Epoch 453/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9835 - val_loss: 618.1633\n",
      "Epoch 454/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 22.9836 - val_loss: 618.1631\n",
      "Epoch 455/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.9837 - val_loss: 618.1627\n",
      "Epoch 456/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.9838 - val_loss: 618.1627\n",
      "Epoch 457/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9838 - val_loss: 618.1624\n",
      "Epoch 458/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9839 - val_loss: 618.1619\n",
      "Epoch 459/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9840 - val_loss: 618.1619\n",
      "Epoch 460/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9840 - val_loss: 618.1616\n",
      "Epoch 461/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9841 - val_loss: 618.1616\n",
      "Epoch 462/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9842 - val_loss: 618.1614\n",
      "Epoch 463/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9842 - val_loss: 618.1614\n",
      "Epoch 464/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9843 - val_loss: 618.1613\n",
      "Epoch 465/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9843 - val_loss: 618.1608\n",
      "Epoch 466/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9844 - val_loss: 618.1608\n",
      "Epoch 467/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 22.9844 - val_loss: 618.1603\n",
      "Epoch 468/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9844 - val_loss: 618.1600\n",
      "Epoch 469/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.9845 - val_loss: 618.1599\n",
      "Epoch 470/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.9845 - val_loss: 618.1595\n",
      "Epoch 471/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9847 - val_loss: 618.1593\n",
      "Epoch 472/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9847 - val_loss: 618.1593\n",
      "Epoch 473/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9847 - val_loss: 618.1592\n",
      "Epoch 474/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9848 - val_loss: 618.1591\n",
      "Epoch 475/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9849 - val_loss: 618.1591\n",
      "Epoch 476/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.9849 - val_loss: 618.1591\n",
      "Epoch 477/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9849 - val_loss: 618.1589\n",
      "Epoch 478/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9849 - val_loss: 618.1589\n",
      "Epoch 479/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9850 - val_loss: 618.1588\n",
      "Epoch 480/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9850 - val_loss: 618.1584\n",
      "Epoch 481/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9851 - val_loss: 618.1583\n",
      "Epoch 482/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9851 - val_loss: 618.1582\n",
      "Epoch 483/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9852 - val_loss: 618.1582\n",
      "Epoch 484/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9852 - val_loss: 618.1580\n",
      "Epoch 485/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9852 - val_loss: 618.1577\n",
      "Epoch 486/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 22.9853 - val_loss: 618.1575\n",
      "Epoch 487/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9853 - val_loss: 618.1573\n",
      "Epoch 488/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9854 - val_loss: 618.1573\n",
      "Epoch 489/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.9854 - val_loss: 618.1573\n",
      "Epoch 490/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9854 - val_loss: 618.1572\n",
      "Epoch 491/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9854 - val_loss: 618.1571\n",
      "Epoch 492/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9854 - val_loss: 618.1566\n",
      "Epoch 493/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9855 - val_loss: 618.1564\n",
      "Epoch 494/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9855 - val_loss: 618.1563\n",
      "Epoch 495/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.9856 - val_loss: 618.1563\n",
      "Epoch 496/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9856 - val_loss: 618.1562\n",
      "Epoch 497/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9856 - val_loss: 618.1558\n",
      "Epoch 498/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9856 - val_loss: 618.1554\n",
      "Epoch 499/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9857 - val_loss: 618.1553\n",
      "Epoch 500/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.9858 - val_loss: 618.1553\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(85.7336, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0020, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0125, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3008, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 416ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.21409430e+01, 6.21129318e+01, 6.20849206e+01, 6.20569094e+01,\n",
       "        6.20288982e+01, 6.20008870e+01, 6.19728758e+01, 6.19448646e+01,\n",
       "        6.19168534e+01, 6.18888422e+01, 6.18608310e+01, 6.18328198e+01,\n",
       "        6.18048086e+01, 6.77987745e+01, 6.73828081e+01, 6.69668417e+01,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.89225023e+01, 6.85191410e+01,\n",
       "        6.81068978e+01, 6.76909314e+01, 6.72749650e+01, 6.68589986e+01,\n",
       "        6.65732610e+01, 6.63337652e+01, 6.60942694e+01, 6.57682478e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.71671219e+01, 6.67511555e+01,\n",
       "        6.65111695e+01, 6.62716737e+01, 6.60321779e+01, 6.57927032e+01,\n",
       "        6.55651765e+01, 6.53352857e+01, 7.22844544e+01, 0.00000000e+00,\n",
       "        0.00000000e+00, 8.12122820e-02, 1.21052124e-01, 2.76572734e-01,\n",
       "        6.77054167e-01, 0.00000000e+00, 0.00000000e+00, 6.41744164e+01,\n",
       "        6.41408030e+01, 6.41071895e+01, 6.40735761e+01, 6.40399627e+01,\n",
       "        6.40063492e+01, 6.39727358e+01, 6.39391223e+01, 6.39055089e+01,\n",
       "        6.38156863e+01, 6.37148459e+01, 6.36140056e+01, 6.35131653e+01,\n",
       "        6.34123249e+01, 6.33114846e+01, 6.32106443e+01, 6.31098039e+01,\n",
       "        6.30089636e+01, 6.29081233e+01, 6.28072829e+01, 6.27064426e+01,\n",
       "        6.26606676e+01, 6.26186508e+01, 6.25766340e+01, 6.25346172e+01,\n",
       "        6.24926004e+01, 7.04044418e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.27657660e-01, 2.30339560e-01, 0.00000000e+00, 1.43897370e-01,\n",
       "        2.98922482e+01, 8.21304917e-01, 1.48800209e-01, 3.27416100e-02,\n",
       "        0.00000000e+00, 3.79147083e-02, 5.56176424e-01, 0.00000000e+00,\n",
       "        2.22418815e-01, 6.24328628e-02, 7.49041364e-02, 0.00000000e+00,\n",
       "        3.07723016e-01, 0.00000000e+00, 0.00000000e+00, 7.25348443e-02,\n",
       "        0.00000000e+00, 2.40430295e-01, 9.44726542e-02, 2.54502863e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58.44736626, 58.43834802, 58.42932978, 58.42031154, 58.4112933 ,\n",
       "       58.40227506, 58.39325681, 58.38423857, 58.37522033, 58.36620209,\n",
       "       58.35718385, 58.34816561, 58.33914737, 58.33012912, 58.32111088,\n",
       "       58.31209264, 58.3030744 , 58.29405616, 58.28503792, 58.27601968,\n",
       "       58.26700143, 58.25798319, 58.24896495, 58.23994671, 58.23092847,\n",
       "       58.22191023, 58.21289199, 58.20387374, 58.1948555 , 58.18583726,\n",
       "       58.17681902, 58.16780078, 58.15878254, 58.1497643 , 58.14074605,\n",
       "       58.13172781, 58.12270957, 58.11369133, 58.10467309, 58.09565485,\n",
       "       58.08663661, 58.07761836, 58.06860012, 58.05958188, 58.05056364,\n",
       "       58.0415454 , 58.03252716, 58.02350892, 58.01449067, 58.00547243,\n",
       "       57.99645419, 57.98743595, 57.97841771, 57.96939947, 57.96038123,\n",
       "       57.95136298, 57.94234474, 57.9333265 , 57.92430826, 57.91529002,\n",
       "       57.90627178, 57.89725354, 57.88823529, 57.87921705, 57.87019881,\n",
       "       57.86118057, 57.85216233, 57.84314409, 57.83412585, 57.8251076 ,\n",
       "       57.81608936, 57.80707112, 57.79805288, 57.78903464, 57.7800164 ,\n",
       "       57.77099816, 57.76197991, 57.75296167, 57.74394343, 57.73492519,\n",
       "       57.72590695, 57.71688871, 57.70787047, 57.69885222, 57.68983398,\n",
       "       57.68081574, 57.6717975 , 57.66277926, 57.65376102, 57.64474278,\n",
       "       57.63572453, 57.62670629, 57.61768805, 57.60866981, 57.59965157,\n",
       "       57.59063333, 57.58161509, 57.57259684, 57.5635786 , 57.55456036])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.06051469503974\n",
      "26.092189494363446\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
