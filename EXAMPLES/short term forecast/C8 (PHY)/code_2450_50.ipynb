{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "2495    55.336073\n",
       "2496    55.327055\n",
       "2497    55.318036\n",
       "2498    55.309018\n",
       "2499    55.300000\n",
       "Name: C8, Length: 2500, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c8_interpolated_2450_50.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "2445     0.696508\n",
       "2446     0.160441\n",
       "2447     0.000000\n",
       "2448     0.000000\n",
       "2449     0.181323\n",
       "Name: C8, Length: 2450, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(50)\n",
    "test\n",
    "training_set = training_set.head(2450)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.987115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.774230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.561345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.348459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.200000  0.000298\n",
       "1     87.987115  0.000298\n",
       "2     87.774230  0.000297\n",
       "3     87.561345  0.000297\n",
       "4     87.348459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 85.7336  # Value for C0\n",
    "K0 = -0.0020  # Value for K0\n",
    "K1 = 0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0125    # Value for b\n",
    "c = 2.3008    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-175)  var1(t-174)  var1(t-173)  var1(t-172)  var1(t-171)  \\\n",
      "175    88.200000    87.987115    87.774230    87.561345    87.348459   \n",
      "176    87.987115    87.774230    87.561345    87.348459    87.135574   \n",
      "177    87.774230    87.561345    87.348459    87.135574    86.922689   \n",
      "178    87.561345    87.348459    87.135574    86.922689    86.709804   \n",
      "179    87.348459    87.135574    86.922689    86.709804    86.496919   \n",
      "\n",
      "     var1(t-170)  var1(t-169)  var1(t-168)  var1(t-167)  var1(t-166)  ...  \\\n",
      "175    87.135574    86.922689    86.709804    86.496919    86.294538  ...   \n",
      "176    86.922689    86.709804    86.496919    86.294538    86.221709  ...   \n",
      "177    86.709804    86.496919    86.294538    86.221709    86.148880  ...   \n",
      "178    86.496919    86.294538    86.221709    86.148880    86.076050  ...   \n",
      "179    86.294538    86.221709    86.148880    86.076050    86.003221  ...   \n",
      "\n",
      "     var1(t+45)  var2(t+45)  var1(t+46)  var2(t+46)  var1(t+47)  var2(t+47)  \\\n",
      "175   79.697526    0.000280   79.676984    0.000280   79.656443    0.000280   \n",
      "176   79.676984    0.000280   79.656443    0.000280   79.635901    0.000280   \n",
      "177   79.656443    0.000280   79.635901    0.000280   79.615359    0.000279   \n",
      "178   79.635901    0.000280   79.615359    0.000279   79.594818    0.000279   \n",
      "179   79.615359    0.000279   79.594818    0.000279   79.574276    0.000279   \n",
      "\n",
      "     var1(t+48)  var2(t+48)  var1(t+49)  var2(t+49)  \n",
      "175   79.635901    0.000280   79.615359    0.000279  \n",
      "176   79.615359    0.000279   79.594818    0.000279  \n",
      "177   79.594818    0.000279   79.574276    0.000279  \n",
      "178   79.574276    0.000279   79.553735    0.000279  \n",
      "179   79.553735    0.000279   79.533193    0.000279  \n",
      "\n",
      "[5 rows x 276 columns]\n",
      "Index(['var1(t-175)', 'var1(t-174)', 'var1(t-173)', 'var1(t-172)',\n",
      "       'var1(t-171)', 'var1(t-170)', 'var1(t-169)', 'var1(t-168)',\n",
      "       'var1(t-167)', 'var1(t-166)',\n",
      "       ...\n",
      "       'var1(t+45)', 'var2(t+45)', 'var1(t+46)', 'var2(t+46)', 'var1(t+47)',\n",
      "       'var2(t+47)', 'var1(t+48)', 'var2(t+48)', 'var1(t+49)', 'var2(t+49)'],\n",
      "      dtype='object', length=276)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 175, n_out = 50)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 176):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-150:]\n",
    "trainX = train[:,:-150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-150:]\n",
    "forecastX = forecast[:,:-150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225, 1, 126) (2225, 150) (1, 1, 126)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "28/28 [==============================] - 2s 19ms/step - loss: 4537.9941 - val_loss: 2533.3320\n",
      "Epoch 2/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4291.7515 - val_loss: 2366.9834\n",
      "Epoch 3/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4079.7668 - val_loss: 2246.4900\n",
      "Epoch 4/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3913.2063 - val_loss: 2156.7341\n",
      "Epoch 5/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3759.3059 - val_loss: 2068.9062\n",
      "Epoch 6/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3612.4119 - val_loss: 1987.3093\n",
      "Epoch 7/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3472.0032 - val_loss: 1909.9764\n",
      "Epoch 8/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3340.1423 - val_loss: 1839.0741\n",
      "Epoch 9/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3214.5693 - val_loss: 1772.7799\n",
      "Epoch 10/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3094.3491 - val_loss: 1710.3274\n",
      "Epoch 11/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 2978.7712 - val_loss: 1652.1857\n",
      "Epoch 12/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2867.2991 - val_loss: 1594.5983\n",
      "Epoch 13/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2759.7407 - val_loss: 1541.0403\n",
      "Epoch 14/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2643.4812 - val_loss: 1481.3380\n",
      "Epoch 15/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2533.0623 - val_loss: 1430.4282\n",
      "Epoch 16/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2431.4529 - val_loss: 1383.6177\n",
      "Epoch 17/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 2334.3071 - val_loss: 1341.0969\n",
      "Epoch 18/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 2240.9846 - val_loss: 1297.6449\n",
      "Epoch 19/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 2151.1562 - val_loss: 1258.7789\n",
      "Epoch 20/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 2064.5762 - val_loss: 1222.2466\n",
      "Epoch 21/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1981.0715 - val_loss: 1188.0892\n",
      "Epoch 22/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1900.5901 - val_loss: 1156.0618\n",
      "Epoch 23/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1822.9658 - val_loss: 1126.0363\n",
      "Epoch 24/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1748.0778 - val_loss: 1098.0951\n",
      "Epoch 25/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1675.8781 - val_loss: 1072.1694\n",
      "Epoch 26/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1606.2601 - val_loss: 1048.1241\n",
      "Epoch 27/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1539.1469 - val_loss: 1025.8955\n",
      "Epoch 28/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1474.4644 - val_loss: 1005.4206\n",
      "Epoch 29/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 1412.1423 - val_loss: 986.6404\n",
      "Epoch 30/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 1352.1117 - val_loss: 969.4975\n",
      "Epoch 31/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1294.3063 - val_loss: 953.8727\n",
      "Epoch 32/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1238.6986 - val_loss: 939.3970\n",
      "Epoch 33/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1185.1517 - val_loss: 926.7931\n",
      "Epoch 34/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1133.6481 - val_loss: 915.5899\n",
      "Epoch 35/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1084.2488 - val_loss: 905.9674\n",
      "Epoch 36/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1036.4174 - val_loss: 894.9374\n",
      "Epoch 37/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 990.8492 - val_loss: 889.0570\n",
      "Epoch 38/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 947.0541 - val_loss: 883.0656\n",
      "Epoch 39/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 904.9308 - val_loss: 877.7703\n",
      "Epoch 40/500\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 864.5770 - val_loss: 873.8881\n",
      "Epoch 41/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 825.9675 - val_loss: 871.1307\n",
      "Epoch 42/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 788.9446 - val_loss: 870.2927\n",
      "Epoch 43/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 753.4787 - val_loss: 869.5833\n",
      "Epoch 44/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 719.5406 - val_loss: 869.7606\n",
      "Epoch 45/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 687.0915 - val_loss: 870.7587\n",
      "Epoch 46/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 656.0628 - val_loss: 872.7728\n",
      "Epoch 47/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 626.4398 - val_loss: 875.5679\n",
      "Epoch 48/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 598.1491 - val_loss: 878.9443\n",
      "Epoch 49/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 571.1694 - val_loss: 883.1338\n",
      "Epoch 50/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 545.4525 - val_loss: 888.0720\n",
      "Epoch 51/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 520.9425 - val_loss: 893.3710\n",
      "Epoch 52/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 497.6127 - val_loss: 899.5109\n",
      "Epoch 53/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 475.4150 - val_loss: 905.9886\n",
      "Epoch 54/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 454.3342 - val_loss: 912.8047\n",
      "Epoch 55/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 434.2589 - val_loss: 920.8916\n",
      "Epoch 56/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 415.2911 - val_loss: 929.7896\n",
      "Epoch 57/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 397.2344 - val_loss: 938.5701\n",
      "Epoch 58/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 378.1172 - val_loss: 948.0186\n",
      "Epoch 59/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 360.6750 - val_loss: 958.1475\n",
      "Epoch 60/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 344.3408 - val_loss: 968.4547\n",
      "Epoch 61/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 329.0659 - val_loss: 979.0035\n",
      "Epoch 62/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 314.7769 - val_loss: 989.7622\n",
      "Epoch 63/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 301.4065 - val_loss: 1000.7010\n",
      "Epoch 64/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 288.8964 - val_loss: 1011.7909\n",
      "Epoch 65/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 277.1940 - val_loss: 1023.0027\n",
      "Epoch 66/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 266.2521 - val_loss: 1034.3092\n",
      "Epoch 67/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 256.0264 - val_loss: 1045.6815\n",
      "Epoch 68/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 246.4763 - val_loss: 1057.0945\n",
      "Epoch 69/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 237.5634 - val_loss: 1068.5236\n",
      "Epoch 70/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 229.2516 - val_loss: 1079.9447\n",
      "Epoch 71/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 221.5064 - val_loss: 1091.3357\n",
      "Epoch 72/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 214.2959 - val_loss: 1102.6747\n",
      "Epoch 73/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 207.5888 - val_loss: 1113.9421\n",
      "Epoch 74/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 201.3560 - val_loss: 1125.1183\n",
      "Epoch 75/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 195.5695 - val_loss: 1136.1855\n",
      "Epoch 76/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 190.2027 - val_loss: 1147.1274\n",
      "Epoch 77/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 185.2303 - val_loss: 1157.9280\n",
      "Epoch 78/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 180.6279 - val_loss: 1168.5721\n",
      "Epoch 79/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 176.3728 - val_loss: 1179.0469\n",
      "Epoch 80/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 172.4429 - val_loss: 1189.3406\n",
      "Epoch 81/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 168.8174 - val_loss: 1199.4410\n",
      "Epoch 82/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 165.4763 - val_loss: 1209.3395\n",
      "Epoch 83/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 162.4009 - val_loss: 1219.0272\n",
      "Epoch 84/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 159.5690 - val_loss: 1228.3677\n",
      "Epoch 85/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 156.3587 - val_loss: 1220.3669\n",
      "Epoch 86/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 156.0079 - val_loss: 1246.8059\n",
      "Epoch 87/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 152.3919 - val_loss: 1255.5627\n",
      "Epoch 88/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 150.3967 - val_loss: 1264.0587\n",
      "Epoch 89/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 148.5740 - val_loss: 1272.2531\n",
      "Epoch 90/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 146.9082 - val_loss: 1279.1178\n",
      "Epoch 91/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 144.9250 - val_loss: 1273.8541\n",
      "Epoch 92/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 144.0309 - val_loss: 1297.5242\n",
      "Epoch 93/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 142.7514 - val_loss: 1304.8115\n",
      "Epoch 94/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 141.6106 - val_loss: 1311.8440\n",
      "Epoch 95/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 140.5760 - val_loss: 1318.6241\n",
      "Epoch 96/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 139.6384 - val_loss: 1325.1542\n",
      "Epoch 97/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 138.7901 - val_loss: 1331.4347\n",
      "Epoch 98/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 138.0233 - val_loss: 1337.4695\n",
      "Epoch 99/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 137.3311 - val_loss: 1343.2612\n",
      "Epoch 100/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 136.7070 - val_loss: 1348.8127\n",
      "Epoch 101/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 136.1450 - val_loss: 1354.1294\n",
      "Epoch 102/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 135.6396 - val_loss: 1359.2157\n",
      "Epoch 103/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 135.1855 - val_loss: 1364.0748\n",
      "Epoch 104/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 134.7781 - val_loss: 1368.7119\n",
      "Epoch 105/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 134.4130 - val_loss: 1373.1337\n",
      "Epoch 106/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 134.0863 - val_loss: 1377.3448\n",
      "Epoch 107/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 133.7941 - val_loss: 1381.3510\n",
      "Epoch 108/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 133.5332 - val_loss: 1385.1577\n",
      "Epoch 109/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 133.3004 - val_loss: 1388.7712\n",
      "Epoch 110/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 133.0930 - val_loss: 1392.1967\n",
      "Epoch 111/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 132.9083 - val_loss: 1395.4415\n",
      "Epoch 112/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.7442 - val_loss: 1398.5127\n",
      "Epoch 113/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.5984 - val_loss: 1401.4148\n",
      "Epoch 114/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.4690 - val_loss: 1404.1547\n",
      "Epoch 115/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.3542 - val_loss: 1406.7384\n",
      "Epoch 116/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 132.2526 - val_loss: 1409.1726\n",
      "Epoch 117/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.1627 - val_loss: 1411.4636\n",
      "Epoch 118/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 132.0833 - val_loss: 1413.6176\n",
      "Epoch 119/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 132.0131 - val_loss: 1415.6399\n",
      "Epoch 120/500\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 131.9512 - val_loss: 1417.5363\n",
      "Epoch 121/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 131.8966 - val_loss: 1419.3113\n",
      "Epoch 122/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 131.8485 - val_loss: 1420.9733\n",
      "Epoch 123/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 131.8062 - val_loss: 1422.5260\n",
      "Epoch 124/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 131.7690 - val_loss: 1423.9758\n",
      "Epoch 125/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 131.7363 - val_loss: 1425.3271\n",
      "Epoch 126/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 131.7076 - val_loss: 1426.5847\n",
      "Epoch 127/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 131.6825 - val_loss: 1427.7546\n",
      "Epoch 128/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 131.6605 - val_loss: 1428.8411\n",
      "Epoch 129/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 131.6413 - val_loss: 1429.8472\n",
      "Epoch 130/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.6246 - val_loss: 1430.7789\n",
      "Epoch 131/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.6100 - val_loss: 1431.6389\n",
      "Epoch 132/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5973 - val_loss: 1432.4315\n",
      "Epoch 133/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5862 - val_loss: 1433.1630\n",
      "Epoch 134/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5767 - val_loss: 1433.8323\n",
      "Epoch 135/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5685 - val_loss: 1434.4452\n",
      "Epoch 136/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5614 - val_loss: 1435.0040\n",
      "Epoch 137/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5554 - val_loss: 1435.5120\n",
      "Epoch 138/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5503 - val_loss: 1435.9690\n",
      "Epoch 139/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5460 - val_loss: 1436.3805\n",
      "Epoch 140/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5424 - val_loss: 1436.7461\n",
      "Epoch 141/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5394 - val_loss: 1437.0642\n",
      "Epoch 142/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5370 - val_loss: 1437.3340\n",
      "Epoch 143/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 131.5351 - val_loss: 1437.5460\n",
      "Epoch 144/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 131.5336 - val_loss: 1437.6676\n",
      "Epoch 145/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5321 - val_loss: 1437.6278\n",
      "Epoch 146/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.4954 - val_loss: 1429.6519\n",
      "Epoch 147/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.4547 - val_loss: 1439.1804\n",
      "Epoch 148/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5259 - val_loss: 1438.9843\n",
      "Epoch 149/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.3895 - val_loss: 1422.7729\n",
      "Epoch 150/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5364 - val_loss: 1434.0568\n",
      "Epoch 151/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.9575 - val_loss: 1441.1309\n",
      "Epoch 152/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5301 - val_loss: 1441.1095\n",
      "Epoch 153/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5304 - val_loss: 1440.9301\n",
      "Epoch 154/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5263 - val_loss: 1439.9216\n",
      "Epoch 155/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.4972 - val_loss: 1430.0421\n",
      "Epoch 156/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6967 - val_loss: 1390.3247\n",
      "Epoch 157/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 134.0697 - val_loss: 1385.2463\n",
      "Epoch 158/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 133.4033 - val_loss: 1390.2119\n",
      "Epoch 159/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 133.1070 - val_loss: 1389.0577\n",
      "Epoch 160/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 131.6412 - val_loss: 1310.7081\n",
      "Epoch 161/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 143.3980 - val_loss: 1305.4905\n",
      "Epoch 162/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 141.7837 - val_loss: 1317.2554\n",
      "Epoch 163/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 140.0847 - val_loss: 1327.8955\n",
      "Epoch 164/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 138.6543 - val_loss: 1337.9525\n",
      "Epoch 165/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 137.4648 - val_loss: 1347.0996\n",
      "Epoch 166/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 136.4765 - val_loss: 1355.3412\n",
      "Epoch 167/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 135.6420 - val_loss: 1355.7631\n",
      "Epoch 168/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 135.0639 - val_loss: 1373.0725\n",
      "Epoch 169/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 134.4140 - val_loss: 1379.4058\n",
      "Epoch 170/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 133.9025 - val_loss: 1362.7366\n",
      "Epoch 171/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 137.2788 - val_loss: 1338.1473\n",
      "Epoch 172/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 137.6954 - val_loss: 1346.9344\n",
      "Epoch 173/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 136.6656 - val_loss: 1352.4222\n",
      "Epoch 174/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 135.8050 - val_loss: 1363.0123\n",
      "Epoch 175/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 135.0919 - val_loss: 1370.1174\n",
      "Epoch 176/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 134.4921 - val_loss: 1378.5376\n",
      "Epoch 177/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 134.0012 - val_loss: 1384.2622\n",
      "Epoch 178/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 133.5333 - val_loss: 1373.6212\n",
      "Epoch 179/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 134.6876 - val_loss: 1400.9602\n",
      "Epoch 180/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 132.9901 - val_loss: 1405.6377\n",
      "Epoch 181/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.7607 - val_loss: 1409.8625\n",
      "Epoch 182/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.5711 - val_loss: 1413.6879\n",
      "Epoch 183/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.4134 - val_loss: 1417.1467\n",
      "Epoch 184/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.2820 - val_loss: 1420.2717\n",
      "Epoch 185/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.1725 - val_loss: 1423.0869\n",
      "Epoch 186/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 132.0810 - val_loss: 1425.6232\n",
      "Epoch 187/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 132.0044 - val_loss: 1427.9059\n",
      "Epoch 188/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.9403 - val_loss: 1429.9562\n",
      "Epoch 189/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.8865 - val_loss: 1431.7936\n",
      "Epoch 190/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.8412 - val_loss: 1433.4363\n",
      "Epoch 191/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.8032 - val_loss: 1434.8992\n",
      "Epoch 192/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.7711 - val_loss: 1436.1960\n",
      "Epoch 193/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.7439 - val_loss: 1437.3290\n",
      "Epoch 194/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.7210 - val_loss: 1438.1472\n",
      "Epoch 195/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6939 - val_loss: 1417.0308\n",
      "Epoch 196/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6976 - val_loss: 1449.8965\n",
      "Epoch 197/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6707 - val_loss: 1450.6927\n",
      "Epoch 198/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6588 - val_loss: 1451.4061\n",
      "Epoch 199/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6485 - val_loss: 1452.0424\n",
      "Epoch 200/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6399 - val_loss: 1452.6107\n",
      "Epoch 201/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6325 - val_loss: 1453.1173\n",
      "Epoch 202/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6263 - val_loss: 1453.5687\n",
      "Epoch 203/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6209 - val_loss: 1453.9709\n",
      "Epoch 204/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6163 - val_loss: 1454.3274\n",
      "Epoch 205/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6124 - val_loss: 1454.6428\n",
      "Epoch 206/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6064 - val_loss: 1454.8429\n",
      "Epoch 207/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 131.6068 - val_loss: 1455.1190\n",
      "Epoch 208/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.6041 - val_loss: 1455.3365\n",
      "Epoch 209/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6021 - val_loss: 1455.5283\n",
      "Epoch 210/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6005 - val_loss: 1455.6949\n",
      "Epoch 211/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5992 - val_loss: 1455.8408\n",
      "Epoch 212/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5732 - val_loss: 1451.7223\n",
      "Epoch 213/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 133.7392 - val_loss: 1456.4884\n",
      "Epoch 214/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5942 - val_loss: 1456.5652\n",
      "Epoch 215/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5915 - val_loss: 1456.5995\n",
      "Epoch 216/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5916 - val_loss: 1456.6241\n",
      "Epoch 217/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5916 - val_loss: 1456.6410\n",
      "Epoch 218/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5919 - val_loss: 1456.6477\n",
      "Epoch 219/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5923 - val_loss: 1456.6475\n",
      "Epoch 220/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5926 - val_loss: 1456.6373\n",
      "Epoch 221/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5930 - val_loss: 1456.6149\n",
      "Epoch 222/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5934 - val_loss: 1456.3977\n",
      "Epoch 223/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5122 - val_loss: 1385.7263\n",
      "Epoch 224/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 142.4784 - val_loss: 1464.3821\n",
      "Epoch 225/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5864 - val_loss: 1464.2115\n",
      "Epoch 226/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 131.5747 - val_loss: 1463.8051\n",
      "Epoch 227/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5762 - val_loss: 1462.7820\n",
      "Epoch 228/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5491 - val_loss: 1454.7284\n",
      "Epoch 229/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.3063 - val_loss: 1413.4135\n",
      "Epoch 230/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 133.9402 - val_loss: 1402.6902\n",
      "Epoch 231/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 133.7121 - val_loss: 1408.9847\n",
      "Epoch 232/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 133.3460 - val_loss: 1414.6554\n",
      "Epoch 233/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 133.0331 - val_loss: 1419.3513\n",
      "Epoch 234/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.7367 - val_loss: 1378.3508\n",
      "Epoch 235/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 136.9557 - val_loss: 1372.1034\n",
      "Epoch 236/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 136.1710 - val_loss: 1381.6306\n",
      "Epoch 237/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 135.3341 - val_loss: 1390.3051\n",
      "Epoch 238/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 134.6477 - val_loss: 1398.0989\n",
      "Epoch 239/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 134.0928 - val_loss: 1405.0878\n",
      "Epoch 240/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 133.6438 - val_loss: 1411.3483\n",
      "Epoch 241/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 133.2800 - val_loss: 1416.9493\n",
      "Epoch 242/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.9847 - val_loss: 1421.9568\n",
      "Epoch 243/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.7443 - val_loss: 1426.4276\n",
      "Epoch 244/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.5484 - val_loss: 1430.4167\n",
      "Epoch 245/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 132.3881 - val_loss: 1433.9701\n",
      "Epoch 246/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.2569 - val_loss: 1437.1306\n",
      "Epoch 247/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.1490 - val_loss: 1439.9365\n",
      "Epoch 248/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.0600 - val_loss: 1442.4214\n",
      "Epoch 249/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.9866 - val_loss: 1444.6132\n",
      "Epoch 250/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.9257 - val_loss: 1446.5327\n",
      "Epoch 251/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.8751 - val_loss: 1448.1934\n",
      "Epoch 252/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.8329 - val_loss: 1449.5846\n",
      "Epoch 253/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.7976 - val_loss: 1450.6260\n",
      "Epoch 254/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.7679 - val_loss: 1450.6736\n",
      "Epoch 255/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6764 - val_loss: 1411.6029\n",
      "Epoch 256/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 133.2502 - val_loss: 1398.4242\n",
      "Epoch 257/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 134.3276 - val_loss: 1405.5385\n",
      "Epoch 258/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 133.8502 - val_loss: 1412.1511\n",
      "Epoch 259/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 133.4492 - val_loss: 1418.0858\n",
      "Epoch 260/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 133.1228 - val_loss: 1423.4014\n",
      "Epoch 261/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.8573 - val_loss: 1428.1583\n",
      "Epoch 262/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.6409 - val_loss: 1432.4131\n",
      "Epoch 263/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 132.4639 - val_loss: 1436.2166\n",
      "Epoch 264/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.3190 - val_loss: 1439.6149\n",
      "Epoch 265/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.1999 - val_loss: 1442.6487\n",
      "Epoch 266/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.1019 - val_loss: 1445.3579\n",
      "Epoch 267/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.0208 - val_loss: 1447.7745\n",
      "Epoch 268/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.9537 - val_loss: 1449.9299\n",
      "Epoch 269/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.8979 - val_loss: 1451.8510\n",
      "Epoch 270/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.8513 - val_loss: 1453.5629\n",
      "Epoch 271/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.8125 - val_loss: 1455.0879\n",
      "Epoch 272/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.7798 - val_loss: 1456.4442\n",
      "Epoch 273/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.7523 - val_loss: 1457.6427\n",
      "Epoch 274/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.7288 - val_loss: 1458.6719\n",
      "Epoch 275/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.7088 - val_loss: 1459.6532\n",
      "Epoch 276/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6911 - val_loss: 1460.1503\n",
      "Epoch 277/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6797 - val_loss: 1461.3293\n",
      "Epoch 278/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6664 - val_loss: 1461.9998\n",
      "Epoch 279/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6560 - val_loss: 1462.5944\n",
      "Epoch 280/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6471 - val_loss: 1463.1200\n",
      "Epoch 281/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 131.6393 - val_loss: 1463.5538\n",
      "Epoch 282/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5509 - val_loss: 1449.3116\n",
      "Epoch 283/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.9267 - val_loss: 1401.4618\n",
      "Epoch 284/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 134.1184 - val_loss: 1408.3221\n",
      "Epoch 285/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 133.6809 - val_loss: 1414.6090\n",
      "Epoch 286/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 133.3132 - val_loss: 1420.2511\n",
      "Epoch 287/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 133.0134 - val_loss: 1425.3058\n",
      "Epoch 288/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 132.7689 - val_loss: 1429.8301\n",
      "Epoch 289/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.5691 - val_loss: 1433.8776\n",
      "Epoch 290/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.4054 - val_loss: 1437.4966\n",
      "Epoch 291/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.2710 - val_loss: 1440.7299\n",
      "Epoch 292/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.1603 - val_loss: 1443.6177\n",
      "Epoch 293/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.0689 - val_loss: 1446.1951\n",
      "Epoch 294/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.9932 - val_loss: 1448.4944\n",
      "Epoch 295/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.9302 - val_loss: 1450.5426\n",
      "Epoch 296/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.8778 - val_loss: 1452.3666\n",
      "Epoch 297/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.8340 - val_loss: 1453.9890\n",
      "Epoch 298/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.7972 - val_loss: 1455.4298\n",
      "Epoch 299/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.7663 - val_loss: 1456.7064\n",
      "Epoch 300/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.7402 - val_loss: 1457.8304\n",
      "Epoch 301/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.7181 - val_loss: 1458.8114\n",
      "Epoch 302/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6994 - val_loss: 1459.6407\n",
      "Epoch 303/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6833 - val_loss: 1460.2333\n",
      "Epoch 304/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6678 - val_loss: 1455.3378\n",
      "Epoch 305/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 131.6621 - val_loss: 1464.9038\n",
      "Epoch 306/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6479 - val_loss: 1465.4686\n",
      "Epoch 307/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6392 - val_loss: 1465.9670\n",
      "Epoch 308/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.6317 - val_loss: 1466.4020\n",
      "Epoch 309/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6253 - val_loss: 1466.7806\n",
      "Epoch 310/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6197 - val_loss: 1467.1056\n",
      "Epoch 311/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6148 - val_loss: 1467.3722\n",
      "Epoch 312/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 131.6105 - val_loss: 1467.4739\n",
      "Epoch 313/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.6068 - val_loss: 1455.7511\n",
      "Epoch 314/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 132.5580 - val_loss: 1468.8320\n",
      "Epoch 315/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5997 - val_loss: 1468.9574\n",
      "Epoch 316/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5964 - val_loss: 1469.0381\n",
      "Epoch 317/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5411 - val_loss: 1437.9784\n",
      "Epoch 318/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.7234 - val_loss: 1470.7119\n",
      "Epoch 319/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5900 - val_loss: 1470.7926\n",
      "Epoch 320/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5877 - val_loss: 1470.8424\n",
      "Epoch 321/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5867 - val_loss: 1470.8732\n",
      "Epoch 322/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5859 - val_loss: 1470.8822\n",
      "Epoch 323/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5852 - val_loss: 1470.8601\n",
      "Epoch 324/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5847 - val_loss: 1470.7809\n",
      "Epoch 325/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5841 - val_loss: 1470.5338\n",
      "Epoch 326/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 131.5834 - val_loss: 1467.4495\n",
      "Epoch 327/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 130.7808 - val_loss: 1421.4197\n",
      "Epoch 328/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 134.4913 - val_loss: 1407.6201\n",
      "Epoch 329/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 134.1920 - val_loss: 1414.5880\n",
      "Epoch 330/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 133.7433 - val_loss: 1420.9746\n",
      "Epoch 331/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 133.3665 - val_loss: 1426.7145\n",
      "Epoch 332/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 133.0587 - val_loss: 1431.8665\n",
      "Epoch 333/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.8073 - val_loss: 1436.4852\n",
      "Epoch 334/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.6014 - val_loss: 1440.6252\n",
      "Epoch 335/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.4325 - val_loss: 1444.3351\n",
      "Epoch 336/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.2935 - val_loss: 1447.6570\n",
      "Epoch 337/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.1789 - val_loss: 1450.6326\n",
      "Epoch 338/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.0840 - val_loss: 1453.2942\n",
      "Epoch 339/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.0054 - val_loss: 1455.6764\n",
      "Epoch 340/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.9399 - val_loss: 1457.8080\n",
      "Epoch 341/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.8853 - val_loss: 1459.7134\n",
      "Epoch 342/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.8396 - val_loss: 1461.4172\n",
      "Epoch 343/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.8013 - val_loss: 1462.9406\n",
      "Epoch 344/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.7689 - val_loss: 1464.3026\n",
      "Epoch 345/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.7415 - val_loss: 1465.5188\n",
      "Epoch 346/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.7185 - val_loss: 1466.6064\n",
      "Epoch 347/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6988 - val_loss: 1467.5776\n",
      "Epoch 348/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6820 - val_loss: 1468.4451\n",
      "Epoch 349/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6676 - val_loss: 1469.2205\n",
      "Epoch 350/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6553 - val_loss: 1469.9126\n",
      "Epoch 351/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6447 - val_loss: 1470.5311\n",
      "Epoch 352/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 131.6356 - val_loss: 1471.0834\n",
      "Epoch 353/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6276 - val_loss: 1471.5764\n",
      "Epoch 354/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 131.6207 - val_loss: 1472.0160\n",
      "Epoch 355/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.6148 - val_loss: 1472.4089\n",
      "Epoch 356/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.6095 - val_loss: 1472.7585\n",
      "Epoch 357/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.6051 - val_loss: 1473.0717\n",
      "Epoch 358/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.6011 - val_loss: 1473.3506\n",
      "Epoch 359/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5976 - val_loss: 1473.6000\n",
      "Epoch 360/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5947 - val_loss: 1473.8217\n",
      "Epoch 361/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 131.5920 - val_loss: 1474.0199\n",
      "Epoch 362/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5895 - val_loss: 1474.1960\n",
      "Epoch 363/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5875 - val_loss: 1474.3538\n",
      "Epoch 364/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5857 - val_loss: 1474.4946\n",
      "Epoch 365/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5841 - val_loss: 1474.6200\n",
      "Epoch 366/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5827 - val_loss: 1474.7307\n",
      "Epoch 367/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5814 - val_loss: 1474.8301\n",
      "Epoch 368/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5804 - val_loss: 1474.9182\n",
      "Epoch 369/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5794 - val_loss: 1474.9979\n",
      "Epoch 370/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5785 - val_loss: 1475.0680\n",
      "Epoch 371/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 131.5777 - val_loss: 1475.1295\n",
      "Epoch 372/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5770 - val_loss: 1475.1858\n",
      "Epoch 373/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5764 - val_loss: 1475.2345\n",
      "Epoch 374/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5759 - val_loss: 1475.2778\n",
      "Epoch 375/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5755 - val_loss: 1475.3165\n",
      "Epoch 376/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5750 - val_loss: 1475.3503\n",
      "Epoch 377/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5747 - val_loss: 1475.3800\n",
      "Epoch 378/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5744 - val_loss: 1475.4071\n",
      "Epoch 379/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5740 - val_loss: 1475.4299\n",
      "Epoch 380/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5738 - val_loss: 1475.4502\n",
      "Epoch 381/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5736 - val_loss: 1475.4685\n",
      "Epoch 382/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5734 - val_loss: 1475.4844\n",
      "Epoch 383/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5732 - val_loss: 1475.4978\n",
      "Epoch 384/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5731 - val_loss: 1475.5104\n",
      "Epoch 385/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5729 - val_loss: 1475.5214\n",
      "Epoch 386/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 131.5728 - val_loss: 1475.5389\n",
      "Epoch 387/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 131.5726 - val_loss: 1475.6561\n",
      "Epoch 388/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5726 - val_loss: 1475.5055\n",
      "Epoch 389/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5726 - val_loss: 1475.5022\n",
      "Epoch 390/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5725 - val_loss: 1475.4839\n",
      "Epoch 391/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 131.5683 - val_loss: 1471.9863\n",
      "Epoch 392/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 132.3066 - val_loss: 1475.6270\n",
      "Epoch 393/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5716 - val_loss: 1475.5933\n",
      "Epoch 394/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5709 - val_loss: 1475.4601\n",
      "Epoch 395/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5710 - val_loss: 1472.9901\n",
      "Epoch 396/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5735 - val_loss: 1477.0223\n",
      "Epoch 397/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5712 - val_loss: 1477.0151\n",
      "Epoch 398/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5713 - val_loss: 1477.0094\n",
      "Epoch 399/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5713 - val_loss: 1477.0039\n",
      "Epoch 400/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5715 - val_loss: 1476.9988\n",
      "Epoch 401/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5715 - val_loss: 1476.9946\n",
      "Epoch 402/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5716 - val_loss: 1476.9910\n",
      "Epoch 403/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5716 - val_loss: 1476.9875\n",
      "Epoch 404/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5717 - val_loss: 1476.9844\n",
      "Epoch 405/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5717 - val_loss: 1476.9819\n",
      "Epoch 406/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5717 - val_loss: 1476.9795\n",
      "Epoch 407/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5718 - val_loss: 1476.9767\n",
      "Epoch 408/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5718 - val_loss: 1476.9747\n",
      "Epoch 409/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5719 - val_loss: 1476.9731\n",
      "Epoch 410/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5719 - val_loss: 1476.9714\n",
      "Epoch 411/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5719 - val_loss: 1476.9694\n",
      "Epoch 412/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5719 - val_loss: 1476.9675\n",
      "Epoch 413/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5720 - val_loss: 1476.9666\n",
      "Epoch 414/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5719 - val_loss: 1476.9652\n",
      "Epoch 415/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5720 - val_loss: 1476.9639\n",
      "Epoch 416/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5720 - val_loss: 1476.9633\n",
      "Epoch 417/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5720 - val_loss: 1476.9615\n",
      "Epoch 418/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5721 - val_loss: 1476.9604\n",
      "Epoch 419/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5721 - val_loss: 1476.9587\n",
      "Epoch 420/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5721 - val_loss: 1476.9581\n",
      "Epoch 421/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5721 - val_loss: 1476.9578\n",
      "Epoch 422/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5721 - val_loss: 1476.9573\n",
      "Epoch 423/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5721 - val_loss: 1476.9568\n",
      "Epoch 424/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5721 - val_loss: 1476.9561\n",
      "Epoch 425/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5722 - val_loss: 1476.9554\n",
      "Epoch 426/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5722 - val_loss: 1476.9551\n",
      "Epoch 427/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5722 - val_loss: 1476.9545\n",
      "Epoch 428/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5722 - val_loss: 1476.9536\n",
      "Epoch 429/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5722 - val_loss: 1476.9534\n",
      "Epoch 430/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5722 - val_loss: 1476.9528\n",
      "Epoch 431/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 131.5722 - val_loss: 1476.9525\n",
      "Epoch 432/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5722 - val_loss: 1476.9520\n",
      "Epoch 433/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5722 - val_loss: 1476.9514\n",
      "Epoch 434/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9500\n",
      "Epoch 435/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9500\n",
      "Epoch 436/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9497\n",
      "Epoch 437/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9495\n",
      "Epoch 438/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9489\n",
      "Epoch 439/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5723 - val_loss: 1476.9486\n",
      "Epoch 440/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5722 - val_loss: 1476.9480\n",
      "Epoch 441/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9475\n",
      "Epoch 442/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5722 - val_loss: 1476.9463\n",
      "Epoch 443/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5722 - val_loss: 1476.9452\n",
      "Epoch 444/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9446\n",
      "Epoch 445/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9437\n",
      "Epoch 446/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9424\n",
      "Epoch 447/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9415\n",
      "Epoch 448/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9407\n",
      "Epoch 449/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 131.5723 - val_loss: 1476.9399\n",
      "Epoch 450/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5723 - val_loss: 1476.9387\n",
      "Epoch 451/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9376\n",
      "Epoch 452/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5723 - val_loss: 1476.9364\n",
      "Epoch 453/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9349\n",
      "Epoch 454/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9335\n",
      "Epoch 455/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9319\n",
      "Epoch 456/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9303\n",
      "Epoch 457/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9287\n",
      "Epoch 458/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9265\n",
      "Epoch 459/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9244\n",
      "Epoch 460/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9219\n",
      "Epoch 461/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9191\n",
      "Epoch 462/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9160\n",
      "Epoch 463/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9126\n",
      "Epoch 464/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9086\n",
      "Epoch 465/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9039\n",
      "Epoch 466/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.8989\n",
      "Epoch 467/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5724 - val_loss: 1476.8925\n",
      "Epoch 468/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5724 - val_loss: 1476.8849\n",
      "Epoch 469/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5724 - val_loss: 1476.8750\n",
      "Epoch 470/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5724 - val_loss: 1476.8622\n",
      "Epoch 471/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5724 - val_loss: 1476.8450\n",
      "Epoch 472/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 131.5723 - val_loss: 1476.8198\n",
      "Epoch 473/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5723 - val_loss: 1476.7791\n",
      "Epoch 474/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5724 - val_loss: 1476.6998\n",
      "Epoch 475/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 131.5724 - val_loss: 1476.4366\n",
      "Epoch 476/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5667 - val_loss: 1472.5684\n",
      "Epoch 477/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5731 - val_loss: 1476.9969\n",
      "Epoch 478/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9961\n",
      "Epoch 479/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9948\n",
      "Epoch 480/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9933\n",
      "Epoch 481/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9924\n",
      "Epoch 482/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5724 - val_loss: 1476.9919\n",
      "Epoch 483/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9918\n",
      "Epoch 484/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5723 - val_loss: 1476.9913\n",
      "Epoch 485/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5724 - val_loss: 1476.9907\n",
      "Epoch 486/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5724 - val_loss: 1476.9906\n",
      "Epoch 487/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5724 - val_loss: 1476.9901\n",
      "Epoch 488/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5724 - val_loss: 1476.9901\n",
      "Epoch 489/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5724 - val_loss: 1476.9899\n",
      "Epoch 490/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 131.5724 - val_loss: 1476.9899\n",
      "Epoch 491/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5724 - val_loss: 1476.9897\n",
      "Epoch 492/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5724 - val_loss: 1476.9896\n",
      "Epoch 493/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5724 - val_loss: 1476.9894\n",
      "Epoch 494/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5724 - val_loss: 1476.9894\n",
      "Epoch 495/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5724 - val_loss: 1476.9891\n",
      "Epoch 496/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5724 - val_loss: 1476.9890\n",
      "Epoch 497/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5724 - val_loss: 1476.9888\n",
      "Epoch 498/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5724 - val_loss: 1476.9888\n",
      "Epoch 499/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5724 - val_loss: 1476.9885\n",
      "Epoch 500/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 131.5724 - val_loss: 1476.9884\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(85.7336, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0020, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0125, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3008, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 371ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 126)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 126)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 186)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:150:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.43447533e-01, 6.41632120e+01, 6.40623716e+01, 6.39615313e+01,\n",
       "        6.37820728e+01, 6.34795518e+01, 6.31770308e+01, 6.28745098e+01,\n",
       "        6.26466620e+01, 6.25206116e+01, 1.80943000e-01, 1.86248840e-01,\n",
       "        6.36028011e+01, 6.33002801e+01, 6.29977591e+01, 6.26960159e+01,\n",
       "        6.25719655e+01, 6.24449150e+01, 6.23198646e+01, 6.21876284e+01,\n",
       "        6.19355275e+01, 3.55150253e-01, 2.78546482e-01, 5.85645638e+01,\n",
       "        3.60960543e-01, 0.00000000e+00, 0.00000000e+00, 8.31274509e-01,\n",
       "        4.89992410e-01, 0.00000000e+00, 6.29430618e+01, 1.81835026e-01,\n",
       "        5.48016667e-01, 1.89595520e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.76804662e-01, 1.80284068e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.70446527e-01, 4.68440093e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.02905059e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.10687762e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([55.74189383, 55.73287559, 55.72385735, 55.71483911, 55.70582086,\n",
       "       55.69680262, 55.68778438, 55.67876614, 55.6697479 , 55.66072966,\n",
       "       55.65171142, 55.64269317, 55.63367493, 55.62465669, 55.61563845,\n",
       "       55.60662021, 55.59760197, 55.58858373, 55.57956548, 55.57054724,\n",
       "       55.561529  , 55.55251076, 55.54349252, 55.53447428, 55.52545604,\n",
       "       55.51643779, 55.50741955, 55.49840131, 55.48938307, 55.48036483,\n",
       "       55.47134659, 55.46232835, 55.4533101 , 55.44429186, 55.43527362,\n",
       "       55.42625538, 55.41723714, 55.4082189 , 55.39920066, 55.39018241,\n",
       "       55.38116417, 55.37214593, 55.36312769, 55.35410945, 55.34509121,\n",
       "       55.33607297, 55.32705472, 55.31803648, 55.30901824, 55.3       ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.033536494778865\n",
      "36.014164590360835\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
