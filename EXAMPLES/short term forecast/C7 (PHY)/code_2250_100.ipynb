{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       92.600000\n",
       "1       92.387115\n",
       "2       92.174230\n",
       "3       91.961345\n",
       "4       91.748459\n",
       "          ...    \n",
       "2345    63.578384\n",
       "2346    63.570732\n",
       "2347    63.563080\n",
       "2348    63.555428\n",
       "2349    63.547776\n",
       "Name: C7, Length: 2350, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c7_interpolated_2250_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       92.600000\n",
       "1       92.387115\n",
       "2       92.174230\n",
       "3       91.961345\n",
       "4       91.748459\n",
       "          ...    \n",
       "2245     0.000000\n",
       "2246     0.829569\n",
       "2247     0.137789\n",
       "2248     0.000000\n",
       "2249     0.000000\n",
       "Name: C7, Length: 2250, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2250)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92.387115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92.174230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.961345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91.748459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     92.600000  0.000298\n",
       "1     92.387115  0.000298\n",
       "2     92.174230  0.000297\n",
       "3     91.961345  0.000297\n",
       "4     91.748459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 89.3180  # Value for C0\n",
    "K0 = -0.0010  # Value for K0\n",
    "K1 = 0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = -0.0204    # Value for b\n",
    "c = 2.2194    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    92.600000    92.387115    92.174230    91.961345    91.748459   \n",
      "351    92.387115    92.174230    91.961345    91.748459    91.535574   \n",
      "352    92.174230    91.961345    91.748459    91.535574    91.322689   \n",
      "353    91.961345    91.748459    91.535574    91.322689    91.109804   \n",
      "354    91.748459    91.535574    91.322689    91.109804    90.896919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    91.535574    91.322689    91.109804    90.896919    90.691597  ...   \n",
      "351    91.322689    91.109804    90.896919    90.691597    90.579552  ...   \n",
      "352    91.109804    90.896919    90.691597    90.579552    90.467507  ...   \n",
      "353    90.896919    90.691597    90.579552    90.467507    90.355462  ...   \n",
      "354    90.691597    90.579552    90.467507    90.355462    90.243417  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   80.261671    0.000263   80.252334    0.000263   80.242997    0.000263   \n",
      "351   80.252334    0.000263   80.242997    0.000263   80.233660    0.000262   \n",
      "352   80.242997    0.000263   80.233660    0.000262   80.224323    0.000262   \n",
      "353   80.233660    0.000262   80.224323    0.000262   80.214986    0.000262   \n",
      "354   80.224323    0.000262   80.214986    0.000262   80.205649    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   80.233660    0.000262   80.224323    0.000262  \n",
      "351   80.224323    0.000262   80.214986    0.000262  \n",
      "352   80.214986    0.000262   80.205649    0.000262  \n",
      "353   80.205649    0.000262   80.196312    0.000262  \n",
      "354   80.196312    0.000262   80.186975    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 1, 251) (1800, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "23/23 [==============================] - 2s 24ms/step - loss: 5430.0361 - val_loss: 3901.5669\n",
      "Epoch 2/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 5337.7476 - val_loss: 3847.7063\n",
      "Epoch 3/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 5261.5464 - val_loss: 3796.3108\n",
      "Epoch 4/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 5187.3301 - val_loss: 3745.4812\n",
      "Epoch 5/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 5093.3882 - val_loss: 3667.1975\n",
      "Epoch 6/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4994.8516 - val_loss: 3611.4392\n",
      "Epoch 7/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4914.7041 - val_loss: 3557.4888\n",
      "Epoch 8/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4836.8623 - val_loss: 3504.9595\n",
      "Epoch 9/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4760.7515 - val_loss: 3453.5215\n",
      "Epoch 10/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4686.0220 - val_loss: 3403.0051\n",
      "Epoch 11/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4612.4888 - val_loss: 3353.3152\n",
      "Epoch 12/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4540.0425 - val_loss: 3304.3899\n",
      "Epoch 13/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 4468.6118 - val_loss: 3256.1882\n",
      "Epoch 14/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4398.1479 - val_loss: 3208.6787\n",
      "Epoch 15/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 4328.6138 - val_loss: 3161.8379\n",
      "Epoch 16/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4259.9780 - val_loss: 3115.6438\n",
      "Epoch 17/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4192.2202 - val_loss: 3070.0706\n",
      "Epoch 18/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4125.3188 - val_loss: 3025.0171\n",
      "Epoch 19/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 4052.6562 - val_loss: 2970.5938\n",
      "Epoch 20/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 3976.8608 - val_loss: 2921.2080\n",
      "Epoch 21/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3905.8660 - val_loss: 2873.7517\n",
      "Epoch 22/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3837.0869 - val_loss: 2827.6938\n",
      "Epoch 23/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3769.9558 - val_loss: 2782.7312\n",
      "Epoch 24/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3704.1775 - val_loss: 2738.7046\n",
      "Epoch 25/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3639.5847 - val_loss: 2695.5183\n",
      "Epoch 26/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3576.0764 - val_loss: 2653.1089\n",
      "Epoch 27/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3513.5791 - val_loss: 2611.4321\n",
      "Epoch 28/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3452.0447 - val_loss: 2570.4534\n",
      "Epoch 29/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3391.4312 - val_loss: 2530.1467\n",
      "Epoch 30/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3331.7070 - val_loss: 2490.4900\n",
      "Epoch 31/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3272.8472 - val_loss: 2451.4651\n",
      "Epoch 32/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3214.8296 - val_loss: 2413.0569\n",
      "Epoch 33/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3157.6333 - val_loss: 2375.2507\n",
      "Epoch 34/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3101.2424 - val_loss: 2338.0347\n",
      "Epoch 35/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3045.6416 - val_loss: 2301.3975\n",
      "Epoch 36/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2990.8174 - val_loss: 2265.3303\n",
      "Epoch 37/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2936.7568 - val_loss: 2229.8225\n",
      "Epoch 38/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2883.4475 - val_loss: 2194.8652\n",
      "Epoch 39/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2830.8789 - val_loss: 2160.4509\n",
      "Epoch 40/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2779.0400 - val_loss: 2126.5718\n",
      "Epoch 41/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2727.9229 - val_loss: 2093.2209\n",
      "Epoch 42/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2677.5159 - val_loss: 2060.3909\n",
      "Epoch 43/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2627.8115 - val_loss: 2028.0748\n",
      "Epoch 44/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2578.8010 - val_loss: 1996.2665\n",
      "Epoch 45/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 2530.4758 - val_loss: 1964.9598\n",
      "Epoch 46/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2482.8289 - val_loss: 1934.1488\n",
      "Epoch 47/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2435.8508 - val_loss: 1903.8280\n",
      "Epoch 48/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2389.5364 - val_loss: 1873.9908\n",
      "Epoch 49/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2343.8765 - val_loss: 1844.6323\n",
      "Epoch 50/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2298.8650 - val_loss: 1815.7476\n",
      "Epoch 51/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2254.4946 - val_loss: 1787.3303\n",
      "Epoch 52/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2210.7590 - val_loss: 1759.3759\n",
      "Epoch 53/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2167.6511 - val_loss: 1731.8792\n",
      "Epoch 54/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2125.1638 - val_loss: 1704.8356\n",
      "Epoch 55/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2083.2925 - val_loss: 1678.2396\n",
      "Epoch 56/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2042.0287 - val_loss: 1652.0867\n",
      "Epoch 57/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2001.3672 - val_loss: 1626.3718\n",
      "Epoch 58/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1961.3026 - val_loss: 1601.0909\n",
      "Epoch 59/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1921.8286 - val_loss: 1576.2394\n",
      "Epoch 60/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1882.9388 - val_loss: 1551.8121\n",
      "Epoch 61/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1844.6279 - val_loss: 1527.8051\n",
      "Epoch 62/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1806.8903 - val_loss: 1504.2135\n",
      "Epoch 63/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1769.7186 - val_loss: 1481.0330\n",
      "Epoch 64/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1733.1093 - val_loss: 1458.2603\n",
      "Epoch 65/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1697.0559 - val_loss: 1435.8894\n",
      "Epoch 66/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1661.5531 - val_loss: 1413.9171\n",
      "Epoch 67/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1626.5950 - val_loss: 1392.3391\n",
      "Epoch 68/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1592.1771 - val_loss: 1371.1510\n",
      "Epoch 69/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1558.2932 - val_loss: 1350.3494\n",
      "Epoch 70/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1524.9386 - val_loss: 1329.9290\n",
      "Epoch 71/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1492.1080 - val_loss: 1309.8862\n",
      "Epoch 72/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1459.7957 - val_loss: 1290.2181\n",
      "Epoch 73/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1427.9978 - val_loss: 1270.9197\n",
      "Epoch 74/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1396.7083 - val_loss: 1251.9872\n",
      "Epoch 75/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1365.9218 - val_loss: 1233.4163\n",
      "Epoch 76/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1335.6342 - val_loss: 1215.2036\n",
      "Epoch 77/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1305.8402 - val_loss: 1197.3450\n",
      "Epoch 78/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1276.5344 - val_loss: 1179.8368\n",
      "Epoch 79/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1247.7124 - val_loss: 1162.6752\n",
      "Epoch 80/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1219.3690 - val_loss: 1145.8566\n",
      "Epoch 81/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1191.4998 - val_loss: 1129.3760\n",
      "Epoch 82/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1164.0994 - val_loss: 1113.2310\n",
      "Epoch 83/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1137.1637 - val_loss: 1097.4175\n",
      "Epoch 84/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1110.6874 - val_loss: 1081.9315\n",
      "Epoch 85/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1084.6659 - val_loss: 1066.7698\n",
      "Epoch 86/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1059.0947 - val_loss: 1051.9285\n",
      "Epoch 87/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1033.9691 - val_loss: 1037.4037\n",
      "Epoch 88/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1009.2845 - val_loss: 1023.1920\n",
      "Epoch 89/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 985.0358 - val_loss: 1009.2893\n",
      "Epoch 90/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 961.2188 - val_loss: 995.6926\n",
      "Epoch 91/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 937.8289 - val_loss: 982.3981\n",
      "Epoch 92/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 914.8618 - val_loss: 969.4023\n",
      "Epoch 93/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 892.3124 - val_loss: 956.7013\n",
      "Epoch 94/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 870.1762 - val_loss: 944.2917\n",
      "Epoch 95/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 848.4493 - val_loss: 932.1700\n",
      "Epoch 96/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 827.1266 - val_loss: 920.3328\n",
      "Epoch 97/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 806.2044 - val_loss: 908.7763\n",
      "Epoch 98/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 785.6775 - val_loss: 897.4969\n",
      "Epoch 99/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 765.5419 - val_loss: 886.4916\n",
      "Epoch 100/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 745.7930 - val_loss: 875.7570\n",
      "Epoch 101/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 726.4266 - val_loss: 865.2887\n",
      "Epoch 102/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 707.4379 - val_loss: 855.0839\n",
      "Epoch 103/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 688.8231 - val_loss: 845.1390\n",
      "Epoch 104/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 670.5778 - val_loss: 835.4507\n",
      "Epoch 105/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 652.6971 - val_loss: 826.0154\n",
      "Epoch 106/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 635.1771 - val_loss: 816.8295\n",
      "Epoch 107/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 618.0136 - val_loss: 807.8900\n",
      "Epoch 108/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 601.2021 - val_loss: 799.1933\n",
      "Epoch 109/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 584.7387 - val_loss: 790.7360\n",
      "Epoch 110/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 568.6184 - val_loss: 782.5145\n",
      "Epoch 111/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 552.8377 - val_loss: 774.5255\n",
      "Epoch 112/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 537.3916 - val_loss: 766.7656\n",
      "Epoch 113/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 522.2767 - val_loss: 759.2321\n",
      "Epoch 114/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 507.4884 - val_loss: 751.9205\n",
      "Epoch 115/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 493.0225 - val_loss: 744.8282\n",
      "Epoch 116/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 478.8748 - val_loss: 737.9518\n",
      "Epoch 117/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 465.0410 - val_loss: 731.2877\n",
      "Epoch 118/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 451.5175 - val_loss: 724.8327\n",
      "Epoch 119/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 438.2994 - val_loss: 718.5833\n",
      "Epoch 120/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 425.3829 - val_loss: 712.5361\n",
      "Epoch 121/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 412.7639 - val_loss: 706.6887\n",
      "Epoch 122/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 400.4380 - val_loss: 701.0366\n",
      "Epoch 123/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 388.4016 - val_loss: 695.5771\n",
      "Epoch 124/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 376.6502 - val_loss: 690.3067\n",
      "Epoch 125/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 365.1796 - val_loss: 685.2225\n",
      "Epoch 126/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 353.9864 - val_loss: 680.3209\n",
      "Epoch 127/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 343.0658 - val_loss: 675.5988\n",
      "Epoch 128/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 332.4142 - val_loss: 671.0526\n",
      "Epoch 129/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 322.0269 - val_loss: 666.6795\n",
      "Epoch 130/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 311.9009 - val_loss: 662.4759\n",
      "Epoch 131/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 302.0313 - val_loss: 658.4389\n",
      "Epoch 132/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 292.4144 - val_loss: 654.5650\n",
      "Epoch 133/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 283.0462 - val_loss: 650.8510\n",
      "Epoch 134/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 273.9226 - val_loss: 647.2939\n",
      "Epoch 135/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 265.0397 - val_loss: 643.8902\n",
      "Epoch 136/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 256.3935 - val_loss: 640.6371\n",
      "Epoch 137/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 247.9800 - val_loss: 637.5310\n",
      "Epoch 138/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 239.7954 - val_loss: 634.5693\n",
      "Epoch 139/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 231.8359 - val_loss: 631.7484\n",
      "Epoch 140/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 224.0977 - val_loss: 629.0653\n",
      "Epoch 141/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 216.5766 - val_loss: 626.5169\n",
      "Epoch 142/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 209.2688 - val_loss: 624.1002\n",
      "Epoch 143/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 202.1704 - val_loss: 621.8119\n",
      "Epoch 144/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 195.2776 - val_loss: 619.6491\n",
      "Epoch 145/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 188.5865 - val_loss: 617.6085\n",
      "Epoch 146/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 182.0934 - val_loss: 615.6875\n",
      "Epoch 147/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 175.7947 - val_loss: 613.8826\n",
      "Epoch 148/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 169.6862 - val_loss: 612.1909\n",
      "Epoch 149/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 163.7642 - val_loss: 610.6094\n",
      "Epoch 150/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 158.0251 - val_loss: 609.1353\n",
      "Epoch 151/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 152.4654 - val_loss: 607.7656\n",
      "Epoch 152/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 147.0812 - val_loss: 606.4971\n",
      "Epoch 153/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 141.8686 - val_loss: 605.3273\n",
      "Epoch 154/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 136.8242 - val_loss: 604.2528\n",
      "Epoch 155/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 131.9446 - val_loss: 603.2713\n",
      "Epoch 156/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 127.2261 - val_loss: 602.3795\n",
      "Epoch 157/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 122.6649 - val_loss: 601.5748\n",
      "Epoch 158/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 118.2575 - val_loss: 600.8542\n",
      "Epoch 159/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 114.0006 - val_loss: 600.2152\n",
      "Epoch 160/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 109.8904 - val_loss: 599.6548\n",
      "Epoch 161/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 105.9238 - val_loss: 599.1704\n",
      "Epoch 162/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 102.0970 - val_loss: 598.7593\n",
      "Epoch 163/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 98.4068 - val_loss: 598.4188\n",
      "Epoch 164/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 94.8499 - val_loss: 598.1462\n",
      "Epoch 165/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 91.4228 - val_loss: 597.9391\n",
      "Epoch 166/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 88.1223 - val_loss: 597.7947\n",
      "Epoch 167/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 84.9451 - val_loss: 597.7106\n",
      "Epoch 168/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 81.8879 - val_loss: 597.6841\n",
      "Epoch 169/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 78.9475 - val_loss: 597.7130\n",
      "Epoch 170/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 76.1207 - val_loss: 597.7947\n",
      "Epoch 171/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 73.4044 - val_loss: 597.9268\n",
      "Epoch 172/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 70.7956 - val_loss: 598.1069\n",
      "Epoch 173/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 68.2912 - val_loss: 598.3326\n",
      "Epoch 174/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 65.8882 - val_loss: 598.6019\n",
      "Epoch 175/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 63.5836 - val_loss: 598.9124\n",
      "Epoch 176/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 61.3744 - val_loss: 599.2618\n",
      "Epoch 177/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 59.2577 - val_loss: 599.6480\n",
      "Epoch 178/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 57.2308 - val_loss: 600.0688\n",
      "Epoch 179/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 55.2906 - val_loss: 600.5222\n",
      "Epoch 180/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 53.4348 - val_loss: 601.0060\n",
      "Epoch 181/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 51.6604 - val_loss: 601.5184\n",
      "Epoch 182/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 49.9648 - val_loss: 602.0574\n",
      "Epoch 183/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 48.3453 - val_loss: 602.6211\n",
      "Epoch 184/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 46.7992 - val_loss: 603.2075\n",
      "Epoch 185/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 45.3244 - val_loss: 603.8148\n",
      "Epoch 186/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 43.9182 - val_loss: 604.4414\n",
      "Epoch 187/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 42.5779 - val_loss: 605.0854\n",
      "Epoch 188/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 41.3015 - val_loss: 605.7452\n",
      "Epoch 189/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 40.0866 - val_loss: 606.4191\n",
      "Epoch 190/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 38.9308 - val_loss: 607.1055\n",
      "Epoch 191/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 37.8320 - val_loss: 607.8030\n",
      "Epoch 192/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 36.7878 - val_loss: 608.5101\n",
      "Epoch 193/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 35.7963 - val_loss: 609.2251\n",
      "Epoch 194/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 34.8553 - val_loss: 609.9470\n",
      "Epoch 195/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.9627 - val_loss: 610.6742\n",
      "Epoch 196/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 33.1167 - val_loss: 611.4055\n",
      "Epoch 197/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 32.3153 - val_loss: 612.1398\n",
      "Epoch 198/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 31.5565 - val_loss: 612.8757\n",
      "Epoch 199/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 30.8386 - val_loss: 613.6119\n",
      "Epoch 200/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 30.1599 - val_loss: 614.3478\n",
      "Epoch 201/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 29.5185 - val_loss: 615.0820\n",
      "Epoch 202/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 28.9128 - val_loss: 615.8135\n",
      "Epoch 203/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 28.3413 - val_loss: 616.5416\n",
      "Epoch 204/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 27.8022 - val_loss: 617.2653\n",
      "Epoch 205/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 27.2943 - val_loss: 617.9833\n",
      "Epoch 206/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 26.8159 - val_loss: 618.6955\n",
      "Epoch 207/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 26.3656 - val_loss: 619.4008\n",
      "Epoch 208/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.9421 - val_loss: 620.0984\n",
      "Epoch 209/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.5440 - val_loss: 620.7878\n",
      "Epoch 210/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.1700 - val_loss: 621.4686\n",
      "Epoch 211/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.8190 - val_loss: 622.1398\n",
      "Epoch 212/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 24.4897 - val_loss: 622.8010\n",
      "Epoch 213/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 24.1810 - val_loss: 623.4518\n",
      "Epoch 214/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.8917 - val_loss: 624.0917\n",
      "Epoch 215/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 23.6209 - val_loss: 624.7203\n",
      "Epoch 216/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 23.3675 - val_loss: 625.3372\n",
      "Epoch 217/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 23.1308 - val_loss: 625.9420\n",
      "Epoch 218/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 22.9095 - val_loss: 626.5344\n",
      "Epoch 219/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 22.7029 - val_loss: 627.1143\n",
      "Epoch 220/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 22.5101 - val_loss: 627.6812\n",
      "Epoch 221/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 22.3303 - val_loss: 628.2350\n",
      "Epoch 222/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 22.1629 - val_loss: 628.7759\n",
      "Epoch 223/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 22.0069 - val_loss: 629.3035\n",
      "Epoch 224/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.8617 - val_loss: 629.8174\n",
      "Epoch 225/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.7267 - val_loss: 630.3182\n",
      "Epoch 226/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.6012 - val_loss: 630.8054\n",
      "Epoch 227/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.4846 - val_loss: 631.2788\n",
      "Epoch 228/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.3764 - val_loss: 631.7386\n",
      "Epoch 229/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.2760 - val_loss: 632.1853\n",
      "Epoch 230/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.1830 - val_loss: 632.6182\n",
      "Epoch 231/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.0967 - val_loss: 633.0377\n",
      "Epoch 232/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 21.0169 - val_loss: 633.4439\n",
      "Epoch 233/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.9430 - val_loss: 633.8373\n",
      "Epoch 234/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.8746 - val_loss: 634.2175\n",
      "Epoch 235/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.8114 - val_loss: 634.5848\n",
      "Epoch 236/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.7530 - val_loss: 634.9394\n",
      "Epoch 237/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.6991 - val_loss: 635.2817\n",
      "Epoch 238/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.6493 - val_loss: 635.6111\n",
      "Epoch 239/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.6033 - val_loss: 635.9287\n",
      "Epoch 240/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.5610 - val_loss: 636.2344\n",
      "Epoch 241/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.5220 - val_loss: 636.5284\n",
      "Epoch 242/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.4860 - val_loss: 636.8109\n",
      "Epoch 243/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.4529 - val_loss: 637.0826\n",
      "Epoch 244/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.4224 - val_loss: 637.3431\n",
      "Epoch 245/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.3943 - val_loss: 637.5928\n",
      "Epoch 246/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.3686 - val_loss: 637.8323\n",
      "Epoch 247/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.3448 - val_loss: 638.0615\n",
      "Epoch 248/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.3231 - val_loss: 638.2808\n",
      "Epoch 249/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.3031 - val_loss: 638.4906\n",
      "Epoch 250/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.2847 - val_loss: 638.6910\n",
      "Epoch 251/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.2678 - val_loss: 638.8824\n",
      "Epoch 252/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 20.2523 - val_loss: 639.0649\n",
      "Epoch 253/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.2381 - val_loss: 639.2391\n",
      "Epoch 254/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.2251 - val_loss: 639.4047\n",
      "Epoch 255/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.2132 - val_loss: 639.5626\n",
      "Epoch 256/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.2022 - val_loss: 639.7126\n",
      "Epoch 257/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1922 - val_loss: 639.8555\n",
      "Epoch 258/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1831 - val_loss: 639.9912\n",
      "Epoch 259/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1747 - val_loss: 640.1200\n",
      "Epoch 260/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1671 - val_loss: 640.2422\n",
      "Epoch 261/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1601 - val_loss: 640.3581\n",
      "Epoch 262/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1537 - val_loss: 640.4676\n",
      "Epoch 263/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1479 - val_loss: 640.5716\n",
      "Epoch 264/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1425 - val_loss: 640.6698\n",
      "Epoch 265/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1377 - val_loss: 640.7625\n",
      "Epoch 266/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.1333 - val_loss: 640.8500\n",
      "Epoch 267/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1293 - val_loss: 640.9326\n",
      "Epoch 268/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1257 - val_loss: 641.0105\n",
      "Epoch 269/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1224 - val_loss: 641.0841\n",
      "Epoch 270/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 20.1194 - val_loss: 641.1533\n",
      "Epoch 271/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1167 - val_loss: 641.2184\n",
      "Epoch 272/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1143 - val_loss: 641.2796\n",
      "Epoch 273/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1121 - val_loss: 641.3369\n",
      "Epoch 274/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1102 - val_loss: 641.3908\n",
      "Epoch 275/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1085 - val_loss: 641.4415\n",
      "Epoch 276/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1069 - val_loss: 641.4888\n",
      "Epoch 277/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1055 - val_loss: 641.5330\n",
      "Epoch 278/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1044 - val_loss: 641.5745\n",
      "Epoch 279/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1034 - val_loss: 641.6133\n",
      "Epoch 280/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1024 - val_loss: 641.6494\n",
      "Epoch 281/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1017 - val_loss: 641.6832\n",
      "Epoch 282/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 20.1010 - val_loss: 641.7148\n",
      "Epoch 283/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1005 - val_loss: 641.7441\n",
      "Epoch 284/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1001 - val_loss: 641.7714\n",
      "Epoch 285/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.0997 - val_loss: 641.7967\n",
      "Epoch 286/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.0994 - val_loss: 641.8202\n",
      "Epoch 287/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.0993 - val_loss: 641.8420\n",
      "Epoch 288/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 20.0993 - val_loss: 641.8622\n",
      "Epoch 289/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.0992 - val_loss: 641.8812\n",
      "Epoch 290/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.0993 - val_loss: 641.8985\n",
      "Epoch 291/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.0994 - val_loss: 641.9146\n",
      "Epoch 292/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.0995 - val_loss: 641.9292\n",
      "Epoch 293/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.0997 - val_loss: 641.9431\n",
      "Epoch 294/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1000 - val_loss: 641.9558\n",
      "Epoch 295/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1003 - val_loss: 641.9675\n",
      "Epoch 296/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1007 - val_loss: 641.9781\n",
      "Epoch 297/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1011 - val_loss: 641.9883\n",
      "Epoch 298/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 20.1014 - val_loss: 641.9969\n",
      "Epoch 299/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 20.1019 - val_loss: 642.0050\n",
      "Epoch 300/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1023 - val_loss: 642.0123\n",
      "Epoch 301/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1028 - val_loss: 642.0191\n",
      "Epoch 302/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1033 - val_loss: 642.0250\n",
      "Epoch 303/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1039 - val_loss: 642.0304\n",
      "Epoch 304/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 20.1045 - val_loss: 642.0353\n",
      "Epoch 305/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1051 - val_loss: 642.0400\n",
      "Epoch 306/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1057 - val_loss: 642.0440\n",
      "Epoch 307/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1063 - val_loss: 642.0477\n",
      "Epoch 308/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1069 - val_loss: 642.0510\n",
      "Epoch 309/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1076 - val_loss: 642.0538\n",
      "Epoch 310/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1082 - val_loss: 642.0563\n",
      "Epoch 311/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1089 - val_loss: 642.0585\n",
      "Epoch 312/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1095 - val_loss: 642.0606\n",
      "Epoch 313/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1102 - val_loss: 642.0623\n",
      "Epoch 314/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1109 - val_loss: 642.0638\n",
      "Epoch 315/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1116 - val_loss: 642.0650\n",
      "Epoch 316/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1123 - val_loss: 642.0660\n",
      "Epoch 317/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1130 - val_loss: 642.0668\n",
      "Epoch 318/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1137 - val_loss: 642.0675\n",
      "Epoch 319/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1144 - val_loss: 642.0681\n",
      "Epoch 320/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 20.1151 - val_loss: 642.0685\n",
      "Epoch 321/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1158 - val_loss: 642.0688\n",
      "Epoch 322/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1165 - val_loss: 642.0689\n",
      "Epoch 323/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1172 - val_loss: 642.0690\n",
      "Epoch 324/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1178 - val_loss: 642.0690\n",
      "Epoch 325/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1185 - val_loss: 642.0689\n",
      "Epoch 326/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1192 - val_loss: 642.0686\n",
      "Epoch 327/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1199 - val_loss: 642.0681\n",
      "Epoch 328/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1206 - val_loss: 642.0680\n",
      "Epoch 329/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1213 - val_loss: 642.0674\n",
      "Epoch 330/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1220 - val_loss: 642.0669\n",
      "Epoch 331/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1227 - val_loss: 642.0665\n",
      "Epoch 332/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1233 - val_loss: 642.0660\n",
      "Epoch 333/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1240 - val_loss: 642.0653\n",
      "Epoch 334/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1247 - val_loss: 642.0646\n",
      "Epoch 335/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1253 - val_loss: 642.0640\n",
      "Epoch 336/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 20.1260 - val_loss: 642.0632\n",
      "Epoch 337/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1266 - val_loss: 642.0626\n",
      "Epoch 338/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1273 - val_loss: 642.0617\n",
      "Epoch 339/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1279 - val_loss: 642.0611\n",
      "Epoch 340/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1285 - val_loss: 642.0603\n",
      "Epoch 341/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1291 - val_loss: 642.0598\n",
      "Epoch 342/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1297 - val_loss: 642.0590\n",
      "Epoch 343/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1303 - val_loss: 642.0582\n",
      "Epoch 344/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1309 - val_loss: 642.0575\n",
      "Epoch 345/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1315 - val_loss: 642.0566\n",
      "Epoch 346/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1321 - val_loss: 642.0558\n",
      "Epoch 347/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1326 - val_loss: 642.0549\n",
      "Epoch 348/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1332 - val_loss: 642.0542\n",
      "Epoch 349/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1338 - val_loss: 642.0535\n",
      "Epoch 350/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1344 - val_loss: 642.0527\n",
      "Epoch 351/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1349 - val_loss: 642.0519\n",
      "Epoch 352/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 20.1355 - val_loss: 642.0511\n",
      "Epoch 353/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1360 - val_loss: 642.0504\n",
      "Epoch 354/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1366 - val_loss: 642.0496\n",
      "Epoch 355/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1370 - val_loss: 642.0485\n",
      "Epoch 356/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1376 - val_loss: 642.0479\n",
      "Epoch 357/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1381 - val_loss: 642.0471\n",
      "Epoch 358/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1386 - val_loss: 642.0463\n",
      "Epoch 359/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1391 - val_loss: 642.0455\n",
      "Epoch 360/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1396 - val_loss: 642.0447\n",
      "Epoch 361/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1401 - val_loss: 642.0440\n",
      "Epoch 362/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1406 - val_loss: 642.0432\n",
      "Epoch 363/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1410 - val_loss: 642.0427\n",
      "Epoch 364/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1415 - val_loss: 642.0421\n",
      "Epoch 365/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1420 - val_loss: 642.0415\n",
      "Epoch 366/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1424 - val_loss: 642.0406\n",
      "Epoch 367/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1428 - val_loss: 642.0401\n",
      "Epoch 368/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 20.1432 - val_loss: 642.0394\n",
      "Epoch 369/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1437 - val_loss: 642.0387\n",
      "Epoch 370/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1441 - val_loss: 642.0381\n",
      "Epoch 371/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1444 - val_loss: 642.0372\n",
      "Epoch 372/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1449 - val_loss: 642.0366\n",
      "Epoch 373/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1453 - val_loss: 642.0360\n",
      "Epoch 374/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1457 - val_loss: 642.0354\n",
      "Epoch 375/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1461 - val_loss: 642.0347\n",
      "Epoch 376/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1465 - val_loss: 642.0342\n",
      "Epoch 377/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.1468 - val_loss: 642.0335\n",
      "Epoch 378/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1472 - val_loss: 642.0328\n",
      "Epoch 379/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1476 - val_loss: 642.0323\n",
      "Epoch 380/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1480 - val_loss: 642.0319\n",
      "Epoch 381/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1483 - val_loss: 642.0316\n",
      "Epoch 382/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1486 - val_loss: 642.0312\n",
      "Epoch 383/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1489 - val_loss: 642.0305\n",
      "Epoch 384/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1492 - val_loss: 642.0298\n",
      "Epoch 385/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.1497 - val_loss: 642.0295\n",
      "Epoch 386/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1499 - val_loss: 642.0289\n",
      "Epoch 387/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1503 - val_loss: 642.0283\n",
      "Epoch 388/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1506 - val_loss: 642.0278\n",
      "Epoch 389/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1509 - val_loss: 642.0274\n",
      "Epoch 390/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1512 - val_loss: 642.0269\n",
      "Epoch 391/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1514 - val_loss: 642.0266\n",
      "Epoch 392/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1517 - val_loss: 642.0261\n",
      "Epoch 393/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1520 - val_loss: 642.0256\n",
      "Epoch 394/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1523 - val_loss: 642.0250\n",
      "Epoch 395/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1526 - val_loss: 642.0245\n",
      "Epoch 396/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1528 - val_loss: 642.0242\n",
      "Epoch 397/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1531 - val_loss: 642.0237\n",
      "Epoch 398/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1533 - val_loss: 642.0231\n",
      "Epoch 399/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1536 - val_loss: 642.0228\n",
      "Epoch 400/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 20.1539 - val_loss: 642.0225\n",
      "Epoch 401/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1541 - val_loss: 642.0222\n",
      "Epoch 402/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1543 - val_loss: 642.0219\n",
      "Epoch 403/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1546 - val_loss: 642.0214\n",
      "Epoch 404/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1547 - val_loss: 642.0208\n",
      "Epoch 405/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1550 - val_loss: 642.0206\n",
      "Epoch 406/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1553 - val_loss: 642.0204\n",
      "Epoch 407/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1555 - val_loss: 642.0202\n",
      "Epoch 408/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1557 - val_loss: 642.0199\n",
      "Epoch 409/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1558 - val_loss: 642.0195\n",
      "Epoch 410/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1561 - val_loss: 642.0193\n",
      "Epoch 411/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1563 - val_loss: 642.0193\n",
      "Epoch 412/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1564 - val_loss: 642.0189\n",
      "Epoch 413/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1566 - val_loss: 642.0184\n",
      "Epoch 414/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1568 - val_loss: 642.0182\n",
      "Epoch 415/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1570 - val_loss: 642.0179\n",
      "Epoch 416/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1572 - val_loss: 642.0178\n",
      "Epoch 417/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1573 - val_loss: 642.0173\n",
      "Epoch 418/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 20.1575 - val_loss: 642.0168\n",
      "Epoch 419/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1577 - val_loss: 642.0166\n",
      "Epoch 420/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1579 - val_loss: 642.0163\n",
      "Epoch 421/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1581 - val_loss: 642.0162\n",
      "Epoch 422/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1582 - val_loss: 642.0159\n",
      "Epoch 423/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1583 - val_loss: 642.0157\n",
      "Epoch 424/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1585 - val_loss: 642.0153\n",
      "Epoch 425/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1586 - val_loss: 642.0148\n",
      "Epoch 426/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1588 - val_loss: 642.0146\n",
      "Epoch 427/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1589 - val_loss: 642.0143\n",
      "Epoch 428/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1591 - val_loss: 642.0140\n",
      "Epoch 429/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1592 - val_loss: 642.0137\n",
      "Epoch 430/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1594 - val_loss: 642.0134\n",
      "Epoch 431/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1595 - val_loss: 642.0133\n",
      "Epoch 432/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1596 - val_loss: 642.0131\n",
      "Epoch 433/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1597 - val_loss: 642.0128\n",
      "Epoch 434/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1599 - val_loss: 642.0126\n",
      "Epoch 435/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1600 - val_loss: 642.0124\n",
      "Epoch 436/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1601 - val_loss: 642.0124\n",
      "Epoch 437/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1602 - val_loss: 642.0121\n",
      "Epoch 438/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1603 - val_loss: 642.0118\n",
      "Epoch 439/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1605 - val_loss: 642.0117\n",
      "Epoch 440/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1605 - val_loss: 642.0116\n",
      "Epoch 441/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 20.1607 - val_loss: 642.0114\n",
      "Epoch 442/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1608 - val_loss: 642.0112\n",
      "Epoch 443/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1609 - val_loss: 642.0110\n",
      "Epoch 444/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1610 - val_loss: 642.0107\n",
      "Epoch 445/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1611 - val_loss: 642.0107\n",
      "Epoch 446/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1611 - val_loss: 642.0106\n",
      "Epoch 447/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1612 - val_loss: 642.0103\n",
      "Epoch 448/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1613 - val_loss: 642.0101\n",
      "Epoch 449/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1615 - val_loss: 642.0101\n",
      "Epoch 450/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1615 - val_loss: 642.0099\n",
      "Epoch 451/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1616 - val_loss: 642.0098\n",
      "Epoch 452/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1617 - val_loss: 642.0098\n",
      "Epoch 453/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1618 - val_loss: 642.0095\n",
      "Epoch 454/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1619 - val_loss: 642.0094\n",
      "Epoch 455/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1619 - val_loss: 642.0093\n",
      "Epoch 456/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1620 - val_loss: 642.0092\n",
      "Epoch 457/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1621 - val_loss: 642.0091\n",
      "Epoch 458/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1621 - val_loss: 642.0089\n",
      "Epoch 459/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1622 - val_loss: 642.0088\n",
      "Epoch 460/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 20.1623 - val_loss: 642.0087\n",
      "Epoch 461/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1624 - val_loss: 642.0085\n",
      "Epoch 462/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1624 - val_loss: 642.0084\n",
      "Epoch 463/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1625 - val_loss: 642.0084\n",
      "Epoch 464/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1626 - val_loss: 642.0084\n",
      "Epoch 465/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1627 - val_loss: 642.0084\n",
      "Epoch 466/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1627 - val_loss: 642.0084\n",
      "Epoch 467/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1627 - val_loss: 642.0084\n",
      "Epoch 468/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1628 - val_loss: 642.0080\n",
      "Epoch 469/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1628 - val_loss: 642.0079\n",
      "Epoch 470/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1629 - val_loss: 642.0079\n",
      "Epoch 471/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1630 - val_loss: 642.0078\n",
      "Epoch 472/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1630 - val_loss: 642.0078\n",
      "Epoch 473/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1631 - val_loss: 642.0075\n",
      "Epoch 474/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1631 - val_loss: 642.0074\n",
      "Epoch 475/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1632 - val_loss: 642.0075\n",
      "Epoch 476/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1632 - val_loss: 642.0076\n",
      "Epoch 477/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1632 - val_loss: 642.0074\n",
      "Epoch 478/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1633 - val_loss: 642.0073\n",
      "Epoch 479/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1634 - val_loss: 642.0070\n",
      "Epoch 480/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1634 - val_loss: 642.0068\n",
      "Epoch 481/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 20.1635 - val_loss: 642.0068\n",
      "Epoch 482/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1635 - val_loss: 642.0068\n",
      "Epoch 483/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1635 - val_loss: 642.0068\n",
      "Epoch 484/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1636 - val_loss: 642.0068\n",
      "Epoch 485/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1636 - val_loss: 642.0067\n",
      "Epoch 486/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1636 - val_loss: 642.0064\n",
      "Epoch 487/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1637 - val_loss: 642.0064\n",
      "Epoch 488/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1638 - val_loss: 642.0064\n",
      "Epoch 489/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1638 - val_loss: 642.0064\n",
      "Epoch 490/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1638 - val_loss: 642.0062\n",
      "Epoch 491/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1638 - val_loss: 642.0062\n",
      "Epoch 492/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1639 - val_loss: 642.0062\n",
      "Epoch 493/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1639 - val_loss: 642.0059\n",
      "Epoch 494/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1639 - val_loss: 642.0056\n",
      "Epoch 495/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1640 - val_loss: 642.0056\n",
      "Epoch 496/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1640 - val_loss: 642.0058\n",
      "Epoch 497/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1640 - val_loss: 642.0057\n",
      "Epoch 498/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1641 - val_loss: 642.0056\n",
      "Epoch 499/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1641 - val_loss: 642.0056\n",
      "Epoch 500/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.1641 - val_loss: 642.0054\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(89.3180, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0010, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(-0.0204, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.2194, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 393ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.07611111e+01, 7.06434641e+01, 7.05258170e+01, 7.04081699e+01,\n",
       "        7.02905229e+01, 7.01728758e+01, 7.00552288e+01, 1.70905810e-01,\n",
       "        2.39051640e-01, 0.00000000e+00, 5.89051310e-01, 0.00000000e+00,\n",
       "        5.79741060e-01, 7.05388889e+01, 7.04212418e+01, 7.03035948e+01,\n",
       "        7.01859477e+01, 7.00683006e+01, 6.99506536e+01, 6.98330065e+01,\n",
       "        6.97153595e+01, 6.95977124e+01, 6.94900327e+01, 6.94312092e+01,\n",
       "        6.93723856e+01, 6.93135621e+01, 6.92547386e+01, 6.91959150e+01,\n",
       "        6.91370915e+01, 6.90782680e+01, 6.90194444e+01, 6.89606209e+01,\n",
       "        6.89017974e+01, 6.88429739e+01, 6.87977358e+01, 6.87893324e+01,\n",
       "        6.87809290e+01, 6.87725257e+01, 6.87641223e+01, 6.87557190e+01,\n",
       "        2.61637956e-01, 1.06601441e+00, 0.00000000e+00, 5.99627720e-02,\n",
       "        6.04974806e-01, 4.62526143e-01, 0.00000000e+00, 7.04604575e+01,\n",
       "        7.03428105e+01, 7.02251634e+01, 7.01075163e+01, 6.99898693e+01,\n",
       "        6.98722222e+01, 6.97545752e+01, 6.96369281e+01, 6.95192811e+01,\n",
       "        6.94508170e+01, 6.93919935e+01, 6.93331699e+01, 6.92743464e+01,\n",
       "        6.92155229e+01, 6.91566993e+01, 6.90987758e+01, 6.90380523e+01,\n",
       "        6.89822880e+01, 6.89214052e+01, 6.88625817e+01, 6.88037582e+01,\n",
       "        6.87921335e+01, 6.87837302e+01, 6.87753268e+01, 6.87669234e+01,\n",
       "        6.87585201e+01, 7.66012421e+01, 5.02604306e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.92042941e-01, 5.32064199e-01, 0.00000000e+00,\n",
       "        4.54188614e+01, 0.00000000e+00, 3.49431276e-01, 2.91158468e-01,\n",
       "        2.25519612e-01, 0.00000000e+00, 8.49562734e-02, 3.20042707e-02,\n",
       "        0.00000000e+00, 1.54605359e-02, 6.79415017e-02, 6.77158535e-01,\n",
       "        7.24539340e-01, 1.26410097e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.17600942e-01, 0.00000000e+00, 9.74744320e-01, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([64.30530846, 64.29765662, 64.29000478, 64.28235294, 64.2747011 ,\n",
       "       64.26704926, 64.25939742, 64.25174558, 64.24409374, 64.23644189,\n",
       "       64.22879005, 64.22113821, 64.21348637, 64.20583453, 64.19818269,\n",
       "       64.19053085, 64.18287901, 64.17522716, 64.16757532, 64.15992348,\n",
       "       64.15227164, 64.1446198 , 64.13696796, 64.12931612, 64.12166428,\n",
       "       64.11401243, 64.10636059, 64.09870875, 64.09105691, 64.08340507,\n",
       "       64.07575323, 64.06810139, 64.06044955, 64.0527977 , 64.04514586,\n",
       "       64.03749402, 64.02984218, 64.02219034, 64.0145385 , 64.00688666,\n",
       "       63.99923482, 63.99158297, 63.98393113, 63.97627929, 63.96862745,\n",
       "       63.96097561, 63.95332377, 63.94567193, 63.93802009, 63.93036824,\n",
       "       63.9227164 , 63.91506456, 63.90741272, 63.89976088, 63.89210904,\n",
       "       63.8844572 , 63.87680536, 63.86915352, 63.86150167, 63.85384983,\n",
       "       63.84619799, 63.83854615, 63.83089431, 63.82324247, 63.81559063,\n",
       "       63.80793879, 63.80028694, 63.7926351 , 63.78498326, 63.77733142,\n",
       "       63.76967958, 63.76202774, 63.7543759 , 63.74672406, 63.73907221,\n",
       "       63.73142037, 63.72376853, 63.71611669, 63.70846485, 63.70081301,\n",
       "       63.69316117, 63.68550933, 63.67785748, 63.67020564, 63.6625538 ,\n",
       "       63.65490196, 63.64725012, 63.63959828, 63.63194644, 63.6242946 ,\n",
       "       63.61664275, 63.60899091, 63.60133907, 63.59368723, 63.58603539,\n",
       "       63.57838355, 63.57073171, 63.56307987, 63.55542802, 63.54777618])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.465272430616885\n",
      "27.74243172487269\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
