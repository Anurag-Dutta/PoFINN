{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       92.600000\n",
       "1       92.387115\n",
       "2       92.174230\n",
       "3       91.961345\n",
       "4       91.748459\n",
       "          ...    \n",
       "1945    67.043873\n",
       "1946    67.034069\n",
       "1947    67.024265\n",
       "1948    67.014461\n",
       "1949    67.004657\n",
       "Name: C7, Length: 1950, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c7_interpolated_1850_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       92.600000\n",
       "1       92.387115\n",
       "2       92.174230\n",
       "3       91.961345\n",
       "4       91.748459\n",
       "          ...    \n",
       "1845     0.000000\n",
       "1846     0.000000\n",
       "1847     0.579741\n",
       "1848     0.334582\n",
       "1849     0.000000\n",
       "Name: C7, Length: 1850, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1850)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92.387115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92.174230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.961345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91.748459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     92.600000  0.000298\n",
       "1     92.387115  0.000298\n",
       "2     92.174230  0.000297\n",
       "3     91.961345  0.000297\n",
       "4     91.748459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 89.3180  # Value for C0\n",
    "K0 = -0.0010  # Value for K0\n",
    "K1 = 0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = -0.0204    # Value for b\n",
    "c = 2.2194    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    92.600000    92.387115    92.174230    91.961345    91.748459   \n",
      "351    92.387115    92.174230    91.961345    91.748459    91.535574   \n",
      "352    92.174230    91.961345    91.748459    91.535574    91.322689   \n",
      "353    91.961345    91.748459    91.535574    91.322689    91.109804   \n",
      "354    91.748459    91.535574    91.322689    91.109804    90.896919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    91.535574    91.322689    91.109804    90.896919    90.691597  ...   \n",
      "351    91.322689    91.109804    90.896919    90.691597    90.579552  ...   \n",
      "352    91.109804    90.896919    90.691597    90.579552    90.467507  ...   \n",
      "353    90.896919    90.691597    90.579552    90.467507    90.355462  ...   \n",
      "354    90.691597    90.579552    90.467507    90.355462    90.243417  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   80.261671    0.000263   80.252334    0.000263   80.242997    0.000263   \n",
      "351   80.252334    0.000263   80.242997    0.000263   80.233660    0.000262   \n",
      "352   80.242997    0.000263   80.233660    0.000262   80.224323    0.000262   \n",
      "353   80.233660    0.000262   80.224323    0.000262   80.214986    0.000262   \n",
      "354   80.224323    0.000262   80.214986    0.000262   80.205649    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   80.233660    0.000262   80.224323    0.000262  \n",
      "351   80.224323    0.000262   80.214986    0.000262  \n",
      "352   80.214986    0.000262   80.205649    0.000262  \n",
      "353   80.205649    0.000262   80.196312    0.000262  \n",
      "354   80.196312    0.000262   80.186975    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 1, 251) (1400, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "18/18 [==============================] - 3s 28ms/step - loss: 5700.1050 - val_loss: 4595.1802\n",
      "Epoch 2/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5613.1333 - val_loss: 4546.3711\n",
      "Epoch 3/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5559.1880 - val_loss: 4497.8086\n",
      "Epoch 4/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5505.6255 - val_loss: 4449.6807\n",
      "Epoch 5/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5443.9155 - val_loss: 4390.3394\n",
      "Epoch 6/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5377.7642 - val_loss: 4329.3037\n",
      "Epoch 7/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5316.2285 - val_loss: 4276.1299\n",
      "Epoch 8/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5257.8115 - val_loss: 4224.1543\n",
      "Epoch 9/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5200.5566 - val_loss: 4163.0898\n",
      "Epoch 10/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5131.2437 - val_loss: 4108.7881\n",
      "Epoch 11/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5070.9863 - val_loss: 4055.2170\n",
      "Epoch 12/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5012.0005 - val_loss: 4002.8923\n",
      "Epoch 13/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4954.2329 - val_loss: 3951.5576\n",
      "Epoch 14/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4897.4370 - val_loss: 3901.0388\n",
      "Epoch 15/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4841.4556 - val_loss: 3851.2295\n",
      "Epoch 16/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4786.1909 - val_loss: 3802.0603\n",
      "Epoch 17/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4731.5811 - val_loss: 3753.4866\n",
      "Epoch 18/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4677.5811 - val_loss: 3705.4736\n",
      "Epoch 19/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 4624.1558 - val_loss: 3657.9939\n",
      "Epoch 20/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4571.2837 - val_loss: 3611.0291\n",
      "Epoch 21/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4518.9419 - val_loss: 3564.5615\n",
      "Epoch 22/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4467.1138 - val_loss: 3518.5781\n",
      "Epoch 23/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4415.7861 - val_loss: 3473.0664\n",
      "Epoch 24/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4364.9458 - val_loss: 3428.0181\n",
      "Epoch 25/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4314.5854 - val_loss: 3383.4221\n",
      "Epoch 26/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4264.6934 - val_loss: 3339.2722\n",
      "Epoch 27/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4215.2627 - val_loss: 3295.5608\n",
      "Epoch 28/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4166.2856 - val_loss: 3252.2805\n",
      "Epoch 29/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4117.7563 - val_loss: 3209.4275\n",
      "Epoch 30/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4069.6687 - val_loss: 3166.9944\n",
      "Epoch 31/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4022.0168 - val_loss: 3124.9775\n",
      "Epoch 32/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3974.7952 - val_loss: 3083.3718\n",
      "Epoch 33/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3928.0005 - val_loss: 3042.1733\n",
      "Epoch 34/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3881.6260 - val_loss: 3001.3762\n",
      "Epoch 35/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 3835.6692 - val_loss: 2960.9778\n",
      "Epoch 36/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3790.1240 - val_loss: 2920.9746\n",
      "Epoch 37/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3744.9883 - val_loss: 2881.3613\n",
      "Epoch 38/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3700.2581 - val_loss: 2842.1355\n",
      "Epoch 39/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3655.9297 - val_loss: 2803.2937\n",
      "Epoch 40/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3611.9988 - val_loss: 2764.8325\n",
      "Epoch 41/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3568.4622 - val_loss: 2726.7488\n",
      "Epoch 42/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3525.3169 - val_loss: 2689.0388\n",
      "Epoch 43/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3482.5605 - val_loss: 2651.7004\n",
      "Epoch 44/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3440.1885 - val_loss: 2614.7307\n",
      "Epoch 45/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3398.1982 - val_loss: 2578.1257\n",
      "Epoch 46/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3356.5879 - val_loss: 2541.8840\n",
      "Epoch 47/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3315.3530 - val_loss: 2506.0012\n",
      "Epoch 48/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3274.4919 - val_loss: 2470.4761\n",
      "Epoch 49/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3234.0012 - val_loss: 2435.3057\n",
      "Epoch 50/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3193.8787 - val_loss: 2400.4866\n",
      "Epoch 51/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3154.1206 - val_loss: 2366.0166\n",
      "Epoch 52/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3114.7263 - val_loss: 2331.8928\n",
      "Epoch 53/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3075.6909 - val_loss: 2298.1138\n",
      "Epoch 54/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3037.0139 - val_loss: 2264.6763\n",
      "Epoch 55/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2998.6914 - val_loss: 2231.5786\n",
      "Epoch 56/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2960.7219 - val_loss: 2198.8184\n",
      "Epoch 57/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2923.1025 - val_loss: 2166.3918\n",
      "Epoch 58/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2885.8303 - val_loss: 2134.2981\n",
      "Epoch 59/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2848.9043 - val_loss: 2102.5332\n",
      "Epoch 60/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2812.3213 - val_loss: 2071.0962\n",
      "Epoch 61/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2776.0784 - val_loss: 2039.9850\n",
      "Epoch 62/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2740.1738 - val_loss: 2009.1975\n",
      "Epoch 63/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 2704.6069 - val_loss: 1978.7303\n",
      "Epoch 64/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2669.3733 - val_loss: 1948.5819\n",
      "Epoch 65/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2634.4714 - val_loss: 1918.7498\n",
      "Epoch 66/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2599.8984 - val_loss: 1889.2330\n",
      "Epoch 67/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2565.6538 - val_loss: 1860.0281\n",
      "Epoch 68/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2531.7341 - val_loss: 1831.1339\n",
      "Epoch 69/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2498.1382 - val_loss: 1802.5474\n",
      "Epoch 70/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2464.8633 - val_loss: 1774.2677\n",
      "Epoch 71/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2431.9075 - val_loss: 1746.2916\n",
      "Epoch 72/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2399.2693 - val_loss: 1718.6176\n",
      "Epoch 73/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2366.9451 - val_loss: 1691.2438\n",
      "Epoch 74/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2334.9346 - val_loss: 1664.1682\n",
      "Epoch 75/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2303.2351 - val_loss: 1637.3884\n",
      "Epoch 76/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2271.8438 - val_loss: 1610.9031\n",
      "Epoch 77/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2240.7607 - val_loss: 1584.7101\n",
      "Epoch 78/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2209.9822 - val_loss: 1558.8066\n",
      "Epoch 79/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2179.5063 - val_loss: 1533.1920\n",
      "Epoch 80/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2149.3318 - val_loss: 1507.8632\n",
      "Epoch 81/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2119.4565 - val_loss: 1482.8192\n",
      "Epoch 82/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2089.8789 - val_loss: 1458.0579\n",
      "Epoch 83/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2060.5969 - val_loss: 1433.5775\n",
      "Epoch 84/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2031.6075 - val_loss: 1409.3750\n",
      "Epoch 85/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2002.9108 - val_loss: 1385.4504\n",
      "Epoch 86/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1974.5038 - val_loss: 1361.8009\n",
      "Epoch 87/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1946.3848 - val_loss: 1338.4249\n",
      "Epoch 88/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1918.5520 - val_loss: 1315.3197\n",
      "Epoch 89/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1891.0035 - val_loss: 1292.4850\n",
      "Epoch 90/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1863.7378 - val_loss: 1269.9180\n",
      "Epoch 91/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1836.7531 - val_loss: 1247.6166\n",
      "Epoch 92/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1810.0474 - val_loss: 1225.5802\n",
      "Epoch 93/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1783.6189 - val_loss: 1203.8064\n",
      "Epoch 94/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1757.4659 - val_loss: 1182.2933\n",
      "Epoch 95/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1731.5867 - val_loss: 1161.0391\n",
      "Epoch 96/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1705.9799 - val_loss: 1140.0425\n",
      "Epoch 97/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1680.6433 - val_loss: 1119.3018\n",
      "Epoch 98/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1655.5751 - val_loss: 1098.8146\n",
      "Epoch 99/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1630.7740 - val_loss: 1078.5797\n",
      "Epoch 100/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1606.2383 - val_loss: 1058.5956\n",
      "Epoch 101/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1581.9661 - val_loss: 1038.8594\n",
      "Epoch 102/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1557.9552 - val_loss: 1019.3708\n",
      "Epoch 103/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1534.2047 - val_loss: 1000.1279\n",
      "Epoch 104/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1510.7123 - val_loss: 981.1281\n",
      "Epoch 105/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1487.4771 - val_loss: 962.3704\n",
      "Epoch 106/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1464.4967 - val_loss: 943.8532\n",
      "Epoch 107/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1441.7701 - val_loss: 925.5746\n",
      "Epoch 108/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1419.2953 - val_loss: 907.5331\n",
      "Epoch 109/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1397.0706 - val_loss: 889.7266\n",
      "Epoch 110/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1375.0945 - val_loss: 872.1547\n",
      "Epoch 111/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1353.3655 - val_loss: 854.8141\n",
      "Epoch 112/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1331.8818 - val_loss: 837.7045\n",
      "Epoch 113/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1310.6417 - val_loss: 820.8237\n",
      "Epoch 114/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1289.6439 - val_loss: 804.1695\n",
      "Epoch 115/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1268.8865 - val_loss: 787.7418\n",
      "Epoch 116/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1248.3680 - val_loss: 771.5375\n",
      "Epoch 117/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1228.0874 - val_loss: 755.5554\n",
      "Epoch 118/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1208.0425 - val_loss: 739.7952\n",
      "Epoch 119/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1188.2312 - val_loss: 724.2536\n",
      "Epoch 120/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1168.6532 - val_loss: 708.9297\n",
      "Epoch 121/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1149.3064 - val_loss: 693.8213\n",
      "Epoch 122/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1130.1891 - val_loss: 678.9277\n",
      "Epoch 123/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1111.2994 - val_loss: 664.2473\n",
      "Epoch 124/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1092.6365 - val_loss: 649.7781\n",
      "Epoch 125/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1074.1986 - val_loss: 635.5184\n",
      "Epoch 126/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1055.9843 - val_loss: 621.4674\n",
      "Epoch 127/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1037.9915 - val_loss: 607.6222\n",
      "Epoch 128/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1020.2194 - val_loss: 593.9823\n",
      "Epoch 129/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1002.6655 - val_loss: 580.5459\n",
      "Epoch 130/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 985.3297 - val_loss: 567.3122\n",
      "Epoch 131/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 968.2094 - val_loss: 554.2776\n",
      "Epoch 132/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 951.3033 - val_loss: 541.4431\n",
      "Epoch 133/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 934.6101 - val_loss: 528.8050\n",
      "Epoch 134/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 918.1282 - val_loss: 516.3635\n",
      "Epoch 135/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 901.8562 - val_loss: 504.1156\n",
      "Epoch 136/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 885.7922 - val_loss: 492.0605\n",
      "Epoch 137/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 869.9352 - val_loss: 480.1964\n",
      "Epoch 138/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 854.2830 - val_loss: 468.5220\n",
      "Epoch 139/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 838.8353 - val_loss: 457.0358\n",
      "Epoch 140/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 823.5897 - val_loss: 445.7361\n",
      "Epoch 141/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 808.5449 - val_loss: 434.6218\n",
      "Epoch 142/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 793.6999 - val_loss: 423.6905\n",
      "Epoch 143/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 779.0522 - val_loss: 412.9417\n",
      "Epoch 144/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 764.6014 - val_loss: 402.3729\n",
      "Epoch 145/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 750.3452 - val_loss: 391.9831\n",
      "Epoch 146/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 736.2823 - val_loss: 381.7709\n",
      "Epoch 147/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 722.4119 - val_loss: 371.7344\n",
      "Epoch 148/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 708.7320 - val_loss: 361.8729\n",
      "Epoch 149/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 695.2413 - val_loss: 352.1838\n",
      "Epoch 150/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 681.9382 - val_loss: 342.6657\n",
      "Epoch 151/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 668.8208 - val_loss: 333.3180\n",
      "Epoch 152/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 655.8885 - val_loss: 324.1391\n",
      "Epoch 153/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 643.1398 - val_loss: 315.1267\n",
      "Epoch 154/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 630.5729 - val_loss: 306.2794\n",
      "Epoch 155/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 618.1858 - val_loss: 297.5962\n",
      "Epoch 156/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 605.9780 - val_loss: 289.0753\n",
      "Epoch 157/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 593.9481 - val_loss: 280.7155\n",
      "Epoch 158/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 582.0942 - val_loss: 272.5148\n",
      "Epoch 159/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 570.4148 - val_loss: 264.4722\n",
      "Epoch 160/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 558.9085 - val_loss: 256.5859\n",
      "Epoch 161/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 547.5738 - val_loss: 248.8539\n",
      "Epoch 162/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 536.4093 - val_loss: 241.2754\n",
      "Epoch 163/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 525.4134 - val_loss: 233.8486\n",
      "Epoch 164/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 514.5851 - val_loss: 226.5725\n",
      "Epoch 165/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 503.9230 - val_loss: 219.4453\n",
      "Epoch 166/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 493.4256 - val_loss: 212.4655\n",
      "Epoch 167/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 483.0911 - val_loss: 205.6315\n",
      "Epoch 168/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 472.9185 - val_loss: 198.9416\n",
      "Epoch 169/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 462.9060 - val_loss: 192.3952\n",
      "Epoch 170/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 453.0526 - val_loss: 185.9895\n",
      "Epoch 171/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 443.3563 - val_loss: 179.7243\n",
      "Epoch 172/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 433.8163 - val_loss: 173.5972\n",
      "Epoch 173/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 424.4309 - val_loss: 167.6071\n",
      "Epoch 174/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 415.1984 - val_loss: 161.7523\n",
      "Epoch 175/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 406.1178 - val_loss: 156.0314\n",
      "Epoch 176/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 397.1871 - val_loss: 150.4424\n",
      "Epoch 177/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 388.4054 - val_loss: 144.9849\n",
      "Epoch 178/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 379.7712 - val_loss: 139.6566\n",
      "Epoch 179/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 371.2830 - val_loss: 134.4563\n",
      "Epoch 180/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 362.9390 - val_loss: 129.3821\n",
      "Epoch 181/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 354.7384 - val_loss: 124.4333\n",
      "Epoch 182/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 346.6797 - val_loss: 119.6078\n",
      "Epoch 183/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 338.7616 - val_loss: 114.9045\n",
      "Epoch 184/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 330.9823 - val_loss: 110.3214\n",
      "Epoch 185/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 323.3402 - val_loss: 105.8573\n",
      "Epoch 186/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 315.8343 - val_loss: 101.5107\n",
      "Epoch 187/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 308.4632 - val_loss: 97.2802\n",
      "Epoch 188/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 301.2253 - val_loss: 93.1641\n",
      "Epoch 189/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 294.1192 - val_loss: 89.1611\n",
      "Epoch 190/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 287.1437 - val_loss: 85.2698\n",
      "Epoch 191/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 280.2971 - val_loss: 81.4881\n",
      "Epoch 192/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 273.5779 - val_loss: 77.8155\n",
      "Epoch 193/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 266.9854 - val_loss: 74.2499\n",
      "Epoch 194/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 260.5175 - val_loss: 70.7898\n",
      "Epoch 195/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 254.1731 - val_loss: 67.4341\n",
      "Epoch 196/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 247.9507 - val_loss: 64.1808\n",
      "Epoch 197/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 241.8489 - val_loss: 61.0288\n",
      "Epoch 198/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 235.8663 - val_loss: 57.9765\n",
      "Epoch 199/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 230.0014 - val_loss: 55.0224\n",
      "Epoch 200/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 224.2527 - val_loss: 52.1648\n",
      "Epoch 201/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 218.6191 - val_loss: 49.4029\n",
      "Epoch 202/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 213.0994 - val_loss: 46.7348\n",
      "Epoch 203/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 207.6918 - val_loss: 44.1590\n",
      "Epoch 204/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 202.3950 - val_loss: 41.6742\n",
      "Epoch 205/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 197.2077 - val_loss: 39.2787\n",
      "Epoch 206/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 192.1284 - val_loss: 36.9711\n",
      "Epoch 207/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 187.1557 - val_loss: 34.7502\n",
      "Epoch 208/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 182.2885 - val_loss: 32.6144\n",
      "Epoch 209/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 177.5252 - val_loss: 30.5622\n",
      "Epoch 210/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 172.8644 - val_loss: 28.5922\n",
      "Epoch 211/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 168.3047 - val_loss: 26.7028\n",
      "Epoch 212/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 163.8447 - val_loss: 24.8929\n",
      "Epoch 213/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 159.4834 - val_loss: 23.1609\n",
      "Epoch 214/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 155.2192 - val_loss: 21.5052\n",
      "Epoch 215/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 151.0506 - val_loss: 19.9246\n",
      "Epoch 216/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 146.9765 - val_loss: 18.4177\n",
      "Epoch 217/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 142.9954 - val_loss: 16.9829\n",
      "Epoch 218/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 139.1059 - val_loss: 15.6188\n",
      "Epoch 219/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 135.3066 - val_loss: 14.3240\n",
      "Epoch 220/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 131.5964 - val_loss: 13.0973\n",
      "Epoch 221/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 127.9738 - val_loss: 11.9371\n",
      "Epoch 222/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 124.4375 - val_loss: 10.8420\n",
      "Epoch 223/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 120.9863 - val_loss: 9.8109\n",
      "Epoch 224/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 117.6186 - val_loss: 8.8420\n",
      "Epoch 225/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 114.3333 - val_loss: 7.9342\n",
      "Epoch 226/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 111.1291 - val_loss: 7.0861\n",
      "Epoch 227/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 108.0046 - val_loss: 6.2962\n",
      "Epoch 228/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 104.9586 - val_loss: 5.5633\n",
      "Epoch 229/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 101.9898 - val_loss: 4.8860\n",
      "Epoch 230/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.0966 - val_loss: 4.2630\n",
      "Epoch 231/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 96.2781 - val_loss: 3.6929\n",
      "Epoch 232/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 93.5327 - val_loss: 3.1743\n",
      "Epoch 233/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 90.8593 - val_loss: 2.7061\n",
      "Epoch 234/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 88.2567 - val_loss: 2.2869\n",
      "Epoch 235/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 85.7235 - val_loss: 1.9154\n",
      "Epoch 236/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 83.2588 - val_loss: 1.5902\n",
      "Epoch 237/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 80.8608 - val_loss: 1.3102\n",
      "Epoch 238/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 78.5286 - val_loss: 1.0740\n",
      "Epoch 239/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 76.2610 - val_loss: 0.8805\n",
      "Epoch 240/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 74.0567 - val_loss: 0.7283\n",
      "Epoch 241/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 71.9145 - val_loss: 0.6162\n",
      "Epoch 242/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 69.8332 - val_loss: 0.5429\n",
      "Epoch 243/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 67.8116 - val_loss: 0.5074\n",
      "Epoch 244/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 65.8484 - val_loss: 0.5083\n",
      "Epoch 245/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 63.9426 - val_loss: 0.5444\n",
      "Epoch 246/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 62.0928 - val_loss: 0.6147\n",
      "Epoch 247/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 60.2981 - val_loss: 0.7178\n",
      "Epoch 248/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 58.5572 - val_loss: 0.8527\n",
      "Epoch 249/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 56.8692 - val_loss: 1.0183\n",
      "Epoch 250/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 55.2325 - val_loss: 1.2133\n",
      "Epoch 251/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 53.6463 - val_loss: 1.4366\n",
      "Epoch 252/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 52.1096 - val_loss: 1.6873\n",
      "Epoch 253/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 50.6210 - val_loss: 1.9640\n",
      "Epoch 254/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 49.1797 - val_loss: 2.2659\n",
      "Epoch 255/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 47.7845 - val_loss: 2.5918\n",
      "Epoch 256/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 46.4342 - val_loss: 2.9406\n",
      "Epoch 257/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 45.1281 - val_loss: 3.3114\n",
      "Epoch 258/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 43.8648 - val_loss: 3.7031\n",
      "Epoch 259/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 42.6434 - val_loss: 4.1146\n",
      "Epoch 260/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 41.4630 - val_loss: 4.5452\n",
      "Epoch 261/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 40.3223 - val_loss: 4.9936\n",
      "Epoch 262/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 39.2206 - val_loss: 5.4591\n",
      "Epoch 263/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 38.1567 - val_loss: 5.9407\n",
      "Epoch 264/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 37.1297 - val_loss: 6.4373\n",
      "Epoch 265/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 36.1387 - val_loss: 6.9482\n",
      "Epoch 266/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 35.1827 - val_loss: 7.4724\n",
      "Epoch 267/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 34.2608 - val_loss: 8.0091\n",
      "Epoch 268/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 33.3721 - val_loss: 8.5574\n",
      "Epoch 269/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 32.5156 - val_loss: 9.1166\n",
      "Epoch 270/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 31.6907 - val_loss: 9.6855\n",
      "Epoch 271/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 30.8962 - val_loss: 10.2635\n",
      "Epoch 272/500\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 30.1314 - val_loss: 10.8499\n",
      "Epoch 273/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 29.3954 - val_loss: 11.4440\n",
      "Epoch 274/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 28.6873 - val_loss: 12.0449\n",
      "Epoch 275/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 28.0064 - val_loss: 12.6519\n",
      "Epoch 276/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 27.3518 - val_loss: 13.2641\n",
      "Epoch 277/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 26.7228 - val_loss: 13.8810\n",
      "Epoch 278/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 26.1186 - val_loss: 14.5020\n",
      "Epoch 279/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 25.5385 - val_loss: 15.1265\n",
      "Epoch 280/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.9814 - val_loss: 15.7535\n",
      "Epoch 281/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.4470 - val_loss: 16.3824\n",
      "Epoch 282/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 23.9345 - val_loss: 17.0129\n",
      "Epoch 283/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 23.4430 - val_loss: 17.6444\n",
      "Epoch 284/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 22.9719 - val_loss: 18.2762\n",
      "Epoch 285/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.5206 - val_loss: 18.9077\n",
      "Epoch 286/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 22.0883 - val_loss: 19.5384\n",
      "Epoch 287/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 21.6745 - val_loss: 20.1679\n",
      "Epoch 288/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 21.2786 - val_loss: 20.7954\n",
      "Epoch 289/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 20.8997 - val_loss: 21.4208\n",
      "Epoch 290/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 20.5375 - val_loss: 22.0435\n",
      "Epoch 291/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 20.1912 - val_loss: 22.6629\n",
      "Epoch 292/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.8604 - val_loss: 23.2788\n",
      "Epoch 293/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.5444 - val_loss: 23.8907\n",
      "Epoch 294/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 19.2428 - val_loss: 24.4983\n",
      "Epoch 295/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 18.9548 - val_loss: 25.1011\n",
      "Epoch 296/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 18.6802 - val_loss: 25.6985\n",
      "Epoch 297/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 18.4183 - val_loss: 26.2908\n",
      "Epoch 298/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 18.1687 - val_loss: 26.8773\n",
      "Epoch 299/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 17.9308 - val_loss: 27.4577\n",
      "Epoch 300/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 17.7043 - val_loss: 28.0318\n",
      "Epoch 301/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 17.4886 - val_loss: 28.5993\n",
      "Epoch 302/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 17.2833 - val_loss: 29.1599\n",
      "Epoch 303/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 17.0881 - val_loss: 29.7132\n",
      "Epoch 304/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 16.9024 - val_loss: 30.2595\n",
      "Epoch 305/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.7260 - val_loss: 30.7982\n",
      "Epoch 306/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 16.5583 - val_loss: 31.3291\n",
      "Epoch 307/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 16.3990 - val_loss: 31.8519\n",
      "Epoch 308/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.2479 - val_loss: 32.3671\n",
      "Epoch 309/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.1043 - val_loss: 32.8738\n",
      "Epoch 310/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 15.9683 - val_loss: 33.3722\n",
      "Epoch 311/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 15.8392 - val_loss: 33.8622\n",
      "Epoch 312/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 15.7169 - val_loss: 34.3435\n",
      "Epoch 313/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 15.6010 - val_loss: 34.8162\n",
      "Epoch 314/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 15.4912 - val_loss: 35.2802\n",
      "Epoch 315/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 15.3873 - val_loss: 35.7353\n",
      "Epoch 316/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 15.2889 - val_loss: 36.1818\n",
      "Epoch 317/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 15.1958 - val_loss: 36.6192\n",
      "Epoch 318/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 15.1079 - val_loss: 37.0477\n",
      "Epoch 319/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 15.0247 - val_loss: 37.4672\n",
      "Epoch 320/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 14.9461 - val_loss: 37.8779\n",
      "Epoch 321/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.8719 - val_loss: 38.2794\n",
      "Epoch 322/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.8017 - val_loss: 38.6719\n",
      "Epoch 323/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.7356 - val_loss: 39.0556\n",
      "Epoch 324/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 14.6731 - val_loss: 39.4301\n",
      "Epoch 325/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 14.6142 - val_loss: 39.7959\n",
      "Epoch 326/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.5587 - val_loss: 40.1528\n",
      "Epoch 327/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.5064 - val_loss: 40.5009\n",
      "Epoch 328/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 14.4570 - val_loss: 40.8403\n",
      "Epoch 329/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.4106 - val_loss: 41.1710\n",
      "Epoch 330/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.3668 - val_loss: 41.4929\n",
      "Epoch 331/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.3257 - val_loss: 41.8063\n",
      "Epoch 332/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.2869 - val_loss: 42.1114\n",
      "Epoch 333/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 14.2505 - val_loss: 42.4084\n",
      "Epoch 334/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.2161 - val_loss: 42.6966\n",
      "Epoch 335/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.1839 - val_loss: 42.9770\n",
      "Epoch 336/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 14.1536 - val_loss: 43.2493\n",
      "Epoch 337/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.1251 - val_loss: 43.5139\n",
      "Epoch 338/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.0983 - val_loss: 43.7701\n",
      "Epoch 339/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 14.0732 - val_loss: 44.0189\n",
      "Epoch 340/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 14.0496 - val_loss: 44.2603\n",
      "Epoch 341/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.0274 - val_loss: 44.4943\n",
      "Epoch 342/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.0066 - val_loss: 44.7206\n",
      "Epoch 343/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.9871 - val_loss: 44.9398\n",
      "Epoch 344/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.9688 - val_loss: 45.1519\n",
      "Epoch 345/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.9516 - val_loss: 45.3570\n",
      "Epoch 346/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.9354 - val_loss: 45.5555\n",
      "Epoch 347/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.9203 - val_loss: 45.7473\n",
      "Epoch 348/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.9061 - val_loss: 45.9324\n",
      "Epoch 349/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.8929 - val_loss: 46.1113\n",
      "Epoch 350/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.8804 - val_loss: 46.2838\n",
      "Epoch 351/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.8688 - val_loss: 46.4506\n",
      "Epoch 352/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 13.8578 - val_loss: 46.6112\n",
      "Epoch 353/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.8476 - val_loss: 46.7659\n",
      "Epoch 354/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.8380 - val_loss: 46.9150\n",
      "Epoch 355/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.8290 - val_loss: 47.0587\n",
      "Epoch 356/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.8206 - val_loss: 47.1973\n",
      "Epoch 357/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.8127 - val_loss: 47.3302\n",
      "Epoch 358/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.8053 - val_loss: 47.4583\n",
      "Epoch 359/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7984 - val_loss: 47.5813\n",
      "Epoch 360/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7920 - val_loss: 47.6994\n",
      "Epoch 361/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 13.7859 - val_loss: 47.8129\n",
      "Epoch 362/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7802 - val_loss: 47.9219\n",
      "Epoch 363/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7749 - val_loss: 48.0265\n",
      "Epoch 364/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7700 - val_loss: 48.1267\n",
      "Epoch 365/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7654 - val_loss: 48.2228\n",
      "Epoch 366/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7610 - val_loss: 48.3149\n",
      "Epoch 367/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7570 - val_loss: 48.4033\n",
      "Epoch 368/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7532 - val_loss: 48.4877\n",
      "Epoch 369/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7496 - val_loss: 48.5683\n",
      "Epoch 370/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7463 - val_loss: 48.6457\n",
      "Epoch 371/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7432 - val_loss: 48.7194\n",
      "Epoch 372/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7404 - val_loss: 48.7901\n",
      "Epoch 373/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7377 - val_loss: 48.8578\n",
      "Epoch 374/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7352 - val_loss: 48.9220\n",
      "Epoch 375/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7328 - val_loss: 48.9836\n",
      "Epoch 376/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7306 - val_loss: 49.0421\n",
      "Epoch 377/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7286 - val_loss: 49.0982\n",
      "Epoch 378/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7267 - val_loss: 49.1513\n",
      "Epoch 379/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7249 - val_loss: 49.2020\n",
      "Epoch 380/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7233 - val_loss: 49.2505\n",
      "Epoch 381/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7218 - val_loss: 49.2962\n",
      "Epoch 382/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7203 - val_loss: 49.3399\n",
      "Epoch 383/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 13.7190 - val_loss: 49.3814\n",
      "Epoch 384/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7178 - val_loss: 49.4209\n",
      "Epoch 385/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7167 - val_loss: 49.4583\n",
      "Epoch 386/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7157 - val_loss: 49.4940\n",
      "Epoch 387/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7147 - val_loss: 49.5278\n",
      "Epoch 388/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7138 - val_loss: 49.5600\n",
      "Epoch 389/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7130 - val_loss: 49.5903\n",
      "Epoch 390/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7122 - val_loss: 49.6192\n",
      "Epoch 391/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7115 - val_loss: 49.6464\n",
      "Epoch 392/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7109 - val_loss: 49.6720\n",
      "Epoch 393/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7104 - val_loss: 49.6964\n",
      "Epoch 394/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7098 - val_loss: 49.7193\n",
      "Epoch 395/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7094 - val_loss: 49.7413\n",
      "Epoch 396/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7089 - val_loss: 49.7618\n",
      "Epoch 397/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7085 - val_loss: 49.7811\n",
      "Epoch 398/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7082 - val_loss: 49.7995\n",
      "Epoch 399/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 13.7079 - val_loss: 49.8166\n",
      "Epoch 400/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7077 - val_loss: 49.8333\n",
      "Epoch 401/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7074 - val_loss: 49.8486\n",
      "Epoch 402/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7072 - val_loss: 49.8631\n",
      "Epoch 403/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7070 - val_loss: 49.8763\n",
      "Epoch 404/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7069 - val_loss: 49.8892\n",
      "Epoch 405/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7068 - val_loss: 49.9012\n",
      "Epoch 406/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7067 - val_loss: 49.9124\n",
      "Epoch 407/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7067 - val_loss: 49.9228\n",
      "Epoch 408/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7067 - val_loss: 49.9328\n",
      "Epoch 409/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7067 - val_loss: 49.9421\n",
      "Epoch 410/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7067 - val_loss: 49.9507\n",
      "Epoch 411/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7067 - val_loss: 49.9588\n",
      "Epoch 412/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7068 - val_loss: 49.9665\n",
      "Epoch 413/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7068 - val_loss: 49.9736\n",
      "Epoch 414/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7069 - val_loss: 49.9802\n",
      "Epoch 415/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7069 - val_loss: 49.9861\n",
      "Epoch 416/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 13.7070 - val_loss: 49.9917\n",
      "Epoch 417/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7072 - val_loss: 49.9974\n",
      "Epoch 418/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7073 - val_loss: 50.0021\n",
      "Epoch 419/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 13.7075 - val_loss: 50.0071\n",
      "Epoch 420/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7076 - val_loss: 50.0110\n",
      "Epoch 421/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 13.7078 - val_loss: 50.0149\n",
      "Epoch 422/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 13.7079 - val_loss: 50.0187\n",
      "Epoch 423/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7081 - val_loss: 50.0220\n",
      "Epoch 424/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7083 - val_loss: 50.0251\n",
      "Epoch 425/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7085 - val_loss: 50.0279\n",
      "Epoch 426/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7087 - val_loss: 50.0307\n",
      "Epoch 427/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7089 - val_loss: 50.0329\n",
      "Epoch 428/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7092 - val_loss: 50.0353\n",
      "Epoch 429/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7094 - val_loss: 50.0373\n",
      "Epoch 430/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7096 - val_loss: 50.0394\n",
      "Epoch 431/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7098 - val_loss: 50.0413\n",
      "Epoch 432/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7100 - val_loss: 50.0428\n",
      "Epoch 433/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7103 - val_loss: 50.0443\n",
      "Epoch 434/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7105 - val_loss: 50.0455\n",
      "Epoch 435/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7108 - val_loss: 50.0467\n",
      "Epoch 436/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7110 - val_loss: 50.0478\n",
      "Epoch 437/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7113 - val_loss: 50.0485\n",
      "Epoch 438/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 13.7115 - val_loss: 50.0493\n",
      "Epoch 439/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 13.7118 - val_loss: 50.0502\n",
      "Epoch 440/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7120 - val_loss: 50.0507\n",
      "Epoch 441/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7123 - val_loss: 50.0513\n",
      "Epoch 442/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 13.7125 - val_loss: 50.0516\n",
      "Epoch 443/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7128 - val_loss: 50.0518\n",
      "Epoch 444/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 13.7130 - val_loss: 50.0521\n",
      "Epoch 445/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7134 - val_loss: 50.0525\n",
      "Epoch 446/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 13.7136 - val_loss: 50.0527\n",
      "Epoch 447/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7138 - val_loss: 50.0527\n",
      "Epoch 448/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7141 - val_loss: 50.0528\n",
      "Epoch 449/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 13.7144 - val_loss: 50.0528\n",
      "Epoch 450/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7146 - val_loss: 50.0529\n",
      "Epoch 451/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7149 - val_loss: 50.0532\n",
      "Epoch 452/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 13.7151 - val_loss: 50.0528\n",
      "Epoch 453/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7154 - val_loss: 50.0528\n",
      "Epoch 454/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7157 - val_loss: 50.0527\n",
      "Epoch 455/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7160 - val_loss: 50.0525\n",
      "Epoch 456/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7162 - val_loss: 50.0522\n",
      "Epoch 457/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7165 - val_loss: 50.0519\n",
      "Epoch 458/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7167 - val_loss: 50.0518\n",
      "Epoch 459/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7170 - val_loss: 50.0516\n",
      "Epoch 460/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 13.7173 - val_loss: 50.0514\n",
      "Epoch 461/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7175 - val_loss: 50.0512\n",
      "Epoch 462/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7178 - val_loss: 50.0510\n",
      "Epoch 463/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7180 - val_loss: 50.0508\n",
      "Epoch 464/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7182 - val_loss: 50.0504\n",
      "Epoch 465/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7185 - val_loss: 50.0499\n",
      "Epoch 466/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 13.7188 - val_loss: 50.0494\n",
      "Epoch 467/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7190 - val_loss: 50.0492\n",
      "Epoch 468/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 13.7193 - val_loss: 50.0488\n",
      "Epoch 469/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7195 - val_loss: 50.0485\n",
      "Epoch 470/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7198 - val_loss: 50.0483\n",
      "Epoch 471/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7200 - val_loss: 50.0480\n",
      "Epoch 472/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7202 - val_loss: 50.0476\n",
      "Epoch 473/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7204 - val_loss: 50.0471\n",
      "Epoch 474/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7207 - val_loss: 50.0466\n",
      "Epoch 475/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7209 - val_loss: 50.0464\n",
      "Epoch 476/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 13.7211 - val_loss: 50.0459\n",
      "Epoch 477/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7214 - val_loss: 50.0454\n",
      "Epoch 478/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7216 - val_loss: 50.0452\n",
      "Epoch 479/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7218 - val_loss: 50.0449\n",
      "Epoch 480/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 13.7220 - val_loss: 50.0444\n",
      "Epoch 481/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7223 - val_loss: 50.0442\n",
      "Epoch 482/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 13.7225 - val_loss: 50.0437\n",
      "Epoch 483/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7227 - val_loss: 50.0434\n",
      "Epoch 484/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7229 - val_loss: 50.0430\n",
      "Epoch 485/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7232 - val_loss: 50.0427\n",
      "Epoch 486/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7234 - val_loss: 50.0423\n",
      "Epoch 487/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 13.7236 - val_loss: 50.0421\n",
      "Epoch 488/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7238 - val_loss: 50.0417\n",
      "Epoch 489/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7239 - val_loss: 50.0413\n",
      "Epoch 490/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7242 - val_loss: 50.0411\n",
      "Epoch 491/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7244 - val_loss: 50.0407\n",
      "Epoch 492/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 13.7246 - val_loss: 50.0402\n",
      "Epoch 493/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 13.7248 - val_loss: 50.0398\n",
      "Epoch 494/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 13.7250 - val_loss: 50.0396\n",
      "Epoch 495/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7252 - val_loss: 50.0392\n",
      "Epoch 496/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7254 - val_loss: 50.0390\n",
      "Epoch 497/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 13.7255 - val_loss: 50.0388\n",
      "Epoch 498/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7257 - val_loss: 50.0384\n",
      "Epoch 499/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7259 - val_loss: 50.0380\n",
      "Epoch 500/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7261 - val_loss: 50.0372\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(89.3180, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0010, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(-0.0204, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.2194, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 379ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.05388889e+01, 7.04996732e+01, 7.04604575e+01, 7.04212418e+01,\n",
       "        7.03820261e+01, 7.03428105e+01, 7.03035948e+01, 7.02643791e+01,\n",
       "        7.02251634e+01, 7.01859477e+01, 7.01467320e+01, 7.01075163e+01,\n",
       "        7.00683006e+01, 7.00290850e+01, 6.99898693e+01, 6.99506536e+01,\n",
       "        6.99114379e+01, 6.98722222e+01, 6.98330065e+01, 6.97937908e+01,\n",
       "        6.97545752e+01, 6.97153595e+01, 6.96761438e+01, 6.96369281e+01,\n",
       "        6.95977124e+01, 6.95584967e+01, 6.95192811e+01, 6.94900327e+01,\n",
       "        6.94704248e+01, 6.94508170e+01, 6.94312092e+01, 6.94116013e+01,\n",
       "        6.93919935e+01, 6.93723856e+01, 6.93527778e+01, 6.93331699e+01,\n",
       "        6.93135621e+01, 6.92939542e+01, 6.92743464e+01, 6.92547386e+01,\n",
       "        6.92351307e+01, 6.92155229e+01, 6.91959150e+01, 6.91763072e+01,\n",
       "        6.91566993e+01, 6.91370915e+01, 6.91174837e+01, 6.90978758e+01,\n",
       "        6.90782680e+01, 6.90586601e+01, 6.90390523e+01, 6.90194444e+01,\n",
       "        6.89998366e+01, 6.89802288e+01, 6.89606209e+01, 6.89410131e+01,\n",
       "        6.89214052e+01, 6.89017974e+01, 6.88821895e+01, 6.88625817e+01,\n",
       "        6.88429739e+01, 6.88233660e+01, 6.88037582e+01, 6.87977358e+01,\n",
       "        6.87949346e+01, 6.87921335e+01, 6.87893324e+01, 6.87865313e+01,\n",
       "        6.87837302e+01, 6.87809290e+01, 6.87781279e+01, 6.87753268e+01,\n",
       "        6.87725257e+01, 6.87697246e+01, 6.87669234e+01, 6.87641223e+01,\n",
       "        6.87613212e+01, 6.87585201e+01, 6.87557190e+01, 6.87529178e+01,\n",
       "        7.66012421e+01, 2.61637956e-01, 3.22384357e-01, 5.02604306e-01,\n",
       "        1.06601441e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.86938378e-02, 0.00000000e+00, 5.99627718e-02, 0.00000000e+00,\n",
       "        2.92042941e-01, 6.04974806e-01, 3.08255672e-01, 5.32064199e-01,\n",
       "        4.62526143e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([68.07170868, 68.0605042 , 68.04929972, 68.03809524, 68.02689076,\n",
       "       68.01568627, 68.00448179, 67.99327731, 67.98207283, 67.97086835,\n",
       "       67.95966387, 67.94845938, 67.9372549 , 67.92605042, 67.91484594,\n",
       "       67.90364146, 67.89243697, 67.88123249, 67.87002801, 67.85882353,\n",
       "       67.84761905, 67.83641457, 67.82521008, 67.8140056 , 67.80280112,\n",
       "       67.79159664, 67.78039216, 67.76918768, 67.75798319, 67.74677871,\n",
       "       67.73557423, 67.72436975, 67.71316527, 67.70196078, 67.6907563 ,\n",
       "       67.67955182, 67.66834734, 67.65714286, 67.64593838, 67.63473389,\n",
       "       67.62352941, 67.61232493, 67.60112045, 67.58991597, 67.57871148,\n",
       "       67.567507  , 67.55630252, 67.54509804, 67.53389356, 67.52268908,\n",
       "       67.51148459, 67.50028011, 67.48907563, 67.47787115, 67.46666667,\n",
       "       67.45546218, 67.4442577 , 67.43305322, 67.42184874, 67.41064426,\n",
       "       67.39943978, 67.38823529, 67.37703081, 67.36582633, 67.35462185,\n",
       "       67.34341737, 67.33221289, 67.3210084 , 67.30980392, 67.29877451,\n",
       "       67.28897059, 67.27916667, 67.26936275, 67.25955882, 67.2497549 ,\n",
       "       67.23995098, 67.23014706, 67.22034314, 67.21053922, 67.20073529,\n",
       "       67.19093137, 67.18112745, 67.17132353, 67.16151961, 67.15171569,\n",
       "       67.14191176, 67.13210784, 67.12230392, 67.1125    , 67.10269608,\n",
       "       67.09289216, 67.08308824, 67.07328431, 67.06348039, 67.05367647,\n",
       "       67.04387255, 67.03406863, 67.02426471, 67.01446078, 67.00465686])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.200962238645232\n",
      "14.18568696455946\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
