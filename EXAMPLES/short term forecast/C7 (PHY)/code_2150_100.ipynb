{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       92.600000\n",
       "1       92.387115\n",
       "2       92.174230\n",
       "3       91.961345\n",
       "4       91.748459\n",
       "          ...    \n",
       "2245    64.343568\n",
       "2246    64.335916\n",
       "2247    64.328264\n",
       "2248    64.320612\n",
       "2249    64.312960\n",
       "Name: C7, Length: 2250, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c7_interpolated_2150_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       92.600000\n",
       "1       92.387115\n",
       "2       92.174230\n",
       "3       91.961345\n",
       "4       91.748459\n",
       "          ...    \n",
       "2145     0.000000\n",
       "2146     0.000000\n",
       "2147     0.229470\n",
       "2148     0.220610\n",
       "2149     0.000000\n",
       "Name: C7, Length: 2150, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2150)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92.387115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92.174230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.961345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91.748459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     92.600000  0.000298\n",
       "1     92.387115  0.000298\n",
       "2     92.174230  0.000297\n",
       "3     91.961345  0.000297\n",
       "4     91.748459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAAsTAAALEwEAmpwYAAAfs0lEQVR4nO3de3Sc9X3n8fd3ZnSx7pJ1tS1ZxleMDcGIS0KAUNPEQFtoNockXRJONlm6u7l2u6eht3OyPW1P0rRN0pNsGtKQ0JQTypJ0ocslgCELCcQgA8aWje93S7Zs2brZus5v/5hHsixL1syj0czzSJ/XOZyZeWZ+Mz89jD76+ff8LuacQ0REwieS7QqIiIg/CnARkZBSgIuIhJQCXEQkpBTgIiIhFcvkh1VWVrrGxsZMfqSISOht3rz5pHOuavzxjAZ4Y2Mjzc3NmfxIEZHQM7ODEx1XF4qISEgpwEVEQkoBLiISUgpwEZGQUoCLiISUAlxEJKQU4CIiIRWKAP/3Lcf4l19POAxSRGTOCkWAP7utjW++sIvhuNYuFxEZEYoAv31tLSd7BnjjQEe2qyIiEhihCPBbV1aTF4vwzNbWbFdFRCQwQhHghXkxPrCyime2tRFXN4qICBCSAAe4Y20dJ7r7eW778WxXRUQkEEIV4Ktqi/mLf2+ht38o29UREcm60AR4TjTCX/3uGo519vGN53dluzoiIlkXmgAHuGZxBR+/roEfvnqAlmOd2a6OiEhWhSrAAR7YsIryghzue+h1Hnx5r7pTRGTOCl2Alxbk8KNPXceq2hL++ul3ef/XXuTbL+6mq28w21UTEckocy5zw/KamppcOrdU23zwNN95aQ8vvnuC4vwYH7+ugfctnc/VDeWUzstJ2+eIiGSTmW12zjVddDzMAT5i29FOvv3iHp7b3kbcgRmsqC7mmsZybl5eyQdX1xKJWNo/V0QkE2Z1gI/o7R/i7cNn2HzwNM0HT/PWwdN09w+xqraYP9qwkltXVmOmIBeRcJkTAT7ecNzx1NZW/u65nRw8dZZrG8v58oZVNDVWZKwOIiLTNScDfMTgcJx/feMw39q4m/bufm5aXsmtK6u5/rIKLq8tUfeKiATaZAEey0ZlMi0nGuHeGxbz4XUL+eGvDvCT1w/xyu7tAJTkx7huSQXXLang+iXzuWJBCbFo6AbniMgcNCda4BM5euYcr+8/xaZ9Hby+v4N9J3sBKMqLcc3icq5bUsENl1WwdmEZuTEFuohkz5zuQknGia4+Nu3vYNP+U7y+v4Ndx3sAyM+JJAK9cT7XX1bBe+rLyM+JZrm2IjKXKMBTdKqnnzcOdCRCfV8HO9q6cA5yosaahaU0LS6nqbGCpsXlzC/Ky3Z1RWQWU4BPU+fZQZoPdtB88DTNBzrYcriTgeE4AJdVFXLt4gqaGhOh3ji/QMMVRSRtFOBp1j80zLajnbxxIBHozQdPc+ZsYjp/ZVEuTYsTfegfWlNLXem8LNdWRMJMAT7D4nHH3vaeCwL9UMdZANY1lHHH2jruWFvHgjKFuYikRgGeBfvae3hmWxtPvdPK9tYuAN5TX8ada+u4fW0ti8oLslxDEQkDBXiW7T/Zy9NbW3l6aystxxJhflV9GXeureX2NXXUVyjMRWRi0wpwM/sD4DOAA7YCnwLqgEeB+cBm4BPOuYFLvc9cDvCxDp7q5emtbTy9tZWtRxMbU1y5qJQ71tZx51qFuYhcyHeAm9lC4JfAaufcOTN7DHgauAP4mXPuUTP7R2CLc+67l3ovBfjFDp06yzPbEi3zLUcSYb52YSnvXTqfhooCGioKWDy/gAVl88jRDFGROWm6U+ljwDwzGwQKgFbgN4Df855/GPgKcMkAl4s1zC/g929Zyu/fspTDHSNh3saPXj3AwFB89HURgwVl81g8PxHq9RUFLK4oTIT8/AKtfy4yB00Z4M65o2b2t8Ah4BzwHIkukzPOuZH9zI4ACycqb2b3A/cDNDQ0pKPOs1Z9RQH337yU+29eSjzuONHdz8FTvRzqOHvBf8+1HOdU74W9VaXzckbDvKGigAWl+ZQX5lJRkJu4LcylrCCHvJhmkYrMFlMGuJmVA3cBS4AzwP8GNiT7Ac65B4EHIdGF4quWc1AkYtSW5lNbms/1l82/6Pme/iEOnUoE+uGOsxzs6OVQxzm2H+viuZY2BocnPtWFuVHKC3MpHwn2ghzKChIBX16Yy/zCXFbUFLGksoioVmkUCbRkulBuA/Y759oBzOxnwI1AmZnFvFb4IuDozFVTxivKi7F6QQmrF5Rc9Nxw3HGqp5/TZwfp6B3gzNkBOs4OcLp3gNNnBznd6z0+O8iBk72c7h2ge9zm0Pk5EVbVlnDFghKuWFDKFQtKWFlbrHVgRAIkmQA/BNxgZgUkulDWA83AS8BHSIxEuQ94YqYqKamJRozqknyqS/KTLjMwFOfMuQFOdPWzs62blmNdtBzr5Mktx3hk06HR911aVTga6KvrEuFeWqD+d5FsSHYY4f8EPgoMAW+RGFK4kER4V3jH7nXO9V/qfTQKJXyccxzuOEfLsU62t3aNBvvxrvP/qxeWzUsE+oISllYVJbp+SvKpLslTn7uk1f6Tvbx16DQfXrco21W5pNf2nsI5x/uWVabl/TSRR9LqZE8/Lce62O4F+vZjXew/1cv4r1NFYS41JfnUluRRW5rv3c+npjSfOi/oS+flaPEvScqqP3+GvsE4B756Z7arckmNDzwFkLZ6zukdeST9KovyuGVFFbesqBo91ts/xOHTZ2nr7ON4Vx9tnf20dY3c7+OdI50XjZ4ByItFRsN9aVUhK2uKWVlbwqraYsoLczP5Y0nA9Q3Gp37RBIaG42w92snVDeUpl918sIOr68sDufWiAlzSpjAvxqraElbVXnxhdUT/0DAnuvoToe4F+4nufto6+2jtPMcz29r4yeuHR19fXZzHytpiVtWeD/Vl1UW6mCop+fpzO/ne/9vHU194P1csKE263Mu72vnkQ6/zZ3dezmduumwGa+iPAlwyKi8Wpd6biDQR5xLj399t62ZnW5d3283Drx0cndgUMWisLEyEek1idMy6hrKULtrK3NJyNLH+UMcE/wK8lMOnEyuK7m3vTXud0kEBLoFiZtSUJLpTxnbPDA3HOXDqLDvHBHvLsS6e2dY22u9+5aJS1q+q4bbV1ayuK1G/uoyKe1+SSIrfibj33Qpg7wmgAJeQiEUjLKsuYll1EXdeWTd6/OzAEDvbunlt3yle2H6cb27cxTde2MWC0nzWX17D+suree/S+RoNM8eNBHiqf9Odz+DPFAW4hFpBboyrG8q5uqGc//aBZZzs6efFd0+wccdxHt98hB//+iCFuVFuWl7FbatruHVllfYwnYNGWtJGii3w+EiAp7tG6aEAl1mlsiiPe5rquaepnr7B4dGW+cYdJ3i2pQ0zWNdQzm2X13Db5dUsqy5SV8sccL4lnVq50eAP6HdEAS6zVn5OlFtXVnPrymr+8m5Hy7EuXthxnBd2HOdrz77L1559l8XzC1i/qoamxnKuXFTKwrJ5gf1lFf9GgjjV9X389p1nigJc5gQzY83CUtYsLOVLt62gtfMcG3ckulr+ZdNBHvrVfgDmF+aydlEpVy4q48qFpVxZX0p1sUa3hN35PvDUgtjpIqZI8NSVzuPeGxZz7w2L6R8a5t3Wbt452sk7h8+w9WgnL+/aPdpqqy3J58pFpd5/ZaxdWKoJRiHjdzTJaAs8oAmuAJc5Ly8W5ar6Mq6qL4MbFgOJ0S3bj3Wx5UgnW4+c4Z0jnTy3/fhomYaKAtYuKmV1XQml83IomZdDSX6M4vwcSufFKMnPoTg/h/yciLpkptDdN0j/UJzKGby47Hy2wM/3gU/9/v1D8YxPMFOAi0ygIDdGU2MFTY0Vo8c6zw3ScrQzEepHz/D2oTM89U7rJd8nJ2qU5CcCvjg/5t2PUZyXuC3Jz6GsIIdqb42Y2tJ8Kovy5tRa7Pf+4HW2HD5DTUne6AqXNy6r5NrGcmJp2kYw7vsiZnJ94M9ua+O/PvIm9zQt4iu/c4WvOvqhABdJUum8HN63rPKCFebODQzT1TdI17lBuvqGRu93j94forvPe+7cIN19g7R19Y2+5tzg8EWfE40Y1cV5owt/jawTUzeyGJi3CNi83Nkxtv1kdz+raotZXVdCy7EuXt59km+/tIeKwlxuu7yaDWtquXFZ5bTG8se9JVRSvRiZ7OiVI6fPAfBY85GMztpUgItMw7zcKPNyo9T4nMY/sg77cW/hr8T6MOdo60ysF7OnvYdf7jlJz7gNNwCqikdarOc33WioKAhsf+1kBofjvKe+jK/+hyuBxKJoL+9q59mWNp7Z2sZjzUcoyY/xhfXLLyh3uOMsJ7r7uGZxxURve4GxE3k6egdoOdbJjUsrpzxX5/vOL/26Qe8vxDc+ehVffnzrlPVJFwW4SBblxiJUF+dTXZzPWiZfZKmnf2h0lcdW73Zvew/bj3Xxyz0nGfaSpigvxuV1xVyxoDSxY1NdCStqismNpacrYiYMxR2x6PmALMyLcfvaOm5fW8fAUJxX957kh786wF8+teOCcl995l2e2trKLSuq+LM7L2d5TfGkn+HGBPHDrx7gWxt3s3ZhKX/+W6u5bsnkfwCSHb0y5G1h+NtXLuDcQJw/+bfMhLgCXCQEivJio0sJjNc3OMzu4z0XbLrxWPNhzg4kumdyosby6mJWLzjfWl9RUxSYddgHh+LEIhP/gcmNRfjAympuWVHFCztO8J//+fx+Av1DwxTnx3jz0Gk2fOsVfu+6Br502/IJZ9qO7cvuH4pjBqd6+rnne69x+5paHrh9FYvnF05QDq/cFD/DcOI9oxHjo9fWK8BFJDn5OVHWLipl7aLzLfjhuOPgqV5vB6Uutrd28YudJ3h885HR1+RGI1QW5VJZnEdVUR6VRXlUFedRWZRLVXG+d5tHZXEexXmxGQv7wXh8yn8hmBm/ubqGz966lO+8tHf0eENFAT/+9PV884VdPLLpED95/RDrFpdzy4oqblpeyZoFpUQidtFFzJxohI1/+AG+/8o+vvuLvTzb0saaBaXctLySm1dUsa6hnNxYJOm1UAaHHTmRxIijqMH7l1VOeH0j3RTgIrNQNGJcVlXEZVVF/PZVC4DzS/VuP9bF3vYeTvYMcLKnn/buflo7+9h6NLHhxkh3zFh5schowM8vzCUvJ0I0EiEnYsSiRiwaIRYxYpEIOVEjGkkcy4mMeS56/lg0YuREI8SixuCwI5Zkv33E7KIhfRWFufzFXWv45HsX89M3j/Lyrna+/vOdfP3nO6kozGVdQ9nohcWxf4Tm5Ub5wvrlfPTaeh574zAv727ney/v43/9Yi+FuVGuaaxg84EO73Ph2Jlz/O3Pd4JBQW6UeTlR5uXGKMiNsuXwmQu6gUbO90xTgIvMEWOX6r11VfWEr4nHHafPDtDe08/J7gHae/q820TQn+zp51hnH4PDcYbjjsHhOEPDjqG4YyieuD/y3NAEfwgmU5HkxKixETk+H5dVF/PlDav48oZVtHf386s9J3l5Vzstx7rGnANwXFiwpiSfz69fzufXL6e7b5BX957ild3tvHnwDL1eN1RNST6v7T3Fz946Sk1JHoPDjnMDwxe0spdWFV7wOZmgABeRUZGIMb8oL9GPXDu993IuEeKTBf1Q3DE0HCfumLBvfzqqivO4++qF3H31QgCeePsoX3z07YtCf7zi/Bw+dEUtH7oi8cNvP9bFHf/wCsX5MXr7E2H9+H953+iGJPG4o29omLMDwxTlZT5OFeAiMiPMjJyokRNlxmYoJtvSHd9/n2wDear3j0SMgtwYBbnZidLgji0SEZFLUoCLSKiMdIP4vkTos2CqxWb+EqYCXETCJADj1oNEAS4ioZXqFmkj7eLU98ZM8WMyRAEuIrOe33Z70Bv8CnARCSW/E2X8NqZT/bxMtNoV4CISGkFoECfTKs/UGjMKcBGZM/y2igPaBa4AF5HwSn4iz7jHSbblx74uiCGuABcRCSkFuIiEjnPjl6RKrWwmaCKPiMgYQRjWl8wFykxVUwEuIqHlbxqPz4k8KTaptxw+w1eebEmtUIqSCnAzKzOzx83sXTPbYWbvNbMKM3vezHZ7t+UzWlMREZ9Sn7HplZtmU/pHrx6Y3htMIdkW+LeAZ51zq4CrgB3AA8BG59xyYKP3WERkxjk3jSGBQRxO4tOUAW5mpcDNwA8AnHMDzrkzwF3Aw97LHgbunpkqiogk+G1Jp7cOSbwmQ9VMpgW+BGgHfmhmb5nZP5lZIVDjnGv1XtMG1ExU2MzuN7NmM2tub29PT61FRCDlpBxpfafed+4u2ootCJIJ8BiwDviuc+5qoJdx3SXOTd7F75x70DnX5Jxrqqqqmm59RURS5rdFnP32/qUlE+BHgCPOuU3e48dJBPpxM6sD8G5PzEwVRUQu5nsceFprkV1TBrhzrg04bGYrvUPrge3Ak8B93rH7gCdmpIYiIuNkM4SDMBZ9RLI7cX4eeMTMcoF9wKdIhP9jZvZp4CBwz8xUUUQk4eI1Tfy+T+olUxm9kqmMTyrAnXNvA00TPLU+rbUREZlBfi9EBnXooWZiikgopbKmydgWcUot6QB1l0xEAS4ikoIgjEUfoQAXkdAZaX1ncnhgKr0o2pFHRGSc6caiduQREQmZsQ3i1C5kBqe7ZCIKcBGRFATpwqYCXERCZ3Rdb79v4KNgEIcSKsBFJDSC1Pq9FO3IIyJyCX5axH7XEc/UPpqpUoCLyByQvh15gvSPAAW4iISW3/HW/saBB68VrgAXkdAJaI9GxinARSQ0pjvDMVOt6CBtqSYiEjiphHFap9wHqBNcAS4ikoQgdtsowEUkdEZa35nc0CGIFOAiImmn1QhFRCblfyJP6gXHFtF64CIiGZSOrpYAdoErwEUkvHyPLglOI3paFOAiEjpBHBGSDQpwEQmNCzZmyODuOmPHnCfTetdEHhGRNEnLmikBbPYrwEUktPyOCJklXeAKcBGRsFKAi0go+V2Yaro9Icm03rUjj4jIONOdROP7wqfTOHARkfRKMs/HvyzZi5pBHy+uABcRCSkFuIiEjt/NiWH6mzoEaSVDBbiIhMZ0s3M6Fz5T+YOhiTwiIlNINifHB2rS5QI+YlwBLiISUkkHuJlFzewtM/u/3uMlZrbJzPaY2b+aWe7MVVNEJBiC1CZPpQX+RWDHmMdfA77hnFsGnAY+nc6KiYhMxk3jUqT/seApbKIcpB15zGwRcCfwT95jA34DeNx7ycPA3TNQPxGRUReP506t/EgGp1wutZdnTLIt8G8CfwTEvcfzgTPOuSHv8RFgYXqrJiKSHrN144cpA9zMfgs44Zzb7OcDzOx+M2s2s+b29nY/byEiEhhBCvVkWuA3Ar9jZgeAR0l0nXwLKDOzmPeaRcDRiQo75x50zjU555qqqqrSUGURmeucw3e/hu++c5/lZtKUAe6c+2Pn3CLnXCPwMeBF59x/BF4CPuK97D7giRmrpYgIE43nTq05PGZfndTKpXjlMwwTeb4M/Hcz20OiT/wH6amSiEh6BX1Cjl+xqV9ynnPuF8AvvPv7gOvSXyURkeAK0h8DzcQUkVDK9IYOAdwSUwEuIuETwCzNCgW4iITG+O6L1CfyOH/lUnt5KC5iioiEQzon8gSnC1wBLiLh5L9P2mffud+Pm0EKcBGRkFKAi0joTLcvO+VekBSb34FajVBEJAh8L0rl+/MuLhm2tVBERAInyOuBZ4oCXEQkpBTgIhI65/uy/fVnBKkbZDoU4CIyZ/juPkl1HUNN5BERmVxKe1SOaXKnEuJBb6grwEVEQkoBLiKh5X9YYdDb1slRgItI6GR8Gn3KE3kyQwEuIqEx0cSapMr5/rzz90dC3G8dZoICXERCyf/mxMGbkOOXAlxE5pwANaKnRQEuIuEzexrR06IAF5HQmG7D2f9EnvNdL8nUIVP95ApwEQmllCbkTHAxMqlyAR9uqAAXkdDK9KiUoFGAi4iElAJcREInG0MBz48Dn/q1msgjIjLOBX3ZPso7v+UCOupFAS4is95sXTdcAS4ioZXOvS7DSAEuIpKE6e4CNBMU4CISOs7hq2PaOX/92aleNM1UA18BLiKhkY5VBTPxeZmiABeR0JolXdm+KcBFRJKQyjjwTFGAi0jo+B3PnSgb0EHdPkwZ4GZWb2Yvmdl2M2sxsy96xyvM7Hkz2+3dls98dUVkLpvu8L9UdrK/sFxqrw/STMwh4A+dc6uBG4DPmtlq4AFgo3NuObDReywikjHJBuX41yX9dyBA3SUTmTLAnXOtzrk3vfvdwA5gIXAX8LD3soeBu2eojiIiWRfErpeU+sDNrBG4GtgE1DjnWr2n2oCaScrcb2bNZtbc3t4+nbqKiMgYSQe4mRUBPwW+5JzrGvucS3QsTfjnyTn3oHOuyTnXVFVVNa3KiohAoi/b34QcfF39TLVIoHbkMbMcEuH9iHPuZ97h42ZW5z1fB5yYmSqKiCSMz8Wkg/KicskWC3YneDKjUAz4AbDDOff3Y556ErjPu38f8ET6qyciEgxBHAceS+I1NwKfALaa2dvesT8Bvgo8ZmafBg4C98xIDUVEZEJTBrhz7pdMPphmfXqrIyIytbG7xPspO1toJqaIhMZF47lTLD/aDZJqyRSvmAZpIo+ISKilc0eeIF3YVICLiISUAlxEQsnvRsN+10MJIgW4iISO3yF9Ixc+Uy+XIu3IIyIyjs9B2OnYkWek5R6kceAKcBGRkFKAi0go+e4DT281skoBLiIh5rNLJc21yBYFuIiEju+1uX2PXBk7CWhqmRorrgAXkdDwG4u+ywXpiuUEFOAiEkq+NzWeRZ3gCnARkZBSgItI+PieyDNSLrWCzjnfZWeSAlxEQsP3hBy/E4D8fVzGJvsowEVEQkoBLiKh5HdRqll0DVMBLiLhlclhhamMA88UBbiIhE6mhxCmWkw78oiIjJPOnXVmslymKMBFZE7Rhg4iIgHgu4Xso5zfzSBmkgJcRELHf1+2z5ErAW20K8BFJDTSsbNOauUy2+eeKgW4iISSNnRQgItIiPluIfsoc34j5eB0givARURCSgEuIqGT6YuRqU/k0Y48IiIXGBuLqYS4XVjQ1wcGse9cAS4ioZXp5WWDRgEuIhJSCnARmTP8L4IVxA4UBbiIhJBzqV6QPN9l4r/v3Ge5GaQAF5HQGB+MqQZl3AvhVPPV+dyDc6ZNK8DNbIOZ7TSzPWb2QLoqJSJyKcNxfwMJP/XDN+gfjKdc7q+e3sG5wWEfnzizfAe4mUWB7wC3A6uBj5vZ6nRVTERkvHMDiRC96W9eYs+JHp7e2pZkyfNxv/HdE+w+0ZPyZ3//lf1J96JsOdI5er/xgad4be+plD8vGdNpgV8H7HHO7XPODQCPAnelp1oiIhcbivu7mDi/MM9XubyYv4h8T33ZBY8//v1f09E74Ou9LmU6Ab4QODzm8RHv2AXM7H4zazaz5vb29ml8nIjMdR+9tp6lVYWjj//x3nVJlWusLOTBT1zDdY0VXLGghK9+eG1S5fJiUf7PZ2/kpuWVrKgp4ovrlydV7g9+czn1FfMuONbTN5RU2VSY3+ExZvYRYINz7jPe408A1zvnPjdZmaamJtfc3Ozr80RE5ioz2+ycaxp/fDot8KNA/ZjHi7xjIiKSAdMJ8DeA5Wa2xMxygY8BT6anWiIiMpWY34LOuSEz+xzwcyAKPOSca0lbzURE5JJ8BziAc+5p4Ok01UVERFKgmZgiIiGlABcRCSkFuIhISCnARURCyvdEHl8fZtYOHPRZvBI4mcbqzCY6N5PTuZmYzsvkgnhuFjvnqsYfzGiAT4eZNU80E0l0bi5F52ZiOi+TC9O5UReKiEhIKcBFREIqTAH+YLYrEGA6N5PTuZmYzsvkQnNuQtMHLiIiFwpTC1xERMZQgIuIhFQoAnyub55sZgfMbKuZvW1mzd6xCjN73sx2e7fl3nEzs3/wztU7ZpbcliUhYWYPmdkJM9s25ljK58LM7vNev9vM7svGz5Juk5ybr5jZUe+787aZ3THmuT/2zs1OM/vQmOOz6vfNzOrN7CUz225mLWb2Re94+L83zrlA/0diqdq9wGVALrAFWJ3temX4HBwAKscd+xvgAe/+A8DXvPt3AM8ABtwAbMp2/dN8Lm4G1gHb/J4LoALY592We/fLs/2zzdC5+QrwPyZ47WrvdykPWOL9jkVn4+8bUAes8+4XA7u8nz/035swtMC1efLE7gIe9u4/DNw95vg/u4RfA2VmVpeF+s0I59zLQMe4w6meiw8BzzvnOpxzp4HngQ0zXvkZNsm5mcxdwKPOuX7n3H5gD4nftVn3++aca3XOvend7wZ2kNi/N/TfmzAEeFKbJ89yDnjOzDab2f3esRrnXKt3vw2o8e7PxfOV6rmYa+foc15XwEMj3QTM0XNjZo3A1cAmZsH3JgwBLvB+59w64Hbgs2Z289gnXeLfdxoPis7FBL4LLAXeA7QCf5fV2mSRmRUBPwW+5JzrGvtcWL83YQjwOb95snPuqHd7Avg3Ev/MPT7SNeLdnvBePhfPV6rnYs6cI+fccefcsHMuDnyfxHcH5ti5MbMcEuH9iHPuZ97h0H9vwhDgc3rzZDMrNLPikfvAB4FtJM7ByFXw+4AnvPtPAp/0rqTfAHSO+WfibJXqufg58EEzK/e6FD7oHZt1xl3/+F0S3x1InJuPmVmemS0BlgOvMwt/38zMgB8AO5xzfz/mqfB/b7J9hTjJq8h3kLhyvBf402zXJ8M/+2UkRgJsAVpGfn5gPrAR2A28AFR4xw34jneutgJN2f4Z0nw+fkKiK2CQRB/kp/2cC+A/kbhwtwf4VLZ/rhk8Nz/2fvZ3SART3ZjX/6l3bnYCt485Pqt+34D3k+geeQd42/vvjtnwvdFUehGRkApDF4qIiExAAS4iElIKcBGRkFKAi4iElAJcRCSkFOAiIiGlABcRCan/DwsV9F2R8ojhAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 89.3180  # Value for C0\n",
    "K0 = -0.0010  # Value for K0\n",
    "K1 = 0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = -0.0204    # Value for b\n",
    "c = 2.2194    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    92.600000    92.387115    92.174230    91.961345    91.748459   \n",
      "351    92.387115    92.174230    91.961345    91.748459    91.535574   \n",
      "352    92.174230    91.961345    91.748459    91.535574    91.322689   \n",
      "353    91.961345    91.748459    91.535574    91.322689    91.109804   \n",
      "354    91.748459    91.535574    91.322689    91.109804    90.896919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    91.535574    91.322689    91.109804    90.896919    90.691597  ...   \n",
      "351    91.322689    91.109804    90.896919    90.691597    90.579552  ...   \n",
      "352    91.109804    90.896919    90.691597    90.579552    90.467507  ...   \n",
      "353    90.896919    90.691597    90.579552    90.467507    90.355462  ...   \n",
      "354    90.691597    90.579552    90.467507    90.355462    90.243417  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   80.261671    0.000263   80.252334    0.000263   80.242997    0.000263   \n",
      "351   80.252334    0.000263   80.242997    0.000263   80.233660    0.000262   \n",
      "352   80.242997    0.000263   80.233660    0.000262   80.224323    0.000262   \n",
      "353   80.233660    0.000262   80.224323    0.000262   80.214986    0.000262   \n",
      "354   80.224323    0.000262   80.214986    0.000262   80.205649    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   80.233660    0.000262   80.224323    0.000262  \n",
      "351   80.224323    0.000262   80.214986    0.000262  \n",
      "352   80.214986    0.000262   80.205649    0.000262  \n",
      "353   80.205649    0.000262   80.196312    0.000262  \n",
      "354   80.196312    0.000262   80.186975    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1700, 1, 251) (1700, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 2s 22ms/step - loss: 5636.5356 - val_loss: 4198.3765\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5559.0405 - val_loss: 4152.3096\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5500.7612 - val_loss: 4106.4570\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 5442.5156 - val_loss: 4056.5801\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5378.1777 - val_loss: 4007.9500\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5317.0205 - val_loss: 3960.1787\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5257.0254 - val_loss: 3913.3188\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 5197.9907 - val_loss: 3867.1670\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5139.7251 - val_loss: 3821.6135\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5082.1226 - val_loss: 3776.5974\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5025.1240 - val_loss: 3732.0823\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4968.6943 - val_loss: 3688.0427\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4912.8037 - val_loss: 3644.4614\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4857.4355 - val_loss: 3601.3240\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4802.5728 - val_loss: 3558.6199\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4748.2070 - val_loss: 3516.3411\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4694.3267 - val_loss: 3474.4795\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4640.9243 - val_loss: 3433.0286\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4587.9912 - val_loss: 3391.9832\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4535.5220 - val_loss: 3351.3374\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4483.5112 - val_loss: 3311.0867\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4431.9531 - val_loss: 3271.2280\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4380.8433 - val_loss: 3231.7551\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4330.1768 - val_loss: 3192.6658\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4279.9487 - val_loss: 3153.9563\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4230.1572 - val_loss: 3115.6223\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4180.7949 - val_loss: 3077.6616\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4131.8618 - val_loss: 3040.0701\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4083.3518 - val_loss: 3002.8455\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 4035.2617 - val_loss: 2965.9839\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3987.5889 - val_loss: 2929.4834\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3940.3298 - val_loss: 2893.3408\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3893.4824 - val_loss: 2857.5532\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3847.0413 - val_loss: 2822.1187\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3801.0063 - val_loss: 2787.0334\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3755.3713 - val_loss: 2752.2954\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3710.1365 - val_loss: 2717.9021\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3665.2971 - val_loss: 2683.8506\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3620.8501 - val_loss: 2650.1394\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3576.7937 - val_loss: 2616.7654\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3533.1255 - val_loss: 2583.7263\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3489.8416 - val_loss: 2551.0198\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3446.9404 - val_loss: 2518.6428\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3404.4177 - val_loss: 2486.5952\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3362.2732 - val_loss: 2454.8721\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3320.5034 - val_loss: 2423.4734\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3279.1060 - val_loss: 2392.3953\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3238.0771 - val_loss: 2361.6367\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3197.4172 - val_loss: 2331.1953\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3157.1208 - val_loss: 2301.0688\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3117.1870 - val_loss: 2271.2546\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3077.6143 - val_loss: 2241.7512\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3038.3989 - val_loss: 2212.5569\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2999.5388 - val_loss: 2183.6689\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2961.0320 - val_loss: 2155.0857\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2922.8770 - val_loss: 2126.8042\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2885.0698 - val_loss: 2098.8245\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2847.6099 - val_loss: 2071.1426\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2810.4944 - val_loss: 2043.7573\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2773.7207 - val_loss: 2016.6667\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2737.2869 - val_loss: 1989.8695\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2701.1919 - val_loss: 1963.3633\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2665.4326 - val_loss: 1937.1456\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2630.0063 - val_loss: 1911.2148\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2594.9124 - val_loss: 1885.5698\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2560.1477 - val_loss: 1860.2083\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2525.7117 - val_loss: 1835.1282\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2491.6001 - val_loss: 1810.3281\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2457.8130 - val_loss: 1785.8063\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2424.3472 - val_loss: 1761.5604\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2391.2009 - val_loss: 1737.5894\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2358.3728 - val_loss: 1713.8909\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2325.8604 - val_loss: 1690.4640\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2293.6621 - val_loss: 1667.3057\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2261.7754 - val_loss: 1644.4147\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2228.5071 - val_loss: 1617.3674\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2189.0894 - val_loss: 1589.0927\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2150.8784 - val_loss: 1562.5054\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2114.5803 - val_loss: 1537.0725\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2079.5837 - val_loss: 1512.4766\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2045.5570 - val_loss: 1488.5471\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2012.3180 - val_loss: 1465.1858\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1979.7557 - val_loss: 1442.3278\n",
      "Epoch 84/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1947.7947 - val_loss: 1419.9276\n",
      "Epoch 85/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1916.3833 - val_loss: 1397.9529\n",
      "Epoch 86/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 1885.4816 - val_loss: 1376.3781\n",
      "Epoch 87/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1855.0596 - val_loss: 1355.1835\n",
      "Epoch 88/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1825.0930 - val_loss: 1334.3517\n",
      "Epoch 89/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1795.5610 - val_loss: 1313.8704\n",
      "Epoch 90/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1766.4478 - val_loss: 1293.7268\n",
      "Epoch 91/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1737.7388 - val_loss: 1273.9114\n",
      "Epoch 92/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1709.4216 - val_loss: 1254.4160\n",
      "Epoch 93/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1681.4862 - val_loss: 1235.2319\n",
      "Epoch 94/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1653.9233 - val_loss: 1216.3528\n",
      "Epoch 95/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1626.7234 - val_loss: 1197.7726\n",
      "Epoch 96/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1599.8801 - val_loss: 1179.4851\n",
      "Epoch 97/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1573.3853 - val_loss: 1161.4851\n",
      "Epoch 98/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1547.2334 - val_loss: 1143.7682\n",
      "Epoch 99/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1521.4188 - val_loss: 1126.3295\n",
      "Epoch 100/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1495.9357 - val_loss: 1109.1647\n",
      "Epoch 101/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1470.7784 - val_loss: 1092.2701\n",
      "Epoch 102/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1445.9432 - val_loss: 1075.6418\n",
      "Epoch 103/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1421.4252 - val_loss: 1059.2762\n",
      "Epoch 104/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1397.2205 - val_loss: 1043.1708\n",
      "Epoch 105/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1373.3246 - val_loss: 1027.3207\n",
      "Epoch 106/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1349.7341 - val_loss: 1011.7238\n",
      "Epoch 107/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1326.4449 - val_loss: 996.3770\n",
      "Epoch 108/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1303.4539 - val_loss: 981.2776\n",
      "Epoch 109/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1280.7579 - val_loss: 966.4225\n",
      "Epoch 110/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1258.3534 - val_loss: 951.8094\n",
      "Epoch 111/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1236.2375 - val_loss: 937.4355\n",
      "Epoch 112/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1214.4075 - val_loss: 923.2980\n",
      "Epoch 113/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1192.8596 - val_loss: 909.3944\n",
      "Epoch 114/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1171.5913 - val_loss: 895.7227\n",
      "Epoch 115/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1150.6007 - val_loss: 882.2805\n",
      "Epoch 116/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1129.8845 - val_loss: 869.0659\n",
      "Epoch 117/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1109.4402 - val_loss: 856.0757\n",
      "Epoch 118/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1089.2651 - val_loss: 843.3083\n",
      "Epoch 119/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1069.3567 - val_loss: 830.7612\n",
      "Epoch 120/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1049.7126 - val_loss: 818.4321\n",
      "Epoch 121/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1030.3306 - val_loss: 806.3195\n",
      "Epoch 122/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1011.2080 - val_loss: 794.4207\n",
      "Epoch 123/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 992.3432 - val_loss: 782.7349\n",
      "Epoch 124/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 973.7338 - val_loss: 771.2585\n",
      "Epoch 125/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 955.3772 - val_loss: 759.9907\n",
      "Epoch 126/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 937.2711 - val_loss: 748.9282\n",
      "Epoch 127/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 919.4134 - val_loss: 738.0700\n",
      "Epoch 128/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 901.8018 - val_loss: 727.4144\n",
      "Epoch 129/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 884.4351 - val_loss: 716.9587\n",
      "Epoch 130/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 867.3104 - val_loss: 706.7015\n",
      "Epoch 131/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 850.4262 - val_loss: 696.6414\n",
      "Epoch 132/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 833.7798 - val_loss: 686.7754\n",
      "Epoch 133/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 817.3699 - val_loss: 677.1025\n",
      "Epoch 134/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 801.1943 - val_loss: 667.6205\n",
      "Epoch 135/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 785.2509 - val_loss: 658.3278\n",
      "Epoch 136/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 769.5377 - val_loss: 649.2228\n",
      "Epoch 137/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 754.0533 - val_loss: 640.3029\n",
      "Epoch 138/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 738.7948 - val_loss: 631.5668\n",
      "Epoch 139/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 723.7607 - val_loss: 623.0126\n",
      "Epoch 140/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 708.9490 - val_loss: 614.6383\n",
      "Epoch 141/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 694.3583 - val_loss: 606.4430\n",
      "Epoch 142/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 679.9862 - val_loss: 598.4240\n",
      "Epoch 143/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 665.8312 - val_loss: 590.5799\n",
      "Epoch 144/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 651.8916 - val_loss: 582.9091\n",
      "Epoch 145/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 638.1648 - val_loss: 575.4095\n",
      "Epoch 146/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 624.6499 - val_loss: 568.0797\n",
      "Epoch 147/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 611.3445 - val_loss: 560.9180\n",
      "Epoch 148/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 598.2469 - val_loss: 553.9219\n",
      "Epoch 149/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 585.3551 - val_loss: 547.0907\n",
      "Epoch 150/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 572.6677 - val_loss: 540.4221\n",
      "Epoch 151/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 560.1827 - val_loss: 533.9146\n",
      "Epoch 152/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 547.8983 - val_loss: 527.5664\n",
      "Epoch 153/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 535.8129 - val_loss: 521.3755\n",
      "Epoch 154/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 523.9244 - val_loss: 515.3409\n",
      "Epoch 155/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 512.2312 - val_loss: 509.4601\n",
      "Epoch 156/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 500.7314 - val_loss: 503.7317\n",
      "Epoch 157/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 489.4233 - val_loss: 498.1541\n",
      "Epoch 158/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 478.3053 - val_loss: 492.7255\n",
      "Epoch 159/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 467.3758 - val_loss: 487.4442\n",
      "Epoch 160/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 456.6325 - val_loss: 482.3087\n",
      "Epoch 161/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 446.0742 - val_loss: 477.3168\n",
      "Epoch 162/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 435.6986 - val_loss: 472.4672\n",
      "Epoch 163/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 425.5045 - val_loss: 467.7581\n",
      "Epoch 164/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 415.4897 - val_loss: 463.1875\n",
      "Epoch 165/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 405.6525 - val_loss: 458.7540\n",
      "Epoch 166/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 395.9915 - val_loss: 454.4557\n",
      "Epoch 167/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 386.5045 - val_loss: 450.2912\n",
      "Epoch 168/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 377.1900 - val_loss: 446.2585\n",
      "Epoch 169/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 368.0466 - val_loss: 442.3561\n",
      "Epoch 170/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 359.0720 - val_loss: 438.5821\n",
      "Epoch 171/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 350.2648 - val_loss: 434.9348\n",
      "Epoch 172/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 341.6232 - val_loss: 431.4126\n",
      "Epoch 173/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 333.1452 - val_loss: 428.0135\n",
      "Epoch 174/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 324.8292 - val_loss: 424.7364\n",
      "Epoch 175/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 316.6740 - val_loss: 421.5788\n",
      "Epoch 176/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 308.6769 - val_loss: 418.5396\n",
      "Epoch 177/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 300.8369 - val_loss: 415.6167\n",
      "Epoch 178/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 293.1522 - val_loss: 412.8087\n",
      "Epoch 179/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 285.6208 - val_loss: 410.1135\n",
      "Epoch 180/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 278.2408 - val_loss: 407.5296\n",
      "Epoch 181/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 271.0110 - val_loss: 405.0555\n",
      "Epoch 182/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 263.9298 - val_loss: 402.6891\n",
      "Epoch 183/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 256.9948 - val_loss: 400.4287\n",
      "Epoch 184/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 250.2044 - val_loss: 398.2728\n",
      "Epoch 185/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 243.5571 - val_loss: 396.2195\n",
      "Epoch 186/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 237.0512 - val_loss: 394.2672\n",
      "Epoch 187/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 230.6848 - val_loss: 392.4140\n",
      "Epoch 188/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 224.4560 - val_loss: 390.6583\n",
      "Epoch 189/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 218.3636 - val_loss: 388.9984\n",
      "Epoch 190/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 212.4055 - val_loss: 387.4324\n",
      "Epoch 191/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 206.5802 - val_loss: 385.9589\n",
      "Epoch 192/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 200.8858 - val_loss: 384.5757\n",
      "Epoch 193/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 195.3202 - val_loss: 383.2813\n",
      "Epoch 194/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 189.8822 - val_loss: 382.0742\n",
      "Epoch 195/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 184.5701 - val_loss: 380.9523\n",
      "Epoch 196/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 179.3819 - val_loss: 379.9142\n",
      "Epoch 197/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 174.3159 - val_loss: 378.9579\n",
      "Epoch 198/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 169.3705 - val_loss: 378.0818\n",
      "Epoch 199/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 164.5439 - val_loss: 377.2842\n",
      "Epoch 200/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 159.8344 - val_loss: 376.5634\n",
      "Epoch 201/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 155.2403 - val_loss: 375.9176\n",
      "Epoch 202/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 150.7598 - val_loss: 375.3452\n",
      "Epoch 203/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 146.3913 - val_loss: 374.8443\n",
      "Epoch 204/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 142.1328 - val_loss: 374.4135\n",
      "Epoch 205/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 137.9830 - val_loss: 374.0507\n",
      "Epoch 206/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 133.9398 - val_loss: 373.7545\n",
      "Epoch 207/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 130.0017 - val_loss: 373.5232\n",
      "Epoch 208/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 126.1671 - val_loss: 373.3549\n",
      "Epoch 209/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 122.4342 - val_loss: 373.2482\n",
      "Epoch 210/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 118.8011 - val_loss: 373.2012\n",
      "Epoch 211/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 115.2665 - val_loss: 373.2123\n",
      "Epoch 212/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 111.8286 - val_loss: 373.2798\n",
      "Epoch 213/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 108.4857 - val_loss: 373.4022\n",
      "Epoch 214/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 105.2360 - val_loss: 373.5776\n",
      "Epoch 215/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 102.0782 - val_loss: 373.8045\n",
      "Epoch 216/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 99.0102 - val_loss: 374.0813\n",
      "Epoch 217/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 96.0302 - val_loss: 374.4063\n",
      "Epoch 218/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 93.1371 - val_loss: 374.7779\n",
      "Epoch 219/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 90.3289 - val_loss: 375.1944\n",
      "Epoch 220/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 87.6041 - val_loss: 375.6544\n",
      "Epoch 221/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 84.9610 - val_loss: 376.1562\n",
      "Epoch 222/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 82.3981 - val_loss: 376.6982\n",
      "Epoch 223/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 79.9135 - val_loss: 377.2789\n",
      "Epoch 224/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 77.5059 - val_loss: 377.8967\n",
      "Epoch 225/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 75.1736 - val_loss: 378.5500\n",
      "Epoch 226/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 72.9150 - val_loss: 379.2375\n",
      "Epoch 227/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 70.7286 - val_loss: 379.9574\n",
      "Epoch 228/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 68.6126 - val_loss: 380.7084\n",
      "Epoch 229/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 66.5657 - val_loss: 381.4890\n",
      "Epoch 230/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 64.5863 - val_loss: 382.2975\n",
      "Epoch 231/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 62.6730 - val_loss: 383.1328\n",
      "Epoch 232/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 60.8240 - val_loss: 383.9931\n",
      "Epoch 233/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 59.0380 - val_loss: 384.8773\n",
      "Epoch 234/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 57.3133 - val_loss: 385.7840\n",
      "Epoch 235/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 55.6487 - val_loss: 386.7116\n",
      "Epoch 236/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 54.0424 - val_loss: 387.6587\n",
      "Epoch 237/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 52.4933 - val_loss: 388.6243\n",
      "Epoch 238/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 50.9996 - val_loss: 389.6068\n",
      "Epoch 239/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 49.5600 - val_loss: 390.6049\n",
      "Epoch 240/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 48.1733 - val_loss: 391.6176\n",
      "Epoch 241/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 46.8379 - val_loss: 392.6432\n",
      "Epoch 242/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 45.5525 - val_loss: 393.6808\n",
      "Epoch 243/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 44.3158 - val_loss: 394.7289\n",
      "Epoch 244/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 43.1265 - val_loss: 395.7865\n",
      "Epoch 245/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 41.9830 - val_loss: 396.8525\n",
      "Epoch 246/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 40.8842 - val_loss: 397.9256\n",
      "Epoch 247/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 39.8288 - val_loss: 399.0048\n",
      "Epoch 248/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 38.8154 - val_loss: 400.0887\n",
      "Epoch 249/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 37.8430 - val_loss: 401.1766\n",
      "Epoch 250/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 36.9102 - val_loss: 402.2672\n",
      "Epoch 251/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 36.0156 - val_loss: 403.3597\n",
      "Epoch 252/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 35.1583 - val_loss: 404.4530\n",
      "Epoch 253/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 34.3371 - val_loss: 405.5461\n",
      "Epoch 254/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 33.5507 - val_loss: 406.6381\n",
      "Epoch 255/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.7981 - val_loss: 407.7279\n",
      "Epoch 256/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.0782 - val_loss: 408.8149\n",
      "Epoch 257/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 31.3897 - val_loss: 409.8983\n",
      "Epoch 258/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 30.7316 - val_loss: 410.9770\n",
      "Epoch 259/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 30.1030 - val_loss: 412.0503\n",
      "Epoch 260/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 29.5027 - val_loss: 413.1173\n",
      "Epoch 261/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 28.9299 - val_loss: 414.1775\n",
      "Epoch 262/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 28.3835 - val_loss: 415.2299\n",
      "Epoch 263/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 27.8625 - val_loss: 416.2743\n",
      "Epoch 264/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 27.3659 - val_loss: 417.3094\n",
      "Epoch 265/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 26.8930 - val_loss: 418.3350\n",
      "Epoch 266/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 26.4427 - val_loss: 419.3505\n",
      "Epoch 267/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 26.0141 - val_loss: 420.3551\n",
      "Epoch 268/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 25.6066 - val_loss: 421.3484\n",
      "Epoch 269/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 25.2192 - val_loss: 422.3299\n",
      "Epoch 270/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 24.8510 - val_loss: 423.2990\n",
      "Epoch 271/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.5014 - val_loss: 424.2554\n",
      "Epoch 272/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 24.1695 - val_loss: 425.1986\n",
      "Epoch 273/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 23.8546 - val_loss: 426.1282\n",
      "Epoch 274/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.5561 - val_loss: 427.0439\n",
      "Epoch 275/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 23.2730 - val_loss: 427.9451\n",
      "Epoch 276/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.0049 - val_loss: 428.8318\n",
      "Epoch 277/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.7510 - val_loss: 429.7035\n",
      "Epoch 278/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.5107 - val_loss: 430.5602\n",
      "Epoch 279/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.2834 - val_loss: 431.4013\n",
      "Epoch 280/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.0685 - val_loss: 432.2267\n",
      "Epoch 281/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.8654 - val_loss: 433.0363\n",
      "Epoch 282/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.6736 - val_loss: 433.8300\n",
      "Epoch 283/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.4925 - val_loss: 434.6074\n",
      "Epoch 284/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.3216 - val_loss: 435.3687\n",
      "Epoch 285/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.1604 - val_loss: 436.1137\n",
      "Epoch 286/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.0085 - val_loss: 436.8424\n",
      "Epoch 287/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 20.8653 - val_loss: 437.5543\n",
      "Epoch 288/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 20.7304 - val_loss: 438.2500\n",
      "Epoch 289/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 20.6034 - val_loss: 438.9290\n",
      "Epoch 290/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 20.4839 - val_loss: 439.5919\n",
      "Epoch 291/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 20.3715 - val_loss: 440.2381\n",
      "Epoch 292/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 20.2658 - val_loss: 440.8680\n",
      "Epoch 293/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 20.1665 - val_loss: 441.4814\n",
      "Epoch 294/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 20.0732 - val_loss: 442.0787\n",
      "Epoch 295/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 19.9856 - val_loss: 442.6597\n",
      "Epoch 296/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.9033 - val_loss: 443.2248\n",
      "Epoch 297/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.8262 - val_loss: 443.7741\n",
      "Epoch 298/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.7538 - val_loss: 444.3077\n",
      "Epoch 299/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.6860 - val_loss: 444.8256\n",
      "Epoch 300/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.6224 - val_loss: 445.3281\n",
      "Epoch 301/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 19.5629 - val_loss: 445.8157\n",
      "Epoch 302/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.5070 - val_loss: 446.2880\n",
      "Epoch 303/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.4548 - val_loss: 446.7456\n",
      "Epoch 304/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 19.4059 - val_loss: 447.1886\n",
      "Epoch 305/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 19.3602 - val_loss: 447.6173\n",
      "Epoch 306/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 19.3174 - val_loss: 448.0319\n",
      "Epoch 307/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 19.2773 - val_loss: 448.4324\n",
      "Epoch 308/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 19.2400 - val_loss: 448.8196\n",
      "Epoch 309/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 19.2050 - val_loss: 449.1933\n",
      "Epoch 310/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 19.1723 - val_loss: 449.5541\n",
      "Epoch 311/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.1418 - val_loss: 449.9018\n",
      "Epoch 312/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.1133 - val_loss: 450.2370\n",
      "Epoch 313/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.0867 - val_loss: 450.5599\n",
      "Epoch 314/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.0619 - val_loss: 450.8710\n",
      "Epoch 315/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.0387 - val_loss: 451.1702\n",
      "Epoch 316/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 19.0171 - val_loss: 451.4580\n",
      "Epoch 317/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.9969 - val_loss: 451.7347\n",
      "Epoch 318/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.9780 - val_loss: 452.0005\n",
      "Epoch 319/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.9605 - val_loss: 452.2557\n",
      "Epoch 320/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.9441 - val_loss: 452.5005\n",
      "Epoch 321/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.9288 - val_loss: 452.7355\n",
      "Epoch 322/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.9146 - val_loss: 452.9607\n",
      "Epoch 323/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 18.9013 - val_loss: 453.1766\n",
      "Epoch 324/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.8890 - val_loss: 453.3832\n",
      "Epoch 325/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.8774 - val_loss: 453.5810\n",
      "Epoch 326/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.8667 - val_loss: 453.7700\n",
      "Epoch 327/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.8567 - val_loss: 453.9508\n",
      "Epoch 328/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.8474 - val_loss: 454.1236\n",
      "Epoch 329/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.8387 - val_loss: 454.2885\n",
      "Epoch 330/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.8307 - val_loss: 454.4461\n",
      "Epoch 331/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.8232 - val_loss: 454.5964\n",
      "Epoch 332/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.8162 - val_loss: 454.7397\n",
      "Epoch 333/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.8096 - val_loss: 454.8761\n",
      "Epoch 334/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.8036 - val_loss: 455.0059\n",
      "Epoch 335/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7980 - val_loss: 455.1300\n",
      "Epoch 336/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7928 - val_loss: 455.2476\n",
      "Epoch 337/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7879 - val_loss: 455.3596\n",
      "Epoch 338/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7834 - val_loss: 455.4656\n",
      "Epoch 339/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7792 - val_loss: 455.5667\n",
      "Epoch 340/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7753 - val_loss: 455.6627\n",
      "Epoch 341/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 18.7717 - val_loss: 455.7536\n",
      "Epoch 342/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7684 - val_loss: 455.8400\n",
      "Epoch 343/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7653 - val_loss: 455.9216\n",
      "Epoch 344/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7624 - val_loss: 455.9990\n",
      "Epoch 345/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7598 - val_loss: 456.0723\n",
      "Epoch 346/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7574 - val_loss: 456.1417\n",
      "Epoch 347/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7551 - val_loss: 456.2070\n",
      "Epoch 348/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7531 - val_loss: 456.2691\n",
      "Epoch 349/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7512 - val_loss: 456.3276\n",
      "Epoch 350/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7494 - val_loss: 456.3829\n",
      "Epoch 351/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7478 - val_loss: 456.4349\n",
      "Epoch 352/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7463 - val_loss: 456.4840\n",
      "Epoch 353/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7450 - val_loss: 456.5302\n",
      "Epoch 354/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7438 - val_loss: 456.5739\n",
      "Epoch 355/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7427 - val_loss: 456.6148\n",
      "Epoch 356/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7417 - val_loss: 456.6534\n",
      "Epoch 357/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7408 - val_loss: 456.6897\n",
      "Epoch 358/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7399 - val_loss: 456.7238\n",
      "Epoch 359/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7392 - val_loss: 456.7557\n",
      "Epoch 360/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7386 - val_loss: 456.7857\n",
      "Epoch 361/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7380 - val_loss: 456.8136\n",
      "Epoch 362/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 18.7375 - val_loss: 456.8401\n",
      "Epoch 363/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7370 - val_loss: 456.8645\n",
      "Epoch 364/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7366 - val_loss: 456.8877\n",
      "Epoch 365/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7363 - val_loss: 456.9094\n",
      "Epoch 366/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7361 - val_loss: 456.9292\n",
      "Epoch 367/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7359 - val_loss: 456.9482\n",
      "Epoch 368/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7357 - val_loss: 456.9658\n",
      "Epoch 369/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7356 - val_loss: 456.9824\n",
      "Epoch 370/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7355 - val_loss: 456.9977\n",
      "Epoch 371/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7354 - val_loss: 457.0118\n",
      "Epoch 372/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7354 - val_loss: 457.0249\n",
      "Epoch 373/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7354 - val_loss: 457.0370\n",
      "Epoch 374/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7355 - val_loss: 457.0486\n",
      "Epoch 375/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7355 - val_loss: 457.0591\n",
      "Epoch 376/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7356 - val_loss: 457.0690\n",
      "Epoch 377/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7358 - val_loss: 457.0778\n",
      "Epoch 378/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7359 - val_loss: 457.0862\n",
      "Epoch 379/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7361 - val_loss: 457.0938\n",
      "Epoch 380/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7363 - val_loss: 457.1011\n",
      "Epoch 381/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 18.7366 - val_loss: 457.1079\n",
      "Epoch 382/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7367 - val_loss: 457.1138\n",
      "Epoch 383/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7370 - val_loss: 457.1197\n",
      "Epoch 384/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7372 - val_loss: 457.1245\n",
      "Epoch 385/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7375 - val_loss: 457.1292\n",
      "Epoch 386/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7378 - val_loss: 457.1335\n",
      "Epoch 387/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7381 - val_loss: 457.1375\n",
      "Epoch 388/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7383 - val_loss: 457.1411\n",
      "Epoch 389/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7387 - val_loss: 457.1444\n",
      "Epoch 390/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7389 - val_loss: 457.1472\n",
      "Epoch 391/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7393 - val_loss: 457.1499\n",
      "Epoch 392/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7396 - val_loss: 457.1521\n",
      "Epoch 393/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7399 - val_loss: 457.1544\n",
      "Epoch 394/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7403 - val_loss: 457.1565\n",
      "Epoch 395/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7406 - val_loss: 457.1584\n",
      "Epoch 396/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7409 - val_loss: 457.1600\n",
      "Epoch 397/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7413 - val_loss: 457.1613\n",
      "Epoch 398/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 18.7416 - val_loss: 457.1625\n",
      "Epoch 399/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7420 - val_loss: 457.1636\n",
      "Epoch 400/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7423 - val_loss: 457.1647\n",
      "Epoch 401/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7426 - val_loss: 457.1653\n",
      "Epoch 402/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7430 - val_loss: 457.1658\n",
      "Epoch 403/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7433 - val_loss: 457.1668\n",
      "Epoch 404/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7437 - val_loss: 457.1671\n",
      "Epoch 405/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7440 - val_loss: 457.1674\n",
      "Epoch 406/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7444 - val_loss: 457.1676\n",
      "Epoch 407/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7447 - val_loss: 457.1679\n",
      "Epoch 408/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7450 - val_loss: 457.1680\n",
      "Epoch 409/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7454 - val_loss: 457.1681\n",
      "Epoch 410/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7457 - val_loss: 457.1681\n",
      "Epoch 411/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 18.7460 - val_loss: 457.1681\n",
      "Epoch 412/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7464 - val_loss: 457.1678\n",
      "Epoch 413/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7467 - val_loss: 457.1675\n",
      "Epoch 414/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7471 - val_loss: 457.1674\n",
      "Epoch 415/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7474 - val_loss: 457.1672\n",
      "Epoch 416/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7477 - val_loss: 457.1670\n",
      "Epoch 417/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7480 - val_loss: 457.1666\n",
      "Epoch 418/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7483 - val_loss: 457.1664\n",
      "Epoch 419/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7486 - val_loss: 457.1658\n",
      "Epoch 420/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7490 - val_loss: 457.1653\n",
      "Epoch 421/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7492 - val_loss: 457.1649\n",
      "Epoch 422/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7496 - val_loss: 457.1648\n",
      "Epoch 423/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7498 - val_loss: 457.1642\n",
      "Epoch 424/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7501 - val_loss: 457.1637\n",
      "Epoch 425/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7504 - val_loss: 457.1631\n",
      "Epoch 426/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7507 - val_loss: 457.1626\n",
      "Epoch 427/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7510 - val_loss: 457.1621\n",
      "Epoch 428/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 18.7513 - val_loss: 457.1616\n",
      "Epoch 429/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7516 - val_loss: 457.1614\n",
      "Epoch 430/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7518 - val_loss: 457.1609\n",
      "Epoch 431/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7521 - val_loss: 457.1602\n",
      "Epoch 432/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7524 - val_loss: 457.1599\n",
      "Epoch 433/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7526 - val_loss: 457.1595\n",
      "Epoch 434/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7529 - val_loss: 457.1589\n",
      "Epoch 435/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7531 - val_loss: 457.1584\n",
      "Epoch 436/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7534 - val_loss: 457.1578\n",
      "Epoch 437/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7537 - val_loss: 457.1573\n",
      "Epoch 438/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7539 - val_loss: 457.1569\n",
      "Epoch 439/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7541 - val_loss: 457.1563\n",
      "Epoch 440/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7543 - val_loss: 457.1559\n",
      "Epoch 441/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7546 - val_loss: 457.1554\n",
      "Epoch 442/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7548 - val_loss: 457.1551\n",
      "Epoch 443/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7550 - val_loss: 457.1547\n",
      "Epoch 444/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7552 - val_loss: 457.1543\n",
      "Epoch 445/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7554 - val_loss: 457.1538\n",
      "Epoch 446/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 18.7556 - val_loss: 457.1532\n",
      "Epoch 447/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7559 - val_loss: 457.1526\n",
      "Epoch 448/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7561 - val_loss: 457.1522\n",
      "Epoch 449/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7563 - val_loss: 457.1517\n",
      "Epoch 450/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7565 - val_loss: 457.1514\n",
      "Epoch 451/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7567 - val_loss: 457.1509\n",
      "Epoch 452/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7569 - val_loss: 457.1506\n",
      "Epoch 453/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7571 - val_loss: 457.1501\n",
      "Epoch 454/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7572 - val_loss: 457.1497\n",
      "Epoch 455/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7574 - val_loss: 457.1491\n",
      "Epoch 456/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7576 - val_loss: 457.1489\n",
      "Epoch 457/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7578 - val_loss: 457.1484\n",
      "Epoch 458/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7579 - val_loss: 457.1479\n",
      "Epoch 459/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7581 - val_loss: 457.1475\n",
      "Epoch 460/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7583 - val_loss: 457.1472\n",
      "Epoch 461/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7585 - val_loss: 457.1472\n",
      "Epoch 462/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7586 - val_loss: 457.1466\n",
      "Epoch 463/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7588 - val_loss: 457.1463\n",
      "Epoch 464/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7589 - val_loss: 457.1460\n",
      "Epoch 465/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7590 - val_loss: 457.1454\n",
      "Epoch 466/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 18.7592 - val_loss: 457.1449\n",
      "Epoch 467/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7594 - val_loss: 457.1445\n",
      "Epoch 468/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7596 - val_loss: 457.1443\n",
      "Epoch 469/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7597 - val_loss: 457.1438\n",
      "Epoch 470/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7598 - val_loss: 457.1436\n",
      "Epoch 471/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7599 - val_loss: 457.1434\n",
      "Epoch 472/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7601 - val_loss: 457.1432\n",
      "Epoch 473/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7602 - val_loss: 457.1428\n",
      "Epoch 474/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7603 - val_loss: 457.1426\n",
      "Epoch 475/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7604 - val_loss: 457.1423\n",
      "Epoch 476/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7606 - val_loss: 457.1418\n",
      "Epoch 477/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7607 - val_loss: 457.1414\n",
      "Epoch 478/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 18.7608 - val_loss: 457.1411\n",
      "Epoch 479/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7609 - val_loss: 457.1408\n",
      "Epoch 480/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7611 - val_loss: 457.1407\n",
      "Epoch 481/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7612 - val_loss: 457.1405\n",
      "Epoch 482/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7613 - val_loss: 457.1404\n",
      "Epoch 483/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7614 - val_loss: 457.1402\n",
      "Epoch 484/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7615 - val_loss: 457.1401\n",
      "Epoch 485/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7616 - val_loss: 457.1400\n",
      "Epoch 486/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7617 - val_loss: 457.1399\n",
      "Epoch 487/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7618 - val_loss: 457.1396\n",
      "Epoch 488/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7618 - val_loss: 457.1393\n",
      "Epoch 489/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7620 - val_loss: 457.1391\n",
      "Epoch 490/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7621 - val_loss: 457.1388\n",
      "Epoch 491/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7621 - val_loss: 457.1385\n",
      "Epoch 492/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7622 - val_loss: 457.1382\n",
      "Epoch 493/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7623 - val_loss: 457.1380\n",
      "Epoch 494/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7624 - val_loss: 457.1379\n",
      "Epoch 495/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7625 - val_loss: 457.1374\n",
      "Epoch 496/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 18.7626 - val_loss: 457.1373\n",
      "Epoch 497/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7627 - val_loss: 457.1371\n",
      "Epoch 498/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.7627 - val_loss: 457.1370\n",
      "Epoch 499/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7628 - val_loss: 457.1370\n",
      "Epoch 500/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7629 - val_loss: 457.1367\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(89.3180, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0010, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(-0.0204, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.2194, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 370ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.86881886e+01, 6.86825864e+01, 6.86769841e+01, 6.86713819e+01,\n",
       "        6.86657797e+01, 6.86601774e+01, 6.86545752e+01, 6.86489729e+01,\n",
       "        6.86433707e+01, 6.86377684e+01, 6.86321662e+01, 6.86265640e+01,\n",
       "        6.86209617e+01, 7.18376961e+01, 7.18049230e+01, 7.17721499e+01,\n",
       "        7.17393768e+01, 7.17066036e+01, 7.16738305e+01, 7.16410574e+01,\n",
       "        7.16082843e+01, 7.15560457e+01, 7.14972222e+01, 7.14383987e+01,\n",
       "        7.13795752e+01, 7.13207516e+01, 7.12619281e+01, 7.12031046e+01,\n",
       "        7.11442811e+01, 7.10854575e+01, 7.10266340e+01, 7.09678105e+01,\n",
       "        7.09089869e+01, 7.08003268e+01, 7.06826797e+01, 7.05650327e+01,\n",
       "        7.04473856e+01, 7.03297386e+01, 7.02120915e+01, 7.00944444e+01,\n",
       "        1.01569690e-01, 0.00000000e+00, 3.28962000e-01, 3.64839260e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.04604575e+01,\n",
       "        7.03428105e+01, 7.02251634e+01, 7.01075163e+01, 6.99898693e+01,\n",
       "        6.98722222e+01, 6.97545752e+01, 6.96369281e+01, 6.95192811e+01,\n",
       "        6.94508170e+01, 6.93919935e+01, 6.93331699e+01, 6.92743464e+01,\n",
       "        6.92155229e+01, 6.91566993e+01, 6.90987758e+01, 6.90390523e+01,\n",
       "        6.89802288e+01, 6.89214052e+01, 6.88625817e+01, 6.88037582e+01,\n",
       "        6.87921335e+01, 6.87837302e+01, 6.87753268e+01, 6.87669234e+01,\n",
       "        6.87585201e+01, 7.66012421e+01, 5.02604306e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.92042941e-01, 5.32064199e-01, 0.00000000e+00,\n",
       "        5.11216431e+01, 0.00000000e+00, 8.64292160e-02, 5.21909118e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.25713027e-01, 2.20390141e-01,\n",
       "        4.10099030e-01, 6.02742322e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 8.29568505e-01,\n",
       "        1.37789130e-01, 0.00000000e+00, 0.00000000e+00, 5.64808011e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([65.07049259, 65.06284075, 65.0551889 , 65.04753706, 65.03988522,\n",
       "       65.03223338, 65.02458154, 65.0169297 , 65.00927786, 65.00162602,\n",
       "       64.99397418, 64.98632233, 64.97867049, 64.97101865, 64.96336681,\n",
       "       64.95571497, 64.94806313, 64.94041129, 64.93275945, 64.9251076 ,\n",
       "       64.91745576, 64.90980392, 64.90215208, 64.89450024, 64.8868484 ,\n",
       "       64.87919656, 64.87154472, 64.86389287, 64.85624103, 64.84858919,\n",
       "       64.84093735, 64.83328551, 64.82563367, 64.81798183, 64.81032999,\n",
       "       64.80267814, 64.7950263 , 64.78737446, 64.77972262, 64.77207078,\n",
       "       64.76441894, 64.7567671 , 64.74911526, 64.74146341, 64.73381157,\n",
       "       64.72615973, 64.71850789, 64.71085605, 64.70320421, 64.69555237,\n",
       "       64.68790053, 64.68024868, 64.67259684, 64.664945  , 64.65729316,\n",
       "       64.64964132, 64.64198948, 64.63433764, 64.6266858 , 64.61903396,\n",
       "       64.61138211, 64.60373027, 64.59607843, 64.58842659, 64.58077475,\n",
       "       64.57312291, 64.56547107, 64.55781923, 64.55016738, 64.54251554,\n",
       "       64.5348637 , 64.52721186, 64.51956002, 64.51190818, 64.50425634,\n",
       "       64.4966045 , 64.48895265, 64.48130081, 64.47364897, 64.46599713,\n",
       "       64.45834529, 64.45069345, 64.44304161, 64.43538977, 64.42773792,\n",
       "       64.42008608, 64.41243424, 64.4047824 , 64.39713056, 64.38947872,\n",
       "       64.38182688, 64.37417504, 64.36652319, 64.35887135, 64.35121951,\n",
       "       64.34356767, 64.33591583, 64.32826399, 64.32061215, 64.31296031])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.67507253954394\n",
      "24.239851147781206\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
