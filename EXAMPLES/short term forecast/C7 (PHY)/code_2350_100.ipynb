{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       92.600000\n",
       "1       92.387115\n",
       "2       92.174230\n",
       "3       91.961345\n",
       "4       91.748459\n",
       "          ...    \n",
       "2445    62.813199\n",
       "2446    62.805548\n",
       "2447    62.797896\n",
       "2448    62.790244\n",
       "2449    62.782592\n",
       "Name: C7, Length: 2450, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c7_interpolated_2350_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       92.600000\n",
       "1       92.387115\n",
       "2       92.174230\n",
       "3       91.961345\n",
       "4       91.748459\n",
       "          ...    \n",
       "2345     0.000000\n",
       "2346     0.717601\n",
       "2347     0.000000\n",
       "2348     0.974744\n",
       "2349     0.000000\n",
       "Name: C7, Length: 2350, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2350)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92.387115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92.174230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.961345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91.748459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     92.600000  0.000298\n",
       "1     92.387115  0.000298\n",
       "2     92.174230  0.000297\n",
       "3     91.961345  0.000297\n",
       "4     91.748459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 89.3180  # Value for C0\n",
    "K0 = -0.0010  # Value for K0\n",
    "K1 = 0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = -0.0204    # Value for b\n",
    "c = 2.2194    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    92.600000    92.387115    92.174230    91.961345    91.748459   \n",
      "351    92.387115    92.174230    91.961345    91.748459    91.535574   \n",
      "352    92.174230    91.961345    91.748459    91.535574    91.322689   \n",
      "353    91.961345    91.748459    91.535574    91.322689    91.109804   \n",
      "354    91.748459    91.535574    91.322689    91.109804    90.896919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    91.535574    91.322689    91.109804    90.896919    90.691597  ...   \n",
      "351    91.322689    91.109804    90.896919    90.691597    90.579552  ...   \n",
      "352    91.109804    90.896919    90.691597    90.579552    90.467507  ...   \n",
      "353    90.896919    90.691597    90.579552    90.467507    90.355462  ...   \n",
      "354    90.691597    90.579552    90.467507    90.355462    90.243417  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   80.261671    0.000263   80.252334    0.000263   80.242997    0.000263   \n",
      "351   80.252334    0.000263   80.242997    0.000263   80.233660    0.000262   \n",
      "352   80.242997    0.000263   80.233660    0.000262   80.224323    0.000262   \n",
      "353   80.233660    0.000262   80.224323    0.000262   80.214986    0.000262   \n",
      "354   80.224323    0.000262   80.214986    0.000262   80.205649    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   80.233660    0.000262   80.224323    0.000262  \n",
      "351   80.224323    0.000262   80.214986    0.000262  \n",
      "352   80.214986    0.000262   80.205649    0.000262  \n",
      "353   80.205649    0.000262   80.196312    0.000262  \n",
      "354   80.196312    0.000262   80.186975    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1900, 1, 251) (1900, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "24/24 [==============================] - 2s 21ms/step - loss: 5470.3013 - val_loss: 3718.4651\n",
      "Epoch 2/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 5378.4731 - val_loss: 3657.3848\n",
      "Epoch 3/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 5287.4136 - val_loss: 3606.6697\n",
      "Epoch 4/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 5209.8525 - val_loss: 3557.5151\n",
      "Epoch 5/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 5133.4238 - val_loss: 3509.1270\n",
      "Epoch 6/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 5058.0039 - val_loss: 3461.4197\n",
      "Epoch 7/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 4983.5088 - val_loss: 3414.3477\n",
      "Epoch 8/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 4909.8921 - val_loss: 3367.8862\n",
      "Epoch 9/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 4837.1265 - val_loss: 3322.0178\n",
      "Epoch 10/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 4765.1953 - val_loss: 3276.7310\n",
      "Epoch 11/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 4694.0825 - val_loss: 3232.0154\n",
      "Epoch 12/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 4623.7764 - val_loss: 3187.8647\n",
      "Epoch 13/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 4554.2661 - val_loss: 3144.2747\n",
      "Epoch 14/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 4485.5444 - val_loss: 3101.2388\n",
      "Epoch 15/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 4417.6001 - val_loss: 3058.7527\n",
      "Epoch 16/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 4350.4268 - val_loss: 3016.8093\n",
      "Epoch 17/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 4284.0156 - val_loss: 2975.4014\n",
      "Epoch 18/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 4218.3604 - val_loss: 2934.5247\n",
      "Epoch 19/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 4153.4531 - val_loss: 2894.1736\n",
      "Epoch 20/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 4089.2876 - val_loss: 2854.3455\n",
      "Epoch 21/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 4025.8569 - val_loss: 2815.0427\n",
      "Epoch 22/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3963.1541 - val_loss: 2773.7319\n",
      "Epoch 23/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3881.9829 - val_loss: 2720.6555\n",
      "Epoch 24/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3812.5623 - val_loss: 2677.7715\n",
      "Epoch 25/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3745.6665 - val_loss: 2636.5220\n",
      "Epoch 26/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3678.7683 - val_loss: 2592.8459\n",
      "Epoch 27/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3605.7156 - val_loss: 2549.1409\n",
      "Epoch 28/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 3535.6399 - val_loss: 2507.4363\n",
      "Epoch 29/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 3468.5107 - val_loss: 2467.2068\n",
      "Epoch 30/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3403.4954 - val_loss: 2428.1208\n",
      "Epoch 31/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 3340.1399 - val_loss: 2390.0002\n",
      "Epoch 32/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 3278.1914 - val_loss: 2352.7358\n",
      "Epoch 33/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 3217.4941 - val_loss: 2316.2556\n",
      "Epoch 34/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3157.9404 - val_loss: 2280.5068\n",
      "Epoch 35/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3099.4517 - val_loss: 2245.4490\n",
      "Epoch 36/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3041.9680 - val_loss: 2211.0503\n",
      "Epoch 37/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2985.4414 - val_loss: 2177.2849\n",
      "Epoch 38/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2929.8340 - val_loss: 2144.1306\n",
      "Epoch 39/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 2875.1118 - val_loss: 2111.5691\n",
      "Epoch 40/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 2821.2488 - val_loss: 2079.5845\n",
      "Epoch 41/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 2768.2197 - val_loss: 2048.1611\n",
      "Epoch 42/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 2716.0027 - val_loss: 2017.2865\n",
      "Epoch 43/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 2664.5813 - val_loss: 1986.9493\n",
      "Epoch 44/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 2613.9360 - val_loss: 1957.1378\n",
      "Epoch 45/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 2564.0522 - val_loss: 1927.8411\n",
      "Epoch 46/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 2514.9150 - val_loss: 1899.0477\n",
      "Epoch 47/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 2466.5120 - val_loss: 1870.7377\n",
      "Epoch 48/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2416.9946 - val_loss: 1835.9602\n",
      "Epoch 49/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 2353.8677 - val_loss: 1803.2723\n",
      "Epoch 50/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2299.7207 - val_loss: 1772.3605\n",
      "Epoch 51/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2248.2542 - val_loss: 1742.8916\n",
      "Epoch 52/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 2198.7217 - val_loss: 1714.4786\n",
      "Epoch 53/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 2150.6594 - val_loss: 1686.9255\n",
      "Epoch 54/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 2103.8237 - val_loss: 1660.1232\n",
      "Epoch 55/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2058.0701 - val_loss: 1634.0000\n",
      "Epoch 56/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2013.3026 - val_loss: 1608.5074\n",
      "Epoch 57/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1969.4541 - val_loss: 1583.6088\n",
      "Epoch 58/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 1926.4731 - val_loss: 1559.2759\n",
      "Epoch 59/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1884.3190 - val_loss: 1535.4857\n",
      "Epoch 60/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1842.9589 - val_loss: 1512.2188\n",
      "Epoch 61/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1802.3661 - val_loss: 1489.4596\n",
      "Epoch 62/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1762.5156 - val_loss: 1467.1921\n",
      "Epoch 63/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1723.3875 - val_loss: 1445.4058\n",
      "Epoch 64/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1684.9636 - val_loss: 1424.0875\n",
      "Epoch 65/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1647.2269 - val_loss: 1403.2273\n",
      "Epoch 66/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1610.1622 - val_loss: 1382.8158\n",
      "Epoch 67/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1573.7565 - val_loss: 1362.8441\n",
      "Epoch 68/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1537.9963 - val_loss: 1343.3037\n",
      "Epoch 69/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1502.8711 - val_loss: 1324.1871\n",
      "Epoch 70/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1468.3689 - val_loss: 1305.4868\n",
      "Epoch 71/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1434.4788 - val_loss: 1287.1957\n",
      "Epoch 72/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1401.1917 - val_loss: 1269.3071\n",
      "Epoch 73/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1368.4977 - val_loss: 1251.8147\n",
      "Epoch 74/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1336.3877 - val_loss: 1234.7118\n",
      "Epoch 75/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1304.8525 - val_loss: 1217.9928\n",
      "Epoch 76/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1273.8846 - val_loss: 1201.6516\n",
      "Epoch 77/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1243.4755 - val_loss: 1185.6829\n",
      "Epoch 78/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1213.6172 - val_loss: 1170.0812\n",
      "Epoch 79/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1184.3026 - val_loss: 1154.8407\n",
      "Epoch 80/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1155.5237 - val_loss: 1139.9568\n",
      "Epoch 81/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1127.2743 - val_loss: 1125.4240\n",
      "Epoch 82/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 1099.5461 - val_loss: 1111.2373\n",
      "Epoch 83/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1072.3330 - val_loss: 1097.3920\n",
      "Epoch 84/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1045.6284 - val_loss: 1083.8829\n",
      "Epoch 85/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1019.4252 - val_loss: 1070.7054\n",
      "Epoch 86/500\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 993.7171 - val_loss: 1057.8552\n",
      "Epoch 87/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 968.4974 - val_loss: 1045.3273\n",
      "Epoch 88/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 943.7604 - val_loss: 1033.1168\n",
      "Epoch 89/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 919.5000 - val_loss: 1021.2201\n",
      "Epoch 90/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 895.7095 - val_loss: 1009.6318\n",
      "Epoch 91/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 872.3835 - val_loss: 998.3483\n",
      "Epoch 92/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 849.5161 - val_loss: 987.3651\n",
      "Epoch 93/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 827.1013 - val_loss: 976.6774\n",
      "Epoch 94/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 805.1332 - val_loss: 966.2814\n",
      "Epoch 95/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 783.6061 - val_loss: 956.1726\n",
      "Epoch 96/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 762.5145 - val_loss: 946.3470\n",
      "Epoch 97/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 741.8531 - val_loss: 936.8006\n",
      "Epoch 98/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 721.6160 - val_loss: 927.5288\n",
      "Epoch 99/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 701.7980 - val_loss: 918.5282\n",
      "Epoch 100/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 682.3932 - val_loss: 909.7938\n",
      "Epoch 101/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 663.3966 - val_loss: 901.3224\n",
      "Epoch 102/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 644.8028 - val_loss: 893.1097\n",
      "Epoch 103/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 626.6067 - val_loss: 885.1517\n",
      "Epoch 104/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 608.8030 - val_loss: 877.4446\n",
      "Epoch 105/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 591.3862 - val_loss: 869.9842\n",
      "Epoch 106/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 574.3513 - val_loss: 862.7663\n",
      "Epoch 107/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 557.6926 - val_loss: 855.7874\n",
      "Epoch 108/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 541.4056 - val_loss: 849.0438\n",
      "Epoch 109/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 525.4854 - val_loss: 842.5313\n",
      "Epoch 110/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 509.9266 - val_loss: 836.2460\n",
      "Epoch 111/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 494.7243 - val_loss: 830.1847\n",
      "Epoch 112/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 479.8734 - val_loss: 824.3428\n",
      "Epoch 113/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 465.3687 - val_loss: 818.7169\n",
      "Epoch 114/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 451.2059 - val_loss: 813.3030\n",
      "Epoch 115/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 437.3796 - val_loss: 808.0974\n",
      "Epoch 116/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 423.8844 - val_loss: 803.0966\n",
      "Epoch 117/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 410.7164 - val_loss: 798.2963\n",
      "Epoch 118/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 397.8698 - val_loss: 793.6932\n",
      "Epoch 119/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 385.3407 - val_loss: 789.2836\n",
      "Epoch 120/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 373.1239 - val_loss: 785.0635\n",
      "Epoch 121/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 361.2142 - val_loss: 781.0296\n",
      "Epoch 122/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 349.6077 - val_loss: 777.1780\n",
      "Epoch 123/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 338.2993 - val_loss: 773.5052\n",
      "Epoch 124/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 327.2841 - val_loss: 770.0075\n",
      "Epoch 125/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 316.5573 - val_loss: 766.6809\n",
      "Epoch 126/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 306.1141 - val_loss: 763.5220\n",
      "Epoch 127/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 295.9502 - val_loss: 760.5274\n",
      "Epoch 128/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 286.0609 - val_loss: 757.6934\n",
      "Epoch 129/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 276.4414 - val_loss: 755.0162\n",
      "Epoch 130/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 267.0869 - val_loss: 752.4925\n",
      "Epoch 131/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 257.9932 - val_loss: 750.1187\n",
      "Epoch 132/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 249.1555 - val_loss: 747.8911\n",
      "Epoch 133/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 240.5694 - val_loss: 745.8063\n",
      "Epoch 134/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 232.2300 - val_loss: 743.8608\n",
      "Epoch 135/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 224.1333 - val_loss: 742.0511\n",
      "Epoch 136/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 216.2746 - val_loss: 740.3737\n",
      "Epoch 137/500\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 208.6494 - val_loss: 738.8253\n",
      "Epoch 138/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 201.2532 - val_loss: 737.4022\n",
      "Epoch 139/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 194.0813 - val_loss: 736.1011\n",
      "Epoch 140/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 187.1298 - val_loss: 734.9186\n",
      "Epoch 141/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 180.3942 - val_loss: 733.8516\n",
      "Epoch 142/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 173.8703 - val_loss: 732.8963\n",
      "Epoch 143/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 167.5533 - val_loss: 732.0497\n",
      "Epoch 144/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 161.4394 - val_loss: 731.3082\n",
      "Epoch 145/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 155.5239 - val_loss: 730.6686\n",
      "Epoch 146/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 149.8027 - val_loss: 730.1279\n",
      "Epoch 147/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 144.2716 - val_loss: 729.6825\n",
      "Epoch 148/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 138.9264 - val_loss: 729.3295\n",
      "Epoch 149/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 133.7629 - val_loss: 729.0653\n",
      "Epoch 150/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 128.7770 - val_loss: 728.8871\n",
      "Epoch 151/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 123.9646 - val_loss: 728.7917\n",
      "Epoch 152/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 119.3216 - val_loss: 728.7760\n",
      "Epoch 153/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 114.8439 - val_loss: 728.8369\n",
      "Epoch 154/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 110.5277 - val_loss: 728.9713\n",
      "Epoch 155/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 106.3690 - val_loss: 729.1762\n",
      "Epoch 156/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 102.3636 - val_loss: 729.4488\n",
      "Epoch 157/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 98.5078 - val_loss: 729.7858\n",
      "Epoch 158/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 94.7978 - val_loss: 730.1847\n",
      "Epoch 159/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 91.2298 - val_loss: 730.6425\n",
      "Epoch 160/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 87.7997 - val_loss: 731.1562\n",
      "Epoch 161/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 84.5039 - val_loss: 731.7232\n",
      "Epoch 162/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 81.3389 - val_loss: 732.3408\n",
      "Epoch 163/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 78.3009 - val_loss: 733.0061\n",
      "Epoch 164/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 75.3862 - val_loss: 733.7166\n",
      "Epoch 165/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 72.5914 - val_loss: 734.4695\n",
      "Epoch 166/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 69.9128 - val_loss: 735.2623\n",
      "Epoch 167/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 67.3470 - val_loss: 736.0926\n",
      "Epoch 168/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 64.8906 - val_loss: 736.9577\n",
      "Epoch 169/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 62.5401 - val_loss: 737.8555\n",
      "Epoch 170/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 60.2923 - val_loss: 738.7832\n",
      "Epoch 171/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 58.1440 - val_loss: 739.7387\n",
      "Epoch 172/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 56.0919 - val_loss: 740.7197\n",
      "Epoch 173/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 54.1327 - val_loss: 741.7239\n",
      "Epoch 174/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 52.2635 - val_loss: 742.7490\n",
      "Epoch 175/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 50.4809 - val_loss: 743.7932\n",
      "Epoch 176/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 48.7823 - val_loss: 744.8539\n",
      "Epoch 177/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 47.1646 - val_loss: 745.9296\n",
      "Epoch 178/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 45.6248 - val_loss: 747.0182\n",
      "Epoch 179/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 44.1601 - val_loss: 748.1177\n",
      "Epoch 180/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 42.7677 - val_loss: 749.2261\n",
      "Epoch 181/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 41.4449 - val_loss: 750.3419\n",
      "Epoch 182/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 40.1890 - val_loss: 751.4631\n",
      "Epoch 183/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 38.9975 - val_loss: 752.5883\n",
      "Epoch 184/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 37.8677 - val_loss: 753.7156\n",
      "Epoch 185/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 36.7973 - val_loss: 754.8436\n",
      "Epoch 186/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 35.7837 - val_loss: 755.9708\n",
      "Epoch 187/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 34.8246 - val_loss: 757.0955\n",
      "Epoch 188/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 33.9178 - val_loss: 758.2167\n",
      "Epoch 189/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 33.0610 - val_loss: 759.3327\n",
      "Epoch 190/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 32.2519 - val_loss: 760.4423\n",
      "Epoch 191/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 31.4885 - val_loss: 761.5445\n",
      "Epoch 192/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 30.7688 - val_loss: 762.6382\n",
      "Epoch 193/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 30.0907 - val_loss: 763.7220\n",
      "Epoch 194/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 29.4522 - val_loss: 764.7949\n",
      "Epoch 195/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 28.8516 - val_loss: 765.8560\n",
      "Epoch 196/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 28.2870 - val_loss: 766.9046\n",
      "Epoch 197/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 27.7566 - val_loss: 767.9396\n",
      "Epoch 198/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 27.2587 - val_loss: 768.9602\n",
      "Epoch 199/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 26.7918 - val_loss: 769.9658\n",
      "Epoch 200/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 26.3542 - val_loss: 770.9554\n",
      "Epoch 201/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 25.9443 - val_loss: 771.9288\n",
      "Epoch 202/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 25.5608 - val_loss: 772.8849\n",
      "Epoch 203/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 25.2023 - val_loss: 773.8237\n",
      "Epoch 204/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 24.8674 - val_loss: 774.7441\n",
      "Epoch 205/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 24.5546 - val_loss: 775.6462\n",
      "Epoch 206/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 24.2630 - val_loss: 776.5294\n",
      "Epoch 207/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 23.9912 - val_loss: 777.3933\n",
      "Epoch 208/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 23.7381 - val_loss: 778.2379\n",
      "Epoch 209/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 23.5026 - val_loss: 779.0627\n",
      "Epoch 210/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 23.2836 - val_loss: 779.8676\n",
      "Epoch 211/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 23.0802 - val_loss: 780.6525\n",
      "Epoch 212/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 22.8914 - val_loss: 781.4171\n",
      "Epoch 213/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 22.7163 - val_loss: 782.1611\n",
      "Epoch 214/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 22.5540 - val_loss: 782.8849\n",
      "Epoch 215/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 22.4039 - val_loss: 783.5886\n",
      "Epoch 216/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 22.2648 - val_loss: 784.2716\n",
      "Epoch 217/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 22.1364 - val_loss: 784.9346\n",
      "Epoch 218/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 22.0177 - val_loss: 785.5773\n",
      "Epoch 219/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 21.9083 - val_loss: 786.2001\n",
      "Epoch 220/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 21.8073 - val_loss: 786.8033\n",
      "Epoch 221/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 21.7142 - val_loss: 787.3865\n",
      "Epoch 222/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 21.6286 - val_loss: 787.9505\n",
      "Epoch 223/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 21.5499 - val_loss: 788.4952\n",
      "Epoch 224/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 21.4774 - val_loss: 789.0206\n",
      "Epoch 225/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 21.4109 - val_loss: 789.5275\n",
      "Epoch 226/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 21.3500 - val_loss: 790.0161\n",
      "Epoch 227/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 21.2940 - val_loss: 790.4867\n",
      "Epoch 228/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 21.2426 - val_loss: 790.9391\n",
      "Epoch 229/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 21.1958 - val_loss: 791.3739\n",
      "Epoch 230/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 21.1529 - val_loss: 791.7919\n",
      "Epoch 231/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 21.1136 - val_loss: 792.1932\n",
      "Epoch 232/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 21.0778 - val_loss: 792.5781\n",
      "Epoch 233/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 21.0451 - val_loss: 792.9473\n",
      "Epoch 234/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 21.0153 - val_loss: 793.3008\n",
      "Epoch 235/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.9881 - val_loss: 793.6392\n",
      "Epoch 236/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.9634 - val_loss: 793.9628\n",
      "Epoch 237/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.9409 - val_loss: 794.2722\n",
      "Epoch 238/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.9204 - val_loss: 794.5673\n",
      "Epoch 239/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.9018 - val_loss: 794.8491\n",
      "Epoch 240/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8849 - val_loss: 795.1177\n",
      "Epoch 241/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8696 - val_loss: 795.3737\n",
      "Epoch 242/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.8557 - val_loss: 795.6174\n",
      "Epoch 243/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8432 - val_loss: 795.8490\n",
      "Epoch 244/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.8318 - val_loss: 796.0692\n",
      "Epoch 245/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8215 - val_loss: 796.2786\n",
      "Epoch 246/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8123 - val_loss: 796.4769\n",
      "Epoch 247/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8039 - val_loss: 796.6650\n",
      "Epoch 248/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 20.7964 - val_loss: 796.8437\n",
      "Epoch 249/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7896 - val_loss: 797.0128\n",
      "Epoch 250/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.7835 - val_loss: 797.1726\n",
      "Epoch 251/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7780 - val_loss: 797.3237\n",
      "Epoch 252/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7731 - val_loss: 797.4666\n",
      "Epoch 253/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7687 - val_loss: 797.6014\n",
      "Epoch 254/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7648 - val_loss: 797.7286\n",
      "Epoch 255/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7613 - val_loss: 797.8484\n",
      "Epoch 256/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7582 - val_loss: 797.9612\n",
      "Epoch 257/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7555 - val_loss: 798.0674\n",
      "Epoch 258/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7531 - val_loss: 798.1674\n",
      "Epoch 259/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.7509 - val_loss: 798.2613\n",
      "Epoch 260/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7490 - val_loss: 798.3492\n",
      "Epoch 261/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7474 - val_loss: 798.4318\n",
      "Epoch 262/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7461 - val_loss: 798.5090\n",
      "Epoch 263/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7448 - val_loss: 798.5814\n",
      "Epoch 264/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7438 - val_loss: 798.6492\n",
      "Epoch 265/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7430 - val_loss: 798.7125\n",
      "Epoch 266/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 20.7423 - val_loss: 798.7714\n",
      "Epoch 267/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.7417 - val_loss: 798.8265\n",
      "Epoch 268/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7413 - val_loss: 798.8781\n",
      "Epoch 269/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7411 - val_loss: 798.9259\n",
      "Epoch 270/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7408 - val_loss: 798.9705\n",
      "Epoch 271/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7407 - val_loss: 799.0119\n",
      "Epoch 272/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7408 - val_loss: 799.0504\n",
      "Epoch 273/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7409 - val_loss: 799.0862\n",
      "Epoch 274/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7409 - val_loss: 799.1191\n",
      "Epoch 275/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7411 - val_loss: 799.1496\n",
      "Epoch 276/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7414 - val_loss: 799.1776\n",
      "Epoch 277/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.7417 - val_loss: 799.2038\n",
      "Epoch 278/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7421 - val_loss: 799.2277\n",
      "Epoch 279/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7425 - val_loss: 799.2498\n",
      "Epoch 280/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7430 - val_loss: 799.2699\n",
      "Epoch 281/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7436 - val_loss: 799.2886\n",
      "Epoch 282/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7441 - val_loss: 799.3057\n",
      "Epoch 283/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.7447 - val_loss: 799.3214\n",
      "Epoch 284/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 20.7453 - val_loss: 799.3357\n",
      "Epoch 285/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7459 - val_loss: 799.3489\n",
      "Epoch 286/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.7466 - val_loss: 799.3608\n",
      "Epoch 287/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7473 - val_loss: 799.3715\n",
      "Epoch 288/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7480 - val_loss: 799.3812\n",
      "Epoch 289/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7488 - val_loss: 799.3901\n",
      "Epoch 290/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7495 - val_loss: 799.3982\n",
      "Epoch 291/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7502 - val_loss: 799.4053\n",
      "Epoch 292/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7510 - val_loss: 799.4121\n",
      "Epoch 293/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7518 - val_loss: 799.4177\n",
      "Epoch 294/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7526 - val_loss: 799.4229\n",
      "Epoch 295/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7534 - val_loss: 799.4275\n",
      "Epoch 296/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7542 - val_loss: 799.4313\n",
      "Epoch 297/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 20.7551 - val_loss: 799.4349\n",
      "Epoch 298/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7558 - val_loss: 799.4379\n",
      "Epoch 299/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7567 - val_loss: 799.4406\n",
      "Epoch 300/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7575 - val_loss: 799.4430\n",
      "Epoch 301/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 20.7584 - val_loss: 799.4450\n",
      "Epoch 302/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7592 - val_loss: 799.4468\n",
      "Epoch 303/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.7600 - val_loss: 799.4479\n",
      "Epoch 304/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7609 - val_loss: 799.4489\n",
      "Epoch 305/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7617 - val_loss: 799.4499\n",
      "Epoch 306/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7625 - val_loss: 799.4503\n",
      "Epoch 307/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 20.7634 - val_loss: 799.4507\n",
      "Epoch 308/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7642 - val_loss: 799.4508\n",
      "Epoch 309/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 20.7651 - val_loss: 799.4509\n",
      "Epoch 310/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7659 - val_loss: 799.4508\n",
      "Epoch 311/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7667 - val_loss: 799.4507\n",
      "Epoch 312/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7675 - val_loss: 799.4503\n",
      "Epoch 313/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7683 - val_loss: 799.4498\n",
      "Epoch 314/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7691 - val_loss: 799.4489\n",
      "Epoch 315/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7699 - val_loss: 799.4481\n",
      "Epoch 316/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7708 - val_loss: 799.4473\n",
      "Epoch 317/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7715 - val_loss: 799.4465\n",
      "Epoch 318/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7723 - val_loss: 799.4456\n",
      "Epoch 319/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7731 - val_loss: 799.4448\n",
      "Epoch 320/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7739 - val_loss: 799.4438\n",
      "Epoch 321/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7746 - val_loss: 799.4427\n",
      "Epoch 322/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.7754 - val_loss: 799.4416\n",
      "Epoch 323/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7762 - val_loss: 799.4408\n",
      "Epoch 324/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.7769 - val_loss: 799.4395\n",
      "Epoch 325/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7776 - val_loss: 799.4382\n",
      "Epoch 326/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7784 - val_loss: 799.4373\n",
      "Epoch 327/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7791 - val_loss: 799.4361\n",
      "Epoch 328/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7798 - val_loss: 799.4349\n",
      "Epoch 329/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.7805 - val_loss: 799.4335\n",
      "Epoch 330/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7812 - val_loss: 799.4323\n",
      "Epoch 331/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 20.7819 - val_loss: 799.4311\n",
      "Epoch 332/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7826 - val_loss: 799.4299\n",
      "Epoch 333/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7833 - val_loss: 799.4286\n",
      "Epoch 334/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.7839 - val_loss: 799.4275\n",
      "Epoch 335/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7845 - val_loss: 799.4263\n",
      "Epoch 336/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7852 - val_loss: 799.4249\n",
      "Epoch 337/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7859 - val_loss: 799.4237\n",
      "Epoch 338/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7864 - val_loss: 799.4224\n",
      "Epoch 339/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7871 - val_loss: 799.4214\n",
      "Epoch 340/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7877 - val_loss: 799.4198\n",
      "Epoch 341/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7884 - val_loss: 799.4186\n",
      "Epoch 342/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7889 - val_loss: 799.4178\n",
      "Epoch 343/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7895 - val_loss: 799.4164\n",
      "Epoch 344/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.7901 - val_loss: 799.4153\n",
      "Epoch 345/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7906 - val_loss: 799.4141\n",
      "Epoch 346/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7912 - val_loss: 799.4131\n",
      "Epoch 347/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7918 - val_loss: 799.4122\n",
      "Epoch 348/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7923 - val_loss: 799.4110\n",
      "Epoch 349/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7928 - val_loss: 799.4100\n",
      "Epoch 350/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7933 - val_loss: 799.4087\n",
      "Epoch 351/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7939 - val_loss: 799.4076\n",
      "Epoch 352/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 20.7944 - val_loss: 799.4067\n",
      "Epoch 353/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.7948 - val_loss: 799.4055\n",
      "Epoch 354/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.7954 - val_loss: 799.4044\n",
      "Epoch 355/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7959 - val_loss: 799.4034\n",
      "Epoch 356/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7964 - val_loss: 799.4022\n",
      "Epoch 357/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7968 - val_loss: 799.4015\n",
      "Epoch 358/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7973 - val_loss: 799.4004\n",
      "Epoch 359/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7978 - val_loss: 799.3998\n",
      "Epoch 360/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7982 - val_loss: 799.3987\n",
      "Epoch 361/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7986 - val_loss: 799.3977\n",
      "Epoch 362/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.7991 - val_loss: 799.3967\n",
      "Epoch 363/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7995 - val_loss: 799.3959\n",
      "Epoch 364/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.7999 - val_loss: 799.3948\n",
      "Epoch 365/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8003 - val_loss: 799.3941\n",
      "Epoch 366/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8008 - val_loss: 799.3933\n",
      "Epoch 367/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8011 - val_loss: 799.3926\n",
      "Epoch 368/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8015 - val_loss: 799.3918\n",
      "Epoch 369/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8019 - val_loss: 799.3909\n",
      "Epoch 370/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8023 - val_loss: 799.3903\n",
      "Epoch 371/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8026 - val_loss: 799.3895\n",
      "Epoch 372/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.8030 - val_loss: 799.3889\n",
      "Epoch 373/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8033 - val_loss: 799.3880\n",
      "Epoch 374/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8037 - val_loss: 799.3871\n",
      "Epoch 375/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 20.8040 - val_loss: 799.3865\n",
      "Epoch 376/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8044 - val_loss: 799.3860\n",
      "Epoch 377/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8047 - val_loss: 799.3853\n",
      "Epoch 378/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8050 - val_loss: 799.3845\n",
      "Epoch 379/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8053 - val_loss: 799.3839\n",
      "Epoch 380/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.8056 - val_loss: 799.3834\n",
      "Epoch 381/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8059 - val_loss: 799.3826\n",
      "Epoch 382/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8062 - val_loss: 799.3820\n",
      "Epoch 383/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8065 - val_loss: 799.3814\n",
      "Epoch 384/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8067 - val_loss: 799.3806\n",
      "Epoch 385/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8071 - val_loss: 799.3801\n",
      "Epoch 386/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8073 - val_loss: 799.3793\n",
      "Epoch 387/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8076 - val_loss: 799.3788\n",
      "Epoch 388/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8079 - val_loss: 799.3785\n",
      "Epoch 389/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.8081 - val_loss: 799.3781\n",
      "Epoch 390/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8084 - val_loss: 799.3776\n",
      "Epoch 391/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8086 - val_loss: 799.3770\n",
      "Epoch 392/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8089 - val_loss: 799.3766\n",
      "Epoch 393/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 20.8091 - val_loss: 799.3761\n",
      "Epoch 394/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8094 - val_loss: 799.3757\n",
      "Epoch 395/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8096 - val_loss: 799.3750\n",
      "Epoch 396/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.8098 - val_loss: 799.3747\n",
      "Epoch 397/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8100 - val_loss: 799.3743\n",
      "Epoch 398/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8102 - val_loss: 799.3740\n",
      "Epoch 399/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.8104 - val_loss: 799.3735\n",
      "Epoch 400/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8106 - val_loss: 799.3731\n",
      "Epoch 401/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.8108 - val_loss: 799.3727\n",
      "Epoch 402/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8110 - val_loss: 799.3719\n",
      "Epoch 403/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8112 - val_loss: 799.3714\n",
      "Epoch 404/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8114 - val_loss: 799.3710\n",
      "Epoch 405/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8116 - val_loss: 799.3708\n",
      "Epoch 406/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8118 - val_loss: 799.3704\n",
      "Epoch 407/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.8120 - val_loss: 799.3702\n",
      "Epoch 408/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8121 - val_loss: 799.3696\n",
      "Epoch 409/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8123 - val_loss: 799.3694\n",
      "Epoch 410/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8124 - val_loss: 799.3691\n",
      "Epoch 411/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8126 - val_loss: 799.3686\n",
      "Epoch 412/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8128 - val_loss: 799.3683\n",
      "Epoch 413/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8129 - val_loss: 799.3677\n",
      "Epoch 414/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8131 - val_loss: 799.3672\n",
      "Epoch 415/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 20.8132 - val_loss: 799.3670\n",
      "Epoch 416/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8133 - val_loss: 799.3668\n",
      "Epoch 417/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8135 - val_loss: 799.3663\n",
      "Epoch 418/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8136 - val_loss: 799.3661\n",
      "Epoch 419/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8138 - val_loss: 799.3658\n",
      "Epoch 420/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8139 - val_loss: 799.3654\n",
      "Epoch 421/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8140 - val_loss: 799.3651\n",
      "Epoch 422/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8142 - val_loss: 799.3649\n",
      "Epoch 423/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8143 - val_loss: 799.3648\n",
      "Epoch 424/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8144 - val_loss: 799.3645\n",
      "Epoch 425/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.8146 - val_loss: 799.3643\n",
      "Epoch 426/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8146 - val_loss: 799.3640\n",
      "Epoch 427/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8147 - val_loss: 799.3638\n",
      "Epoch 428/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8149 - val_loss: 799.3636\n",
      "Epoch 429/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8150 - val_loss: 799.3633\n",
      "Epoch 430/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8151 - val_loss: 799.3632\n",
      "Epoch 431/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8151 - val_loss: 799.3630\n",
      "Epoch 432/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8153 - val_loss: 799.3629\n",
      "Epoch 433/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8154 - val_loss: 799.3625\n",
      "Epoch 434/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8155 - val_loss: 799.3622\n",
      "Epoch 435/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8156 - val_loss: 799.3620\n",
      "Epoch 436/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8156 - val_loss: 799.3617\n",
      "Epoch 437/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 20.8158 - val_loss: 799.3615\n",
      "Epoch 438/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8158 - val_loss: 799.3613\n",
      "Epoch 439/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8159 - val_loss: 799.3613\n",
      "Epoch 440/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8160 - val_loss: 799.3611\n",
      "Epoch 441/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8161 - val_loss: 799.3610\n",
      "Epoch 442/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8162 - val_loss: 799.3607\n",
      "Epoch 443/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8163 - val_loss: 799.3606\n",
      "Epoch 444/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8163 - val_loss: 799.3604\n",
      "Epoch 445/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8165 - val_loss: 799.3604\n",
      "Epoch 446/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8165 - val_loss: 799.3600\n",
      "Epoch 447/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.8166 - val_loss: 799.3599\n",
      "Epoch 448/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8166 - val_loss: 799.3597\n",
      "Epoch 449/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8167 - val_loss: 799.3594\n",
      "Epoch 450/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8167 - val_loss: 799.3592\n",
      "Epoch 451/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8168 - val_loss: 799.3589\n",
      "Epoch 452/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8169 - val_loss: 799.3588\n",
      "Epoch 453/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8169 - val_loss: 799.3587\n",
      "Epoch 454/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8170 - val_loss: 799.3586\n",
      "Epoch 455/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8170 - val_loss: 799.3584\n",
      "Epoch 456/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8171 - val_loss: 799.3582\n",
      "Epoch 457/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 20.8171 - val_loss: 799.3582\n",
      "Epoch 458/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8173 - val_loss: 799.3582\n",
      "Epoch 459/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8172 - val_loss: 799.3582\n",
      "Epoch 460/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8173 - val_loss: 799.3577\n",
      "Epoch 461/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8174 - val_loss: 799.3575\n",
      "Epoch 462/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8174 - val_loss: 799.3574\n",
      "Epoch 463/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8175 - val_loss: 799.3572\n",
      "Epoch 464/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8175 - val_loss: 799.3572\n",
      "Epoch 465/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8176 - val_loss: 799.3571\n",
      "Epoch 466/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8176 - val_loss: 799.3569\n",
      "Epoch 467/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.8177 - val_loss: 799.3569\n",
      "Epoch 468/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8177 - val_loss: 799.3568\n",
      "Epoch 469/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8178 - val_loss: 799.3567\n",
      "Epoch 470/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8178 - val_loss: 799.3567\n",
      "Epoch 471/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8178 - val_loss: 799.3566\n",
      "Epoch 472/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8179 - val_loss: 799.3563\n",
      "Epoch 473/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 20.8179 - val_loss: 799.3561\n",
      "Epoch 474/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8180 - val_loss: 799.3561\n",
      "Epoch 475/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8180 - val_loss: 799.3560\n",
      "Epoch 476/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.8180 - val_loss: 799.3559\n",
      "Epoch 477/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8181 - val_loss: 799.3556\n",
      "Epoch 478/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8181 - val_loss: 799.3556\n",
      "Epoch 479/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8181 - val_loss: 799.3554\n",
      "Epoch 480/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8182 - val_loss: 799.3551\n",
      "Epoch 481/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8182 - val_loss: 799.3551\n",
      "Epoch 482/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8183 - val_loss: 799.3551\n",
      "Epoch 483/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8183 - val_loss: 799.3549\n",
      "Epoch 484/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8183 - val_loss: 799.3548\n",
      "Epoch 485/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8183 - val_loss: 799.3547\n",
      "Epoch 486/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8184 - val_loss: 799.3547\n",
      "Epoch 487/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8184 - val_loss: 799.3547\n",
      "Epoch 488/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8184 - val_loss: 799.3547\n",
      "Epoch 489/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.8184 - val_loss: 799.3546\n",
      "Epoch 490/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8184 - val_loss: 799.3544\n",
      "Epoch 491/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 20.8185 - val_loss: 799.3546\n",
      "Epoch 492/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8185 - val_loss: 799.3544\n",
      "Epoch 493/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8186 - val_loss: 799.3544\n",
      "Epoch 494/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8186 - val_loss: 799.3544\n",
      "Epoch 495/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8186 - val_loss: 799.3544\n",
      "Epoch 496/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8186 - val_loss: 799.3542\n",
      "Epoch 497/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8186 - val_loss: 799.3541\n",
      "Epoch 498/500\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 20.8187 - val_loss: 799.3542\n",
      "Epoch 499/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8187 - val_loss: 799.3542\n",
      "Epoch 500/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 20.8187 - val_loss: 799.3542\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(89.3180, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0010, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(-0.0204, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.2194, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 344ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.88233660e+01, 6.87949346e+01, 6.87865313e+01, 6.87778128e+01,\n",
       "        6.87697246e+01, 6.87613212e+01, 6.87529178e+01, 3.22384357e-01,\n",
       "        0.00000000e+00, 3.86938380e-02, 0.00000000e+00, 3.08255672e-01,\n",
       "        0.00000000e+00, 7.05388889e+01, 7.04212418e+01, 7.03035948e+01,\n",
       "        7.01859477e+01, 7.00683006e+01, 6.99506536e+01, 6.98330065e+01,\n",
       "        6.97153595e+01, 6.95977124e+01, 6.94900327e+01, 6.94312092e+01,\n",
       "        6.93723856e+01, 6.93135621e+01, 6.92547386e+01, 6.91959150e+01,\n",
       "        6.91370915e+01, 6.90782680e+01, 6.90184444e+01, 6.89606209e+01,\n",
       "        6.89017974e+01, 6.88429739e+01, 6.87977358e+01, 6.87893324e+01,\n",
       "        6.87809290e+01, 6.87725257e+01, 6.87641223e+01, 6.87557190e+01,\n",
       "        2.61637956e-01, 1.06601441e+00, 0.00000000e+00, 5.99627720e-02,\n",
       "        6.04974806e-01, 4.62526143e-01, 0.00000000e+00, 6.87818628e+01,\n",
       "        6.87734594e+01, 6.87650550e+01, 6.87566527e+01, 6.87482493e+01,\n",
       "        6.87389460e+01, 6.87305426e+01, 6.87221392e+01, 6.87137358e+01,\n",
       "        6.87053325e+01, 6.86969292e+01, 6.86885259e+01, 6.86801226e+01,\n",
       "        6.86717193e+01, 6.86633160e+01, 6.86549127e+01, 7.17830742e+01,\n",
       "        7.16847549e+01, 7.15756536e+01, 7.13991830e+01, 7.12227124e+01,\n",
       "        7.10462418e+01, 7.08395425e+01, 7.04866013e+01, 7.01336601e+01,\n",
       "        5.73052470e-01, 0.00000000e+00, 0.00000000e+00, 1.88981742e-01,\n",
       "        0.00000000e+00, 4.43688333e-01, 0.00000000e+00, 2.20610276e-01,\n",
       "        4.72162361e+01, 6.02305353e-01, 4.56899971e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.05581948e-01, 0.00000000e+00, 7.28137791e-02,\n",
       "        0.00000000e+00, 7.19909728e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.13549078e-01, 4.15871710e-01, 2.89427280e-01,\n",
       "        3.49062830e-02, 0.00000000e+00, 4.44268286e-01, 1.08536816e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([63.54012434, 63.5324725 , 63.52482066, 63.51716882, 63.50951698,\n",
       "       63.50186514, 63.4942133 , 63.48656145, 63.47890961, 63.47125777,\n",
       "       63.46360593, 63.45595409, 63.44830225, 63.44065041, 63.43299857,\n",
       "       63.42534672, 63.41769488, 63.41004304, 63.4023912 , 63.39473936,\n",
       "       63.38708752, 63.37943568, 63.37178384, 63.36413199, 63.35648015,\n",
       "       63.34882831, 63.34117647, 63.33352463, 63.32587279, 63.31822095,\n",
       "       63.31056911, 63.30291726, 63.29526542, 63.28761358, 63.27996174,\n",
       "       63.2723099 , 63.26465806, 63.25700622, 63.24935438, 63.24170253,\n",
       "       63.23405069, 63.22639885, 63.21874701, 63.21109517, 63.20344333,\n",
       "       63.19579149, 63.18813965, 63.1804878 , 63.17283596, 63.16518412,\n",
       "       63.15753228, 63.14988044, 63.1422286 , 63.13457676, 63.12692492,\n",
       "       63.11927308, 63.11162123, 63.10396939, 63.09631755, 63.08866571,\n",
       "       63.08101387, 63.07336203, 63.06571019, 63.05805835, 63.0504065 ,\n",
       "       63.04275466, 63.03510282, 63.02745098, 63.01979914, 63.0121473 ,\n",
       "       63.00449546, 62.99684362, 62.98919177, 62.98153993, 62.97388809,\n",
       "       62.96623625, 62.95858441, 62.95093257, 62.94328073, 62.93562889,\n",
       "       62.92797704, 62.9203252 , 62.91267336, 62.90502152, 62.89736968,\n",
       "       62.88971784, 62.882066  , 62.87441416, 62.86676231, 62.85911047,\n",
       "       62.85145863, 62.84380679, 62.83615495, 62.82850311, 62.82085127,\n",
       "       62.81319943, 62.80554758, 62.79789574, 62.7902439 , 62.78259206])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.036762678299056\n",
      "28.901323636989854\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
