{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       92.600000\n",
       "1       92.387115\n",
       "2       92.174230\n",
       "3       91.961345\n",
       "4       91.748459\n",
       "          ...    \n",
       "2495    62.430607\n",
       "2496    62.422956\n",
       "2497    62.415304\n",
       "2498    62.407652\n",
       "2499    62.400000\n",
       "Name: C7, Length: 2500, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c7_interpolated_2450_50.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       92.600000\n",
       "1       92.387115\n",
       "2       92.174230\n",
       "3       91.961345\n",
       "4       91.748459\n",
       "          ...    \n",
       "2445     0.289427\n",
       "2446     0.034906\n",
       "2447     0.000000\n",
       "2448     0.444268\n",
       "2449     1.085368\n",
       "Name: C7, Length: 2450, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(50)\n",
    "test\n",
    "training_set = training_set.head(2450)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92.387115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92.174230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.961345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91.748459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     92.600000  0.000298\n",
       "1     92.387115  0.000298\n",
       "2     92.174230  0.000297\n",
       "3     91.961345  0.000297\n",
       "4     91.748459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAAsTAAALEwEAmpwYAAAnFElEQVR4nO3deZhcZYHv8e/b+76lO52kO0lnBUNCCGkgGSAyoCAoog4uc1VyHR3uMyMjznUWnO16Z3R0rhe9Xh9HhxFGZvSKC4wsgoBsyp4EkhASErJv3Ukn6e5s3eml3vtHneq1uuucU3WqTlX/Ps+Tp2s5y3u60r/z1nvO+77GWouIiGS/vEwXQEREUkOBLiKSIxToIiI5QoEuIpIjFOgiIjmiIJ07q6+vty0tLencpYhI1tuwYcMxa21DouXSGugtLS2sX78+nbsUEcl6xph9bpZTk4uISI5QoIuI5AgFuohIjlCgi4jkCAW6iEiOUKCLiOQIBbqISI7IikB/aNNhfviyq9swRUSmrKwI9F9taePbT7+Nxm4XEZlYVgT6VedN58jJc2xrO5XpooiIhFZ2BPri6BAGz2w/muGSiIiEV1YE+vSqEpY2VfGsAl1EZEJZEegAV583nQ37Otl3/EymiyIiEkpZE+gfXzWX4oJ8vv749kwXRUQklLIm0BurSvjMlfN4ZHMbmw50Zbo4IiKhkzWBDnDrmvlMKy/iHx/dplsYRUTGyKpArywp5HPXLOKVPSe47f+9zv7jZzNdJBGR0EjrjEWp8IlVc+k828e/PLebJ7a2s3Z1C7ddvZCasqJMF01EJKNMOpsuWltbbaqmoDtyspdvPLGDn244QFVJIf/1d1q46rwGljVVU5CfVV88REQmZYzZYK1tTbhctgZ6zLa2k3ztsbd4bkcHAJXFBVw2fxpXLqrnw63NlBVl3ZcQEZFRpkygxxw7fY6Xdh3nxV3HeGHncfafOMus6hL+9n1LeM/SGRhjAtmviEjQplygj/XqnhP83YNbeKv9FFcuqudL77+ABQ0Vadm3iEgqTflABxgYjPDDl/dx55M76O0f5OaVs3nXO6azav40yovVFCMi2UGBPkLHqXN8/fG3eHhTGz39gxTl59HaUss7FzewZnED58+oVJOMiISWAj2O3v5B1u/t5Ddvd/Dc9g62H4kOxzu9spg1TrhfubCe2nLdAiki4aFAd6G9uzca7js6eP7tY3T39GMMXNhcwzsX1bNmcQPLZ9dQqNsgRSSDFOgeDUYsmw528Zsd0YDfdKCLiIWK4gJWza/j8oX1XLmongUNFWqeEZG0UqAnqetsHy/uOs7zO4/xws5j7HOGGWisKh4K98sX1DO9qiTDJRWRXKdAT7EDJ87yws5j/HbnMV7ceYzOs/0ALG6s4PKF9bx32UxWzq1V7V1EUk6BHqBIxLK17SQv7DzG8zuP8eqeE5wbiDCvvpybVzbzoYubmFldmuliikiOUKCn0ZlzAzy2pZ2frT/AK3tOkGfgikUN3LyymWuXNFJSmJ/pIopIFlOgZ8i+42e4/7VD3L/hIIe6eqgqKeDG5bP4cOtsljdXq0lGRDxLaaAbY/4U+AxggTeATwEzgfuAacAG4JPW2r7JtjMVAj0mErG8vPs4P9twkMe2tNHbH2HR9ApuXtnMBy9uYnqlLqaKiDspC3RjTBPwPLDEWttjjPkp8ChwA/CAtfY+Y8z3gE3W2u9Otq2pFOgjnezt59HNbfxsw0E27OskP8+wZlE9F8+pZcH0CuY3lNMyrVxNMyISl9tAdzugSQFQaozpB8qANuBq4L84798LfAmYNNCnqqqSQj526Rw+dukcdnec5ucbDvLw5sM8s71jaBljoKmmlPkNFcyvL2fB9AoW1Jczv6GCxqpiNdWISEJum1xuB74C9ABPALcDL1trFzrvzwYes9YujbPurcCtAHPmzFm5b9++1JU+y505N8CeY2fY1XGa3R1n2H3sDLudxz39g0PLlRflM6+hnPn10dr8rOpS6iuLmFZeTH1lMdPKi1S7F8lhKauhG2NqgZuAeUAX8DPgPW4LYq29C7gLok0ubtebCsqLC1jaVM3SpupRr1traT/Zy66jZ9h9LBrwuzpOs2FfJw9tOhx3W5XFBUyrKKK+ophpFUVMqyimvqKY+ooipleWsKy5mlnVJarpi+QwN00u7wL2WGs7AIwxDwCXAzXGmAJr7QDQDBwKrphTizGGmdWlzKwu5YpF9aPe6+0fpOPUOY6dPsex030cPz38+Njpcxw/3ceeY2dYt7eTzrN9jPwC1lBZzIrZNVw0p4YVs2u5sLlawwiL5BA3f837gVXGmDKiTS7XAOuBZ4Cbid7pshZ4MKhCyrCSwnxm15Uxu64s4bIDgxFOnO3jcFcvmw92sXF/F68f6OKJrUcAyDOwuLGSFXNqWTG7hhVzaljQUEFenmrxItnIbRv6/wQ+CgwArxO9hbGJaJjXOa99wlp7brLtTNW7XMKm80wfGw928fr+LjYe6GLj/k5O9g4A0aab5bNruGh2DefPrKSpppSm2lIaKnRhVqLfEH/w4l4+c8W80EzGvvXwSfYdP8P1y2ZmuiiBUccicS0Ssew5fsYJ+E5e39/FW+2nGIwM/98oKsiLhrvzb5YT9E01pTTXljKjukTDDE8B33xyB9966m2+/IGlfGLV3EwXB4CWO34JwN6vvTfDJQlOqm9blByWl2dY0FDBgoZoxyeAnr5B9h4/w6HOHg519XC4q4eDXT0c6uzh6e1H6Tg1+suYMdBYWUJTbSmLGytY2lTNhU01LJ5RQXGB7sDJFaecb3K9I+7CSmTvsTOUFxfQUFnsavne/kHePnKaZc3ViRf2obd/kJ1HT4+7GWEy29pOMruujIqQX3MKd+kkY0qL8nnHzCreMbMq7vu9/YO0d/dyyAn5g07oHzhxlkffaOfHrx4AoDDfcN6MSpY1VbOsqYZlTdUK+SwWcb7R53lofrvqfz8LuK9B/9V/vsEDrx3i5S9ew4zq1Peo/oufb+ahTYdZ99fvcnWSiUQs13/rt1w2r46f/LfVKS9PKinQxZeSwnxa6stpqS8f9561loOdPbxxqJvNB7vZcqh7gpCPBvxFs2t4x0zN65oN7FCgB7ePjfu7ADjTNxDI9l/b3wm4/5Yx6Bzz+n2dgZQnlRToknLGmKE7cW5wLlRZazlwIhry0X9d/HLzYX786n4AWqaVcePyWdy4fBaLGyszWXyZROyySpB3QsW+BQS1B6+XDWPLZ8PNXwp0SQtjDHOmlTFnWhnvvXB0yL+0+xgPb2rjO8/s5NtP7+S8xkref9Es3nfhTOZOG/8NQDIn6LCF6AiA4K1Zx9P2Y98yXCb00DFnwTdIBbpkzHDIz+Gjl8yh49Q5HtvSxkMbD/P1x7fz9ce3s7y5mhuXz+J9F84KpD1VvImFbZDh5qed3tv2cbbvbnnV0EV8aKgs5pbVLdyyuoVDXT38cvNhHt7Uxpd/uY2vPLqNS1rquHH5LG5YOoNpFe7umJDUsgGHLUAkEv0Z1C6Gv2V4rKEH+r0kNRToEkpNNaXcumYBt65ZwO6O0zyyuY2HNh3mb3+xhS899CaXL6zn2iWNXNJSx6Lp6t2aLrGwTcevO6hAH27SCWb5TFKgS+jNb6jgc9cs4k+uXshb7ad4eNNhHt58mL/5RXT44aqSAlpb6mhtqeWSljqWNVVr9MmABN0cko59WI9t4uk45lRRoEvWMMYM3Rv/59edx4ETPazbe4L1+06wbm8nT791FICi/DwubK5mZUstl8ytY+XcWmrLizJc+tww1Hk4wGwLXRu6860kC1pcFOiSnUbeNfN7Tu/WE2f62LCvk/V7T7Bu7wnueX4P//LcbgAWTa+gtaWOhdMrqCktpKYs+q+6tMj5WTjlhy44crKXmrLCSTt9WdJRQ8fZx8TL9A1EyM8z5E+w0JlzAxTm51FUMP4z9XrXitdjttZibbC3dk5EgS45o668iHcvaeTdSxqBaMeRzQe7o7X4vSd4ZPPhoa7r8ZQX5VNTVkR1nMCvKS1kelUxTTVlNNWW0lhZHJrBqVJhd8dprr7zOYry81gyq4oVc2pYs7iBKxfWjzrOdNzxYV18C/jUD17lxV3H+dkEPTc//L2X2Np2kp/cuorL5k8b9V4k4u1G9Njibs9hH/2Xlzk3GOHBz17uaT+poECXnFVSmM+l8+q4dF4dEP1DPnVugO6z/XT19NF1tp+unn66zw4/7jrbT7fz3o4jp+nu6afrbB/9g6NDID/PMKMqOnZN85jBymI/s6kd//iZ6PzuV53XQNfZfn786n7+7YW91FcUcePyWXxoRTNLm6rS1J6c+K6SF3YeB+CWe16N+/7WtpMArP23V/nhpy+jtaVuzNbd83rMr+49AUS/8TRWpfdWWwW6TBl5eYbq0mjzyhwSjycfY63lbN8g7Sd7hwYrG/nzlT0naD/ZO2p0SoA5dWVc5Iwzf9HsGpbMqgrtGDb9g9GG4k9dPo/VC6ZxbmCQZ7d38J+vHeJHL0fDvXVuLb0D0e7yxsDOo6fpH4xMON7PWH0DEZ7f2cEVCxviNoXEuKlAN9WUsmB6BfuPn2Hv8bPj3q8pK+TyhfVsO3ySP/jBOh667QoqSgqoryge1VO0fzDCLXe/ys0rm5lZXcLvLByeUGb/8bPMmVY2ItBH7+OFncdY2lRNdWlh3DI+u/0oH71kTuKDSSEFukgCxhjKiwuGRqSMZ2AwMirwD3b2sK3tJK/uOTE0bWCsOSMW8itm1zK7rjQUPRBjJ6PC/GhZigvyue6CGVx3wQy6z/bz4KZD3PnEDrp7+oHo7+SvHniDdftOsHZ1C39+3XkJZ7/69bYj/PGPXuP8GZV87fcu5KLZNXGXi7jom98/GGFWdQlfvP58rv/Wb8cfz6ClsbKEP71lMdd+87mhAcK2/v11o7Z/qneAl3Yf56Xd0Rr/j/9wFasXTGPLoW7e9+3n+cK7F/ORS2YPHXNM30CEj3//FapKCtj8petG7bupppRDXT1sazuV8DhSTYEukgIF+Xk015bRXDu+5t/W3cNGZzKR1/d3cd+6/fzgxb0ATCsv4iJnQpEVc2q5YFYVNWWFaQ/5AadJKd5FxuqyQm5Z3cJ1F8zgnV9/ht7+CJUlBfQODFJeVMC9L+3lya1H+McPLeOdixsm3EdsMKxDXT186J9f4FOXz+ML1y6mrGh0DLlp4x6IWPLzzITfDvojEQryDQunV7BmcQPPbo/e4nr3b/dMesL42q/e4hd//DucPhe91nLnkzu4cfksYHQNPbaNk70DbD18kiWzhssROzm2d/cmPI5UU6CLBGxmdSkzl5UOzagzMBjhrfZTQwG/8UAnTzm3XAKUFObRWFVCY1UJM6pKmFFdwvTKYmZUR5/H3pus2cKrgaEa+sTbbKwq4e61l/Dx779CuRPCl7TU8tnfXchf3r+Ztfe8yqLpFVx7QSPXLpnBhc3VcU9MP/7DVdy3bj93P7+Hn6w7wFXnNXDdBTO46rwGKksKh5pE7CSt3f2DkaGy/v6lc/j1tiOjj2fQUuAk8MgxzL/33C56+6PNS9baceXbdKCLe17Yy7IRY6V/+ZfbgInb9P/y/s38+x9cOnRr7ECs91UGKNBF0qwgP4+lTdUsbaoemvWn+2w/mw52sePIKdq7ezly6hxHunvZeKCL9jd76RsYHxLTyouYXlVCY1UxZUX5FOTlUZBvKMqP/izMz3P+GQryoj8L8/MoyM+jKN9QkJ9HQZ6hqCCPLYe6nbJ5/2bQ2lLHo7dfyU/XHeCxLe1877ndfOeZXcyoKmHFnJpxy1eWFPDlDyzjgyua+dn6Azy59QiPbG6jMN9w6bw6Tp0bvhPpe8/tYlvbScqLCygvyqesqICK4gLO9UeGAnukoyd7eWxLOwMRO+4upLKifGrLizjT1zPhsbxzcQP/8MhWGquiQ0ucP6Ny6GQRy/7H32znp+uiQ0GvnFvLG4e6ef93nqd1bh1lRfmT3kkVNAW6SAhUlxWyZnEDa+I0WVhr6TrbT/vJXo44/9q7zw09P3qql97+CAODEfoHLf2DEQYi0Z/9gxEGBu1QDTyRmlJ/HbCKC/L55OoWPrm6hc4zfTz91lGe2NrOjiPRduR4LUgr59aycm4tX/mg5bX9nTzxZjsv7jo+apl/fmYnERu9Y+nMuQF6RoxhHm+wtvvWHeAbT+4Aoie8kWbVlPLDT1/Gqq8+NfTa2Ck4v/Pxi/mjH27gt28fA+Bv3ruE1/d3cueTO4aadx7edHjoG9W73tHIn117Hr//ry9z4MQhjPE+PG8qKdBFQs4YQ215EbXlRa7vKBnLWkv/oGUgMiL0B0eEfsRSXlzgekTLWOeZeGrLi/i9lc1DHb7e/Y3nWNRYMeHy+XmGS1rquMS5tfCHL+/jb36xZej9D7c28z9uvACItk/39A/S0zdIfcVwYMe2PeDcrfPsn13FnLrx1zNmVJfw9zddwN89+GbcshTkGT7/rkVDgW4M/Mk1i/jGr3ew1GknH9tMs3rBNJY2VbHl0EmuOb+Rb350OTd/96VJm4yCokAXmQKMMRQVGIpIrt090/fj5OcZKooLRrWLj639G0PcmbT8lD3ROvG+eVSWFAY2sFgiudPVTUTSznX3+RGVVdfD0KaogjtRGb1uPoMtKa4p0EUkUF5rqyOXDyJ0k62pj213DxMFuoikRRhjMH6ZvEd+WCa/UKCLiC/puujnLiyHyzLR0m6+KYxbJsE64xYf+e0iA2cwBbqIeBam2nbCC5dpKUU4KNBFxL2xd5S4XG3URdH0XhOdePsedxDipvMhCnQRCZTX9uVRywcRuj7uKRx5p0yYc12BLiK+eK/hhi8K4xXJzz3kIRgwE1Cgi0jIeb/tMf4Kbr4pjF0m0TrjOjWNeJyJ05cCXUQ8C1tle7LyZKL2nKkx7hXoIuLauBqs6wuc3s8A1rrsKOQzO72WKRNjs3ilQBeRQCXTU9QrN6E7ulnEXUiPKlK8dndXWwmeAl1EfPF8UdTnftyE5fEzffytM0JjsuGazPpZ0bHIGFNjjPm5MeYtY8w2Y8xqY0ydMeZJY8zbzs/aoAsrIjKR/3h536Tv++kpmmidiS6aZqrG7raG/i3gV9ba84HlwDbgDuApa+0i4CnnuYhMAcNNFX7u6U5tWRLuL727y6iEgW6MqQbWAHcDWGv7rLVdwE3Avc5i9wIfCKaIIhIW6Q5jN/euj6wle2rm8NlpKcwXR93U0OcBHcC/GWNeN8Z83xhTDjRaa9ucZdqBxngrG2NuNcasN8as7+joSE2pRSSrWIv3AA0oOI2Pm8UTnshC0rPITaAXABcD37XWrgDOMKZ5xdqJPy5r7V3W2lZrbWtDw/j5EkUkOwVVT51sBENX6yeZrV7uIR/fsSizXYvcBPpB4KC19hXn+c+JBvwRY8xMAOfn0WCKKCLin6eATvDc/T59rpikhIFurW0HDhhjznNeugbYCjwErHVeWws8GEgJRSR8nMqnv3FPwtE8kYvcThL9J8CPjDFFwG7gU0RPBj81xnwa2Ad8JJgiikhYpDuKvfYU9XpN1MvxxLYdd0AvD9sJkqtAt9ZuBFrjvHVNSksjIjkpepHNY1d7n03QXobrjbeLVHUsygT1FBURX9wOh+u1iWV8557UpGRsO14C3886MaHtKSoikirprsR62V/qTh4p2YxnCnQR8cx/P1GP+8nBaeKCpEAXEdfCeIdKMiXycwJI1SxHQVCgi0jgrE3f6IxuE36i8kwUzm7K7/eOm1RRoItIoLxWXsdPA5facnipTQ8tO8E6yUx3FwQFuoikVdqbJ7wEeHClSAsFuoh4ZpPoKeppPzk4TVyQFOgi4lpy08MFI5kLtb7mOo2zznDLTGbr+Ap0EUkDP5NEx74GeFvP7eIThflEoTxZkMddPgP3UCrQRcQXt3nluQKd4kpuLKBj5Zhs8+OHw/VXJHUsEpEpwdNYK14ruVO7CV2BLiL+ZbrNOBmrv/q0r/XidywKx+9BgS4invm6mJim2rOXbE15r091LBKRbOE365LpYp+qbwH+JuMYWaDUbDNICnQR8cVtLX38cLgJlvdZnlTsf8KenwHdaZNqCnQRCS1dE/VGgS4ivoWtySEdUj3LUSop0EXEM19t4gHVn8ffO57cFHRJlWXktjVjkYiE2ajhYT0EVjLZlqpvAf4urg6v46lmnqGvLgp0EQmU1+FwTYoHFR+7Pz/B7medl3Yf58mtRzyvlwwFuoj4FrY29EyMnwLxfw99AxH+8N/Xp7UcCnQRySneOhaltoNUpnuMKtBFxDM/9WC/lee0z4cRsm8dXijQRcSDyS8STiTVNeGRErVvD42ymERP0b7BCHf9Zje7O06PeX+iKegyQ4EuIoEal3keeop6nrHIzeI+0nYwEt3w3z34pveV00iBLiK+ZfNoi6kUlt+DAl1E0iJd9594iVZ/7foTr5TpWFegi4hn6bw9MHUdi9wuZ0Y8TrDsBAtoxiIRCb3RPUXdh/rIJd02T/gdzXHc+xmvN6ePAl1EApXMfJxevwgEdE00Qxv1ToEuIv6FJMhGCrpzz+QdiwLddUIKdBFJjwx1y/cq06GcDNeBbozJN8a8box5xHk+zxjzijFmpzHmJ8aYouCKKSJhks5oTlkbuI+kTlTbn+jdbOhYdDuwbcTzfwK+aa1dCHQCn05lwUQkfBJMsTmhkZVzt7lqrfc28ZRP+pxlXAW6MaYZeC/wfee5Aa4Gfu4sci/wgQDKJyLZzmOiBh3AQbSxh+Wc4baG/n+AvwAizvNpQJe1dsB5fhBoSm3RRCTswhJkIwXdsWiyVTL9+0gY6MaY9wFHrbUb/OzAGHOrMWa9MWZ9R0eHn02ISA7Ijkui2d1E46aGfjnwfmPMXuA+ok0t3wJqjDEFzjLNwKF4K1tr77LWtlprWxsaGlJQZBHJuCTSOZn70n3tLzbaop913W583MshnYLOWvtFa22ztbYF+BjwtLX248AzwM3OYmuBBwMrpYiEgt/p4XyNn4673qiji5Qt3wOCkcx96H8J/HdjzE6ibep3p6ZIIpJLPNfIk22J9jA8b6oMj7me2faagsSLDLPWPgs86zzeDVya+iKJSLbwEmBh7Ffkp0afqXlL3VBPUREJtbRPQedhj9ncsUhEJGmeavWp2aOzXx9rZtkdLwp0EfHMemys8DenqNvhc0fMc5qh1pCwDNGrQBcR1zxODxpdxmPWJX2bYsDbT2bfQVOgi0hahPFiop+xX8J3FMMU6CISbimqUvvrWJRgtEVNQSciU1nwkziP2V+m20HSSIEuIp5ZG54mlDDk9dBJQzMWiUi2GFvbDbL26/d0kXBSiiTLHJLzWFwKdBEJVCw/w5iD8coUhhq/Xwp0EUkrz7cxJru/of36mYIu0bYnGG0xQ6cFBbqI+BLKGneG2kPCcuFVgS4inoWqHTmN7fqJZLrHqAJdRFwbG1hu48vXVG8BnTRGHoOvIQlSWZgUU6CLSKBibddus3NsW3e6a9wp2Z86FonIVOC+WSI1tedkpqBLJCxt5zEKdBHxJVTt6BmW6bbzGAW6iHgW5izPZLRmusauQBcR18b3FHWXYF7HTwcPbe4eI9znPNfD64T4q4kCXUQC5X2S6LHPUzTaouvNDC+YuGORt9eDpkAXkfRymXb+as/xdhd8vGa6qSVGgS4ivnhvREkPP138U7bvjO05SoEuIp6FuR15KlOgi4hvXnqKej0JuO+I5GmzCfcRluYTPxToIhIor2Oox96PNel4Cdh4zUDDHYt8jLaoKehERCQTFOgi4ktYm9HVsUhExIOhLM/i9uYoH+PFhPREBgp0EfHAbw00yOFzkzmnxL9vPXmasUhEcpLXMdRjy8fC1ks0ThrQsYujHs5KfqegyxQFuojklEy0Y8dOEpkOeAW6iPgS5rbkqUqBLiK+eamRhvEE4Gu8mJAOeQAKdBHxwWs4+wlBtx2L3DaxxFssfk9R96MtTnQ+C23HImPMbGPMM8aYrcaYN40xtzuv1xljnjTGvO38rA2+uCKSSb7aiD2OoT7UU9TPnTFxt+dvYmsvwnJp1E0NfQD4grV2CbAK+KwxZglwB/CUtXYR8JTzXEQkwzw0A6W4+ST0HYustW3W2tecx6eAbUATcBNwr7PYvcAHAiqjiIi44KkN3RjTAqwAXgEarbVtzlvtQOME69xqjFlvjFnf0dGRTFlFJGSSHTgr0/x0LHLTFBTaNvQYY0wFcD/weWvtyZHv2ei4mHEP01p7l7W21Vrb2tDQkFRhRSQs/A+F63lKukRt7iO2OFnYxttMonBOONri5KunnatAN8YUEg3zH1lrH3BePmKMmem8PxM4GkwRRSQs/NQ8/c4p6vc2x1TdFeNnn6FvQzfR0+PdwDZr7TdGvPUQsNZ5vBZ4MPXFExERtwpcLHM58EngDWPMRue1vwK+BvzUGPNpYB/wkUBKKCKhpGnowidhoFtrn2fib03XpLY4IpJNkh04K9Mmm+FownXcXBTVaIsiki289xQd5rad2e1dMaO3520KuoQXRRO2x8cG5QoHBbqIuObroqjHdZLpKQruh+cNhkZbFJEsFMIWlClPgS4iaRHGE4C/8WLCeCRRCnQR8S3I+65jsempN2oKstZLk8xES4a+p6iISIzn3BzVU9Rd2qXqtsjY/oLsFDV0cTTsHYtERGL8XFD0vo635ccu7aWnaJibT/xQoIuIL2G8r3yqU6CLSFqE8QTg66JoCI8jRoEuIr75aoJx3bHI+z7iTyk38X7jz3AU/3E8w52WwkGBLiKepWVOUd8dizIXr5kOdgW6iLiWzp6iQa2Q6dANkgJdRHzJtTtEcoECXUR8C3YKuuBPGH7udXezRqJZloKiQBeRUEv2pGGGfsYZbTHx3id/NyQzFcUo0EUkcOm8PTCT4ZrpYFegi4hnXppPPF8U9ViWZK6hhvmecj8U6CLi2sjwzLUwdC3Ex61AFxHfghwJ0U9uej/JeJ+Czo1Mtbwo0EUkrdLdBDNpT9Fkp6Abccl19PPMUKCLiOQIBbqIeBam5hPvTSQhuccwAAp0EXFt9FjiLtfxOr55Gu/983WicXP3umYsEpHsE9yEF7FenN46FsXZ3yQbiD864/DyiXatjkUiIgHKVLf76L4ztmtAgS4ikjMU6CLimedhtgIaBAv8tNGP2EeO9Y5SoIuIB8Np6PUuFLfhGdtDOsZ/idvmnoJ9qGORiGSdtEx4kWQ8DnX98VFYt+uYMT8zRYEuIpIjFOgiMmXlWBO6Al1EvPN6MTGoDjwQ/Ngw2USBLiKujQ5Pj6HutSu/n4uiXsvkY7TF+J2Xxj7XFHQikmXcxtZgZDgG3a4TW8NzDXzccIuT7DfRaIsJ9xVdInayihfkJ3v72d1xmv7BSIKtJS+pQDfGvMcYs90Ys9MYc0eqCiUi4Wat+xr3b98+xuaD3ezqOO1pH7ff97qPknmzrf3UuNcmO67D3T1s2Nc57vUnth6ZcJ1//c1urr7zOXZ3nPFVRi98B7oxJh/4DnA9sAT4fWPMklQVTETC6/M/2cjxM308/ma763V+sfGwq+ViNdljp/uc55OfOXYeHT5RPPDaIQ519Yze3kB0/a6zfePW/YdHto577Wzf4NDjsXseG8qRyOglfr7hIDD6G8m3n94JwIzqkokOIWWSqaFfCuy01u621vYB9wE3paZYIhJGhXmjIyMWupNpqin1tI+Z1aOX74wTxCNd2Fw96vnYGvbmg10AbDrYDUB1aeGk26srLxp63N3TP+myv9nRMer56XMDAKzfe2LcslUlBZNuKxWSCfQm4MCI5wed10YxxtxqjFlvjFnf0dEx9m0RySKz60q55vzpQ89/9JnLEq7zq89fybuXNGIM3Pa7CynInzx2ls+u4SsfXEpdeREXza7hI62zJ13+o5fMZu60sqHnn1g1Z9T7/3TzhQD888cvHtr+SEUF0fJ85op5QDTQP3f1QhY3VrCsKXqy+OqHlo1aJ/b6HTecD8DTX3gnRQV53P9HqwH47idWjlr+c9csSsuFUuN3LANjzM3Ae6y1n3GefxK4zFp720TrtLa22vXr1/van4jIVGWM2WCtbU20XDI19EPAyFNns/OaiIhkQDKBvg5YZIyZZ4wpAj4GPJSaYomIiFe+W+mttQPGmNuAx4F84B5r7ZspK5mIiHiS1GVXa+2jwKMpKouIiCRBPUVFRHKEAl1EJEco0EVEcoQCXUQkR/juWORrZ8Z0APt8rl4PHEthcbKFjntqmarHDVP32N0c91xrbUOiDaU10JNhjFnvpqdUrtFxTy1T9bhh6h57Ko9bTS4iIjlCgS4ikiOyKdDvynQBMkTHPbVM1eOGqXvsKTvurGlDFxGRyWVTDV1ERCahQBcRyRFZEei5Phm1MWavMeYNY8xGY8x657U6Y8yTxpi3nZ+1zuvGGPN/nd/FZmPMxZktvXvGmHuMMUeNMVtGvOb5OI0xa53l3zbGrM3EsXgxwXF/yRhzyPnMNxpjbhjx3hed495ujLluxOtZ9XdgjJltjHnGGLPVGPOmMeZ25/Wc/swnOe7gP3Nrbaj/ER2adxcwHygCNgFLMl2uFB/jXqB+zGv/C7jDeXwH8E/O4xuAxwADrAJeyXT5PRznGuBiYIvf4wTqgN3Oz1rncW2mj83HcX8J+LM4yy5x/o8XA/Oc//v52fh3AMwELnYeVwI7nOPL6c98kuMO/DPPhhr6VJ2M+ibgXufxvcAHRrz+7zbqZaDGGDMzA+XzzFr7G2Ds7Llej/M64Elr7QlrbSfwJPCewAufhAmOeyI3AfdZa89Za/cAO4n+DWTd34G1ts1a+5rz+BSwjei8wzn9mU9y3BNJ2WeeDYHuajLqLGeBJ4wxG4wxtzqvNVpr25zH7UCj8zjXfh9ejzOXjv82p2nhnlizAzl63MaYFmAF8ApT6DMfc9wQ8GeeDYE+FVxhrb0YuB74rDFmzcg3bfR7Wc7fXzpVjtPxXWABcBHQBtyZ0dIEyBhTAdwPfN5ae3Lke7n8mcc57sA/82wI9JyfjNpae8j5eRT4T6JftY7EmlKcn0edxXPt9+H1OHPi+K21R6y1g9baCPCvRD9zyLHjNsYUEg21H1lrH3BezvnPPN5xp+Mzz4ZAz+nJqI0x5caYythj4FpgC9FjjF3NXws86Dx+CLjFuSNgFdA94utrNvJ6nI8D1xpjap2vrNc6r2WVMdc9Pkj0M4focX/MGFNsjJkHLAJeJQv/DowxBrgb2Gat/caIt3L6M5/ouNPymWf6irDLq8Y3EL1SvAv460yXJ8XHNp/o1etNwJux4wOmAU8BbwO/Buqc1w3wHed38QbQmulj8HCsPyb6VbOfaHvgp/0cJ/AHRC8c7QQ+lenj8nnc/+Ec12bnj3TmiOX/2jnu7cD1I17Pqr8D4AqizSmbgY3Ovxty/TOf5LgD/8zV9V9EJEdkQ5OLiIi4oEAXEckRCnQRkRyhQBcRyREKdBGRHKFAFxHJEQp0EZEc8f8Bmsrw8cD5npUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 89.3180  # Value for C0\n",
    "K0 = -0.0010  # Value for K0\n",
    "K1 = 0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = -0.0204    # Value for b\n",
    "c = 2.2194    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-175)  var1(t-174)  var1(t-173)  var1(t-172)  var1(t-171)  \\\n",
      "175    92.600000    92.387115    92.174230    91.961345    91.748459   \n",
      "176    92.387115    92.174230    91.961345    91.748459    91.535574   \n",
      "177    92.174230    91.961345    91.748459    91.535574    91.322689   \n",
      "178    91.961345    91.748459    91.535574    91.322689    91.109804   \n",
      "179    91.748459    91.535574    91.322689    91.109804    90.896919   \n",
      "\n",
      "     var1(t-170)  var1(t-169)  var1(t-168)  var1(t-167)  var1(t-166)  ...  \\\n",
      "175    91.535574    91.322689    91.109804    90.896919    90.691597  ...   \n",
      "176    91.322689    91.109804    90.896919    90.691597    90.579552  ...   \n",
      "177    91.109804    90.896919    90.691597    90.579552    90.467507  ...   \n",
      "178    90.896919    90.691597    90.579552    90.467507    90.355462  ...   \n",
      "179    90.691597    90.579552    90.467507    90.355462    90.243417  ...   \n",
      "\n",
      "     var1(t+45)  var2(t+45)  var1(t+46)  var2(t+46)  var1(t+47)  var2(t+47)  \\\n",
      "175   84.280019    0.000280   84.265079    0.000280   84.250140    0.000280   \n",
      "176   84.265079    0.000280   84.250140    0.000280   84.235201    0.000280   \n",
      "177   84.250140    0.000280   84.235201    0.000280   84.220261    0.000279   \n",
      "178   84.235201    0.000280   84.220261    0.000279   84.205322    0.000279   \n",
      "179   84.220261    0.000279   84.205322    0.000279   84.190383    0.000279   \n",
      "\n",
      "     var1(t+48)  var2(t+48)  var1(t+49)  var2(t+49)  \n",
      "175   84.235201    0.000280   84.220261    0.000279  \n",
      "176   84.220261    0.000279   84.205322    0.000279  \n",
      "177   84.205322    0.000279   84.190383    0.000279  \n",
      "178   84.190383    0.000279   84.175444    0.000279  \n",
      "179   84.175444    0.000279   84.160504    0.000279  \n",
      "\n",
      "[5 rows x 276 columns]\n",
      "Index(['var1(t-175)', 'var1(t-174)', 'var1(t-173)', 'var1(t-172)',\n",
      "       'var1(t-171)', 'var1(t-170)', 'var1(t-169)', 'var1(t-168)',\n",
      "       'var1(t-167)', 'var1(t-166)',\n",
      "       ...\n",
      "       'var1(t+45)', 'var2(t+45)', 'var1(t+46)', 'var2(t+46)', 'var1(t+47)',\n",
      "       'var2(t+47)', 'var1(t+48)', 'var2(t+48)', 'var1(t+49)', 'var2(t+49)'],\n",
      "      dtype='object', length=276)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 175, n_out = 50)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 176):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-150:]\n",
    "trainX = train[:,:-150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-150:]\n",
    "forecastX = forecast[:,:-150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225, 1, 126) (2225, 150) (1, 1, 126)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "28/28 [==============================] - 2s 18ms/step - loss: 5362.1040 - val_loss: 3218.3630\n",
      "Epoch 2/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 5153.1172 - val_loss: 3052.8608\n",
      "Epoch 3/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4973.6123 - val_loss: 2950.2556\n",
      "Epoch 4/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4842.5371 - val_loss: 2873.0405\n",
      "Epoch 5/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4721.2065 - val_loss: 2799.9724\n",
      "Epoch 6/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4604.7124 - val_loss: 2729.8450\n",
      "Epoch 7/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 4482.2583 - val_loss: 2653.6758\n",
      "Epoch 8/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 4363.7544 - val_loss: 2583.5078\n",
      "Epoch 9/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4251.3584 - val_loss: 2517.2246\n",
      "Epoch 10/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4142.4302 - val_loss: 2453.2354\n",
      "Epoch 11/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4036.3992 - val_loss: 2391.2366\n",
      "Epoch 12/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3932.9744 - val_loss: 2331.3093\n",
      "Epoch 13/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3831.9546 - val_loss: 2273.2095\n",
      "Epoch 14/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 3733.2068 - val_loss: 2216.8523\n",
      "Epoch 15/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3636.6313 - val_loss: 2162.1731\n",
      "Epoch 16/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3542.1494 - val_loss: 2109.1199\n",
      "Epoch 17/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3449.6938 - val_loss: 2057.6470\n",
      "Epoch 18/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 3359.2090 - val_loss: 2007.7133\n",
      "Epoch 19/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3270.6438 - val_loss: 1959.2828\n",
      "Epoch 20/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 3183.9551 - val_loss: 1912.3212\n",
      "Epoch 21/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 3099.1008 - val_loss: 1866.7966\n",
      "Epoch 22/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3016.0442 - val_loss: 1822.6799\n",
      "Epoch 23/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2934.7498 - val_loss: 1779.9420\n",
      "Epoch 24/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2855.1846 - val_loss: 1738.5568\n",
      "Epoch 25/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2777.3174 - val_loss: 1698.4980\n",
      "Epoch 26/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2701.1182 - val_loss: 1659.7399\n",
      "Epoch 27/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2626.5591 - val_loss: 1622.2592\n",
      "Epoch 28/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2553.6106 - val_loss: 1586.0311\n",
      "Epoch 29/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2482.2478 - val_loss: 1551.0339\n",
      "Epoch 30/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2412.4434 - val_loss: 1517.2438\n",
      "Epoch 31/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2344.1731 - val_loss: 1484.6396\n",
      "Epoch 32/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2277.4114 - val_loss: 1453.1987\n",
      "Epoch 33/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2212.1353 - val_loss: 1422.9004\n",
      "Epoch 34/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2148.3206 - val_loss: 1393.7241\n",
      "Epoch 35/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2085.9438 - val_loss: 1365.6482\n",
      "Epoch 36/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2024.9834 - val_loss: 1338.6528\n",
      "Epoch 37/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1965.4167 - val_loss: 1312.7174\n",
      "Epoch 38/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1907.2214 - val_loss: 1287.8199\n",
      "Epoch 39/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1850.3766 - val_loss: 1263.9294\n",
      "Epoch 40/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1794.8613 - val_loss: 1240.8068\n",
      "Epoch 41/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1740.6537 - val_loss: 1219.3556\n",
      "Epoch 42/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1687.7335 - val_loss: 1198.4377\n",
      "Epoch 43/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 1636.0806 - val_loss: 1178.4658\n",
      "Epoch 44/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1585.6748 - val_loss: 1159.4204\n",
      "Epoch 45/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1536.4963 - val_loss: 1141.2838\n",
      "Epoch 46/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1488.5259 - val_loss: 1124.0381\n",
      "Epoch 47/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1441.7438 - val_loss: 1107.6644\n",
      "Epoch 48/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1396.1305 - val_loss: 1092.1453\n",
      "Epoch 49/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1351.6677 - val_loss: 1077.4633\n",
      "Epoch 50/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1308.3359 - val_loss: 1063.6003\n",
      "Epoch 51/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1266.1166 - val_loss: 1050.5396\n",
      "Epoch 52/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1224.9919 - val_loss: 1038.2634\n",
      "Epoch 53/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1184.9432 - val_loss: 1026.7566\n",
      "Epoch 54/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1145.9524 - val_loss: 1015.9633\n",
      "Epoch 55/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1108.0027 - val_loss: 1005.9357\n",
      "Epoch 56/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1071.0732 - val_loss: 996.6251\n",
      "Epoch 57/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1035.1493 - val_loss: 988.0142\n",
      "Epoch 58/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1000.2122 - val_loss: 980.0857\n",
      "Epoch 59/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 966.2449 - val_loss: 972.8231\n",
      "Epoch 60/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 933.2303 - val_loss: 966.2098\n",
      "Epoch 61/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 901.1511 - val_loss: 960.2304\n",
      "Epoch 62/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 869.9903 - val_loss: 954.8675\n",
      "Epoch 63/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 839.7314 - val_loss: 950.1051\n",
      "Epoch 64/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 810.3574 - val_loss: 945.9281\n",
      "Epoch 65/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 781.8520 - val_loss: 942.3190\n",
      "Epoch 66/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 754.1985 - val_loss: 939.2631\n",
      "Epoch 67/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 727.3806 - val_loss: 936.7424\n",
      "Epoch 68/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 701.3824 - val_loss: 934.4490\n",
      "Epoch 69/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 668.4915 - val_loss: 932.8997\n",
      "Epoch 70/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 639.1801 - val_loss: 932.1406\n",
      "Epoch 71/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 612.3589 - val_loss: 932.0073\n",
      "Epoch 72/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 587.2087 - val_loss: 932.4296\n",
      "Epoch 73/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 563.3718 - val_loss: 933.3692\n",
      "Epoch 74/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 540.6626 - val_loss: 934.7928\n",
      "Epoch 75/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 518.9674 - val_loss: 936.6719\n",
      "Epoch 76/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 498.2074 - val_loss: 938.9828\n",
      "Epoch 77/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 478.3227 - val_loss: 941.7037\n",
      "Epoch 78/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 459.2655 - val_loss: 944.8125\n",
      "Epoch 79/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 440.9956 - val_loss: 948.2877\n",
      "Epoch 80/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 423.4779 - val_loss: 952.1134\n",
      "Epoch 81/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 406.6815 - val_loss: 956.2683\n",
      "Epoch 82/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 390.5781 - val_loss: 960.7357\n",
      "Epoch 83/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 375.1418 - val_loss: 965.4988\n",
      "Epoch 84/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 360.3484 - val_loss: 970.5419\n",
      "Epoch 85/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 346.1752 - val_loss: 975.8488\n",
      "Epoch 86/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 332.6008 - val_loss: 981.4039\n",
      "Epoch 87/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 319.6047 - val_loss: 987.1903\n",
      "Epoch 88/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 307.1671 - val_loss: 993.1923\n",
      "Epoch 89/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 295.2693 - val_loss: 999.3907\n",
      "Epoch 90/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 283.8929 - val_loss: 1005.7751\n",
      "Epoch 91/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 273.0206 - val_loss: 1012.3317\n",
      "Epoch 92/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 262.6350 - val_loss: 1019.0463\n",
      "Epoch 93/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 252.7197 - val_loss: 1025.9058\n",
      "Epoch 94/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 243.2586 - val_loss: 1032.8954\n",
      "Epoch 95/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 234.2360 - val_loss: 1040.0031\n",
      "Epoch 96/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 225.6364 - val_loss: 1047.2158\n",
      "Epoch 97/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 217.4452 - val_loss: 1054.5212\n",
      "Epoch 98/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 209.6476 - val_loss: 1061.9066\n",
      "Epoch 99/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 202.2294 - val_loss: 1069.3604\n",
      "Epoch 100/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 195.1770 - val_loss: 1076.8708\n",
      "Epoch 101/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 188.4766 - val_loss: 1084.4266\n",
      "Epoch 102/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 182.1149 - val_loss: 1092.0168\n",
      "Epoch 103/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 176.0793 - val_loss: 1099.6309\n",
      "Epoch 104/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 170.3569 - val_loss: 1107.2581\n",
      "Epoch 105/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 164.9356 - val_loss: 1114.8882\n",
      "Epoch 106/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 159.8033 - val_loss: 1122.5120\n",
      "Epoch 107/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 154.9481 - val_loss: 1130.1194\n",
      "Epoch 108/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 150.3589 - val_loss: 1137.7019\n",
      "Epoch 109/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 146.0242 - val_loss: 1145.2502\n",
      "Epoch 110/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 141.9336 - val_loss: 1152.7566\n",
      "Epoch 111/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 138.0760 - val_loss: 1160.2122\n",
      "Epoch 112/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 134.4417 - val_loss: 1167.6095\n",
      "Epoch 113/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 131.0201 - val_loss: 1174.9415\n",
      "Epoch 114/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 127.8019 - val_loss: 1182.2006\n",
      "Epoch 115/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 124.7775 - val_loss: 1189.3799\n",
      "Epoch 116/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 121.9377 - val_loss: 1196.4739\n",
      "Epoch 117/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 119.2737 - val_loss: 1203.4756\n",
      "Epoch 118/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 116.7768 - val_loss: 1210.3800\n",
      "Epoch 119/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 114.4388 - val_loss: 1217.1820\n",
      "Epoch 120/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 112.2515 - val_loss: 1223.8756\n",
      "Epoch 121/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 110.2073 - val_loss: 1230.4570\n",
      "Epoch 122/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 108.2985 - val_loss: 1236.9210\n",
      "Epoch 123/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 106.5179 - val_loss: 1243.2650\n",
      "Epoch 124/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 104.8587 - val_loss: 1249.4847\n",
      "Epoch 125/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 103.3137 - val_loss: 1255.5770\n",
      "Epoch 126/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 101.8769 - val_loss: 1261.5385\n",
      "Epoch 127/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 100.5419 - val_loss: 1267.3669\n",
      "Epoch 128/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 99.3029 - val_loss: 1273.0603\n",
      "Epoch 129/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 98.1540 - val_loss: 1278.6163\n",
      "Epoch 130/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 97.0900 - val_loss: 1284.0330\n",
      "Epoch 131/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 96.1055 - val_loss: 1289.3096\n",
      "Epoch 132/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 95.1955 - val_loss: 1294.4453\n",
      "Epoch 133/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 94.3552 - val_loss: 1299.4391\n",
      "Epoch 134/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 93.5802 - val_loss: 1304.2899\n",
      "Epoch 135/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 92.8663 - val_loss: 1308.9984\n",
      "Epoch 136/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 92.2093 - val_loss: 1313.5642\n",
      "Epoch 137/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 91.6053 - val_loss: 1317.9871\n",
      "Epoch 138/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 91.0506 - val_loss: 1322.2693\n",
      "Epoch 139/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 90.5418 - val_loss: 1326.4104\n",
      "Epoch 140/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 90.0755 - val_loss: 1330.4113\n",
      "Epoch 141/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 89.6488 - val_loss: 1334.2729\n",
      "Epoch 142/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 89.2588 - val_loss: 1337.9980\n",
      "Epoch 143/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 88.9025 - val_loss: 1341.5880\n",
      "Epoch 144/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 88.5774 - val_loss: 1345.0437\n",
      "Epoch 145/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 88.2813 - val_loss: 1348.3674\n",
      "Epoch 146/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 88.0117 - val_loss: 1351.5615\n",
      "Epoch 147/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 87.7666 - val_loss: 1354.6289\n",
      "Epoch 148/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 87.5440 - val_loss: 1357.5703\n",
      "Epoch 149/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 87.3421 - val_loss: 1360.3892\n",
      "Epoch 150/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 87.1591 - val_loss: 1363.0885\n",
      "Epoch 151/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 86.9933 - val_loss: 1365.6708\n",
      "Epoch 152/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 86.8435 - val_loss: 1368.1382\n",
      "Epoch 153/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 86.7081 - val_loss: 1370.4944\n",
      "Epoch 154/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 86.5860 - val_loss: 1372.7416\n",
      "Epoch 155/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 86.4758 - val_loss: 1374.8832\n",
      "Epoch 156/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 86.3767 - val_loss: 1376.9222\n",
      "Epoch 157/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 86.2874 - val_loss: 1378.8621\n",
      "Epoch 158/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 86.2072 - val_loss: 1380.7045\n",
      "Epoch 159/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 86.1352 - val_loss: 1382.4545\n",
      "Epoch 160/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 86.0706 - val_loss: 1384.1134\n",
      "Epoch 161/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 86.0127 - val_loss: 1385.6853\n",
      "Epoch 162/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.9609 - val_loss: 1387.1727\n",
      "Epoch 163/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.9146 - val_loss: 1388.5785\n",
      "Epoch 164/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.8732 - val_loss: 1389.9067\n",
      "Epoch 165/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.8363 - val_loss: 1391.1595\n",
      "Epoch 166/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.8032 - val_loss: 1392.3401\n",
      "Epoch 167/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.7739 - val_loss: 1393.4503\n",
      "Epoch 168/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.7477 - val_loss: 1394.4943\n",
      "Epoch 169/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.7246 - val_loss: 1395.4735\n",
      "Epoch 170/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.7039 - val_loss: 1396.3915\n",
      "Epoch 171/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6857 - val_loss: 1397.2511\n",
      "Epoch 172/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6694 - val_loss: 1398.0531\n",
      "Epoch 173/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6551 - val_loss: 1398.8020\n",
      "Epoch 174/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6424 - val_loss: 1399.4990\n",
      "Epoch 175/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6313 - val_loss: 1400.1464\n",
      "Epoch 176/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6214 - val_loss: 1400.7451\n",
      "Epoch 177/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 85.6128 - val_loss: 1401.2983\n",
      "Epoch 178/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6053 - val_loss: 1401.8069\n",
      "Epoch 179/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 85.5987 - val_loss: 1402.2717\n",
      "Epoch 180/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.5929 - val_loss: 1402.6943\n",
      "Epoch 181/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.5880 - val_loss: 1403.0754\n",
      "Epoch 182/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.5837 - val_loss: 1403.4130\n",
      "Epoch 183/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 85.5800 - val_loss: 1403.7074\n",
      "Epoch 184/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.5768 - val_loss: 1403.9535\n",
      "Epoch 185/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 85.5741 - val_loss: 1404.1447\n",
      "Epoch 186/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.5719 - val_loss: 1404.2676\n",
      "Epoch 187/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.5700 - val_loss: 1404.2913\n",
      "Epoch 188/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.5686 - val_loss: 1404.1476\n",
      "Epoch 189/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.5673 - val_loss: 1403.6328\n",
      "Epoch 190/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.5659 - val_loss: 1401.4409\n",
      "Epoch 191/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.4606 - val_loss: 1374.9017\n",
      "Epoch 192/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 84.7649 - val_loss: 1299.8822\n",
      "Epoch 193/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 99.8889 - val_loss: 1282.4330\n",
      "Epoch 194/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 98.1351 - val_loss: 1292.7432\n",
      "Epoch 195/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 96.1937 - val_loss: 1293.1331\n",
      "Epoch 196/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 94.7542 - val_loss: 1313.3710\n",
      "Epoch 197/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 93.1594 - val_loss: 1322.1714\n",
      "Epoch 198/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 91.9803 - val_loss: 1330.3446\n",
      "Epoch 199/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 90.9848 - val_loss: 1337.9266\n",
      "Epoch 200/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 90.1444 - val_loss: 1344.9574\n",
      "Epoch 201/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 89.4315 - val_loss: 1351.4208\n",
      "Epoch 202/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 88.7472 - val_loss: 1349.4614\n",
      "Epoch 203/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 89.0379 - val_loss: 1306.5978\n",
      "Epoch 204/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 96.6762 - val_loss: 1303.6840\n",
      "Epoch 205/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 95.1311 - val_loss: 1313.6427\n",
      "Epoch 206/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 93.6158 - val_loss: 1322.9760\n",
      "Epoch 207/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 92.3315 - val_loss: 1331.6243\n",
      "Epoch 208/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 91.2528 - val_loss: 1339.6183\n",
      "Epoch 209/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 90.3477 - val_loss: 1346.9935\n",
      "Epoch 210/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 89.5885 - val_loss: 1353.7834\n",
      "Epoch 211/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 88.9518 - val_loss: 1360.0188\n",
      "Epoch 212/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 88.4175 - val_loss: 1365.6919\n",
      "Epoch 213/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 87.9135 - val_loss: 1364.8854\n",
      "Epoch 214/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 89.2304 - val_loss: 1376.5468\n",
      "Epoch 215/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 87.2673 - val_loss: 1381.0178\n",
      "Epoch 216/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 87.0048 - val_loss: 1385.0972\n",
      "Epoch 217/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 86.7859 - val_loss: 1388.8264\n",
      "Epoch 218/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 86.6024 - val_loss: 1392.2334\n",
      "Epoch 219/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 86.4485 - val_loss: 1395.3416\n",
      "Epoch 220/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 86.3192 - val_loss: 1398.1755\n",
      "Epoch 221/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 86.2107 - val_loss: 1400.7565\n",
      "Epoch 222/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 86.1194 - val_loss: 1403.1038\n",
      "Epoch 223/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 86.0427 - val_loss: 1405.2379\n",
      "Epoch 224/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.9780 - val_loss: 1407.1755\n",
      "Epoch 225/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.9237 - val_loss: 1408.9326\n",
      "Epoch 226/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.8777 - val_loss: 1410.5245\n",
      "Epoch 227/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.8387 - val_loss: 1411.9364\n",
      "Epoch 228/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.7924 - val_loss: 1410.8710\n",
      "Epoch 229/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.9923 - val_loss: 1412.0389\n",
      "Epoch 230/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.8821 - val_loss: 1413.6138\n",
      "Epoch 231/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 85.8297 - val_loss: 1414.9534\n",
      "Epoch 232/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.7945 - val_loss: 1416.1265\n",
      "Epoch 233/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.7674 - val_loss: 1417.1626\n",
      "Epoch 234/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.7456 - val_loss: 1418.0806\n",
      "Epoch 235/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.7278 - val_loss: 1418.8961\n",
      "Epoch 236/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.7131 - val_loss: 1419.6212\n",
      "Epoch 237/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.7006 - val_loss: 1420.2634\n",
      "Epoch 238/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6892 - val_loss: 1420.8154\n",
      "Epoch 239/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6768 - val_loss: 1420.9481\n",
      "Epoch 240/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6680 - val_loss: 1421.2007\n",
      "Epoch 241/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.2668 - val_loss: 1415.8584\n",
      "Epoch 242/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.5482 - val_loss: 1395.6127\n",
      "Epoch 243/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 90.7682 - val_loss: 1340.8539\n",
      "Epoch 244/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 89.9898 - val_loss: 1349.8649\n",
      "Epoch 245/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 89.1280 - val_loss: 1357.9551\n",
      "Epoch 246/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 88.4394 - val_loss: 1365.1218\n",
      "Epoch 247/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 87.8960 - val_loss: 1371.4761\n",
      "Epoch 248/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 87.4653 - val_loss: 1377.1135\n",
      "Epoch 249/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 87.1225 - val_loss: 1382.1179\n",
      "Epoch 250/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 86.8487 - val_loss: 1386.5609\n",
      "Epoch 251/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 86.6291 - val_loss: 1390.5048\n",
      "Epoch 252/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 86.4525 - val_loss: 1394.0057\n",
      "Epoch 253/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 86.3100 - val_loss: 1397.1122\n",
      "Epoch 254/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 86.1946 - val_loss: 1399.8691\n",
      "Epoch 255/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 86.1009 - val_loss: 1402.3141\n",
      "Epoch 256/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 86.0245 - val_loss: 1404.4817\n",
      "Epoch 257/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.9621 - val_loss: 1406.4038\n",
      "Epoch 258/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.9108 - val_loss: 1408.1064\n",
      "Epoch 259/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.8686 - val_loss: 1409.6147\n",
      "Epoch 260/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.8338 - val_loss: 1410.9509\n",
      "Epoch 261/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.8050 - val_loss: 1412.1343\n",
      "Epoch 262/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 85.7810 - val_loss: 1413.1809\n",
      "Epoch 263/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.7610 - val_loss: 1414.1079\n",
      "Epoch 264/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.7443 - val_loss: 1414.9271\n",
      "Epoch 265/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.7302 - val_loss: 1415.6514\n",
      "Epoch 266/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.7184 - val_loss: 1416.2911\n",
      "Epoch 267/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.7084 - val_loss: 1416.8571\n",
      "Epoch 268/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.7000 - val_loss: 1417.3566\n",
      "Epoch 269/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6929 - val_loss: 1417.7983\n",
      "Epoch 270/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6869 - val_loss: 1418.1876\n",
      "Epoch 271/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6817 - val_loss: 1418.5311\n",
      "Epoch 272/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6773 - val_loss: 1418.8346\n",
      "Epoch 273/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6736 - val_loss: 1419.1025\n",
      "Epoch 274/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6705 - val_loss: 1419.3376\n",
      "Epoch 275/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6678 - val_loss: 1419.5459\n",
      "Epoch 276/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6655 - val_loss: 1419.7291\n",
      "Epoch 277/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6635 - val_loss: 1419.8899\n",
      "Epoch 278/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6619 - val_loss: 1420.0308\n",
      "Epoch 279/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6605 - val_loss: 1420.1554\n",
      "Epoch 280/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 85.6593 - val_loss: 1420.2642\n",
      "Epoch 281/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6583 - val_loss: 1420.3599\n",
      "Epoch 282/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6574 - val_loss: 1420.4427\n",
      "Epoch 283/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6568 - val_loss: 1420.5149\n",
      "Epoch 284/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6563 - val_loss: 1420.5781\n",
      "Epoch 285/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6558 - val_loss: 1420.6324\n",
      "Epoch 286/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6555 - val_loss: 1420.6790\n",
      "Epoch 287/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6552 - val_loss: 1420.7177\n",
      "Epoch 288/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6550 - val_loss: 1420.7479\n",
      "Epoch 289/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6548 - val_loss: 1420.7388\n",
      "Epoch 290/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.5654 - val_loss: 1399.6855\n",
      "Epoch 291/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 89.7408 - val_loss: 1354.0951\n",
      "Epoch 292/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 89.1972 - val_loss: 1362.1338\n",
      "Epoch 293/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 88.5129 - val_loss: 1369.4438\n",
      "Epoch 294/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 87.9556 - val_loss: 1375.9565\n",
      "Epoch 295/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 87.5117 - val_loss: 1381.7469\n",
      "Epoch 296/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 87.1581 - val_loss: 1386.8898\n",
      "Epoch 297/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 86.8756 - val_loss: 1391.4540\n",
      "Epoch 298/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 86.6495 - val_loss: 1395.5012\n",
      "Epoch 299/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 86.4680 - val_loss: 1399.0894\n",
      "Epoch 300/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 86.3219 - val_loss: 1402.2676\n",
      "Epoch 301/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 86.2039 - val_loss: 1405.0806\n",
      "Epoch 302/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 86.1083 - val_loss: 1407.5697\n",
      "Epoch 303/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 86.0306 - val_loss: 1409.7709\n",
      "Epoch 304/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.9673 - val_loss: 1411.7172\n",
      "Epoch 305/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.9155 - val_loss: 1413.4373\n",
      "Epoch 306/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.8728 - val_loss: 1414.9570\n",
      "Epoch 307/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.8378 - val_loss: 1416.2988\n",
      "Epoch 308/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.8088 - val_loss: 1417.4838\n",
      "Epoch 309/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.7846 - val_loss: 1418.5300\n",
      "Epoch 310/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.7644 - val_loss: 1419.4521\n",
      "Epoch 311/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.7477 - val_loss: 1420.2659\n",
      "Epoch 312/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.7335 - val_loss: 1420.9823\n",
      "Epoch 313/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.7216 - val_loss: 1421.6134\n",
      "Epoch 314/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 85.7116 - val_loss: 1422.1680\n",
      "Epoch 315/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.7030 - val_loss: 1422.6526\n",
      "Epoch 316/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6958 - val_loss: 1423.0649\n",
      "Epoch 317/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6889 - val_loss: 1423.3248\n",
      "Epoch 318/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6807 - val_loss: 1423.5864\n",
      "Epoch 319/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6831 - val_loss: 1424.1707\n",
      "Epoch 320/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6760 - val_loss: 1424.4318\n",
      "Epoch 321/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6727 - val_loss: 1424.6615\n",
      "Epoch 322/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6698 - val_loss: 1424.8634\n",
      "Epoch 323/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6674 - val_loss: 1425.0416\n",
      "Epoch 324/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6653 - val_loss: 1425.1979\n",
      "Epoch 325/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6635 - val_loss: 1425.3367\n",
      "Epoch 326/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6619 - val_loss: 1425.4576\n",
      "Epoch 327/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6605 - val_loss: 1425.5637\n",
      "Epoch 328/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6594 - val_loss: 1425.6569\n",
      "Epoch 329/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6584 - val_loss: 1425.7390\n",
      "Epoch 330/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6576 - val_loss: 1425.8115\n",
      "Epoch 331/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6569 - val_loss: 1425.8759\n",
      "Epoch 332/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6561 - val_loss: 1425.9307\n",
      "Epoch 333/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6557 - val_loss: 1425.9801\n",
      "Epoch 334/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6552 - val_loss: 1426.0234\n",
      "Epoch 335/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 85.6548 - val_loss: 1426.0610\n",
      "Epoch 336/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6545 - val_loss: 1426.0941\n",
      "Epoch 337/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6541 - val_loss: 1426.1229\n",
      "Epoch 338/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 85.6539 - val_loss: 1426.1477\n",
      "Epoch 339/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6537 - val_loss: 1426.1694\n",
      "Epoch 340/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6536 - val_loss: 1426.1890\n",
      "Epoch 341/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6534 - val_loss: 1426.2053\n",
      "Epoch 342/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6534 - val_loss: 1426.2207\n",
      "Epoch 343/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6533 - val_loss: 1426.2330\n",
      "Epoch 344/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6532 - val_loss: 1426.2438\n",
      "Epoch 345/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6532 - val_loss: 1426.2538\n",
      "Epoch 346/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6532 - val_loss: 1426.2627\n",
      "Epoch 347/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6532 - val_loss: 1426.2705\n",
      "Epoch 348/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6531 - val_loss: 1426.2770\n",
      "Epoch 349/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6530 - val_loss: 1426.2808\n",
      "Epoch 350/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6531 - val_loss: 1426.2860\n",
      "Epoch 351/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6532 - val_loss: 1426.2899\n",
      "Epoch 352/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6532 - val_loss: 1426.2924\n",
      "Epoch 353/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6532 - val_loss: 1426.2954\n",
      "Epoch 354/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6533 - val_loss: 1426.2977\n",
      "Epoch 355/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6533 - val_loss: 1426.2993\n",
      "Epoch 356/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6533 - val_loss: 1426.3009\n",
      "Epoch 357/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6534 - val_loss: 1426.3026\n",
      "Epoch 358/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6534 - val_loss: 1426.3031\n",
      "Epoch 359/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6534 - val_loss: 1426.3037\n",
      "Epoch 360/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6534 - val_loss: 1426.3033\n",
      "Epoch 361/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 85.6536 - val_loss: 1426.3040\n",
      "Epoch 362/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6536 - val_loss: 1426.3040\n",
      "Epoch 363/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6536 - val_loss: 1426.3040\n",
      "Epoch 364/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6536 - val_loss: 1426.3031\n",
      "Epoch 365/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6537 - val_loss: 1426.3035\n",
      "Epoch 366/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6537 - val_loss: 1426.3020\n",
      "Epoch 367/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6538 - val_loss: 1426.3005\n",
      "Epoch 368/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6538 - val_loss: 1426.2992\n",
      "Epoch 369/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6538 - val_loss: 1426.2980\n",
      "Epoch 370/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6539 - val_loss: 1426.2970\n",
      "Epoch 371/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6540 - val_loss: 1426.2966\n",
      "Epoch 372/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6540 - val_loss: 1426.2952\n",
      "Epoch 373/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6541 - val_loss: 1426.2944\n",
      "Epoch 374/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6542 - val_loss: 1426.2936\n",
      "Epoch 375/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6541 - val_loss: 1426.2926\n",
      "Epoch 376/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6541 - val_loss: 1426.2911\n",
      "Epoch 377/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6541 - val_loss: 1426.2892\n",
      "Epoch 378/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6542 - val_loss: 1426.2870\n",
      "Epoch 379/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6543 - val_loss: 1426.2864\n",
      "Epoch 380/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6544 - val_loss: 1426.2853\n",
      "Epoch 381/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6543 - val_loss: 1426.2834\n",
      "Epoch 382/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 85.6544 - val_loss: 1426.2819\n",
      "Epoch 383/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6544 - val_loss: 1426.2800\n",
      "Epoch 384/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6543 - val_loss: 1426.2778\n",
      "Epoch 385/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6544 - val_loss: 1426.2758\n",
      "Epoch 386/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6545 - val_loss: 1426.2742\n",
      "Epoch 387/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6545 - val_loss: 1426.2722\n",
      "Epoch 388/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6546 - val_loss: 1426.2706\n",
      "Epoch 389/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6546 - val_loss: 1426.2694\n",
      "Epoch 390/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6546 - val_loss: 1426.2679\n",
      "Epoch 391/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6546 - val_loss: 1426.2655\n",
      "Epoch 392/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6546 - val_loss: 1426.2634\n",
      "Epoch 393/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6547 - val_loss: 1426.2621\n",
      "Epoch 394/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6547 - val_loss: 1426.2605\n",
      "Epoch 395/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6547 - val_loss: 1426.2585\n",
      "Epoch 396/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6547 - val_loss: 1426.2556\n",
      "Epoch 397/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6548 - val_loss: 1426.2534\n",
      "Epoch 398/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6548 - val_loss: 1426.2511\n",
      "Epoch 399/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6548 - val_loss: 1426.2491\n",
      "Epoch 400/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6548 - val_loss: 1426.2469\n",
      "Epoch 401/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6549 - val_loss: 1426.2445\n",
      "Epoch 402/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6549 - val_loss: 1426.2428\n",
      "Epoch 403/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6550 - val_loss: 1426.2413\n",
      "Epoch 404/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 85.6550 - val_loss: 1426.2395\n",
      "Epoch 405/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6550 - val_loss: 1426.2380\n",
      "Epoch 406/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6550 - val_loss: 1426.2357\n",
      "Epoch 407/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6550 - val_loss: 1426.2334\n",
      "Epoch 408/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6550 - val_loss: 1426.2306\n",
      "Epoch 409/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6550 - val_loss: 1426.2279\n",
      "Epoch 410/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6550 - val_loss: 1426.2256\n",
      "Epoch 411/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6551 - val_loss: 1426.2231\n",
      "Epoch 412/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6551 - val_loss: 1426.2209\n",
      "Epoch 413/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6551 - val_loss: 1426.2184\n",
      "Epoch 414/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6551 - val_loss: 1426.2155\n",
      "Epoch 415/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6551 - val_loss: 1426.2125\n",
      "Epoch 416/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6551 - val_loss: 1426.2098\n",
      "Epoch 417/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6552 - val_loss: 1426.2074\n",
      "Epoch 418/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6551 - val_loss: 1426.2048\n",
      "Epoch 419/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6551 - val_loss: 1426.2020\n",
      "Epoch 420/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6552 - val_loss: 1426.1991\n",
      "Epoch 421/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6552 - val_loss: 1426.1962\n",
      "Epoch 422/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6552 - val_loss: 1426.1929\n",
      "Epoch 423/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6552 - val_loss: 1426.1896\n",
      "Epoch 424/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6552 - val_loss: 1426.1864\n",
      "Epoch 425/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 85.6553 - val_loss: 1426.1837\n",
      "Epoch 426/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6553 - val_loss: 1426.1808\n",
      "Epoch 427/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6553 - val_loss: 1426.1779\n",
      "Epoch 428/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6552 - val_loss: 1426.1743\n",
      "Epoch 429/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6552 - val_loss: 1426.1708\n",
      "Epoch 430/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6552 - val_loss: 1426.1674\n",
      "Epoch 431/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6552 - val_loss: 1426.1636\n",
      "Epoch 432/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6553 - val_loss: 1426.1597\n",
      "Epoch 433/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6553 - val_loss: 1426.1558\n",
      "Epoch 434/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6553 - val_loss: 1426.1519\n",
      "Epoch 435/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6553 - val_loss: 1426.1476\n",
      "Epoch 436/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6554 - val_loss: 1426.1437\n",
      "Epoch 437/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6554 - val_loss: 1426.1400\n",
      "Epoch 438/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6554 - val_loss: 1426.1364\n",
      "Epoch 439/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6554 - val_loss: 1426.1327\n",
      "Epoch 440/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6554 - val_loss: 1426.1293\n",
      "Epoch 441/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6554 - val_loss: 1426.1256\n",
      "Epoch 442/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6554 - val_loss: 1426.1211\n",
      "Epoch 443/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6554 - val_loss: 1426.1167\n",
      "Epoch 444/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6554 - val_loss: 1426.1123\n",
      "Epoch 445/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 85.6554 - val_loss: 1426.1080\n",
      "Epoch 446/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 85.6554 - val_loss: 1426.1034\n",
      "Epoch 447/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6554 - val_loss: 1426.0978\n",
      "Epoch 448/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6554 - val_loss: 1426.0920\n",
      "Epoch 449/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6554 - val_loss: 1426.0862\n",
      "Epoch 450/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6554 - val_loss: 1426.0806\n",
      "Epoch 451/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6554 - val_loss: 1426.0752\n",
      "Epoch 452/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6554 - val_loss: 1426.0691\n",
      "Epoch 453/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6554 - val_loss: 1426.0629\n",
      "Epoch 454/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6554 - val_loss: 1426.0570\n",
      "Epoch 455/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6554 - val_loss: 1426.0507\n",
      "Epoch 456/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6554 - val_loss: 1426.0444\n",
      "Epoch 457/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6554 - val_loss: 1426.0374\n",
      "Epoch 458/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6554 - val_loss: 1426.0308\n",
      "Epoch 459/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6554 - val_loss: 1426.0236\n",
      "Epoch 460/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6554 - val_loss: 1426.0162\n",
      "Epoch 461/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6554 - val_loss: 1426.0084\n",
      "Epoch 462/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6554 - val_loss: 1426.0004\n",
      "Epoch 463/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6554 - val_loss: 1425.9927\n",
      "Epoch 464/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6554 - val_loss: 1425.9843\n",
      "Epoch 465/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 85.6554 - val_loss: 1425.9756\n",
      "Epoch 466/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 85.6554 - val_loss: 1425.9670\n",
      "Epoch 467/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6554 - val_loss: 1425.9580\n",
      "Epoch 468/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6554 - val_loss: 1425.9491\n",
      "Epoch 469/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6554 - val_loss: 1425.9402\n",
      "Epoch 470/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.9303\n",
      "Epoch 471/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.9199\n",
      "Epoch 472/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.9095\n",
      "Epoch 473/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.8987\n",
      "Epoch 474/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.8878\n",
      "Epoch 475/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.8761\n",
      "Epoch 476/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.8639\n",
      "Epoch 477/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.8514\n",
      "Epoch 478/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.8383\n",
      "Epoch 479/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.8247\n",
      "Epoch 480/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6555 - val_loss: 1425.8104\n",
      "Epoch 481/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.7958\n",
      "Epoch 482/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.7804\n",
      "Epoch 483/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.7646\n",
      "Epoch 484/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 85.6555 - val_loss: 1425.7466\n",
      "Epoch 485/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.7284\n",
      "Epoch 486/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.7089\n",
      "Epoch 487/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.6887\n",
      "Epoch 488/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.6688\n",
      "Epoch 489/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.6469\n",
      "Epoch 490/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.6235\n",
      "Epoch 491/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.6006\n",
      "Epoch 492/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.5767\n",
      "Epoch 493/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.5503\n",
      "Epoch 494/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.5223\n",
      "Epoch 495/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6555 - val_loss: 1425.4929\n",
      "Epoch 496/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6555 - val_loss: 1425.4618\n",
      "Epoch 497/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.4279\n",
      "Epoch 498/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.3920\n",
      "Epoch 499/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.6555 - val_loss: 1425.3534\n",
      "Epoch 500/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 85.6555 - val_loss: 1425.3121\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(89.3180, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0010, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(-0.0204, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.2194, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 395ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 126)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 126)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 186)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:150:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 7.04212418e+01, 7.00683006e+01, 6.97153595e+01,\n",
       "        6.94312092e+01, 6.92547386e+01, 6.90782680e+01, 6.89017974e+01,\n",
       "        6.87893324e+01, 6.87641223e+01, 1.06601441e+00, 6.04974806e-01,\n",
       "        7.04604575e+01, 7.01075163e+01, 6.97545752e+01, 6.94508170e+01,\n",
       "        6.92743464e+01, 6.90987758e+01, 6.89214052e+01, 6.87921335e+01,\n",
       "        6.87669234e+01, 5.02604306e-01, 2.92042941e-01, 4.54188614e+01,\n",
       "        2.91158468e-01, 8.49562730e-02, 1.54605360e-02, 7.24539340e-01,\n",
       "        0.00000000e+00, 9.74744320e-01, 6.62910233e+01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.63608336e-01, 0.00000000e+00,\n",
       "        4.75232333e-01, 5.51850736e-01, 1.04397535e-01, 1.43292212e+00,\n",
       "        5.92755020e-01, 0.00000000e+00, 1.19628645e-01, 6.35984302e-01,\n",
       "        3.39572072e-01, 0.00000000e+00, 0.00000000e+00, 1.49526566e-01,\n",
       "        0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([62.77494022, 62.76728838, 62.75963654, 62.7519847 , 62.74433286,\n",
       "       62.73668101, 62.72902917, 62.72137733, 62.71372549, 62.70607365,\n",
       "       62.69842181, 62.69076997, 62.68311813, 62.67546628, 62.66781444,\n",
       "       62.6601626 , 62.65251076, 62.64485892, 62.63720708, 62.62955524,\n",
       "       62.6219034 , 62.61425155, 62.60659971, 62.59894787, 62.59129603,\n",
       "       62.58364419, 62.57599235, 62.56834051, 62.56068867, 62.55303682,\n",
       "       62.54538498, 62.53773314, 62.5300813 , 62.52242946, 62.51477762,\n",
       "       62.50712578, 62.49947394, 62.49182209, 62.48417025, 62.47651841,\n",
       "       62.46886657, 62.46121473, 62.45356289, 62.44591105, 62.43825921,\n",
       "       62.43060736, 62.42295552, 62.41530368, 62.40765184, 62.4       ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.42597174221664\n",
      "40.1595758492733\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
