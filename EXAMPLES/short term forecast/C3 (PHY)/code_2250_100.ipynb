{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "2345    67.967958\n",
       "2346    67.961672\n",
       "2347    67.955387\n",
       "2348    67.949102\n",
       "2349    67.942816\n",
       "Name: C3, Length: 2350, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c3_interpolated_2250_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "2245     0.365818\n",
       "2246     0.000000\n",
       "2247     0.546683\n",
       "2248     0.752465\n",
       "2249     0.000000\n",
       "Name: C3, Length: 2250, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2250)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.100000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.875910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.651821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90.427731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90.203641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     91.100000  0.000298\n",
       "1     90.875910  0.000298\n",
       "2     90.651821  0.000297\n",
       "3     90.427731  0.000297\n",
       "4     90.203641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAAsTAAALEwEAmpwYAAAh70lEQVR4nO3de3zcdZ3v8dcn90uTNPfe0hstTQsuUIJUAVcqArIq6HrQs4o9XhY8D92j7uoK6mP18dA9D3fPel0ve1Bw8bIiIApHFLkj4INLWkBo0yttSUubpE3SpLk0t+/5YyaTSTJNZn4z+c1vMu/n49HHzPzm95vfd6bJe775/r4Xc84hIiKZJyfdBRAREW8U4CIiGUoBLiKSoRTgIiIZSgEuIpKh8vw8WU1NjVu5cqWfpxQRyXhbt2495pyrnbrd1wBfuXIlzc3Nfp5SRCTjmdnBWNvVhCIikqEU4CIiGUoBLiKSoRTgIiIZSgEuIpKhFOAiIhlKAS4ikqEyIsDvffE1fvZ0zG6QIiJZKyMC/A8vH+U7D+9hbExzl4uIjMuIAN/cWEd77ym2v9aT7qKIiARGRgT4pY115Bg81NKW7qKIiARGRgR4VWkBG5dX8sjO9nQXRUQkMDIiwAE2r6/jpcMnePnwiXQXRUQkEDImwN/b1MCSiiKu/0kzHb2n0l0cEZG0y5gAr15QyM0fbKKrf5gbftrMqZHRdBdJRCStMibAAc5eWsHXrz2Hba92899vfpoXWrvTXSQRkbTJqAAHuOp1i/nWe8/l1c4BrvneU3zq9uc53D2Q7mKJiPjO1xV5UuWa85Zy2YZ6/uOxffzwiVf4/ctH+eglq7hsfT2LKoqoXVBIXm7GfTeJiCTEnPNvdGNTU5NL9ZJqh7sH+D/37+Q3L7wW2ZZjUFtWyJKFxVy2vp6/3riMRRVFKT2viIhfzGyrc65p2vZMD/Bx+4/1sa/9JEd7BmnrGeToiUH2dZxk26vd5Bi8eV0d1zYtY3NjPQV5qp2LSOY4XYBnZBNKLKtqSllVUzpt+4Fjfdy5tZW7th7iYz9rp7q0gGvOW8qVZy9i4/JKcnMsDaUVEUnevKmBz2ZkdIwn9hzjjuZWHmppY3jUUVmSz6WNdbx1fT2XnFnLgsJ5830mIvPIvK+BzyYvN4dLG+u4tLGO3sFh/rj7GA+1tPFwSzt3bztMQW4Om86o5rL1dbxlfT1LFxanu8giIjPKmhr46YyMjrH1YBcPtbTxUEs7+4/1AdC4qIzzli/kzPoy1tWXceaiMmoWFKa5tCKSjeb9RcxU2ddxkodb2nhsVwc7jvTQ3T8cea66tIB1i8pCoR6+PbN+AWVF+WkssYjMdwpwD5xzdJw8xe6jJ9l5tIfdbb3sajvJnrZe+ocmhvIvXVgcFewLOLO+jDNqF1CUn5vG0ovIfJH1beBemBl1ZUXUlRVx8dqayPaxMcfh7gF2He1lV1svu472srutlyf2dDA8OvGFWFdWSENVCQ2VxTRUlbCsspiGyhIaqkpYXFGkwUYikhQFuAc5ORYK5qoSLttQH9k+PDrGgWN97GrrZV97H4e6+mnt6ue5A13c++JrRK8Il5tjLK4omhTqK6pLeOMZNdSWqa1dRGanAE+h/Nwc1taXsba+bNpzw6NjHD0xSGtnKNRbOwfCAT/A47s7aI+aIvecZRVc2ljH5sY6zl5SQY76qotIDHG1gZvZp4GPAg54CfgQsBi4HagGtgLXOeeGZnqdTGsD99Pg8Ch72k7y2K52HtnVzgut3TgHNQsKefO6WjY31nHx2hrKdcFUJOt4vohpZkuBJ4ENzrkBM7sD+B1wFXC3c+52M/sP4EXn3A9mei0FePyOnzzF47s7eHRXB4/vaqdncIS8HOOClVVsbqzj0sZazqhdgJlq5yLzXbIB/jRwDtAD/Ab4d+DnwCLn3IiZvQH4snPuipleSwHuzcjoGNte7eaRne08urOdXW29ADRUFbN5XWhw0tr6MqpLC9TzRWQeSqoboZl9EvhnYAB4APgk8LRzbk34+Qbg9865s2Mcez1wPcDy5cvPP3jwYDLvQwjNwPhoOMyf2neMweGxyHPF+blUlRac9l9lyeTHC4vz1cYuEnCeuxGaWSVwNbAK6AbuBK6M98TOuZuBmyFUA4/3ODm9pQuL+cCmFXxg0woGh0d57kAnh7sG6OwfovPkUOi2b4iuviH2dZykq2+IvqHYS9DlGCwcD/XwbWVpAdWlBVQvKKC+vIj68sJQd8ryQgrzVMMXCYp4eqFcBux3znUAmNndwEXAQjPLc86NAMuAw3NXTDmdovxcLllbO+t+g8OjdPUPcfzkEF3hgI/1b1/HSboOhu6Pxfi6rSzJj4T5eLg3VJawojo0G2R9eaHa5UV8Ek+AvwpsMrMSQk0obwGagUeB9xDqibIFuGeuCinJK8rPZXFFMYsr4puka2zM0dU/RHvvKdp6BmnvCd229Q7S1nOK9t5T7G0/RnvvKUajkr4oP4eV1aWsrC5lRU0Jq6pLI+FeV1ao5hqZ5vlXuzizvozSAM4GOjg8yvbXTnD+iqp0FyWmWT8x59wzZnYXsA0YAZ4n1CRyH3C7mX01vO2WuSyo+Csnx6heUEj1gkLWLy4/7X6jY47Xugc4cLyPA8f7OXCsj4PH+9jT3ssjO9sZGp1onx8P9xXVJZy9pIKmlVWc27CQ4gI1y2SrE/3DvOv7f+Ky9XX8aMsFcR93/8tH+K9nW/nJh1+f0Pn2H+ujrqww7i+LL92znV82t/LYZ97MyhjrDZzOZ+98kbOXVrDljSsTKl+i4noXzrkvAV+asvkVILFPT+ad3KhRqZesnfzceLgfPN7P/uN9E+HedpI/bG8DIC/HOGtJOeevqOKClZWcv7KSujItf5ct+odHAHjp8ImEjvvYz7Z5Ot+l//YYr19VxR03vCGu/V9+LVSu3sGRhM5z59ZD3Ln1UDACXMSL6HCPnksGoLt/iG2vdtF8IPTv588c5Nan9gOwvKqEppWVNK2oomllJWtqF6jpZZ4ab33L9fG6ybP7O+Ped7x5MCeg0xYpwCUtFpYUsLmxns2NoblkhkbGePm1E2w90MVzBzp5fFcHd28LXRevKM7n/BWVnL+iknMbFrK8qoRFFUXkazKwjDcWCchgfkGP97IO6tKLCnAJhIK8HDYur2Tj8kr+9k2rcc5x4Hg/zx3oZOuBLpoPdvLIzvbI/jkGiyuKWVZZzLLKkvDtxH3N9pgZIjVcH2rgXqbOHnX+lc8LBbgEkplFFqq+tqkBgM6+IVqO9HC4a4DWrn4OdYUmBPvTvmMc7RnEnWa2x4mAD03tu6yqhEXlRYGtVWWTsfB/mh//F7G6xc5+jAJcJCWqSgu4aE1NzOeGRsY4cmIgMsvjeLgf6hrgyT3HaOudHPB5OcbihUUsryqJTOcbPXd7dWmB+rP7YDxU/fioxzzUwMcPCep3vQJc5oWCvBxWhPucx3JqZJTXugcjod7aGbp9tbOfh1raOHZy8kSaJQW54WAvZsnCYkoL8ygtyKWkII+SglxKwo+LC3IpLcijtDDquYI8CvLUfBMPP2u4XgLczyYeLxTgkhUK83IjTTKx9A+N0No5EJmv/dXOiTnbnzvQRd+pEUYS+Bs8P9cozs+ltDAU6hXF+aF+9eEpCqpKo+8XULOgkMqSgnkT/D9+aj8Hj/ezaXUVr19VTVVpQcz9Ik0ovrSBh25nq03/51P7WbywmCvOWhQJ8KlNPN9/bC9LKoq5+twlaf1LTQEuApQU5LFuUWix6tMZGhljYGiUvqER+odG6B8ape/UKP1DI/QNjTIwNBJ53D80Gn4+dL97YIjWzn5eaO2mq2/otF8GZUV51CwoZFF5EesXl3PWknI2LClnTd2CjOp1c/MfX+HIiUH+808HADh7aTl/t3ktl2+onxR4o1G9UJxztHYO0FBVPCeh6CLNNTO/9pf/3w4APnvFusiFz+heMs45/vX+XQDsauvlc1c2AqFZQ/2+cK4AF4lTQV4OBXk5VJQkt6iGc46egRGO9Z2is2+I4ydPcbwvNBHZ8b7Qv1c7+/mvZw9GZposyM1hbf0CNiwOBfqGxeWsX1Ie2AU+hkfHeG9TA9desIynX+nk7m2HuOGnW7lwVRVf/KsNvG5ZBTC5Vvzk3mNcd8uzbFpdxReumtgnVSaaa2beryA3h6HRMb754O7IF230IdFTR/zfx/fx9r9YzPGTQ3zw1me55+MXcU7DwpSWeyYKcBGfmRkVJflUlORzxgzzkI2OOfYf62PHkR52vNbDjiM9PLqrnTu3Hors01BVHAr1xRWhYF9SzpKKorRfgB0aGaOkMJfzV1Rx/ooqbnjTan7xXCvffHA37/juk7z7vKV89sp1k9rAu/qHAXix9QTv+O6TXHPuEj5zxTqWVZakpEzj5zJm/2ze29TA/duPcmJgeNpz4wuX/883n8Gdza188Tcvc+VZi0LbfraVJz+3OSXljYcCXCSgcnOMNXULWFO3gHeesySyvb1nkO1Rod7yWg8P7GiL1GYLcnOoLSuktqyQurJC6sanA46+X15IdWnhnHXfGxodoyCqOSEvN4frNq3g6nOX8P1H93HrU/v57Z+PsKZuATC5ieIX12/ige1HueXJ/dz30hEuXFUdWSM21jWMbzywi+dbu/nLM2u5tLGO1TWlMb/A4unx4pxjaHSM+ooiPndlI5//9Usx3xuEljv8/FXr+fs7XuRE+MvntRODfOuh3bN/QCmiABfJMHXlRdSVF3HpurrItr5TI+w82suOIz0c6uqnIzxj5IHjfTx7oJPu/uk1yRwLhdB4qFeXhi6i5ueGmorycoz83Bzyc8dvJ+7nhe8XhO8X5OVQUZxPZUk+C0sKGB51Mdvsy4vyufFtjbz/wuX8+KkD3P186K+J6qiLnGVFefzjlY18YNMKbvvTAR7e2c5XfruDr/x2B6tqSrlgZeWk13ywpZ09bb08secYX72vhYaqYi5cVc36xeV8YNPyyBz2Lqq2f/LUCN9+aHfor6HifMqL8igvzqdmQSEABbnG+y5omBbgh7sHeOlQd2ifvBzedd5SfrXtEE/tPQ7AptVVfOeRvQn9fyZDAS4yD5QW5kWmG4jl1MgoHb2hUG/vOUVH72DkfnvvIG09g7Qc6WF4dIyhkTGGRx0jY2OR5gIvZupR01BVwj+9YwP/cPmZnPWlP3DByunTtS5ZWMxNV63npqvW09rZzyM720PLCu7qmLbvpY11/NPbN/DY7g4eC+9319ZD3PzHffztJav5mwuXT2pv33awix8+sZ+8HIt5QbkwL5ecHONr734dN949EeJf/PVLkfMX5uVgZly8pjYS4P/2387hrq2H+NZDeyb9BTJXFOAiWaAwLzc8IjWx9mTn3ESYj4SaF2LdPzUyyomBYbr6h+nuH6J3cIR3b1w66+tH96+eaah7Q1UJW964MjK73zce2DWtpttQVcJ1m1Zw3aYVADy19xj//sgevnpfC99/bB9fufrsyDnHz/TLG97AWUvK6RkcpmdgmNbOAXYe7eWd54aarMaLN77/wPAoq2tK+ZsLl3P5hvpp5TQzPnXZmfzm+cP8xbKFs77/ZCnAReS0zIyCPKOAHIjdlTvJ15+7Ay9aU8NFa2rYerCT//WLF/ja/S3hYyfvV5SfS1F+LnVlRaypK+PSxommqVgXPGvKCvnoJatjFytSPH8uImdOx1IRyQqJRt9sk1Sdv6KKj/3lalo7Bzy9fpApwEVk3vvr85dRURzqMz8+aMiLWIels8emAlxEMt5sGVpSkMf7L1wOBHdeEy8U4CKSdg7vvV3i9T/CF0Cj+5LHm+X/+76WSK19pkOiX2/u35ECXEQykJc6dF15EQV5OTStjN3VcqYT3ffSEdp7TyV02OGufnoHp/e/TyUFuIgEylz24Bh/ZS+140RLte3Vbt753ac8nCl+CnARkTjFCv6Zgn3/sb65KgqgABeRgPDSMSTSLu2x0p7plzMV4CKSdomGdzo7ksx07sjAH5/KpwAXkbSJFYaJD+RJ9ID4dw16DV0BLiJZw5K5ijnba6aBAlxEAiGZvuDxLNKQEn507k6AAlxEMpbHEfERXroszvRl4XdtXAEuIhnHtxq3R36VTgEuImkTK4gTrcUm2vTiEjgm3WuLzkYBLiJZI9mae6zgT+dfAwpwEQmEZNqz/RjIE7Drl4ACXEQyWDpCdeaBPP5SgItI2kUvsBBPk8SkaVt9SvFEwllLqonIvJeOa4TOubhDP57iBX4gj5ktNLO7zGynmbWY2RvMrMrMHjSzPeHbBCbZFRGZzI828Kn7JdzjJWAN4fHWwL8N3O+cawTOAVqAG4GHnXNrgYfDj0VEslfQBvKYWQXwJuAWAOfckHOuG7gauC28223ANXNTRBGR2LwuTpyMIHUNj6cGvgroAH5sZs+b2Y/MrBSod84dCe9zFKiPdbCZXW9mzWbW3NHRkZpSi8i8Ep3D8QRk9C5eInyucz9IIzHzgI3AD5xz5wF9TGkucaGvwZgfiXPuZudck3Ouqba2Ntnyisg8ko7KbCLhPfXLJGBN4HEF+CHgkHPumfDjuwgFepuZLQYI37bPTRFFJBskE47xjoaculfQ51SZzawB7pw7CrSa2brwprcAO4B7gS3hbVuAe+akhCIiATLjbIQ+fyHkxbnf3wE/N7MC4BXgQ4TC/w4z+whwELh2boooIhJb0Jo0/BZXgDvnXgCaYjz1lpSWRkSyUqJBPHkkZuIxnsrgjzXq0q+eKhqJKSJpEx1+fnUJjD7LbEE76YuC9HRbnIkCXEQyX9wjMTP7ouVUCnARyVh+VojHo3/G2QiDNhJTRMRPcQ3kiW568XCOuW4K8as3igJcRNLOz5q013OdbrRiOhtlFOAikjbJDomP9Tqp2G9i/2C3mSvARUTiENfc4HNeiskU4CKSsRJdkX7iuPlBAS4igZJwV780p3Gs4mogj4hkDYfzLYija+2JBG1oIE/qy5MMBbiIpE2qaqpx19oTPF/Qx/0owEVE4jAxkGeG2Qh9TnwFuIhkLM99ugPWFOKVAlxEAiWu7npTJpnyfq74a8ynO48G8ohIVnPOe5dASCxEk6l9B63irgAXkbTxu804mbPZlNtUv74XCnARyUJBq0t7owAXkUBJeBxPGhaCiJbOOcYV4CKScVI1yVTC2Rvnl4Vfoa4AF5G0S3aUox95GcQxPQpwEckaidaME12wWCvyiIjMMS+1/SBe9lSAi0igJDS4Jsk5sFJRY07nfCkKcBEJhESCeGpo+pWh8ZbRr/IowEUk/XycnMRrt8P4BvJoMisRySJ+NkGMnytoNWmvFOAikrEczrfKuyazEhGZRTw18lSFZqJNHvF+WWhJNRHJKskN5Jn7xAxic4oCXETSzs8+1smea8YvCw3kEZFs4mfmjZ/LS1OIBvKIiMwi0cUZkloIIhXfHpqNUESyXSJBnK7MjLeMuogpIhKndDTDxHwuqG3gZpZrZs+b2W/Dj1eZ2TNmttfMfmlmBXNXTBGZz/xcJd5rs0sQV7JPpAb+SaAl6vG/AN90zq0BuoCPpLJgIpId/FzRZuq5ZjtzPP3EAz+Qx8yWAX8F/Cj82IDNwF3hXW4DrpmD8olItkkgEZNdCCLTxVsD/xbwj8BY+HE10O2cGwk/PgQsjXWgmV1vZs1m1tzR0ZFMWUVkHkskiKfVjH2qBsfd/dCnAs0a4Gb2dqDdObfVywmcczc755qcc021tbVeXkJEJKW8LegQOihA43jIi2Ofi4B3mtlVQBFQDnwbWGhmeeFa+DLg8NwVU0Tms2T6cqf7XIFe0ME5d5NzbplzbiXwPuAR59z7gUeB94R32wLcM2elFJF5K9n8S6zpZcrjWU6eznCORzL9wD8H/L2Z7SXUJn5LaookItloPIcTW1LN3yuYQbtgGk8TSoRz7jHgsfD9V4DXp75IIiIzm76kWjCqyuPdFDUSU0RkjnipSE/UvoPxZQEKcBEJAL9HYk5IPozTWftXgItIWk1rDpnDPEz0tYNT145NAS4iweChGh658OnXQB5/ThM3BbiIZB0vvVfi+bKwKbdzTQEuIpKEQA/kERGZa+laEzPoA3VmowAXkbQa78UxMZAnfs4l2hySWGJPDXi/Bw7NRgEuIhkn0Xm9UyGe8I4Uy6eqvQJcRCQBQWp1UYCLSNZKyaL0KXgNrxTgIpJ2yTQtJ3powJqxk6IAF5H0Cldhx4M1oTUyEwzjxJumJw4IYvArwEUk4yQ6r/dUyYTxzAN5wrMRen/5hCjARSRrJVTbP+1rpKAgHinARUQylAJcRNLO4SL9rBOt0CbeHOK9/SRo7eAKcBFJq2RaICIrxcf5KuN7xbuwcdCH2ivARSTjpDNYZ/qyGC+XllQTEZljqRnIoxV5REQ8ibc5ZD5SgItI+jn/VtfxeiHSueB9WSjARSStkgnsidGbiZ0r3hCP9bJBurCpABeRjJOqDE1JGMd4DY3EFBGRGSnARSRQEu3V4dfgmqC1f4MCXEQCwDF3E0xNO5dL7kJmkCjARSStkulHnWieJnquWJNdzTgboc8XOBXgIpJxUjGLIKRmEI5W5BERCbigNZ+AAlxEAiKSj4kuzpDqgqTgXH6VSQEuImnnPFZvJ46LP/Vdkv1JZpzMyucGFQW4iKSVl+Zsr03gU4+b7XXiOU3MC53xFykpCnARkQw1a4CbWYOZPWpmO8xsu5l9Mry9ysweNLM94dvKuS+uiMhk/g3kmZt9kxFPDXwE+Afn3AZgE/BxM9sA3Ag87JxbCzwcfiwi4klkSTUf2h+Sb3MPhlkD3Dl3xDm3LXy/F2gBlgJXA7eFd7sNuGaOyigi85znkZHh20RHYiYlUwfymNlK4DzgGaDeOXck/NRRoD61RRORbOAl87zmZKLHxRPIMaecTfA8XsUd4Ga2APgV8CnnXE/0cy70d0XM7zUzu97Mms2suaOjI6nCioikSyLNJ0FqA8fM8gmF98+dc3eHN7eZ2eLw84uB9ljHOududs41OeeaamtrU1FmEZnHEq+9+tcuHawW8Ph6oRhwC9DinPtG1FP3AlvC97cA96S+eCIis0s09OcqiP2eFyUvjn0uAq4DXjKzF8LbPg98DbjDzD4CHASunZMSisi85zVQvVyQjD7E0yCiqY/TOJvVrAHunHuS03+xvCW1xRGRbONpZkGPqZnouaJ3D1rzCWgkpohkuIB1zfaVAlxEAmFihXk/RvL4fNwcUYCLSMbzPrlV4gfOdIwvXz5RFOAiknbeR2ImfmCq18NM50VMBbiIpJWfIzETP086F0ybnQJcRALB6zILXo7y81xzSQEuIoHirUYejJqy36VQgItI2iW3yJl3qWi+SeeXhwJcRNIrmfzzNBIzgYMmlS1oDSgKcBHJQKlaEzPTKcBFJBAmBvIkepyfXQmDVQtXgItIxvOzRj7TMYFekUdEZC4ku6SaHzSQR0RkCt+7DXq9hhlACnARCQQ/5pcary0HqyXbOwW4iASKl9q190WOEztytuDXZFYiIgE0Ht7xRLRfnVUU4CKSsQLWq893CnARSStvc3J7P19ibeZel27zdFjCFOAiEgipnqc7lvE2bz/O5QcFuIgEirfBNXNfU/Z1ybc4KcBFRJIQK9B1EVNEsobXOUbSNQ1tUCjARSStPDWZJHE+51zcwe+5f7kuYopINvG8zFkCtfepwZpozgatxq8AFxGJw3h4T1+RJ8a+agMXEZGZKMBFJO08T2Q1T/pze6UAF5G0Gm+CSGhATlS7RaJZ7GX2wsixcR6si5gikpVSvUrOpP1m3XB67jSzWcU6t9rARURkRgpwEZEMpQAXkbTzc01M5+Z+RR61gYtIVvA0nSzermJOPddsK/JEP+9c/F80agMXkazkbUk1/2YInHqu2c7tdZ6XeCQV4GZ2pZntMrO9ZnZjqgolItnDOcfWg11zGnTRWo70sP3wiYSPGxkb4+SpkYSPW3XT7/jn+3YkfFw88rweaGa5wPeAtwKHgOfM7F7n3NyUVETmpa7+Ybr6h9lxpCfuY04MDANw2dcfZ2B4lOHRsbiP3dN+kj3tJ+Pctzdy/53ffSrmPoPDo9O2TW0V+uET+/n0W8+kpMBz5MaUTA389cBe59wrzrkh4Hbg6tQUS0SyVTxN4v1DodAcCIfnrqO9M+0e0d0/NOlxefHMgdoXo8a9r2Ny+D+xp2PaPqtqSqdtO35yaNq2ZCUT4EuB1qjHh8LbJjGz682s2cyaOzqmv1ERyW43va0xcv/Tl51Jfu7ssfTRS1ZNenzjVY2n2XOyz105sd+7Ny6lMC93xv0/fPGqads+c/m6SY//6R1nAbDlDSsi22540xnTjsvJSX07vXltdzKz9wBXOuc+Gn58HXChc+4TpzumqanJNTc3ezqfiEi2MrOtzrmmqduTqYEfBhqiHi8LbxMRER8kE+DPAWvNbJWZFQDvA+5NTbFERGQ2ni+JOudGzOwTwB+AXOBW59z2lJVMRERmlFSfFufc74DfpagsIiKSAI3EFBHJUApwEZEMpQAXEclQCnARkQzleSCPp5OZdQAHPR5eAxxLYXEynT6PCfosJtPnMdl8+DxWOOdqp270NcCTYWbNsUYiZSt9HhP0WUymz2Oy+fx5qAlFRCRDKcBFRDJUJgX4zekuQMDo85igz2IyfR6TzdvPI2PawEVEZLJMqoGLiEgUBbiISIbKiADPxsWTzeyAmb1kZi+YWXN4W5WZPWhme8K3leHtZmbfCX8+fzazjektffLM7FYzazezl6O2Jfz+zWxLeP89ZrYlHe8lFU7zeXzZzA6Hf0ZeMLOrop67Kfx57DKzK6K2Z/zvkpk1mNmjZrbDzLab2SfD27Pv58M5F+h/hKaq3QesBgqAF4EN6S6XD+/7AFAzZdu/AjeG798I/Ev4/lXA7wEDNgHPpLv8KXj/bwI2Ai97ff9AFfBK+LYyfL8y3e8thZ/Hl4HPxNh3Q/j3pBBYFf79yZ0vv0vAYmBj+H4ZsDv8nrPu5yMTauBaPHnC1cBt4fu3AddEbf+JC3kaWGhmi9NQvpRxzv0R6JyyOdH3fwXwoHOu0znXBTwIXDnnhZ8Dp/k8Tudq4Hbn3Cnn3H5gL6Hfo3nxu+ScO+Kc2xa+3wu0EFqPN+t+PjIhwONaPHkecsADZrbVzK4Pb6t3zh0J3z8K1IfvZ8tnlOj7z4bP5RPhZoFbx5sMyKLPw8xWAucBz5CFPx+ZEODZ6mLn3EbgbcDHzexN0U+60N+AWdsHNNvff9gPgDOAc4EjwNfTWhqfmdkC4FfAp5xzPdHPZcvPRyYEeFYunuycOxy+bQd+TejP37bxppHwbXt492z5jBJ9//P6c3HOtTnnRp1zY8APCf2MQBZ8HmaWTyi8f+6cuzu8Oet+PjIhwLNu8WQzKzWzsvH7wOXAy4Te9/iV8i3APeH79wIfDF9t3wSciPpTcj5J9P3/AbjczCrDzQuXh7fNC1Ouc7yL0M8IhD6P95lZoZmtAtYCzzJPfpfMzIBbgBbn3Deinsq+n490X0WN5x+hq8i7CV1B/0K6y+PD+11NqIfAi8D28fcMVAMPA3uAh4Cq8HYDvhf+fF4CmtL9HlLwGfyCULPAMKG2yY94ef/AhwldxNsLfCjd7yvFn8dPw+/3z4RCanHU/l8Ifx67gLdFbc/43yXgYkLNI38GXgj/uyobfz40lF5EJENlQhOKiIjEoAAXEclQCnARkQylABcRyVAKcBGRDKUAFxHJUApwEZEM9f8BjwgzOzKylQAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.0403  # Value for C0\n",
    "K0 = -0.0012  # Value for K0\n",
    "K1 = -0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0120    # Value for b\n",
    "c = 2.0334    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    91.100000    90.875910    90.651821    90.427731    90.203641   \n",
      "351    90.875910    90.651821    90.427731    90.203641    89.979552   \n",
      "352    90.651821    90.427731    90.203641    89.979552    89.755462   \n",
      "353    90.427731    90.203641    89.979552    89.755462    89.531373   \n",
      "354    90.203641    89.979552    89.755462    89.531373    89.307283   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    89.979552    89.755462    89.531373    89.307283    89.094118  ...   \n",
      "351    89.755462    89.531373    89.307283    89.094118    89.015686  ...   \n",
      "352    89.531373    89.307283    89.094118    89.015686    88.937255  ...   \n",
      "353    89.307283    89.094118    89.015686    88.937255    88.858824  ...   \n",
      "354    89.094118    89.015686    88.937255    88.858824    88.780392  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   81.423343    0.000263   81.404669    0.000263   81.385994    0.000263   \n",
      "351   81.404669    0.000263   81.385994    0.000263   81.367320    0.000262   \n",
      "352   81.385994    0.000263   81.367320    0.000262   81.348646    0.000262   \n",
      "353   81.367320    0.000262   81.348646    0.000262   81.329972    0.000262   \n",
      "354   81.348646    0.000262   81.329972    0.000262   81.311298    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   81.367320    0.000262   81.348646    0.000262  \n",
      "351   81.348646    0.000262   81.329972    0.000262  \n",
      "352   81.329972    0.000262   81.311298    0.000262  \n",
      "353   81.311298    0.000262   81.292624    0.000262  \n",
      "354   81.292624    0.000262   81.273950    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 1, 251) (1800, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "23/23 [==============================] - 2s 22ms/step - loss: 5819.5967 - val_loss: 4296.8726\n",
      "Epoch 2/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 5743.9507 - val_loss: 4247.5063\n",
      "Epoch 3/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 5673.4331 - val_loss: 4206.3975\n",
      "Epoch 4/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 5618.6011 - val_loss: 4167.1001\n",
      "Epoch 5/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 5564.6138 - val_loss: 4127.9531\n",
      "Epoch 6/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 5497.3442 - val_loss: 4076.2698\n",
      "Epoch 7/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 5438.7563 - val_loss: 4034.6001\n",
      "Epoch 8/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 5382.0830 - val_loss: 3993.9456\n",
      "Epoch 9/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 5326.5225 - val_loss: 3954.0237\n",
      "Epoch 10/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 5271.7915 - val_loss: 3914.6824\n",
      "Epoch 11/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 5217.7412 - val_loss: 3875.8389\n",
      "Epoch 12/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 5164.2876 - val_loss: 3837.4434\n",
      "Epoch 13/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 5111.3755 - val_loss: 3799.4631\n",
      "Epoch 14/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 5058.9717 - val_loss: 3761.8750\n",
      "Epoch 15/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 5007.0498 - val_loss: 3724.6621\n",
      "Epoch 16/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4955.5869 - val_loss: 3687.8115\n",
      "Epoch 17/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4904.5718 - val_loss: 3651.3110\n",
      "Epoch 18/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4853.9902 - val_loss: 3615.1538\n",
      "Epoch 19/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4803.8306 - val_loss: 3579.3323\n",
      "Epoch 20/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4754.0864 - val_loss: 3543.8386\n",
      "Epoch 21/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 4704.7480 - val_loss: 3508.6685\n",
      "Epoch 22/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4655.8110 - val_loss: 3473.8169\n",
      "Epoch 23/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4607.2671 - val_loss: 3439.2795\n",
      "Epoch 24/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4559.1123 - val_loss: 3405.0518\n",
      "Epoch 25/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4511.3423 - val_loss: 3371.1306\n",
      "Epoch 26/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 4463.9512 - val_loss: 3337.5117\n",
      "Epoch 27/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4416.9360 - val_loss: 3304.1926\n",
      "Epoch 28/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4370.2925 - val_loss: 3271.1704\n",
      "Epoch 29/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4324.0161 - val_loss: 3238.4402\n",
      "Epoch 30/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4278.1050 - val_loss: 3205.9990\n",
      "Epoch 31/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4232.4634 - val_loss: 3172.7878\n",
      "Epoch 32/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 4181.2075 - val_loss: 3133.9490\n",
      "Epoch 33/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4127.0493 - val_loss: 3096.7424\n",
      "Epoch 34/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 4075.2722 - val_loss: 3060.9917\n",
      "Epoch 35/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4025.1816 - val_loss: 3026.2341\n",
      "Epoch 36/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3976.2805 - val_loss: 2992.2327\n",
      "Epoch 37/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3928.3086 - val_loss: 2958.8567\n",
      "Epoch 38/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 3881.1187 - val_loss: 2926.0256\n",
      "Epoch 39/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 3834.6133 - val_loss: 2893.6860\n",
      "Epoch 40/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3788.7278 - val_loss: 2861.7991\n",
      "Epoch 41/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3743.4138 - val_loss: 2830.3357\n",
      "Epoch 42/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3698.6379 - val_loss: 2799.2737\n",
      "Epoch 43/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3654.3677 - val_loss: 2768.5952\n",
      "Epoch 44/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3610.5833 - val_loss: 2738.2856\n",
      "Epoch 45/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3567.2646 - val_loss: 2708.3320\n",
      "Epoch 46/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 3524.3958 - val_loss: 2678.7251\n",
      "Epoch 47/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3481.9656 - val_loss: 2649.4548\n",
      "Epoch 48/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3439.9590 - val_loss: 2620.5144\n",
      "Epoch 49/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3398.3684 - val_loss: 2591.8955\n",
      "Epoch 50/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3357.1826 - val_loss: 2563.5930\n",
      "Epoch 51/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3316.3955 - val_loss: 2535.6003\n",
      "Epoch 52/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3275.9985 - val_loss: 2507.9131\n",
      "Epoch 53/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3235.9861 - val_loss: 2480.5264\n",
      "Epoch 54/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3196.3511 - val_loss: 2453.4360\n",
      "Epoch 55/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3157.0879 - val_loss: 2426.6367\n",
      "Epoch 56/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3118.1917 - val_loss: 2400.1260\n",
      "Epoch 57/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3079.6580 - val_loss: 2373.8999\n",
      "Epoch 58/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3041.4810 - val_loss: 2347.9553\n",
      "Epoch 59/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3003.6570 - val_loss: 2322.2883\n",
      "Epoch 60/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2966.1833 - val_loss: 2296.8967\n",
      "Epoch 61/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2929.0549 - val_loss: 2271.7778\n",
      "Epoch 62/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2892.2678 - val_loss: 2246.9282\n",
      "Epoch 63/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2855.8188 - val_loss: 2222.3452\n",
      "Epoch 64/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2819.7043 - val_loss: 2198.0269\n",
      "Epoch 65/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2783.9219 - val_loss: 2173.9697\n",
      "Epoch 66/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2748.4678 - val_loss: 2150.1729\n",
      "Epoch 67/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2713.3398 - val_loss: 2126.6331\n",
      "Epoch 68/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2678.5352 - val_loss: 2103.3484\n",
      "Epoch 69/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2644.0500 - val_loss: 2080.3171\n",
      "Epoch 70/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2609.8831 - val_loss: 2057.5366\n",
      "Epoch 71/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2576.0298 - val_loss: 2035.0054\n",
      "Epoch 72/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2542.4900 - val_loss: 2012.7207\n",
      "Epoch 73/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2509.2595 - val_loss: 1990.6809\n",
      "Epoch 74/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2476.3374 - val_loss: 1968.8844\n",
      "Epoch 75/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 2443.7197 - val_loss: 1947.3293\n",
      "Epoch 76/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2411.4050 - val_loss: 1926.0137\n",
      "Epoch 77/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2379.3914 - val_loss: 1904.9359\n",
      "Epoch 78/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2347.6765 - val_loss: 1884.0946\n",
      "Epoch 79/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2316.2581 - val_loss: 1863.4868\n",
      "Epoch 80/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2285.1333 - val_loss: 1843.1127\n",
      "Epoch 81/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2254.3015 - val_loss: 1822.9689\n",
      "Epoch 82/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2223.7603 - val_loss: 1803.0552\n",
      "Epoch 83/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2193.5076 - val_loss: 1783.3688\n",
      "Epoch 84/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2163.5413 - val_loss: 1763.9094\n",
      "Epoch 85/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2133.8599 - val_loss: 1744.6743\n",
      "Epoch 86/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2104.4607 - val_loss: 1725.6627\n",
      "Epoch 87/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2075.3425 - val_loss: 1706.8729\n",
      "Epoch 88/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2046.5037 - val_loss: 1688.3033\n",
      "Epoch 89/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2017.9419 - val_loss: 1669.9524\n",
      "Epoch 90/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1989.6558 - val_loss: 1651.8195\n",
      "Epoch 91/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1961.6434 - val_loss: 1633.9019\n",
      "Epoch 92/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1933.9038 - val_loss: 1616.1995\n",
      "Epoch 93/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1906.4346 - val_loss: 1598.7101\n",
      "Epoch 94/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1879.2335 - val_loss: 1581.4329\n",
      "Epoch 95/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1852.2998 - val_loss: 1564.3656\n",
      "Epoch 96/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1825.6323 - val_loss: 1547.5078\n",
      "Epoch 97/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1799.2279 - val_loss: 1530.8574\n",
      "Epoch 98/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1773.0858 - val_loss: 1514.4137\n",
      "Epoch 99/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1747.2051 - val_loss: 1498.1753\n",
      "Epoch 100/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1721.5835 - val_loss: 1482.1410\n",
      "Epoch 101/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1696.2192 - val_loss: 1466.3085\n",
      "Epoch 102/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1671.1111 - val_loss: 1450.6777\n",
      "Epoch 103/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1646.2577 - val_loss: 1435.2467\n",
      "Epoch 104/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1621.6570 - val_loss: 1420.0145\n",
      "Epoch 105/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1597.3081 - val_loss: 1404.9800\n",
      "Epoch 106/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1573.2098 - val_loss: 1390.1416\n",
      "Epoch 107/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1549.3595 - val_loss: 1375.4984\n",
      "Epoch 108/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1525.7570 - val_loss: 1361.0487\n",
      "Epoch 109/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1502.4004 - val_loss: 1346.7916\n",
      "Epoch 110/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1479.2875 - val_loss: 1332.7260\n",
      "Epoch 111/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1456.4182 - val_loss: 1318.8505\n",
      "Epoch 112/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1433.7905 - val_loss: 1305.1639\n",
      "Epoch 113/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1411.4023 - val_loss: 1291.6652\n",
      "Epoch 114/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1389.2534 - val_loss: 1278.3529\n",
      "Epoch 115/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1367.3420 - val_loss: 1265.2261\n",
      "Epoch 116/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1345.6666 - val_loss: 1252.2834\n",
      "Epoch 117/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1324.2258 - val_loss: 1239.5236\n",
      "Epoch 118/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1303.0181 - val_loss: 1226.9457\n",
      "Epoch 119/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1282.0425 - val_loss: 1214.5485\n",
      "Epoch 120/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1261.2971 - val_loss: 1202.3307\n",
      "Epoch 121/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1240.7817 - val_loss: 1190.2916\n",
      "Epoch 122/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1220.4946 - val_loss: 1178.4297\n",
      "Epoch 123/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1200.4335 - val_loss: 1166.7438\n",
      "Epoch 124/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1180.5981 - val_loss: 1155.2329\n",
      "Epoch 125/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1160.9867 - val_loss: 1143.8956\n",
      "Epoch 126/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1141.5979 - val_loss: 1132.7312\n",
      "Epoch 127/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1122.4305 - val_loss: 1121.7382\n",
      "Epoch 128/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1103.4834 - val_loss: 1110.9154\n",
      "Epoch 129/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1084.7555 - val_loss: 1100.2618\n",
      "Epoch 130/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1066.2446 - val_loss: 1089.7767\n",
      "Epoch 131/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1047.9506 - val_loss: 1079.4584\n",
      "Epoch 132/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1029.8712 - val_loss: 1069.3058\n",
      "Epoch 133/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1012.0054 - val_loss: 1059.3179\n",
      "Epoch 134/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 994.3521 - val_loss: 1049.4935\n",
      "Epoch 135/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 976.9105 - val_loss: 1039.8318\n",
      "Epoch 136/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 959.6783 - val_loss: 1030.3319\n",
      "Epoch 137/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 942.6557 - val_loss: 1020.9920\n",
      "Epoch 138/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 925.8404 - val_loss: 1011.8111\n",
      "Epoch 139/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 909.2310 - val_loss: 1002.7883\n",
      "Epoch 140/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 892.8267 - val_loss: 993.9221\n",
      "Epoch 141/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 876.6260 - val_loss: 985.2118\n",
      "Epoch 142/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 860.6281 - val_loss: 976.6563\n",
      "Epoch 143/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 844.8312 - val_loss: 968.2543\n",
      "Epoch 144/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 829.2346 - val_loss: 960.0046\n",
      "Epoch 145/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 813.8364 - val_loss: 951.9066\n",
      "Epoch 146/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 798.6362 - val_loss: 943.9583\n",
      "Epoch 147/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 783.6323 - val_loss: 936.1594\n",
      "Epoch 148/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 768.8234 - val_loss: 928.5084\n",
      "Epoch 149/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 754.2085 - val_loss: 921.0041\n",
      "Epoch 150/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 739.7863 - val_loss: 913.6456\n",
      "Epoch 151/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 725.5553 - val_loss: 906.4315\n",
      "Epoch 152/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 711.5146 - val_loss: 899.3610\n",
      "Epoch 153/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 697.6630 - val_loss: 892.4328\n",
      "Epoch 154/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 683.9988 - val_loss: 885.6456\n",
      "Epoch 155/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 670.5210 - val_loss: 878.9984\n",
      "Epoch 156/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 657.2285 - val_loss: 872.4901\n",
      "Epoch 157/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 644.1200 - val_loss: 866.1198\n",
      "Epoch 158/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 631.1940 - val_loss: 859.8859\n",
      "Epoch 159/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 618.4500 - val_loss: 853.7877\n",
      "Epoch 160/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 605.8859 - val_loss: 847.8235\n",
      "Epoch 161/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 593.5009 - val_loss: 841.9924\n",
      "Epoch 162/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 581.2934 - val_loss: 836.2938\n",
      "Epoch 163/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 569.2633 - val_loss: 830.7259\n",
      "Epoch 164/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 557.4081 - val_loss: 825.2877\n",
      "Epoch 165/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 545.7269 - val_loss: 819.9780\n",
      "Epoch 166/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 534.2186 - val_loss: 814.7956\n",
      "Epoch 167/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 522.8817 - val_loss: 809.7396\n",
      "Epoch 168/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 511.7154 - val_loss: 804.8087\n",
      "Epoch 169/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 500.7181 - val_loss: 800.0016\n",
      "Epoch 170/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 489.8886 - val_loss: 795.3172\n",
      "Epoch 171/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 479.2259 - val_loss: 790.7545\n",
      "Epoch 172/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 468.7284 - val_loss: 786.3119\n",
      "Epoch 173/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 458.3950 - val_loss: 781.9886\n",
      "Epoch 174/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 448.2245 - val_loss: 777.7834\n",
      "Epoch 175/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 438.2155 - val_loss: 773.6949\n",
      "Epoch 176/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 428.3668 - val_loss: 769.7220\n",
      "Epoch 177/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 418.6774 - val_loss: 765.8635\n",
      "Epoch 178/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 409.1455 - val_loss: 762.1182\n",
      "Epoch 179/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 399.7701 - val_loss: 758.4848\n",
      "Epoch 180/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 390.5497 - val_loss: 754.9620\n",
      "Epoch 181/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 381.4833 - val_loss: 751.5490\n",
      "Epoch 182/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 372.5695 - val_loss: 748.2442\n",
      "Epoch 183/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 363.8072 - val_loss: 745.0465\n",
      "Epoch 184/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 355.1949 - val_loss: 741.9546\n",
      "Epoch 185/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 346.7312 - val_loss: 738.9673\n",
      "Epoch 186/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 338.4150 - val_loss: 736.0834\n",
      "Epoch 187/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 330.2449 - val_loss: 733.3016\n",
      "Epoch 188/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 322.2197 - val_loss: 730.6205\n",
      "Epoch 189/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 314.3378 - val_loss: 728.0391\n",
      "Epoch 190/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 306.5978 - val_loss: 725.5562\n",
      "Epoch 191/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 298.9991 - val_loss: 723.1702\n",
      "Epoch 192/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 291.5398 - val_loss: 720.8801\n",
      "Epoch 193/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 284.2191 - val_loss: 718.6846\n",
      "Epoch 194/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 277.0351 - val_loss: 716.5823\n",
      "Epoch 195/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 269.9865 - val_loss: 714.5720\n",
      "Epoch 196/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 263.0723 - val_loss: 712.6524\n",
      "Epoch 197/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 256.2908 - val_loss: 710.8223\n",
      "Epoch 198/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 249.6410 - val_loss: 709.0801\n",
      "Epoch 199/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 243.1211 - val_loss: 707.4249\n",
      "Epoch 200/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 236.7303 - val_loss: 705.8552\n",
      "Epoch 201/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 230.4671 - val_loss: 704.3698\n",
      "Epoch 202/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 224.3300 - val_loss: 702.9670\n",
      "Epoch 203/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 218.3175 - val_loss: 701.6461\n",
      "Epoch 204/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 212.4282 - val_loss: 700.4053\n",
      "Epoch 205/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 206.6611 - val_loss: 699.2435\n",
      "Epoch 206/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 201.0146 - val_loss: 698.1593\n",
      "Epoch 207/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 195.4875 - val_loss: 697.1514\n",
      "Epoch 208/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 190.0784 - val_loss: 696.2185\n",
      "Epoch 209/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 184.7857 - val_loss: 695.3592\n",
      "Epoch 210/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 179.6082 - val_loss: 694.5723\n",
      "Epoch 211/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 174.5446 - val_loss: 693.8563\n",
      "Epoch 212/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 169.5932 - val_loss: 693.2099\n",
      "Epoch 213/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 164.7529 - val_loss: 692.6319\n",
      "Epoch 214/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 160.0220 - val_loss: 692.1205\n",
      "Epoch 215/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 155.3993 - val_loss: 691.6750\n",
      "Epoch 216/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 150.8835 - val_loss: 691.2936\n",
      "Epoch 217/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 146.4730 - val_loss: 690.9752\n",
      "Epoch 218/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 142.1664 - val_loss: 690.7181\n",
      "Epoch 219/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 137.9626 - val_loss: 690.5211\n",
      "Epoch 220/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 133.8598 - val_loss: 690.3832\n",
      "Epoch 221/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 129.8568 - val_loss: 690.3026\n",
      "Epoch 222/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 125.9520 - val_loss: 690.2780\n",
      "Epoch 223/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 122.1442 - val_loss: 690.3082\n",
      "Epoch 224/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 118.4320 - val_loss: 690.3917\n",
      "Epoch 225/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 114.8137 - val_loss: 690.5273\n",
      "Epoch 226/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 111.2881 - val_loss: 690.7134\n",
      "Epoch 227/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 107.8538 - val_loss: 690.9490\n",
      "Epoch 228/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 104.5094 - val_loss: 691.2325\n",
      "Epoch 229/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 101.2533 - val_loss: 691.5626\n",
      "Epoch 230/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 98.0843 - val_loss: 691.9378\n",
      "Epoch 231/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 95.0010 - val_loss: 692.3571\n",
      "Epoch 232/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 92.0018 - val_loss: 692.8188\n",
      "Epoch 233/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 89.0855 - val_loss: 693.3217\n",
      "Epoch 234/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 86.2504 - val_loss: 693.8645\n",
      "Epoch 235/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 83.4954 - val_loss: 694.4459\n",
      "Epoch 236/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 80.8190 - val_loss: 695.0645\n",
      "Epoch 237/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 78.2199 - val_loss: 695.7188\n",
      "Epoch 238/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 75.6966 - val_loss: 696.4078\n",
      "Epoch 239/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 73.2475 - val_loss: 697.1300\n",
      "Epoch 240/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 70.8714 - val_loss: 697.8843\n",
      "Epoch 241/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 68.5668 - val_loss: 698.6690\n",
      "Epoch 242/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 66.3325 - val_loss: 699.4832\n",
      "Epoch 243/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 64.1671 - val_loss: 700.3252\n",
      "Epoch 244/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 62.0694 - val_loss: 701.1942\n",
      "Epoch 245/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 60.0376 - val_loss: 702.0886\n",
      "Epoch 246/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 58.0705 - val_loss: 703.0074\n",
      "Epoch 247/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 56.1666 - val_loss: 703.9492\n",
      "Epoch 248/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 54.3248 - val_loss: 704.9127\n",
      "Epoch 249/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 52.5438 - val_loss: 705.8966\n",
      "Epoch 250/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 50.8221 - val_loss: 706.9000\n",
      "Epoch 251/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 49.1585 - val_loss: 707.9214\n",
      "Epoch 252/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 47.5518 - val_loss: 708.9598\n",
      "Epoch 253/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 46.0003 - val_loss: 710.0137\n",
      "Epoch 254/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 44.5032 - val_loss: 711.0823\n",
      "Epoch 255/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 43.0586 - val_loss: 712.1644\n",
      "Epoch 256/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 41.6656 - val_loss: 713.2588\n",
      "Epoch 257/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 40.3230 - val_loss: 714.3641\n",
      "Epoch 258/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 39.0295 - val_loss: 715.4794\n",
      "Epoch 259/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 37.7837 - val_loss: 716.6038\n",
      "Epoch 260/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 36.5845 - val_loss: 717.7358\n",
      "Epoch 261/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 35.4306 - val_loss: 718.8749\n",
      "Epoch 262/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 34.3209 - val_loss: 720.0196\n",
      "Epoch 263/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 33.2539 - val_loss: 721.1691\n",
      "Epoch 264/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 32.2288 - val_loss: 722.3223\n",
      "Epoch 265/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 31.2441 - val_loss: 723.4783\n",
      "Epoch 266/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 30.2989 - val_loss: 724.6361\n",
      "Epoch 267/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 29.3921 - val_loss: 725.7946\n",
      "Epoch 268/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 28.5224 - val_loss: 726.9531\n",
      "Epoch 269/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 27.6886 - val_loss: 728.1105\n",
      "Epoch 270/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 26.8897 - val_loss: 729.2663\n",
      "Epoch 271/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 26.1248 - val_loss: 730.4190\n",
      "Epoch 272/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.3927 - val_loss: 731.5684\n",
      "Epoch 273/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.6924 - val_loss: 732.7131\n",
      "Epoch 274/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.0227 - val_loss: 733.8528\n",
      "Epoch 275/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 23.3828 - val_loss: 734.9867\n",
      "Epoch 276/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 22.7715 - val_loss: 736.1137\n",
      "Epoch 277/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 22.1881 - val_loss: 737.2333\n",
      "Epoch 278/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.6314 - val_loss: 738.3450\n",
      "Epoch 279/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.1006 - val_loss: 739.4479\n",
      "Epoch 280/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.5945 - val_loss: 740.5413\n",
      "Epoch 281/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 20.1126 - val_loss: 741.6248\n",
      "Epoch 282/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 19.6538 - val_loss: 742.6975\n",
      "Epoch 283/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 19.2173 - val_loss: 743.7590\n",
      "Epoch 284/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 18.8023 - val_loss: 744.8088\n",
      "Epoch 285/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 18.4078 - val_loss: 745.8464\n",
      "Epoch 286/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 18.0331 - val_loss: 746.8710\n",
      "Epoch 287/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 17.6776 - val_loss: 747.8824\n",
      "Epoch 288/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 17.3403 - val_loss: 748.8802\n",
      "Epoch 289/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 17.0204 - val_loss: 749.8641\n",
      "Epoch 290/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 16.7173 - val_loss: 750.8333\n",
      "Epoch 291/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 16.4303 - val_loss: 751.7878\n",
      "Epoch 292/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 16.1587 - val_loss: 752.7269\n",
      "Epoch 293/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 15.9018 - val_loss: 753.6508\n",
      "Epoch 294/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 15.6589 - val_loss: 754.5590\n",
      "Epoch 295/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 15.4295 - val_loss: 755.4507\n",
      "Epoch 296/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 15.2129 - val_loss: 756.3263\n",
      "Epoch 297/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 15.0086 - val_loss: 757.1855\n",
      "Epoch 298/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 14.8159 - val_loss: 758.0282\n",
      "Epoch 299/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 14.6343 - val_loss: 758.8542\n",
      "Epoch 300/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 14.4633 - val_loss: 759.6630\n",
      "Epoch 301/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 14.3023 - val_loss: 760.4543\n",
      "Epoch 302/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 14.1510 - val_loss: 761.2290\n",
      "Epoch 303/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 14.0087 - val_loss: 761.9865\n",
      "Epoch 304/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 13.8750 - val_loss: 762.7265\n",
      "Epoch 305/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 13.7495 - val_loss: 763.4494\n",
      "Epoch 306/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 13.6316 - val_loss: 764.1548\n",
      "Epoch 307/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 13.5212 - val_loss: 764.8430\n",
      "Epoch 308/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 13.4177 - val_loss: 765.5140\n",
      "Epoch 309/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 13.3208 - val_loss: 766.1678\n",
      "Epoch 310/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 13.2301 - val_loss: 766.8038\n",
      "Epoch 311/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 13.1453 - val_loss: 767.4233\n",
      "Epoch 312/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 13.0659 - val_loss: 768.0256\n",
      "Epoch 313/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 12.9918 - val_loss: 768.6110\n",
      "Epoch 314/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.9227 - val_loss: 769.1799\n",
      "Epoch 315/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.8581 - val_loss: 769.7322\n",
      "Epoch 316/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.7979 - val_loss: 770.2683\n",
      "Epoch 317/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.7417 - val_loss: 770.7880\n",
      "Epoch 318/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.6894 - val_loss: 771.2917\n",
      "Epoch 319/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.6407 - val_loss: 771.7800\n",
      "Epoch 320/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 12.5954 - val_loss: 772.2524\n",
      "Epoch 321/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 12.5532 - val_loss: 772.7095\n",
      "Epoch 322/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 12.5140 - val_loss: 773.1512\n",
      "Epoch 323/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.4777 - val_loss: 773.5783\n",
      "Epoch 324/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.4439 - val_loss: 773.9905\n",
      "Epoch 325/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.4125 - val_loss: 774.3885\n",
      "Epoch 326/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.3834 - val_loss: 774.7723\n",
      "Epoch 327/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 12.3564 - val_loss: 775.1425\n",
      "Epoch 328/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.3314 - val_loss: 775.4990\n",
      "Epoch 329/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.3083 - val_loss: 775.8423\n",
      "Epoch 330/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.2868 - val_loss: 776.1730\n",
      "Epoch 331/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.2670 - val_loss: 776.4906\n",
      "Epoch 332/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.2486 - val_loss: 776.7958\n",
      "Epoch 333/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.2316 - val_loss: 777.0894\n",
      "Epoch 334/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.2159 - val_loss: 777.3712\n",
      "Epoch 335/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.2014 - val_loss: 777.6417\n",
      "Epoch 336/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.1879 - val_loss: 777.9005\n",
      "Epoch 337/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.1756 - val_loss: 778.1491\n",
      "Epoch 338/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.1641 - val_loss: 778.3867\n",
      "Epoch 339/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.1536 - val_loss: 778.6141\n",
      "Epoch 340/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 12.1439 - val_loss: 778.8320\n",
      "Epoch 341/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.1349 - val_loss: 779.0399\n",
      "Epoch 342/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.1266 - val_loss: 779.2389\n",
      "Epoch 343/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.1190 - val_loss: 779.4285\n",
      "Epoch 344/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.1119 - val_loss: 779.6093\n",
      "Epoch 345/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.1055 - val_loss: 779.7818\n",
      "Epoch 346/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0996 - val_loss: 779.9462\n",
      "Epoch 347/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0941 - val_loss: 780.1028\n",
      "Epoch 348/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0890 - val_loss: 780.2516\n",
      "Epoch 349/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0844 - val_loss: 780.3931\n",
      "Epoch 350/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0802 - val_loss: 780.5280\n",
      "Epoch 351/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0763 - val_loss: 780.6559\n",
      "Epoch 352/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0727 - val_loss: 780.7773\n",
      "Epoch 353/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0694 - val_loss: 780.8925\n",
      "Epoch 354/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0664 - val_loss: 781.0017\n",
      "Epoch 355/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 12.0636 - val_loss: 781.1053\n",
      "Epoch 356/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0611 - val_loss: 781.2034\n",
      "Epoch 357/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0588 - val_loss: 781.2961\n",
      "Epoch 358/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0566 - val_loss: 781.3836\n",
      "Epoch 359/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0547 - val_loss: 781.4664\n",
      "Epoch 360/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0529 - val_loss: 781.5446\n",
      "Epoch 361/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0514 - val_loss: 781.6184\n",
      "Epoch 362/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0499 - val_loss: 781.6882\n",
      "Epoch 363/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0486 - val_loss: 781.7537\n",
      "Epoch 364/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0474 - val_loss: 781.8156\n",
      "Epoch 365/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0463 - val_loss: 781.8741\n",
      "Epoch 366/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0453 - val_loss: 781.9286\n",
      "Epoch 367/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0445 - val_loss: 781.9802\n",
      "Epoch 368/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0437 - val_loss: 782.0283\n",
      "Epoch 369/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 12.0430 - val_loss: 782.0737\n",
      "Epoch 370/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0424 - val_loss: 782.1163\n",
      "Epoch 371/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0418 - val_loss: 782.1559\n",
      "Epoch 372/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0414 - val_loss: 782.1930\n",
      "Epoch 373/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0409 - val_loss: 782.2276\n",
      "Epoch 374/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0406 - val_loss: 782.2603\n",
      "Epoch 375/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0403 - val_loss: 782.2906\n",
      "Epoch 376/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0401 - val_loss: 782.3189\n",
      "Epoch 377/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0399 - val_loss: 782.3458\n",
      "Epoch 378/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0397 - val_loss: 782.3701\n",
      "Epoch 379/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0397 - val_loss: 782.3932\n",
      "Epoch 380/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0395 - val_loss: 782.4144\n",
      "Epoch 381/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0395 - val_loss: 782.4342\n",
      "Epoch 382/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 12.0395 - val_loss: 782.4523\n",
      "Epoch 383/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0396 - val_loss: 782.4694\n",
      "Epoch 384/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0396 - val_loss: 782.4852\n",
      "Epoch 385/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0397 - val_loss: 782.4998\n",
      "Epoch 386/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0398 - val_loss: 782.5135\n",
      "Epoch 387/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0399 - val_loss: 782.5261\n",
      "Epoch 388/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0400 - val_loss: 782.5374\n",
      "Epoch 389/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0402 - val_loss: 782.5479\n",
      "Epoch 390/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0404 - val_loss: 782.5577\n",
      "Epoch 391/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0406 - val_loss: 782.5668\n",
      "Epoch 392/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0408 - val_loss: 782.5748\n",
      "Epoch 393/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 12.0410 - val_loss: 782.5824\n",
      "Epoch 394/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0413 - val_loss: 782.5892\n",
      "Epoch 395/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 12.0415 - val_loss: 782.5956\n",
      "Epoch 396/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0418 - val_loss: 782.6016\n",
      "Epoch 397/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0420 - val_loss: 782.6067\n",
      "Epoch 398/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0423 - val_loss: 782.6115\n",
      "Epoch 399/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0426 - val_loss: 782.6157\n",
      "Epoch 400/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0429 - val_loss: 782.6200\n",
      "Epoch 401/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0431 - val_loss: 782.6231\n",
      "Epoch 402/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0435 - val_loss: 782.6264\n",
      "Epoch 403/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0438 - val_loss: 782.6293\n",
      "Epoch 404/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0441 - val_loss: 782.6317\n",
      "Epoch 405/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0444 - val_loss: 782.6342\n",
      "Epoch 406/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0447 - val_loss: 782.6357\n",
      "Epoch 407/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0450 - val_loss: 782.6371\n",
      "Epoch 408/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 12.0453 - val_loss: 782.6384\n",
      "Epoch 409/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 12.0457 - val_loss: 782.6396\n",
      "Epoch 410/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0460 - val_loss: 782.6408\n",
      "Epoch 411/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0463 - val_loss: 782.6416\n",
      "Epoch 412/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0466 - val_loss: 782.6423\n",
      "Epoch 413/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0470 - val_loss: 782.6429\n",
      "Epoch 414/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0473 - val_loss: 782.6437\n",
      "Epoch 415/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0476 - val_loss: 782.6441\n",
      "Epoch 416/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0480 - val_loss: 782.6445\n",
      "Epoch 417/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0482 - val_loss: 782.6442\n",
      "Epoch 418/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0486 - val_loss: 782.6442\n",
      "Epoch 419/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0489 - val_loss: 782.6443\n",
      "Epoch 420/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 12.0492 - val_loss: 782.6442\n",
      "Epoch 421/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0495 - val_loss: 782.6440\n",
      "Epoch 422/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0498 - val_loss: 782.6436\n",
      "Epoch 423/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0501 - val_loss: 782.6427\n",
      "Epoch 424/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0504 - val_loss: 782.6423\n",
      "Epoch 425/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0507 - val_loss: 782.6418\n",
      "Epoch 426/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0511 - val_loss: 782.6413\n",
      "Epoch 427/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0514 - val_loss: 782.6408\n",
      "Epoch 428/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0517 - val_loss: 782.6406\n",
      "Epoch 429/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0519 - val_loss: 782.6398\n",
      "Epoch 430/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0523 - val_loss: 782.6392\n",
      "Epoch 431/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0525 - val_loss: 782.6387\n",
      "Epoch 432/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0528 - val_loss: 782.6384\n",
      "Epoch 433/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0531 - val_loss: 782.6378\n",
      "Epoch 434/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0533 - val_loss: 782.6370\n",
      "Epoch 435/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0536 - val_loss: 782.6360\n",
      "Epoch 436/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 12.0539 - val_loss: 782.6356\n",
      "Epoch 437/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0542 - val_loss: 782.6347\n",
      "Epoch 438/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0544 - val_loss: 782.6340\n",
      "Epoch 439/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0547 - val_loss: 782.6332\n",
      "Epoch 440/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0550 - val_loss: 782.6324\n",
      "Epoch 441/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0552 - val_loss: 782.6318\n",
      "Epoch 442/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0555 - val_loss: 782.6312\n",
      "Epoch 443/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0557 - val_loss: 782.6302\n",
      "Epoch 444/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0560 - val_loss: 782.6295\n",
      "Epoch 445/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0562 - val_loss: 782.6288\n",
      "Epoch 446/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0565 - val_loss: 782.6281\n",
      "Epoch 447/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0567 - val_loss: 782.6274\n",
      "Epoch 448/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 12.0570 - val_loss: 782.6271\n",
      "Epoch 449/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 12.0571 - val_loss: 782.6264\n",
      "Epoch 450/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0574 - val_loss: 782.6258\n",
      "Epoch 451/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0576 - val_loss: 782.6254\n",
      "Epoch 452/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0578 - val_loss: 782.6246\n",
      "Epoch 453/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0580 - val_loss: 782.6241\n",
      "Epoch 454/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0582 - val_loss: 782.6234\n",
      "Epoch 455/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0585 - val_loss: 782.6231\n",
      "Epoch 456/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0586 - val_loss: 782.6226\n",
      "Epoch 457/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0589 - val_loss: 782.6223\n",
      "Epoch 458/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0590 - val_loss: 782.6211\n",
      "Epoch 459/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 12.0592 - val_loss: 782.6205\n",
      "Epoch 460/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0594 - val_loss: 782.6201\n",
      "Epoch 461/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0596 - val_loss: 782.6196\n",
      "Epoch 462/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0598 - val_loss: 782.6189\n",
      "Epoch 463/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0599 - val_loss: 782.6182\n",
      "Epoch 464/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0602 - val_loss: 782.6177\n",
      "Epoch 465/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0603 - val_loss: 782.6169\n",
      "Epoch 466/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0605 - val_loss: 782.6161\n",
      "Epoch 467/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0607 - val_loss: 782.6155\n",
      "Epoch 468/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0609 - val_loss: 782.6151\n",
      "Epoch 469/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0610 - val_loss: 782.6141\n",
      "Epoch 470/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0612 - val_loss: 782.6136\n",
      "Epoch 471/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 12.0613 - val_loss: 782.6133\n",
      "Epoch 472/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0615 - val_loss: 782.6127\n",
      "Epoch 473/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0616 - val_loss: 782.6119\n",
      "Epoch 474/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0618 - val_loss: 782.6116\n",
      "Epoch 475/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0619 - val_loss: 782.6115\n",
      "Epoch 476/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0620 - val_loss: 782.6108\n",
      "Epoch 477/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0622 - val_loss: 782.6106\n",
      "Epoch 478/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0624 - val_loss: 782.6104\n",
      "Epoch 479/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0625 - val_loss: 782.6096\n",
      "Epoch 480/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0626 - val_loss: 782.6091\n",
      "Epoch 481/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0627 - val_loss: 782.6086\n",
      "Epoch 482/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 12.0629 - val_loss: 782.6083\n",
      "Epoch 483/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0630 - val_loss: 782.6079\n",
      "Epoch 484/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0631 - val_loss: 782.6077\n",
      "Epoch 485/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0632 - val_loss: 782.6075\n",
      "Epoch 486/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0634 - val_loss: 782.6071\n",
      "Epoch 487/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0635 - val_loss: 782.6068\n",
      "Epoch 488/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0636 - val_loss: 782.6064\n",
      "Epoch 489/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0637 - val_loss: 782.6061\n",
      "Epoch 490/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0638 - val_loss: 782.6059\n",
      "Epoch 491/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0639 - val_loss: 782.6053\n",
      "Epoch 492/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0640 - val_loss: 782.6049\n",
      "Epoch 493/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 12.0641 - val_loss: 782.6044\n",
      "Epoch 494/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0642 - val_loss: 782.6039\n",
      "Epoch 495/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0643 - val_loss: 782.6035\n",
      "Epoch 496/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0645 - val_loss: 782.6035\n",
      "Epoch 497/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0645 - val_loss: 782.6031\n",
      "Epoch 498/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 12.0646 - val_loss: 782.6030\n",
      "Epoch 499/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0647 - val_loss: 782.6024\n",
      "Epoch 500/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.0648 - val_loss: 782.6018\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.0403, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0012, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0120, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.0334, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 357ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.24769608e+01, 7.24171373e+01, 7.23553137e+01, 7.22934902e+01,\n",
       "        7.22316667e+01, 7.21698413e+01, 7.21080170e+01, 0.00000000e+00,\n",
       "        1.91479445e-01, 0.00000000e+00, 4.14856046e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.34646825e+01, 7.33543880e+01, 7.32461951e+01,\n",
       "        7.31369514e+01, 7.30277078e+01, 7.29184640e+01, 7.28092204e+01,\n",
       "        7.26999767e+01, 7.25907330e+01, 7.24900327e+01, 7.24312092e+01,\n",
       "        7.23723856e+01, 7.23135621e+01, 7.22547386e+01, 7.21959150e+01,\n",
       "        7.21370915e+01, 7.20782680e+01, 7.20204444e+01, 7.19640621e+01,\n",
       "        7.19017974e+01, 7.18429739e+01, 7.17977358e+01, 7.17893324e+01,\n",
       "        7.17809290e+01, 7.17725257e+01, 7.17641223e+01, 7.17557190e+01,\n",
       "        4.18090254e-01, 1.47510827e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.24880058e-01, 3.20035875e-01, 0.00000000e+00, 7.23266340e+01,\n",
       "        7.22678105e+01, 7.22089869e+01, 7.21501634e+01, 7.20913399e+01,\n",
       "        7.20325163e+01, 7.19736928e+01, 7.19148693e+01, 7.18560458e+01,\n",
       "        7.17996032e+01, 7.17911998e+01, 7.17827965e+01, 7.17743931e+01,\n",
       "        7.16598970e+01, 7.15757640e+01, 7.14918300e+01, 7.14077960e+01,\n",
       "        7.13237630e+01, 7.12397290e+01, 7.11556960e+01, 7.10716620e+01,\n",
       "        7.09381420e+01, 7.05179740e+01, 7.00978060e+01, 6.96776380e+01,\n",
       "        6.92574700e+01, 7.76459885e+01, 4.20036280e-02, 0.00000000e+00,\n",
       "        7.46001244e-01, 1.45638764e-01, 3.99314523e-01, 0.00000000e+00,\n",
       "        3.46528587e+01, 0.00000000e+00, 0.00000000e+00, 2.25153655e-01,\n",
       "        1.98707566e-01, 4.62124676e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.90721738e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.96239606e-01, 2.28752315e-01, 5.13487697e-01,\n",
       "        0.00000000e+00, 7.54345179e-01, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([68.56507481, 68.55878937, 68.55250393, 68.54621849, 68.53993305,\n",
       "       68.53364761, 68.52736216, 68.52107672, 68.51479128, 68.50850584,\n",
       "       68.5022204 , 68.49593496, 68.48964952, 68.48336408, 68.47707864,\n",
       "       68.4707932 , 68.46450775, 68.45822231, 68.45193687, 68.44565143,\n",
       "       68.43936599, 68.43308055, 68.42679511, 68.42050967, 68.41422423,\n",
       "       68.40793879, 68.40165334, 68.3953679 , 68.38908246, 68.38279702,\n",
       "       68.37651158, 68.37022614, 68.3639407 , 68.35765526, 68.35136982,\n",
       "       68.34508438, 68.33879893, 68.33251349, 68.32622805, 68.31994261,\n",
       "       68.31365717, 68.30737173, 68.30108629, 68.29480085, 68.28851541,\n",
       "       68.28222997, 68.27594452, 68.26965908, 68.26337364, 68.2570882 ,\n",
       "       68.25080276, 68.24451732, 68.23823188, 68.23194644, 68.225661  ,\n",
       "       68.21937556, 68.21309011, 68.20680467, 68.20051923, 68.19423379,\n",
       "       68.18794835, 68.18166291, 68.17537747, 68.16909203, 68.16280659,\n",
       "       68.15652115, 68.1502357 , 68.14395026, 68.13766482, 68.13137938,\n",
       "       68.12509394, 68.1188085 , 68.11252306, 68.10623762, 68.09995218,\n",
       "       68.09366673, 68.08738129, 68.08109585, 68.07481041, 68.06852497,\n",
       "       68.06223953, 68.05595409, 68.04966865, 68.04338321, 68.03709777,\n",
       "       68.03081232, 68.02452688, 68.01824144, 68.011956  , 68.00567056,\n",
       "       67.99938512, 67.99309968, 67.98681424, 67.9805288 , 67.97424336,\n",
       "       67.96795791, 67.96167247, 67.95538703, 67.94910159, 67.94281615])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.14906517195918\n",
      "28.440315934816972\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
