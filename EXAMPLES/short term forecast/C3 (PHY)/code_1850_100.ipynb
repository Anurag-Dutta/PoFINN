{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "1945    70.265838\n",
       "1946    70.260703\n",
       "1947    70.255567\n",
       "1948    70.250432\n",
       "1949    70.245296\n",
       "Name: C3, Length: 1950, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c3_interpolated_1850_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "1845     0.067414\n",
       "1846     0.000000\n",
       "1847     0.000000\n",
       "1848     0.000000\n",
       "1849     0.122182\n",
       "Name: C3, Length: 1850, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1850)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.100000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.875910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.651821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90.427731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90.203641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     91.100000  0.000298\n",
       "1     90.875910  0.000298\n",
       "2     90.651821  0.000297\n",
       "3     90.427731  0.000297\n",
       "4     90.203641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.0403  # Value for C0\n",
    "K0 = -0.0012  # Value for K0\n",
    "K1 = -0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0120    # Value for b\n",
    "c = 2.0334    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    91.100000    90.875910    90.651821    90.427731    90.203641   \n",
      "351    90.875910    90.651821    90.427731    90.203641    89.979552   \n",
      "352    90.651821    90.427731    90.203641    89.979552    89.755462   \n",
      "353    90.427731    90.203641    89.979552    89.755462    89.531373   \n",
      "354    90.203641    89.979552    89.755462    89.531373    89.307283   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    89.979552    89.755462    89.531373    89.307283    89.094118  ...   \n",
      "351    89.755462    89.531373    89.307283    89.094118    89.015686  ...   \n",
      "352    89.531373    89.307283    89.094118    89.015686    88.937255  ...   \n",
      "353    89.307283    89.094118    89.015686    88.937255    88.858824  ...   \n",
      "354    89.094118    89.015686    88.937255    88.858824    88.780392  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   81.423343    0.000263   81.404669    0.000263   81.385994    0.000263   \n",
      "351   81.404669    0.000263   81.385994    0.000263   81.367320    0.000262   \n",
      "352   81.385994    0.000263   81.367320    0.000262   81.348646    0.000262   \n",
      "353   81.367320    0.000262   81.348646    0.000262   81.329972    0.000262   \n",
      "354   81.348646    0.000262   81.329972    0.000262   81.311298    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   81.367320    0.000262   81.348646    0.000262  \n",
      "351   81.348646    0.000262   81.329972    0.000262  \n",
      "352   81.329972    0.000262   81.311298    0.000262  \n",
      "353   81.311298    0.000262   81.292624    0.000262  \n",
      "354   81.292624    0.000262   81.273950    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 1, 251) (1400, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "18/18 [==============================] - 3s 40ms/step - loss: 5862.8350 - val_loss: 4998.5547\n",
      "Epoch 2/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5781.4888 - val_loss: 4936.2578\n",
      "Epoch 3/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 5714.5708 - val_loss: 4874.4746\n",
      "Epoch 4/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5647.3018 - val_loss: 4801.9502\n",
      "Epoch 5/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5568.7578 - val_loss: 4738.1162\n",
      "Epoch 6/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5500.0669 - val_loss: 4674.9634\n",
      "Epoch 7/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5432.3340 - val_loss: 4612.7734\n",
      "Epoch 8/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5365.5415 - val_loss: 4551.4136\n",
      "Epoch 9/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5299.5679 - val_loss: 4490.7954\n",
      "Epoch 10/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5234.3311 - val_loss: 4430.8633\n",
      "Epoch 11/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5169.7881 - val_loss: 4371.5835\n",
      "Epoch 12/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5105.9038 - val_loss: 4312.9316\n",
      "Epoch 13/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5042.6602 - val_loss: 4254.8931\n",
      "Epoch 14/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4980.0396 - val_loss: 4197.4517\n",
      "Epoch 15/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4918.0298 - val_loss: 4140.5991\n",
      "Epoch 16/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4856.6216 - val_loss: 4084.3250\n",
      "Epoch 17/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4795.8037 - val_loss: 4028.6223\n",
      "Epoch 18/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4735.5698 - val_loss: 3973.4834\n",
      "Epoch 19/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4675.9126 - val_loss: 3918.9019\n",
      "Epoch 20/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4602.2441 - val_loss: 3843.4299\n",
      "Epoch 21/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4532.2227 - val_loss: 3784.5361\n",
      "Epoch 22/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4468.8828 - val_loss: 3727.2397\n",
      "Epoch 23/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4407.1055 - val_loss: 3671.2305\n",
      "Epoch 24/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4346.5571 - val_loss: 3616.2542\n",
      "Epoch 25/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4287.0171 - val_loss: 3562.1643\n",
      "Epoch 26/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4228.3594 - val_loss: 3508.8684\n",
      "Epoch 27/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4170.5015 - val_loss: 3456.3076\n",
      "Epoch 28/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4113.3867 - val_loss: 3404.4395\n",
      "Epoch 29/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4056.9763 - val_loss: 3353.2327\n",
      "Epoch 30/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4001.2407 - val_loss: 3302.6619\n",
      "Epoch 31/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3946.1558 - val_loss: 3252.7090\n",
      "Epoch 32/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3891.7021 - val_loss: 3203.3562\n",
      "Epoch 33/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3837.8638 - val_loss: 3154.5898\n",
      "Epoch 34/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3784.6260 - val_loss: 3106.3984\n",
      "Epoch 35/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3731.9780 - val_loss: 3058.7720\n",
      "Epoch 36/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3679.9092 - val_loss: 3011.6992\n",
      "Epoch 37/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3628.4092 - val_loss: 2965.1731\n",
      "Epoch 38/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3577.4700 - val_loss: 2919.1858\n",
      "Epoch 39/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3527.0837 - val_loss: 2873.7295\n",
      "Epoch 40/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3477.2432 - val_loss: 2828.7976\n",
      "Epoch 41/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3427.9414 - val_loss: 2784.3835\n",
      "Epoch 42/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3379.1711 - val_loss: 2740.4822\n",
      "Epoch 43/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3330.9277 - val_loss: 2697.0869\n",
      "Epoch 44/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3283.2053 - val_loss: 2654.1936\n",
      "Epoch 45/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3235.9978 - val_loss: 2611.7957\n",
      "Epoch 46/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3189.3003 - val_loss: 2569.8879\n",
      "Epoch 47/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3143.1072 - val_loss: 2528.4680\n",
      "Epoch 48/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3097.4153 - val_loss: 2487.5286\n",
      "Epoch 49/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3052.2180 - val_loss: 2447.0664\n",
      "Epoch 50/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3007.5117 - val_loss: 2407.0779\n",
      "Epoch 51/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2963.2920 - val_loss: 2367.5566\n",
      "Epoch 52/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2919.5544 - val_loss: 2328.4998\n",
      "Epoch 53/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2876.2947 - val_loss: 2289.9036\n",
      "Epoch 54/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2833.5095 - val_loss: 2251.7642\n",
      "Epoch 55/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2791.1938 - val_loss: 2214.0767\n",
      "Epoch 56/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2749.3445 - val_loss: 2176.8372\n",
      "Epoch 57/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2707.9565 - val_loss: 2140.0435\n",
      "Epoch 58/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 2667.0273 - val_loss: 2103.6897\n",
      "Epoch 59/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2626.5522 - val_loss: 2067.7734\n",
      "Epoch 60/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2586.5281 - val_loss: 2032.2924\n",
      "Epoch 61/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2546.9519 - val_loss: 1997.2404\n",
      "Epoch 62/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2507.8193 - val_loss: 1962.6152\n",
      "Epoch 63/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2469.1265 - val_loss: 1928.4130\n",
      "Epoch 64/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2430.8696 - val_loss: 1894.6304\n",
      "Epoch 65/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2393.0464 - val_loss: 1861.2639\n",
      "Epoch 66/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2355.6533 - val_loss: 1828.3112\n",
      "Epoch 67/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2318.6863 - val_loss: 1795.7679\n",
      "Epoch 68/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2282.1436 - val_loss: 1763.6318\n",
      "Epoch 69/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2246.0198 - val_loss: 1731.8990\n",
      "Epoch 70/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2210.3137 - val_loss: 1700.5659\n",
      "Epoch 71/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2175.0203 - val_loss: 1669.6302\n",
      "Epoch 72/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2140.1377 - val_loss: 1639.0879\n",
      "Epoch 73/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2105.6624 - val_loss: 1608.9362\n",
      "Epoch 74/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2071.5906 - val_loss: 1579.1719\n",
      "Epoch 75/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2037.9203 - val_loss: 1549.7921\n",
      "Epoch 76/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2004.6476 - val_loss: 1520.7936\n",
      "Epoch 77/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1971.7699 - val_loss: 1492.1740\n",
      "Epoch 78/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1939.2834 - val_loss: 1463.9294\n",
      "Epoch 79/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1907.1866 - val_loss: 1436.0577\n",
      "Epoch 80/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1875.4752 - val_loss: 1408.5549\n",
      "Epoch 81/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1844.1464 - val_loss: 1381.4188\n",
      "Epoch 82/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1813.1981 - val_loss: 1354.6454\n",
      "Epoch 83/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1782.6263 - val_loss: 1328.2332\n",
      "Epoch 84/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1752.4290 - val_loss: 1302.1791\n",
      "Epoch 85/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1722.6028 - val_loss: 1276.4795\n",
      "Epoch 86/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1693.1451 - val_loss: 1251.1320\n",
      "Epoch 87/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1664.0527 - val_loss: 1226.1334\n",
      "Epoch 88/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1635.3235 - val_loss: 1201.4819\n",
      "Epoch 89/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1606.9542 - val_loss: 1177.1738\n",
      "Epoch 90/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1578.9421 - val_loss: 1153.2052\n",
      "Epoch 91/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1551.2839 - val_loss: 1129.5757\n",
      "Epoch 92/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1523.9779 - val_loss: 1106.2817\n",
      "Epoch 93/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1497.0203 - val_loss: 1083.3197\n",
      "Epoch 94/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1470.4091 - val_loss: 1060.6876\n",
      "Epoch 95/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1444.1412 - val_loss: 1038.3818\n",
      "Epoch 96/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1418.2140 - val_loss: 1016.4010\n",
      "Epoch 97/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1392.6252 - val_loss: 994.7416\n",
      "Epoch 98/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1367.3715 - val_loss: 973.4014\n",
      "Epoch 99/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1342.4508 - val_loss: 952.3768\n",
      "Epoch 100/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1317.8600 - val_loss: 931.6663\n",
      "Epoch 101/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1293.5967 - val_loss: 911.2667\n",
      "Epoch 102/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1269.6580 - val_loss: 891.1755\n",
      "Epoch 103/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1246.0415 - val_loss: 871.3896\n",
      "Epoch 104/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1222.7448 - val_loss: 851.9064\n",
      "Epoch 105/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1199.7651 - val_loss: 832.7243\n",
      "Epoch 106/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1177.1000 - val_loss: 813.8401\n",
      "Epoch 107/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1154.7468 - val_loss: 795.2513\n",
      "Epoch 108/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1132.7025 - val_loss: 776.9547\n",
      "Epoch 109/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1110.9653 - val_loss: 758.9473\n",
      "Epoch 110/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1089.5319 - val_loss: 741.2282\n",
      "Epoch 111/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1068.4003 - val_loss: 723.7941\n",
      "Epoch 112/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1047.5681 - val_loss: 706.6420\n",
      "Epoch 113/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1027.0328 - val_loss: 689.7704\n",
      "Epoch 114/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1006.7912 - val_loss: 673.1757\n",
      "Epoch 115/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 986.8416 - val_loss: 656.8558\n",
      "Epoch 116/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 967.1809 - val_loss: 640.8085\n",
      "Epoch 117/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 947.8070 - val_loss: 625.0300\n",
      "Epoch 118/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 928.7171 - val_loss: 609.5198\n",
      "Epoch 119/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 909.9094 - val_loss: 594.2740\n",
      "Epoch 120/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 891.3813 - val_loss: 579.2907\n",
      "Epoch 121/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 873.1299 - val_loss: 564.5677\n",
      "Epoch 122/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 855.1525 - val_loss: 550.1005\n",
      "Epoch 123/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 837.4473 - val_loss: 535.8896\n",
      "Epoch 124/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 820.0120 - val_loss: 521.9306\n",
      "Epoch 125/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 802.8444 - val_loss: 508.2218\n",
      "Epoch 126/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 785.9407 - val_loss: 494.7597\n",
      "Epoch 127/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 769.2997 - val_loss: 481.5433\n",
      "Epoch 128/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 752.9184 - val_loss: 468.5691\n",
      "Epoch 129/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 736.7946 - val_loss: 455.8353\n",
      "Epoch 130/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 720.9261 - val_loss: 443.3387\n",
      "Epoch 131/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 705.3104 - val_loss: 431.0779\n",
      "Epoch 132/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 689.9453 - val_loss: 419.0498\n",
      "Epoch 133/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 674.8282 - val_loss: 407.2524\n",
      "Epoch 134/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 659.9567 - val_loss: 395.6829\n",
      "Epoch 135/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 645.3286 - val_loss: 384.3390\n",
      "Epoch 136/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 630.9415 - val_loss: 373.2179\n",
      "Epoch 137/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 616.7931 - val_loss: 362.3185\n",
      "Epoch 138/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 602.8809 - val_loss: 351.6372\n",
      "Epoch 139/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 589.2028 - val_loss: 341.1715\n",
      "Epoch 140/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 575.7562 - val_loss: 330.9203\n",
      "Epoch 141/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 562.5394 - val_loss: 320.8802\n",
      "Epoch 142/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 549.5494 - val_loss: 311.0491\n",
      "Epoch 143/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 536.7840 - val_loss: 301.4241\n",
      "Epoch 144/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 524.2408 - val_loss: 292.0038\n",
      "Epoch 145/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 511.9178 - val_loss: 282.7848\n",
      "Epoch 146/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 499.8124 - val_loss: 273.7657\n",
      "Epoch 147/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 487.9227 - val_loss: 264.9435\n",
      "Epoch 148/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 476.2460 - val_loss: 256.3163\n",
      "Epoch 149/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 464.7807 - val_loss: 247.8816\n",
      "Epoch 150/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 453.5237 - val_loss: 239.6369\n",
      "Epoch 151/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 442.4728 - val_loss: 231.5801\n",
      "Epoch 152/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 431.6263 - val_loss: 223.7087\n",
      "Epoch 153/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 420.9814 - val_loss: 216.0201\n",
      "Epoch 154/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 410.5357 - val_loss: 208.5124\n",
      "Epoch 155/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 400.2876 - val_loss: 201.1833\n",
      "Epoch 156/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 390.2349 - val_loss: 194.0305\n",
      "Epoch 157/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 380.3748 - val_loss: 187.0521\n",
      "Epoch 158/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 370.7054 - val_loss: 180.2442\n",
      "Epoch 159/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 361.2238 - val_loss: 173.6062\n",
      "Epoch 160/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 351.9286 - val_loss: 167.1353\n",
      "Epoch 161/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 342.8170 - val_loss: 160.8284\n",
      "Epoch 162/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 333.8874 - val_loss: 154.6846\n",
      "Epoch 163/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 325.1371 - val_loss: 148.7010\n",
      "Epoch 164/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 316.5641 - val_loss: 142.8747\n",
      "Epoch 165/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 308.1660 - val_loss: 137.2041\n",
      "Epoch 166/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 299.9405 - val_loss: 131.6874\n",
      "Epoch 167/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 291.8858 - val_loss: 126.3216\n",
      "Epoch 168/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 283.9995 - val_loss: 121.1044\n",
      "Epoch 169/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 276.2793 - val_loss: 116.0339\n",
      "Epoch 170/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 268.7233 - val_loss: 111.1077\n",
      "Epoch 171/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 261.3292 - val_loss: 106.3237\n",
      "Epoch 172/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 254.0945 - val_loss: 101.6797\n",
      "Epoch 173/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 247.0177 - val_loss: 97.1734\n",
      "Epoch 174/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 240.0963 - val_loss: 92.8023\n",
      "Epoch 175/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 233.3280 - val_loss: 88.5646\n",
      "Epoch 176/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 226.7108 - val_loss: 84.4582\n",
      "Epoch 177/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 220.2425 - val_loss: 80.4806\n",
      "Epoch 178/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 213.9210 - val_loss: 76.6293\n",
      "Epoch 179/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 207.7442 - val_loss: 72.9027\n",
      "Epoch 180/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 201.7101 - val_loss: 69.2988\n",
      "Epoch 181/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 195.8167 - val_loss: 65.8152\n",
      "Epoch 182/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 190.0617 - val_loss: 62.4492\n",
      "Epoch 183/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 184.4428 - val_loss: 59.1994\n",
      "Epoch 184/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 178.9582 - val_loss: 56.0632\n",
      "Epoch 185/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 173.6055 - val_loss: 53.0386\n",
      "Epoch 186/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 168.3832 - val_loss: 50.1238\n",
      "Epoch 187/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 163.2887 - val_loss: 47.3164\n",
      "Epoch 188/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 158.3204 - val_loss: 44.6141\n",
      "Epoch 189/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 153.4759 - val_loss: 42.0154\n",
      "Epoch 190/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 148.7535 - val_loss: 39.5177\n",
      "Epoch 191/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 144.1510 - val_loss: 37.1191\n",
      "Epoch 192/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 139.6661 - val_loss: 34.8176\n",
      "Epoch 193/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 135.2975 - val_loss: 32.6113\n",
      "Epoch 194/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 131.0426 - val_loss: 30.4980\n",
      "Epoch 195/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 126.8996 - val_loss: 28.4755\n",
      "Epoch 196/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 122.8666 - val_loss: 26.5422\n",
      "Epoch 197/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 118.9417 - val_loss: 24.6959\n",
      "Epoch 198/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 115.1228 - val_loss: 22.9345\n",
      "Epoch 199/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 111.4079 - val_loss: 21.2563\n",
      "Epoch 200/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 107.7954 - val_loss: 19.6592\n",
      "Epoch 201/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 104.2832 - val_loss: 18.1413\n",
      "Epoch 202/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.8692 - val_loss: 16.7005\n",
      "Epoch 203/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 97.5517 - val_loss: 15.3353\n",
      "Epoch 204/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 94.3291 - val_loss: 14.0435\n",
      "Epoch 205/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 91.1991 - val_loss: 12.8234\n",
      "Epoch 206/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 88.1603 - val_loss: 11.6729\n",
      "Epoch 207/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 85.2104 - val_loss: 10.5904\n",
      "Epoch 208/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 82.3478 - val_loss: 9.5739\n",
      "Epoch 209/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 79.5708 - val_loss: 8.6217\n",
      "Epoch 210/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 76.8773 - val_loss: 7.7318\n",
      "Epoch 211/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 74.2657 - val_loss: 6.9028\n",
      "Epoch 212/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 71.7347 - val_loss: 6.1328\n",
      "Epoch 213/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 69.2819 - val_loss: 5.4199\n",
      "Epoch 214/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 66.9060 - val_loss: 4.7627\n",
      "Epoch 215/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 64.6052 - val_loss: 4.1591\n",
      "Epoch 216/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 62.3776 - val_loss: 3.6077\n",
      "Epoch 217/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 60.2217 - val_loss: 3.1068\n",
      "Epoch 218/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 58.1359 - val_loss: 2.6547\n",
      "Epoch 219/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 56.1185 - val_loss: 2.2498\n",
      "Epoch 220/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 54.1678 - val_loss: 1.8905\n",
      "Epoch 221/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 52.2823 - val_loss: 1.5753\n",
      "Epoch 222/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 50.4605 - val_loss: 1.3026\n",
      "Epoch 223/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 48.7006 - val_loss: 1.0707\n",
      "Epoch 224/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 47.0011 - val_loss: 0.8784\n",
      "Epoch 225/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 45.3607 - val_loss: 0.7240\n",
      "Epoch 226/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 43.7777 - val_loss: 0.6061\n",
      "Epoch 227/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 42.2506 - val_loss: 0.5233\n",
      "Epoch 228/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 40.7781 - val_loss: 0.4741\n",
      "Epoch 229/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 39.3585 - val_loss: 0.4571\n",
      "Epoch 230/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 37.9907 - val_loss: 0.4710\n",
      "Epoch 231/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 36.6731 - val_loss: 0.5145\n",
      "Epoch 232/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 35.4043 - val_loss: 0.5861\n",
      "Epoch 233/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 34.1831 - val_loss: 0.6846\n",
      "Epoch 234/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 33.0079 - val_loss: 0.8088\n",
      "Epoch 235/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 31.8776 - val_loss: 0.9574\n",
      "Epoch 236/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 30.7907 - val_loss: 1.1291\n",
      "Epoch 237/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 29.7460 - val_loss: 1.3228\n",
      "Epoch 238/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.7424 - val_loss: 1.5373\n",
      "Epoch 239/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 27.7785 - val_loss: 1.7714\n",
      "Epoch 240/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 26.8532 - val_loss: 2.0241\n",
      "Epoch 241/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 25.9651 - val_loss: 2.2941\n",
      "Epoch 242/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 25.1133 - val_loss: 2.5806\n",
      "Epoch 243/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.2965 - val_loss: 2.8824\n",
      "Epoch 244/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 23.5137 - val_loss: 3.1985\n",
      "Epoch 245/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.7636 - val_loss: 3.5278\n",
      "Epoch 246/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.0453 - val_loss: 3.8695\n",
      "Epoch 247/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.3577 - val_loss: 4.2227\n",
      "Epoch 248/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 20.6997 - val_loss: 4.5863\n",
      "Epoch 249/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 20.0703 - val_loss: 4.9595\n",
      "Epoch 250/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.4685 - val_loss: 5.3416\n",
      "Epoch 251/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 18.8934 - val_loss: 5.7315\n",
      "Epoch 252/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 18.3440 - val_loss: 6.1286\n",
      "Epoch 253/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 17.8194 - val_loss: 6.5319\n",
      "Epoch 254/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 17.3188 - val_loss: 6.9407\n",
      "Epoch 255/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 16.8412 - val_loss: 7.3546\n",
      "Epoch 256/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 16.3856 - val_loss: 7.7725\n",
      "Epoch 257/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 15.9515 - val_loss: 8.1938\n",
      "Epoch 258/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 15.5379 - val_loss: 8.6180\n",
      "Epoch 259/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 15.1439 - val_loss: 9.0443\n",
      "Epoch 260/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.7690 - val_loss: 9.4722\n",
      "Epoch 261/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 14.4123 - val_loss: 9.9010\n",
      "Epoch 262/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.0730 - val_loss: 10.3303\n",
      "Epoch 263/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7505 - val_loss: 10.7594\n",
      "Epoch 264/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4441 - val_loss: 11.1880\n",
      "Epoch 265/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.1531 - val_loss: 11.6153\n",
      "Epoch 266/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 12.8770 - val_loss: 12.0411\n",
      "Epoch 267/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 12.6149 - val_loss: 12.4650\n",
      "Epoch 268/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 12.3664 - val_loss: 12.8863\n",
      "Epoch 269/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 12.1309 - val_loss: 13.3045\n",
      "Epoch 270/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 11.9078 - val_loss: 13.7199\n",
      "Epoch 271/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 11.6964 - val_loss: 14.1317\n",
      "Epoch 272/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 11.4963 - val_loss: 14.5394\n",
      "Epoch 273/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 11.3070 - val_loss: 14.9432\n",
      "Epoch 274/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 11.1280 - val_loss: 15.3424\n",
      "Epoch 275/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 10.9588 - val_loss: 15.7365\n",
      "Epoch 276/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 10.7990 - val_loss: 16.1259\n",
      "Epoch 277/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 10.6481 - val_loss: 16.5098\n",
      "Epoch 278/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 10.5057 - val_loss: 16.8881\n",
      "Epoch 279/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 10.3713 - val_loss: 17.2609\n",
      "Epoch 280/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 10.2447 - val_loss: 17.6278\n",
      "Epoch 281/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 10.1252 - val_loss: 17.9886\n",
      "Epoch 282/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 10.0127 - val_loss: 18.3432\n",
      "Epoch 283/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 9.9068 - val_loss: 18.6913\n",
      "Epoch 284/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 9.8071 - val_loss: 19.0330\n",
      "Epoch 285/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 9.7133 - val_loss: 19.3682\n",
      "Epoch 286/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 9.6252 - val_loss: 19.6968\n",
      "Epoch 287/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 9.5423 - val_loss: 20.0184\n",
      "Epoch 288/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 9.4645 - val_loss: 20.3334\n",
      "Epoch 289/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 9.3914 - val_loss: 20.6414\n",
      "Epoch 290/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 9.3228 - val_loss: 20.9424\n",
      "Epoch 291/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 9.2585 - val_loss: 21.2367\n",
      "Epoch 292/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 9.1981 - val_loss: 21.5238\n",
      "Epoch 293/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 9.1416 - val_loss: 21.8041\n",
      "Epoch 294/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 9.0886 - val_loss: 22.0774\n",
      "Epoch 295/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 9.0390 - val_loss: 22.3438\n",
      "Epoch 296/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 8.9925 - val_loss: 22.6033\n",
      "Epoch 297/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.9491 - val_loss: 22.8558\n",
      "Epoch 298/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.9085 - val_loss: 23.1014\n",
      "Epoch 299/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.8704 - val_loss: 23.3404\n",
      "Epoch 300/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.8349 - val_loss: 23.5724\n",
      "Epoch 301/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.8018 - val_loss: 23.7978\n",
      "Epoch 302/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 8.7708 - val_loss: 24.0165\n",
      "Epoch 303/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 8.7419 - val_loss: 24.2287\n",
      "Epoch 304/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.7149 - val_loss: 24.4344\n",
      "Epoch 305/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.6898 - val_loss: 24.6338\n",
      "Epoch 306/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 8.6663 - val_loss: 24.8270\n",
      "Epoch 307/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 8.6444 - val_loss: 25.0137\n",
      "Epoch 308/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.6240 - val_loss: 25.1947\n",
      "Epoch 309/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 8.6050 - val_loss: 25.3694\n",
      "Epoch 310/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.5873 - val_loss: 25.5382\n",
      "Epoch 311/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.5708 - val_loss: 25.7011\n",
      "Epoch 312/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.5555 - val_loss: 25.8586\n",
      "Epoch 313/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.5412 - val_loss: 26.0103\n",
      "Epoch 314/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 8.5280 - val_loss: 26.1566\n",
      "Epoch 315/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 8.5157 - val_loss: 26.2979\n",
      "Epoch 316/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.5041 - val_loss: 26.4338\n",
      "Epoch 317/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.4935 - val_loss: 26.5647\n",
      "Epoch 318/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 8.4835 - val_loss: 26.6905\n",
      "Epoch 319/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.4743 - val_loss: 26.8115\n",
      "Epoch 320/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.4657 - val_loss: 26.9275\n",
      "Epoch 321/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.4578 - val_loss: 27.0393\n",
      "Epoch 322/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.4504 - val_loss: 27.1465\n",
      "Epoch 323/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.4436 - val_loss: 27.2492\n",
      "Epoch 324/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.4372 - val_loss: 27.3481\n",
      "Epoch 325/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.4312 - val_loss: 27.4426\n",
      "Epoch 326/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.4258 - val_loss: 27.5329\n",
      "Epoch 327/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 8.4207 - val_loss: 27.6196\n",
      "Epoch 328/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.4159 - val_loss: 27.7026\n",
      "Epoch 329/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.4116 - val_loss: 27.7819\n",
      "Epoch 330/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.4075 - val_loss: 27.8576\n",
      "Epoch 331/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.4038 - val_loss: 27.9300\n",
      "Epoch 332/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 8.4003 - val_loss: 27.9992\n",
      "Epoch 333/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 8.3970 - val_loss: 28.0649\n",
      "Epoch 334/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 8.3941 - val_loss: 28.1279\n",
      "Epoch 335/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3912 - val_loss: 28.1878\n",
      "Epoch 336/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3887 - val_loss: 28.2448\n",
      "Epoch 337/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3863 - val_loss: 28.2990\n",
      "Epoch 338/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3841 - val_loss: 28.3505\n",
      "Epoch 339/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3821 - val_loss: 28.3997\n",
      "Epoch 340/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 8.3802 - val_loss: 28.4464\n",
      "Epoch 341/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3785 - val_loss: 28.4908\n",
      "Epoch 342/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3769 - val_loss: 28.5330\n",
      "Epoch 343/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3754 - val_loss: 28.5730\n",
      "Epoch 344/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3740 - val_loss: 28.6109\n",
      "Epoch 345/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3728 - val_loss: 28.6468\n",
      "Epoch 346/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3716 - val_loss: 28.6810\n",
      "Epoch 347/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3705 - val_loss: 28.7129\n",
      "Epoch 348/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3696 - val_loss: 28.7433\n",
      "Epoch 349/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3687 - val_loss: 28.7722\n",
      "Epoch 350/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3678 - val_loss: 28.7992\n",
      "Epoch 351/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3672 - val_loss: 28.8247\n",
      "Epoch 352/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3665 - val_loss: 28.8489\n",
      "Epoch 353/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3659 - val_loss: 28.8718\n",
      "Epoch 354/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3653 - val_loss: 28.8933\n",
      "Epoch 355/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 8.3648 - val_loss: 28.9134\n",
      "Epoch 356/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3644 - val_loss: 28.9326\n",
      "Epoch 357/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3640 - val_loss: 28.9503\n",
      "Epoch 358/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3636 - val_loss: 28.9671\n",
      "Epoch 359/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3633 - val_loss: 28.9829\n",
      "Epoch 360/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3630 - val_loss: 28.9978\n",
      "Epoch 361/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3628 - val_loss: 29.0119\n",
      "Epoch 362/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3626 - val_loss: 29.0247\n",
      "Epoch 363/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3624 - val_loss: 29.0366\n",
      "Epoch 364/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3623 - val_loss: 29.0478\n",
      "Epoch 365/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3622 - val_loss: 29.0586\n",
      "Epoch 366/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3621 - val_loss: 29.0686\n",
      "Epoch 367/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3620 - val_loss: 29.0779\n",
      "Epoch 368/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3620 - val_loss: 29.0863\n",
      "Epoch 369/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3620 - val_loss: 29.0943\n",
      "Epoch 370/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3620 - val_loss: 29.1018\n",
      "Epoch 371/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 8.3621 - val_loss: 29.1089\n",
      "Epoch 372/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3621 - val_loss: 29.1152\n",
      "Epoch 373/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3622 - val_loss: 29.1212\n",
      "Epoch 374/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3622 - val_loss: 29.1267\n",
      "Epoch 375/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3623 - val_loss: 29.1318\n",
      "Epoch 376/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3624 - val_loss: 29.1365\n",
      "Epoch 377/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 8.3625 - val_loss: 29.1408\n",
      "Epoch 378/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 8.3627 - val_loss: 29.1450\n",
      "Epoch 379/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3628 - val_loss: 29.1486\n",
      "Epoch 380/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3630 - val_loss: 29.1518\n",
      "Epoch 381/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3631 - val_loss: 29.1549\n",
      "Epoch 382/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3633 - val_loss: 29.1576\n",
      "Epoch 383/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3636 - val_loss: 29.1600\n",
      "Epoch 384/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3638 - val_loss: 29.1625\n",
      "Epoch 385/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3640 - val_loss: 29.1645\n",
      "Epoch 386/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3642 - val_loss: 29.1663\n",
      "Epoch 387/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3644 - val_loss: 29.1681\n",
      "Epoch 388/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3646 - val_loss: 29.1697\n",
      "Epoch 389/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3648 - val_loss: 29.1711\n",
      "Epoch 390/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3651 - val_loss: 29.1721\n",
      "Epoch 391/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 8.3654 - val_loss: 29.1733\n",
      "Epoch 392/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3656 - val_loss: 29.1743\n",
      "Epoch 393/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3658 - val_loss: 29.1750\n",
      "Epoch 394/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3661 - val_loss: 29.1755\n",
      "Epoch 395/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 8.3664 - val_loss: 29.1763\n",
      "Epoch 396/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3667 - val_loss: 29.1768\n",
      "Epoch 397/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3669 - val_loss: 29.1771\n",
      "Epoch 398/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 8.3672 - val_loss: 29.1775\n",
      "Epoch 399/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 8.3675 - val_loss: 29.1775\n",
      "Epoch 400/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 8.3678 - val_loss: 29.1779\n",
      "Epoch 401/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3680 - val_loss: 29.1781\n",
      "Epoch 402/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3683 - val_loss: 29.1781\n",
      "Epoch 403/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3686 - val_loss: 29.1780\n",
      "Epoch 404/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.3689 - val_loss: 29.1781\n",
      "Epoch 405/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3692 - val_loss: 29.1779\n",
      "Epoch 406/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 8.3694 - val_loss: 29.1776\n",
      "Epoch 407/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 8.3697 - val_loss: 29.1774\n",
      "Epoch 408/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3701 - val_loss: 29.1773\n",
      "Epoch 409/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3703 - val_loss: 29.1770\n",
      "Epoch 410/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3706 - val_loss: 29.1768\n",
      "Epoch 411/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3709 - val_loss: 29.1765\n",
      "Epoch 412/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3713 - val_loss: 29.1760\n",
      "Epoch 413/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3715 - val_loss: 29.1754\n",
      "Epoch 414/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 8.3719 - val_loss: 29.1749\n",
      "Epoch 415/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3722 - val_loss: 29.1745\n",
      "Epoch 416/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 8.3725 - val_loss: 29.1741\n",
      "Epoch 417/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 8.3728 - val_loss: 29.1737\n",
      "Epoch 418/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3731 - val_loss: 29.1733\n",
      "Epoch 419/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3733 - val_loss: 29.1726\n",
      "Epoch 420/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3736 - val_loss: 29.1719\n",
      "Epoch 421/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3740 - val_loss: 29.1714\n",
      "Epoch 422/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3743 - val_loss: 29.1711\n",
      "Epoch 423/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3746 - val_loss: 29.1704\n",
      "Epoch 424/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3749 - val_loss: 29.1697\n",
      "Epoch 425/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3752 - val_loss: 29.1690\n",
      "Epoch 426/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3755 - val_loss: 29.1685\n",
      "Epoch 427/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3758 - val_loss: 29.1683\n",
      "Epoch 428/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 8.3761 - val_loss: 29.1677\n",
      "Epoch 429/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 8.3764 - val_loss: 29.1672\n",
      "Epoch 430/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 8.3767 - val_loss: 29.1665\n",
      "Epoch 431/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 8.3770 - val_loss: 29.1658\n",
      "Epoch 432/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 8.3773 - val_loss: 29.1655\n",
      "Epoch 433/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3776 - val_loss: 29.1649\n",
      "Epoch 434/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 8.3779 - val_loss: 29.1642\n",
      "Epoch 435/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3782 - val_loss: 29.1635\n",
      "Epoch 436/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3785 - val_loss: 29.1629\n",
      "Epoch 437/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3788 - val_loss: 29.1627\n",
      "Epoch 438/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3791 - val_loss: 29.1622\n",
      "Epoch 439/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3793 - val_loss: 29.1614\n",
      "Epoch 440/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3797 - val_loss: 29.1608\n",
      "Epoch 441/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3800 - val_loss: 29.1603\n",
      "Epoch 442/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3802 - val_loss: 29.1599\n",
      "Epoch 443/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3805 - val_loss: 29.1591\n",
      "Epoch 444/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 8.3808 - val_loss: 29.1584\n",
      "Epoch 445/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3811 - val_loss: 29.1579\n",
      "Epoch 446/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3814 - val_loss: 29.1573\n",
      "Epoch 447/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3817 - val_loss: 29.1569\n",
      "Epoch 448/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3820 - val_loss: 29.1564\n",
      "Epoch 449/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3822 - val_loss: 29.1557\n",
      "Epoch 450/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3825 - val_loss: 29.1551\n",
      "Epoch 451/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3828 - val_loss: 29.1546\n",
      "Epoch 452/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3831 - val_loss: 29.1543\n",
      "Epoch 453/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3834 - val_loss: 29.1538\n",
      "Epoch 454/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3836 - val_loss: 29.1531\n",
      "Epoch 455/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 8.3839 - val_loss: 29.1524\n",
      "Epoch 456/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 8.3841 - val_loss: 29.1519\n",
      "Epoch 457/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 8.3844 - val_loss: 29.1512\n",
      "Epoch 458/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 8.3847 - val_loss: 29.1506\n",
      "Epoch 459/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3850 - val_loss: 29.1500\n",
      "Epoch 460/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3852 - val_loss: 29.1492\n",
      "Epoch 461/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3855 - val_loss: 29.1488\n",
      "Epoch 462/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3858 - val_loss: 29.1483\n",
      "Epoch 463/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3860 - val_loss: 29.1478\n",
      "Epoch 464/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3863 - val_loss: 29.1474\n",
      "Epoch 465/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3865 - val_loss: 29.1466\n",
      "Epoch 466/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3868 - val_loss: 29.1461\n",
      "Epoch 467/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3870 - val_loss: 29.1457\n",
      "Epoch 468/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3873 - val_loss: 29.1452\n",
      "Epoch 469/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3876 - val_loss: 29.1447\n",
      "Epoch 470/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3878 - val_loss: 29.1443\n",
      "Epoch 471/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 8.3880 - val_loss: 29.1438\n",
      "Epoch 472/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3883 - val_loss: 29.1432\n",
      "Epoch 473/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3885 - val_loss: 29.1430\n",
      "Epoch 474/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3888 - val_loss: 29.1425\n",
      "Epoch 475/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 8.3890 - val_loss: 29.1419\n",
      "Epoch 476/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3893 - val_loss: 29.1416\n",
      "Epoch 477/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 8.3895 - val_loss: 29.1410\n",
      "Epoch 478/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3897 - val_loss: 29.1404\n",
      "Epoch 479/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3899 - val_loss: 29.1399\n",
      "Epoch 480/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 8.3902 - val_loss: 29.1396\n",
      "Epoch 481/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 8.3904 - val_loss: 29.1391\n",
      "Epoch 482/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3906 - val_loss: 29.1386\n",
      "Epoch 483/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3908 - val_loss: 29.1381\n",
      "Epoch 484/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3911 - val_loss: 29.1376\n",
      "Epoch 485/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 8.3913 - val_loss: 29.1372\n",
      "Epoch 486/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 8.3915 - val_loss: 29.1367\n",
      "Epoch 487/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3917 - val_loss: 29.1361\n",
      "Epoch 488/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 8.3920 - val_loss: 29.1358\n",
      "Epoch 489/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3922 - val_loss: 29.1352\n",
      "Epoch 490/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3924 - val_loss: 29.1348\n",
      "Epoch 491/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3926 - val_loss: 29.1345\n",
      "Epoch 492/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3928 - val_loss: 29.1342\n",
      "Epoch 493/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3930 - val_loss: 29.1336\n",
      "Epoch 494/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3932 - val_loss: 29.1332\n",
      "Epoch 495/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3934 - val_loss: 29.1327\n",
      "Epoch 496/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3936 - val_loss: 29.1325\n",
      "Epoch 497/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.3938 - val_loss: 29.1319\n",
      "Epoch 498/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3940 - val_loss: 29.1318\n",
      "Epoch 499/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 8.3941 - val_loss: 29.1313\n",
      "Epoch 500/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 8.3943 - val_loss: 29.1306\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.0403, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0012, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0120, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.0334, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 375ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.34646825e+01, 7.34282680e+01, 7.33918534e+01, 7.33554388e+01,\n",
       "        7.33190243e+01, 7.32826097e+01, 7.32461951e+01, 7.32097806e+01,\n",
       "        7.31733660e+01, 7.31369514e+01, 7.31005369e+01, 7.30641223e+01,\n",
       "        7.30277078e+01, 7.29912932e+01, 7.29548786e+01, 7.29184640e+01,\n",
       "        7.28820495e+01, 7.28456349e+01, 7.28092204e+01, 7.27728058e+01,\n",
       "        7.27363912e+01, 7.26999767e+01, 7.26635621e+01, 7.26271475e+01,\n",
       "        7.25907330e+01, 7.25543184e+01, 7.25179038e+01, 7.24900327e+01,\n",
       "        7.24704248e+01, 7.24508170e+01, 7.24312092e+01, 7.24116013e+01,\n",
       "        7.23919935e+01, 7.23723856e+01, 7.23527778e+01, 7.23331699e+01,\n",
       "        7.23135621e+01, 7.22939542e+01, 7.22743464e+01, 7.22547386e+01,\n",
       "        7.22351307e+01, 7.22155229e+01, 7.21959150e+01, 7.21763072e+01,\n",
       "        7.21566993e+01, 7.21370915e+01, 7.21174837e+01, 7.20978758e+01,\n",
       "        7.20782680e+01, 7.20586601e+01, 7.20390523e+01, 7.20194444e+01,\n",
       "        7.19998366e+01, 7.19802288e+01, 7.19606209e+01, 7.19410131e+01,\n",
       "        7.19214052e+01, 7.19017974e+01, 7.18821895e+01, 7.18625817e+01,\n",
       "        7.18429739e+01, 7.18233660e+01, 7.18037582e+01, 7.17977358e+01,\n",
       "        7.17949346e+01, 7.17921335e+01, 7.17893324e+01, 7.17865313e+01,\n",
       "        7.17837302e+01, 7.17809290e+01, 7.17781279e+01, 7.17753268e+01,\n",
       "        7.17725257e+01, 7.17697246e+01, 7.17669234e+01, 7.17641223e+01,\n",
       "        7.17613212e+01, 7.17585201e+01, 7.17557190e+01, 7.17529178e+01,\n",
       "        7.78936920e+01, 4.18090254e-01, 0.00000000e+00, 5.45291901e-01,\n",
       "        1.47510827e-01, 1.14750028e-01, 1.87882394e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.24880058e-01, 7.00237229e-02, 7.90040269e-02, 3.20035875e-01,\n",
       "        2.78152108e-01, 0.00000000e+00, 0.00000000e+00, 2.63176173e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([70.91447246, 70.9070028 , 70.89953315, 70.89206349, 70.88459384,\n",
       "       70.87712418, 70.86965453, 70.86218487, 70.85471522, 70.84724556,\n",
       "       70.83977591, 70.83230626, 70.8248366 , 70.81736695, 70.80989729,\n",
       "       70.80242764, 70.79495798, 70.78748833, 70.78001867, 70.77254902,\n",
       "       70.76507937, 70.75760971, 70.75014006, 70.7426704 , 70.73520075,\n",
       "       70.72773109, 70.72026144, 70.71279178, 70.70532213, 70.69785247,\n",
       "       70.69038282, 70.68291317, 70.67544351, 70.66797386, 70.6605042 ,\n",
       "       70.65303455, 70.64556489, 70.63809524, 70.63062558, 70.62315593,\n",
       "       70.61568627, 70.60821662, 70.60074697, 70.59327731, 70.58580766,\n",
       "       70.578338  , 70.57086835, 70.56339869, 70.55592904, 70.54845938,\n",
       "       70.54098973, 70.53352007, 70.52605042, 70.51858077, 70.51111111,\n",
       "       70.50364146, 70.4961718 , 70.48870215, 70.48123249, 70.47376284,\n",
       "       70.46629318, 70.45882353, 70.45135387, 70.44388422, 70.43641457,\n",
       "       70.42894491, 70.42147526, 70.4140056 , 70.40653595, 70.39935808,\n",
       "       70.39422269, 70.3890873 , 70.38395191, 70.37881653, 70.37368114,\n",
       "       70.36854575, 70.36341036, 70.35827498, 70.35313959, 70.3480042 ,\n",
       "       70.34286881, 70.33773343, 70.33259804, 70.32746265, 70.32232726,\n",
       "       70.31719188, 70.31205649, 70.3069211 , 70.30178571, 70.29665033,\n",
       "       70.29151494, 70.28637955, 70.28124416, 70.27610878, 70.27097339,\n",
       "       70.265838  , 70.26070261, 70.25556723, 70.25043184, 70.24529645])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.624300031249327\n",
      "14.787892340605783\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
