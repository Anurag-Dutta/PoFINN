{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "2495    67.025142\n",
       "2496    67.018856\n",
       "2497    67.012571\n",
       "2498    67.006285\n",
       "2499    67.000000\n",
       "Name: C3, Length: 2500, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c3_interpolated_2450_50.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "2445     0.000000\n",
       "2446     0.000000\n",
       "2447     0.000000\n",
       "2448     0.033660\n",
       "2449     0.000000\n",
       "Name: C3, Length: 2450, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(50)\n",
    "test\n",
    "training_set = training_set.head(2450)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.100000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.875910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.651821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90.427731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90.203641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     91.100000  0.000298\n",
       "1     90.875910  0.000298\n",
       "2     90.651821  0.000297\n",
       "3     90.427731  0.000297\n",
       "4     90.203641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.0403  # Value for C0\n",
    "K0 = -0.0012  # Value for K0\n",
    "K1 = -0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0120    # Value for b\n",
    "c = 2.0334    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-175)  var1(t-174)  var1(t-173)  var1(t-172)  var1(t-171)  \\\n",
      "175    91.100000    90.875910    90.651821    90.427731    90.203641   \n",
      "176    90.875910    90.651821    90.427731    90.203641    89.979552   \n",
      "177    90.651821    90.427731    90.203641    89.979552    89.755462   \n",
      "178    90.427731    90.203641    89.979552    89.755462    89.531373   \n",
      "179    90.203641    89.979552    89.755462    89.531373    89.307283   \n",
      "\n",
      "     var1(t-170)  var1(t-169)  var1(t-168)  var1(t-167)  var1(t-166)  ...  \\\n",
      "175    89.979552    89.755462    89.531373    89.307283    89.094118  ...   \n",
      "176    89.755462    89.531373    89.307283    89.094118    89.015686  ...   \n",
      "177    89.531373    89.307283    89.094118    89.015686    88.937255  ...   \n",
      "178    89.307283    89.094118    89.015686    88.937255    88.858824  ...   \n",
      "179    89.094118    89.015686    88.937255    88.858824    88.780392  ...   \n",
      "\n",
      "     var1(t+45)  var2(t+45)  var1(t+46)  var2(t+46)  var1(t+47)  var2(t+47)  \\\n",
      "175   83.735014    0.000280   83.723810    0.000280   83.712605    0.000280   \n",
      "176   83.723810    0.000280   83.712605    0.000280   83.701401    0.000280   \n",
      "177   83.712605    0.000280   83.701401    0.000280   83.690196    0.000279   \n",
      "178   83.701401    0.000280   83.690196    0.000279   83.678992    0.000279   \n",
      "179   83.690196    0.000279   83.678992    0.000279   83.667787    0.000279   \n",
      "\n",
      "     var1(t+48)  var2(t+48)  var1(t+49)  var2(t+49)  \n",
      "175   83.701401    0.000280   83.690196    0.000279  \n",
      "176   83.690196    0.000279   83.678992    0.000279  \n",
      "177   83.678992    0.000279   83.667787    0.000279  \n",
      "178   83.667787    0.000279   83.656583    0.000279  \n",
      "179   83.656583    0.000279   83.645378    0.000279  \n",
      "\n",
      "[5 rows x 276 columns]\n",
      "Index(['var1(t-175)', 'var1(t-174)', 'var1(t-173)', 'var1(t-172)',\n",
      "       'var1(t-171)', 'var1(t-170)', 'var1(t-169)', 'var1(t-168)',\n",
      "       'var1(t-167)', 'var1(t-166)',\n",
      "       ...\n",
      "       'var1(t+45)', 'var2(t+45)', 'var1(t+46)', 'var2(t+46)', 'var1(t+47)',\n",
      "       'var2(t+47)', 'var1(t+48)', 'var2(t+48)', 'var1(t+49)', 'var2(t+49)'],\n",
      "      dtype='object', length=276)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 175, n_out = 50)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 176):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-150:]\n",
    "trainX = train[:,:-150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-150:]\n",
    "forecastX = forecast[:,:-150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225, 1, 126) (2225, 150) (1, 1, 126)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "28/28 [==============================] - 2s 19ms/step - loss: 5552.4751 - val_loss: 3424.7124\n",
      "Epoch 2/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 5303.2188 - val_loss: 3218.4119\n",
      "Epoch 3/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 5052.6943 - val_loss: 3084.5444\n",
      "Epoch 4/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4866.0791 - val_loss: 2966.8926\n",
      "Epoch 5/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4692.7314 - val_loss: 2868.1118\n",
      "Epoch 6/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4533.7329 - val_loss: 2775.9363\n",
      "Epoch 7/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 4382.5264 - val_loss: 2688.4202\n",
      "Epoch 8/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4237.2578 - val_loss: 2604.7578\n",
      "Epoch 9/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 4093.5583 - val_loss: 2520.6743\n",
      "Epoch 10/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3950.7190 - val_loss: 2440.5349\n",
      "Epoch 11/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 3813.7200 - val_loss: 2364.1238\n",
      "Epoch 12/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3681.7202 - val_loss: 2290.9658\n",
      "Epoch 13/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 3554.0857 - val_loss: 2220.7896\n",
      "Epoch 14/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3430.4500 - val_loss: 2153.4170\n",
      "Epoch 15/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3310.5698 - val_loss: 2088.7144\n",
      "Epoch 16/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 3194.2632 - val_loss: 2026.5730\n",
      "Epoch 17/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3081.3850 - val_loss: 1966.9012\n",
      "Epoch 18/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2971.8127 - val_loss: 1909.6183\n",
      "Epoch 19/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2865.4404 - val_loss: 1854.6505\n",
      "Epoch 20/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2762.1736 - val_loss: 1801.9301\n",
      "Epoch 21/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2661.9260 - val_loss: 1751.3953\n",
      "Epoch 22/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2564.6194 - val_loss: 1702.9862\n",
      "Epoch 23/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2470.1785 - val_loss: 1656.6465\n",
      "Epoch 24/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2378.5347 - val_loss: 1612.3218\n",
      "Epoch 25/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2289.6196 - val_loss: 1569.9601\n",
      "Epoch 26/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2203.3721 - val_loss: 1529.5112\n",
      "Epoch 27/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2119.7302 - val_loss: 1490.9257\n",
      "Epoch 28/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2038.6344 - val_loss: 1454.1561\n",
      "Epoch 29/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1960.0282 - val_loss: 1419.1552\n",
      "Epoch 30/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1883.8555 - val_loss: 1385.8776\n",
      "Epoch 31/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1810.0624 - val_loss: 1354.2787\n",
      "Epoch 32/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1738.5959 - val_loss: 1324.3140\n",
      "Epoch 33/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1669.4048 - val_loss: 1295.9407\n",
      "Epoch 34/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1602.4376 - val_loss: 1269.1163\n",
      "Epoch 35/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1537.6451 - val_loss: 1243.7982\n",
      "Epoch 36/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 1474.9784 - val_loss: 1219.9462\n",
      "Epoch 37/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1414.3893 - val_loss: 1197.5186\n",
      "Epoch 38/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1355.8304 - val_loss: 1176.4753\n",
      "Epoch 39/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1299.2555 - val_loss: 1156.7770\n",
      "Epoch 40/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1244.6187 - val_loss: 1138.3842\n",
      "Epoch 41/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1191.8749 - val_loss: 1121.2581\n",
      "Epoch 42/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1140.9789 - val_loss: 1105.3612\n",
      "Epoch 43/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 1091.8878 - val_loss: 1090.6545\n",
      "Epoch 44/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1044.5571 - val_loss: 1077.1014\n",
      "Epoch 45/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 998.9448 - val_loss: 1064.6646\n",
      "Epoch 46/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 955.0084 - val_loss: 1053.3077\n",
      "Epoch 47/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 912.7059 - val_loss: 1042.9948\n",
      "Epoch 48/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 871.9962 - val_loss: 1033.6901\n",
      "Epoch 49/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 832.8386 - val_loss: 1025.3582\n",
      "Epoch 50/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 795.1929 - val_loss: 1017.9641\n",
      "Epoch 51/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 759.0192 - val_loss: 1011.4737\n",
      "Epoch 52/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 724.2784 - val_loss: 1005.8527\n",
      "Epoch 53/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 690.9316 - val_loss: 1001.0676\n",
      "Epoch 54/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 658.9405 - val_loss: 997.0853\n",
      "Epoch 55/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 628.2672 - val_loss: 993.8729\n",
      "Epoch 56/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 598.8746 - val_loss: 991.3980\n",
      "Epoch 57/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 570.7256 - val_loss: 989.6289\n",
      "Epoch 58/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 543.7842 - val_loss: 988.5341\n",
      "Epoch 59/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 518.0142 - val_loss: 988.0822\n",
      "Epoch 60/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 493.3800 - val_loss: 988.2429\n",
      "Epoch 61/500\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 469.8470 - val_loss: 988.9860\n",
      "Epoch 62/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 447.3809 - val_loss: 990.2822\n",
      "Epoch 63/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 425.9473 - val_loss: 992.1021\n",
      "Epoch 64/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 405.5129 - val_loss: 994.4168\n",
      "Epoch 65/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 386.0449 - val_loss: 997.1987\n",
      "Epoch 66/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 367.5106 - val_loss: 1000.4194\n",
      "Epoch 67/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 349.8780 - val_loss: 1004.0519\n",
      "Epoch 68/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 333.1162 - val_loss: 1008.0703\n",
      "Epoch 69/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 317.1936 - val_loss: 1012.4481\n",
      "Epoch 70/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 302.0805 - val_loss: 1017.1595\n",
      "Epoch 71/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 287.7468 - val_loss: 1022.1805\n",
      "Epoch 72/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 274.1633 - val_loss: 1027.4855\n",
      "Epoch 73/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 261.3012 - val_loss: 1033.0513\n",
      "Epoch 74/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 249.1327 - val_loss: 1038.8550\n",
      "Epoch 75/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 237.6299 - val_loss: 1044.8737\n",
      "Epoch 76/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 226.7661 - val_loss: 1051.0854\n",
      "Epoch 77/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 216.5146 - val_loss: 1057.4690\n",
      "Epoch 78/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 206.8499 - val_loss: 1064.0035\n",
      "Epoch 79/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 197.7467 - val_loss: 1070.6691\n",
      "Epoch 80/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 189.1803 - val_loss: 1077.4463\n",
      "Epoch 81/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 181.1268 - val_loss: 1084.3158\n",
      "Epoch 82/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 173.5627 - val_loss: 1091.2610\n",
      "Epoch 83/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 166.4655 - val_loss: 1098.2637\n",
      "Epoch 84/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 159.8129 - val_loss: 1105.3066\n",
      "Epoch 85/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 153.5835 - val_loss: 1112.3743\n",
      "Epoch 86/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 147.7565 - val_loss: 1119.4518\n",
      "Epoch 87/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 142.3114 - val_loss: 1126.5249\n",
      "Epoch 88/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 137.2287 - val_loss: 1133.5785\n",
      "Epoch 89/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 132.4895 - val_loss: 1140.5996\n",
      "Epoch 90/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 128.0755 - val_loss: 1147.5762\n",
      "Epoch 91/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 123.9691 - val_loss: 1154.4966\n",
      "Epoch 92/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 120.1529 - val_loss: 1161.3491\n",
      "Epoch 93/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 116.6106 - val_loss: 1168.1235\n",
      "Epoch 94/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 113.3265 - val_loss: 1174.8110\n",
      "Epoch 95/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 110.2853 - val_loss: 1181.4028\n",
      "Epoch 96/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 107.4724 - val_loss: 1187.8888\n",
      "Epoch 97/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 104.8739 - val_loss: 1194.2615\n",
      "Epoch 98/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 102.4764 - val_loss: 1200.5151\n",
      "Epoch 99/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 100.2671 - val_loss: 1206.6428\n",
      "Epoch 100/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 98.2337 - val_loss: 1212.6387\n",
      "Epoch 101/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 96.3648 - val_loss: 1218.4963\n",
      "Epoch 102/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 94.6491 - val_loss: 1224.2126\n",
      "Epoch 103/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 93.0762 - val_loss: 1229.7848\n",
      "Epoch 104/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 91.6360 - val_loss: 1235.2080\n",
      "Epoch 105/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 90.3192 - val_loss: 1240.4778\n",
      "Epoch 106/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 89.1168 - val_loss: 1245.5940\n",
      "Epoch 107/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 88.0203 - val_loss: 1250.5560\n",
      "Epoch 108/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 87.0218 - val_loss: 1255.3600\n",
      "Epoch 109/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 86.1137 - val_loss: 1260.0045\n",
      "Epoch 110/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 85.2890 - val_loss: 1264.4906\n",
      "Epoch 111/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 84.5411 - val_loss: 1268.8188\n",
      "Epoch 112/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.8637 - val_loss: 1272.9896\n",
      "Epoch 113/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.2511 - val_loss: 1277.0043\n",
      "Epoch 114/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.6979 - val_loss: 1280.8634\n",
      "Epoch 115/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.1989 - val_loss: 1284.5681\n",
      "Epoch 116/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7495 - val_loss: 1288.1206\n",
      "Epoch 117/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.3455 - val_loss: 1291.5234\n",
      "Epoch 118/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 80.9826 - val_loss: 1294.7782\n",
      "Epoch 119/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 80.6571 - val_loss: 1297.8887\n",
      "Epoch 120/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 80.3657 - val_loss: 1300.8553\n",
      "Epoch 121/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 80.1050 - val_loss: 1303.6831\n",
      "Epoch 122/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 79.8722 - val_loss: 1306.3757\n",
      "Epoch 123/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 79.6646 - val_loss: 1308.9354\n",
      "Epoch 124/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 79.4797 - val_loss: 1311.3654\n",
      "Epoch 125/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 79.3153 - val_loss: 1313.6696\n",
      "Epoch 126/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 79.1693 - val_loss: 1315.8508\n",
      "Epoch 127/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 79.0397 - val_loss: 1317.9143\n",
      "Epoch 128/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.9249 - val_loss: 1319.8638\n",
      "Epoch 129/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.8234 - val_loss: 1321.7023\n",
      "Epoch 130/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.7337 - val_loss: 1323.4335\n",
      "Epoch 131/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 78.6545 - val_loss: 1325.0609\n",
      "Epoch 132/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 78.5847 - val_loss: 1326.5883\n",
      "Epoch 133/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 78.5232 - val_loss: 1328.0190\n",
      "Epoch 134/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.4693 - val_loss: 1329.3566\n",
      "Epoch 135/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.4218 - val_loss: 1330.6035\n",
      "Epoch 136/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.3802 - val_loss: 1331.7621\n",
      "Epoch 137/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3438 - val_loss: 1332.8322\n",
      "Epoch 138/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.3119 - val_loss: 1333.8152\n",
      "Epoch 139/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.2840 - val_loss: 1334.7059\n",
      "Epoch 140/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.2597 - val_loss: 1335.4941\n",
      "Epoch 141/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.2385 - val_loss: 1336.1287\n",
      "Epoch 142/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2198 - val_loss: 1313.5697\n",
      "Epoch 143/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.6722 - val_loss: 1343.3334\n",
      "Epoch 144/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1890 - val_loss: 1344.0085\n",
      "Epoch 145/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1768 - val_loss: 1344.6282\n",
      "Epoch 146/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1666 - val_loss: 1345.1986\n",
      "Epoch 147/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1577 - val_loss: 1345.7241\n",
      "Epoch 148/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1502 - val_loss: 1346.2070\n",
      "Epoch 149/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1437 - val_loss: 1346.6486\n",
      "Epoch 150/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1382 - val_loss: 1347.0533\n",
      "Epoch 151/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 78.1336 - val_loss: 1347.4220\n",
      "Epoch 152/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.1296 - val_loss: 1347.7573\n",
      "Epoch 153/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.1263 - val_loss: 1348.0607\n",
      "Epoch 154/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.1236 - val_loss: 1348.3357\n",
      "Epoch 155/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1215 - val_loss: 1348.5837\n",
      "Epoch 156/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1197 - val_loss: 1348.8049\n",
      "Epoch 157/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1183 - val_loss: 1349.0018\n",
      "Epoch 158/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1174 - val_loss: 1349.1759\n",
      "Epoch 159/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1166 - val_loss: 1349.3278\n",
      "Epoch 160/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1163 - val_loss: 1349.4601\n",
      "Epoch 161/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1161 - val_loss: 1349.5729\n",
      "Epoch 162/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1161 - val_loss: 1349.6671\n",
      "Epoch 163/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1163 - val_loss: 1349.7434\n",
      "Epoch 164/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1167 - val_loss: 1349.8030\n",
      "Epoch 165/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1173 - val_loss: 1349.8462\n",
      "Epoch 166/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1180 - val_loss: 1349.8722\n",
      "Epoch 167/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1188 - val_loss: 1349.8817\n",
      "Epoch 168/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1197 - val_loss: 1349.8733\n",
      "Epoch 169/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1207 - val_loss: 1349.8453\n",
      "Epoch 170/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1219 - val_loss: 1349.7979\n",
      "Epoch 171/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1230 - val_loss: 1349.7241\n",
      "Epoch 172/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 78.1243 - val_loss: 1349.6207\n",
      "Epoch 173/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.1256 - val_loss: 1349.4730\n",
      "Epoch 174/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1270 - val_loss: 1349.2606\n",
      "Epoch 175/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1274 - val_loss: 1344.2024\n",
      "Epoch 176/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.4209 - val_loss: 1349.6327\n",
      "Epoch 177/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1309 - val_loss: 1349.1235\n",
      "Epoch 178/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1320 - val_loss: 1347.8450\n",
      "Epoch 179/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1170 - val_loss: 1340.6658\n",
      "Epoch 180/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1437 - val_loss: 1352.5414\n",
      "Epoch 181/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1371 - val_loss: 1352.7494\n",
      "Epoch 182/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1387 - val_loss: 1352.7456\n",
      "Epoch 183/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1404 - val_loss: 1352.7245\n",
      "Epoch 184/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1421 - val_loss: 1352.6997\n",
      "Epoch 185/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1438 - val_loss: 1352.6726\n",
      "Epoch 186/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1455 - val_loss: 1352.6417\n",
      "Epoch 187/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1472 - val_loss: 1352.6082\n",
      "Epoch 188/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1489 - val_loss: 1352.5710\n",
      "Epoch 189/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1506 - val_loss: 1352.5299\n",
      "Epoch 190/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1524 - val_loss: 1352.4858\n",
      "Epoch 191/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1540 - val_loss: 1352.4365\n",
      "Epoch 192/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1557 - val_loss: 1352.3772\n",
      "Epoch 193/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 78.1572 - val_loss: 1351.9929\n",
      "Epoch 194/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3081 - val_loss: 1352.4386\n",
      "Epoch 195/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1605 - val_loss: 1352.3500\n",
      "Epoch 196/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1619 - val_loss: 1352.2396\n",
      "Epoch 197/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1636 - val_loss: 1352.0977\n",
      "Epoch 198/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1653 - val_loss: 1351.8940\n",
      "Epoch 199/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1669 - val_loss: 1351.4885\n",
      "Epoch 200/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.1646 - val_loss: 1344.6007\n",
      "Epoch 201/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2159 - val_loss: 1361.8378\n",
      "Epoch 202/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.1717 - val_loss: 1361.8229\n",
      "Epoch 203/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1732 - val_loss: 1361.8069\n",
      "Epoch 204/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1748 - val_loss: 1361.7903\n",
      "Epoch 205/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1764 - val_loss: 1361.7733\n",
      "Epoch 206/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1780 - val_loss: 1361.7540\n",
      "Epoch 207/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.1795 - val_loss: 1361.7279\n",
      "Epoch 208/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1810 - val_loss: 1361.6429\n",
      "Epoch 209/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1825 - val_loss: 1352.5336\n",
      "Epoch 210/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1841 - val_loss: 1361.4117\n",
      "Epoch 211/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.1854 - val_loss: 1361.3982\n",
      "Epoch 212/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1869 - val_loss: 1361.3838\n",
      "Epoch 213/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.1882 - val_loss: 1361.3695\n",
      "Epoch 214/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1896 - val_loss: 1361.3552\n",
      "Epoch 215/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1910 - val_loss: 1361.3407\n",
      "Epoch 216/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1923 - val_loss: 1361.3262\n",
      "Epoch 217/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1936 - val_loss: 1361.3104\n",
      "Epoch 218/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1949 - val_loss: 1361.2955\n",
      "Epoch 219/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 78.1962 - val_loss: 1361.2792\n",
      "Epoch 220/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1975 - val_loss: 1361.2627\n",
      "Epoch 221/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.1988 - val_loss: 1361.2461\n",
      "Epoch 222/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2000 - val_loss: 1361.2284\n",
      "Epoch 223/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2012 - val_loss: 1361.2097\n",
      "Epoch 224/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2023 - val_loss: 1361.1897\n",
      "Epoch 225/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2035 - val_loss: 1361.1674\n",
      "Epoch 226/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2047 - val_loss: 1361.1423\n",
      "Epoch 227/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.2058 - val_loss: 1361.1108\n",
      "Epoch 228/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2069 - val_loss: 1361.0637\n",
      "Epoch 229/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.2079 - val_loss: 1360.9768\n",
      "Epoch 230/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2084 - val_loss: 1360.8594\n",
      "Epoch 231/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2087 - val_loss: 1360.6575\n",
      "Epoch 232/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1048 - val_loss: 1359.7717\n",
      "Epoch 233/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3787 - val_loss: 1360.0020\n",
      "Epoch 234/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2866 - val_loss: 1360.4048\n",
      "Epoch 235/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2708 - val_loss: 1360.7091\n",
      "Epoch 236/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2621 - val_loss: 1360.9521\n",
      "Epoch 237/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2560 - val_loss: 1361.1469\n",
      "Epoch 238/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2516 - val_loss: 1361.3021\n",
      "Epoch 239/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2481 - val_loss: 1361.4230\n",
      "Epoch 240/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 78.2456 - val_loss: 1361.5155\n",
      "Epoch 241/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2437 - val_loss: 1361.5823\n",
      "Epoch 242/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2422 - val_loss: 1361.6268\n",
      "Epoch 243/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2411 - val_loss: 1361.6492\n",
      "Epoch 244/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2403 - val_loss: 1361.6495\n",
      "Epoch 245/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2397 - val_loss: 1361.6265\n",
      "Epoch 246/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2393 - val_loss: 1361.5731\n",
      "Epoch 247/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2392 - val_loss: 1361.4766\n",
      "Epoch 248/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2390 - val_loss: 1361.3059\n",
      "Epoch 249/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2390 - val_loss: 1360.9875\n",
      "Epoch 250/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2362 - val_loss: 1359.1191\n",
      "Epoch 251/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 77.0748 - val_loss: 1324.1733\n",
      "Epoch 252/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 80.8773 - val_loss: 1315.2030\n",
      "Epoch 253/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 80.4061 - val_loss: 1322.4441\n",
      "Epoch 254/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 79.8537 - val_loss: 1328.6907\n",
      "Epoch 255/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 79.4413 - val_loss: 1333.9525\n",
      "Epoch 256/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 79.1405 - val_loss: 1338.3711\n",
      "Epoch 257/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.9203 - val_loss: 1342.0747\n",
      "Epoch 258/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.7580 - val_loss: 1345.1750\n",
      "Epoch 259/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.6375 - val_loss: 1347.7671\n",
      "Epoch 260/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.5475 - val_loss: 1349.9320\n",
      "Epoch 261/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.4795 - val_loss: 1351.7372\n",
      "Epoch 262/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.4280 - val_loss: 1353.2415\n",
      "Epoch 263/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3885 - val_loss: 1354.4937\n",
      "Epoch 264/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 78.3581 - val_loss: 1355.5354\n",
      "Epoch 265/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3345 - val_loss: 1356.4000\n",
      "Epoch 266/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3160 - val_loss: 1357.1173\n",
      "Epoch 267/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3015 - val_loss: 1357.7106\n",
      "Epoch 268/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2900 - val_loss: 1358.2015\n",
      "Epoch 269/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2810 - val_loss: 1358.6069\n",
      "Epoch 270/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2737 - val_loss: 1358.9399\n",
      "Epoch 271/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2679 - val_loss: 1359.2124\n",
      "Epoch 272/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2633 - val_loss: 1359.4357\n",
      "Epoch 273/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2596 - val_loss: 1359.6163\n",
      "Epoch 274/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2566 - val_loss: 1359.7628\n",
      "Epoch 275/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2542 - val_loss: 1359.8785\n",
      "Epoch 276/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2523 - val_loss: 1359.9691\n",
      "Epoch 277/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2508 - val_loss: 1360.0385\n",
      "Epoch 278/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2496 - val_loss: 1360.0887\n",
      "Epoch 279/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2488 - val_loss: 1360.1228\n",
      "Epoch 280/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2480 - val_loss: 1360.1406\n",
      "Epoch 281/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2475 - val_loss: 1360.1421\n",
      "Epoch 282/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2472 - val_loss: 1360.1206\n",
      "Epoch 283/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2468 - val_loss: 1360.0093\n",
      "Epoch 284/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2393 - val_loss: 1356.9462\n",
      "Epoch 285/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.4882 - val_loss: 1356.8356\n",
      "Epoch 286/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3322 - val_loss: 1358.6559\n",
      "Epoch 287/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3014 - val_loss: 1358.9460\n",
      "Epoch 288/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 78.2913 - val_loss: 1359.1599\n",
      "Epoch 289/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2843 - val_loss: 1359.3159\n",
      "Epoch 290/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2791 - val_loss: 1359.4225\n",
      "Epoch 291/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2751 - val_loss: 1359.4821\n",
      "Epoch 292/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2720 - val_loss: 1359.4968\n",
      "Epoch 293/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2695 - val_loss: 1359.4568\n",
      "Epoch 294/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2675 - val_loss: 1359.3312\n",
      "Epoch 295/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2659 - val_loss: 1359.0004\n",
      "Epoch 296/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2643 - val_loss: 1355.3591\n",
      "Epoch 297/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 77.0315 - val_loss: 1315.7285\n",
      "Epoch 298/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 80.9677 - val_loss: 1314.8154\n",
      "Epoch 299/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 80.4614 - val_loss: 1322.6195\n",
      "Epoch 300/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 79.8726 - val_loss: 1329.2719\n",
      "Epoch 301/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 79.4409 - val_loss: 1334.8042\n",
      "Epoch 302/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 79.1319 - val_loss: 1339.3917\n",
      "Epoch 303/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.9093 - val_loss: 1343.1895\n",
      "Epoch 304/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 78.7477 - val_loss: 1346.3311\n",
      "Epoch 305/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.6293 - val_loss: 1348.9276\n",
      "Epoch 306/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.5417 - val_loss: 1351.0720\n",
      "Epoch 307/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.4764 - val_loss: 1352.8418\n",
      "Epoch 308/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.4271 - val_loss: 1354.3019\n",
      "Epoch 309/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3897 - val_loss: 1355.5057\n",
      "Epoch 310/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3611 - val_loss: 1356.4973\n",
      "Epoch 311/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3388 - val_loss: 1357.3142\n",
      "Epoch 312/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3215 - val_loss: 1357.9868\n",
      "Epoch 313/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3079 - val_loss: 1358.5402\n",
      "Epoch 314/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2972 - val_loss: 1358.9948\n",
      "Epoch 315/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2887 - val_loss: 1359.3679\n",
      "Epoch 316/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2818 - val_loss: 1359.6744\n",
      "Epoch 317/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2764 - val_loss: 1359.9259\n",
      "Epoch 318/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2720 - val_loss: 1360.1309\n",
      "Epoch 319/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2685 - val_loss: 1360.2986\n",
      "Epoch 320/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2656 - val_loss: 1360.4357\n",
      "Epoch 321/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2634 - val_loss: 1360.5466\n",
      "Epoch 322/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2615 - val_loss: 1360.6360\n",
      "Epoch 323/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 78.2599 - val_loss: 1360.7079\n",
      "Epoch 324/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2587 - val_loss: 1360.7643\n",
      "Epoch 325/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2577 - val_loss: 1360.8094\n",
      "Epoch 326/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2569 - val_loss: 1360.8435\n",
      "Epoch 327/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2563 - val_loss: 1360.8682\n",
      "Epoch 328/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2557 - val_loss: 1360.8842\n",
      "Epoch 329/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2553 - val_loss: 1360.8928\n",
      "Epoch 330/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2550 - val_loss: 1360.8918\n",
      "Epoch 331/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2547 - val_loss: 1360.8795\n",
      "Epoch 332/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2545 - val_loss: 1360.8448\n",
      "Epoch 333/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2542 - val_loss: 1360.5724\n",
      "Epoch 334/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 77.8308 - val_loss: 1311.4717\n",
      "Epoch 335/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.0636 - val_loss: 1312.4447\n",
      "Epoch 336/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 80.5340 - val_loss: 1320.2612\n",
      "Epoch 337/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 79.9304 - val_loss: 1326.9469\n",
      "Epoch 338/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 79.4865 - val_loss: 1332.5292\n",
      "Epoch 339/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 79.1671 - val_loss: 1337.1761\n",
      "Epoch 340/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.9361 - val_loss: 1341.0374\n",
      "Epoch 341/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.7679 - val_loss: 1344.2399\n",
      "Epoch 342/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 78.6441 - val_loss: 1346.8856\n",
      "Epoch 343/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.5524 - val_loss: 1349.0437\n",
      "Epoch 344/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.4835 - val_loss: 1350.7499\n",
      "Epoch 345/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.4319 - val_loss: 1352.3062\n",
      "Epoch 346/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3918 - val_loss: 1353.4789\n",
      "Epoch 347/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3611 - val_loss: 1354.4041\n",
      "Epoch 348/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3565 - val_loss: 1355.5972\n",
      "Epoch 349/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3206 - val_loss: 1356.3002\n",
      "Epoch 350/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3062 - val_loss: 1356.8793\n",
      "Epoch 351/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2949 - val_loss: 1357.3562\n",
      "Epoch 352/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2859 - val_loss: 1357.7507\n",
      "Epoch 353/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2786 - val_loss: 1357.9965\n",
      "Epoch 354/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2381 - val_loss: 1350.2454\n",
      "Epoch 355/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 80.0518 - val_loss: 1307.7828\n",
      "Epoch 356/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 80.6157 - val_loss: 1315.5818\n",
      "Epoch 357/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 79.9979 - val_loss: 1322.2965\n",
      "Epoch 358/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 79.5396 - val_loss: 1327.9271\n",
      "Epoch 359/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 79.2082 - val_loss: 1332.6317\n",
      "Epoch 360/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.9675 - val_loss: 1336.5585\n",
      "Epoch 361/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.7915 - val_loss: 1339.8304\n",
      "Epoch 362/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.6617 - val_loss: 1342.5563\n",
      "Epoch 363/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.5652 - val_loss: 1344.8237\n",
      "Epoch 364/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 78.4928 - val_loss: 1346.7086\n",
      "Epoch 365/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.4381 - val_loss: 1348.2745\n",
      "Epoch 366/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3964 - val_loss: 1349.5750\n",
      "Epoch 367/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3643 - val_loss: 1350.6527\n",
      "Epoch 368/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3393 - val_loss: 1351.5465\n",
      "Epoch 369/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3199 - val_loss: 1352.2871\n",
      "Epoch 370/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3045 - val_loss: 1352.8989\n",
      "Epoch 371/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2924 - val_loss: 1353.4043\n",
      "Epoch 372/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2827 - val_loss: 1353.8219\n",
      "Epoch 373/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2751 - val_loss: 1354.1663\n",
      "Epoch 374/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2688 - val_loss: 1354.4489\n",
      "Epoch 375/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2637 - val_loss: 1354.6793\n",
      "Epoch 376/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2597 - val_loss: 1354.8669\n",
      "Epoch 377/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2563 - val_loss: 1355.0175\n",
      "Epoch 378/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2536 - val_loss: 1355.1377\n",
      "Epoch 379/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2515 - val_loss: 1355.2312\n",
      "Epoch 380/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2496 - val_loss: 1355.3011\n",
      "Epoch 381/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2482 - val_loss: 1355.3506\n",
      "Epoch 382/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 78.2469 - val_loss: 1355.3705\n",
      "Epoch 383/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 78.1778 - val_loss: 1331.3085\n",
      "Epoch 384/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.1744 - val_loss: 1301.3364\n",
      "Epoch 385/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 80.6414 - val_loss: 1308.9778\n",
      "Epoch 386/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 80.0219 - val_loss: 1315.5150\n",
      "Epoch 387/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 79.5615 - val_loss: 1319.4182\n",
      "Epoch 388/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.1622 - val_loss: 1286.3767\n",
      "Epoch 389/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.7110 - val_loss: 1293.1799\n",
      "Epoch 390/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.4847 - val_loss: 1303.5317\n",
      "Epoch 391/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.3599 - val_loss: 1312.4612\n",
      "Epoch 392/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 80.5366 - val_loss: 1320.0305\n",
      "Epoch 393/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 79.9409 - val_loss: 1326.4283\n",
      "Epoch 394/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 79.5080 - val_loss: 1331.8267\n",
      "Epoch 395/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 79.1917 - val_loss: 1336.3748\n",
      "Epoch 396/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.9593 - val_loss: 1340.2031\n",
      "Epoch 397/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.7871 - val_loss: 1343.4210\n",
      "Epoch 398/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.6589 - val_loss: 1346.1249\n",
      "Epoch 399/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.5626 - val_loss: 1348.3937\n",
      "Epoch 400/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.4897 - val_loss: 1350.2977\n",
      "Epoch 401/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.4342 - val_loss: 1351.8948\n",
      "Epoch 402/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3914 - val_loss: 1353.2319\n",
      "Epoch 403/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3583 - val_loss: 1354.3527\n",
      "Epoch 404/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3325 - val_loss: 1355.2914\n",
      "Epoch 405/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 78.3122 - val_loss: 1356.0765\n",
      "Epoch 406/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.2961 - val_loss: 1356.7339\n",
      "Epoch 407/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2834 - val_loss: 1357.2848\n",
      "Epoch 408/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2731 - val_loss: 1357.7446\n",
      "Epoch 409/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2648 - val_loss: 1358.1290\n",
      "Epoch 410/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2582 - val_loss: 1358.4497\n",
      "Epoch 411/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2527 - val_loss: 1358.7140\n",
      "Epoch 412/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2483 - val_loss: 1358.8804\n",
      "Epoch 413/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2366 - val_loss: 1346.9019\n",
      "Epoch 414/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 79.9787 - val_loss: 1306.9615\n",
      "Epoch 415/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 80.5794 - val_loss: 1314.2764\n",
      "Epoch 416/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 79.9944 - val_loss: 1320.6456\n",
      "Epoch 417/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 79.5524 - val_loss: 1326.0463\n",
      "Epoch 418/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 79.2273 - val_loss: 1330.6139\n",
      "Epoch 419/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.9871 - val_loss: 1334.4713\n",
      "Epoch 420/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.8088 - val_loss: 1337.7261\n",
      "Epoch 421/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.6753 - val_loss: 1340.4695\n",
      "Epoch 422/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.5749 - val_loss: 1342.7802\n",
      "Epoch 423/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.4987 - val_loss: 1344.7268\n",
      "Epoch 424/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 78.4403 - val_loss: 1346.3640\n",
      "Epoch 425/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3954 - val_loss: 1347.7405\n",
      "Epoch 426/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3606 - val_loss: 1348.8984\n",
      "Epoch 427/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3334 - val_loss: 1349.8718\n",
      "Epoch 428/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3118 - val_loss: 1350.6882\n",
      "Epoch 429/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2950 - val_loss: 1351.3761\n",
      "Epoch 430/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2813 - val_loss: 1351.9530\n",
      "Epoch 431/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2704 - val_loss: 1352.4373\n",
      "Epoch 432/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2617 - val_loss: 1352.8446\n",
      "Epoch 433/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2546 - val_loss: 1353.1857\n",
      "Epoch 434/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2487 - val_loss: 1353.4719\n",
      "Epoch 435/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2440 - val_loss: 1353.7120\n",
      "Epoch 436/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2401 - val_loss: 1353.9141\n",
      "Epoch 437/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2369 - val_loss: 1354.0820\n",
      "Epoch 438/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.2344 - val_loss: 1354.2240\n",
      "Epoch 439/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2322 - val_loss: 1354.3435\n",
      "Epoch 440/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2304 - val_loss: 1354.4435\n",
      "Epoch 441/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2289 - val_loss: 1354.5272\n",
      "Epoch 442/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2276 - val_loss: 1354.5973\n",
      "Epoch 443/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 78.2267 - val_loss: 1354.6561\n",
      "Epoch 444/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2258 - val_loss: 1354.7042\n",
      "Epoch 445/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2251 - val_loss: 1354.7446\n",
      "Epoch 446/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2247 - val_loss: 1354.7802\n",
      "Epoch 447/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2241 - val_loss: 1354.8087\n",
      "Epoch 448/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2237 - val_loss: 1354.8334\n",
      "Epoch 449/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2234 - val_loss: 1354.8534\n",
      "Epoch 450/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2231 - val_loss: 1354.8698\n",
      "Epoch 451/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2229 - val_loss: 1354.8833\n",
      "Epoch 452/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2228 - val_loss: 1354.8955\n",
      "Epoch 453/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2226 - val_loss: 1354.9052\n",
      "Epoch 454/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2225 - val_loss: 1354.9136\n",
      "Epoch 455/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2225 - val_loss: 1354.9209\n",
      "Epoch 456/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2223 - val_loss: 1354.9260\n",
      "Epoch 457/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2222 - val_loss: 1354.9287\n",
      "Epoch 458/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2222 - val_loss: 1354.9308\n",
      "Epoch 459/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2221 - val_loss: 1354.9315\n",
      "Epoch 460/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2221 - val_loss: 1354.9291\n",
      "Epoch 461/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 78.2221 - val_loss: 1354.9133\n",
      "Epoch 462/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.2003 - val_loss: 1351.2037\n",
      "Epoch 463/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.4705 - val_loss: 1352.0978\n",
      "Epoch 464/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.3089 - val_loss: 1352.6570\n",
      "Epoch 465/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2872 - val_loss: 1353.0665\n",
      "Epoch 466/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2755 - val_loss: 1353.3977\n",
      "Epoch 467/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2670 - val_loss: 1353.6681\n",
      "Epoch 468/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2605 - val_loss: 1353.8906\n",
      "Epoch 469/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2555 - val_loss: 1354.0748\n",
      "Epoch 470/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2514 - val_loss: 1354.2275\n",
      "Epoch 471/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2481 - val_loss: 1354.3535\n",
      "Epoch 472/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2454 - val_loss: 1354.4584\n",
      "Epoch 473/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2432 - val_loss: 1354.5455\n",
      "Epoch 474/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2413 - val_loss: 1354.6173\n",
      "Epoch 475/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2398 - val_loss: 1354.6771\n",
      "Epoch 476/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2385 - val_loss: 1354.7279\n",
      "Epoch 477/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2375 - val_loss: 1354.7698\n",
      "Epoch 478/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2364 - val_loss: 1354.8038\n",
      "Epoch 479/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2358 - val_loss: 1354.8339\n",
      "Epoch 480/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2350 - val_loss: 1354.8573\n",
      "Epoch 481/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.2345 - val_loss: 1354.8782\n",
      "Epoch 482/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2340 - val_loss: 1354.8940\n",
      "Epoch 483/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2336 - val_loss: 1354.9086\n",
      "Epoch 484/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2333 - val_loss: 1354.9207\n",
      "Epoch 485/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2329 - val_loss: 1354.9301\n",
      "Epoch 486/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2326 - val_loss: 1354.9384\n",
      "Epoch 487/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2323 - val_loss: 1354.9443\n",
      "Epoch 488/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2322 - val_loss: 1354.9498\n",
      "Epoch 489/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2319 - val_loss: 1354.9536\n",
      "Epoch 490/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.2318 - val_loss: 1354.9565\n",
      "Epoch 491/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2317 - val_loss: 1354.9589\n",
      "Epoch 492/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2315 - val_loss: 1354.9581\n",
      "Epoch 493/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2314 - val_loss: 1354.9487\n",
      "Epoch 494/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2313 - val_loss: 1354.8595\n",
      "Epoch 495/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2313 - val_loss: 1354.9760\n",
      "Epoch 496/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 78.2311 - val_loss: 1354.9777\n",
      "Epoch 497/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 78.2310 - val_loss: 1354.9792\n",
      "Epoch 498/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2310 - val_loss: 1354.9807\n",
      "Epoch 499/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2309 - val_loss: 1354.9822\n",
      "Epoch 500/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 78.2308 - val_loss: 1354.9835\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.0403, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0012, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0120, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.0334, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 375ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 126)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 126)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 186)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:150:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 7.33543880e+01, 7.30277078e+01, 7.26999767e+01,\n",
       "        7.24312092e+01, 7.22547386e+01, 7.20782680e+01, 7.19017974e+01,\n",
       "        7.17893324e+01, 7.17641223e+01, 1.47510827e-01, 4.24880058e-01,\n",
       "        7.23266340e+01, 7.21501634e+01, 7.19736928e+01, 7.17996032e+01,\n",
       "        7.17743931e+01, 7.14918300e+01, 7.12397290e+01, 7.10938142e+01,\n",
       "        6.96776380e+01, 4.20036280e-02, 1.45638764e-01, 3.46528587e+01,\n",
       "        2.25153655e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.13487697e-01, 0.00000000e+00, 4.99015312e+01, 2.81719685e-01,\n",
       "        3.71721298e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 8.93081367e-01, 0.00000000e+00,\n",
       "        3.34201008e-01, 2.79544622e-01, 8.55977595e-01, 4.21371549e-01,\n",
       "        1.59168541e-01, 0.00000000e+00, 9.46579397e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.26286614e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([67.30798661, 67.30170117, 67.29541573, 67.28913029, 67.28284485,\n",
       "       67.2765594 , 67.27027396, 67.26398852, 67.25770308, 67.25141764,\n",
       "       67.2451322 , 67.23884676, 67.23256132, 67.22627588, 67.21999044,\n",
       "       67.21370499, 67.20741955, 67.20113411, 67.19484867, 67.18856323,\n",
       "       67.18227779, 67.17599235, 67.16970691, 67.16342147, 67.15713603,\n",
       "       67.15085058, 67.14456514, 67.1382797 , 67.13199426, 67.12570882,\n",
       "       67.11942338, 67.11313794, 67.1068525 , 67.10056706, 67.09428162,\n",
       "       67.08799617, 67.08171073, 67.07542529, 67.06913985, 67.06285441,\n",
       "       67.05656897, 67.05028353, 67.04399809, 67.03771265, 67.03142721,\n",
       "       67.02514176, 67.01885632, 67.01257088, 67.00628544, 67.        ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.151673701705164\n",
      "42.81788731772587\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
