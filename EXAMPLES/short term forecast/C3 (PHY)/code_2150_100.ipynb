{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "2245    68.596502\n",
       "2246    68.590217\n",
       "2247    68.583931\n",
       "2248    68.577646\n",
       "2249    68.571360\n",
       "Name: C3, Length: 2250, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c3_interpolated_2150_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "2145     0.000000\n",
       "2146     0.197619\n",
       "2147     0.000000\n",
       "2148     0.027514\n",
       "2149     0.000000\n",
       "Name: C3, Length: 2150, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2150)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.100000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.875910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.651821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90.427731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90.203641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     91.100000  0.000298\n",
       "1     90.875910  0.000298\n",
       "2     90.651821  0.000297\n",
       "3     90.427731  0.000297\n",
       "4     90.203641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.0403  # Value for C0\n",
    "K0 = -0.0012  # Value for K0\n",
    "K1 = -0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0120    # Value for b\n",
    "c = 2.0334    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    91.100000    90.875910    90.651821    90.427731    90.203641   \n",
      "351    90.875910    90.651821    90.427731    90.203641    89.979552   \n",
      "352    90.651821    90.427731    90.203641    89.979552    89.755462   \n",
      "353    90.427731    90.203641    89.979552    89.755462    89.531373   \n",
      "354    90.203641    89.979552    89.755462    89.531373    89.307283   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    89.979552    89.755462    89.531373    89.307283    89.094118  ...   \n",
      "351    89.755462    89.531373    89.307283    89.094118    89.015686  ...   \n",
      "352    89.531373    89.307283    89.094118    89.015686    88.937255  ...   \n",
      "353    89.307283    89.094118    89.015686    88.937255    88.858824  ...   \n",
      "354    89.094118    89.015686    88.937255    88.858824    88.780392  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   81.423343    0.000263   81.404669    0.000263   81.385994    0.000263   \n",
      "351   81.404669    0.000263   81.385994    0.000263   81.367320    0.000262   \n",
      "352   81.385994    0.000263   81.367320    0.000262   81.348646    0.000262   \n",
      "353   81.367320    0.000262   81.348646    0.000262   81.329972    0.000262   \n",
      "354   81.348646    0.000262   81.329972    0.000262   81.311298    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   81.367320    0.000262   81.348646    0.000262  \n",
      "351   81.348646    0.000262   81.329972    0.000262  \n",
      "352   81.329972    0.000262   81.311298    0.000262  \n",
      "353   81.311298    0.000262   81.292624    0.000262  \n",
      "354   81.292624    0.000262   81.273950    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1700, 1, 251) (1700, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 2s 26ms/step - loss: 5718.4072 - val_loss: 4413.0425\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 5616.2227 - val_loss: 4327.5352\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5510.4941 - val_loss: 4253.7319\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5414.6895 - val_loss: 4173.1196\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5314.6841 - val_loss: 4098.0146\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5220.9414 - val_loss: 4024.6272\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5129.1064 - val_loss: 3952.7368\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5038.8906 - val_loss: 3882.1440\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4950.1099 - val_loss: 3812.7397\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4862.6616 - val_loss: 3744.4526\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4775.7109 - val_loss: 3668.3923\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4674.1821 - val_loss: 3598.3052\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4584.3027 - val_loss: 3529.9062\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4496.8535 - val_loss: 3463.2102\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4411.4395 - val_loss: 3397.9331\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4327.7192 - val_loss: 3333.9214\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4245.5034 - val_loss: 3271.0820\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4164.6816 - val_loss: 3209.3577\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4085.1765 - val_loss: 3148.7056\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4006.9360 - val_loss: 3089.0916\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3929.9165 - val_loss: 3030.4900\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3854.0857 - val_loss: 2972.8789\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3779.4148 - val_loss: 2916.2375\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3705.8792 - val_loss: 2860.5508\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3633.4573 - val_loss: 2805.8184\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3553.6978 - val_loss: 2740.1790\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3473.1460 - val_loss: 2681.7461\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3397.6089 - val_loss: 2625.5505\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3324.5283 - val_loss: 2571.0774\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3253.3611 - val_loss: 2518.0269\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3183.8098 - val_loss: 2466.2310\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3115.7051 - val_loss: 2415.5874\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 3048.9375 - val_loss: 2366.0276\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2983.4307 - val_loss: 2317.4998\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2919.1282 - val_loss: 2269.9639\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2855.9836 - val_loss: 2223.3884\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2793.9619 - val_loss: 2177.7454\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2733.0308 - val_loss: 2133.0112\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2673.1636 - val_loss: 2089.1655\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2614.3367 - val_loss: 2046.1893\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2556.5283 - val_loss: 2004.0660\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2499.7190 - val_loss: 1962.7798\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2443.8911 - val_loss: 1922.3154\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2389.0273 - val_loss: 1882.6595\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2335.1121 - val_loss: 1843.7991\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2282.1311 - val_loss: 1805.7216\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2230.0696 - val_loss: 1768.4154\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2178.9143 - val_loss: 1731.8682\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2128.6514 - val_loss: 1696.0690\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2079.2695 - val_loss: 1661.0079\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2030.7559 - val_loss: 1626.6732\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1983.0991 - val_loss: 1593.0559\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1936.2871 - val_loss: 1560.1454\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1890.3096 - val_loss: 1527.9324\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1845.1555 - val_loss: 1496.4066\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1800.8135 - val_loss: 1465.5592\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1757.2737 - val_loss: 1435.3806\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1714.5255 - val_loss: 1405.8618\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1672.5593 - val_loss: 1376.9945\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1631.3655 - val_loss: 1348.7690\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1590.9332 - val_loss: 1321.1771\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1551.2540 - val_loss: 1294.2108\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1512.3176 - val_loss: 1267.8599\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1474.1149 - val_loss: 1242.1172\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1436.6366 - val_loss: 1216.9745\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1399.8737 - val_loss: 1192.4237\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1363.8171 - val_loss: 1168.4556\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1328.4578 - val_loss: 1145.0627\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1293.7874 - val_loss: 1122.2379\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1259.7968 - val_loss: 1099.9722\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1226.4773 - val_loss: 1078.2579\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1193.8208 - val_loss: 1057.0881\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1161.8182 - val_loss: 1036.4537\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1130.4615 - val_loss: 1016.3477\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1099.7419 - val_loss: 996.7626\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1069.6520 - val_loss: 977.6910\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1040.1826 - val_loss: 959.1246\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1011.3260 - val_loss: 941.0567\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 983.0741 - val_loss: 923.4800\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 955.4188 - val_loss: 906.3862\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 928.3523 - val_loss: 889.7692\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 901.8663 - val_loss: 873.6205\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 875.9529 - val_loss: 857.9337\n",
      "Epoch 84/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 850.6047 - val_loss: 842.7006\n",
      "Epoch 85/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 825.8134 - val_loss: 827.9151\n",
      "Epoch 86/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 801.5718 - val_loss: 813.5698\n",
      "Epoch 87/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 777.8717 - val_loss: 799.6575\n",
      "Epoch 88/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 754.7059 - val_loss: 786.1710\n",
      "Epoch 89/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 732.0665 - val_loss: 773.1037\n",
      "Epoch 90/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 709.9463 - val_loss: 760.4482\n",
      "Epoch 91/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 688.3373 - val_loss: 748.1976\n",
      "Epoch 92/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 667.2321 - val_loss: 736.3448\n",
      "Epoch 93/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 646.6234 - val_loss: 724.8835\n",
      "Epoch 94/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 626.5044 - val_loss: 713.8069\n",
      "Epoch 95/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 606.8669 - val_loss: 703.1074\n",
      "Epoch 96/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 587.7038 - val_loss: 692.7789\n",
      "Epoch 97/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 569.0078 - val_loss: 682.8143\n",
      "Epoch 98/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 550.7720 - val_loss: 673.2068\n",
      "Epoch 99/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 532.9893 - val_loss: 663.9506\n",
      "Epoch 100/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 515.6520 - val_loss: 655.0380\n",
      "Epoch 101/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 498.7532 - val_loss: 646.4628\n",
      "Epoch 102/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 482.2858 - val_loss: 638.2184\n",
      "Epoch 103/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 466.2429 - val_loss: 630.2984\n",
      "Epoch 104/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 450.6174 - val_loss: 622.6960\n",
      "Epoch 105/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 435.4024 - val_loss: 615.4052\n",
      "Epoch 106/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 420.5911 - val_loss: 608.4191\n",
      "Epoch 107/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 406.1766 - val_loss: 601.7313\n",
      "Epoch 108/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 392.1516 - val_loss: 595.3356\n",
      "Epoch 109/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 378.5095 - val_loss: 589.2255\n",
      "Epoch 110/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 365.2438 - val_loss: 583.3952\n",
      "Epoch 111/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 352.3475 - val_loss: 577.8380\n",
      "Epoch 112/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 339.8141 - val_loss: 572.5475\n",
      "Epoch 113/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 327.6368 - val_loss: 567.5179\n",
      "Epoch 114/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 315.8091 - val_loss: 562.7430\n",
      "Epoch 115/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 304.3244 - val_loss: 558.2164\n",
      "Epoch 116/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 293.1760 - val_loss: 553.9324\n",
      "Epoch 117/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 282.3575 - val_loss: 549.8846\n",
      "Epoch 118/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 271.8625 - val_loss: 546.0672\n",
      "Epoch 119/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 261.6844 - val_loss: 542.4741\n",
      "Epoch 120/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 251.8170 - val_loss: 539.0997\n",
      "Epoch 121/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 242.2541 - val_loss: 535.9379\n",
      "Epoch 122/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 232.9892 - val_loss: 532.9830\n",
      "Epoch 123/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 224.0163 - val_loss: 530.2292\n",
      "Epoch 124/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 215.3292 - val_loss: 527.6707\n",
      "Epoch 125/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 206.9214 - val_loss: 525.3019\n",
      "Epoch 126/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 198.7872 - val_loss: 523.1171\n",
      "Epoch 127/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 190.9203 - val_loss: 521.1109\n",
      "Epoch 128/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 183.3149 - val_loss: 519.2776\n",
      "Epoch 129/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 175.9650 - val_loss: 517.6118\n",
      "Epoch 130/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 168.8648 - val_loss: 516.1080\n",
      "Epoch 131/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 162.0081 - val_loss: 514.7610\n",
      "Epoch 132/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 155.3895 - val_loss: 513.5652\n",
      "Epoch 133/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 149.0032 - val_loss: 512.5159\n",
      "Epoch 134/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 142.8434 - val_loss: 511.6073\n",
      "Epoch 135/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 136.9047 - val_loss: 510.8347\n",
      "Epoch 136/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 131.1815 - val_loss: 510.1930\n",
      "Epoch 137/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 125.6683 - val_loss: 509.6770\n",
      "Epoch 138/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 120.3595 - val_loss: 509.2820\n",
      "Epoch 139/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 115.2499 - val_loss: 509.0028\n",
      "Epoch 140/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 110.3340 - val_loss: 508.8350\n",
      "Epoch 141/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 105.6066 - val_loss: 508.7736\n",
      "Epoch 142/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 101.0627 - val_loss: 508.8141\n",
      "Epoch 143/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 96.6972 - val_loss: 508.9518\n",
      "Epoch 144/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5049 - val_loss: 509.1823\n",
      "Epoch 145/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.4807 - val_loss: 509.5010\n",
      "Epoch 146/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 84.6199 - val_loss: 509.9037\n",
      "Epoch 147/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 80.9176 - val_loss: 510.3860\n",
      "Epoch 148/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 77.3692 - val_loss: 510.9439\n",
      "Epoch 149/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 73.9696 - val_loss: 511.5731\n",
      "Epoch 150/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 70.7144 - val_loss: 512.2696\n",
      "Epoch 151/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 67.5994 - val_loss: 513.0294\n",
      "Epoch 152/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 64.6196 - val_loss: 513.8487\n",
      "Epoch 153/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 61.7708 - val_loss: 514.7237\n",
      "Epoch 154/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 59.0488 - val_loss: 515.6506\n",
      "Epoch 155/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 56.4491 - val_loss: 516.6259\n",
      "Epoch 156/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 53.9676 - val_loss: 517.6459\n",
      "Epoch 157/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 51.6006 - val_loss: 518.7073\n",
      "Epoch 158/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 49.3437 - val_loss: 519.8068\n",
      "Epoch 159/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 47.1928 - val_loss: 520.9409\n",
      "Epoch 160/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 45.1446 - val_loss: 522.1067\n",
      "Epoch 161/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 43.1951 - val_loss: 523.3008\n",
      "Epoch 162/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 41.3406 - val_loss: 524.5206\n",
      "Epoch 163/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 39.5773 - val_loss: 525.7629\n",
      "Epoch 164/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 37.9021 - val_loss: 527.0250\n",
      "Epoch 165/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 36.3114 - val_loss: 528.3041\n",
      "Epoch 166/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 34.8018 - val_loss: 529.5977\n",
      "Epoch 167/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 33.3700 - val_loss: 530.9031\n",
      "Epoch 168/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.0130 - val_loss: 532.2181\n",
      "Epoch 169/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 30.7274 - val_loss: 533.5403\n",
      "Epoch 170/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 29.5105 - val_loss: 534.8671\n",
      "Epoch 171/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 28.3594 - val_loss: 536.1966\n",
      "Epoch 172/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 27.2710 - val_loss: 537.5268\n",
      "Epoch 173/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 26.2426 - val_loss: 538.8556\n",
      "Epoch 174/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 25.2717 - val_loss: 540.1813\n",
      "Epoch 175/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.3554 - val_loss: 541.5018\n",
      "Epoch 176/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 23.4914 - val_loss: 542.8156\n",
      "Epoch 177/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.6773 - val_loss: 544.1208\n",
      "Epoch 178/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.9106 - val_loss: 545.4162\n",
      "Epoch 179/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.1891 - val_loss: 546.7005\n",
      "Epoch 180/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 20.5106 - val_loss: 547.9717\n",
      "Epoch 181/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 19.8729 - val_loss: 549.2289\n",
      "Epoch 182/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 19.2740 - val_loss: 550.4711\n",
      "Epoch 183/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7119 - val_loss: 551.6966\n",
      "Epoch 184/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.1848 - val_loss: 552.9050\n",
      "Epoch 185/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.6908 - val_loss: 554.0949\n",
      "Epoch 186/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.2281 - val_loss: 555.2656\n",
      "Epoch 187/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.7950 - val_loss: 556.4166\n",
      "Epoch 188/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 16.3900 - val_loss: 557.5464\n",
      "Epoch 189/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 16.0114 - val_loss: 558.6549\n",
      "Epoch 190/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.6579 - val_loss: 559.7413\n",
      "Epoch 191/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 15.3279 - val_loss: 560.8052\n",
      "Epoch 192/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.0202 - val_loss: 561.8461\n",
      "Epoch 193/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 14.7333 - val_loss: 562.8636\n",
      "Epoch 194/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 14.4661 - val_loss: 563.8572\n",
      "Epoch 195/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 14.2174 - val_loss: 564.8271\n",
      "Epoch 196/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 13.9861 - val_loss: 565.7723\n",
      "Epoch 197/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 13.7712 - val_loss: 566.6936\n",
      "Epoch 198/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 13.5714 - val_loss: 567.5900\n",
      "Epoch 199/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 13.3861 - val_loss: 568.4619\n",
      "Epoch 200/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 13.2141 - val_loss: 569.3089\n",
      "Epoch 201/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 13.0547 - val_loss: 570.1317\n",
      "Epoch 202/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 12.9070 - val_loss: 570.9297\n",
      "Epoch 203/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.7702 - val_loss: 571.7029\n",
      "Epoch 204/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.6438 - val_loss: 572.4522\n",
      "Epoch 205/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.5268 - val_loss: 573.1775\n",
      "Epoch 206/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.4186 - val_loss: 573.8788\n",
      "Epoch 207/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.3188 - val_loss: 574.5570\n",
      "Epoch 208/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 12.2265 - val_loss: 575.2113\n",
      "Epoch 209/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 12.1415 - val_loss: 575.8427\n",
      "Epoch 210/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 12.0631 - val_loss: 576.4510\n",
      "Epoch 211/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.9909 - val_loss: 577.0370\n",
      "Epoch 212/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.9243 - val_loss: 577.6014\n",
      "Epoch 213/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.8630 - val_loss: 578.1440\n",
      "Epoch 214/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.8066 - val_loss: 578.6656\n",
      "Epoch 215/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.7548 - val_loss: 579.1667\n",
      "Epoch 216/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.7071 - val_loss: 579.6475\n",
      "Epoch 217/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.6633 - val_loss: 580.1085\n",
      "Epoch 218/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.6230 - val_loss: 580.5499\n",
      "Epoch 219/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.5861 - val_loss: 580.9728\n",
      "Epoch 220/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.5521 - val_loss: 581.3773\n",
      "Epoch 221/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.5210 - val_loss: 581.7640\n",
      "Epoch 222/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.4924 - val_loss: 582.1331\n",
      "Epoch 223/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.4662 - val_loss: 582.4858\n",
      "Epoch 224/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.4422 - val_loss: 582.8217\n",
      "Epoch 225/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.4202 - val_loss: 583.1420\n",
      "Epoch 226/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 11.4000 - val_loss: 583.4468\n",
      "Epoch 227/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.3816 - val_loss: 583.7371\n",
      "Epoch 228/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.3646 - val_loss: 584.0129\n",
      "Epoch 229/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.3491 - val_loss: 584.2748\n",
      "Epoch 230/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.3350 - val_loss: 584.5237\n",
      "Epoch 231/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.3220 - val_loss: 584.7596\n",
      "Epoch 232/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.3101 - val_loss: 584.9833\n",
      "Epoch 233/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2992 - val_loss: 585.1949\n",
      "Epoch 234/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2893 - val_loss: 585.3954\n",
      "Epoch 235/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2802 - val_loss: 585.5847\n",
      "Epoch 236/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2719 - val_loss: 585.7638\n",
      "Epoch 237/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2643 - val_loss: 585.9325\n",
      "Epoch 238/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2574 - val_loss: 586.0916\n",
      "Epoch 239/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2511 - val_loss: 586.2413\n",
      "Epoch 240/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2453 - val_loss: 586.3826\n",
      "Epoch 241/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2400 - val_loss: 586.5154\n",
      "Epoch 242/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 11.2352 - val_loss: 586.6402\n",
      "Epoch 243/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2309 - val_loss: 586.7573\n",
      "Epoch 244/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2269 - val_loss: 586.8672\n",
      "Epoch 245/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2233 - val_loss: 586.9702\n",
      "Epoch 246/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.2201 - val_loss: 587.0671\n",
      "Epoch 247/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2170 - val_loss: 587.1572\n",
      "Epoch 248/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2144 - val_loss: 587.2419\n",
      "Epoch 249/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.2120 - val_loss: 587.3206\n",
      "Epoch 250/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.2097 - val_loss: 587.3940\n",
      "Epoch 251/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2078 - val_loss: 587.4624\n",
      "Epoch 252/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2060 - val_loss: 587.5261\n",
      "Epoch 253/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.2045 - val_loss: 587.5857\n",
      "Epoch 254/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2031 - val_loss: 587.6409\n",
      "Epoch 255/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.2018 - val_loss: 587.6920\n",
      "Epoch 256/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2007 - val_loss: 587.7393\n",
      "Epoch 257/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 11.1998 - val_loss: 587.7832\n",
      "Epoch 258/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.1990 - val_loss: 587.8238\n",
      "Epoch 259/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.1983 - val_loss: 587.8615\n",
      "Epoch 260/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.1977 - val_loss: 587.8962\n",
      "Epoch 261/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.1972 - val_loss: 587.9283\n",
      "Epoch 262/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.1968 - val_loss: 587.9575\n",
      "Epoch 263/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.1965 - val_loss: 587.9847\n",
      "Epoch 264/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.1962 - val_loss: 588.0096\n",
      "Epoch 265/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.1961 - val_loss: 588.0320\n",
      "Epoch 266/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.1960 - val_loss: 588.0529\n",
      "Epoch 267/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.1960 - val_loss: 588.0717\n",
      "Epoch 268/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.1960 - val_loss: 588.0890\n",
      "Epoch 269/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.1961 - val_loss: 588.1046\n",
      "Epoch 270/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.1963 - val_loss: 588.1189\n",
      "Epoch 271/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 11.1964 - val_loss: 588.1317\n",
      "Epoch 272/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.1967 - val_loss: 588.1432\n",
      "Epoch 273/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.1969 - val_loss: 588.1537\n",
      "Epoch 274/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.1973 - val_loss: 588.1634\n",
      "Epoch 275/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.1977 - val_loss: 588.1721\n",
      "Epoch 276/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.1980 - val_loss: 588.1796\n",
      "Epoch 277/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 11.1984 - val_loss: 588.1861\n",
      "Epoch 278/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 11.1988 - val_loss: 588.1920\n",
      "Epoch 279/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 11.1993 - val_loss: 588.1971\n",
      "Epoch 280/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.1998 - val_loss: 588.2019\n",
      "Epoch 281/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.2003 - val_loss: 588.2061\n",
      "Epoch 282/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2008 - val_loss: 588.2097\n",
      "Epoch 283/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2013 - val_loss: 588.2122\n",
      "Epoch 284/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2019 - val_loss: 588.2147\n",
      "Epoch 285/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2025 - val_loss: 588.2166\n",
      "Epoch 286/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2031 - val_loss: 588.2180\n",
      "Epoch 287/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2037 - val_loss: 588.2191\n",
      "Epoch 288/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2043 - val_loss: 588.2202\n",
      "Epoch 289/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2049 - val_loss: 588.2205\n",
      "Epoch 290/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2056 - val_loss: 588.2209\n",
      "Epoch 291/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2062 - val_loss: 588.2211\n",
      "Epoch 292/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2069 - val_loss: 588.2209\n",
      "Epoch 293/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2075 - val_loss: 588.2204\n",
      "Epoch 294/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2082 - val_loss: 588.2202\n",
      "Epoch 295/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.2089 - val_loss: 588.2194\n",
      "Epoch 296/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 11.2096 - val_loss: 588.2186\n",
      "Epoch 297/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2103 - val_loss: 588.2175\n",
      "Epoch 298/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2110 - val_loss: 588.2166\n",
      "Epoch 299/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2116 - val_loss: 588.2151\n",
      "Epoch 300/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2124 - val_loss: 588.2141\n",
      "Epoch 301/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2130 - val_loss: 588.2126\n",
      "Epoch 302/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2137 - val_loss: 588.2108\n",
      "Epoch 303/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2144 - val_loss: 588.2095\n",
      "Epoch 304/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2152 - val_loss: 588.2075\n",
      "Epoch 305/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2159 - val_loss: 588.2060\n",
      "Epoch 306/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2166 - val_loss: 588.2043\n",
      "Epoch 307/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2173 - val_loss: 588.2025\n",
      "Epoch 308/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2180 - val_loss: 588.2005\n",
      "Epoch 309/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2187 - val_loss: 588.1987\n",
      "Epoch 310/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2194 - val_loss: 588.1968\n",
      "Epoch 311/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 11.2202 - val_loss: 588.1950\n",
      "Epoch 312/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2208 - val_loss: 588.1930\n",
      "Epoch 313/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2215 - val_loss: 588.1910\n",
      "Epoch 314/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2223 - val_loss: 588.1891\n",
      "Epoch 315/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2230 - val_loss: 588.1870\n",
      "Epoch 316/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2237 - val_loss: 588.1851\n",
      "Epoch 317/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2244 - val_loss: 588.1833\n",
      "Epoch 318/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2251 - val_loss: 588.1812\n",
      "Epoch 319/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2258 - val_loss: 588.1795\n",
      "Epoch 320/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2265 - val_loss: 588.1777\n",
      "Epoch 321/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2272 - val_loss: 588.1757\n",
      "Epoch 322/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2279 - val_loss: 588.1738\n",
      "Epoch 323/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2286 - val_loss: 588.1718\n",
      "Epoch 324/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2292 - val_loss: 588.1694\n",
      "Epoch 325/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2300 - val_loss: 588.1672\n",
      "Epoch 326/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 11.2306 - val_loss: 588.1649\n",
      "Epoch 327/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2313 - val_loss: 588.1628\n",
      "Epoch 328/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2320 - val_loss: 588.1610\n",
      "Epoch 329/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2327 - val_loss: 588.1588\n",
      "Epoch 330/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2334 - val_loss: 588.1570\n",
      "Epoch 331/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2340 - val_loss: 588.1548\n",
      "Epoch 332/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2347 - val_loss: 588.1531\n",
      "Epoch 333/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2353 - val_loss: 588.1509\n",
      "Epoch 334/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2360 - val_loss: 588.1491\n",
      "Epoch 335/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2367 - val_loss: 588.1472\n",
      "Epoch 336/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2373 - val_loss: 588.1454\n",
      "Epoch 337/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2380 - val_loss: 588.1431\n",
      "Epoch 338/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2386 - val_loss: 588.1417\n",
      "Epoch 339/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2392 - val_loss: 588.1392\n",
      "Epoch 340/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2399 - val_loss: 588.1378\n",
      "Epoch 341/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 11.2405 - val_loss: 588.1364\n",
      "Epoch 342/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2411 - val_loss: 588.1341\n",
      "Epoch 343/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2417 - val_loss: 588.1324\n",
      "Epoch 344/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2423 - val_loss: 588.1305\n",
      "Epoch 345/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2430 - val_loss: 588.1285\n",
      "Epoch 346/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2436 - val_loss: 588.1271\n",
      "Epoch 347/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2441 - val_loss: 588.1252\n",
      "Epoch 348/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2448 - val_loss: 588.1235\n",
      "Epoch 349/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2453 - val_loss: 588.1218\n",
      "Epoch 350/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2459 - val_loss: 588.1204\n",
      "Epoch 351/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2465 - val_loss: 588.1186\n",
      "Epoch 352/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2471 - val_loss: 588.1169\n",
      "Epoch 353/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2476 - val_loss: 588.1152\n",
      "Epoch 354/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.2482 - val_loss: 588.1137\n",
      "Epoch 355/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2488 - val_loss: 588.1122\n",
      "Epoch 356/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2493 - val_loss: 588.1105\n",
      "Epoch 357/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2498 - val_loss: 588.1086\n",
      "Epoch 358/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2504 - val_loss: 588.1068\n",
      "Epoch 359/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2509 - val_loss: 588.1053\n",
      "Epoch 360/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2514 - val_loss: 588.1039\n",
      "Epoch 361/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2520 - val_loss: 588.1022\n",
      "Epoch 362/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2525 - val_loss: 588.1011\n",
      "Epoch 363/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2530 - val_loss: 588.0992\n",
      "Epoch 364/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2535 - val_loss: 588.0977\n",
      "Epoch 365/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2540 - val_loss: 588.0961\n",
      "Epoch 366/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2545 - val_loss: 588.0948\n",
      "Epoch 367/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2550 - val_loss: 588.0934\n",
      "Epoch 368/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2554 - val_loss: 588.0917\n",
      "Epoch 369/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2559 - val_loss: 588.0903\n",
      "Epoch 370/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 11.2564 - val_loss: 588.0893\n",
      "Epoch 371/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2568 - val_loss: 588.0876\n",
      "Epoch 372/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2573 - val_loss: 588.0863\n",
      "Epoch 373/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2578 - val_loss: 588.0851\n",
      "Epoch 374/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2582 - val_loss: 588.0839\n",
      "Epoch 375/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2586 - val_loss: 588.0823\n",
      "Epoch 376/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2591 - val_loss: 588.0811\n",
      "Epoch 377/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2595 - val_loss: 588.0800\n",
      "Epoch 378/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2600 - val_loss: 588.0786\n",
      "Epoch 379/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2604 - val_loss: 588.0778\n",
      "Epoch 380/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2607 - val_loss: 588.0764\n",
      "Epoch 381/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2612 - val_loss: 588.0750\n",
      "Epoch 382/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2616 - val_loss: 588.0739\n",
      "Epoch 383/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2620 - val_loss: 588.0726\n",
      "Epoch 384/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2624 - val_loss: 588.0712\n",
      "Epoch 385/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2628 - val_loss: 588.0702\n",
      "Epoch 386/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2632 - val_loss: 588.0693\n",
      "Epoch 387/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2635 - val_loss: 588.0684\n",
      "Epoch 388/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2639 - val_loss: 588.0671\n",
      "Epoch 389/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2642 - val_loss: 588.0659\n",
      "Epoch 390/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2646 - val_loss: 588.0650\n",
      "Epoch 391/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2649 - val_loss: 588.0639\n",
      "Epoch 392/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2653 - val_loss: 588.0630\n",
      "Epoch 393/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2656 - val_loss: 588.0620\n",
      "Epoch 394/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2660 - val_loss: 588.0607\n",
      "Epoch 395/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2663 - val_loss: 588.0598\n",
      "Epoch 396/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 11.2666 - val_loss: 588.0588\n",
      "Epoch 397/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2669 - val_loss: 588.0579\n",
      "Epoch 398/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2673 - val_loss: 588.0572\n",
      "Epoch 399/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2676 - val_loss: 588.0562\n",
      "Epoch 400/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2679 - val_loss: 588.0552\n",
      "Epoch 401/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2682 - val_loss: 588.0541\n",
      "Epoch 402/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2685 - val_loss: 588.0535\n",
      "Epoch 403/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2688 - val_loss: 588.0529\n",
      "Epoch 404/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2690 - val_loss: 588.0522\n",
      "Epoch 405/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2693 - val_loss: 588.0509\n",
      "Epoch 406/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2696 - val_loss: 588.0501\n",
      "Epoch 407/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2699 - val_loss: 588.0498\n",
      "Epoch 408/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2701 - val_loss: 588.0489\n",
      "Epoch 409/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2704 - val_loss: 588.0483\n",
      "Epoch 410/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 11.2706 - val_loss: 588.0474\n",
      "Epoch 411/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2709 - val_loss: 588.0467\n",
      "Epoch 412/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2711 - val_loss: 588.0453\n",
      "Epoch 413/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2714 - val_loss: 588.0447\n",
      "Epoch 414/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2716 - val_loss: 588.0439\n",
      "Epoch 415/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2719 - val_loss: 588.0431\n",
      "Epoch 416/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2721 - val_loss: 588.0425\n",
      "Epoch 417/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2723 - val_loss: 588.0419\n",
      "Epoch 418/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2726 - val_loss: 588.0410\n",
      "Epoch 419/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2728 - val_loss: 588.0405\n",
      "Epoch 420/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2730 - val_loss: 588.0401\n",
      "Epoch 421/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2732 - val_loss: 588.0391\n",
      "Epoch 422/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2735 - val_loss: 588.0389\n",
      "Epoch 423/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2736 - val_loss: 588.0381\n",
      "Epoch 424/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2738 - val_loss: 588.0376\n",
      "Epoch 425/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 11.2740 - val_loss: 588.0372\n",
      "Epoch 426/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2743 - val_loss: 588.0368\n",
      "Epoch 427/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2744 - val_loss: 588.0361\n",
      "Epoch 428/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2746 - val_loss: 588.0358\n",
      "Epoch 429/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2747 - val_loss: 588.0349\n",
      "Epoch 430/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2749 - val_loss: 588.0344\n",
      "Epoch 431/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2751 - val_loss: 588.0334\n",
      "Epoch 432/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2753 - val_loss: 588.0330\n",
      "Epoch 433/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2755 - val_loss: 588.0327\n",
      "Epoch 434/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2756 - val_loss: 588.0320\n",
      "Epoch 435/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2758 - val_loss: 588.0314\n",
      "Epoch 436/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2760 - val_loss: 588.0308\n",
      "Epoch 437/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2761 - val_loss: 588.0303\n",
      "Epoch 438/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 11.2763 - val_loss: 588.0298\n",
      "Epoch 439/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2765 - val_loss: 588.0296\n",
      "Epoch 440/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2766 - val_loss: 588.0287\n",
      "Epoch 441/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2767 - val_loss: 588.0284\n",
      "Epoch 442/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2769 - val_loss: 588.0280\n",
      "Epoch 443/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2770 - val_loss: 588.0273\n",
      "Epoch 444/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2772 - val_loss: 588.0271\n",
      "Epoch 445/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2773 - val_loss: 588.0270\n",
      "Epoch 446/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2774 - val_loss: 588.0267\n",
      "Epoch 447/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2775 - val_loss: 588.0260\n",
      "Epoch 448/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2777 - val_loss: 588.0257\n",
      "Epoch 449/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2778 - val_loss: 588.0254\n",
      "Epoch 450/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 11.2779 - val_loss: 588.0252\n",
      "Epoch 451/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2780 - val_loss: 588.0244\n",
      "Epoch 452/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2782 - val_loss: 588.0241\n",
      "Epoch 453/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2783 - val_loss: 588.0236\n",
      "Epoch 454/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2784 - val_loss: 588.0231\n",
      "Epoch 455/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2785 - val_loss: 588.0228\n",
      "Epoch 456/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2786 - val_loss: 588.0224\n",
      "Epoch 457/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2788 - val_loss: 588.0224\n",
      "Epoch 458/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2788 - val_loss: 588.0221\n",
      "Epoch 459/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2789 - val_loss: 588.0219\n",
      "Epoch 460/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2790 - val_loss: 588.0217\n",
      "Epoch 461/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2791 - val_loss: 588.0214\n",
      "Epoch 462/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2792 - val_loss: 588.0211\n",
      "Epoch 463/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.2793 - val_loss: 588.0209\n",
      "Epoch 464/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2794 - val_loss: 588.0205\n",
      "Epoch 465/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2795 - val_loss: 588.0204\n",
      "Epoch 466/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2796 - val_loss: 588.0200\n",
      "Epoch 467/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2797 - val_loss: 588.0201\n",
      "Epoch 468/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2797 - val_loss: 588.0196\n",
      "Epoch 469/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2798 - val_loss: 588.0192\n",
      "Epoch 470/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2799 - val_loss: 588.0192\n",
      "Epoch 471/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2800 - val_loss: 588.0190\n",
      "Epoch 472/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2801 - val_loss: 588.0189\n",
      "Epoch 473/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2801 - val_loss: 588.0187\n",
      "Epoch 474/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2802 - val_loss: 588.0186\n",
      "Epoch 475/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2803 - val_loss: 588.0181\n",
      "Epoch 476/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 11.2804 - val_loss: 588.0179\n",
      "Epoch 477/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2804 - val_loss: 588.0174\n",
      "Epoch 478/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2805 - val_loss: 588.0174\n",
      "Epoch 479/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2806 - val_loss: 588.0170\n",
      "Epoch 480/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2806 - val_loss: 588.0169\n",
      "Epoch 481/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2807 - val_loss: 588.0168\n",
      "Epoch 482/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2808 - val_loss: 588.0168\n",
      "Epoch 483/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2808 - val_loss: 588.0167\n",
      "Epoch 484/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2809 - val_loss: 588.0165\n",
      "Epoch 485/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2809 - val_loss: 588.0161\n",
      "Epoch 486/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2810 - val_loss: 588.0159\n",
      "Epoch 487/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2810 - val_loss: 588.0157\n",
      "Epoch 488/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 11.2811 - val_loss: 588.0156\n",
      "Epoch 489/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2812 - val_loss: 588.0157\n",
      "Epoch 490/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2812 - val_loss: 588.0156\n",
      "Epoch 491/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2813 - val_loss: 588.0155\n",
      "Epoch 492/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2813 - val_loss: 588.0153\n",
      "Epoch 493/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2813 - val_loss: 588.0150\n",
      "Epoch 494/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2814 - val_loss: 588.0150\n",
      "Epoch 495/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2815 - val_loss: 588.0149\n",
      "Epoch 496/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2815 - val_loss: 588.0144\n",
      "Epoch 497/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2816 - val_loss: 588.0144\n",
      "Epoch 498/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 11.2816 - val_loss: 588.0142\n",
      "Epoch 499/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2816 - val_loss: 588.0142\n",
      "Epoch 500/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.2817 - val_loss: 588.0138\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.0403, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0012, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0120, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.0334, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 416ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[71.67047152, 71.65646592, 71.64246032, 71.62845472, 71.61444911,\n",
       "        71.60044351, 71.58643791, 71.57243231, 71.5584267 , 71.5444211 ,\n",
       "        71.5304155 , 71.5164099 , 71.5024043 , 74.1378852 , 74.0958684 ,\n",
       "        74.0538515 , 74.0118347 , 73.9698179 , 73.9278011 , 73.8877843 ,\n",
       "        73.8437675 , 73.8017507 , 73.6953081 , 73.5860644 , 73.4768207 ,\n",
       "        73.367577  , 73.2583333 , 73.1490896 , 73.0398459 , 72.9306022 ,\n",
       "        72.8213585 , 72.7121149 , 72.6028711 , 72.4965686 , 72.4377451 ,\n",
       "        72.3759216 , 72.314098  , 72.2522745 , 72.1904501 , 72.1286257 ,\n",
       "         0.        ,  0.39783034,  0.        ,  0.09585822,  0.        ,\n",
       "         0.        ,  0.12218247, 73.3918534 , 73.2826097 , 73.173366  ,\n",
       "        73.0641223 , 72.9548786 , 72.8456349 , 72.7363912 , 72.6271475 ,\n",
       "        72.5179038 , 72.450817  , 72.3919935 , 72.3331699 , 72.2743464 ,\n",
       "        72.2155229 , 72.1566993 , 72.0978758 , 72.0390523 , 71.9832096 ,\n",
       "        71.9214052 , 71.8625817 , 71.8037582 , 71.7921335 , 71.7837302 ,\n",
       "        71.7753268 , 71.7669234 , 71.7585201 , 77.893692  ,  0.5452919 ,\n",
       "         0.18788239,  0.        ,  0.        ,  0.07900403,  0.        ,\n",
       "        67.9621582 ,  0.        ,  0.        ,  0.34058553,  0.        ,\n",
       "         0.10459059,  1.08311224,  0.        ,  0.68475485,  0.69384545,\n",
       "         0.        ,  0.        ,  0.        ,  0.3406029 ,  0.93061215,\n",
       "         0.3658179 ,  0.        ,  0.5466826 ,  0.75246495,  0.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([69.19361891, 69.18733347, 69.18104803, 69.17476259, 69.16847715,\n",
       "       69.16219171, 69.15590626, 69.14962082, 69.14333538, 69.13704994,\n",
       "       69.1307645 , 69.12447906, 69.11819362, 69.11190818, 69.10562274,\n",
       "       69.0993373 , 69.09305185, 69.08676641, 69.08048097, 69.07419553,\n",
       "       69.06791009, 69.06162465, 69.05533921, 69.04905377, 69.04276833,\n",
       "       69.03648289, 69.03019744, 69.023912  , 69.01762656, 69.01134112,\n",
       "       69.00505568, 68.99877024, 68.9924848 , 68.98619936, 68.97991392,\n",
       "       68.97362848, 68.96734303, 68.96105759, 68.95477215, 68.94848671,\n",
       "       68.94220127, 68.93591583, 68.92963039, 68.92334495, 68.91705951,\n",
       "       68.91077407, 68.90448862, 68.89820318, 68.89191774, 68.8856323 ,\n",
       "       68.87934686, 68.87306142, 68.86677598, 68.86049054, 68.8542051 ,\n",
       "       68.84791966, 68.84163421, 68.83534877, 68.82906333, 68.82277789,\n",
       "       68.81649245, 68.81020701, 68.80392157, 68.79763613, 68.79135069,\n",
       "       68.78506525, 68.7787798 , 68.77249436, 68.76620892, 68.75992348,\n",
       "       68.75363804, 68.7473526 , 68.74106716, 68.73478172, 68.72849628,\n",
       "       68.72221084, 68.71592539, 68.70963995, 68.70335451, 68.69706907,\n",
       "       68.69078363, 68.68449819, 68.67821275, 68.67192731, 68.66564187,\n",
       "       68.65935643, 68.65307098, 68.64678554, 68.6405001 , 68.63421466,\n",
       "       68.62792922, 68.62164378, 68.61535834, 68.6090729 , 68.60278746,\n",
       "       68.59650202, 68.59021657, 68.58393113, 68.57764569, 68.57136025])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.864604976410895\n",
      "24.373252528093808\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
