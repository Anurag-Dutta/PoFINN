{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "2345    56.893831\n",
       "2346    56.881533\n",
       "2347    56.869236\n",
       "2348    56.856938\n",
       "2349    56.844640\n",
       "Name: C4, Length: 2350, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c4_interpolated_2250_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "2245     0.000000\n",
       "2246     0.000000\n",
       "2247     0.018294\n",
       "2248     0.000000\n",
       "2249     0.077405\n",
       "Name: C4, Length: 2250, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2250)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.409524</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.219048</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.028571</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.838095</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.600000  0.000298\n",
       "1     88.409524  0.000298\n",
       "2     88.219048  0.000297\n",
       "3     88.028571  0.000297\n",
       "4     87.838095  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6856  # Value for C0\n",
    "K0 = -0.0008  # Value for K0\n",
    "K1 = -0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0128    # Value for b\n",
    "c = -2.3003    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.600000    88.409524    88.219048    88.028571    87.838095   \n",
      "351    88.409524    88.219048    88.028571    87.838095    87.647619   \n",
      "352    88.219048    88.028571    87.838095    87.647619    87.457143   \n",
      "353    88.028571    87.838095    87.647619    87.457143    87.266667   \n",
      "354    87.838095    87.647619    87.457143    87.266667    87.076190   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.647619    87.457143    87.266667    87.076190    86.896218  ...   \n",
      "351    87.457143    87.266667    87.076190    86.896218    86.845798  ...   \n",
      "352    87.266667    87.076190    86.896218    86.845798    86.795378  ...   \n",
      "353    87.076190    86.896218    86.845798    86.795378    86.744958  ...   \n",
      "354    86.896218    86.845798    86.795378    86.744958    86.694538  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   78.880345    0.000263   78.856069    0.000263   78.831793    0.000263   \n",
      "351   78.856069    0.000263   78.831793    0.000263   78.807516    0.000262   \n",
      "352   78.831793    0.000263   78.807516    0.000262   78.783240    0.000262   \n",
      "353   78.807516    0.000262   78.783240    0.000262   78.758964    0.000262   \n",
      "354   78.783240    0.000262   78.758964    0.000262   78.734687    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   78.807516    0.000262   78.783240    0.000262  \n",
      "351   78.783240    0.000262   78.758964    0.000262  \n",
      "352   78.758964    0.000262   78.734687    0.000262  \n",
      "353   78.734687    0.000262   78.710411    0.000262  \n",
      "354   78.710411    0.000262   78.686134    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 1, 251) (1800, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "23/23 [==============================] - 2s 21ms/step - loss: 5209.0586 - val_loss: 3903.7500\n",
      "Epoch 2/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 5119.7622 - val_loss: 3850.5520\n",
      "Epoch 3/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 5034.0610 - val_loss: 3792.2327\n",
      "Epoch 4/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4956.2178 - val_loss: 3736.6587\n",
      "Epoch 5/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4880.8906 - val_loss: 3686.8008\n",
      "Epoch 6/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 4810.0186 - val_loss: 3638.1323\n",
      "Epoch 7/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4740.7241 - val_loss: 3590.3816\n",
      "Epoch 8/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 4672.6572 - val_loss: 3543.3904\n",
      "Epoch 9/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 4605.6270 - val_loss: 3497.0627\n",
      "Epoch 10/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4539.5259 - val_loss: 3451.2771\n",
      "Epoch 11/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4468.1636 - val_loss: 3394.6011\n",
      "Epoch 12/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4385.7817 - val_loss: 3346.3618\n",
      "Epoch 13/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4316.2075 - val_loss: 3299.5098\n",
      "Epoch 14/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4248.6606 - val_loss: 3253.7542\n",
      "Epoch 15/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4182.6299 - val_loss: 3208.8650\n",
      "Epoch 16/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4117.8169 - val_loss: 3164.7229\n",
      "Epoch 17/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4054.0593 - val_loss: 3121.2588\n",
      "Epoch 18/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3991.2612 - val_loss: 3078.4260\n",
      "Epoch 19/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3929.3594 - val_loss: 3036.1948\n",
      "Epoch 20/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3868.3049 - val_loss: 2994.5403\n",
      "Epoch 21/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3808.0652 - val_loss: 2953.4426\n",
      "Epoch 22/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3748.6108 - val_loss: 2912.8884\n",
      "Epoch 23/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3689.9185 - val_loss: 2872.8638\n",
      "Epoch 24/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3631.9712 - val_loss: 2833.3579\n",
      "Epoch 25/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 3574.7510 - val_loss: 2794.3608\n",
      "Epoch 26/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3518.2437 - val_loss: 2755.8638\n",
      "Epoch 27/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3462.4365 - val_loss: 2717.8594\n",
      "Epoch 28/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3407.3184 - val_loss: 2680.3396\n",
      "Epoch 29/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3352.8774 - val_loss: 2643.2983\n",
      "Epoch 30/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 3299.1055 - val_loss: 2606.7290\n",
      "Epoch 31/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3245.9919 - val_loss: 2570.6252\n",
      "Epoch 32/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3193.5291 - val_loss: 2534.9824\n",
      "Epoch 33/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3141.7083 - val_loss: 2499.7947\n",
      "Epoch 34/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3090.5222 - val_loss: 2465.0564\n",
      "Epoch 35/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3039.9639 - val_loss: 2430.7637\n",
      "Epoch 36/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2990.0256 - val_loss: 2396.9106\n",
      "Epoch 37/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2940.7009 - val_loss: 2363.4937\n",
      "Epoch 38/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2891.9829 - val_loss: 2330.5078\n",
      "Epoch 39/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2843.8655 - val_loss: 2297.9482\n",
      "Epoch 40/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2796.3428 - val_loss: 2265.8115\n",
      "Epoch 41/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2749.4084 - val_loss: 2234.0928\n",
      "Epoch 42/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2703.0569 - val_loss: 2202.7888\n",
      "Epoch 43/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2657.2822 - val_loss: 2171.8953\n",
      "Epoch 44/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2612.0791 - val_loss: 2141.4080\n",
      "Epoch 45/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2567.4419 - val_loss: 2111.3237\n",
      "Epoch 46/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2523.3657 - val_loss: 2081.6377\n",
      "Epoch 47/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2479.8445 - val_loss: 2052.3472\n",
      "Epoch 48/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2436.8735 - val_loss: 2023.4485\n",
      "Epoch 49/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 2394.4480 - val_loss: 1994.9374\n",
      "Epoch 50/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2352.5635 - val_loss: 1966.8108\n",
      "Epoch 51/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2311.2136 - val_loss: 1939.0653\n",
      "Epoch 52/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2270.3950 - val_loss: 1911.6975\n",
      "Epoch 53/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2230.1016 - val_loss: 1884.7039\n",
      "Epoch 54/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2190.3296 - val_loss: 1858.0809\n",
      "Epoch 55/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2151.0745 - val_loss: 1831.8257\n",
      "Epoch 56/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2112.3315 - val_loss: 1805.9343\n",
      "Epoch 57/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2074.0959 - val_loss: 1780.4045\n",
      "Epoch 58/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2036.3635 - val_loss: 1755.2322\n",
      "Epoch 59/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1999.1299 - val_loss: 1730.4146\n",
      "Epoch 60/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1962.3910 - val_loss: 1705.9486\n",
      "Epoch 61/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1926.1415 - val_loss: 1681.8311\n",
      "Epoch 62/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1890.3783 - val_loss: 1658.0587\n",
      "Epoch 63/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1855.0963 - val_loss: 1634.6284\n",
      "Epoch 64/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1820.2915 - val_loss: 1611.5374\n",
      "Epoch 65/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1785.9604 - val_loss: 1588.7826\n",
      "Epoch 66/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1752.0983 - val_loss: 1566.3611\n",
      "Epoch 67/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1718.7009 - val_loss: 1544.2700\n",
      "Epoch 68/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 1685.7648 - val_loss: 1522.5062\n",
      "Epoch 69/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1653.2861 - val_loss: 1501.0670\n",
      "Epoch 70/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1621.2600 - val_loss: 1479.9493\n",
      "Epoch 71/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1589.6833 - val_loss: 1459.1501\n",
      "Epoch 72/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1558.5515 - val_loss: 1438.6669\n",
      "Epoch 73/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1527.8613 - val_loss: 1418.4971\n",
      "Epoch 74/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1497.6093 - val_loss: 1398.6375\n",
      "Epoch 75/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1467.7903 - val_loss: 1379.0850\n",
      "Epoch 76/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1438.4016 - val_loss: 1359.8374\n",
      "Epoch 77/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1409.4385 - val_loss: 1340.8912\n",
      "Epoch 78/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1380.8977 - val_loss: 1322.2445\n",
      "Epoch 79/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1352.7760 - val_loss: 1303.8947\n",
      "Epoch 80/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1325.0695 - val_loss: 1285.8381\n",
      "Epoch 81/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1297.7738 - val_loss: 1268.0729\n",
      "Epoch 82/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1270.8861 - val_loss: 1250.5959\n",
      "Epoch 83/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1244.4025 - val_loss: 1233.4047\n",
      "Epoch 84/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1218.3190 - val_loss: 1216.4966\n",
      "Epoch 85/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1192.6328 - val_loss: 1199.8691\n",
      "Epoch 86/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1167.3400 - val_loss: 1183.5190\n",
      "Epoch 87/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1142.4365 - val_loss: 1167.4446\n",
      "Epoch 88/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1117.9198 - val_loss: 1151.6427\n",
      "Epoch 89/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1093.7856 - val_loss: 1136.1107\n",
      "Epoch 90/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1070.0308 - val_loss: 1120.8466\n",
      "Epoch 91/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1046.6517 - val_loss: 1105.8470\n",
      "Epoch 92/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1023.6451 - val_loss: 1091.1104\n",
      "Epoch 93/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1001.0076 - val_loss: 1076.6332\n",
      "Epoch 94/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 978.7352 - val_loss: 1062.4130\n",
      "Epoch 95/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 956.8248 - val_loss: 1048.4478\n",
      "Epoch 96/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 935.2730 - val_loss: 1034.7349\n",
      "Epoch 97/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 914.0767 - val_loss: 1021.2717\n",
      "Epoch 98/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 893.2321 - val_loss: 1008.0555\n",
      "Epoch 99/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 872.7361 - val_loss: 995.0845\n",
      "Epoch 100/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 852.5851 - val_loss: 982.3552\n",
      "Epoch 101/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 832.7759 - val_loss: 969.8661\n",
      "Epoch 102/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 813.3053 - val_loss: 957.6143\n",
      "Epoch 103/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 794.1700 - val_loss: 945.5970\n",
      "Epoch 104/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 775.3665 - val_loss: 933.8125\n",
      "Epoch 105/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 756.8916 - val_loss: 922.2576\n",
      "Epoch 106/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 738.7422 - val_loss: 910.9307\n",
      "Epoch 107/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 720.9147 - val_loss: 899.8284\n",
      "Epoch 108/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 703.4060 - val_loss: 888.9492\n",
      "Epoch 109/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 686.2129 - val_loss: 878.2901\n",
      "Epoch 110/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 669.3321 - val_loss: 867.8487\n",
      "Epoch 111/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 652.7603 - val_loss: 857.6227\n",
      "Epoch 112/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 636.4941 - val_loss: 847.6094\n",
      "Epoch 113/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 620.5306 - val_loss: 837.8069\n",
      "Epoch 114/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 604.8662 - val_loss: 828.2123\n",
      "Epoch 115/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 589.4979 - val_loss: 818.8237\n",
      "Epoch 116/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 574.4229 - val_loss: 809.6384\n",
      "Epoch 117/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 559.6373 - val_loss: 800.6541\n",
      "Epoch 118/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 545.1382 - val_loss: 791.8682\n",
      "Epoch 119/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 530.9227 - val_loss: 783.2786\n",
      "Epoch 120/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 516.9872 - val_loss: 774.8829\n",
      "Epoch 121/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 503.3288 - val_loss: 766.6786\n",
      "Epoch 122/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 489.9443 - val_loss: 758.6635\n",
      "Epoch 123/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 476.8306 - val_loss: 750.8350\n",
      "Epoch 124/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 463.9839 - val_loss: 743.1907\n",
      "Epoch 125/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 451.4020 - val_loss: 735.7285\n",
      "Epoch 126/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 439.0811 - val_loss: 728.4457\n",
      "Epoch 127/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 427.0182 - val_loss: 721.3403\n",
      "Epoch 128/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 415.2100 - val_loss: 714.4095\n",
      "Epoch 129/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 403.6535 - val_loss: 707.6514\n",
      "Epoch 130/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 392.3459 - val_loss: 701.0636\n",
      "Epoch 131/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 381.2837 - val_loss: 694.6435\n",
      "Epoch 132/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 370.4641 - val_loss: 688.3890\n",
      "Epoch 133/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 359.8838 - val_loss: 682.2976\n",
      "Epoch 134/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 349.5397 - val_loss: 676.3671\n",
      "Epoch 135/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 339.4286 - val_loss: 670.5950\n",
      "Epoch 136/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 329.5477 - val_loss: 664.9794\n",
      "Epoch 137/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 319.8938 - val_loss: 659.5175\n",
      "Epoch 138/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 310.4637 - val_loss: 654.2070\n",
      "Epoch 139/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 301.2541 - val_loss: 649.0455\n",
      "Epoch 140/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 292.2620 - val_loss: 644.0309\n",
      "Epoch 141/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 283.4847 - val_loss: 639.1611\n",
      "Epoch 142/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 274.9187 - val_loss: 634.4335\n",
      "Epoch 143/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 266.5613 - val_loss: 629.8457\n",
      "Epoch 144/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 258.4090 - val_loss: 625.3955\n",
      "Epoch 145/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 250.4591 - val_loss: 621.0807\n",
      "Epoch 146/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 242.7086 - val_loss: 616.8989\n",
      "Epoch 147/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 235.1541 - val_loss: 612.8480\n",
      "Epoch 148/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 227.7930 - val_loss: 608.9252\n",
      "Epoch 149/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 220.6220 - val_loss: 605.1289\n",
      "Epoch 150/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 213.6380 - val_loss: 601.4564\n",
      "Epoch 151/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 206.8381 - val_loss: 597.9055\n",
      "Epoch 152/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 200.2194 - val_loss: 594.4741\n",
      "Epoch 153/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 193.7788 - val_loss: 591.1597\n",
      "Epoch 154/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 187.5132 - val_loss: 587.9601\n",
      "Epoch 155/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 181.4199 - val_loss: 584.8732\n",
      "Epoch 156/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 175.4956 - val_loss: 581.8968\n",
      "Epoch 157/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 169.7374 - val_loss: 579.0282\n",
      "Epoch 158/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 164.1427 - val_loss: 576.2659\n",
      "Epoch 159/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 158.7084 - val_loss: 573.6074\n",
      "Epoch 160/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 153.4314 - val_loss: 571.0502\n",
      "Epoch 161/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 148.3088 - val_loss: 568.5925\n",
      "Epoch 162/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 143.3378 - val_loss: 566.2321\n",
      "Epoch 163/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 138.5157 - val_loss: 563.9665\n",
      "Epoch 164/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 133.8391 - val_loss: 561.7938\n",
      "Epoch 165/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 129.3055 - val_loss: 559.7118\n",
      "Epoch 166/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 124.9121 - val_loss: 557.7182\n",
      "Epoch 167/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 120.6557 - val_loss: 555.8111\n",
      "Epoch 168/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 116.5339 - val_loss: 553.9884\n",
      "Epoch 169/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 112.5436 - val_loss: 552.2478\n",
      "Epoch 170/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 108.6820 - val_loss: 550.5873\n",
      "Epoch 171/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 104.9464 - val_loss: 549.0049\n",
      "Epoch 172/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 101.3341 - val_loss: 547.4985\n",
      "Epoch 173/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 97.8425 - val_loss: 546.0660\n",
      "Epoch 174/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 94.4685 - val_loss: 544.7055\n",
      "Epoch 175/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 91.2098 - val_loss: 543.4149\n",
      "Epoch 176/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 88.0635 - val_loss: 542.1923\n",
      "Epoch 177/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 85.0271 - val_loss: 541.0357\n",
      "Epoch 178/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 82.0979 - val_loss: 539.9430\n",
      "Epoch 179/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 79.2730 - val_loss: 538.9124\n",
      "Epoch 180/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 76.5502 - val_loss: 537.9420\n",
      "Epoch 181/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 73.9268 - val_loss: 537.0298\n",
      "Epoch 182/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 71.4003 - val_loss: 536.1740\n",
      "Epoch 183/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 68.9682 - val_loss: 535.3728\n",
      "Epoch 184/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 66.6279 - val_loss: 534.6243\n",
      "Epoch 185/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 64.3770 - val_loss: 533.9267\n",
      "Epoch 186/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 62.2131 - val_loss: 533.2783\n",
      "Epoch 187/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 60.1338 - val_loss: 532.6772\n",
      "Epoch 188/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 58.1368 - val_loss: 532.1216\n",
      "Epoch 189/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 56.2195 - val_loss: 531.6101\n",
      "Epoch 190/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 54.3799 - val_loss: 531.1408\n",
      "Epoch 191/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 52.6155 - val_loss: 530.7120\n",
      "Epoch 192/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 50.9241 - val_loss: 530.3224\n",
      "Epoch 193/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 49.3035 - val_loss: 529.9700\n",
      "Epoch 194/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 47.7514 - val_loss: 529.6534\n",
      "Epoch 195/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 46.2660 - val_loss: 529.3710\n",
      "Epoch 196/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 44.8448 - val_loss: 529.1215\n",
      "Epoch 197/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 43.4859 - val_loss: 528.9030\n",
      "Epoch 198/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 42.1873 - val_loss: 528.7145\n",
      "Epoch 199/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 40.9468 - val_loss: 528.5542\n",
      "Epoch 200/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 39.7626 - val_loss: 528.4210\n",
      "Epoch 201/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 38.6327 - val_loss: 528.3133\n",
      "Epoch 202/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 37.5551 - val_loss: 528.2300\n",
      "Epoch 203/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 36.5281 - val_loss: 528.1696\n",
      "Epoch 204/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 35.5498 - val_loss: 528.1310\n",
      "Epoch 205/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 34.6185 - val_loss: 528.1129\n",
      "Epoch 206/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 33.7323 - val_loss: 528.1140\n",
      "Epoch 207/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 32.8896 - val_loss: 528.1334\n",
      "Epoch 208/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 32.0887 - val_loss: 528.1696\n",
      "Epoch 209/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 31.3280 - val_loss: 528.2217\n",
      "Epoch 210/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 30.6059 - val_loss: 528.2887\n",
      "Epoch 211/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 29.9208 - val_loss: 528.3695\n",
      "Epoch 212/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 29.2711 - val_loss: 528.4631\n",
      "Epoch 213/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 28.6557 - val_loss: 528.5683\n",
      "Epoch 214/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 28.0728 - val_loss: 528.6846\n",
      "Epoch 215/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 27.5211 - val_loss: 528.8108\n",
      "Epoch 216/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 26.9993 - val_loss: 528.9460\n",
      "Epoch 217/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 26.5061 - val_loss: 529.0897\n",
      "Epoch 218/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 26.0401 - val_loss: 529.2407\n",
      "Epoch 219/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.6001 - val_loss: 529.3984\n",
      "Epoch 220/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 25.1851 - val_loss: 529.5621\n",
      "Epoch 221/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 24.7937 - val_loss: 529.7310\n",
      "Epoch 222/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 24.4249 - val_loss: 529.9045\n",
      "Epoch 223/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 24.0776 - val_loss: 530.0819\n",
      "Epoch 224/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 23.7508 - val_loss: 530.2626\n",
      "Epoch 225/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 23.4433 - val_loss: 530.4460\n",
      "Epoch 226/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 23.1544 - val_loss: 530.6315\n",
      "Epoch 227/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 22.8829 - val_loss: 530.8188\n",
      "Epoch 228/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.6281 - val_loss: 531.0070\n",
      "Epoch 229/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 22.3890 - val_loss: 531.1961\n",
      "Epoch 230/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 22.1649 - val_loss: 531.3853\n",
      "Epoch 231/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 21.9549 - val_loss: 531.5744\n",
      "Epoch 232/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.7582 - val_loss: 531.7627\n",
      "Epoch 233/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.5741 - val_loss: 531.9502\n",
      "Epoch 234/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.4019 - val_loss: 532.1364\n",
      "Epoch 235/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.2410 - val_loss: 532.3210\n",
      "Epoch 236/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.0907 - val_loss: 532.5037\n",
      "Epoch 237/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.9504 - val_loss: 532.6843\n",
      "Epoch 238/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.8194 - val_loss: 532.8624\n",
      "Epoch 239/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.6973 - val_loss: 533.0379\n",
      "Epoch 240/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.5836 - val_loss: 533.2105\n",
      "Epoch 241/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.4776 - val_loss: 533.3802\n",
      "Epoch 242/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.3790 - val_loss: 533.5465\n",
      "Epoch 243/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.2873 - val_loss: 533.7097\n",
      "Epoch 244/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 20.2020 - val_loss: 533.8694\n",
      "Epoch 245/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 20.1227 - val_loss: 534.0252\n",
      "Epoch 246/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 20.0491 - val_loss: 534.1775\n",
      "Epoch 247/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.9807 - val_loss: 534.3262\n",
      "Epoch 248/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 19.9174 - val_loss: 534.4709\n",
      "Epoch 249/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.8586 - val_loss: 534.6117\n",
      "Epoch 250/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.8041 - val_loss: 534.7487\n",
      "Epoch 251/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.7536 - val_loss: 534.8816\n",
      "Epoch 252/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.7068 - val_loss: 535.0106\n",
      "Epoch 253/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.6635 - val_loss: 535.1356\n",
      "Epoch 254/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.6235 - val_loss: 535.2566\n",
      "Epoch 255/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.5865 - val_loss: 535.3738\n",
      "Epoch 256/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.5522 - val_loss: 535.4868\n",
      "Epoch 257/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.5205 - val_loss: 535.5961\n",
      "Epoch 258/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.4913 - val_loss: 535.7013\n",
      "Epoch 259/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.4643 - val_loss: 535.8028\n",
      "Epoch 260/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.4394 - val_loss: 535.9006\n",
      "Epoch 261/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.4165 - val_loss: 535.9947\n",
      "Epoch 262/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.3952 - val_loss: 536.0850\n",
      "Epoch 263/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.3756 - val_loss: 536.1717\n",
      "Epoch 264/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 19.3576 - val_loss: 536.2549\n",
      "Epoch 265/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.3410 - val_loss: 536.3348\n",
      "Epoch 266/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.3257 - val_loss: 536.4114\n",
      "Epoch 267/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.3116 - val_loss: 536.4844\n",
      "Epoch 268/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2986 - val_loss: 536.5543\n",
      "Epoch 269/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2866 - val_loss: 536.6213\n",
      "Epoch 270/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2757 - val_loss: 536.6850\n",
      "Epoch 271/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2655 - val_loss: 536.7457\n",
      "Epoch 272/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2562 - val_loss: 536.8037\n",
      "Epoch 273/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2476 - val_loss: 536.8591\n",
      "Epoch 274/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2398 - val_loss: 536.9117\n",
      "Epoch 275/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2325 - val_loss: 536.9617\n",
      "Epoch 276/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 19.2259 - val_loss: 537.0091\n",
      "Epoch 277/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2198 - val_loss: 537.0540\n",
      "Epoch 278/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2143 - val_loss: 537.0969\n",
      "Epoch 279/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2091 - val_loss: 537.1374\n",
      "Epoch 280/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2045 - val_loss: 537.1758\n",
      "Epoch 281/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2001 - val_loss: 537.2120\n",
      "Epoch 282/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 19.1962 - val_loss: 537.2463\n",
      "Epoch 283/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.1927 - val_loss: 537.2790\n",
      "Epoch 284/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1893 - val_loss: 537.3096\n",
      "Epoch 285/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1864 - val_loss: 537.3384\n",
      "Epoch 286/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1836 - val_loss: 537.3655\n",
      "Epoch 287/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1812 - val_loss: 537.3912\n",
      "Epoch 288/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.1789 - val_loss: 537.4153\n",
      "Epoch 289/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1769 - val_loss: 537.4379\n",
      "Epoch 290/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1751 - val_loss: 537.4592\n",
      "Epoch 291/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1734 - val_loss: 537.4793\n",
      "Epoch 292/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 19.1719 - val_loss: 537.4979\n",
      "Epoch 293/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1706 - val_loss: 537.5156\n",
      "Epoch 294/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1694 - val_loss: 537.5319\n",
      "Epoch 295/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1684 - val_loss: 537.5473\n",
      "Epoch 296/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1675 - val_loss: 537.5616\n",
      "Epoch 297/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1667 - val_loss: 537.5750\n",
      "Epoch 298/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1660 - val_loss: 537.5875\n",
      "Epoch 299/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1653 - val_loss: 537.5991\n",
      "Epoch 300/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1648 - val_loss: 537.6100\n",
      "Epoch 301/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 19.1644 - val_loss: 537.6201\n",
      "Epoch 302/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 19.1641 - val_loss: 537.6295\n",
      "Epoch 303/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1638 - val_loss: 537.6381\n",
      "Epoch 304/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1636 - val_loss: 537.6461\n",
      "Epoch 305/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1634 - val_loss: 537.6534\n",
      "Epoch 306/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1634 - val_loss: 537.6605\n",
      "Epoch 307/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1634 - val_loss: 537.6667\n",
      "Epoch 308/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1634 - val_loss: 537.6726\n",
      "Epoch 309/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1634 - val_loss: 537.6780\n",
      "Epoch 310/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1635 - val_loss: 537.6829\n",
      "Epoch 311/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1637 - val_loss: 537.6874\n",
      "Epoch 312/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1639 - val_loss: 537.6916\n",
      "Epoch 313/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1641 - val_loss: 537.6952\n",
      "Epoch 314/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.1644 - val_loss: 537.6990\n",
      "Epoch 315/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1646 - val_loss: 537.7019\n",
      "Epoch 316/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.1650 - val_loss: 537.7048\n",
      "Epoch 317/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 19.1653 - val_loss: 537.7075\n",
      "Epoch 318/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.1657 - val_loss: 537.7098\n",
      "Epoch 319/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 19.1660 - val_loss: 537.7119\n",
      "Epoch 320/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.1664 - val_loss: 537.7137\n",
      "Epoch 321/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1668 - val_loss: 537.7152\n",
      "Epoch 322/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1673 - val_loss: 537.7169\n",
      "Epoch 323/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1677 - val_loss: 537.7183\n",
      "Epoch 324/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1681 - val_loss: 537.7193\n",
      "Epoch 325/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1686 - val_loss: 537.7203\n",
      "Epoch 326/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1691 - val_loss: 537.7211\n",
      "Epoch 327/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1696 - val_loss: 537.7220\n",
      "Epoch 328/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1701 - val_loss: 537.7226\n",
      "Epoch 329/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1706 - val_loss: 537.7232\n",
      "Epoch 330/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1711 - val_loss: 537.7236\n",
      "Epoch 331/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1716 - val_loss: 537.7241\n",
      "Epoch 332/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1721 - val_loss: 537.7242\n",
      "Epoch 333/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1726 - val_loss: 537.7243\n",
      "Epoch 334/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 19.1731 - val_loss: 537.7244\n",
      "Epoch 335/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1736 - val_loss: 537.7244\n",
      "Epoch 336/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1742 - val_loss: 537.7244\n",
      "Epoch 337/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1747 - val_loss: 537.7244\n",
      "Epoch 338/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1752 - val_loss: 537.7242\n",
      "Epoch 339/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1757 - val_loss: 537.7241\n",
      "Epoch 340/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1762 - val_loss: 537.7238\n",
      "Epoch 341/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1767 - val_loss: 537.7236\n",
      "Epoch 342/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.1773 - val_loss: 537.7233\n",
      "Epoch 343/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.1778 - val_loss: 537.7230\n",
      "Epoch 344/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1783 - val_loss: 537.7228\n",
      "Epoch 345/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 19.1788 - val_loss: 537.7224\n",
      "Epoch 346/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1793 - val_loss: 537.7220\n",
      "Epoch 347/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 19.1798 - val_loss: 537.7217\n",
      "Epoch 348/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1803 - val_loss: 537.7213\n",
      "Epoch 349/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 19.1808 - val_loss: 537.7209\n",
      "Epoch 350/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1813 - val_loss: 537.7204\n",
      "Epoch 351/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.1818 - val_loss: 537.7199\n",
      "Epoch 352/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.1823 - val_loss: 537.7195\n",
      "Epoch 353/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.1828 - val_loss: 537.7191\n",
      "Epoch 354/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.1832 - val_loss: 537.7187\n",
      "Epoch 355/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.1837 - val_loss: 537.7181\n",
      "Epoch 356/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1842 - val_loss: 537.7178\n",
      "Epoch 357/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.1846 - val_loss: 537.7172\n",
      "Epoch 358/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1851 - val_loss: 537.7168\n",
      "Epoch 359/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1855 - val_loss: 537.7161\n",
      "Epoch 360/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1860 - val_loss: 537.7158\n",
      "Epoch 361/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1864 - val_loss: 537.7153\n",
      "Epoch 362/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1868 - val_loss: 537.7148\n",
      "Epoch 363/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 19.1873 - val_loss: 537.7143\n",
      "Epoch 364/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1877 - val_loss: 537.7138\n",
      "Epoch 365/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1881 - val_loss: 537.7134\n",
      "Epoch 366/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1885 - val_loss: 537.7130\n",
      "Epoch 367/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1889 - val_loss: 537.7122\n",
      "Epoch 368/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1893 - val_loss: 537.7119\n",
      "Epoch 369/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1897 - val_loss: 537.7115\n",
      "Epoch 370/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1901 - val_loss: 537.7110\n",
      "Epoch 371/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1905 - val_loss: 537.7104\n",
      "Epoch 372/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1909 - val_loss: 537.7101\n",
      "Epoch 373/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1912 - val_loss: 537.7095\n",
      "Epoch 374/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1916 - val_loss: 537.7092\n",
      "Epoch 375/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1920 - val_loss: 537.7087\n",
      "Epoch 376/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1924 - val_loss: 537.7083\n",
      "Epoch 377/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.1927 - val_loss: 537.7079\n",
      "Epoch 378/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 19.1930 - val_loss: 537.7075\n",
      "Epoch 379/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1934 - val_loss: 537.7072\n",
      "Epoch 380/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1937 - val_loss: 537.7065\n",
      "Epoch 381/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1940 - val_loss: 537.7061\n",
      "Epoch 382/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1944 - val_loss: 537.7058\n",
      "Epoch 383/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1947 - val_loss: 537.7053\n",
      "Epoch 384/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1950 - val_loss: 537.7048\n",
      "Epoch 385/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1953 - val_loss: 537.7046\n",
      "Epoch 386/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1956 - val_loss: 537.7040\n",
      "Epoch 387/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1959 - val_loss: 537.7037\n",
      "Epoch 388/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1962 - val_loss: 537.7032\n",
      "Epoch 389/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1965 - val_loss: 537.7029\n",
      "Epoch 390/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1968 - val_loss: 537.7026\n",
      "Epoch 391/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1970 - val_loss: 537.7021\n",
      "Epoch 392/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.1973 - val_loss: 537.7018\n",
      "Epoch 393/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 19.1976 - val_loss: 537.7015\n",
      "Epoch 394/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1978 - val_loss: 537.7010\n",
      "Epoch 395/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1982 - val_loss: 537.7006\n",
      "Epoch 396/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.1984 - val_loss: 537.7004\n",
      "Epoch 397/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1986 - val_loss: 537.7000\n",
      "Epoch 398/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1989 - val_loss: 537.6998\n",
      "Epoch 399/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1991 - val_loss: 537.6995\n",
      "Epoch 400/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1993 - val_loss: 537.6989\n",
      "Epoch 401/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.1996 - val_loss: 537.6987\n",
      "Epoch 402/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.1998 - val_loss: 537.6984\n",
      "Epoch 403/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.2001 - val_loss: 537.6979\n",
      "Epoch 404/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.2003 - val_loss: 537.6977\n",
      "Epoch 405/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.2005 - val_loss: 537.6975\n",
      "Epoch 406/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 19.2007 - val_loss: 537.6970\n",
      "Epoch 407/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2009 - val_loss: 537.6967\n",
      "Epoch 408/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2012 - val_loss: 537.6965\n",
      "Epoch 409/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2013 - val_loss: 537.6963\n",
      "Epoch 410/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2015 - val_loss: 537.6960\n",
      "Epoch 411/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2017 - val_loss: 537.6957\n",
      "Epoch 412/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2019 - val_loss: 537.6955\n",
      "Epoch 413/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.2021 - val_loss: 537.6951\n",
      "Epoch 414/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.2023 - val_loss: 537.6949\n",
      "Epoch 415/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.2024 - val_loss: 537.6944\n",
      "Epoch 416/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2027 - val_loss: 537.6943\n",
      "Epoch 417/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.2028 - val_loss: 537.6940\n",
      "Epoch 418/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.2030 - val_loss: 537.6937\n",
      "Epoch 419/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 19.2031 - val_loss: 537.6935\n",
      "Epoch 420/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.2033 - val_loss: 537.6932\n",
      "Epoch 421/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.2034 - val_loss: 537.6932\n",
      "Epoch 422/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.2036 - val_loss: 537.6928\n",
      "Epoch 423/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.2037 - val_loss: 537.6925\n",
      "Epoch 424/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.2039 - val_loss: 537.6923\n",
      "Epoch 425/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.2041 - val_loss: 537.6920\n",
      "Epoch 426/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.2042 - val_loss: 537.6918\n",
      "Epoch 427/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.2043 - val_loss: 537.6915\n",
      "Epoch 428/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.2045 - val_loss: 537.6913\n",
      "Epoch 429/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.2046 - val_loss: 537.6910\n",
      "Epoch 430/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.2047 - val_loss: 537.6909\n",
      "Epoch 431/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 19.2049 - val_loss: 537.6907\n",
      "Epoch 432/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.2050 - val_loss: 537.6904\n",
      "Epoch 433/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 19.2052 - val_loss: 537.6902\n",
      "Epoch 434/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 19.2053 - val_loss: 537.6899\n",
      "Epoch 435/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 19.2054 - val_loss: 537.6898\n",
      "Epoch 436/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.2055 - val_loss: 537.6896\n",
      "Epoch 437/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2056 - val_loss: 537.6894\n",
      "Epoch 438/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2057 - val_loss: 537.6890\n",
      "Epoch 439/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2059 - val_loss: 537.6888\n",
      "Epoch 440/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2059 - val_loss: 537.6887\n",
      "Epoch 441/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2060 - val_loss: 537.6886\n",
      "Epoch 442/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2061 - val_loss: 537.6884\n",
      "Epoch 443/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 19.2062 - val_loss: 537.6882\n",
      "Epoch 444/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2063 - val_loss: 537.6880\n",
      "Epoch 445/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 19.2065 - val_loss: 537.6879\n",
      "Epoch 446/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2065 - val_loss: 537.6877\n",
      "Epoch 447/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2066 - val_loss: 537.6873\n",
      "Epoch 448/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2068 - val_loss: 537.6871\n",
      "Epoch 449/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2068 - val_loss: 537.6871\n",
      "Epoch 450/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2069 - val_loss: 537.6870\n",
      "Epoch 451/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2070 - val_loss: 537.6866\n",
      "Epoch 452/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2071 - val_loss: 537.6865\n",
      "Epoch 453/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2072 - val_loss: 537.6865\n",
      "Epoch 454/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2072 - val_loss: 537.6863\n",
      "Epoch 455/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 19.2073 - val_loss: 537.6861\n",
      "Epoch 456/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2074 - val_loss: 537.6859\n",
      "Epoch 457/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2074 - val_loss: 537.6859\n",
      "Epoch 458/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 19.2075 - val_loss: 537.6858\n",
      "Epoch 459/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2076 - val_loss: 537.6856\n",
      "Epoch 460/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2076 - val_loss: 537.6855\n",
      "Epoch 461/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2077 - val_loss: 537.6854\n",
      "Epoch 462/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2077 - val_loss: 537.6852\n",
      "Epoch 463/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2079 - val_loss: 537.6852\n",
      "Epoch 464/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2079 - val_loss: 537.6848\n",
      "Epoch 465/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2080 - val_loss: 537.6847\n",
      "Epoch 466/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2080 - val_loss: 537.6846\n",
      "Epoch 467/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 19.2081 - val_loss: 537.6843\n",
      "Epoch 468/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.2082 - val_loss: 537.6841\n",
      "Epoch 469/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2082 - val_loss: 537.6840\n",
      "Epoch 470/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2083 - val_loss: 537.6840\n",
      "Epoch 471/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2083 - val_loss: 537.6838\n",
      "Epoch 472/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2084 - val_loss: 537.6837\n",
      "Epoch 473/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2084 - val_loss: 537.6836\n",
      "Epoch 474/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2085 - val_loss: 537.6834\n",
      "Epoch 475/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2085 - val_loss: 537.6833\n",
      "Epoch 476/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2086 - val_loss: 537.6833\n",
      "Epoch 477/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2087 - val_loss: 537.6832\n",
      "Epoch 478/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2087 - val_loss: 537.6830\n",
      "Epoch 479/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2087 - val_loss: 537.6830\n",
      "Epoch 480/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 19.2088 - val_loss: 537.6828\n",
      "Epoch 481/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.2088 - val_loss: 537.6826\n",
      "Epoch 482/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2089 - val_loss: 537.6826\n",
      "Epoch 483/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2089 - val_loss: 537.6825\n",
      "Epoch 484/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2089 - val_loss: 537.6824\n",
      "Epoch 485/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2090 - val_loss: 537.6823\n",
      "Epoch 486/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2090 - val_loss: 537.6822\n",
      "Epoch 487/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2090 - val_loss: 537.6820\n",
      "Epoch 488/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2091 - val_loss: 537.6819\n",
      "Epoch 489/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2091 - val_loss: 537.6817\n",
      "Epoch 490/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2091 - val_loss: 537.6816\n",
      "Epoch 491/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2092 - val_loss: 537.6815\n",
      "Epoch 492/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2092 - val_loss: 537.6815\n",
      "Epoch 493/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2092 - val_loss: 537.6813\n",
      "Epoch 494/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2093 - val_loss: 537.6813\n",
      "Epoch 495/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.2093 - val_loss: 537.6812\n",
      "Epoch 496/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 19.2094 - val_loss: 537.6811\n",
      "Epoch 497/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2094 - val_loss: 537.6809\n",
      "Epoch 498/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2094 - val_loss: 537.6808\n",
      "Epoch 499/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2094 - val_loss: 537.6807\n",
      "Epoch 500/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 19.2095 - val_loss: 537.6807\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6856, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0008, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0128, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(-2.3003, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 354ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.77647829e+01, 6.76748669e+01, 6.75849510e+01, 6.74950350e+01,\n",
       "        6.74051190e+01, 6.73152031e+01, 6.72252871e+01, 0.00000000e+00,\n",
       "        5.07241070e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.46943590e-01, 6.92841270e+01, 6.91160598e+01, 6.89479925e+01,\n",
       "        6.87799253e+01, 6.86118581e+01, 6.84437908e+01, 6.82757236e+01,\n",
       "        6.81076564e+01, 6.79395892e+01, 6.77847642e+01, 6.76948483e+01,\n",
       "        6.76049323e+01, 6.75150163e+01, 6.74251004e+01, 6.73351844e+01,\n",
       "        6.72452684e+01, 6.71553525e+01, 6.70654365e+01, 6.69755205e+01,\n",
       "        6.68856046e+01, 6.67956886e+01, 6.67057441e+01, 6.66158161e+01,\n",
       "        6.65258981e+01, 6.64359802e+01, 6.63460624e+01, 6.62561445e+01,\n",
       "        7.43834152e+01, 0.00000000e+00, 0.00000000e+00, 4.55104053e-01,\n",
       "        8.12869070e-02, 0.00000000e+00, 0.00000000e+00, 6.75349977e+01,\n",
       "        6.74450817e+01, 6.73551657e+01, 6.72652498e+01, 6.71753338e+01,\n",
       "        6.70854178e+01, 6.69955019e+01, 6.69055859e+01, 6.68156699e+01,\n",
       "        6.67255159e+01, 6.66305579e+01, 6.65355999e+01, 6.64406419e+01,\n",
       "        6.63468390e+01, 6.62540720e+01, 6.61612657e+01, 6.60684587e+01,\n",
       "        6.59756510e+01, 6.58828412e+01, 6.57900314e+01, 6.56972216e+01,\n",
       "        6.56044118e+01, 6.55116020e+01, 6.54187922e+01, 6.53259824e+01,\n",
       "        6.52331726e+01, 6.51403628e+01, 6.50475530e+01, 6.49547432e+01,\n",
       "        6.48619334e+01, 6.47691236e+01, 6.46763138e+01, 6.45835040e+01,\n",
       "        4.90063705e+01, 0.00000000e+00, 3.40098321e-01, 2.30149925e-01,\n",
       "        6.17533475e-02, 0.00000000e+00, 0.00000000e+00, 2.70521700e-01,\n",
       "        0.00000000e+00, 6.24974728e-01, 2.21945688e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.45203495e-02,\n",
       "        0.00000000e+00, 3.20691645e-01, 9.11838710e-02, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58.06210289, 58.04980529, 58.03750769, 58.02521008, 58.01291248,\n",
       "       58.00061488, 57.98831728, 57.97601968, 57.96372207, 57.95142447,\n",
       "       57.93912687, 57.92682927, 57.91453167, 57.90223406, 57.88993646,\n",
       "       57.87763886, 57.86534126, 57.85304366, 57.84074605, 57.82844845,\n",
       "       57.81615085, 57.80385325, 57.79155565, 57.77925804, 57.76696044,\n",
       "       57.75466284, 57.74236524, 57.73006764, 57.71777003, 57.70547243,\n",
       "       57.69317483, 57.68087723, 57.66857963, 57.65628203, 57.64398442,\n",
       "       57.63168682, 57.61938922, 57.60709162, 57.59479402, 57.58249641,\n",
       "       57.57019881, 57.55790121, 57.54560361, 57.53330601, 57.5210084 ,\n",
       "       57.5087108 , 57.4964132 , 57.4841156 , 57.471818  , 57.45952039,\n",
       "       57.44722279, 57.43492519, 57.42262759, 57.41032999, 57.39803238,\n",
       "       57.38573478, 57.37343718, 57.36113958, 57.34884198, 57.33654437,\n",
       "       57.32424677, 57.31194917, 57.29965157, 57.28735397, 57.27505636,\n",
       "       57.26275876, 57.25046116, 57.23816356, 57.22586596, 57.21356835,\n",
       "       57.20127075, 57.18897315, 57.17667555, 57.16437795, 57.15208034,\n",
       "       57.13978274, 57.12748514, 57.11518754, 57.10288994, 57.09059233,\n",
       "       57.07829473, 57.06599713, 57.05369953, 57.04140193, 57.02910432,\n",
       "       57.01680672, 57.00450912, 56.99221152, 56.97991392, 56.96761631,\n",
       "       56.95531871, 56.94302111, 56.93072351, 56.91842591, 56.9061283 ,\n",
       "       56.8938307 , 56.8815331 , 56.8692355 , 56.8569379 , 56.8446403 ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.761184954419065\n",
      "24.16695890660037\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
