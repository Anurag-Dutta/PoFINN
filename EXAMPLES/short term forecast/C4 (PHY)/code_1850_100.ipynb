{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "1945    63.275549\n",
       "1946    63.255474\n",
       "1947    63.235399\n",
       "1948    63.215324\n",
       "1949    63.195250\n",
       "Name: C4, Length: 1950, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c4_interpolated_1850_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "1845     0.000000\n",
       "1846     0.000000\n",
       "1847     0.746944\n",
       "1848     0.000000\n",
       "1849     0.215248\n",
       "Name: C4, Length: 1850, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1850)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.409524</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.219048</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.028571</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.838095</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.600000  0.000298\n",
       "1     88.409524  0.000298\n",
       "2     88.219048  0.000297\n",
       "3     88.028571  0.000297\n",
       "4     87.838095  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6856  # Value for C0\n",
    "K0 = -0.0008  # Value for K0\n",
    "K1 = -0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0128    # Value for b\n",
    "c = -2.3003    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.600000    88.409524    88.219048    88.028571    87.838095   \n",
      "351    88.409524    88.219048    88.028571    87.838095    87.647619   \n",
      "352    88.219048    88.028571    87.838095    87.647619    87.457143   \n",
      "353    88.028571    87.838095    87.647619    87.457143    87.266667   \n",
      "354    87.838095    87.647619    87.457143    87.266667    87.076190   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.647619    87.457143    87.266667    87.076190    86.896218  ...   \n",
      "351    87.457143    87.266667    87.076190    86.896218    86.845798  ...   \n",
      "352    87.266667    87.076190    86.896218    86.845798    86.795378  ...   \n",
      "353    87.076190    86.896218    86.845798    86.795378    86.744958  ...   \n",
      "354    86.896218    86.845798    86.795378    86.744958    86.694538  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   78.880345    0.000263   78.856069    0.000263   78.831793    0.000263   \n",
      "351   78.856069    0.000263   78.831793    0.000263   78.807516    0.000262   \n",
      "352   78.831793    0.000263   78.807516    0.000262   78.783240    0.000262   \n",
      "353   78.807516    0.000262   78.783240    0.000262   78.758964    0.000262   \n",
      "354   78.783240    0.000262   78.758964    0.000262   78.734687    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   78.807516    0.000262   78.783240    0.000262  \n",
      "351   78.783240    0.000262   78.758964    0.000262  \n",
      "352   78.758964    0.000262   78.734687    0.000262  \n",
      "353   78.734687    0.000262   78.710411    0.000262  \n",
      "354   78.710411    0.000262   78.686134    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 1, 251) (1400, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "18/18 [==============================] - 3s 29ms/step - loss: 5464.0474 - val_loss: 4476.9404\n",
      "Epoch 2/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 5399.6226 - val_loss: 4414.0674\n",
      "Epoch 3/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5333.5581 - val_loss: 4359.1978\n",
      "Epoch 4/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5280.7773 - val_loss: 4315.5151\n",
      "Epoch 5/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5233.0054 - val_loss: 4272.5928\n",
      "Epoch 6/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5186.0137 - val_loss: 4230.3193\n",
      "Epoch 7/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5139.6484 - val_loss: 4188.5679\n",
      "Epoch 8/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5093.0854 - val_loss: 4143.3403\n",
      "Epoch 9/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5042.0132 - val_loss: 4098.2524\n",
      "Epoch 10/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4992.5312 - val_loss: 4053.9866\n",
      "Epoch 11/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4944.0728 - val_loss: 4010.6299\n",
      "Epoch 12/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4896.4902 - val_loss: 3967.9790\n",
      "Epoch 13/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4849.5928 - val_loss: 3925.9050\n",
      "Epoch 14/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 4803.2676 - val_loss: 3884.3293\n",
      "Epoch 15/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4757.4443 - val_loss: 3843.2031\n",
      "Epoch 16/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4712.0781 - val_loss: 3802.4934\n",
      "Epoch 17/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4667.1372 - val_loss: 3762.1755\n",
      "Epoch 18/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4622.5981 - val_loss: 3722.2312\n",
      "Epoch 19/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4575.5029 - val_loss: 3667.9243\n",
      "Epoch 20/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4515.5444 - val_loss: 3623.1116\n",
      "Epoch 21/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4466.2041 - val_loss: 3579.4282\n",
      "Epoch 22/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4418.2466 - val_loss: 3536.9358\n",
      "Epoch 23/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4371.4282 - val_loss: 3495.3357\n",
      "Epoch 24/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4325.4746 - val_loss: 3454.4446\n",
      "Epoch 25/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4280.2236 - val_loss: 3414.1528\n",
      "Epoch 26/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4235.5771 - val_loss: 3374.3914\n",
      "Epoch 27/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4191.4707 - val_loss: 3335.1130\n",
      "Epoch 28/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4147.8579 - val_loss: 3296.2847\n",
      "Epoch 29/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4104.7080 - val_loss: 3257.8801\n",
      "Epoch 30/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 4061.9941 - val_loss: 3219.8789\n",
      "Epoch 31/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4019.6982 - val_loss: 3182.2656\n",
      "Epoch 32/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3977.8018 - val_loss: 3145.0276\n",
      "Epoch 33/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3936.2932 - val_loss: 3108.1526\n",
      "Epoch 34/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3895.1611 - val_loss: 3071.6316\n",
      "Epoch 35/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3854.3945 - val_loss: 3035.4558\n",
      "Epoch 36/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3813.9866 - val_loss: 2999.6187\n",
      "Epoch 37/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3773.9282 - val_loss: 2964.1135\n",
      "Epoch 38/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3734.2134 - val_loss: 2928.9346\n",
      "Epoch 39/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 3694.8372 - val_loss: 2894.0764\n",
      "Epoch 40/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3655.7927 - val_loss: 2859.5344\n",
      "Epoch 41/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3608.2449 - val_loss: 2813.1162\n",
      "Epoch 42/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3561.8381 - val_loss: 2773.2166\n",
      "Epoch 43/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3517.6289 - val_loss: 2734.7910\n",
      "Epoch 44/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3474.8616 - val_loss: 2697.4695\n",
      "Epoch 45/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3433.1628 - val_loss: 2660.9893\n",
      "Epoch 46/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3392.2983 - val_loss: 2625.1985\n",
      "Epoch 47/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3352.1289 - val_loss: 2590.0007\n",
      "Epoch 48/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3312.5664 - val_loss: 2555.3340\n",
      "Epoch 49/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3273.5493 - val_loss: 2521.1531\n",
      "Epoch 50/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 3235.0356 - val_loss: 2487.4255\n",
      "Epoch 51/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 3196.9910 - val_loss: 2454.1243\n",
      "Epoch 52/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 3159.3901 - val_loss: 2421.2302\n",
      "Epoch 53/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3122.2117 - val_loss: 2388.7258\n",
      "Epoch 54/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 3085.4390 - val_loss: 2356.5977\n",
      "Epoch 55/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3049.0581 - val_loss: 2324.8330\n",
      "Epoch 56/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3013.0569 - val_loss: 2293.4221\n",
      "Epoch 57/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 2977.4238 - val_loss: 2262.3562\n",
      "Epoch 58/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2942.1499 - val_loss: 2231.6270\n",
      "Epoch 59/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2907.2263 - val_loss: 2201.2278\n",
      "Epoch 60/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2872.6472 - val_loss: 2171.1521\n",
      "Epoch 61/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2838.4050 - val_loss: 2141.3926\n",
      "Epoch 62/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2804.4934 - val_loss: 2111.9465\n",
      "Epoch 63/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2770.9072 - val_loss: 2082.8064\n",
      "Epoch 64/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2737.6409 - val_loss: 2053.9697\n",
      "Epoch 65/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 2704.1626 - val_loss: 2019.1283\n",
      "Epoch 66/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2661.7170 - val_loss: 1984.6562\n",
      "Epoch 67/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2622.3975 - val_loss: 1951.2375\n",
      "Epoch 68/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2584.6470 - val_loss: 1919.1962\n",
      "Epoch 69/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2548.2485 - val_loss: 1888.1703\n",
      "Epoch 70/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 2512.8535 - val_loss: 1857.9316\n",
      "Epoch 71/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2478.2544 - val_loss: 1828.3446\n",
      "Epoch 72/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2444.3235 - val_loss: 1799.3226\n",
      "Epoch 73/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2410.9800 - val_loss: 1770.8090\n",
      "Epoch 74/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2378.1658 - val_loss: 1742.7612\n",
      "Epoch 75/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2345.8401 - val_loss: 1715.1493\n",
      "Epoch 76/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2313.9712 - val_loss: 1687.9480\n",
      "Epoch 77/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2282.5325 - val_loss: 1661.1370\n",
      "Epoch 78/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2251.5054 - val_loss: 1634.6996\n",
      "Epoch 79/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2220.8711 - val_loss: 1608.6228\n",
      "Epoch 80/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2190.6167 - val_loss: 1582.8948\n",
      "Epoch 81/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2160.7283 - val_loss: 1557.5057\n",
      "Epoch 82/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2131.1968 - val_loss: 1532.4458\n",
      "Epoch 83/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2102.0117 - val_loss: 1507.7070\n",
      "Epoch 84/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2073.1653 - val_loss: 1483.2832\n",
      "Epoch 85/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2044.6498 - val_loss: 1459.1667\n",
      "Epoch 86/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2016.4578 - val_loss: 1435.3525\n",
      "Epoch 87/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1987.4553 - val_loss: 1407.8049\n",
      "Epoch 88/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1953.2362 - val_loss: 1378.8613\n",
      "Epoch 89/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1919.7562 - val_loss: 1351.3064\n",
      "Epoch 90/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1887.8365 - val_loss: 1324.9512\n",
      "Epoch 91/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1857.1136 - val_loss: 1299.4857\n",
      "Epoch 92/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1827.2939 - val_loss: 1274.7255\n",
      "Epoch 93/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1798.2037 - val_loss: 1250.5602\n",
      "Epoch 94/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1769.7369 - val_loss: 1226.9180\n",
      "Epoch 95/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1741.8220 - val_loss: 1203.7496\n",
      "Epoch 96/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1714.4094 - val_loss: 1181.0187\n",
      "Epoch 97/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1687.4615 - val_loss: 1158.6973\n",
      "Epoch 98/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1660.9492 - val_loss: 1136.7627\n",
      "Epoch 99/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1634.8495 - val_loss: 1115.1975\n",
      "Epoch 100/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1609.1427 - val_loss: 1093.9857\n",
      "Epoch 101/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1583.8131 - val_loss: 1073.1154\n",
      "Epoch 102/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1558.8472 - val_loss: 1052.5758\n",
      "Epoch 103/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1534.2332 - val_loss: 1032.3557\n",
      "Epoch 104/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1509.9601 - val_loss: 1012.4470\n",
      "Epoch 105/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1486.0188 - val_loss: 992.8428\n",
      "Epoch 106/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1462.4015 - val_loss: 973.5353\n",
      "Epoch 107/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1439.1002 - val_loss: 954.5174\n",
      "Epoch 108/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1416.1082 - val_loss: 935.7848\n",
      "Epoch 109/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1393.4193 - val_loss: 917.3305\n",
      "Epoch 110/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1371.0273 - val_loss: 899.1505\n",
      "Epoch 111/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1348.9270 - val_loss: 881.2401\n",
      "Epoch 112/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1327.1139 - val_loss: 863.5944\n",
      "Epoch 113/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1305.5826 - val_loss: 846.2094\n",
      "Epoch 114/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1284.3289 - val_loss: 829.0808\n",
      "Epoch 115/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1263.3486 - val_loss: 812.2051\n",
      "Epoch 116/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1242.6376 - val_loss: 795.5790\n",
      "Epoch 117/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1222.1927 - val_loss: 779.1979\n",
      "Epoch 118/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1202.0093 - val_loss: 763.0603\n",
      "Epoch 119/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1182.0846 - val_loss: 747.1622\n",
      "Epoch 120/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1162.4153 - val_loss: 731.5002\n",
      "Epoch 121/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1142.9980 - val_loss: 716.0716\n",
      "Epoch 122/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1123.8301 - val_loss: 700.8737\n",
      "Epoch 123/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1104.9077 - val_loss: 685.9044\n",
      "Epoch 124/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1086.2290 - val_loss: 671.1600\n",
      "Epoch 125/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1067.7906 - val_loss: 656.6377\n",
      "Epoch 126/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1049.5902 - val_loss: 642.3363\n",
      "Epoch 127/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1031.6248 - val_loss: 628.2523\n",
      "Epoch 128/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1013.8924 - val_loss: 614.3843\n",
      "Epoch 129/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 996.3903 - val_loss: 600.7288\n",
      "Epoch 130/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 979.1157 - val_loss: 587.2843\n",
      "Epoch 131/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 962.0670 - val_loss: 574.0485\n",
      "Epoch 132/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 945.2415 - val_loss: 561.0194\n",
      "Epoch 133/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 928.6370 - val_loss: 548.1935\n",
      "Epoch 134/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 912.2510 - val_loss: 535.5703\n",
      "Epoch 135/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 896.0818 - val_loss: 523.1467\n",
      "Epoch 136/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 880.1272 - val_loss: 510.9214\n",
      "Epoch 137/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 864.3852 - val_loss: 498.8922\n",
      "Epoch 138/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 848.8537 - val_loss: 487.0570\n",
      "Epoch 139/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 833.5305 - val_loss: 475.4137\n",
      "Epoch 140/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 818.4136 - val_loss: 463.9607\n",
      "Epoch 141/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 803.5015 - val_loss: 452.6961\n",
      "Epoch 142/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 788.7921 - val_loss: 441.6180\n",
      "Epoch 143/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 774.2835 - val_loss: 430.7239\n",
      "Epoch 144/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 759.9736 - val_loss: 420.0126\n",
      "Epoch 145/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 745.8607 - val_loss: 409.4827\n",
      "Epoch 146/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 731.9432 - val_loss: 399.1315\n",
      "Epoch 147/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 718.2192 - val_loss: 388.9581\n",
      "Epoch 148/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 704.6872 - val_loss: 378.9599\n",
      "Epoch 149/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 691.3447 - val_loss: 369.1361\n",
      "Epoch 150/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 678.1907 - val_loss: 359.4839\n",
      "Epoch 151/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 665.2230 - val_loss: 350.0022\n",
      "Epoch 152/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 652.4403 - val_loss: 340.6893\n",
      "Epoch 153/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 639.8406 - val_loss: 331.5434\n",
      "Epoch 154/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 627.4222 - val_loss: 322.5631\n",
      "Epoch 155/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 615.1841 - val_loss: 313.7469\n",
      "Epoch 156/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 603.1242 - val_loss: 305.0924\n",
      "Epoch 157/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 591.2406 - val_loss: 296.5982\n",
      "Epoch 158/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 579.5320 - val_loss: 288.2631\n",
      "Epoch 159/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 567.9968 - val_loss: 280.0853\n",
      "Epoch 160/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 556.6334 - val_loss: 272.0634\n",
      "Epoch 161/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 545.4401 - val_loss: 264.1952\n",
      "Epoch 162/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 534.4155 - val_loss: 256.4795\n",
      "Epoch 163/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 523.5583 - val_loss: 248.9150\n",
      "Epoch 164/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 512.8663 - val_loss: 241.4994\n",
      "Epoch 165/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 502.3386 - val_loss: 234.2318\n",
      "Epoch 166/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 491.9734 - val_loss: 227.1105\n",
      "Epoch 167/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 481.7690 - val_loss: 220.1334\n",
      "Epoch 168/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 471.7237 - val_loss: 213.2995\n",
      "Epoch 169/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 461.8364 - val_loss: 206.6077\n",
      "Epoch 170/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 452.1060 - val_loss: 200.0556\n",
      "Epoch 171/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 442.5303 - val_loss: 193.6420\n",
      "Epoch 172/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 433.1080 - val_loss: 187.3654\n",
      "Epoch 173/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 423.8376 - val_loss: 181.2245\n",
      "Epoch 174/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 414.7178 - val_loss: 175.2172\n",
      "Epoch 175/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 405.7471 - val_loss: 169.3422\n",
      "Epoch 176/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 396.9236 - val_loss: 163.5986\n",
      "Epoch 177/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 388.2466 - val_loss: 157.9843\n",
      "Epoch 178/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 379.7143 - val_loss: 152.4978\n",
      "Epoch 179/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 371.3253 - val_loss: 147.1380\n",
      "Epoch 180/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 363.0778 - val_loss: 141.9030\n",
      "Epoch 181/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 354.9708 - val_loss: 136.7917\n",
      "Epoch 182/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 347.0028 - val_loss: 131.8022\n",
      "Epoch 183/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 339.1721 - val_loss: 126.9336\n",
      "Epoch 184/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 331.4776 - val_loss: 122.1837\n",
      "Epoch 185/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 323.9177 - val_loss: 117.5515\n",
      "Epoch 186/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 316.4910 - val_loss: 113.0355\n",
      "Epoch 187/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 309.1964 - val_loss: 108.6344\n",
      "Epoch 188/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 302.0320 - val_loss: 104.3463\n",
      "Epoch 189/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 294.9963 - val_loss: 100.1698\n",
      "Epoch 190/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 288.0887 - val_loss: 96.1040\n",
      "Epoch 191/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 281.3071 - val_loss: 92.1469\n",
      "Epoch 192/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 274.6503 - val_loss: 88.2973\n",
      "Epoch 193/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 268.1172 - val_loss: 84.5536\n",
      "Epoch 194/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 261.7060 - val_loss: 80.9146\n",
      "Epoch 195/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 255.4156 - val_loss: 77.3787\n",
      "Epoch 196/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 249.2446 - val_loss: 73.9443\n",
      "Epoch 197/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 243.1913 - val_loss: 70.6103\n",
      "Epoch 198/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 237.2546 - val_loss: 67.3748\n",
      "Epoch 199/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 231.4330 - val_loss: 64.2369\n",
      "Epoch 200/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 225.7252 - val_loss: 61.1951\n",
      "Epoch 201/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 220.1299 - val_loss: 58.2477\n",
      "Epoch 202/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 214.6459 - val_loss: 55.3934\n",
      "Epoch 203/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 209.2717 - val_loss: 52.6312\n",
      "Epoch 204/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 204.0059 - val_loss: 49.9589\n",
      "Epoch 205/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 198.8470 - val_loss: 47.3758\n",
      "Epoch 206/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 193.7940 - val_loss: 44.8800\n",
      "Epoch 207/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 188.8453 - val_loss: 42.4706\n",
      "Epoch 208/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 183.9996 - val_loss: 40.1455\n",
      "Epoch 209/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 179.2557 - val_loss: 37.9040\n",
      "Epoch 210/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 174.6120 - val_loss: 35.7444\n",
      "Epoch 211/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 170.0674 - val_loss: 33.6654\n",
      "Epoch 212/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 165.6206 - val_loss: 31.6655\n",
      "Epoch 213/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 161.2704 - val_loss: 29.7435\n",
      "Epoch 214/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 157.0151 - val_loss: 27.8979\n",
      "Epoch 215/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 152.8534 - val_loss: 26.1271\n",
      "Epoch 216/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 148.7843 - val_loss: 24.4304\n",
      "Epoch 217/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 144.8064 - val_loss: 22.8059\n",
      "Epoch 218/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 140.9184 - val_loss: 21.2524\n",
      "Epoch 219/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 137.1190 - val_loss: 19.7686\n",
      "Epoch 220/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 133.4070 - val_loss: 18.3530\n",
      "Epoch 221/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 129.7809 - val_loss: 17.0044\n",
      "Epoch 222/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 126.2396 - val_loss: 15.7215\n",
      "Epoch 223/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 122.7816 - val_loss: 14.5028\n",
      "Epoch 224/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 119.4059 - val_loss: 13.3471\n",
      "Epoch 225/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 116.1110 - val_loss: 12.2531\n",
      "Epoch 226/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 112.8957 - val_loss: 11.2195\n",
      "Epoch 227/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.7590 - val_loss: 10.2449\n",
      "Epoch 228/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 106.6994 - val_loss: 9.3281\n",
      "Epoch 229/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 103.7156 - val_loss: 8.4678\n",
      "Epoch 230/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.8064 - val_loss: 7.6626\n",
      "Epoch 231/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 97.9707 - val_loss: 6.9114\n",
      "Epoch 232/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 95.2071 - val_loss: 6.2128\n",
      "Epoch 233/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 92.5146 - val_loss: 5.5656\n",
      "Epoch 234/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 89.8919 - val_loss: 4.9685\n",
      "Epoch 235/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 87.3377 - val_loss: 4.4203\n",
      "Epoch 236/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 84.8510 - val_loss: 3.9198\n",
      "Epoch 237/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 82.4303 - val_loss: 3.4657\n",
      "Epoch 238/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 80.0746 - val_loss: 3.0568\n",
      "Epoch 239/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 77.7826 - val_loss: 2.6919\n",
      "Epoch 240/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 75.5532 - val_loss: 2.3698\n",
      "Epoch 241/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 73.3853 - val_loss: 2.0893\n",
      "Epoch 242/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 71.2776 - val_loss: 1.8492\n",
      "Epoch 243/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 69.2292 - val_loss: 1.6483\n",
      "Epoch 244/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 67.2389 - val_loss: 1.4855\n",
      "Epoch 245/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 65.3052 - val_loss: 1.3597\n",
      "Epoch 246/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 63.4274 - val_loss: 1.2697\n",
      "Epoch 247/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 61.6041 - val_loss: 1.2143\n",
      "Epoch 248/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 59.8344 - val_loss: 1.1924\n",
      "Epoch 249/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 58.1171 - val_loss: 1.2030\n",
      "Epoch 250/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 56.4512 - val_loss: 1.2449\n",
      "Epoch 251/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 54.8355 - val_loss: 1.3171\n",
      "Epoch 252/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 53.2689 - val_loss: 1.4184\n",
      "Epoch 253/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 51.7503 - val_loss: 1.5478\n",
      "Epoch 254/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 50.2789 - val_loss: 1.7044\n",
      "Epoch 255/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 48.8535 - val_loss: 1.8869\n",
      "Epoch 256/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 47.4730 - val_loss: 2.0944\n",
      "Epoch 257/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 46.1365 - val_loss: 2.3260\n",
      "Epoch 258/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 44.8429 - val_loss: 2.5805\n",
      "Epoch 259/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 43.5914 - val_loss: 2.8571\n",
      "Epoch 260/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 42.3808 - val_loss: 3.1547\n",
      "Epoch 261/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 41.2102 - val_loss: 3.4724\n",
      "Epoch 262/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 40.0787 - val_loss: 3.8093\n",
      "Epoch 263/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 38.9851 - val_loss: 4.1644\n",
      "Epoch 264/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 37.9286 - val_loss: 4.5368\n",
      "Epoch 265/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 36.9084 - val_loss: 4.9257\n",
      "Epoch 266/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 35.9235 - val_loss: 5.3300\n",
      "Epoch 267/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 34.9729 - val_loss: 5.7491\n",
      "Epoch 268/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 34.0557 - val_loss: 6.1821\n",
      "Epoch 269/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 33.1711 - val_loss: 6.6280\n",
      "Epoch 270/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 32.3183 - val_loss: 7.0862\n",
      "Epoch 271/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 31.4962 - val_loss: 7.5557\n",
      "Epoch 272/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 30.7043 - val_loss: 8.0359\n",
      "Epoch 273/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 29.9414 - val_loss: 8.5257\n",
      "Epoch 274/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 29.2070 - val_loss: 9.0248\n",
      "Epoch 275/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 28.5001 - val_loss: 9.5321\n",
      "Epoch 276/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 27.8200 - val_loss: 10.0471\n",
      "Epoch 277/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 27.1659 - val_loss: 10.5690\n",
      "Epoch 278/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 26.5370 - val_loss: 11.0971\n",
      "Epoch 279/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 25.9326 - val_loss: 11.6307\n",
      "Epoch 280/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.3520 - val_loss: 12.1692\n",
      "Epoch 281/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.7943 - val_loss: 12.7120\n",
      "Epoch 282/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.2589 - val_loss: 13.2586\n",
      "Epoch 283/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 23.7451 - val_loss: 13.8082\n",
      "Epoch 284/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 23.2522 - val_loss: 14.3602\n",
      "Epoch 285/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.7796 - val_loss: 14.9140\n",
      "Epoch 286/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.3265 - val_loss: 15.4693\n",
      "Epoch 287/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 21.8924 - val_loss: 16.0253\n",
      "Epoch 288/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 21.4765 - val_loss: 16.5817\n",
      "Epoch 289/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 21.0784 - val_loss: 17.1378\n",
      "Epoch 290/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 20.6974 - val_loss: 17.6934\n",
      "Epoch 291/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 20.3327 - val_loss: 18.2479\n",
      "Epoch 292/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.9841 - val_loss: 18.8006\n",
      "Epoch 293/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 19.6508 - val_loss: 19.3514\n",
      "Epoch 294/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.3323 - val_loss: 19.8996\n",
      "Epoch 295/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 19.0281 - val_loss: 20.4451\n",
      "Epoch 296/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 18.7377 - val_loss: 20.9874\n",
      "Epoch 297/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 18.4604 - val_loss: 21.5261\n",
      "Epoch 298/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 18.1960 - val_loss: 22.0609\n",
      "Epoch 299/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 17.9438 - val_loss: 22.5914\n",
      "Epoch 300/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 17.7034 - val_loss: 23.1173\n",
      "Epoch 301/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 17.4742 - val_loss: 23.6385\n",
      "Epoch 302/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 17.2560 - val_loss: 24.1544\n",
      "Epoch 303/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 17.0483 - val_loss: 24.6649\n",
      "Epoch 304/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 16.8506 - val_loss: 25.1698\n",
      "Epoch 305/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.6625 - val_loss: 25.6688\n",
      "Epoch 306/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 16.4836 - val_loss: 26.1616\n",
      "Epoch 307/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 16.3136 - val_loss: 26.6482\n",
      "Epoch 308/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 16.1521 - val_loss: 27.1280\n",
      "Epoch 309/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 15.9987 - val_loss: 27.6012\n",
      "Epoch 310/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 15.8530 - val_loss: 28.0674\n",
      "Epoch 311/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 15.7149 - val_loss: 28.5264\n",
      "Epoch 312/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 15.5838 - val_loss: 28.9785\n",
      "Epoch 313/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 15.4596 - val_loss: 29.4233\n",
      "Epoch 314/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 15.3418 - val_loss: 29.8605\n",
      "Epoch 315/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 15.2302 - val_loss: 30.2902\n",
      "Epoch 316/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 15.1245 - val_loss: 30.7123\n",
      "Epoch 317/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 15.0245 - val_loss: 31.1265\n",
      "Epoch 318/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.9299 - val_loss: 31.5331\n",
      "Epoch 319/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.8404 - val_loss: 31.9322\n",
      "Epoch 320/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 14.7557 - val_loss: 32.3228\n",
      "Epoch 321/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.6757 - val_loss: 32.7057\n",
      "Epoch 322/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.6002 - val_loss: 33.0807\n",
      "Epoch 323/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.5288 - val_loss: 33.4476\n",
      "Epoch 324/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.4615 - val_loss: 33.8068\n",
      "Epoch 325/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 14.3979 - val_loss: 34.1579\n",
      "Epoch 326/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.3379 - val_loss: 34.5009\n",
      "Epoch 327/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 14.2814 - val_loss: 34.8360\n",
      "Epoch 328/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.2281 - val_loss: 35.1633\n",
      "Epoch 329/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.1779 - val_loss: 35.4827\n",
      "Epoch 330/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.1306 - val_loss: 35.7942\n",
      "Epoch 331/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.0861 - val_loss: 36.0980\n",
      "Epoch 332/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 14.0442 - val_loss: 36.3938\n",
      "Epoch 333/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 14.0048 - val_loss: 36.6822\n",
      "Epoch 334/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.9677 - val_loss: 36.9628\n",
      "Epoch 335/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.9328 - val_loss: 37.2360\n",
      "Epoch 336/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 13.9001 - val_loss: 37.5015\n",
      "Epoch 337/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.8693 - val_loss: 37.7599\n",
      "Epoch 338/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 13.8404 - val_loss: 38.0108\n",
      "Epoch 339/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.8132 - val_loss: 38.2546\n",
      "Epoch 340/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.7877 - val_loss: 38.4914\n",
      "Epoch 341/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7638 - val_loss: 38.7211\n",
      "Epoch 342/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7414 - val_loss: 38.9438\n",
      "Epoch 343/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 13.7203 - val_loss: 39.1601\n",
      "Epoch 344/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.7006 - val_loss: 39.3694\n",
      "Epoch 345/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.6820 - val_loss: 39.5723\n",
      "Epoch 346/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.6647 - val_loss: 39.7685\n",
      "Epoch 347/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.6485 - val_loss: 39.9586\n",
      "Epoch 348/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.6332 - val_loss: 40.1426\n",
      "Epoch 349/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.6190 - val_loss: 40.3205\n",
      "Epoch 350/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 13.6056 - val_loss: 40.4923\n",
      "Epoch 351/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.5931 - val_loss: 40.6584\n",
      "Epoch 352/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.5814 - val_loss: 40.8187\n",
      "Epoch 353/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.5704 - val_loss: 40.9734\n",
      "Epoch 354/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.5602 - val_loss: 41.1228\n",
      "Epoch 355/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.5506 - val_loss: 41.2669\n",
      "Epoch 356/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 13.5416 - val_loss: 41.4056\n",
      "Epoch 357/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.5332 - val_loss: 41.5393\n",
      "Epoch 358/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.5254 - val_loss: 41.6681\n",
      "Epoch 359/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.5180 - val_loss: 41.7922\n",
      "Epoch 360/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.5112 - val_loss: 41.9112\n",
      "Epoch 361/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.5048 - val_loss: 42.0261\n",
      "Epoch 362/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4988 - val_loss: 42.1363\n",
      "Epoch 363/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 13.4932 - val_loss: 42.2424\n",
      "Epoch 364/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4879 - val_loss: 42.3441\n",
      "Epoch 365/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4831 - val_loss: 42.4419\n",
      "Epoch 366/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 13.4785 - val_loss: 42.5357\n",
      "Epoch 367/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4743 - val_loss: 42.6256\n",
      "Epoch 368/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4703 - val_loss: 42.7117\n",
      "Epoch 369/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4666 - val_loss: 42.7943\n",
      "Epoch 370/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4632 - val_loss: 42.8735\n",
      "Epoch 371/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 13.4599 - val_loss: 42.9491\n",
      "Epoch 372/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4569 - val_loss: 43.0216\n",
      "Epoch 373/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4541 - val_loss: 43.0908\n",
      "Epoch 374/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.4515 - val_loss: 43.1572\n",
      "Epoch 375/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4491 - val_loss: 43.2202\n",
      "Epoch 376/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.4469 - val_loss: 43.2810\n",
      "Epoch 377/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4448 - val_loss: 43.3387\n",
      "Epoch 378/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4428 - val_loss: 43.3940\n",
      "Epoch 379/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4410 - val_loss: 43.4467\n",
      "Epoch 380/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4393 - val_loss: 43.4966\n",
      "Epoch 381/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.4377 - val_loss: 43.5443\n",
      "Epoch 382/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 13.4363 - val_loss: 43.5897\n",
      "Epoch 383/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4349 - val_loss: 43.6331\n",
      "Epoch 384/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4337 - val_loss: 43.6742\n",
      "Epoch 385/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4325 - val_loss: 43.7134\n",
      "Epoch 386/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4314 - val_loss: 43.7505\n",
      "Epoch 387/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4305 - val_loss: 43.7861\n",
      "Epoch 388/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4296 - val_loss: 43.8196\n",
      "Epoch 389/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4288 - val_loss: 43.8515\n",
      "Epoch 390/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4280 - val_loss: 43.8818\n",
      "Epoch 391/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4273 - val_loss: 43.9105\n",
      "Epoch 392/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 13.4267 - val_loss: 43.9377\n",
      "Epoch 393/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4261 - val_loss: 43.9633\n",
      "Epoch 394/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4256 - val_loss: 43.9878\n",
      "Epoch 395/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4251 - val_loss: 44.0107\n",
      "Epoch 396/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4247 - val_loss: 44.0325\n",
      "Epoch 397/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4243 - val_loss: 44.0532\n",
      "Epoch 398/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4240 - val_loss: 44.0727\n",
      "Epoch 399/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4237 - val_loss: 44.0909\n",
      "Epoch 400/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.4234 - val_loss: 44.1083\n",
      "Epoch 401/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 13.4232 - val_loss: 44.1246\n",
      "Epoch 402/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 13.4230 - val_loss: 44.1401\n",
      "Epoch 403/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 13.4228 - val_loss: 44.1545\n",
      "Epoch 404/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4227 - val_loss: 44.1682\n",
      "Epoch 405/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4226 - val_loss: 44.1810\n",
      "Epoch 406/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4225 - val_loss: 44.1930\n",
      "Epoch 407/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4225 - val_loss: 44.2043\n",
      "Epoch 408/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 13.4224 - val_loss: 44.2151\n",
      "Epoch 409/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4224 - val_loss: 44.2252\n",
      "Epoch 410/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4224 - val_loss: 44.2345\n",
      "Epoch 411/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4225 - val_loss: 44.2431\n",
      "Epoch 412/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4225 - val_loss: 44.2516\n",
      "Epoch 413/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4226 - val_loss: 44.2591\n",
      "Epoch 414/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4226 - val_loss: 44.2662\n",
      "Epoch 415/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4228 - val_loss: 44.2730\n",
      "Epoch 416/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.4228 - val_loss: 44.2795\n",
      "Epoch 417/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4230 - val_loss: 44.2854\n",
      "Epoch 418/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4231 - val_loss: 44.2908\n",
      "Epoch 419/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.4232 - val_loss: 44.2957\n",
      "Epoch 420/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4234 - val_loss: 44.3003\n",
      "Epoch 421/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 13.4236 - val_loss: 44.3049\n",
      "Epoch 422/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4237 - val_loss: 44.3088\n",
      "Epoch 423/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4239 - val_loss: 44.3125\n",
      "Epoch 424/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4241 - val_loss: 44.3157\n",
      "Epoch 425/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4243 - val_loss: 44.3193\n",
      "Epoch 426/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4245 - val_loss: 44.3222\n",
      "Epoch 427/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4247 - val_loss: 44.3246\n",
      "Epoch 428/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.4250 - val_loss: 44.3270\n",
      "Epoch 429/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4252 - val_loss: 44.3294\n",
      "Epoch 430/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 13.4254 - val_loss: 44.3315\n",
      "Epoch 431/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4257 - val_loss: 44.3334\n",
      "Epoch 432/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4259 - val_loss: 44.3353\n",
      "Epoch 433/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4261 - val_loss: 44.3369\n",
      "Epoch 434/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.4264 - val_loss: 44.3382\n",
      "Epoch 435/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4266 - val_loss: 44.3396\n",
      "Epoch 436/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4268 - val_loss: 44.3407\n",
      "Epoch 437/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4271 - val_loss: 44.3415\n",
      "Epoch 438/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 13.4274 - val_loss: 44.3425\n",
      "Epoch 439/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4276 - val_loss: 44.3432\n",
      "Epoch 440/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4279 - val_loss: 44.3439\n",
      "Epoch 441/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4282 - val_loss: 44.3444\n",
      "Epoch 442/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4284 - val_loss: 44.3453\n",
      "Epoch 443/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4287 - val_loss: 44.3456\n",
      "Epoch 444/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.4290 - val_loss: 44.3458\n",
      "Epoch 445/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4292 - val_loss: 44.3461\n",
      "Epoch 446/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 13.4295 - val_loss: 44.3465\n",
      "Epoch 447/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.4297 - val_loss: 44.3466\n",
      "Epoch 448/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4300 - val_loss: 44.3466\n",
      "Epoch 449/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4303 - val_loss: 44.3466\n",
      "Epoch 450/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.4306 - val_loss: 44.3466\n",
      "Epoch 451/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4309 - val_loss: 44.3467\n",
      "Epoch 452/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.4311 - val_loss: 44.3466\n",
      "Epoch 453/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4314 - val_loss: 44.3464\n",
      "Epoch 454/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4316 - val_loss: 44.3462\n",
      "Epoch 455/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 13.4319 - val_loss: 44.3462\n",
      "Epoch 456/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.4322 - val_loss: 44.3460\n",
      "Epoch 457/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4324 - val_loss: 44.3458\n",
      "Epoch 458/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4327 - val_loss: 44.3455\n",
      "Epoch 459/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4330 - val_loss: 44.3449\n",
      "Epoch 460/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4333 - val_loss: 44.3446\n",
      "Epoch 461/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4335 - val_loss: 44.3443\n",
      "Epoch 462/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 13.4338 - val_loss: 44.3441\n",
      "Epoch 463/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.4340 - val_loss: 44.3437\n",
      "Epoch 464/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4343 - val_loss: 44.3434\n",
      "Epoch 465/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4345 - val_loss: 44.3429\n",
      "Epoch 466/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 13.4348 - val_loss: 44.3426\n",
      "Epoch 467/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4351 - val_loss: 44.3420\n",
      "Epoch 468/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.4353 - val_loss: 44.3417\n",
      "Epoch 469/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4356 - val_loss: 44.3414\n",
      "Epoch 470/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4358 - val_loss: 44.3410\n",
      "Epoch 471/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.4360 - val_loss: 44.3404\n",
      "Epoch 472/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4363 - val_loss: 44.3400\n",
      "Epoch 473/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4366 - val_loss: 44.3396\n",
      "Epoch 474/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.4368 - val_loss: 44.3390\n",
      "Epoch 475/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 13.4371 - val_loss: 44.3387\n",
      "Epoch 476/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.4373 - val_loss: 44.3382\n",
      "Epoch 477/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4375 - val_loss: 44.3379\n",
      "Epoch 478/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4377 - val_loss: 44.3374\n",
      "Epoch 479/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4380 - val_loss: 44.3371\n",
      "Epoch 480/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4382 - val_loss: 44.3366\n",
      "Epoch 481/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4384 - val_loss: 44.3360\n",
      "Epoch 482/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.4387 - val_loss: 44.3356\n",
      "Epoch 483/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4389 - val_loss: 44.3353\n",
      "Epoch 484/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 13.4391 - val_loss: 44.3349\n",
      "Epoch 485/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4393 - val_loss: 44.3344\n",
      "Epoch 486/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4395 - val_loss: 44.3339\n",
      "Epoch 487/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4398 - val_loss: 44.3334\n",
      "Epoch 488/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4400 - val_loss: 44.3329\n",
      "Epoch 489/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.4402 - val_loss: 44.3326\n",
      "Epoch 490/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4404 - val_loss: 44.3320\n",
      "Epoch 491/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.4406 - val_loss: 44.3315\n",
      "Epoch 492/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4408 - val_loss: 44.3312\n",
      "Epoch 493/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4410 - val_loss: 44.3306\n",
      "Epoch 494/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4412 - val_loss: 44.3302\n",
      "Epoch 495/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4415 - val_loss: 44.3300\n",
      "Epoch 496/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4416 - val_loss: 44.3295\n",
      "Epoch 497/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.4418 - val_loss: 44.3291\n",
      "Epoch 498/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.4420 - val_loss: 44.3288\n",
      "Epoch 499/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 13.4422 - val_loss: 44.3284\n",
      "Epoch 500/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.4424 - val_loss: 44.3277\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6856, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0008, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0128, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(-2.3003, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 340ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.92841270e+01, 6.92281046e+01, 6.91720822e+01, 6.91160598e+01,\n",
       "        6.90600374e+01, 6.90040149e+01, 6.89479925e+01, 6.88919701e+01,\n",
       "        6.88359477e+01, 6.87799253e+01, 6.87239029e+01, 6.86678805e+01,\n",
       "        6.86118581e+01, 6.85558357e+01, 6.84998133e+01, 6.84437908e+01,\n",
       "        6.83877684e+01, 6.83317460e+01, 6.82757236e+01, 6.82197012e+01,\n",
       "        6.81636788e+01, 6.81076564e+01, 6.80516340e+01, 6.79956116e+01,\n",
       "        6.79395892e+01, 6.78835668e+01, 6.78275443e+01, 6.77847642e+01,\n",
       "        6.77547922e+01, 6.77248203e+01, 6.76948483e+01, 6.76648763e+01,\n",
       "        6.76349043e+01, 6.76049323e+01, 6.75749603e+01, 6.75449883e+01,\n",
       "        6.75150163e+01, 6.74850443e+01, 6.74550724e+01, 6.74251004e+01,\n",
       "        6.73951284e+01, 6.73651564e+01, 6.73351844e+01, 6.73052124e+01,\n",
       "        6.72752404e+01, 6.72452684e+01, 6.72152964e+01, 6.71853245e+01,\n",
       "        6.71553525e+01, 6.71253805e+01, 6.70954085e+01, 6.70654365e+01,\n",
       "        6.70354645e+01, 6.70054925e+01, 6.69755205e+01, 6.69455485e+01,\n",
       "        6.69155766e+01, 6.68856046e+01, 6.68556326e+01, 6.68256606e+01,\n",
       "        6.67956886e+01, 6.67657166e+01, 6.67357446e+01, 6.67044141e+01,\n",
       "        6.66727614e+01, 6.66411088e+01, 6.66094561e+01, 6.65778035e+01,\n",
       "        6.65461508e+01, 6.65144981e+01, 6.64828455e+01, 6.64511928e+01,\n",
       "        6.64195402e+01, 6.63878875e+01, 6.63562348e+01, 6.63245822e+01,\n",
       "        6.62929295e+01, 6.62612768e+01, 6.62296242e+01, 6.61979715e+01,\n",
       "        7.43834152e+01, 7.29776502e-01, 3.92893776e-02, 0.00000000e+00,\n",
       "        8.01512241e-01, 6.50058270e-01, 0.00000000e+00, 5.47762811e-01,\n",
       "        0.00000000e+00, 4.55104053e-01, 1.23954430e-01, 0.00000000e+00,\n",
       "        8.12869072e-02, 2.27773413e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.80281007e-01, 4.02470976e-02, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([64.37878151, 64.37037815, 64.36197479, 64.35357143, 64.34516807,\n",
       "       64.33676471, 64.32836134, 64.31995798, 64.31155462, 64.30315126,\n",
       "       64.2947479 , 64.28634454, 64.27794118, 64.26953782, 64.26113445,\n",
       "       64.25273109, 64.24432773, 64.23592437, 64.22752101, 64.21911765,\n",
       "       64.21071429, 64.20231092, 64.19390756, 64.1855042 , 64.17710084,\n",
       "       64.16869748, 64.16029412, 64.15189076, 64.14348739, 64.13508403,\n",
       "       64.12668067, 64.11827731, 64.10987395, 64.10147059, 64.09306723,\n",
       "       64.08466387, 64.0762605 , 64.06785714, 64.05945378, 64.05105042,\n",
       "       64.04264706, 64.0342437 , 64.02584034, 64.01743697, 64.00903361,\n",
       "       64.00063025, 63.99222689, 63.98382353, 63.97542017, 63.96701681,\n",
       "       63.95861345, 63.95021008, 63.94180672, 63.93340336, 63.925     ,\n",
       "       63.91659664, 63.90819328, 63.89978992, 63.89138655, 63.88298319,\n",
       "       63.87457983, 63.86617647, 63.85777311, 63.84936975, 63.84096639,\n",
       "       63.83256303, 63.82415966, 63.8157563 , 63.80735294, 63.79749066,\n",
       "       63.77741597, 63.75734127, 63.73726657, 63.71719188, 63.69711718,\n",
       "       63.67704248, 63.65696779, 63.63689309, 63.61681839, 63.5967437 ,\n",
       "       63.576669  , 63.5565943 , 63.53651961, 63.51644491, 63.49637021,\n",
       "       63.47629552, 63.45622082, 63.43614613, 63.41607143, 63.39599673,\n",
       "       63.37592204, 63.35584734, 63.33577264, 63.31569795, 63.29562325,\n",
       "       63.27554855, 63.25547386, 63.23539916, 63.21532446, 63.19524977])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.737354709340877\n",
      "14.89929291642432\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
