{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "2245    58.123591\n",
       "2246    58.111293\n",
       "2247    58.098996\n",
       "2248    58.086698\n",
       "2249    58.074400\n",
       "Name: C4, Length: 2250, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c4_interpolated_2150_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "2145     0.084953\n",
       "2146     0.000000\n",
       "2147     0.179084\n",
       "2148     0.000000\n",
       "2149     0.000000\n",
       "Name: C4, Length: 2150, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2150)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.409524</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.219048</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.028571</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.838095</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.600000  0.000298\n",
       "1     88.409524  0.000298\n",
       "2     88.219048  0.000297\n",
       "3     88.028571  0.000297\n",
       "4     87.838095  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAAsTAAALEwEAmpwYAAAoQUlEQVR4nO3dd3xUVfrH8c+TTkmoIQlNQClSpEWwAgoKYgFXrKhYcXddd9V1Xd1ud3VtKKLYde2rv5W1gIANBIGggICUUJQEQpPQAyQ5vz/mJgwhIW2Smcl836/XvDJz59y5D5fkPnPKPcecc4iISOSKCnYAIiISXEoEIiIRTolARCTCKRGIiEQ4JQIRkQgXE+wAqqJ58+auXbt2wQ5DRCSszJ8/f4tzLrnk9rBMBO3atSMjIyPYYYiIhBUz+7G07WoaEhGJcEoEIiIRTolARCTCKRGIiEQ4JQIRkQinRCAiEuGUCEREIlxEJYL/fpfNv78pdRitiEjEiqhE8PH3G3h19tpghyEiElIiKhG0bFyP7G170WI8IiIHRVgiSGD3/gJ25OUHOxQRkZARYYmgHgAbtu8NciQiIqEjIhPB+lwlAhGRIhGVCFp5iSA7Ny/IkYiIhI6ISgTNG8YTE2WqEYiI+ImoRBAdZaQ2SmCDEoGISLGISgTg6ydYr6YhEZFiEZcIWjWuR7ZqBCIixSIuEbRsnEDOjjwKCnVTmYgIRGAi6JyaREGhY8bKzcEORUQkJERcIhjWLZWUpHgmfrU62KGIiISEiEsEcTFRXHtKe2at2sqirNxghyMiEnQRlwgALu3XlsT4GJ5VrUBEJDITQWJCLKNPOIpPvt/Aj1t3BzscEZGgCkgiMLNhZrbczDLN7I5S3h9gZt+aWb6ZjSrx3hgzW+k9xgQinoq4+uR2xERF8cS0lZqWWkQiWrUTgZlFA+OBs4CuwKVm1rVEsZ+Aq4A3SuzbFPg70B/oB/zdzJpUN6aKSElK4NpT2/P+d9n88b1FHCgorI3DioiEnJgAfEY/INM5txrAzN4CRgBLiwo459Z675W82g4FpjrnfvbenwoMA94MQFzlun1oZ+Kio3hi+ko279zH+NF9qB8XiFMiIhI+AtE01ApY5/c6y9sW0H3NbKyZZZhZxubNgbkHwMy45YxO3Hd+d75csZlLn5vDz7v3B+SzRUTCRdh0FjvnJjrn0p1z6cnJyQH97NH9j+KZy/uybMMORk2Yxbqf9wT080VEQlkgEkE20MbvdWtvW03vG1Bndkvl9ev6s3X3fi6YMEurmIlIxAhEIpgHdDSz9mYWB1wCTKrgvlOAM82siddJfKa3LSjS2zXl7RtOYM/+Aq5/NYO9+wuCFYqISK2pdiJwzuUDv8F3Af8BeMc5t8TM7jaz8wDM7HgzywIuBJ41syXevj8D9+BLJvOAu4s6joOlS2oS4y7txZL1O7jtPws1tFRE6jwLxwtdenq6y8jIqNFjPPvlKh74ZBm3ntGJ3w7uWKPHEhGpDWY23zmXXnK7xkqWYeyADizP2cmjU1fQKaUhw7qnBTskEZEaETajhmqbmXH/L3rQu21jbnl7IUvWbw92SCIiNUKJ4AgSYqN59vK+NKoXy9hX57Nl175ghyQiEnBKBOVokZTAc1ems3X3Pq59JYPd+/KDHZKISEApEVRAj9aNGHdJbxZnb2fsaxnkHdCwUhGpO5QIKujMbqk8dMFxfJ25ld+++R35mqROROoIJYJKuKBva/5xblc+XbqR299bRGFh+A29FREpScNHK+mqk9uzIy+fR6euIDE+hr+d243oKAt2WCIiVaZEUAU3nX4MO/Ye4PmZa/g+ezsPX9iTo5MbBjssEZEqUdNQFZgZfz77WB6/uBerNu9m+BMzmPjVKgrUVCQiYUiJoIrMjJG9WzH1lgGc2jGZ+z9exqhnZpG5aVewQxMRqRQlgmry3WfQlycu6cWaLbsZPm4Gz36p2oGIhA8lggAwM0b0asWntwxgUKdkHvhkGRdMmEXmpp3BDk1EpFxKBAHUIjGBZ6/w1Q7Wbt3N8HEzeebLVbrnQERCmhJBgBXVDqbeMpDTOifz4CfLuOCZ2azcqNqBiIQmJYIakpwYzzOX9+XJS3vz09bdnD1uJk9/kanagYiEHCWCGmRmnNuzJZ/eMpDTu7TgocnLuWDCLNUORCSkKBHUguTEeCZc3oenLuvNum17VTsQkZCiRFBLzIxzjmvJp7cMYEhXX+3gFxNmsUK1AxEJMiWCWta8YTxPj+7L+Mv6kLVtL+eMm8n4z1U7EJHgUSIIkrOPS2PqLQM4o2sKD09ZzvlPz2J5jmoHIlL7lAiCqFnDeMaP7sP4y/qwPncv5zw5g6c+W8kB1Q5EpBYpEYSAs49L49NbBnBmt1T+9ekKzn/6a5bl7Ah2WCISIZQIQkSzhvGMv6wPT4/uw4bcPM59ciZPTlftQERqnhJBiBneI42ptw5kWPc0Hpm6gpHjv2bhutxghyUidVhAEoGZDTOz5WaWaWZ3lPJ+vJm97b0/x8zaedvbmdleM1vgPZ4JRDzhrmmDOJ68tDfPXN6HjTvyGDH+ay56djZTluRoVlMRCbhqr1BmZtHAeOAMIAuYZ2aTnHNL/YpdC2xzzh1jZpcA/wQu9t5b5ZzrVd046qJh3dM46ZjmvD13HS/PWssNr82nTdN6XHVSey5Kb01iQmywQxSROiAQNYJ+QKZzbrVzbj/wFjCiRJkRwCve8/8Ag81MC/1WQFJCLNcP6MCXfxjEhNF9SE1K4J4Pl3LiA59x1/+W8OPW3cEOUUTCXCDWLG4FrPN7nQX0L6uMcy7fzLYDzbz32pvZd8AO4C/OuRmlHcTMxgJjAdq2bRuAsMNLTHQUZ/VI46weaSzKyuXFmWt4bfaPvDxrLUOOTeHaU9rTv31TlF9FpLKCvXj9BqCtc26rmfUF/mtm3Zxzh42ddM5NBCYCpKenR3RD+XGtG/P4Jb25c/ixvDb7R16f8yNTl26ka1oS15zSnnN7phEfEx3sMEUkTASiaSgbaOP3urW3rdQyZhYDNAK2Ouf2Oee2Ajjn5gOrgE4BiCkipCQlcNvQzsy+czAP/qIH+YWF3PbuQk5+8DMen7aCzTv3BTtEEQkDgUgE84COZtbezOKAS4BJJcpMAsZ4z0cBnznnnJkle53NmFkHoCOwOgAxRZSE2Ggu6deWKTcP4N/X9vfVGKat5OQHP+PV2WuDHZ6IhLhqNw15bf6/AaYA0cCLzrklZnY3kOGcmwS8ALxmZpnAz/iSBcAA4G4zOwAUAr90zv1c3ZgilZlxSsfmnNKxOas27+K+j37gbx8soaDQcfXJ7YMdnoiEKHMu/Jrb09PTXUZGRrDDCHkHCgq56Y3vmLwkh7+d05VrTlEyEIlkZjbfOZdecrvuLK7DYqOjePKy3pzVPZW7P1zKCzPXBDskEQlBSgR1XGx0FOMu7c3wHqnc8+FSnp+hLhgROVSwh49KLYiNjuKJS3pjLODej34A4LpTOwQ5KhEJFUoEESI2OorHL+kFwL0f/YBzcP0AJQMRUSKIKL6aQS8wuO/jH3A4xg44OthhiUiQKRFEmJjoKJ64uBcG3P/xMpyDGwYqGYhEMiWCCBQTHcXjF/fCzHjgk2U44JdKBiIRS4kgQsVER/HYRT0x4MFPfDWDXw1SMhCJREoEESwmOopHL+qJGfxz8jIcjl8POibYYYlILVMiiHAx0VE8cmFPAB6avBzn4MbTlAxEIokSgXg1g15EmfHwlOXsO1DALWd00toGIhFCiUAAiI4y/nVhT+Kioxj3WSZ5+YXceVYXJQORCKBEIMWio4wHftGDhNgoJn61mrfm/kSrJvVp1bgerRon0LJxPVo1qUfLxvVo3bgezRvGExWlRCES7pQI5BBRUcY/zutG91aNWJS1nezcvWRt28Oc1VvZuS//kLJx0VGkNU6gZaN6xUmiKGF0a9mIpg3igvSvEJHKUCKQw5gZF6a34cL0Nods35F3gPW5e8netpf1uXvJyt3L+tw81ufu5evMLWzcmUfRrOb146L57eCOXHNye+JiNLehSChTIpAKS0qIJSk1li6pSaW+f6CgkJzteaz7eQ8vfr2WBz9Zxjvz1vH387oxsFNyLUcrIhWlr2oSMLHRUbRpWp+TjmnO82PSeenq43HAmBfnMvbVDNb9vCfYIYpIKZQIpMac1rkFk28+lduHdWbGyi0MefRLHp+2grwDBcEOTSSoFq7L5anPVgY7jGJKBFKj4mOi+fWgY/jstoGc2S2Vx6etZMijXzJlSQ7huEyqSCCMGP81//p0RbDDKKZEILUirVE9nry0N29efwL146K54bX5jHlpHqs27wp2aCIRT4lAatWJRzfjo9+eyt/O6cp3P25j2ONf8eAny9hdYmiqiNQeJQKpdbHRUVxzSns+u20QI3u14pkvV3H6I1/wwYJsNReJBIESgQRNcmI8D1/Yk/d/fRItEhP43VsLuHjiN3yftT3YoYlEFCUCCbo+bZvw3xtP5v7ze7By407OfWomZ4+bwYsz17B1175ghydSY0KlBqxEICEhOsq4rH9bvrjtNO46rxtRZtz94VL63z+dsa9m8OmSHA4UFAY7TKkjnHMsWJcb7DAoDI08EJhEYGbDzGy5mWWa2R2lvB9vZm97788xs3Z+793pbV9uZkMDEY+Er0b1YxlzUjv+d9MpTL75VK4+uR3f/pTL2Nfmc8L907nrf0tYsl5NR1I9b85dx8jxXzNt6cagxlFYV2oEZhYNjAfOAroCl5pZ1xLFrgW2OeeOAR4D/unt2xW4BOgGDAOe9j5PhC6pSfz57K7MvvN0XhiTTv8OTXn9m584e9xMznpiBi/MXMMWNR1JFWRu8g1bXrt1d6X3venN7/h0SU5A4giRPBCQGkE/INM5t9o5tx94CxhRoswI4BXv+X+Aweab6H4E8JZzbp9zbg2Q6X2eSLHY6CgGH5vC06P7MudPg7l7RDdio417PlzKCfdP5/pXM5i8OIf9+Wo6koopWmajKhfi/y1cz9jX5gckju+zc3l+xuqAfFZ1BGLSuVbAOr/XWUD/sso45/LNbDvQzNv+TYl9W5V2EDMbC4wFaNu2bQDClnDUpEEcV57YjitPbMeKjTt5b34W73+XzdSlG2lSP5YRvVpxab+2dE5NDHaoEsKKltFwVP0r+eLs7WRt28Ow7mlV/owLJswG4JqT2wd1bY+w6Sx2zk10zqU759KTkzWTpUCnlETuHH4ss+84nZeuOp6Tjm7OG3N+YtgTX3HrOwtYn7s32CFKiIryqgTV6ax9/9tsbnt3UUDi2ZkX3BsqA1EjyAb8J65v7W0rrUyWmcUAjYCtFdxX5IhioqM4rUsLTuvSgm279/PMl6t4adZaPlq0gWtOac+vBh1NUkJssMOUUOJ9+a5OZ+2LX6+pdhix0caBAse2Pfu5/+Mf2LZnPxOvTD+s3BPTVvLYtBWc2rE5r11bssGl+gJRI5gHdDSz9mYWh6/zd1KJMpOAMd7zUcBnzjeAdhJwiTeqqD3QEZgbgJgkQjVpEMedw4/ls98PZHiPNCZ8sYqBD33OS1+vUR+CFCuqEQS7s7boC8q2PfvZuDOPjTvyDnn/gY9/4JwnZ7B6i69ze82WynduV0S1E4FzLh/4DTAF+AF4xzm3xMzuNrPzvGIvAM3MLBO4FbjD23cJ8A6wFJgM3Oic0xzFUm2tm9TnsYt78eFNp3BsWhJ3/W8pZzz2JR8t2hAyN/FI8BQ1xxcGeSD/1t37Acjdc4CCQofZwX6C/IJCnv1qNYuzd/DRog3AwQQWaAFZocw59zHwcYltf/N7ngdcWMa+9wH3BSIOkZK6t2rE69f154sVm3nw42Xc+Ma39G7bmD8PP5b0dk2DHZ4EiXGwj2D3vnz25xfSJIhrbG/bsx/nfDdWfrViM/XioslYu634/XwvYdVQHtBSlVL3mRmndW7BgI7J/Gf+Oh75dAWjnpnN0G4p3D6sC0cnNwx2iFLL/EcN3f/xD0xZkkPGX84IWjx/+2AJBYUOh+PKF8tuHQ/pGoFIOIiOMi4+vi3n9mzJCzPW8MyXq5j2w1dc1q8tvxvSkeYN44MdotQS8xs1ZBb8qR52VXAa9poaYBo2w0dFAqV+XAw3De7IF384jUv7teGNuT8x8KHPeXL6SvbuVxdVJDjYWeyIMqtyv1H3VkmV3qeqa3cP6pxcY01DSgQSsZIT47l3ZA8+vWUAJx/TnEemrmDQvz7nnXnrgt6JKDUryu/O4iizCtcI/BNGYkIML445vgaiK92BgsJDOpMDSYlAIt7RyQ2ZeGU679xwImmN6nH7e4sY9cwslufsDHZoUkPM7z4CX9NQxTKBf7HkxHhaJCVU+tix0WVfdo90nT9Q4NQ0JFLT+rVvyv/9+iT+dWFP1mzZzdnjZvDwlGXkHVBzUV2TEOub27JJ/TivaajynxFTxSkhjjStxZHi2J9fWGOdxUoEIn7MjFF9WzP994M4r1dLxn++imGPf8XXmVuCHZoEUIr3Tb5nm8ZEVaZG4Pc8Oqpql8+q3sbiaxqq2r7lUSIQKUXTBnE8elEvXr/Odzv/6OfncOvbC7RiWh1RdC2+6NnZXh9BRZuGDpa7d2S3ah27svbnq49AJChOPqY5k28ewG9OO4ZJC9cz5NEv+c/8LN2dHOYO+f+zin9L9y/W96javSHxQEGh+ghEgiUhNprbhnbm49+dSofkhtz27kJGPz+nxuZ9kdp1IN9VPBE4/+dV+zJQ1f0SYqOpYmtUuZQIRCqoU0oi795wIved353vs7cz9PGveHL6Sk1mF4b8r8W79h2oRB/BwXKzV2+t1rGHHJtS4X1G9mpJWqOE4qkxAk2JQKQSoqKM0f2PYvqtAzmjawqPTF3B2eNmkLH252CHJpXgf0HftS+/StNRN4ir3sQMCbGHX34T42N4aNRxh2133qOm1q7RFBMiVdAiKYHxl/VhVJ9N/OW/ixn1zGwSE2JISUogNSnB97NRPKmN6pFatK1RPM0bxAd1JSrxObRGUFCJG8oOPm8QX7Xl1Ys+I9rv9+Cj357C2eNmsmt/Phelt+Gv/13MPr+aZqHzpsHQXEMioee0Li2YeusA3p63jh+37iFnex45O/JYtWoLm3buo6DEFSYmykhJSiAlKZ7URr6Ekeb9TE1KKN5WNM5dakbRxfi5K9OZ/+M2vlqxGedcpUbl1K9ijaCoNhLtd6zWTeofElfLxvUO6YNyznnTYVTpkOVSIhCppvpxMVx9cvvDthcUOrbs2lecHDbuyGPD9jw2eq+X5ezki+Wb2VPK/EZN6sd6tYqDNYy0RgmkeK9TkxJoXD+2xoYT1nVF6blTSkN+2LAD8H3jji7ndBZdqGOjjcb1q7bqXWk1gkb1YrlnRDeOatYAgMtPOIp7Plx6SLzO1dykc0oEIjUkuvjbfwI9yyjjnGPnvvzi5JCzPe+QxJGzI4/F2TvYUsr9C/ExUcU1iFS/pFG8rVECLRLjjzilQV2yeec+Mjftos9RjYmPOXKNqmjkjmF+8w45yrvUFn2b//2ZnatRI/Dp3qoR787P4rjWjQC44sR2xWWuPaU9p3ZszpmPfVW8k8NpGmqRusjMSEqIJSkhlo4piWWW259fyCZvKcOc7fu8pLGXnB372Lg9jwXrcslZknfYCCYzaN4w/tB+i6SE4r6Lds3rFzdLhLsnpq/g39/8RMP4GAZ2SmZY91TOOS6t1FpT0cXY7NApqctT9G2+OpfjoiSUVC+GxXcN5UAZo846pSTyiz6teP/bbAqdo7BQC9OIRLS4mChaNznyRds5x7Y9B8jZfrA24d8UlbVtDxk//kzungOH7HfVSe24fVjVv+GGij37C2hcP5azuqcy7YdNfPT9Bnbty+fSfm0PL+x30Y8qTgQVHzkUiAuyYTSMj4EjLYPhheS8GkFNNQWG9/+8iBQzM5o2iKNpgzi6tix7nvy9+wuKE8Un32/g5Vlr+WL5Jh6+sCfHh/Hync75FoN/4BfHcV+h4/wJs3jqs0wu6NOauJjSm8d8NYKD+5d7jKL9qlEn8K+NVHwf51tEp8pHPbLIaDwUkWL14qJp17wBJ3Roxl0juvPm9SdQ4BwXPTubez9cGrazrTpvSmnw3e9x85COZOfu5b1vsw4vS9EawAf7CAoqkAmK+xaqcUWuyo3FzruRQJPOiUiNOPHoZkz+3QAu738Uz89cw/AnZvDtT9vK3zHEFLpD1/Qd1CmZnq0bMf7zzMP6Tvzb+otG6ny+bFO5xwjMDFMHk1BFj1dYw53FSgQiQoP4GO4Z2Z3Xr+vPvvxCRk2YxQOf/BBWtYNCvxoB+C60Nw/pRNa2vbxfolbg3zwz5NgUOrZoyBPTVx5230dJgZhrsGodzq54feWaoEQgIsV8s62eysXHt+XZL1dzzpMzWbguN9hhVUhpgz8HdfbVCp4qUSs4eDE2oqOM3w3pSOamXXy4aH35B6Fi3+bL+YjK9RE46N4yiW4tG1X5uEeiRCAih0hMiOWBX/TglWv6sXtfPuc//TUPTV7GvvzQrh0ULUTvr6xawcE+At/r4d3T6JySyLgK1AqgusNHiz6jAk1DXmEH3DWiO38afmw1jlw2JQIRKdXATslMuWUAo/q25ukvVnHek1/zfdb2YIdVJldG00lptYKSzTNRXq1g1ebdR6wVlEwgVYqzCj0NVZkUrzKqlQjMrKmZTTWzld7PJmWUG+OVWWlmY/y2f2Fmy81sgfdoUZ14RCSwkhJieWhUT1666nhy9+5n5NNf8+iny0Ny6u3CUmoEUHqtoLTL6rBuqXRJTTxiX0Fgbigriqvy+9SU6tYI7gCmO+c6AtO914cws6bA34H+QD/g7yUSxmjnXC/vUX63vYjUutO6tODTmwcyoldLxn2WyXlPzWTumtCaevtIF8vDagWlXNGjoozfDe7I6s27mbQwu/RjBDDeiuQBV+JnTaluIhgBvOI9fwUYWUqZocBU59zPzrltwFRgWDWPKyK1rFH9WB69qBfPXZnOjr0HuOjZ2YyaMIvXZq8NibWcSw4f9edfK3hs2ooybwwb6tUKHp68nJzteYd9zsH7CHz7ZW7aVek4q1YjCOGmISDFObfBe54DlLbkTitgnd/rLG9bkZe8ZqG/2hG64s1srJllmFnG5s2bqxm2iFTVGV1TmP77Qdx5Vhd25B3grx8sod/90xnz4lze/zaLXfvygxSZO+LFdVDnZC7t14YJX6zizbm+S1LJ8lFRxkOjjmNHXj6jn//msMn+/Ef8TFmSwxmPfXnY0NTyozyYhiq8T7CbhsxsmpktLuUxwr+cc0X3vlXKaOdcD+BU73FFWQWdcxOdc+nOufTk5ORKHkZEAqleXDQ3DDyaT28ZyOSbT2XsgA5kbtrFre8spO89U7nx9W+ZvDinVu9DOFKNAHzf4u8d2YNze7Ysnnq6tNLHtW7MC2PSyc7dyxUvzCV3z/7DPwtfZ/qJHZpx27sLmbx4w+EfVIbK1AiKylalg7kyyk0EzrkhzrnupTw+ADaaWRqA97O0Nv5soI3f69beNpxzRT93Am/g60MQkTDSJTWJPw7rwsw/nsZ7vzqRS45vwzert/LLf8/n+Pum8Yd3FzJz5ZYKDcusDueOXCMA39Tgj17UkyHHtiAmyspcAKh/h2ZMvCKdVZt2MealecW1HOdXJUiIjea5K9Pp2aYxN735HV+uqFxLRWU6nINeIyjHJKBoFNAY4INSykwBzjSzJl4n8ZnAFDOLMbPmAGYWC5wDLK5mPCISJGZG36OacteI7sz502BevaYfZ3ZN5ZPFOVz+whxOeGA6/5i0hO9+2lYjbd6+O2/Lv7zGRkfxzOV9mf77gTSIL3vezQGdknnqst4szt7ONS/PY+/+goPDR70yDeJjePmqfnRskcgNr2VUqAP9YI2g4qkgpIePAg8CZ5jZSmCI9xozSzez5wGccz8D9wDzvMfd3rZ4fAlhEbAAXy3huWrGIyIhICY6igGdknnkop5k/GUIE0b3oW/bJrwx9yfOf3oWAx/+gn9NWc6KjTsDdszyl5U5NL6iOYaO5MxuqTx2cS/mrf2Zsa9lsO/A4cNmG9WP5dVr+9GycT2ueXkei7Jyy4mz4hf1opLn9mxZ4X2qolrTUDvntgKDS9meAVzn9/pF4MUSZXYDfatzfBEJfQmx0ZzVI42zeqSxI+8AUxbnMGnhep7+IpOnPs+kS2oiI3q14tyeadVaJKem1vQ9r2dL8vYXcPt7i7j1nQXA4e37zRvG8/p1/bnwmdmMeXEub99wIp3KWGiosvcidGjegNH9j6pa8BWkO4tFpNYkJcRyYXobXru2P3P+NIR/nNuV+nHR/HPyMk755+eMmjCLV2evLXVpzvK4CjYNVcVFx7fhrvO6MW+tb1bW0qaHSGtUj9ev609sdBSjn5/DWr/F5w+J0/sZSstNKxGISFAkJ8Zz1cntef/XJzPj9tP4w9DO7MzL528fLKG/Nxz1vflZ7Mw7UP6HUXRncc3FO+akdvxxWBcA6seV3sl8VLMG/Pu6/uQXFDL6+Tmsz917WJnKrGlQ0/cPFNEKZSISdG2a1ufG047hxtOOYVnODiYtWM+khev5/bsLif+/KAYf24LzerZiUOfkMkf6+Kahrtmv2b8adDQnHd2MLmllry/dKSWRV6/pz2XPfcPlz8/hnV+eSPOGh69HWeFVzmqh5qBEICIhpUtqEl2GJfGHoZ359qdcJi3I5qPvN/Dx9zkkxscwrHsq5/VqyYkdmhETfbBRw9XgUo7+erZpXG6ZHq0b8eLVx3PFC3O44oW5vHX9CTSqHwv43WwVQk1DSgQiEpJ8w1Gb0PeoJvz1nK7MWrWVSQvX88niHN6dn0XzhvGcc1waw3uk0adtY5zz3ScQKo5v15SJV6Rz3SsZXPXyXF67tj8N42Mq1VlcOw1DSgQiEgaKhqMO6JTMvSO78/myTUxauJ435v7Ey7PWkpQQQ6GD41rXzMItVTWgUzLjLu3NjW98y/WvZPDS1cdTmaUqoXYqDkoEIhJW/Iej7sw7wMyVW/hi+Wa+WrmZLqlJwQ7vMMO6p/KvC4/jlrcXcuPr33LdqR2CHdJhlAhEJGwlJsQWJ4VQdn7v1uzaV8Bf/7uYrG2+kUQV+qZfS21DSgQiIrXgihOOYs++fB74ZBlQ8fsIanokFOg+AhGRWnPDwKO56fRjAF9tJlSoRiAiUotuPaMTF/ZtQ5um9cotW9PTTxdRIhARqUVmRttmFZ9TqTZGDalpSEQkRNXSDBNKBCIioaw2JqdTIhARiXBKBCIiIUpNQyIiUvFZSqtBiUBEJMIpEYiIhKjauo9AiUBEJIRp1JCIiNQ4JQIRkRClUUMiIlIrlAhERCKcEoGISIiqrTWLq5UIzKypmU01s5XezyZllJtsZrlm9mGJ7e3NbI6ZZZrZ22YWV514RETqmnBYmOYOYLpzriMw3XtdmoeBK0rZ/k/gMefcMcA24NpqxiMiIpVU3UQwAnjFe/4KMLK0Qs656cBO/23mS3OnA/8pb38RkUgULqOGUpxzG7znOUBKJfZtBuQ65/K911lAq7IKm9lYM8sws4zNmzdXLVoRkTBTGwvTlLtCmZlNA1JLeevP/i+cc87Maix/OecmAhMB0tPTa6sPRUSkzis3ETjnhpT1npltNLM059wGM0sDNlXi2FuBxmYW49UKWgPZldhfRKSOC4+5hiYBY7znY4APKrqjc84BnwOjqrK/iEgkCIe5hh4EzjCzlcAQ7zVmlm5mzxcVMrMZwLvAYDPLMrOh3lt/BG41s0x8fQYvVDMeERGppHKbho7EObcVGFzK9gzgOr/Xp5ax/2qgX3ViEBGpq8Jl1JCIiNSgcGgaEhGRMKdEICISosJiriEREalZWrxeRERqnBKBiEiIcrU0bEiJQEQkhGnUkIiI1DglAhGREKVRQyIiUivTUCsRiIhEOCUCEZEQpbmGRESkVoYNKRGIiEQ4JQIRkRClUUMiIqJRQyIiUvOUCEREQpTmGhIREc01JCIiNU+JQEQkwikRiIiEMI0aEhGJYJpiQkREsFCfYsLMmprZVDNb6f1sUka5yWaWa2Yfltj+spmtMbMF3qNXdeIREZHKq26N4A5gunOuIzDde12ah4ErynjvD865Xt5jQTXjERGpM1wtTTJR3UQwAnjFe/4KMLK0Qs656cDOah5LRCTihENncYpzboP3PAdIqcJn3Gdmi8zsMTOLr2Y8IiJSSTHlFTCzaUBqKW/92f+Fc86ZWWXrMXfiSyBxwETgj8DdZcQxFhgL0LZt20oeRkQk/NTWqKFyE4FzbkhZ75nZRjNLc85tMLM0YFNlDu5Xm9hnZi8Btx2h7ER8yYL09PTamp1VRCSowmGKiUnAGO/5GOCDyuzsJQ/MNz5qJLC4mvGIiEglVTcRPAicYWYrgSHea8ws3cyeLypkZjOAd4HBZpZlZkO9t143s++B74HmwL3VjEdEpM4ImaahI3HObQUGl7I9A7jO7/WpZex/enWOLyJS11ktjBvSncUiIhFOiUBEJESFyw1lIiJSk8Jg1JCIiIS5anUWi4hIzTmhQzMKa6F1SIlARCRE3TykU60cR01DIiIRTolARCTCKRGIiEQ4JQIRkQinRCAiEuGUCEREIpwSgYhIhFMiEBGJcOZqa8LrADKzzcCPVdy9ObAlgOHUJTo3ZdO5KZvOTdlC7dwc5ZxLLrkxLBNBdZhZhnMuPdhxhCKdm7Lp3JRN56Zs4XJu1DQkIhLhlAhERCJcJCaCicEOIITp3JRN56ZsOjdlC4tzE3F9BCIicqhIrBGIiIgfJQIRkQgXUYnAzIaZ2XIzyzSzO4IdT20zs7Vm9r2ZLTCzDG9bUzObamYrvZ9NvO1mZuO8c7XIzPoEN/rAMrMXzWyTmS3221bpc2FmY7zyK81sTDD+LYFWxrn5h5lle787C8xsuN97d3rnZrmZDfXbXuf+3sysjZl9bmZLzWyJmf3O2x7evzvOuYh4ANHAKqADEAcsBLoGO65aPgdrgeYltj0E3OE9vwP4p/d8OPAJvqWzTwDmBDv+AJ+LAUAfYHFVzwXQFFjt/WziPW8S7H9bDZ2bfwC3lVK2q/e3FA+09/7Gouvq3xuQBvTxnicCK7xzENa/O5FUI+gHZDrnVjvn9gNvASOCHFMoGAG84j1/BRjpt/1V5/MN0NjM0oIQX41wzn0F/Fxic2XPxVBgqnPuZ+fcNmAqMKzGg69hZZybsowA3nLO7XPOrQEy8f2t1cm/N+fcBufct97zncAPQCvC/HcnkhJBK2Cd3+ssb1skccCnZjbfzMZ621Kccxu85zlAivc8Es9XZc9FpJ2j33jNGy8WNX0QwefGzNoBvYE5hPnvTiQlAoFTnHN9gLOAG81sgP+bzldn1XhidC5KMQE4GugFbAAeCWo0QWZmDYH3gJudczv83wvH351ISgTZQBu/1629bRHDOZft/dwE/B++6vvGoiYf7+cmr3gknq/KnouIOUfOuY3OuQLnXCHwHL7fHYjAc2NmsfiSwOvOufe9zWH9uxNJiWAe0NHM2ptZHHAJMCnIMdUaM2tgZolFz4EzgcX4zkHRiIUxwAfe80nAld6ohxOA7X5V37qqsudiCnCmmTXxmkrO9LbVOSX6h87H97sDvnNziZnFm1l7oCMwlzr692ZmBrwA/OCce9TvrfD+3Ql2L3xtPvD14K/AN5rhz8GOp5b/7R3wjdxYCCwp+vcDzYDpwEpgGtDU227AeO9cfQ+kB/vfEODz8Sa+Jo4D+Npnr63KuQCuwddBmglcHex/Vw2em9e8f/sifBe3NL/yf/bOzXLgLL/tde7vDTgFX7PPImCB9xge7r87mmJCRCTCRVLTkIiIlEKJQEQkwikRiIhEOCUCEZEIp0QgIhLhlAhERCKcEoGISIT7f6qTC1eXWCebAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6856  # Value for C0\n",
    "K0 = -0.0008  # Value for K0\n",
    "K1 = -0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0128    # Value for b\n",
    "c = -2.3003    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.600000    88.409524    88.219048    88.028571    87.838095   \n",
      "351    88.409524    88.219048    88.028571    87.838095    87.647619   \n",
      "352    88.219048    88.028571    87.838095    87.647619    87.457143   \n",
      "353    88.028571    87.838095    87.647619    87.457143    87.266667   \n",
      "354    87.838095    87.647619    87.457143    87.266667    87.076190   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.647619    87.457143    87.266667    87.076190    86.896218  ...   \n",
      "351    87.457143    87.266667    87.076190    86.896218    86.845798  ...   \n",
      "352    87.266667    87.076190    86.896218    86.845798    86.795378  ...   \n",
      "353    87.076190    86.896218    86.845798    86.795378    86.744958  ...   \n",
      "354    86.896218    86.845798    86.795378    86.744958    86.694538  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   78.880345    0.000263   78.856069    0.000263   78.831793    0.000263   \n",
      "351   78.856069    0.000263   78.831793    0.000263   78.807516    0.000262   \n",
      "352   78.831793    0.000263   78.807516    0.000262   78.783240    0.000262   \n",
      "353   78.807516    0.000262   78.783240    0.000262   78.758964    0.000262   \n",
      "354   78.783240    0.000262   78.758964    0.000262   78.734687    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   78.807516    0.000262   78.783240    0.000262  \n",
      "351   78.783240    0.000262   78.758964    0.000262  \n",
      "352   78.758964    0.000262   78.734687    0.000262  \n",
      "353   78.734687    0.000262   78.710411    0.000262  \n",
      "354   78.710411    0.000262   78.686134    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1700, 1, 251) (1700, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 4s 31ms/step - loss: 5336.7778 - val_loss: 3914.2520\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5219.4844 - val_loss: 3839.5688\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5139.1748 - val_loss: 3777.8367\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5061.4580 - val_loss: 3717.2229\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4984.9756 - val_loss: 3657.5925\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 4909.5742 - val_loss: 3598.8516\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4835.1631 - val_loss: 3540.9463\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 4761.6855 - val_loss: 3483.8457\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4689.1069 - val_loss: 3427.5264\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4617.4038 - val_loss: 3371.9707\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4546.5562 - val_loss: 3317.1648\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4476.5508 - val_loss: 3263.0979\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4407.3711 - val_loss: 3209.7581\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4339.0088 - val_loss: 3157.1365\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4271.4526 - val_loss: 3105.2239\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4204.6929 - val_loss: 3054.0107\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4138.7212 - val_loss: 3003.4829\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4073.5286 - val_loss: 2953.5029\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3996.5930 - val_loss: 2888.7517\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3922.2581 - val_loss: 2835.0708\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3853.4177 - val_loss: 2783.2600\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3786.5613 - val_loss: 2732.8545\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3721.2151 - val_loss: 2683.5881\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3657.1243 - val_loss: 2635.3174\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3594.1426 - val_loss: 2587.9543\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3532.1804 - val_loss: 2541.4409\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3471.1746 - val_loss: 2495.7349\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3411.0801 - val_loss: 2450.8044\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3351.8628 - val_loss: 2406.6238\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3293.4922 - val_loss: 2363.1721\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 3235.9470 - val_loss: 2320.4319\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3179.2063 - val_loss: 2278.3875\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 3123.2537 - val_loss: 2237.0249\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3068.0745 - val_loss: 2196.3325\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3013.6531 - val_loss: 2156.2993\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2959.9780 - val_loss: 2116.9138\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2907.0383 - val_loss: 2078.1670\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2854.8232 - val_loss: 2040.0509\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2803.3230 - val_loss: 2002.5553\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2752.5269 - val_loss: 1965.6736\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2702.4275 - val_loss: 1929.3964\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2653.0156 - val_loss: 1893.7170\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2604.2832 - val_loss: 1858.6294\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2556.2222 - val_loss: 1824.1243\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2508.8254 - val_loss: 1790.1963\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2462.0845 - val_loss: 1756.8387\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2415.9929 - val_loss: 1724.0447\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2370.5442 - val_loss: 1691.8082\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2325.7312 - val_loss: 1660.1232\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2281.5474 - val_loss: 1628.9834\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2237.9856 - val_loss: 1598.3834\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2195.0405 - val_loss: 1568.3160\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2152.7043 - val_loss: 1538.7772\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2110.9722 - val_loss: 1509.7598\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 2069.8376 - val_loss: 1481.2598\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2029.2954 - val_loss: 1453.2706\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1989.3379 - val_loss: 1425.7865\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1949.9615 - val_loss: 1398.8031\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1911.1598 - val_loss: 1372.3147\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1872.9263 - val_loss: 1346.3158\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1835.2559 - val_loss: 1320.8015\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1798.1434 - val_loss: 1295.7666\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1761.5829 - val_loss: 1271.2059\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1725.5701 - val_loss: 1247.1146\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1690.0977 - val_loss: 1223.4877\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1655.1625 - val_loss: 1200.3191\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1620.7581 - val_loss: 1177.6058\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1586.8796 - val_loss: 1155.3417\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1553.5219 - val_loss: 1133.5219\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1520.6794 - val_loss: 1112.1420\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1488.3480 - val_loss: 1091.1976\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1456.5227 - val_loss: 1070.6829\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 1425.1974 - val_loss: 1050.5938\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1394.3679 - val_loss: 1030.9255\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1364.0302 - val_loss: 1011.6741\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1334.1780 - val_loss: 992.8336\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 1304.8066 - val_loss: 974.4006\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1275.9120 - val_loss: 956.3692\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1247.4891 - val_loss: 938.7362\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1219.5336 - val_loss: 921.4966\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1192.0400 - val_loss: 904.6453\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1165.0039 - val_loss: 888.1788\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1138.4213 - val_loss: 872.0920\n",
      "Epoch 84/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1112.2866 - val_loss: 856.3807\n",
      "Epoch 85/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1086.5956 - val_loss: 841.0401\n",
      "Epoch 86/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1061.3436 - val_loss: 826.0662\n",
      "Epoch 87/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1036.5269 - val_loss: 811.4540\n",
      "Epoch 88/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1012.1398 - val_loss: 797.2003\n",
      "Epoch 89/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 988.1790 - val_loss: 783.2998\n",
      "Epoch 90/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 964.6393 - val_loss: 769.7484\n",
      "Epoch 91/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 941.5160 - val_loss: 756.5416\n",
      "Epoch 92/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 918.8052 - val_loss: 743.6754\n",
      "Epoch 93/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 896.5021 - val_loss: 731.1453\n",
      "Epoch 94/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 874.6028 - val_loss: 718.9471\n",
      "Epoch 95/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 853.1023 - val_loss: 707.0765\n",
      "Epoch 96/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 831.9967 - val_loss: 695.5293\n",
      "Epoch 97/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 811.2818 - val_loss: 684.3010\n",
      "Epoch 98/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 790.9526 - val_loss: 673.3881\n",
      "Epoch 99/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 771.0059 - val_loss: 662.7858\n",
      "Epoch 100/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 751.4365 - val_loss: 652.4897\n",
      "Epoch 101/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 732.2401 - val_loss: 642.4958\n",
      "Epoch 102/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 713.4128 - val_loss: 632.8000\n",
      "Epoch 103/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 694.9504 - val_loss: 623.3984\n",
      "Epoch 104/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 676.8489 - val_loss: 614.2867\n",
      "Epoch 105/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 659.1033 - val_loss: 605.4603\n",
      "Epoch 106/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 641.7099 - val_loss: 596.9156\n",
      "Epoch 107/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 624.6647 - val_loss: 588.6481\n",
      "Epoch 108/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 607.9633 - val_loss: 580.6537\n",
      "Epoch 109/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 591.6014 - val_loss: 572.9288\n",
      "Epoch 110/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 575.5752 - val_loss: 565.4691\n",
      "Epoch 111/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 559.8807 - val_loss: 558.2700\n",
      "Epoch 112/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 544.5131 - val_loss: 551.3281\n",
      "Epoch 113/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 529.4688 - val_loss: 544.6387\n",
      "Epoch 114/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 514.7434 - val_loss: 538.1983\n",
      "Epoch 115/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 500.3333 - val_loss: 532.0024\n",
      "Epoch 116/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 486.2340 - val_loss: 526.0471\n",
      "Epoch 117/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 472.4417 - val_loss: 520.3286\n",
      "Epoch 118/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 458.9523 - val_loss: 514.8427\n",
      "Epoch 119/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 445.7619 - val_loss: 509.5852\n",
      "Epoch 120/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 432.8659 - val_loss: 504.5526\n",
      "Epoch 121/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 420.2609 - val_loss: 499.7404\n",
      "Epoch 122/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 407.9427 - val_loss: 495.1448\n",
      "Epoch 123/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 395.9073 - val_loss: 490.7618\n",
      "Epoch 124/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 384.1508 - val_loss: 486.5876\n",
      "Epoch 125/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 372.6693 - val_loss: 482.6182\n",
      "Epoch 126/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 361.4588 - val_loss: 478.8495\n",
      "Epoch 127/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 350.5154 - val_loss: 475.2777\n",
      "Epoch 128/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 339.8350 - val_loss: 471.8987\n",
      "Epoch 129/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 329.4139 - val_loss: 468.7089\n",
      "Epoch 130/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 319.2483 - val_loss: 465.7040\n",
      "Epoch 131/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 309.3335 - val_loss: 462.8804\n",
      "Epoch 132/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 299.6664 - val_loss: 460.2342\n",
      "Epoch 133/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 290.2433 - val_loss: 457.7616\n",
      "Epoch 134/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 281.0599 - val_loss: 455.4586\n",
      "Epoch 135/500\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 272.1126 - val_loss: 453.3215\n",
      "Epoch 136/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 263.3979 - val_loss: 451.3463\n",
      "Epoch 137/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 254.9112 - val_loss: 449.5293\n",
      "Epoch 138/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 246.6492 - val_loss: 447.8668\n",
      "Epoch 139/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 238.6081 - val_loss: 446.3550\n",
      "Epoch 140/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 230.7839 - val_loss: 444.9899\n",
      "Epoch 141/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 223.1731 - val_loss: 443.7681\n",
      "Epoch 142/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 215.7719 - val_loss: 442.6857\n",
      "Epoch 143/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 208.5766 - val_loss: 441.7390\n",
      "Epoch 144/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 201.5836 - val_loss: 440.9243\n",
      "Epoch 145/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 194.7890 - val_loss: 440.2379\n",
      "Epoch 146/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 188.1894 - val_loss: 439.6764\n",
      "Epoch 147/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 181.7809 - val_loss: 439.2358\n",
      "Epoch 148/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 175.5600 - val_loss: 438.9130\n",
      "Epoch 149/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 169.5232 - val_loss: 438.7039\n",
      "Epoch 150/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 163.6666 - val_loss: 438.6052\n",
      "Epoch 151/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 157.9870 - val_loss: 438.6134\n",
      "Epoch 152/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 152.4807 - val_loss: 438.7249\n",
      "Epoch 153/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 147.1440 - val_loss: 438.9363\n",
      "Epoch 154/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 141.9736 - val_loss: 439.2441\n",
      "Epoch 155/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 136.9662 - val_loss: 439.6448\n",
      "Epoch 156/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 132.1182 - val_loss: 440.1353\n",
      "Epoch 157/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 127.4260 - val_loss: 440.7118\n",
      "Epoch 158/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 122.8862 - val_loss: 441.3713\n",
      "Epoch 159/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 118.4955 - val_loss: 442.1105\n",
      "Epoch 160/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 114.2507 - val_loss: 442.9260\n",
      "Epoch 161/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 110.1485 - val_loss: 443.8144\n",
      "Epoch 162/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 106.1854 - val_loss: 444.7730\n",
      "Epoch 163/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 102.3583 - val_loss: 445.7981\n",
      "Epoch 164/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 98.6639 - val_loss: 446.8870\n",
      "Epoch 165/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 95.0988 - val_loss: 448.0362\n",
      "Epoch 166/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 91.6602 - val_loss: 449.2429\n",
      "Epoch 167/500\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 88.3447 - val_loss: 450.5042\n",
      "Epoch 168/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 85.1492 - val_loss: 451.8170\n",
      "Epoch 169/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 82.0707 - val_loss: 453.1783\n",
      "Epoch 170/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 79.1061 - val_loss: 454.5855\n",
      "Epoch 171/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 76.2523 - val_loss: 456.0354\n",
      "Epoch 172/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 73.5065 - val_loss: 457.5254\n",
      "Epoch 173/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 70.8656 - val_loss: 459.0527\n",
      "Epoch 174/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 68.3269 - val_loss: 460.6148\n",
      "Epoch 175/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 65.8873 - val_loss: 462.2088\n",
      "Epoch 176/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 63.5441 - val_loss: 463.8322\n",
      "Epoch 177/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 61.2946 - val_loss: 465.4825\n",
      "Epoch 178/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 59.1360 - val_loss: 467.1570\n",
      "Epoch 179/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 57.0655 - val_loss: 468.8534\n",
      "Epoch 180/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 55.0804 - val_loss: 470.5693\n",
      "Epoch 181/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 53.1783 - val_loss: 472.3022\n",
      "Epoch 182/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 51.3566 - val_loss: 474.0501\n",
      "Epoch 183/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 49.6125 - val_loss: 475.8109\n",
      "Epoch 184/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 47.9436 - val_loss: 477.5818\n",
      "Epoch 185/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 46.3477 - val_loss: 479.3612\n",
      "Epoch 186/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 44.8220 - val_loss: 481.1467\n",
      "Epoch 187/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 43.3643 - val_loss: 482.9367\n",
      "Epoch 188/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 41.9724 - val_loss: 484.7289\n",
      "Epoch 189/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 40.6438 - val_loss: 486.5218\n",
      "Epoch 190/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 39.3764 - val_loss: 488.3130\n",
      "Epoch 191/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 38.1681 - val_loss: 490.1012\n",
      "Epoch 192/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 37.0165 - val_loss: 491.8845\n",
      "Epoch 193/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 35.9197 - val_loss: 493.6616\n",
      "Epoch 194/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 34.8757 - val_loss: 495.4304\n",
      "Epoch 195/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 33.8824 - val_loss: 497.1896\n",
      "Epoch 196/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 32.9379 - val_loss: 498.9376\n",
      "Epoch 197/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 32.0404 - val_loss: 500.6731\n",
      "Epoch 198/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 31.1879 - val_loss: 502.3949\n",
      "Epoch 199/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 30.3786 - val_loss: 504.1014\n",
      "Epoch 200/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 29.6107 - val_loss: 505.7917\n",
      "Epoch 201/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 28.8826 - val_loss: 507.4647\n",
      "Epoch 202/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 28.1926 - val_loss: 509.1190\n",
      "Epoch 203/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 27.5390 - val_loss: 510.7537\n",
      "Epoch 204/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 26.9204 - val_loss: 512.3679\n",
      "Epoch 205/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 26.3350 - val_loss: 513.9605\n",
      "Epoch 206/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 25.7816 - val_loss: 515.5308\n",
      "Epoch 207/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 25.2586 - val_loss: 517.0778\n",
      "Epoch 208/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 24.7647 - val_loss: 518.6011\n",
      "Epoch 209/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.2984 - val_loss: 520.0994\n",
      "Epoch 210/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.8587 - val_loss: 521.5726\n",
      "Epoch 211/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 23.4441 - val_loss: 523.0201\n",
      "Epoch 212/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 23.0534 - val_loss: 524.4413\n",
      "Epoch 213/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 22.6855 - val_loss: 525.8356\n",
      "Epoch 214/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 22.3393 - val_loss: 527.2023\n",
      "Epoch 215/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 22.0136 - val_loss: 528.5416\n",
      "Epoch 216/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 21.7075 - val_loss: 529.8533\n",
      "Epoch 217/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 21.4199 - val_loss: 531.1368\n",
      "Epoch 218/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 21.1498 - val_loss: 532.3917\n",
      "Epoch 219/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 20.8963 - val_loss: 533.6179\n",
      "Epoch 220/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 20.6586 - val_loss: 534.8152\n",
      "Epoch 221/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 20.4358 - val_loss: 535.9836\n",
      "Epoch 222/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 20.2271 - val_loss: 537.1234\n",
      "Epoch 223/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 20.0317 - val_loss: 538.2343\n",
      "Epoch 224/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 19.8488 - val_loss: 539.3168\n",
      "Epoch 225/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 19.6776 - val_loss: 540.3701\n",
      "Epoch 226/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 19.5177 - val_loss: 541.3948\n",
      "Epoch 227/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 19.3683 - val_loss: 542.3909\n",
      "Epoch 228/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 19.2287 - val_loss: 543.3589\n",
      "Epoch 229/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 19.0985 - val_loss: 544.2986\n",
      "Epoch 230/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 18.9770 - val_loss: 545.2103\n",
      "Epoch 231/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 18.8637 - val_loss: 546.0945\n",
      "Epoch 232/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 18.7581 - val_loss: 546.9514\n",
      "Epoch 233/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 18.6598 - val_loss: 547.7811\n",
      "Epoch 234/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 18.5682 - val_loss: 548.5842\n",
      "Epoch 235/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 18.4830 - val_loss: 549.3608\n",
      "Epoch 236/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.4037 - val_loss: 550.1115\n",
      "Epoch 237/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 18.3300 - val_loss: 550.8368\n",
      "Epoch 238/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 18.2614 - val_loss: 551.5371\n",
      "Epoch 239/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 18.1977 - val_loss: 552.2122\n",
      "Epoch 240/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.1386 - val_loss: 552.8637\n",
      "Epoch 241/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 18.0837 - val_loss: 553.4911\n",
      "Epoch 242/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.0327 - val_loss: 554.0955\n",
      "Epoch 243/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.9854 - val_loss: 554.6775\n",
      "Epoch 244/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 17.9416 - val_loss: 555.2366\n",
      "Epoch 245/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.9009 - val_loss: 555.7739\n",
      "Epoch 246/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.8631 - val_loss: 556.2905\n",
      "Epoch 247/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.8282 - val_loss: 556.7862\n",
      "Epoch 248/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.7958 - val_loss: 557.2616\n",
      "Epoch 249/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 17.7658 - val_loss: 557.7173\n",
      "Epoch 250/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.7380 - val_loss: 558.1539\n",
      "Epoch 251/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 17.7123 - val_loss: 558.5721\n",
      "Epoch 252/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.6885 - val_loss: 558.9719\n",
      "Epoch 253/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 17.6665 - val_loss: 559.3544\n",
      "Epoch 254/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 17.6461 - val_loss: 559.7202\n",
      "Epoch 255/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 17.6272 - val_loss: 560.0691\n",
      "Epoch 256/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 17.6098 - val_loss: 560.4022\n",
      "Epoch 257/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 17.5937 - val_loss: 560.7202\n",
      "Epoch 258/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 17.5787 - val_loss: 561.0229\n",
      "Epoch 259/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 17.5650 - val_loss: 561.3115\n",
      "Epoch 260/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 17.5522 - val_loss: 561.5861\n",
      "Epoch 261/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 17.5404 - val_loss: 561.8470\n",
      "Epoch 262/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.5295 - val_loss: 562.0952\n",
      "Epoch 263/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.5195 - val_loss: 562.3308\n",
      "Epoch 264/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.5102 - val_loss: 562.5544\n",
      "Epoch 265/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.5017 - val_loss: 562.7668\n",
      "Epoch 266/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4938 - val_loss: 562.9681\n",
      "Epoch 267/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 17.4864 - val_loss: 563.1590\n",
      "Epoch 268/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 17.4797 - val_loss: 563.3394\n",
      "Epoch 269/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4736 - val_loss: 563.5102\n",
      "Epoch 270/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4679 - val_loss: 563.6719\n",
      "Epoch 271/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4626 - val_loss: 563.8245\n",
      "Epoch 272/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4577 - val_loss: 563.9685\n",
      "Epoch 273/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4533 - val_loss: 564.1041\n",
      "Epoch 274/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 17.4492 - val_loss: 564.2323\n",
      "Epoch 275/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 17.4455 - val_loss: 564.3528\n",
      "Epoch 276/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 17.4421 - val_loss: 564.4662\n",
      "Epoch 277/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 17.4389 - val_loss: 564.5732\n",
      "Epoch 278/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 17.4360 - val_loss: 564.6735\n",
      "Epoch 279/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 17.4334 - val_loss: 564.7678\n",
      "Epoch 280/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 17.4310 - val_loss: 564.8561\n",
      "Epoch 281/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4289 - val_loss: 564.9389\n",
      "Epoch 282/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 17.4270 - val_loss: 565.0167\n",
      "Epoch 283/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 17.4251 - val_loss: 565.0897\n",
      "Epoch 284/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4235 - val_loss: 565.1575\n",
      "Epoch 285/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4221 - val_loss: 565.2211\n",
      "Epoch 286/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4208 - val_loss: 565.2805\n",
      "Epoch 287/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4197 - val_loss: 565.3356\n",
      "Epoch 288/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4187 - val_loss: 565.3868\n",
      "Epoch 289/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4178 - val_loss: 565.4353\n",
      "Epoch 290/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 17.4170 - val_loss: 565.4798\n",
      "Epoch 291/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 17.4164 - val_loss: 565.5214\n",
      "Epoch 292/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 17.4158 - val_loss: 565.5599\n",
      "Epoch 293/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4153 - val_loss: 565.5959\n",
      "Epoch 294/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4149 - val_loss: 565.6290\n",
      "Epoch 295/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4147 - val_loss: 565.6599\n",
      "Epoch 296/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4144 - val_loss: 565.6882\n",
      "Epoch 297/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4142 - val_loss: 565.7144\n",
      "Epoch 298/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4141 - val_loss: 565.7385\n",
      "Epoch 299/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4141 - val_loss: 565.7609\n",
      "Epoch 300/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4141 - val_loss: 565.7814\n",
      "Epoch 301/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 17.4142 - val_loss: 565.8002\n",
      "Epoch 302/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 17.4143 - val_loss: 565.8171\n",
      "Epoch 303/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 17.4144 - val_loss: 565.8329\n",
      "Epoch 304/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4147 - val_loss: 565.8478\n",
      "Epoch 305/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 17.4149 - val_loss: 565.8608\n",
      "Epoch 306/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4152 - val_loss: 565.8728\n",
      "Epoch 307/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4154 - val_loss: 565.8835\n",
      "Epoch 308/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4158 - val_loss: 565.8934\n",
      "Epoch 309/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 17.4162 - val_loss: 565.9026\n",
      "Epoch 310/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4166 - val_loss: 565.9108\n",
      "Epoch 311/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4169 - val_loss: 565.9177\n",
      "Epoch 312/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4174 - val_loss: 565.9244\n",
      "Epoch 313/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4178 - val_loss: 565.9301\n",
      "Epoch 314/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4183 - val_loss: 565.9351\n",
      "Epoch 315/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4188 - val_loss: 565.9399\n",
      "Epoch 316/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4193 - val_loss: 565.9439\n",
      "Epoch 317/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4198 - val_loss: 565.9475\n",
      "Epoch 318/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 17.4203 - val_loss: 565.9503\n",
      "Epoch 319/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4209 - val_loss: 565.9525\n",
      "Epoch 320/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4215 - val_loss: 565.9551\n",
      "Epoch 321/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4220 - val_loss: 565.9565\n",
      "Epoch 322/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4226 - val_loss: 565.9579\n",
      "Epoch 323/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4231 - val_loss: 565.9593\n",
      "Epoch 324/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4237 - val_loss: 565.9600\n",
      "Epoch 325/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4243 - val_loss: 565.9604\n",
      "Epoch 326/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4249 - val_loss: 565.9609\n",
      "Epoch 327/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4255 - val_loss: 565.9612\n",
      "Epoch 328/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4261 - val_loss: 565.9612\n",
      "Epoch 329/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4267 - val_loss: 565.9612\n",
      "Epoch 330/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4273 - val_loss: 565.9611\n",
      "Epoch 331/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 17.4278 - val_loss: 565.9603\n",
      "Epoch 332/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4285 - val_loss: 565.9597\n",
      "Epoch 333/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4290 - val_loss: 565.9588\n",
      "Epoch 334/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4296 - val_loss: 565.9578\n",
      "Epoch 335/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4302 - val_loss: 565.9569\n",
      "Epoch 336/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4308 - val_loss: 565.9557\n",
      "Epoch 337/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4314 - val_loss: 565.9545\n",
      "Epoch 338/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4320 - val_loss: 565.9534\n",
      "Epoch 339/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4326 - val_loss: 565.9525\n",
      "Epoch 340/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4332 - val_loss: 565.9509\n",
      "Epoch 341/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4338 - val_loss: 565.9497\n",
      "Epoch 342/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4343 - val_loss: 565.9485\n",
      "Epoch 343/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4348 - val_loss: 565.9467\n",
      "Epoch 344/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4354 - val_loss: 565.9452\n",
      "Epoch 345/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4360 - val_loss: 565.9435\n",
      "Epoch 346/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 17.4366 - val_loss: 565.9417\n",
      "Epoch 347/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4371 - val_loss: 565.9401\n",
      "Epoch 348/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4377 - val_loss: 565.9382\n",
      "Epoch 349/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4383 - val_loss: 565.9370\n",
      "Epoch 350/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4388 - val_loss: 565.9351\n",
      "Epoch 351/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4394 - val_loss: 565.9333\n",
      "Epoch 352/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4399 - val_loss: 565.9315\n",
      "Epoch 353/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4404 - val_loss: 565.9301\n",
      "Epoch 354/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4409 - val_loss: 565.9283\n",
      "Epoch 355/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4415 - val_loss: 565.9268\n",
      "Epoch 356/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4420 - val_loss: 565.9252\n",
      "Epoch 357/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4425 - val_loss: 565.9232\n",
      "Epoch 358/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4430 - val_loss: 565.9218\n",
      "Epoch 359/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4435 - val_loss: 565.9202\n",
      "Epoch 360/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 17.4440 - val_loss: 565.9188\n",
      "Epoch 361/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4445 - val_loss: 565.9173\n",
      "Epoch 362/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4449 - val_loss: 565.9154\n",
      "Epoch 363/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4454 - val_loss: 565.9139\n",
      "Epoch 364/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4459 - val_loss: 565.9121\n",
      "Epoch 365/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4464 - val_loss: 565.9106\n",
      "Epoch 366/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4468 - val_loss: 565.9091\n",
      "Epoch 367/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4473 - val_loss: 565.9075\n",
      "Epoch 368/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4477 - val_loss: 565.9062\n",
      "Epoch 369/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4482 - val_loss: 565.9048\n",
      "Epoch 370/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4486 - val_loss: 565.9034\n",
      "Epoch 371/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 17.4490 - val_loss: 565.9019\n",
      "Epoch 372/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4495 - val_loss: 565.9005\n",
      "Epoch 373/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4499 - val_loss: 565.8991\n",
      "Epoch 374/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 17.4503 - val_loss: 565.8975\n",
      "Epoch 375/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.4507 - val_loss: 565.8963\n",
      "Epoch 376/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4511 - val_loss: 565.8947\n",
      "Epoch 377/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4515 - val_loss: 565.8929\n",
      "Epoch 378/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4519 - val_loss: 565.8919\n",
      "Epoch 379/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4523 - val_loss: 565.8902\n",
      "Epoch 380/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.4527 - val_loss: 565.8889\n",
      "Epoch 381/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4530 - val_loss: 565.8872\n",
      "Epoch 382/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4534 - val_loss: 565.8860\n",
      "Epoch 383/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4538 - val_loss: 565.8848\n",
      "Epoch 384/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4542 - val_loss: 565.8837\n",
      "Epoch 385/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4545 - val_loss: 565.8823\n",
      "Epoch 386/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4549 - val_loss: 565.8813\n",
      "Epoch 387/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4552 - val_loss: 565.8802\n",
      "Epoch 388/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4555 - val_loss: 565.8790\n",
      "Epoch 389/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4559 - val_loss: 565.8781\n",
      "Epoch 390/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4563 - val_loss: 565.8771\n",
      "Epoch 391/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4566 - val_loss: 565.8761\n",
      "Epoch 392/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4569 - val_loss: 565.8750\n",
      "Epoch 393/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4571 - val_loss: 565.8735\n",
      "Epoch 394/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4575 - val_loss: 565.8727\n",
      "Epoch 395/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4578 - val_loss: 565.8716\n",
      "Epoch 396/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.4581 - val_loss: 565.8703\n",
      "Epoch 397/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.4584 - val_loss: 565.8694\n",
      "Epoch 398/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4586 - val_loss: 565.8683\n",
      "Epoch 399/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4590 - val_loss: 565.8671\n",
      "Epoch 400/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.4592 - val_loss: 565.8665\n",
      "Epoch 401/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.4595 - val_loss: 565.8660\n",
      "Epoch 402/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4598 - val_loss: 565.8646\n",
      "Epoch 403/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4600 - val_loss: 565.8636\n",
      "Epoch 404/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4603 - val_loss: 565.8629\n",
      "Epoch 405/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.4606 - val_loss: 565.8619\n",
      "Epoch 406/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4608 - val_loss: 565.8613\n",
      "Epoch 407/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4611 - val_loss: 565.8603\n",
      "Epoch 408/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4613 - val_loss: 565.8596\n",
      "Epoch 409/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4615 - val_loss: 565.8589\n",
      "Epoch 410/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4617 - val_loss: 565.8581\n",
      "Epoch 411/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.4620 - val_loss: 565.8572\n",
      "Epoch 412/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.4622 - val_loss: 565.8565\n",
      "Epoch 413/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4625 - val_loss: 565.8557\n",
      "Epoch 414/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4627 - val_loss: 565.8550\n",
      "Epoch 415/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4628 - val_loss: 565.8541\n",
      "Epoch 416/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 17.4631 - val_loss: 565.8531\n",
      "Epoch 417/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.4634 - val_loss: 565.8529\n",
      "Epoch 418/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.4635 - val_loss: 565.8522\n",
      "Epoch 419/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.4637 - val_loss: 565.8514\n",
      "Epoch 420/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.4639 - val_loss: 565.8510\n",
      "Epoch 421/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.4641 - val_loss: 565.8505\n",
      "Epoch 422/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4642 - val_loss: 565.8496\n",
      "Epoch 423/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 17.4645 - val_loss: 565.8488\n",
      "Epoch 424/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4646 - val_loss: 565.8479\n",
      "Epoch 425/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4648 - val_loss: 565.8470\n",
      "Epoch 426/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.4650 - val_loss: 565.8464\n",
      "Epoch 427/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4651 - val_loss: 565.8457\n",
      "Epoch 428/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 17.4653 - val_loss: 565.8451\n",
      "Epoch 429/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4655 - val_loss: 565.8446\n",
      "Epoch 430/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 17.4657 - val_loss: 565.8442\n",
      "Epoch 431/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4658 - val_loss: 565.8433\n",
      "Epoch 432/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4659 - val_loss: 565.8425\n",
      "Epoch 433/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4661 - val_loss: 565.8418\n",
      "Epoch 434/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4663 - val_loss: 565.8414\n",
      "Epoch 435/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4664 - val_loss: 565.8410\n",
      "Epoch 436/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4666 - val_loss: 565.8405\n",
      "Epoch 437/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4667 - val_loss: 565.8399\n",
      "Epoch 438/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4669 - val_loss: 565.8398\n",
      "Epoch 439/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4670 - val_loss: 565.8391\n",
      "Epoch 440/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4671 - val_loss: 565.8385\n",
      "Epoch 441/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 17.4672 - val_loss: 565.8379\n",
      "Epoch 442/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4674 - val_loss: 565.8372\n",
      "Epoch 443/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4676 - val_loss: 565.8370\n",
      "Epoch 444/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.4676 - val_loss: 565.8366\n",
      "Epoch 445/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4678 - val_loss: 565.8361\n",
      "Epoch 446/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 17.4679 - val_loss: 565.8354\n",
      "Epoch 447/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4680 - val_loss: 565.8350\n",
      "Epoch 448/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4681 - val_loss: 565.8348\n",
      "Epoch 449/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4682 - val_loss: 565.8340\n",
      "Epoch 450/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4683 - val_loss: 565.8336\n",
      "Epoch 451/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4684 - val_loss: 565.8330\n",
      "Epoch 452/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 17.4686 - val_loss: 565.8326\n",
      "Epoch 453/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4687 - val_loss: 565.8325\n",
      "Epoch 454/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.4688 - val_loss: 565.8322\n",
      "Epoch 455/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4689 - val_loss: 565.8318\n",
      "Epoch 456/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4690 - val_loss: 565.8316\n",
      "Epoch 457/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4691 - val_loss: 565.8313\n",
      "Epoch 458/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4691 - val_loss: 565.8310\n",
      "Epoch 459/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 17.4692 - val_loss: 565.8308\n",
      "Epoch 460/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4693 - val_loss: 565.8305\n",
      "Epoch 461/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4694 - val_loss: 565.8301\n",
      "Epoch 462/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4695 - val_loss: 565.8298\n",
      "Epoch 463/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4696 - val_loss: 565.8297\n",
      "Epoch 464/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4696 - val_loss: 565.8292\n",
      "Epoch 465/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 17.4698 - val_loss: 565.8290\n",
      "Epoch 466/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4698 - val_loss: 565.8288\n",
      "Epoch 467/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4699 - val_loss: 565.8281\n",
      "Epoch 468/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4699 - val_loss: 565.8279\n",
      "Epoch 469/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4700 - val_loss: 565.8275\n",
      "Epoch 470/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4701 - val_loss: 565.8273\n",
      "Epoch 471/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4702 - val_loss: 565.8272\n",
      "Epoch 472/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4702 - val_loss: 565.8264\n",
      "Epoch 473/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4703 - val_loss: 565.8264\n",
      "Epoch 474/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4704 - val_loss: 565.8260\n",
      "Epoch 475/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4704 - val_loss: 565.8256\n",
      "Epoch 476/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 17.4705 - val_loss: 565.8254\n",
      "Epoch 477/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4705 - val_loss: 565.8251\n",
      "Epoch 478/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4706 - val_loss: 565.8250\n",
      "Epoch 479/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4706 - val_loss: 565.8245\n",
      "Epoch 480/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4707 - val_loss: 565.8245\n",
      "Epoch 481/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4708 - val_loss: 565.8240\n",
      "Epoch 482/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4709 - val_loss: 565.8239\n",
      "Epoch 483/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4709 - val_loss: 565.8240\n",
      "Epoch 484/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4710 - val_loss: 565.8239\n",
      "Epoch 485/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4710 - val_loss: 565.8237\n",
      "Epoch 486/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4711 - val_loss: 565.8234\n",
      "Epoch 487/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4711 - val_loss: 565.8232\n",
      "Epoch 488/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4712 - val_loss: 565.8232\n",
      "Epoch 489/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 17.4712 - val_loss: 565.8232\n",
      "Epoch 490/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.4713 - val_loss: 565.8230\n",
      "Epoch 491/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.4713 - val_loss: 565.8224\n",
      "Epoch 492/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4714 - val_loss: 565.8221\n",
      "Epoch 493/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.4714 - val_loss: 565.8217\n",
      "Epoch 494/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.4715 - val_loss: 565.8214\n",
      "Epoch 495/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4716 - val_loss: 565.8216\n",
      "Epoch 496/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4716 - val_loss: 565.8214\n",
      "Epoch 497/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.4716 - val_loss: 565.8213\n",
      "Epoch 498/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4716 - val_loss: 565.8209\n",
      "Epoch 499/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 17.4717 - val_loss: 565.8207\n",
      "Epoch 500/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 17.4717 - val_loss: 565.8206\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6856, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0008, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0128, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(-2.3003, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 350ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.55468487e+01, 6.55216387e+01, 6.54964286e+01, 6.54712185e+01,\n",
       "        6.54460084e+01, 6.54207983e+01, 6.53955882e+01, 6.53703781e+01,\n",
       "        6.53451681e+01, 6.53199580e+01, 6.52947479e+01, 6.52695378e+01,\n",
       "        6.52443277e+01, 7.00027311e+01, 6.99775210e+01, 6.99523109e+01,\n",
       "        6.99271008e+01, 6.99018908e+01, 6.98766807e+01, 6.98514706e+01,\n",
       "        6.98262605e+01, 6.98010504e+01, 6.96389356e+01, 6.94708684e+01,\n",
       "        6.93028011e+01, 6.91347339e+01, 6.89666667e+01, 6.87985994e+01,\n",
       "        6.86305322e+01, 6.84624650e+01, 6.82943978e+01, 6.81263305e+01,\n",
       "        6.79582633e+01, 6.77947549e+01, 6.77048389e+01, 6.76149230e+01,\n",
       "        6.75250070e+01, 6.74350910e+01, 6.73451751e+01, 6.72552591e+01,\n",
       "        3.90637370e-01, 0.00000000e+00, 7.33847740e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.15247900e-01, 6.91720822e+01,\n",
       "        6.90040149e+01, 6.88359477e+01, 6.86678805e+01, 6.84998133e+01,\n",
       "        6.83317460e+01, 6.81636788e+01, 6.79956116e+01, 6.78275443e+01,\n",
       "        6.77248203e+01, 6.76349043e+01, 6.75449883e+01, 6.74550724e+01,\n",
       "        6.73651564e+01, 6.72752404e+01, 6.71853245e+01, 6.70954085e+01,\n",
       "        6.70054925e+01, 6.69155766e+01, 6.68256606e+01, 6.67357446e+01,\n",
       "        6.66457888e+01, 6.65558708e+01, 6.64659529e+01, 6.63760350e+01,\n",
       "        6.62861172e+01, 6.61961993e+01, 3.92893780e-02, 6.50058270e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.02470980e-02,\n",
       "        6.01505203e+01, 0.00000000e+00, 5.10494232e-01, 5.22483587e-01,\n",
       "        0.00000000e+00, 8.15376401e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 5.95658243e-01, 1.02467008e-01, 0.00000000e+00,\n",
       "        2.44835913e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.82940736e-02, 0.00000000e+00, 7.74046257e-02]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([59.29186309, 59.27956548, 59.26726788, 59.25497028, 59.24267268,\n",
       "       59.23037508, 59.21807747, 59.20577987, 59.19348227, 59.18118467,\n",
       "       59.16888707, 59.15658947, 59.14429186, 59.13199426, 59.11969666,\n",
       "       59.10739906, 59.09510146, 59.08280385, 59.07050625, 59.05820865,\n",
       "       59.04591105, 59.03361345, 59.02131584, 59.00901824, 58.99672064,\n",
       "       58.98442304, 58.97212544, 58.95982783, 58.94753023, 58.93523263,\n",
       "       58.92293503, 58.91063743, 58.89833982, 58.88604222, 58.87374462,\n",
       "       58.86144702, 58.84914942, 58.83685181, 58.82455421, 58.81225661,\n",
       "       58.79995901, 58.78766141, 58.7753638 , 58.7630662 , 58.7507686 ,\n",
       "       58.738471  , 58.7261734 , 58.71387579, 58.70157819, 58.68928059,\n",
       "       58.67698299, 58.66468539, 58.65238778, 58.64009018, 58.62779258,\n",
       "       58.61549498, 58.60319738, 58.59089977, 58.57860217, 58.56630457,\n",
       "       58.55400697, 58.54170937, 58.52941176, 58.51711416, 58.50481656,\n",
       "       58.49251896, 58.48022136, 58.46792375, 58.45562615, 58.44332855,\n",
       "       58.43103095, 58.41873335, 58.40643575, 58.39413814, 58.38184054,\n",
       "       58.36954294, 58.35724534, 58.34494774, 58.33265013, 58.32035253,\n",
       "       58.30805493, 58.29575733, 58.28345973, 58.27116212, 58.25886452,\n",
       "       58.24656692, 58.23426932, 58.22197172, 58.20967411, 58.19737651,\n",
       "       58.18507891, 58.17278131, 58.16048371, 58.1481861 , 58.1358885 ,\n",
       "       58.1235909 , 58.1112933 , 58.0989957 , 58.08669809, 58.07440049])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.713603040757704\n",
      "24.51317062835143\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
