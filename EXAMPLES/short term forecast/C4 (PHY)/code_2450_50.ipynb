{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "2495    55.049190\n",
       "2496    55.036893\n",
       "2497    55.024595\n",
       "2498    55.012298\n",
       "2499    55.000000\n",
       "Name: C4, Length: 2500, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c4_interpolated_2450_50.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "2445     0.000000\n",
       "2446     0.530742\n",
       "2447     0.000000\n",
       "2448     0.765768\n",
       "2449     0.000000\n",
       "Name: C4, Length: 2450, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(50)\n",
    "test\n",
    "training_set = training_set.head(2450)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.409524</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.219048</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.028571</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.838095</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.600000  0.000298\n",
       "1     88.409524  0.000298\n",
       "2     88.219048  0.000297\n",
       "3     88.028571  0.000297\n",
       "4     87.838095  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6856  # Value for C0\n",
    "K0 = -0.0008  # Value for K0\n",
    "K1 = -0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0128    # Value for b\n",
    "c = -2.3003    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-175)  var1(t-174)  var1(t-173)  var1(t-172)  var1(t-171)  \\\n",
      "175    88.600000    88.409524    88.219048    88.028571    87.838095   \n",
      "176    88.409524    88.219048    88.028571    87.838095    87.647619   \n",
      "177    88.219048    88.028571    87.838095    87.647619    87.457143   \n",
      "178    88.028571    87.838095    87.647619    87.457143    87.266667   \n",
      "179    87.838095    87.647619    87.457143    87.266667    87.076190   \n",
      "\n",
      "     var1(t-170)  var1(t-169)  var1(t-168)  var1(t-167)  var1(t-166)  ...  \\\n",
      "175    87.647619    87.457143    87.266667    87.076190    86.896218  ...   \n",
      "176    87.457143    87.266667    87.076190    86.896218    86.845798  ...   \n",
      "177    87.266667    87.076190    86.896218    86.845798    86.795378  ...   \n",
      "178    87.076190    86.896218    86.845798    86.795378    86.744958  ...   \n",
      "179    86.896218    86.845798    86.795378    86.744958    86.694538  ...   \n",
      "\n",
      "     var1(t+45)  var2(t+45)  var1(t+46)  var2(t+46)  var1(t+47)  var2(t+47)  \\\n",
      "175   82.066270    0.000280   82.050397    0.000280   82.034524    0.000280   \n",
      "176   82.050397    0.000280   82.034524    0.000280   82.018651    0.000280   \n",
      "177   82.034524    0.000280   82.018651    0.000280   82.002778    0.000279   \n",
      "178   82.018651    0.000280   82.002778    0.000279   81.986905    0.000279   \n",
      "179   82.002778    0.000279   81.986905    0.000279   81.971032    0.000279   \n",
      "\n",
      "     var1(t+48)  var2(t+48)  var1(t+49)  var2(t+49)  \n",
      "175   82.018651    0.000280   82.002778    0.000279  \n",
      "176   82.002778    0.000279   81.986905    0.000279  \n",
      "177   81.986905    0.000279   81.971032    0.000279  \n",
      "178   81.971032    0.000279   81.955159    0.000279  \n",
      "179   81.955159    0.000279   81.939286    0.000279  \n",
      "\n",
      "[5 rows x 276 columns]\n",
      "Index(['var1(t-175)', 'var1(t-174)', 'var1(t-173)', 'var1(t-172)',\n",
      "       'var1(t-171)', 'var1(t-170)', 'var1(t-169)', 'var1(t-168)',\n",
      "       'var1(t-167)', 'var1(t-166)',\n",
      "       ...\n",
      "       'var1(t+45)', 'var2(t+45)', 'var1(t+46)', 'var2(t+46)', 'var1(t+47)',\n",
      "       'var2(t+47)', 'var1(t+48)', 'var2(t+48)', 'var1(t+49)', 'var2(t+49)'],\n",
      "      dtype='object', length=276)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 175, n_out = 50)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 176):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-150:]\n",
    "trainX = train[:,:-150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-150:]\n",
    "forecastX = forecast[:,:-150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225, 1, 126) (2225, 150) (1, 1, 126)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "28/28 [==============================] - 2s 19ms/step - loss: 5114.3374 - val_loss: 3064.3391\n",
      "Epoch 2/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4881.7715 - val_loss: 2892.9836\n",
      "Epoch 3/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4656.0034 - val_loss: 2760.6804\n",
      "Epoch 4/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4494.8984 - val_loss: 2664.9717\n",
      "Epoch 5/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 4358.5864 - val_loss: 2582.0261\n",
      "Epoch 6/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4230.7632 - val_loss: 2507.7363\n",
      "Epoch 7/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 4108.0884 - val_loss: 2436.7222\n",
      "Epoch 8/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3989.4758 - val_loss: 2368.4602\n",
      "Epoch 9/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3874.3569 - val_loss: 2302.6709\n",
      "Epoch 10/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 3762.3923 - val_loss: 2239.1753\n",
      "Epoch 11/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3653.3552 - val_loss: 2177.8455\n",
      "Epoch 12/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 3547.0808 - val_loss: 2118.5828\n",
      "Epoch 13/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 3443.4441 - val_loss: 2061.3088\n",
      "Epoch 14/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3342.3452 - val_loss: 2005.9570\n",
      "Epoch 15/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3243.6992 - val_loss: 1952.4696\n",
      "Epoch 16/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3147.4331 - val_loss: 1900.7958\n",
      "Epoch 17/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3053.4844 - val_loss: 1850.8912\n",
      "Epoch 18/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2961.7937 - val_loss: 1802.7404\n",
      "Epoch 19/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 2872.3130 - val_loss: 1756.2719\n",
      "Epoch 20/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2784.9851 - val_loss: 1711.2836\n",
      "Epoch 21/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 2699.7727 - val_loss: 1668.0305\n",
      "Epoch 22/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 2616.6292 - val_loss: 1626.3501\n",
      "Epoch 23/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2535.5144 - val_loss: 1586.2083\n",
      "Epoch 24/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2456.3877 - val_loss: 1547.5728\n",
      "Epoch 25/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 2379.2134 - val_loss: 1510.4121\n",
      "Epoch 26/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2303.9536 - val_loss: 1474.6949\n",
      "Epoch 27/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2230.5730 - val_loss: 1440.3909\n",
      "Epoch 28/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2159.0371 - val_loss: 1407.4694\n",
      "Epoch 29/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 2089.3127 - val_loss: 1375.8899\n",
      "Epoch 30/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2021.3625 - val_loss: 1345.6213\n",
      "Epoch 31/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 1955.2832 - val_loss: 1315.9471\n",
      "Epoch 32/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1890.6790 - val_loss: 1288.5559\n",
      "Epoch 33/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1827.8779 - val_loss: 1262.1458\n",
      "Epoch 34/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1766.7324 - val_loss: 1236.9534\n",
      "Epoch 35/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 1707.2114 - val_loss: 1212.9528\n",
      "Epoch 36/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1649.2854 - val_loss: 1190.1172\n",
      "Epoch 37/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1592.9248 - val_loss: 1168.4211\n",
      "Epoch 38/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1538.1019 - val_loss: 1147.8401\n",
      "Epoch 39/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1484.7878 - val_loss: 1128.3243\n",
      "Epoch 40/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1432.9551 - val_loss: 1109.8486\n",
      "Epoch 41/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1382.5764 - val_loss: 1092.4567\n",
      "Epoch 42/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1333.6251 - val_loss: 1076.0780\n",
      "Epoch 43/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1286.0734 - val_loss: 1060.6886\n",
      "Epoch 44/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1239.8953 - val_loss: 1046.2642\n",
      "Epoch 45/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1195.0649 - val_loss: 1032.7810\n",
      "Epoch 46/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1151.5559 - val_loss: 1020.2150\n",
      "Epoch 47/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 1109.3428 - val_loss: 1008.5426\n",
      "Epoch 48/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1068.4004 - val_loss: 997.7407\n",
      "Epoch 49/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1028.7034 - val_loss: 987.7861\n",
      "Epoch 50/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 990.2264 - val_loss: 978.6559\n",
      "Epoch 51/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 952.9455 - val_loss: 970.3271\n",
      "Epoch 52/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 916.8359 - val_loss: 962.7774\n",
      "Epoch 53/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 881.8740 - val_loss: 955.9842\n",
      "Epoch 54/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 848.0352 - val_loss: 949.9254\n",
      "Epoch 55/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 815.2958 - val_loss: 944.5787\n",
      "Epoch 56/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 783.6326 - val_loss: 939.9226\n",
      "Epoch 57/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 753.0223 - val_loss: 935.9352\n",
      "Epoch 58/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 723.4417 - val_loss: 932.5950\n",
      "Epoch 59/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 694.8679 - val_loss: 929.8809\n",
      "Epoch 60/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 667.2781 - val_loss: 927.7717\n",
      "Epoch 61/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 640.6502 - val_loss: 926.2463\n",
      "Epoch 62/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 614.9616 - val_loss: 925.2840\n",
      "Epoch 63/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 590.1909 - val_loss: 924.8642\n",
      "Epoch 64/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 566.3154 - val_loss: 924.9668\n",
      "Epoch 65/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 543.3141 - val_loss: 925.5712\n",
      "Epoch 66/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 521.1652 - val_loss: 926.6577\n",
      "Epoch 67/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 499.8478 - val_loss: 928.2063\n",
      "Epoch 68/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 479.3410 - val_loss: 930.1974\n",
      "Epoch 69/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 459.6235 - val_loss: 932.6121\n",
      "Epoch 70/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 440.6753 - val_loss: 935.4307\n",
      "Epoch 71/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 422.4755 - val_loss: 938.6343\n",
      "Epoch 72/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 405.0044 - val_loss: 942.2045\n",
      "Epoch 73/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 388.2418 - val_loss: 946.1227\n",
      "Epoch 74/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 372.1682 - val_loss: 950.3705\n",
      "Epoch 75/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 356.7640 - val_loss: 954.9302\n",
      "Epoch 76/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 342.0099 - val_loss: 959.7838\n",
      "Epoch 77/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 327.8872 - val_loss: 964.9136\n",
      "Epoch 78/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 314.3767 - val_loss: 970.3026\n",
      "Epoch 79/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 301.4602 - val_loss: 975.9336\n",
      "Epoch 80/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 289.1191 - val_loss: 981.7902\n",
      "Epoch 81/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 277.3357 - val_loss: 987.8559\n",
      "Epoch 82/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 266.0919 - val_loss: 994.1147\n",
      "Epoch 83/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 255.3703 - val_loss: 1000.5504\n",
      "Epoch 84/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 245.1537 - val_loss: 1007.1480\n",
      "Epoch 85/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 235.4250 - val_loss: 1013.8920\n",
      "Epoch 86/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 226.1674 - val_loss: 1020.7673\n",
      "Epoch 87/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 217.3645 - val_loss: 1027.7599\n",
      "Epoch 88/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 209.0001 - val_loss: 1034.8546\n",
      "Epoch 89/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 201.0584 - val_loss: 1042.0386\n",
      "Epoch 90/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 193.5237 - val_loss: 1049.2981\n",
      "Epoch 91/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 186.3804 - val_loss: 1056.6199\n",
      "Epoch 92/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 179.6139 - val_loss: 1063.9907\n",
      "Epoch 93/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 173.2094 - val_loss: 1071.3987\n",
      "Epoch 94/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 167.1522 - val_loss: 1078.8315\n",
      "Epoch 95/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 161.4285 - val_loss: 1086.2775\n",
      "Epoch 96/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 156.0245 - val_loss: 1093.7258\n",
      "Epoch 97/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 150.9265 - val_loss: 1101.1650\n",
      "Epoch 98/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 146.1214 - val_loss: 1108.5847\n",
      "Epoch 99/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 141.5965 - val_loss: 1115.9751\n",
      "Epoch 100/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 137.3392 - val_loss: 1123.3264\n",
      "Epoch 101/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 133.3373 - val_loss: 1130.6294\n",
      "Epoch 102/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 129.5790 - val_loss: 1137.8756\n",
      "Epoch 103/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 126.0528 - val_loss: 1145.0562\n",
      "Epoch 104/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 122.7475 - val_loss: 1152.1630\n",
      "Epoch 105/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 119.6522 - val_loss: 1159.1890\n",
      "Epoch 106/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 116.7564 - val_loss: 1166.1271\n",
      "Epoch 107/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 114.0500 - val_loss: 1172.9698\n",
      "Epoch 108/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 111.5231 - val_loss: 1179.7117\n",
      "Epoch 109/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 109.1662 - val_loss: 1186.3467\n",
      "Epoch 110/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 106.9701 - val_loss: 1192.8690\n",
      "Epoch 111/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 104.9261 - val_loss: 1199.2743\n",
      "Epoch 112/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 103.0255 - val_loss: 1205.5573\n",
      "Epoch 113/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 101.2601 - val_loss: 1211.7145\n",
      "Epoch 114/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 99.6222 - val_loss: 1217.7415\n",
      "Epoch 115/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 98.1042 - val_loss: 1223.6354\n",
      "Epoch 116/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 96.6988 - val_loss: 1229.3928\n",
      "Epoch 117/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 95.3993 - val_loss: 1235.0115\n",
      "Epoch 118/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 94.1987 - val_loss: 1240.4886\n",
      "Epoch 119/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 93.0911 - val_loss: 1245.8231\n",
      "Epoch 120/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 92.0701 - val_loss: 1251.0137\n",
      "Epoch 121/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 91.1303 - val_loss: 1256.0580\n",
      "Epoch 122/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 90.2661 - val_loss: 1260.9565\n",
      "Epoch 123/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 89.4724 - val_loss: 1265.7079\n",
      "Epoch 124/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 88.7443 - val_loss: 1270.3118\n",
      "Epoch 125/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 88.0770 - val_loss: 1274.7698\n",
      "Epoch 126/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 87.4663 - val_loss: 1279.0809\n",
      "Epoch 127/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 86.9081 - val_loss: 1283.2463\n",
      "Epoch 128/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 86.3983 - val_loss: 1287.2670\n",
      "Epoch 129/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.9334 - val_loss: 1291.1439\n",
      "Epoch 130/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.5100 - val_loss: 1294.8784\n",
      "Epoch 131/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.1248 - val_loss: 1298.4729\n",
      "Epoch 132/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 84.7746 - val_loss: 1301.9281\n",
      "Epoch 133/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 84.4569 - val_loss: 1305.2479\n",
      "Epoch 134/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 84.1688 - val_loss: 1308.4329\n",
      "Epoch 135/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 83.9079 - val_loss: 1311.4862\n",
      "Epoch 136/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.6719 - val_loss: 1314.4102\n",
      "Epoch 137/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 83.4588 - val_loss: 1317.2073\n",
      "Epoch 138/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.2664 - val_loss: 1319.8813\n",
      "Epoch 139/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.0929 - val_loss: 1322.4341\n",
      "Epoch 140/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.9369 - val_loss: 1324.8690\n",
      "Epoch 141/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.7964 - val_loss: 1327.1891\n",
      "Epoch 142/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.6704 - val_loss: 1329.3975\n",
      "Epoch 143/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.5572 - val_loss: 1331.4979\n",
      "Epoch 144/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.4559 - val_loss: 1333.4938\n",
      "Epoch 145/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.3650 - val_loss: 1335.3868\n",
      "Epoch 146/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 82.2837 - val_loss: 1337.1804\n",
      "Epoch 147/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 82.2112 - val_loss: 1338.8806\n",
      "Epoch 148/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.1465 - val_loss: 1340.4879\n",
      "Epoch 149/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.0886 - val_loss: 1342.0057\n",
      "Epoch 150/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.0372 - val_loss: 1343.4393\n",
      "Epoch 151/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9915 - val_loss: 1344.7913\n",
      "Epoch 152/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9509 - val_loss: 1346.0636\n",
      "Epoch 153/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9147 - val_loss: 1347.2594\n",
      "Epoch 154/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8827 - val_loss: 1348.3828\n",
      "Epoch 155/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8543 - val_loss: 1349.4358\n",
      "Epoch 156/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8292 - val_loss: 1350.4224\n",
      "Epoch 157/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8071 - val_loss: 1351.3452\n",
      "Epoch 158/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7875 - val_loss: 1352.2063\n",
      "Epoch 159/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7701 - val_loss: 1353.0099\n",
      "Epoch 160/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7550 - val_loss: 1353.7566\n",
      "Epoch 161/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7416 - val_loss: 1354.4515\n",
      "Epoch 162/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7298 - val_loss: 1355.0942\n",
      "Epoch 163/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7196 - val_loss: 1355.6876\n",
      "Epoch 164/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7106 - val_loss: 1356.2325\n",
      "Epoch 165/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7028 - val_loss: 1356.7288\n",
      "Epoch 166/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.6961 - val_loss: 1357.1813\n",
      "Epoch 167/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.6902 - val_loss: 1357.5896\n",
      "Epoch 168/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 81.6852 - val_loss: 1357.9534\n",
      "Epoch 169/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.6808 - val_loss: 1358.2677\n",
      "Epoch 170/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.6771 - val_loss: 1358.5267\n",
      "Epoch 171/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.6740 - val_loss: 1358.7084\n",
      "Epoch 172/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.6713 - val_loss: 1358.7440\n",
      "Epoch 173/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.6692 - val_loss: 1357.9241\n",
      "Epoch 174/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.6411 - val_loss: 1335.2739\n",
      "Epoch 175/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.3212 - val_loss: 1304.1761\n",
      "Epoch 176/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 84.3691 - val_loss: 1308.8782\n",
      "Epoch 177/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.9740 - val_loss: 1313.3003\n",
      "Epoch 178/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.5812 - val_loss: 1307.5657\n",
      "Epoch 179/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.3966 - val_loss: 1320.8888\n",
      "Epoch 180/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.1056 - val_loss: 1324.2302\n",
      "Epoch 181/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.8958 - val_loss: 1327.1517\n",
      "Epoch 182/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.7175 - val_loss: 1329.2567\n",
      "Epoch 183/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.5400 - val_loss: 1324.9010\n",
      "Epoch 184/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.2522 - val_loss: 1338.0437\n",
      "Epoch 185/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.3225 - val_loss: 1340.2312\n",
      "Epoch 186/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.2291 - val_loss: 1342.1501\n",
      "Epoch 187/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9839 - val_loss: 1330.4817\n",
      "Epoch 188/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.2848 - val_loss: 1347.1918\n",
      "Epoch 189/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 82.0231 - val_loss: 1348.7087\n",
      "Epoch 190/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.9746 - val_loss: 1348.3433\n",
      "Epoch 191/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8417 - val_loss: 1336.4791\n",
      "Epoch 192/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 84.7355 - val_loss: 1291.0166\n",
      "Epoch 193/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 85.6793 - val_loss: 1297.3425\n",
      "Epoch 194/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.0410 - val_loss: 1302.7056\n",
      "Epoch 195/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 84.4622 - val_loss: 1287.2194\n",
      "Epoch 196/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 84.0666 - val_loss: 1314.5315\n",
      "Epoch 197/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.6577 - val_loss: 1319.1716\n",
      "Epoch 198/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.3407 - val_loss: 1323.4156\n",
      "Epoch 199/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.0763 - val_loss: 1327.2936\n",
      "Epoch 200/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.8556 - val_loss: 1330.8331\n",
      "Epoch 201/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.6713 - val_loss: 1334.0593\n",
      "Epoch 202/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.5174 - val_loss: 1336.9979\n",
      "Epoch 203/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.3888 - val_loss: 1339.6699\n",
      "Epoch 204/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.2814 - val_loss: 1342.0985\n",
      "Epoch 205/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.1914 - val_loss: 1344.3029\n",
      "Epoch 206/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.1161 - val_loss: 1346.3015\n",
      "Epoch 207/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.0530 - val_loss: 1348.1128\n",
      "Epoch 208/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 82.0001 - val_loss: 1349.7511\n",
      "Epoch 209/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9557 - val_loss: 1351.2332\n",
      "Epoch 210/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 81.9184 - val_loss: 1352.5725\n",
      "Epoch 211/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.8869 - val_loss: 1353.7798\n",
      "Epoch 212/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8605 - val_loss: 1354.8698\n",
      "Epoch 213/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8382 - val_loss: 1355.8513\n",
      "Epoch 214/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8195 - val_loss: 1356.7357\n",
      "Epoch 215/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8035 - val_loss: 1357.5309\n",
      "Epoch 216/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7901 - val_loss: 1358.2452\n",
      "Epoch 217/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7788 - val_loss: 1358.8868\n",
      "Epoch 218/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 81.7691 - val_loss: 1359.4633\n",
      "Epoch 219/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7610 - val_loss: 1359.9789\n",
      "Epoch 220/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7541 - val_loss: 1360.4415\n",
      "Epoch 221/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7483 - val_loss: 1360.8547\n",
      "Epoch 222/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7435 - val_loss: 1361.2247\n",
      "Epoch 223/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7394 - val_loss: 1361.5557\n",
      "Epoch 224/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7359 - val_loss: 1361.8513\n",
      "Epoch 225/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7330 - val_loss: 1362.1145\n",
      "Epoch 226/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7307 - val_loss: 1362.3501\n",
      "Epoch 227/500\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 81.7287 - val_loss: 1362.5598\n",
      "Epoch 228/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 81.7271 - val_loss: 1362.7455\n",
      "Epoch 229/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 81.7258 - val_loss: 1362.9113\n",
      "Epoch 230/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7248 - val_loss: 1363.0573\n",
      "Epoch 231/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 81.7239 - val_loss: 1363.1641\n",
      "Epoch 232/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 81.6964 - val_loss: 1345.6949\n",
      "Epoch 233/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 83.5259 - val_loss: 1300.7241\n",
      "Epoch 234/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 84.5553 - val_loss: 1306.9821\n",
      "Epoch 235/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 84.0418 - val_loss: 1312.7683\n",
      "Epoch 236/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 83.6148 - val_loss: 1317.9694\n",
      "Epoch 237/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 83.2694 - val_loss: 1322.6323\n",
      "Epoch 238/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 82.9900 - val_loss: 1326.8087\n",
      "Epoch 239/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 82.7638 - val_loss: 1330.5455\n",
      "Epoch 240/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 82.5803 - val_loss: 1333.8851\n",
      "Epoch 241/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 82.4312 - val_loss: 1336.8676\n",
      "Epoch 242/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 82.3098 - val_loss: 1339.5287\n",
      "Epoch 243/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 82.2107 - val_loss: 1341.9010\n",
      "Epoch 244/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 82.1297 - val_loss: 1344.0148\n",
      "Epoch 245/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 82.0632 - val_loss: 1345.8955\n",
      "Epoch 246/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.0086 - val_loss: 1347.5693\n",
      "Epoch 247/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.9636 - val_loss: 1349.0577\n",
      "Epoch 248/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 81.9264 - val_loss: 1350.3805\n",
      "Epoch 249/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 81.8956 - val_loss: 1351.5549\n",
      "Epoch 250/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8700 - val_loss: 1352.5973\n",
      "Epoch 251/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8486 - val_loss: 1353.5219\n",
      "Epoch 252/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8307 - val_loss: 1354.3411\n",
      "Epoch 253/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8159 - val_loss: 1355.0669\n",
      "Epoch 254/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8033 - val_loss: 1355.7097\n",
      "Epoch 255/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7928 - val_loss: 1356.2795\n",
      "Epoch 256/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7839 - val_loss: 1356.7836\n",
      "Epoch 257/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7763 - val_loss: 1357.2285\n",
      "Epoch 258/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7700 - val_loss: 1357.6222\n",
      "Epoch 259/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 81.7646 - val_loss: 1357.9700\n",
      "Epoch 260/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7601 - val_loss: 1358.2771\n",
      "Epoch 261/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7562 - val_loss: 1358.5479\n",
      "Epoch 262/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7529 - val_loss: 1358.7871\n",
      "Epoch 263/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7501 - val_loss: 1358.9980\n",
      "Epoch 264/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7478 - val_loss: 1359.1831\n",
      "Epoch 265/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7458 - val_loss: 1359.3472\n",
      "Epoch 266/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7441 - val_loss: 1359.4907\n",
      "Epoch 267/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7428 - val_loss: 1359.6177\n",
      "Epoch 268/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7416 - val_loss: 1359.7286\n",
      "Epoch 269/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7406 - val_loss: 1359.8264\n",
      "Epoch 270/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7399 - val_loss: 1359.9117\n",
      "Epoch 271/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7392 - val_loss: 1359.9869\n",
      "Epoch 272/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7387 - val_loss: 1360.0530\n",
      "Epoch 273/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7383 - val_loss: 1360.1104\n",
      "Epoch 274/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7380 - val_loss: 1360.1610\n",
      "Epoch 275/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7378 - val_loss: 1360.2052\n",
      "Epoch 276/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7376 - val_loss: 1360.2433\n",
      "Epoch 277/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 81.7375 - val_loss: 1360.2764\n",
      "Epoch 278/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7374 - val_loss: 1360.3053\n",
      "Epoch 279/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7374 - val_loss: 1360.3303\n",
      "Epoch 280/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7375 - val_loss: 1360.3523\n",
      "Epoch 281/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7375 - val_loss: 1360.3705\n",
      "Epoch 282/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7376 - val_loss: 1360.3868\n",
      "Epoch 283/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7377 - val_loss: 1360.4004\n",
      "Epoch 284/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7380 - val_loss: 1360.4120\n",
      "Epoch 285/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7381 - val_loss: 1360.4215\n",
      "Epoch 286/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7383 - val_loss: 1360.4304\n",
      "Epoch 287/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7384 - val_loss: 1360.4365\n",
      "Epoch 288/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7386 - val_loss: 1360.4421\n",
      "Epoch 289/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7388 - val_loss: 1360.4468\n",
      "Epoch 290/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7391 - val_loss: 1360.4506\n",
      "Epoch 291/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7392 - val_loss: 1360.4528\n",
      "Epoch 292/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7394 - val_loss: 1360.4541\n",
      "Epoch 293/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7397 - val_loss: 1360.4551\n",
      "Epoch 294/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 81.7399 - val_loss: 1360.4551\n",
      "Epoch 295/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7401 - val_loss: 1360.4546\n",
      "Epoch 296/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7403 - val_loss: 1360.4523\n",
      "Epoch 297/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7405 - val_loss: 1360.4498\n",
      "Epoch 298/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7407 - val_loss: 1360.4452\n",
      "Epoch 299/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7410 - val_loss: 1360.4368\n",
      "Epoch 300/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7411 - val_loss: 1360.4130\n",
      "Epoch 301/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7413 - val_loss: 1360.1620\n",
      "Epoch 302/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7114 - val_loss: 1353.1528\n",
      "Epoch 303/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.5832 - val_loss: 1291.3073\n",
      "Epoch 304/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 85.1111 - val_loss: 1298.7177\n",
      "Epoch 305/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 84.4505 - val_loss: 1305.4689\n",
      "Epoch 306/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.9120 - val_loss: 1311.4663\n",
      "Epoch 307/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 83.4852 - val_loss: 1316.7797\n",
      "Epoch 308/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 83.1471 - val_loss: 1321.4822\n",
      "Epoch 309/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.8785 - val_loss: 1325.6417\n",
      "Epoch 310/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 82.6645 - val_loss: 1329.3181\n",
      "Epoch 311/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 82.4936 - val_loss: 1332.5645\n",
      "Epoch 312/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 82.3566 - val_loss: 1335.4303\n",
      "Epoch 313/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 82.2464 - val_loss: 1337.9573\n",
      "Epoch 314/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.1575 - val_loss: 1340.1537\n",
      "Epoch 315/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.0802 - val_loss: 1339.7500\n",
      "Epoch 316/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.3261 - val_loss: 1342.5028\n",
      "Epoch 317/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.0739 - val_loss: 1344.4574\n",
      "Epoch 318/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.0048 - val_loss: 1346.0566\n",
      "Epoch 319/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9577 - val_loss: 1347.4227\n",
      "Epoch 320/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9215 - val_loss: 1348.5991\n",
      "Epoch 321/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8929 - val_loss: 1349.6179\n",
      "Epoch 322/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8697 - val_loss: 1350.5002\n",
      "Epoch 323/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8507 - val_loss: 1351.2676\n",
      "Epoch 324/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8352 - val_loss: 1351.9337\n",
      "Epoch 325/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 81.8223 - val_loss: 1352.5129\n",
      "Epoch 326/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8115 - val_loss: 1353.0177\n",
      "Epoch 327/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8025 - val_loss: 1353.4572\n",
      "Epoch 328/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7949 - val_loss: 1353.8401\n",
      "Epoch 329/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7885 - val_loss: 1354.1727\n",
      "Epoch 330/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7830 - val_loss: 1354.4633\n",
      "Epoch 331/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7784 - val_loss: 1354.7162\n",
      "Epoch 332/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7743 - val_loss: 1354.9359\n",
      "Epoch 333/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7710 - val_loss: 1355.1267\n",
      "Epoch 334/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7680 - val_loss: 1355.2938\n",
      "Epoch 335/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7654 - val_loss: 1355.4373\n",
      "Epoch 336/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7633 - val_loss: 1355.5636\n",
      "Epoch 337/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7614 - val_loss: 1355.6719\n",
      "Epoch 338/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7598 - val_loss: 1355.7679\n",
      "Epoch 339/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7583 - val_loss: 1355.8488\n",
      "Epoch 340/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 81.7572 - val_loss: 1355.9204\n",
      "Epoch 341/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7561 - val_loss: 1355.9822\n",
      "Epoch 342/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7551 - val_loss: 1356.0336\n",
      "Epoch 343/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7544 - val_loss: 1356.0791\n",
      "Epoch 344/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7537 - val_loss: 1356.1177\n",
      "Epoch 345/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7531 - val_loss: 1356.1509\n",
      "Epoch 346/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7526 - val_loss: 1356.1799\n",
      "Epoch 347/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7521 - val_loss: 1356.2034\n",
      "Epoch 348/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7516 - val_loss: 1356.2233\n",
      "Epoch 349/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7514 - val_loss: 1356.2406\n",
      "Epoch 350/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7511 - val_loss: 1356.2550\n",
      "Epoch 351/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7508 - val_loss: 1356.2668\n",
      "Epoch 352/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7505 - val_loss: 1356.2759\n",
      "Epoch 353/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7504 - val_loss: 1356.2828\n",
      "Epoch 354/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7502 - val_loss: 1356.2885\n",
      "Epoch 355/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 81.7500 - val_loss: 1356.2919\n",
      "Epoch 356/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7499 - val_loss: 1356.2936\n",
      "Epoch 357/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7497 - val_loss: 1356.2941\n",
      "Epoch 358/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7496 - val_loss: 1356.2932\n",
      "Epoch 359/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7495 - val_loss: 1356.2922\n",
      "Epoch 360/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7494 - val_loss: 1356.2900\n",
      "Epoch 361/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7494 - val_loss: 1356.2856\n",
      "Epoch 362/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7493 - val_loss: 1356.2823\n",
      "Epoch 363/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7493 - val_loss: 1356.2780\n",
      "Epoch 364/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7492 - val_loss: 1356.2727\n",
      "Epoch 365/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7492 - val_loss: 1356.2666\n",
      "Epoch 366/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7492 - val_loss: 1356.2606\n",
      "Epoch 367/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7491 - val_loss: 1356.2543\n",
      "Epoch 368/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7491 - val_loss: 1356.2469\n",
      "Epoch 369/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7492 - val_loss: 1356.2389\n",
      "Epoch 370/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 81.7491 - val_loss: 1356.2317\n",
      "Epoch 371/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7491 - val_loss: 1356.2233\n",
      "Epoch 372/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7491 - val_loss: 1356.2144\n",
      "Epoch 373/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7491 - val_loss: 1356.2056\n",
      "Epoch 374/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7491 - val_loss: 1356.1959\n",
      "Epoch 375/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7492 - val_loss: 1356.1859\n",
      "Epoch 376/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7491 - val_loss: 1356.1761\n",
      "Epoch 377/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7491 - val_loss: 1356.1671\n",
      "Epoch 378/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7490 - val_loss: 1356.1556\n",
      "Epoch 379/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7490 - val_loss: 1356.1428\n",
      "Epoch 380/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7490 - val_loss: 1356.1288\n",
      "Epoch 381/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7489 - val_loss: 1356.1107\n",
      "Epoch 382/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7489 - val_loss: 1356.0775\n",
      "Epoch 383/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 81.7490 - val_loss: 1355.0168\n",
      "Epoch 384/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.8736 - val_loss: 1356.1421\n",
      "Epoch 385/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7490 - val_loss: 1356.1278\n",
      "Epoch 386/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7487 - val_loss: 1356.1105\n",
      "Epoch 387/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7488 - val_loss: 1356.0931\n",
      "Epoch 388/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7488 - val_loss: 1356.0764\n",
      "Epoch 389/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7489 - val_loss: 1356.0587\n",
      "Epoch 390/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7489 - val_loss: 1356.0414\n",
      "Epoch 391/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7488 - val_loss: 1356.0233\n",
      "Epoch 392/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7490 - val_loss: 1356.0048\n",
      "Epoch 393/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7490 - val_loss: 1355.9871\n",
      "Epoch 394/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 81.7490 - val_loss: 1355.9685\n",
      "Epoch 395/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7490 - val_loss: 1355.9493\n",
      "Epoch 396/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7490 - val_loss: 1355.9293\n",
      "Epoch 397/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7490 - val_loss: 1355.9080\n",
      "Epoch 398/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7491 - val_loss: 1355.8868\n",
      "Epoch 399/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7491 - val_loss: 1355.8654\n",
      "Epoch 400/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7491 - val_loss: 1355.8430\n",
      "Epoch 401/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 81.7491 - val_loss: 1355.8203\n",
      "Epoch 402/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7492 - val_loss: 1355.7969\n",
      "Epoch 403/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 81.7492 - val_loss: 1355.7726\n",
      "Epoch 404/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7492 - val_loss: 1355.7469\n",
      "Epoch 405/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7492 - val_loss: 1355.7196\n",
      "Epoch 406/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7492 - val_loss: 1355.6909\n",
      "Epoch 407/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7491 - val_loss: 1355.6605\n",
      "Epoch 408/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7492 - val_loss: 1355.6284\n",
      "Epoch 409/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7491 - val_loss: 1355.5945\n",
      "Epoch 410/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7492 - val_loss: 1355.5586\n",
      "Epoch 411/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7492 - val_loss: 1355.5203\n",
      "Epoch 412/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7492 - val_loss: 1355.4791\n",
      "Epoch 413/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7492 - val_loss: 1355.4340\n",
      "Epoch 414/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7492 - val_loss: 1355.3860\n",
      "Epoch 415/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7492 - val_loss: 1355.3330\n",
      "Epoch 416/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7492 - val_loss: 1355.2747\n",
      "Epoch 417/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7493 - val_loss: 1355.2096\n",
      "Epoch 418/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7492 - val_loss: 1355.1351\n",
      "Epoch 419/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7492 - val_loss: 1355.0457\n",
      "Epoch 420/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7492 - val_loss: 1354.9325\n",
      "Epoch 421/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7493 - val_loss: 1354.7719\n",
      "Epoch 422/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 81.7493 - val_loss: 1354.4491\n",
      "Epoch 423/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7486 - val_loss: 1335.2130\n",
      "Epoch 424/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7559 - val_loss: 1358.3682\n",
      "Epoch 425/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7492 - val_loss: 1358.3351\n",
      "Epoch 426/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7492 - val_loss: 1358.2993\n",
      "Epoch 427/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7492 - val_loss: 1358.2616\n",
      "Epoch 428/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7493 - val_loss: 1358.2212\n",
      "Epoch 429/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7493 - val_loss: 1358.1776\n",
      "Epoch 430/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7493 - val_loss: 1358.1304\n",
      "Epoch 431/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7493 - val_loss: 1358.0786\n",
      "Epoch 432/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7493 - val_loss: 1358.0225\n",
      "Epoch 433/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7493 - val_loss: 1357.9612\n",
      "Epoch 434/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7493 - val_loss: 1357.8934\n",
      "Epoch 435/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7493 - val_loss: 1357.8165\n",
      "Epoch 436/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7493 - val_loss: 1357.7281\n",
      "Epoch 437/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7493 - val_loss: 1357.6252\n",
      "Epoch 438/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7493 - val_loss: 1357.5029\n",
      "Epoch 439/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7493 - val_loss: 1357.3519\n",
      "Epoch 440/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7493 - val_loss: 1357.1580\n",
      "Epoch 441/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 81.7493 - val_loss: 1356.8881\n",
      "Epoch 442/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7493 - val_loss: 1356.4617\n",
      "Epoch 443/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7493 - val_loss: 1355.5787\n",
      "Epoch 444/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7493 - val_loss: 1349.8264\n",
      "Epoch 445/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.5324 - val_loss: 1314.6649\n",
      "Epoch 446/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 85.4716 - val_loss: 1309.6815\n",
      "Epoch 447/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 84.9542 - val_loss: 1317.1963\n",
      "Epoch 448/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 84.3127 - val_loss: 1323.9812\n",
      "Epoch 449/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.7955 - val_loss: 1329.9849\n",
      "Epoch 450/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.3877 - val_loss: 1335.2856\n",
      "Epoch 451/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 83.0660 - val_loss: 1339.9626\n",
      "Epoch 452/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.8112 - val_loss: 1344.0868\n",
      "Epoch 453/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.6090 - val_loss: 1347.7207\n",
      "Epoch 454/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.4479 - val_loss: 1350.9189\n",
      "Epoch 455/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.3192 - val_loss: 1353.7361\n",
      "Epoch 456/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.2158 - val_loss: 1356.2130\n",
      "Epoch 457/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.1326 - val_loss: 1358.3917\n",
      "Epoch 458/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.0654 - val_loss: 1360.3070\n",
      "Epoch 459/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 82.0107 - val_loss: 1361.9905\n",
      "Epoch 460/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.9661 - val_loss: 1363.4691\n",
      "Epoch 461/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 81.9296 - val_loss: 1364.7679\n",
      "Epoch 462/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8996 - val_loss: 1365.9067\n",
      "Epoch 463/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8748 - val_loss: 1366.9082\n",
      "Epoch 464/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8543 - val_loss: 1367.7854\n",
      "Epoch 465/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8371 - val_loss: 1368.5548\n",
      "Epoch 466/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8230 - val_loss: 1369.2312\n",
      "Epoch 467/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8109 - val_loss: 1369.8240\n",
      "Epoch 468/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.8008 - val_loss: 1370.3423\n",
      "Epoch 469/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7923 - val_loss: 1370.7983\n",
      "Epoch 470/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7850 - val_loss: 1371.1980\n",
      "Epoch 471/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7788 - val_loss: 1371.5474\n",
      "Epoch 472/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7737 - val_loss: 1371.8536\n",
      "Epoch 473/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7691 - val_loss: 1372.1213\n",
      "Epoch 474/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7652 - val_loss: 1372.3555\n",
      "Epoch 475/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7619 - val_loss: 1372.5601\n",
      "Epoch 476/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 81.7590 - val_loss: 1372.7395\n",
      "Epoch 477/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 81.7566 - val_loss: 1372.8958\n",
      "Epoch 478/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7544 - val_loss: 1373.0305\n",
      "Epoch 479/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7526 - val_loss: 1373.1482\n",
      "Epoch 480/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 81.7510 - val_loss: 1373.2506\n",
      "Epoch 481/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7497 - val_loss: 1373.3394\n",
      "Epoch 482/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7484 - val_loss: 1373.4149\n",
      "Epoch 483/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7474 - val_loss: 1373.4812\n",
      "Epoch 484/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7466 - val_loss: 1373.5385\n",
      "Epoch 485/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 81.7457 - val_loss: 1373.5884\n",
      "Epoch 486/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7449 - val_loss: 1373.6289\n",
      "Epoch 487/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7445 - val_loss: 1373.6652\n",
      "Epoch 488/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7439 - val_loss: 1373.6963\n",
      "Epoch 489/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7434 - val_loss: 1373.7196\n",
      "Epoch 490/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7431 - val_loss: 1373.7404\n",
      "Epoch 491/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 81.7427 - val_loss: 1373.7550\n",
      "Epoch 492/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 81.7424 - val_loss: 1373.7667\n",
      "Epoch 493/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7422 - val_loss: 1373.7764\n",
      "Epoch 494/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7420 - val_loss: 1373.7825\n",
      "Epoch 495/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7417 - val_loss: 1373.7849\n",
      "Epoch 496/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 81.7414 - val_loss: 1373.7847\n",
      "Epoch 497/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7414 - val_loss: 1373.7820\n",
      "Epoch 498/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7413 - val_loss: 1373.7771\n",
      "Epoch 499/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7413 - val_loss: 1373.7706\n",
      "Epoch 500/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 81.7412 - val_loss: 1373.7637\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6856, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0008, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0128, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(-2.3003, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 354ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 126)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 126)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 186)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:150:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 6.91160598e+01, 6.86118581e+01, 6.81076564e+01,\n",
       "        6.76948483e+01, 6.74251004e+01, 6.71553525e+01, 6.68856046e+01,\n",
       "        6.66158161e+01, 6.63460624e+01, 0.00000000e+00, 8.12869100e-02,\n",
       "        6.75349977e+01, 6.72652498e+01, 6.69955019e+01, 6.67255159e+01,\n",
       "        6.64568390e+01, 6.61864535e+01, 6.59172895e+01, 6.56481325e+01,\n",
       "        6.53789755e+01, 6.51098185e+01, 6.48406615e+01, 4.90063705e+01,\n",
       "        2.30149930e-01, 0.00000000e+00, 6.24974730e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.20691640e-01, 4.89851227e+01, 1.88739300e-02,\n",
       "        2.73354739e-01, 1.37845740e-01, 4.23176736e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.65639734e-01,\n",
       "        1.39531597e-01, 2.09663972e-01, 0.00000000e+00, 7.40632653e-01,\n",
       "        1.93554446e-01, 1.86798289e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.64838243e-02, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([55.6025825 , 55.59028489, 55.57798729, 55.56568969, 55.55339209,\n",
       "       55.54109449, 55.52879688, 55.51649928, 55.50420168, 55.49190408,\n",
       "       55.47960648, 55.46730887, 55.45501127, 55.44271367, 55.43041607,\n",
       "       55.41811847, 55.40582086, 55.39352326, 55.38122566, 55.36892806,\n",
       "       55.35663046, 55.34433286, 55.33203525, 55.31973765, 55.30744005,\n",
       "       55.29514245, 55.28284485, 55.27054724, 55.25824964, 55.24595204,\n",
       "       55.23365444, 55.22135684, 55.20905923, 55.19676163, 55.18446403,\n",
       "       55.17216643, 55.15986883, 55.14757122, 55.13527362, 55.12297602,\n",
       "       55.11067842, 55.09838082, 55.08608321, 55.07378561, 55.06148801,\n",
       "       55.04919041, 55.03689281, 55.0245952 , 55.0122976 , 55.        ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.83264157632516\n",
      "35.61021983476031\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
