{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       84.600000\n",
       "1       84.431933\n",
       "2       84.263866\n",
       "3       84.095798\n",
       "4       83.927731\n",
       "          ...    \n",
       "1695    54.354202\n",
       "1696    54.348599\n",
       "1697    54.342997\n",
       "1698    54.337395\n",
       "1699    54.331793\n",
       "Name: C6, Length: 1700, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c6_interpolated_1600_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       84.600000\n",
       "1       84.431933\n",
       "2       84.263866\n",
       "3       84.095798\n",
       "4       83.927731\n",
       "          ...    \n",
       "1595     0.000000\n",
       "1596     0.281777\n",
       "1597     0.209573\n",
       "1598     0.853143\n",
       "1599     0.000000\n",
       "Name: C6, Length: 1600, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1600)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.431933</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84.263866</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84.095798</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83.927731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     84.600000  0.000298\n",
       "1     84.431933  0.000298\n",
       "2     84.263866  0.000297\n",
       "3     84.095798  0.000297\n",
       "4     83.927731  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 82.7524  # Value for C0\n",
    "K0 = -0.0031  # Value for K0\n",
    "K1 = -0.0003  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0171    # Value for b\n",
    "c = 3.0230    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    84.600000    84.431933    84.263866    84.095798    83.927731   \n",
      "351    84.431933    84.263866    84.095798    83.927731    83.759664   \n",
      "352    84.263866    84.095798    83.927731    83.759664    83.591597   \n",
      "353    84.095798    83.927731    83.759664    83.591597    83.423529   \n",
      "354    83.927731    83.759664    83.591597    83.423529    83.255462   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    83.759664    83.591597    83.423529    83.255462    83.092437  ...   \n",
      "351    83.591597    83.423529    83.255462    83.092437    82.991597  ...   \n",
      "352    83.423529    83.255462    83.092437    82.991597    82.890756  ...   \n",
      "353    83.255462    83.092437    82.991597    82.890756    82.789916  ...   \n",
      "354    83.092437    82.991597    82.890756    82.789916    82.689076  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   70.003688    0.000263   69.960738    0.000263   69.917787    0.000263   \n",
      "351   69.960738    0.000263   69.917787    0.000263   69.874837    0.000262   \n",
      "352   69.917787    0.000263   69.874837    0.000262   69.831886    0.000262   \n",
      "353   69.874837    0.000262   69.831886    0.000262   69.788936    0.000262   \n",
      "354   69.831886    0.000262   69.788936    0.000262   69.745985    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   69.874837    0.000262   69.831886    0.000262  \n",
      "351   69.831886    0.000262   69.788936    0.000262  \n",
      "352   69.788936    0.000262   69.745985    0.000262  \n",
      "353   69.745985    0.000262   69.703035    0.000262  \n",
      "354   69.703035    0.000262   69.660084    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1150, 1, 251) (1150, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "15/15 [==============================] - 3s 37ms/step - loss: 3977.8770 - val_loss: 2974.3948\n",
      "Epoch 2/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3867.2937 - val_loss: 2934.5632\n",
      "Epoch 3/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3821.6436 - val_loss: 2894.5508\n",
      "Epoch 4/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3776.1707 - val_loss: 2854.9622\n",
      "Epoch 5/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3731.1782 - val_loss: 2815.8420\n",
      "Epoch 6/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3686.6738 - val_loss: 2777.1689\n",
      "Epoch 7/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3642.6316 - val_loss: 2738.9211\n",
      "Epoch 8/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3599.0298 - val_loss: 2701.0828\n",
      "Epoch 9/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3555.8542 - val_loss: 2663.6433\n",
      "Epoch 10/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3513.0920 - val_loss: 2626.5940\n",
      "Epoch 11/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 3470.7371 - val_loss: 2589.9280\n",
      "Epoch 12/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3428.7812 - val_loss: 2553.6404\n",
      "Epoch 13/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3387.2185 - val_loss: 2517.7261\n",
      "Epoch 14/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3346.0461 - val_loss: 2482.1809\n",
      "Epoch 15/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3305.2588 - val_loss: 2447.0010\n",
      "Epoch 16/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3264.8523 - val_loss: 2412.1833\n",
      "Epoch 17/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3224.8235 - val_loss: 2377.7244\n",
      "Epoch 18/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3185.1687 - val_loss: 2343.6206\n",
      "Epoch 19/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3145.8845 - val_loss: 2309.8694\n",
      "Epoch 20/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3106.9690 - val_loss: 2276.4680\n",
      "Epoch 21/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3068.4185 - val_loss: 2243.4136\n",
      "Epoch 22/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3030.2310 - val_loss: 2210.7034\n",
      "Epoch 23/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2992.4023 - val_loss: 2178.3345\n",
      "Epoch 24/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2954.9304 - val_loss: 2146.3047\n",
      "Epoch 25/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2917.8130 - val_loss: 2114.6108\n",
      "Epoch 26/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2881.0471 - val_loss: 2083.2510\n",
      "Epoch 27/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2844.6301 - val_loss: 2052.2227\n",
      "Epoch 28/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2808.5598 - val_loss: 2021.5237\n",
      "Epoch 29/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2772.8342 - val_loss: 1991.1504\n",
      "Epoch 30/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2736.1992 - val_loss: 1953.3043\n",
      "Epoch 31/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 2691.4492 - val_loss: 1919.8452\n",
      "Epoch 32/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2652.4248 - val_loss: 1887.0625\n",
      "Epoch 33/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2614.3740 - val_loss: 1855.1897\n",
      "Epoch 34/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2577.2661 - val_loss: 1824.0647\n",
      "Epoch 35/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2540.9236 - val_loss: 1793.5636\n",
      "Epoch 36/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2505.2273 - val_loss: 1763.6046\n",
      "Epoch 37/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2470.0972 - val_loss: 1734.1361\n",
      "Epoch 38/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2435.4824 - val_loss: 1705.1226\n",
      "Epoch 39/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2401.3464 - val_loss: 1676.5374\n",
      "Epoch 40/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2367.6628 - val_loss: 1648.3599\n",
      "Epoch 41/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2334.4104 - val_loss: 1620.5748\n",
      "Epoch 42/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2301.5728 - val_loss: 1593.1691\n",
      "Epoch 43/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2269.1362 - val_loss: 1566.1321\n",
      "Epoch 44/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 2237.0898 - val_loss: 1539.4543\n",
      "Epoch 45/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 2205.4238 - val_loss: 1513.1283\n",
      "Epoch 46/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2174.1299 - val_loss: 1487.1467\n",
      "Epoch 47/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2143.2004 - val_loss: 1461.5033\n",
      "Epoch 48/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2112.6292 - val_loss: 1436.1924\n",
      "Epoch 49/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2082.4094 - val_loss: 1411.2084\n",
      "Epoch 50/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2052.5369 - val_loss: 1386.5470\n",
      "Epoch 51/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2023.0049 - val_loss: 1362.2032\n",
      "Epoch 52/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1993.8097 - val_loss: 1338.1733\n",
      "Epoch 53/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1964.9467 - val_loss: 1314.4532\n",
      "Epoch 54/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1936.4117 - val_loss: 1291.0392\n",
      "Epoch 55/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1908.2010 - val_loss: 1267.9275\n",
      "Epoch 56/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1880.3110 - val_loss: 1245.1145\n",
      "Epoch 57/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1852.7377 - val_loss: 1222.5977\n",
      "Epoch 58/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1825.4777 - val_loss: 1200.3730\n",
      "Epoch 59/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1798.5286 - val_loss: 1178.4377\n",
      "Epoch 60/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1771.8861 - val_loss: 1156.7894\n",
      "Epoch 61/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1745.5480 - val_loss: 1135.4242\n",
      "Epoch 62/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1719.5101 - val_loss: 1114.3403\n",
      "Epoch 63/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1693.7714 - val_loss: 1093.5344\n",
      "Epoch 64/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1668.3274 - val_loss: 1073.0037\n",
      "Epoch 65/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1643.1764 - val_loss: 1052.7460\n",
      "Epoch 66/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1618.3152 - val_loss: 1032.7585\n",
      "Epoch 67/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1593.7413 - val_loss: 1013.0386\n",
      "Epoch 68/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1569.4523 - val_loss: 993.5839\n",
      "Epoch 69/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1545.4458 - val_loss: 974.3923\n",
      "Epoch 70/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1521.7190 - val_loss: 955.4609\n",
      "Epoch 71/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1498.2695 - val_loss: 936.7875\n",
      "Epoch 72/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1475.0950 - val_loss: 918.3698\n",
      "Epoch 73/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1452.1932 - val_loss: 900.2060\n",
      "Epoch 74/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1429.5616 - val_loss: 882.2930\n",
      "Epoch 75/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1407.1984 - val_loss: 864.6290\n",
      "Epoch 76/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1385.1007 - val_loss: 847.2119\n",
      "Epoch 77/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1363.2668 - val_loss: 830.0394\n",
      "Epoch 78/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1341.6942 - val_loss: 813.1090\n",
      "Epoch 79/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1320.3809 - val_loss: 796.4191\n",
      "Epoch 80/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1299.3245 - val_loss: 779.9674\n",
      "Epoch 81/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1278.5232 - val_loss: 763.7516\n",
      "Epoch 82/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1257.9745 - val_loss: 747.7697\n",
      "Epoch 83/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 1237.6765 - val_loss: 732.0198\n",
      "Epoch 84/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1217.6272 - val_loss: 716.4998\n",
      "Epoch 85/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1197.8245 - val_loss: 701.2073\n",
      "Epoch 86/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1178.2661 - val_loss: 686.1407\n",
      "Epoch 87/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1158.9506 - val_loss: 671.2977\n",
      "Epoch 88/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1139.8759 - val_loss: 656.6768\n",
      "Epoch 89/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1121.0389 - val_loss: 642.2753\n",
      "Epoch 90/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1102.4387 - val_loss: 628.0919\n",
      "Epoch 91/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1084.0734 - val_loss: 614.1244\n",
      "Epoch 92/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1065.9407 - val_loss: 600.3708\n",
      "Epoch 93/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1048.0386 - val_loss: 586.8290\n",
      "Epoch 94/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1030.3654 - val_loss: 573.4973\n",
      "Epoch 95/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1012.9190 - val_loss: 560.3743\n",
      "Epoch 96/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 995.6981 - val_loss: 547.4568\n",
      "Epoch 97/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 978.7000 - val_loss: 534.7445\n",
      "Epoch 98/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 961.9233 - val_loss: 522.2344\n",
      "Epoch 99/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 945.3660 - val_loss: 509.9250\n",
      "Epoch 100/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 929.0264 - val_loss: 497.8145\n",
      "Epoch 101/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 912.9025 - val_loss: 485.9011\n",
      "Epoch 102/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 896.9927 - val_loss: 474.1830\n",
      "Epoch 103/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 881.2949 - val_loss: 462.6578\n",
      "Epoch 104/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 865.8071 - val_loss: 451.3243\n",
      "Epoch 105/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 850.5280 - val_loss: 440.1803\n",
      "Epoch 106/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 835.4556 - val_loss: 429.2245\n",
      "Epoch 107/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 820.5883 - val_loss: 418.4544\n",
      "Epoch 108/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 805.9238 - val_loss: 407.8690\n",
      "Epoch 109/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 791.4611 - val_loss: 397.4659\n",
      "Epoch 110/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 777.1981 - val_loss: 387.2438\n",
      "Epoch 111/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 763.1327 - val_loss: 377.2005\n",
      "Epoch 112/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 749.2637 - val_loss: 367.3347\n",
      "Epoch 113/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 735.5892 - val_loss: 357.6443\n",
      "Epoch 114/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 722.1074 - val_loss: 348.1274\n",
      "Epoch 115/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 708.8165 - val_loss: 338.7825\n",
      "Epoch 116/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 695.7147 - val_loss: 329.6085\n",
      "Epoch 117/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 682.8007 - val_loss: 320.6023\n",
      "Epoch 118/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 670.0725 - val_loss: 311.7632\n",
      "Epoch 119/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 657.5285 - val_loss: 303.0892\n",
      "Epoch 120/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 645.1671 - val_loss: 294.5785\n",
      "Epoch 121/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 632.9864 - val_loss: 286.2298\n",
      "Epoch 122/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 620.9850 - val_loss: 278.0410\n",
      "Epoch 123/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 609.1612 - val_loss: 270.0106\n",
      "Epoch 124/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 597.5134 - val_loss: 262.1369\n",
      "Epoch 125/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 586.0397 - val_loss: 254.4182\n",
      "Epoch 126/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 574.7387 - val_loss: 246.8527\n",
      "Epoch 127/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 563.6086 - val_loss: 239.4389\n",
      "Epoch 128/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 552.6479 - val_loss: 232.1750\n",
      "Epoch 129/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 541.8547 - val_loss: 225.0596\n",
      "Epoch 130/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 531.2277 - val_loss: 218.0909\n",
      "Epoch 131/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 520.7653 - val_loss: 211.2672\n",
      "Epoch 132/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 510.4657 - val_loss: 204.5868\n",
      "Epoch 133/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 500.3275 - val_loss: 198.0486\n",
      "Epoch 134/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 490.3489 - val_loss: 191.6501\n",
      "Epoch 135/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 480.5284 - val_loss: 185.3904\n",
      "Epoch 136/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 470.8643 - val_loss: 179.2675\n",
      "Epoch 137/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 461.3552 - val_loss: 173.2797\n",
      "Epoch 138/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 451.9994 - val_loss: 167.4260\n",
      "Epoch 139/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 442.7955 - val_loss: 161.7042\n",
      "Epoch 140/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 433.7419 - val_loss: 156.1130\n",
      "Epoch 141/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 424.8369 - val_loss: 150.6507\n",
      "Epoch 142/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 416.0791 - val_loss: 145.3156\n",
      "Epoch 143/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 407.4669 - val_loss: 140.1063\n",
      "Epoch 144/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 398.9985 - val_loss: 135.0213\n",
      "Epoch 145/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 390.6731 - val_loss: 130.0587\n",
      "Epoch 146/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 382.4883 - val_loss: 125.2176\n",
      "Epoch 147/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 374.4432 - val_loss: 120.4957\n",
      "Epoch 148/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 366.5358 - val_loss: 115.8919\n",
      "Epoch 149/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 358.7653 - val_loss: 111.4046\n",
      "Epoch 150/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 351.1296 - val_loss: 107.0317\n",
      "Epoch 151/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 343.6272 - val_loss: 102.7723\n",
      "Epoch 152/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 336.2567 - val_loss: 98.6249\n",
      "Epoch 153/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 329.0168 - val_loss: 94.5874\n",
      "Epoch 154/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 321.9058 - val_loss: 90.6589\n",
      "Epoch 155/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 314.9223 - val_loss: 86.8375\n",
      "Epoch 156/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 308.0649 - val_loss: 83.1219\n",
      "Epoch 157/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 301.3320 - val_loss: 79.5107\n",
      "Epoch 158/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 294.7223 - val_loss: 76.0020\n",
      "Epoch 159/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 288.2343 - val_loss: 72.5944\n",
      "Epoch 160/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 281.8662 - val_loss: 69.2867\n",
      "Epoch 161/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 275.6170 - val_loss: 66.0772\n",
      "Epoch 162/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 269.4851 - val_loss: 62.9646\n",
      "Epoch 163/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 263.4691 - val_loss: 59.9471\n",
      "Epoch 164/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 257.5674 - val_loss: 57.0235\n",
      "Epoch 165/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 251.7787 - val_loss: 54.1924\n",
      "Epoch 166/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 246.1017 - val_loss: 51.4520\n",
      "Epoch 167/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 240.5347 - val_loss: 48.8014\n",
      "Epoch 168/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 235.0768 - val_loss: 46.2386\n",
      "Epoch 169/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 229.7261 - val_loss: 43.7627\n",
      "Epoch 170/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 224.4814 - val_loss: 41.3718\n",
      "Epoch 171/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 219.3413 - val_loss: 39.0645\n",
      "Epoch 172/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 214.3043 - val_loss: 36.8398\n",
      "Epoch 173/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 209.3692 - val_loss: 34.6961\n",
      "Epoch 174/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 204.5347 - val_loss: 32.6317\n",
      "Epoch 175/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 199.7991 - val_loss: 30.6456\n",
      "Epoch 176/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 195.1615 - val_loss: 28.7362\n",
      "Epoch 177/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 190.6200 - val_loss: 26.9022\n",
      "Epoch 178/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 186.1736 - val_loss: 25.1424\n",
      "Epoch 179/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 181.8212 - val_loss: 23.4551\n",
      "Epoch 180/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 177.5610 - val_loss: 21.8391\n",
      "Epoch 181/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 173.3919 - val_loss: 20.2932\n",
      "Epoch 182/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 169.3125 - val_loss: 18.8158\n",
      "Epoch 183/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 165.3213 - val_loss: 17.4056\n",
      "Epoch 184/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 161.4175 - val_loss: 16.0615\n",
      "Epoch 185/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 157.5995 - val_loss: 14.7820\n",
      "Epoch 186/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 153.8660 - val_loss: 13.5658\n",
      "Epoch 187/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 150.2159 - val_loss: 12.4117\n",
      "Epoch 188/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 146.6477 - val_loss: 11.3183\n",
      "Epoch 189/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 143.1604 - val_loss: 10.2845\n",
      "Epoch 190/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 139.7525 - val_loss: 9.3087\n",
      "Epoch 191/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 136.4228 - val_loss: 8.3898\n",
      "Epoch 192/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 133.1699 - val_loss: 7.5267\n",
      "Epoch 193/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 129.9930 - val_loss: 6.7180\n",
      "Epoch 194/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 126.8904 - val_loss: 5.9624\n",
      "Epoch 195/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 123.8612 - val_loss: 5.2588\n",
      "Epoch 196/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 120.9040 - val_loss: 4.6059\n",
      "Epoch 197/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 118.0177 - val_loss: 4.0025\n",
      "Epoch 198/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 115.2011 - val_loss: 3.4475\n",
      "Epoch 199/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 112.4530 - val_loss: 2.9396\n",
      "Epoch 200/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 109.7722 - val_loss: 2.4776\n",
      "Epoch 201/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 107.1574 - val_loss: 2.0605\n",
      "Epoch 202/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 104.6077 - val_loss: 1.6870\n",
      "Epoch 203/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 102.1220 - val_loss: 1.3559\n",
      "Epoch 204/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 99.6988 - val_loss: 1.0662\n",
      "Epoch 205/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 97.3373 - val_loss: 0.8168\n",
      "Epoch 206/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 95.0362 - val_loss: 0.6064\n",
      "Epoch 207/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 92.7945 - val_loss: 0.4341\n",
      "Epoch 208/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 90.6111 - val_loss: 0.2987\n",
      "Epoch 209/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 88.4848 - val_loss: 0.1991\n",
      "Epoch 210/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 86.4145 - val_loss: 0.1343\n",
      "Epoch 211/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 84.3993 - val_loss: 0.1032\n",
      "Epoch 212/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 82.4379 - val_loss: 0.1048\n",
      "Epoch 213/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 80.5292 - val_loss: 0.1380\n",
      "Epoch 214/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 78.6725 - val_loss: 0.2018\n",
      "Epoch 215/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 76.8665 - val_loss: 0.2952\n",
      "Epoch 216/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 75.1105 - val_loss: 0.4173\n",
      "Epoch 217/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 73.4032 - val_loss: 0.5669\n",
      "Epoch 218/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 71.7436 - val_loss: 0.7433\n",
      "Epoch 219/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 70.1307 - val_loss: 0.9453\n",
      "Epoch 220/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 68.5636 - val_loss: 1.1721\n",
      "Epoch 221/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 67.0413 - val_loss: 1.4227\n",
      "Epoch 222/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 65.5628 - val_loss: 1.6962\n",
      "Epoch 223/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 64.1273 - val_loss: 1.9917\n",
      "Epoch 224/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 62.7338 - val_loss: 2.3083\n",
      "Epoch 225/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 61.3812 - val_loss: 2.6452\n",
      "Epoch 226/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 60.0689 - val_loss: 3.0014\n",
      "Epoch 227/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 58.7957 - val_loss: 3.3761\n",
      "Epoch 228/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 57.5608 - val_loss: 3.7685\n",
      "Epoch 229/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 56.3635 - val_loss: 4.1778\n",
      "Epoch 230/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 55.2026 - val_loss: 4.6031\n",
      "Epoch 231/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 54.0775 - val_loss: 5.0437\n",
      "Epoch 232/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 52.9872 - val_loss: 5.4987\n",
      "Epoch 233/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 51.9310 - val_loss: 5.9675\n",
      "Epoch 234/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 50.9081 - val_loss: 6.4492\n",
      "Epoch 235/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 49.9175 - val_loss: 6.9431\n",
      "Epoch 236/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 48.9586 - val_loss: 7.4485\n",
      "Epoch 237/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 48.0305 - val_loss: 7.9647\n",
      "Epoch 238/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 47.1324 - val_loss: 8.4910\n",
      "Epoch 239/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 46.2637 - val_loss: 9.0266\n",
      "Epoch 240/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 45.4234 - val_loss: 9.5711\n",
      "Epoch 241/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 44.6109 - val_loss: 10.1237\n",
      "Epoch 242/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 43.8254 - val_loss: 10.6837\n",
      "Epoch 243/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 43.0663 - val_loss: 11.2505\n",
      "Epoch 244/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 42.3329 - val_loss: 11.8235\n",
      "Epoch 245/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 41.6246 - val_loss: 12.4021\n",
      "Epoch 246/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 40.9405 - val_loss: 12.9857\n",
      "Epoch 247/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 40.2800 - val_loss: 13.5738\n",
      "Epoch 248/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 39.6425 - val_loss: 14.1659\n",
      "Epoch 249/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 39.0273 - val_loss: 14.7612\n",
      "Epoch 250/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 38.4339 - val_loss: 15.3594\n",
      "Epoch 251/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 37.8614 - val_loss: 15.9602\n",
      "Epoch 252/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 37.3094 - val_loss: 16.5628\n",
      "Epoch 253/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 36.7773 - val_loss: 17.1666\n",
      "Epoch 254/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 36.2645 - val_loss: 17.7716\n",
      "Epoch 255/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 35.7703 - val_loss: 18.3769\n",
      "Epoch 256/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 35.2943 - val_loss: 18.9824\n",
      "Epoch 257/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 34.8359 - val_loss: 19.5874\n",
      "Epoch 258/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 34.3945 - val_loss: 20.1915\n",
      "Epoch 259/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 33.9697 - val_loss: 20.7947\n",
      "Epoch 260/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 33.5609 - val_loss: 21.3961\n",
      "Epoch 261/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 33.1677 - val_loss: 21.9957\n",
      "Epoch 262/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 32.7895 - val_loss: 22.5929\n",
      "Epoch 263/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 32.4257 - val_loss: 23.1876\n",
      "Epoch 264/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 32.0761 - val_loss: 23.7792\n",
      "Epoch 265/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 31.7401 - val_loss: 24.3676\n",
      "Epoch 266/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 31.4173 - val_loss: 24.9525\n",
      "Epoch 267/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 31.1072 - val_loss: 25.5333\n",
      "Epoch 268/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 30.8095 - val_loss: 26.1102\n",
      "Epoch 269/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 30.5236 - val_loss: 26.6827\n",
      "Epoch 270/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 30.2492 - val_loss: 27.2505\n",
      "Epoch 271/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 29.9858 - val_loss: 27.8133\n",
      "Epoch 272/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 29.7332 - val_loss: 28.3711\n",
      "Epoch 273/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 29.4910 - val_loss: 28.9236\n",
      "Epoch 274/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 29.2587 - val_loss: 29.4706\n",
      "Epoch 275/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 29.0360 - val_loss: 30.0117\n",
      "Epoch 276/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 28.8225 - val_loss: 30.5468\n",
      "Epoch 277/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 28.6181 - val_loss: 31.0760\n",
      "Epoch 278/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 28.4223 - val_loss: 31.5988\n",
      "Epoch 279/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 28.2347 - val_loss: 32.1154\n",
      "Epoch 280/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 28.0550 - val_loss: 32.6254\n",
      "Epoch 281/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 27.8831 - val_loss: 33.1287\n",
      "Epoch 282/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 27.7186 - val_loss: 33.6252\n",
      "Epoch 283/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 27.5612 - val_loss: 34.1148\n",
      "Epoch 284/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 27.4107 - val_loss: 34.5976\n",
      "Epoch 285/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 27.2667 - val_loss: 35.0733\n",
      "Epoch 286/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 27.1290 - val_loss: 35.5416\n",
      "Epoch 287/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 26.9974 - val_loss: 36.0028\n",
      "Epoch 288/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 26.8716 - val_loss: 36.4568\n",
      "Epoch 289/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 26.7515 - val_loss: 36.9032\n",
      "Epoch 290/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 26.6367 - val_loss: 37.3425\n",
      "Epoch 291/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 26.5271 - val_loss: 37.7743\n",
      "Epoch 292/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 26.4224 - val_loss: 38.1983\n",
      "Epoch 293/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 26.3225 - val_loss: 38.6150\n",
      "Epoch 294/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 26.2272 - val_loss: 39.0243\n",
      "Epoch 295/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 26.1361 - val_loss: 39.4261\n",
      "Epoch 296/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 26.0493 - val_loss: 39.8204\n",
      "Epoch 297/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 25.9664 - val_loss: 40.2069\n",
      "Epoch 298/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.8875 - val_loss: 40.5860\n",
      "Epoch 299/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 25.8121 - val_loss: 40.9576\n",
      "Epoch 300/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.7403 - val_loss: 41.3219\n",
      "Epoch 301/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.6719 - val_loss: 41.6786\n",
      "Epoch 302/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.6066 - val_loss: 42.0280\n",
      "Epoch 303/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.5444 - val_loss: 42.3697\n",
      "Epoch 304/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.4852 - val_loss: 42.7043\n",
      "Epoch 305/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.4288 - val_loss: 43.0314\n",
      "Epoch 306/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.3751 - val_loss: 43.3516\n",
      "Epoch 307/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.3239 - val_loss: 43.6644\n",
      "Epoch 308/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 25.2752 - val_loss: 43.9697\n",
      "Epoch 309/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2289 - val_loss: 44.2684\n",
      "Epoch 310/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 25.1847 - val_loss: 44.5600\n",
      "Epoch 311/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 25.1427 - val_loss: 44.8446\n",
      "Epoch 312/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.1027 - val_loss: 45.1226\n",
      "Epoch 313/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 25.0646 - val_loss: 45.3935\n",
      "Epoch 314/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 25.0284 - val_loss: 45.6579\n",
      "Epoch 315/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.9940 - val_loss: 45.9156\n",
      "Epoch 316/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.9612 - val_loss: 46.1668\n",
      "Epoch 317/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.9300 - val_loss: 46.4115\n",
      "Epoch 318/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.9004 - val_loss: 46.6500\n",
      "Epoch 319/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.8722 - val_loss: 46.8818\n",
      "Epoch 320/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.8454 - val_loss: 47.1080\n",
      "Epoch 321/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.8199 - val_loss: 47.3281\n",
      "Epoch 322/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 24.7956 - val_loss: 47.5418\n",
      "Epoch 323/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.7726 - val_loss: 47.7498\n",
      "Epoch 324/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 24.7507 - val_loss: 47.9522\n",
      "Epoch 325/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.7298 - val_loss: 48.1489\n",
      "Epoch 326/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.7100 - val_loss: 48.3398\n",
      "Epoch 327/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.6912 - val_loss: 48.5253\n",
      "Epoch 328/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.6733 - val_loss: 48.7054\n",
      "Epoch 329/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.6563 - val_loss: 48.8802\n",
      "Epoch 330/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 24.6401 - val_loss: 49.0500\n",
      "Epoch 331/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.6247 - val_loss: 49.2145\n",
      "Epoch 332/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.6102 - val_loss: 49.3741\n",
      "Epoch 333/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.5963 - val_loss: 49.5290\n",
      "Epoch 334/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.5831 - val_loss: 49.6792\n",
      "Epoch 335/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 24.5705 - val_loss: 49.8243\n",
      "Epoch 336/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 24.5586 - val_loss: 49.9651\n",
      "Epoch 337/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.5473 - val_loss: 50.1014\n",
      "Epoch 338/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.5365 - val_loss: 50.2334\n",
      "Epoch 339/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.5263 - val_loss: 50.3613\n",
      "Epoch 340/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.5166 - val_loss: 50.4850\n",
      "Epoch 341/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.5073 - val_loss: 50.6045\n",
      "Epoch 342/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 24.4985 - val_loss: 50.7198\n",
      "Epoch 343/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.4902 - val_loss: 50.8317\n",
      "Epoch 344/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.4822 - val_loss: 50.9397\n",
      "Epoch 345/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 24.4747 - val_loss: 51.0438\n",
      "Epoch 346/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.4676 - val_loss: 51.1447\n",
      "Epoch 347/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 24.4607 - val_loss: 51.2420\n",
      "Epoch 348/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 24.4543 - val_loss: 51.3356\n",
      "Epoch 349/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.4482 - val_loss: 51.4262\n",
      "Epoch 350/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.4423 - val_loss: 51.5135\n",
      "Epoch 351/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 24.4368 - val_loss: 51.5976\n",
      "Epoch 352/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.4315 - val_loss: 51.6787\n",
      "Epoch 353/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 24.4265 - val_loss: 51.7570\n",
      "Epoch 354/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.4218 - val_loss: 51.8325\n",
      "Epoch 355/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.4173 - val_loss: 51.9051\n",
      "Epoch 356/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.4130 - val_loss: 51.9750\n",
      "Epoch 357/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 24.4089 - val_loss: 52.0423\n",
      "Epoch 358/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.4051 - val_loss: 52.1071\n",
      "Epoch 359/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.4014 - val_loss: 52.1693\n",
      "Epoch 360/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3979 - val_loss: 52.2292\n",
      "Epoch 361/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3946 - val_loss: 52.2868\n",
      "Epoch 362/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 24.3915 - val_loss: 52.3422\n",
      "Epoch 363/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3885 - val_loss: 52.3956\n",
      "Epoch 364/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3857 - val_loss: 52.4466\n",
      "Epoch 365/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3830 - val_loss: 52.4957\n",
      "Epoch 366/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 24.3805 - val_loss: 52.5429\n",
      "Epoch 367/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 24.3781 - val_loss: 52.5881\n",
      "Epoch 368/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 24.3758 - val_loss: 52.6314\n",
      "Epoch 369/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3736 - val_loss: 52.6730\n",
      "Epoch 370/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 24.3716 - val_loss: 52.7129\n",
      "Epoch 371/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3697 - val_loss: 52.7510\n",
      "Epoch 372/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 24.3678 - val_loss: 52.7876\n",
      "Epoch 373/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 24.3662 - val_loss: 52.8228\n",
      "Epoch 374/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 24.3645 - val_loss: 52.8563\n",
      "Epoch 375/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3630 - val_loss: 52.8884\n",
      "Epoch 376/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3615 - val_loss: 52.9193\n",
      "Epoch 377/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3601 - val_loss: 52.9488\n",
      "Epoch 378/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3588 - val_loss: 52.9768\n",
      "Epoch 379/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3576 - val_loss: 53.0036\n",
      "Epoch 380/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3565 - val_loss: 53.0295\n",
      "Epoch 381/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3554 - val_loss: 53.0543\n",
      "Epoch 382/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3544 - val_loss: 53.0775\n",
      "Epoch 383/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3534 - val_loss: 53.0999\n",
      "Epoch 384/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3526 - val_loss: 53.1216\n",
      "Epoch 385/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3517 - val_loss: 53.1421\n",
      "Epoch 386/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 24.3509 - val_loss: 53.1616\n",
      "Epoch 387/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3502 - val_loss: 53.1802\n",
      "Epoch 388/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3495 - val_loss: 53.1977\n",
      "Epoch 389/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3488 - val_loss: 53.2146\n",
      "Epoch 390/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 24.3483 - val_loss: 53.2308\n",
      "Epoch 391/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 24.3477 - val_loss: 53.2461\n",
      "Epoch 392/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3472 - val_loss: 53.2608\n",
      "Epoch 393/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3467 - val_loss: 53.2748\n",
      "Epoch 394/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3463 - val_loss: 53.2882\n",
      "Epoch 395/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 24.3459 - val_loss: 53.3009\n",
      "Epoch 396/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3455 - val_loss: 53.3128\n",
      "Epoch 397/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3451 - val_loss: 53.3240\n",
      "Epoch 398/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 24.3448 - val_loss: 53.3350\n",
      "Epoch 399/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3445 - val_loss: 53.3453\n",
      "Epoch 400/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3443 - val_loss: 53.3550\n",
      "Epoch 401/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3441 - val_loss: 53.3642\n",
      "Epoch 402/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 24.3438 - val_loss: 53.3730\n",
      "Epoch 403/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3436 - val_loss: 53.3814\n",
      "Epoch 404/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3435 - val_loss: 53.3892\n",
      "Epoch 405/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3434 - val_loss: 53.3966\n",
      "Epoch 406/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3433 - val_loss: 53.4039\n",
      "Epoch 407/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3431 - val_loss: 53.4106\n",
      "Epoch 408/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 24.3431 - val_loss: 53.4168\n",
      "Epoch 409/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3431 - val_loss: 53.4230\n",
      "Epoch 410/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3430 - val_loss: 53.4289\n",
      "Epoch 411/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3429 - val_loss: 53.4343\n",
      "Epoch 412/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 24.3429 - val_loss: 53.4393\n",
      "Epoch 413/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3429 - val_loss: 53.4440\n",
      "Epoch 414/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3429 - val_loss: 53.4485\n",
      "Epoch 415/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3429 - val_loss: 53.4528\n",
      "Epoch 416/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3429 - val_loss: 53.4569\n",
      "Epoch 417/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3430 - val_loss: 53.4606\n",
      "Epoch 418/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 24.3431 - val_loss: 53.4639\n",
      "Epoch 419/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 24.3431 - val_loss: 53.4673\n",
      "Epoch 420/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 24.3432 - val_loss: 53.4703\n",
      "Epoch 421/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3433 - val_loss: 53.4731\n",
      "Epoch 422/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3434 - val_loss: 53.4761\n",
      "Epoch 423/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3435 - val_loss: 53.4788\n",
      "Epoch 424/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 24.3436 - val_loss: 53.4812\n",
      "Epoch 425/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3436 - val_loss: 53.4833\n",
      "Epoch 426/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3438 - val_loss: 53.4854\n",
      "Epoch 427/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3439 - val_loss: 53.4874\n",
      "Epoch 428/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3440 - val_loss: 53.4895\n",
      "Epoch 429/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 24.3442 - val_loss: 53.4911\n",
      "Epoch 430/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3443 - val_loss: 53.4929\n",
      "Epoch 431/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3445 - val_loss: 53.4944\n",
      "Epoch 432/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3446 - val_loss: 53.4956\n",
      "Epoch 433/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3448 - val_loss: 53.4969\n",
      "Epoch 434/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 24.3449 - val_loss: 53.4982\n",
      "Epoch 435/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3451 - val_loss: 53.4992\n",
      "Epoch 436/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3453 - val_loss: 53.5004\n",
      "Epoch 437/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3454 - val_loss: 53.5012\n",
      "Epoch 438/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3456 - val_loss: 53.5021\n",
      "Epoch 439/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3458 - val_loss: 53.5029\n",
      "Epoch 440/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 24.3459 - val_loss: 53.5037\n",
      "Epoch 441/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3461 - val_loss: 53.5043\n",
      "Epoch 442/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3463 - val_loss: 53.5050\n",
      "Epoch 443/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3465 - val_loss: 53.5056\n",
      "Epoch 444/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3467 - val_loss: 53.5062\n",
      "Epoch 445/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3468 - val_loss: 53.5065\n",
      "Epoch 446/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3470 - val_loss: 53.5070\n",
      "Epoch 447/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3472 - val_loss: 53.5073\n",
      "Epoch 448/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3474 - val_loss: 53.5075\n",
      "Epoch 449/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3475 - val_loss: 53.5077\n",
      "Epoch 450/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3478 - val_loss: 53.5081\n",
      "Epoch 451/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3479 - val_loss: 53.5084\n",
      "Epoch 452/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 24.3481 - val_loss: 53.5085\n",
      "Epoch 453/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3483 - val_loss: 53.5086\n",
      "Epoch 454/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3485 - val_loss: 53.5086\n",
      "Epoch 455/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3487 - val_loss: 53.5089\n",
      "Epoch 456/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3489 - val_loss: 53.5089\n",
      "Epoch 457/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 24.3491 - val_loss: 53.5091\n",
      "Epoch 458/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 24.3492 - val_loss: 53.5089\n",
      "Epoch 459/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3494 - val_loss: 53.5090\n",
      "Epoch 460/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3496 - val_loss: 53.5089\n",
      "Epoch 461/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3498 - val_loss: 53.5089\n",
      "Epoch 462/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3500 - val_loss: 53.5089\n",
      "Epoch 463/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3502 - val_loss: 53.5089\n",
      "Epoch 464/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3503 - val_loss: 53.5086\n",
      "Epoch 465/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3505 - val_loss: 53.5085\n",
      "Epoch 466/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3507 - val_loss: 53.5084\n",
      "Epoch 467/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 24.3509 - val_loss: 53.5083\n",
      "Epoch 468/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3511 - val_loss: 53.5081\n",
      "Epoch 469/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3512 - val_loss: 53.5080\n",
      "Epoch 470/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3514 - val_loss: 53.5077\n",
      "Epoch 471/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3516 - val_loss: 53.5076\n",
      "Epoch 472/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3518 - val_loss: 53.5073\n",
      "Epoch 473/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3520 - val_loss: 53.5072\n",
      "Epoch 474/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3521 - val_loss: 53.5070\n",
      "Epoch 475/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3523 - val_loss: 53.5068\n",
      "Epoch 476/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3525 - val_loss: 53.5067\n",
      "Epoch 477/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 24.3527 - val_loss: 53.5066\n",
      "Epoch 478/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3528 - val_loss: 53.5065\n",
      "Epoch 479/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3530 - val_loss: 53.5063\n",
      "Epoch 480/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3531 - val_loss: 53.5061\n",
      "Epoch 481/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3533 - val_loss: 53.5059\n",
      "Epoch 482/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3535 - val_loss: 53.5056\n",
      "Epoch 483/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3536 - val_loss: 53.5053\n",
      "Epoch 484/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3538 - val_loss: 53.5052\n",
      "Epoch 485/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3539 - val_loss: 53.5050\n",
      "Epoch 486/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3541 - val_loss: 53.5046\n",
      "Epoch 487/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3543 - val_loss: 53.5044\n",
      "Epoch 488/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3544 - val_loss: 53.5042\n",
      "Epoch 489/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3546 - val_loss: 53.5041\n",
      "Epoch 490/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3547 - val_loss: 53.5037\n",
      "Epoch 491/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3549 - val_loss: 53.5035\n",
      "Epoch 492/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3550 - val_loss: 53.5032\n",
      "Epoch 493/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3552 - val_loss: 53.5029\n",
      "Epoch 494/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3553 - val_loss: 53.5027\n",
      "Epoch 495/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 24.3555 - val_loss: 53.5026\n",
      "Epoch 496/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3556 - val_loss: 53.5024\n",
      "Epoch 497/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3557 - val_loss: 53.5020\n",
      "Epoch 498/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3559 - val_loss: 53.5015\n",
      "Epoch 499/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3561 - val_loss: 53.5014\n",
      "Epoch 500/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.3562 - val_loss: 53.5013\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(82.7524, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0031, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0003, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0171, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(3.0230, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 321ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.77381653e+01, 5.77241597e+01, 5.77101541e+01, 5.76961485e+01,\n",
       "        5.76821429e+01, 5.76681373e+01, 5.76541316e+01, 5.76401260e+01,\n",
       "        5.76261205e+01, 5.76121148e+01, 5.75981092e+01, 5.75841036e+01,\n",
       "        5.75700980e+01, 5.75560924e+01, 5.75420868e+01, 5.75280812e+01,\n",
       "        5.75140756e+01, 5.75000700e+01, 5.74860644e+01, 5.74720588e+01,\n",
       "        5.74580532e+01, 5.74440476e+01, 5.74300420e+01, 5.74160364e+01,\n",
       "        5.74020308e+01, 5.73880252e+01, 5.73740196e+01, 5.73600140e+01,\n",
       "        5.73460084e+01, 5.73320028e+01, 5.73179972e+01, 5.73039916e+01,\n",
       "        5.72899860e+01, 5.72759804e+01, 5.72619748e+01, 5.72479692e+01,\n",
       "        5.72339636e+01, 5.72199580e+01, 5.72059524e+01, 5.71919468e+01,\n",
       "        5.71779412e+01, 5.71639356e+01, 5.71499300e+01, 5.71359244e+01,\n",
       "        5.71219188e+01, 5.71079132e+01, 5.70939076e+01, 5.70799020e+01,\n",
       "        5.70658964e+01, 5.70518908e+01, 5.70378852e+01, 5.70238795e+01,\n",
       "        5.70098739e+01, 5.69958684e+01, 5.69818627e+01, 5.69678571e+01,\n",
       "        5.69538515e+01, 5.69398459e+01, 5.69258403e+01, 5.69118347e+01,\n",
       "        5.68978291e+01, 5.68838235e+01, 5.68698179e+01, 5.68558123e+01,\n",
       "        5.68418067e+01, 5.68278011e+01, 5.68137955e+01, 5.67997899e+01,\n",
       "        5.67857843e+01, 5.67717787e+01, 5.67577731e+01, 5.67437675e+01,\n",
       "        5.67297619e+01, 5.67157563e+01, 5.67017507e+01, 5.66730392e+01,\n",
       "        5.66422269e+01, 5.66114146e+01, 5.65806022e+01, 5.65497899e+01,\n",
       "        6.44609833e+01, 1.04408872e+00, 0.00000000e+00, 2.13815957e-01,\n",
       "        4.30057615e-01, 0.00000000e+00, 1.67232350e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.92497358e-01, 1.88607156e-01, 4.06541169e-01,\n",
       "        1.30032137e-01, 0.00000000e+00, 3.79659384e-02, 4.66377378e-01,\n",
       "        0.00000000e+00, 1.02064446e-01, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([54.88641457, 54.88081232, 54.87521008, 54.86960784, 54.8640056 ,\n",
       "       54.85840336, 54.85280112, 54.84719888, 54.84159664, 54.8359944 ,\n",
       "       54.83039216, 54.82478992, 54.81918768, 54.81358543, 54.80798319,\n",
       "       54.80238095, 54.79677871, 54.79117647, 54.78557423, 54.77997199,\n",
       "       54.77436975, 54.76876751, 54.76316527, 54.75756303, 54.75196078,\n",
       "       54.74635854, 54.7407563 , 54.73515406, 54.72955182, 54.72394958,\n",
       "       54.71834734, 54.7127451 , 54.70714286, 54.70154062, 54.69593838,\n",
       "       54.69033613, 54.68473389, 54.67913165, 54.67352941, 54.66792717,\n",
       "       54.66232493, 54.65672269, 54.65112045, 54.64551821, 54.63991597,\n",
       "       54.63431373, 54.62871148, 54.62310924, 54.617507  , 54.61190476,\n",
       "       54.60630252, 54.60070028, 54.59509804, 54.5894958 , 54.58389356,\n",
       "       54.57829132, 54.57268908, 54.56708683, 54.56148459, 54.55588235,\n",
       "       54.55028011, 54.54467787, 54.53907563, 54.53347339, 54.52787115,\n",
       "       54.52226891, 54.51666667, 54.51106443, 54.50546218, 54.49985994,\n",
       "       54.4942577 , 54.48865546, 54.48305322, 54.47745098, 54.47184874,\n",
       "       54.4662465 , 54.46064426, 54.45504202, 54.44943978, 54.44383754,\n",
       "       54.43823529, 54.43263305, 54.42703081, 54.42142857, 54.41582633,\n",
       "       54.41022409, 54.40462185, 54.39901961, 54.39341737, 54.38781513,\n",
       "       54.38221289, 54.37661064, 54.3710084 , 54.36540616, 54.35980392,\n",
       "       54.35420168, 54.34859944, 54.3429972 , 54.33739496, 54.33179272])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.756187219104973\n",
      "12.412493501119245\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
