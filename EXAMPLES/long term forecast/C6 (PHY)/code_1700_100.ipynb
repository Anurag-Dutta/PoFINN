{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       84.600000\n",
       "1       84.431933\n",
       "2       84.263866\n",
       "3       84.095798\n",
       "4       83.927731\n",
       "          ...    \n",
       "1795    53.540966\n",
       "1796    53.532563\n",
       "1797    53.524160\n",
       "1798    53.515756\n",
       "1799    53.507353\n",
       "Name: C6, Length: 1800, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c6_interpolated_1700_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       84.600000\n",
       "1       84.431933\n",
       "2       84.263866\n",
       "3       84.095798\n",
       "4       83.927731\n",
       "          ...    \n",
       "1695     0.130032\n",
       "1696     0.000000\n",
       "1697     0.037966\n",
       "1698     0.466377\n",
       "1699     0.000000\n",
       "Name: C6, Length: 1700, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1700)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.431933</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84.263866</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84.095798</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83.927731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     84.600000  0.000298\n",
       "1     84.431933  0.000298\n",
       "2     84.263866  0.000297\n",
       "3     84.095798  0.000297\n",
       "4     83.927731  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 82.7524  # Value for C0\n",
    "K0 = -0.0031  # Value for K0\n",
    "K1 = -0.0003  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0171    # Value for b\n",
    "c = 3.0230    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    84.600000    84.431933    84.263866    84.095798    83.927731   \n",
      "351    84.431933    84.263866    84.095798    83.927731    83.759664   \n",
      "352    84.263866    84.095798    83.927731    83.759664    83.591597   \n",
      "353    84.095798    83.927731    83.759664    83.591597    83.423529   \n",
      "354    83.927731    83.759664    83.591597    83.423529    83.255462   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    83.759664    83.591597    83.423529    83.255462    83.092437  ...   \n",
      "351    83.591597    83.423529    83.255462    83.092437    82.991597  ...   \n",
      "352    83.423529    83.255462    83.092437    82.991597    82.890756  ...   \n",
      "353    83.255462    83.092437    82.991597    82.890756    82.789916  ...   \n",
      "354    83.092437    82.991597    82.890756    82.789916    82.689076  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   70.003688    0.000263   69.960738    0.000263   69.917787    0.000263   \n",
      "351   69.960738    0.000263   69.917787    0.000263   69.874837    0.000262   \n",
      "352   69.917787    0.000263   69.874837    0.000262   69.831886    0.000262   \n",
      "353   69.874837    0.000262   69.831886    0.000262   69.788936    0.000262   \n",
      "354   69.831886    0.000262   69.788936    0.000262   69.745985    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   69.874837    0.000262   69.831886    0.000262  \n",
      "351   69.831886    0.000262   69.788936    0.000262  \n",
      "352   69.788936    0.000262   69.745985    0.000262  \n",
      "353   69.745985    0.000262   69.703035    0.000262  \n",
      "354   69.703035    0.000262   69.660084    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1250, 1, 251) (1250, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "16/16 [==============================] - 3s 33ms/step - loss: 3981.7085 - val_loss: 3011.4258\n",
      "Epoch 2/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3907.1069 - val_loss: 2967.9333\n",
      "Epoch 3/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3857.0535 - val_loss: 2925.3494\n",
      "Epoch 4/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3797.7004 - val_loss: 2868.8501\n",
      "Epoch 5/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3743.3132 - val_loss: 2825.0464\n",
      "Epoch 6/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3693.6160 - val_loss: 2781.9570\n",
      "Epoch 7/500\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 3644.6753 - val_loss: 2739.5330\n",
      "Epoch 8/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3596.4106 - val_loss: 2697.7029\n",
      "Epoch 9/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3548.7539 - val_loss: 2656.4185\n",
      "Epoch 10/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3501.6594 - val_loss: 2615.6489\n",
      "Epoch 11/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3454.0464 - val_loss: 2570.3796\n",
      "Epoch 12/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3401.7434 - val_loss: 2527.4314\n",
      "Epoch 13/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3352.6602 - val_loss: 2485.2820\n",
      "Epoch 14/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3304.5759 - val_loss: 2444.0454\n",
      "Epoch 15/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3257.4199 - val_loss: 2403.5828\n",
      "Epoch 16/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3211.0488 - val_loss: 2363.7913\n",
      "Epoch 17/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3165.3677 - val_loss: 2324.6089\n",
      "Epoch 18/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3120.3162 - val_loss: 2285.9929\n",
      "Epoch 19/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3075.8542 - val_loss: 2247.9150\n",
      "Epoch 20/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3031.9529 - val_loss: 2210.3525\n",
      "Epoch 21/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2988.5911 - val_loss: 2173.2893\n",
      "Epoch 22/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2945.7507 - val_loss: 2136.7119\n",
      "Epoch 23/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2903.4189 - val_loss: 2100.6089\n",
      "Epoch 24/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2861.5840 - val_loss: 2064.9707\n",
      "Epoch 25/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2820.2356 - val_loss: 2029.7887\n",
      "Epoch 26/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2779.3655 - val_loss: 1995.0559\n",
      "Epoch 27/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2738.9663 - val_loss: 1960.7649\n",
      "Epoch 28/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2699.0300 - val_loss: 1926.9099\n",
      "Epoch 29/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2659.5510 - val_loss: 1893.4850\n",
      "Epoch 30/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2620.5229 - val_loss: 1860.4843\n",
      "Epoch 31/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2581.9407 - val_loss: 1827.9038\n",
      "Epoch 32/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2543.7991 - val_loss: 1795.7383\n",
      "Epoch 33/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2506.0923 - val_loss: 1763.9825\n",
      "Epoch 34/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2468.8164 - val_loss: 1732.6333\n",
      "Epoch 35/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2431.9670 - val_loss: 1701.6853\n",
      "Epoch 36/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2395.5396 - val_loss: 1671.1349\n",
      "Epoch 37/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2359.5298 - val_loss: 1640.9783\n",
      "Epoch 38/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2323.9338 - val_loss: 1611.2118\n",
      "Epoch 39/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2288.7478 - val_loss: 1581.8314\n",
      "Epoch 40/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2253.9675 - val_loss: 1552.8326\n",
      "Epoch 41/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2219.5898 - val_loss: 1524.2135\n",
      "Epoch 42/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2185.6108 - val_loss: 1495.9690\n",
      "Epoch 43/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2152.0264 - val_loss: 1468.0967\n",
      "Epoch 44/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2118.8340 - val_loss: 1440.5928\n",
      "Epoch 45/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2086.0295 - val_loss: 1413.4539\n",
      "Epoch 46/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2053.6101 - val_loss: 1386.6768\n",
      "Epoch 47/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2021.5718 - val_loss: 1360.2585\n",
      "Epoch 48/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1989.9120 - val_loss: 1334.1954\n",
      "Epoch 49/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1958.6266 - val_loss: 1308.4843\n",
      "Epoch 50/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1927.7137 - val_loss: 1283.1230\n",
      "Epoch 51/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1897.1687 - val_loss: 1258.1071\n",
      "Epoch 52/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1866.9896 - val_loss: 1233.4342\n",
      "Epoch 53/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1837.1730 - val_loss: 1209.1012\n",
      "Epoch 54/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1807.7161 - val_loss: 1185.1055\n",
      "Epoch 55/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1778.6149 - val_loss: 1161.4432\n",
      "Epoch 56/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1749.8674 - val_loss: 1138.1127\n",
      "Epoch 57/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1721.4705 - val_loss: 1115.1095\n",
      "Epoch 58/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1693.4209 - val_loss: 1092.4320\n",
      "Epoch 59/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1665.7163 - val_loss: 1070.0768\n",
      "Epoch 60/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1638.3535 - val_loss: 1048.0410\n",
      "Epoch 61/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1611.3292 - val_loss: 1026.3220\n",
      "Epoch 62/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1584.6417 - val_loss: 1004.9169\n",
      "Epoch 63/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1558.2869 - val_loss: 983.8232\n",
      "Epoch 64/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1532.2629 - val_loss: 963.0376\n",
      "Epoch 65/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1506.5669 - val_loss: 942.5577\n",
      "Epoch 66/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1481.1960 - val_loss: 922.3807\n",
      "Epoch 67/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1456.1473 - val_loss: 902.5040\n",
      "Epoch 68/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1431.4181 - val_loss: 882.9247\n",
      "Epoch 69/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1407.0061 - val_loss: 863.6406\n",
      "Epoch 70/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1382.9083 - val_loss: 844.6483\n",
      "Epoch 71/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1359.1222 - val_loss: 825.9455\n",
      "Epoch 72/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1335.6453 - val_loss: 807.5299\n",
      "Epoch 73/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1312.4745 - val_loss: 789.3986\n",
      "Epoch 74/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1289.6080 - val_loss: 771.5488\n",
      "Epoch 75/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1267.0422 - val_loss: 753.9783\n",
      "Epoch 76/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1244.7756 - val_loss: 736.6843\n",
      "Epoch 77/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1222.8049 - val_loss: 719.6645\n",
      "Epoch 78/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1201.1282 - val_loss: 702.9160\n",
      "Epoch 79/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1179.7421 - val_loss: 686.4368\n",
      "Epoch 80/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1158.6448 - val_loss: 670.2238\n",
      "Epoch 81/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1137.8339 - val_loss: 654.2750\n",
      "Epoch 82/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1117.3062 - val_loss: 638.5874\n",
      "Epoch 83/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1097.0596 - val_loss: 623.1587\n",
      "Epoch 84/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1077.0919 - val_loss: 607.9866\n",
      "Epoch 85/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1057.4008 - val_loss: 593.0693\n",
      "Epoch 86/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1037.9834 - val_loss: 578.4026\n",
      "Epoch 87/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1018.8371 - val_loss: 563.9856\n",
      "Epoch 88/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 999.9601 - val_loss: 549.8156\n",
      "Epoch 89/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 981.3501 - val_loss: 535.8898\n",
      "Epoch 90/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 963.0042 - val_loss: 522.2057\n",
      "Epoch 91/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 944.9198 - val_loss: 508.7614\n",
      "Epoch 92/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 927.0952 - val_loss: 495.5544\n",
      "Epoch 93/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 909.5280 - val_loss: 482.5821\n",
      "Epoch 94/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 892.2152 - val_loss: 469.8422\n",
      "Epoch 95/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 875.1552 - val_loss: 457.3323\n",
      "Epoch 96/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 858.3453 - val_loss: 445.0505\n",
      "Epoch 97/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 841.7834 - val_loss: 432.9943\n",
      "Epoch 98/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 825.4669 - val_loss: 421.1605\n",
      "Epoch 99/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 809.3939 - val_loss: 409.5480\n",
      "Epoch 100/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 793.5616 - val_loss: 398.1536\n",
      "Epoch 101/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 777.9683 - val_loss: 386.9755\n",
      "Epoch 102/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 762.6113 - val_loss: 376.0112\n",
      "Epoch 103/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 747.4885 - val_loss: 365.2588\n",
      "Epoch 104/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 732.5979 - val_loss: 354.7156\n",
      "Epoch 105/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 717.9370 - val_loss: 344.3792\n",
      "Epoch 106/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 703.5033 - val_loss: 334.2478\n",
      "Epoch 107/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 689.2953 - val_loss: 324.3190\n",
      "Epoch 108/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 675.3105 - val_loss: 314.5908\n",
      "Epoch 109/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 661.5466 - val_loss: 305.0602\n",
      "Epoch 110/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 648.0012 - val_loss: 295.7258\n",
      "Epoch 111/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 634.6723 - val_loss: 286.5847\n",
      "Epoch 112/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 621.5577 - val_loss: 277.6352\n",
      "Epoch 113/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 608.6556 - val_loss: 268.8750\n",
      "Epoch 114/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 595.9636 - val_loss: 260.3017\n",
      "Epoch 115/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 583.4794 - val_loss: 251.9133\n",
      "Epoch 116/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 571.2010 - val_loss: 243.7078\n",
      "Epoch 117/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 559.1264 - val_loss: 235.6824\n",
      "Epoch 118/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 547.2531 - val_loss: 227.8358\n",
      "Epoch 119/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 535.5792 - val_loss: 220.1651\n",
      "Epoch 120/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 524.1027 - val_loss: 212.6683\n",
      "Epoch 121/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 512.8213 - val_loss: 205.3432\n",
      "Epoch 122/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 501.7329 - val_loss: 198.1884\n",
      "Epoch 123/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 490.8357 - val_loss: 191.2007\n",
      "Epoch 124/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 480.1271 - val_loss: 184.3788\n",
      "Epoch 125/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 469.6056 - val_loss: 177.7201\n",
      "Epoch 126/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 459.2691 - val_loss: 171.2229\n",
      "Epoch 127/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 449.1152 - val_loss: 164.8849\n",
      "Epoch 128/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 439.1422 - val_loss: 158.7039\n",
      "Epoch 129/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 429.3478 - val_loss: 152.6778\n",
      "Epoch 130/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 419.7300 - val_loss: 146.8048\n",
      "Epoch 131/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 410.2867 - val_loss: 141.0825\n",
      "Epoch 132/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 401.0158 - val_loss: 135.5091\n",
      "Epoch 133/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 391.9157 - val_loss: 130.0821\n",
      "Epoch 134/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 382.9840 - val_loss: 124.7999\n",
      "Epoch 135/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 374.2190 - val_loss: 119.6603\n",
      "Epoch 136/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 365.6183 - val_loss: 114.6615\n",
      "Epoch 137/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 357.1805 - val_loss: 109.8011\n",
      "Epoch 138/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 348.9032 - val_loss: 105.0775\n",
      "Epoch 139/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 340.7847 - val_loss: 100.4881\n",
      "Epoch 140/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 332.8228 - val_loss: 96.0317\n",
      "Epoch 141/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 325.0157 - val_loss: 91.7054\n",
      "Epoch 142/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 317.3612 - val_loss: 87.5078\n",
      "Epoch 143/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 309.8577 - val_loss: 83.4368\n",
      "Epoch 144/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 302.5032 - val_loss: 79.4903\n",
      "Epoch 145/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 295.2956 - val_loss: 75.6666\n",
      "Epoch 146/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 288.2334 - val_loss: 71.9637\n",
      "Epoch 147/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 281.3144 - val_loss: 68.3795\n",
      "Epoch 148/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 274.5367 - val_loss: 64.9119\n",
      "Epoch 149/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 267.8983 - val_loss: 61.5594\n",
      "Epoch 150/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 261.3976 - val_loss: 58.3199\n",
      "Epoch 151/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 255.0326 - val_loss: 55.1912\n",
      "Epoch 152/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 248.8013 - val_loss: 52.1720\n",
      "Epoch 153/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 242.7021 - val_loss: 49.2598\n",
      "Epoch 154/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 236.7329 - val_loss: 46.4530\n",
      "Epoch 155/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 230.8921 - val_loss: 43.7498\n",
      "Epoch 156/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 225.1778 - val_loss: 41.1482\n",
      "Epoch 157/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 219.5880 - val_loss: 38.6462\n",
      "Epoch 158/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 214.1211 - val_loss: 36.2423\n",
      "Epoch 159/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 208.7753 - val_loss: 33.9346\n",
      "Epoch 160/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 203.5488 - val_loss: 31.7211\n",
      "Epoch 161/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 198.4398 - val_loss: 29.6001\n",
      "Epoch 162/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 193.4464 - val_loss: 27.5697\n",
      "Epoch 163/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 188.5669 - val_loss: 25.6281\n",
      "Epoch 164/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 183.7997 - val_loss: 23.7737\n",
      "Epoch 165/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 179.1429 - val_loss: 22.0045\n",
      "Epoch 166/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 174.5949 - val_loss: 20.3189\n",
      "Epoch 167/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 170.1537 - val_loss: 18.7150\n",
      "Epoch 168/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 165.8179 - val_loss: 17.1914\n",
      "Epoch 169/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 161.5857 - val_loss: 15.7460\n",
      "Epoch 170/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 157.4553 - val_loss: 14.3772\n",
      "Epoch 171/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 153.4253 - val_loss: 13.0833\n",
      "Epoch 172/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 149.4937 - val_loss: 11.8627\n",
      "Epoch 173/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 145.6589 - val_loss: 10.7137\n",
      "Epoch 174/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 141.9195 - val_loss: 9.6345\n",
      "Epoch 175/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 138.2736 - val_loss: 8.6235\n",
      "Epoch 176/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 134.7197 - val_loss: 7.6792\n",
      "Epoch 177/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 131.2561 - val_loss: 6.7998\n",
      "Epoch 178/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 127.8811 - val_loss: 5.9838\n",
      "Epoch 179/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 124.5936 - val_loss: 5.2296\n",
      "Epoch 180/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 121.3915 - val_loss: 4.5355\n",
      "Epoch 181/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 118.2735 - val_loss: 3.9001\n",
      "Epoch 182/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 115.2377 - val_loss: 3.3217\n",
      "Epoch 183/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 112.2830 - val_loss: 2.7988\n",
      "Epoch 184/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 109.4076 - val_loss: 2.3299\n",
      "Epoch 185/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 106.6101 - val_loss: 1.9135\n",
      "Epoch 186/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 103.8889 - val_loss: 1.5481\n",
      "Epoch 187/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 101.2426 - val_loss: 1.2322\n",
      "Epoch 188/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 98.6697 - val_loss: 0.9644\n",
      "Epoch 189/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 96.1688 - val_loss: 0.7431\n",
      "Epoch 190/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 93.7384 - val_loss: 0.5671\n",
      "Epoch 191/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 91.3770 - val_loss: 0.4348\n",
      "Epoch 192/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 89.0833 - val_loss: 0.3448\n",
      "Epoch 193/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 86.8558 - val_loss: 0.2958\n",
      "Epoch 194/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 84.6932 - val_loss: 0.2864\n",
      "Epoch 195/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 82.5939 - val_loss: 0.3153\n",
      "Epoch 196/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 80.5569 - val_loss: 0.3812\n",
      "Epoch 197/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 78.5805 - val_loss: 0.4826\n",
      "Epoch 198/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 76.6635 - val_loss: 0.6184\n",
      "Epoch 199/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 74.8045 - val_loss: 0.7872\n",
      "Epoch 200/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 73.0024 - val_loss: 0.9879\n",
      "Epoch 201/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 71.2558 - val_loss: 1.2190\n",
      "Epoch 202/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 69.5635 - val_loss: 1.4795\n",
      "Epoch 203/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 67.9241 - val_loss: 1.7682\n",
      "Epoch 204/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 66.3364 - val_loss: 2.0838\n",
      "Epoch 205/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 64.7991 - val_loss: 2.4251\n",
      "Epoch 206/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 63.3114 - val_loss: 2.7912\n",
      "Epoch 207/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 61.8716 - val_loss: 3.1807\n",
      "Epoch 208/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 60.4787 - val_loss: 3.5925\n",
      "Epoch 209/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 59.1317 - val_loss: 4.0258\n",
      "Epoch 210/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 57.8292 - val_loss: 4.4792\n",
      "Epoch 211/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 56.5703 - val_loss: 4.9519\n",
      "Epoch 212/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 55.3536 - val_loss: 5.4427\n",
      "Epoch 213/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 54.1783 - val_loss: 5.9506\n",
      "Epoch 214/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 53.0432 - val_loss: 6.4748\n",
      "Epoch 215/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 51.9473 - val_loss: 7.0142\n",
      "Epoch 216/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 50.8894 - val_loss: 7.5678\n",
      "Epoch 217/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 49.8685 - val_loss: 8.1348\n",
      "Epoch 218/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 48.8836 - val_loss: 8.7142\n",
      "Epoch 219/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 47.9339 - val_loss: 9.3051\n",
      "Epoch 220/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 47.0182 - val_loss: 9.9066\n",
      "Epoch 221/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 46.1357 - val_loss: 10.5180\n",
      "Epoch 222/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 45.2854 - val_loss: 11.1383\n",
      "Epoch 223/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 44.4663 - val_loss: 11.7669\n",
      "Epoch 224/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 43.6775 - val_loss: 12.4029\n",
      "Epoch 225/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 42.9182 - val_loss: 13.0455\n",
      "Epoch 226/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 42.1874 - val_loss: 13.6940\n",
      "Epoch 227/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 41.4843 - val_loss: 14.3477\n",
      "Epoch 228/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 40.8081 - val_loss: 15.0059\n",
      "Epoch 229/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 40.1581 - val_loss: 15.6678\n",
      "Epoch 230/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 39.5332 - val_loss: 16.3328\n",
      "Epoch 231/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 38.9328 - val_loss: 17.0003\n",
      "Epoch 232/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 38.3560 - val_loss: 17.6697\n",
      "Epoch 233/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 37.8022 - val_loss: 18.3403\n",
      "Epoch 234/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 37.2705 - val_loss: 19.0115\n",
      "Epoch 235/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 36.7603 - val_loss: 19.6830\n",
      "Epoch 236/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 36.2708 - val_loss: 20.3539\n",
      "Epoch 237/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 35.8013 - val_loss: 21.0239\n",
      "Epoch 238/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 35.3513 - val_loss: 21.6922\n",
      "Epoch 239/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 34.9200 - val_loss: 22.3590\n",
      "Epoch 240/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 34.5067 - val_loss: 23.0231\n",
      "Epoch 241/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 34.1108 - val_loss: 23.6842\n",
      "Epoch 242/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 33.7319 - val_loss: 24.3421\n",
      "Epoch 243/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 33.3691 - val_loss: 24.9962\n",
      "Epoch 244/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 33.0220 - val_loss: 25.6463\n",
      "Epoch 245/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 32.6900 - val_loss: 26.2918\n",
      "Epoch 246/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 32.3725 - val_loss: 26.9324\n",
      "Epoch 247/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 32.0690 - val_loss: 27.5681\n",
      "Epoch 248/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 31.7790 - val_loss: 28.1982\n",
      "Epoch 249/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 31.5019 - val_loss: 28.8223\n",
      "Epoch 250/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 31.2373 - val_loss: 29.4404\n",
      "Epoch 251/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 30.9847 - val_loss: 30.0521\n",
      "Epoch 252/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 30.7437 - val_loss: 30.6569\n",
      "Epoch 253/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 30.5138 - val_loss: 31.2550\n",
      "Epoch 254/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 30.2944 - val_loss: 31.8460\n",
      "Epoch 255/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 30.0854 - val_loss: 32.4295\n",
      "Epoch 256/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 29.8861 - val_loss: 33.0056\n",
      "Epoch 257/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 29.6962 - val_loss: 33.5738\n",
      "Epoch 258/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 29.5154 - val_loss: 34.1343\n",
      "Epoch 259/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 29.3432 - val_loss: 34.6867\n",
      "Epoch 260/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 29.1793 - val_loss: 35.2310\n",
      "Epoch 261/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 29.0233 - val_loss: 35.7667\n",
      "Epoch 262/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 28.8750 - val_loss: 36.2942\n",
      "Epoch 263/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 28.7339 - val_loss: 36.8129\n",
      "Epoch 264/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 28.5998 - val_loss: 37.3233\n",
      "Epoch 265/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 28.4723 - val_loss: 37.8248\n",
      "Epoch 266/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 28.3513 - val_loss: 38.3176\n",
      "Epoch 267/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 28.2362 - val_loss: 38.8015\n",
      "Epoch 268/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 28.1270 - val_loss: 39.2766\n",
      "Epoch 269/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 28.0234 - val_loss: 39.7427\n",
      "Epoch 270/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 27.9250 - val_loss: 40.2000\n",
      "Epoch 271/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 27.8317 - val_loss: 40.6481\n",
      "Epoch 272/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 27.7433 - val_loss: 41.0873\n",
      "Epoch 273/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 27.6594 - val_loss: 41.5176\n",
      "Epoch 274/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 27.5799 - val_loss: 41.9389\n",
      "Epoch 275/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 27.5046 - val_loss: 42.3513\n",
      "Epoch 276/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 27.4332 - val_loss: 42.7548\n",
      "Epoch 277/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 27.3657 - val_loss: 43.1492\n",
      "Epoch 278/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 27.3017 - val_loss: 43.5349\n",
      "Epoch 279/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 27.2412 - val_loss: 43.9119\n",
      "Epoch 280/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 27.1839 - val_loss: 44.2803\n",
      "Epoch 281/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 27.1297 - val_loss: 44.6395\n",
      "Epoch 282/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 27.0785 - val_loss: 44.9905\n",
      "Epoch 283/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 27.0301 - val_loss: 45.3328\n",
      "Epoch 284/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.9843 - val_loss: 45.6668\n",
      "Epoch 285/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.9410 - val_loss: 45.9925\n",
      "Epoch 286/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.9000 - val_loss: 46.3099\n",
      "Epoch 287/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.8614 - val_loss: 46.6189\n",
      "Epoch 288/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.8249 - val_loss: 46.9200\n",
      "Epoch 289/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.7905 - val_loss: 47.2131\n",
      "Epoch 290/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.7580 - val_loss: 47.4982\n",
      "Epoch 291/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.7272 - val_loss: 47.7757\n",
      "Epoch 292/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.6983 - val_loss: 48.0453\n",
      "Epoch 293/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 26.6710 - val_loss: 48.3078\n",
      "Epoch 294/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 26.6451 - val_loss: 48.5626\n",
      "Epoch 295/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.6208 - val_loss: 48.8102\n",
      "Epoch 296/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.5979 - val_loss: 49.0503\n",
      "Epoch 297/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.5763 - val_loss: 49.2838\n",
      "Epoch 298/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.5559 - val_loss: 49.5103\n",
      "Epoch 299/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 26.5367 - val_loss: 49.7298\n",
      "Epoch 300/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.5185 - val_loss: 49.9428\n",
      "Epoch 301/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.5014 - val_loss: 50.1489\n",
      "Epoch 302/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.4854 - val_loss: 50.3492\n",
      "Epoch 303/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.4702 - val_loss: 50.5428\n",
      "Epoch 304/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.4560 - val_loss: 50.7304\n",
      "Epoch 305/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 26.4425 - val_loss: 50.9119\n",
      "Epoch 306/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.4298 - val_loss: 51.0874\n",
      "Epoch 307/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.4179 - val_loss: 51.2573\n",
      "Epoch 308/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.4067 - val_loss: 51.4215\n",
      "Epoch 309/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.3961 - val_loss: 51.5801\n",
      "Epoch 310/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.3862 - val_loss: 51.7335\n",
      "Epoch 311/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 26.3768 - val_loss: 51.8815\n",
      "Epoch 312/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 26.3680 - val_loss: 52.0247\n",
      "Epoch 313/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 26.3596 - val_loss: 52.1627\n",
      "Epoch 314/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 26.3519 - val_loss: 52.2960\n",
      "Epoch 315/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.3445 - val_loss: 52.4246\n",
      "Epoch 316/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.3376 - val_loss: 52.5484\n",
      "Epoch 317/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.3311 - val_loss: 52.6678\n",
      "Epoch 318/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.3249 - val_loss: 52.7829\n",
      "Epoch 319/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.3192 - val_loss: 52.8937\n",
      "Epoch 320/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.3138 - val_loss: 53.0005\n",
      "Epoch 321/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.3086 - val_loss: 53.1033\n",
      "Epoch 322/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.3039 - val_loss: 53.2021\n",
      "Epoch 323/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2994 - val_loss: 53.2974\n",
      "Epoch 324/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2951 - val_loss: 53.3889\n",
      "Epoch 325/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2911 - val_loss: 53.4769\n",
      "Epoch 326/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 26.2874 - val_loss: 53.5615\n",
      "Epoch 327/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2838 - val_loss: 53.6426\n",
      "Epoch 328/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2805 - val_loss: 53.7206\n",
      "Epoch 329/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 26.2774 - val_loss: 53.7954\n",
      "Epoch 330/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2745 - val_loss: 53.8671\n",
      "Epoch 331/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2718 - val_loss: 53.9358\n",
      "Epoch 332/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2692 - val_loss: 54.0018\n",
      "Epoch 333/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2668 - val_loss: 54.0652\n",
      "Epoch 334/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2646 - val_loss: 54.1259\n",
      "Epoch 335/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2624 - val_loss: 54.1842\n",
      "Epoch 336/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2604 - val_loss: 54.2400\n",
      "Epoch 337/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2586 - val_loss: 54.2932\n",
      "Epoch 338/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2568 - val_loss: 54.3442\n",
      "Epoch 339/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2552 - val_loss: 54.3929\n",
      "Epoch 340/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2537 - val_loss: 54.4395\n",
      "Epoch 341/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2523 - val_loss: 54.4841\n",
      "Epoch 342/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 26.2509 - val_loss: 54.5267\n",
      "Epoch 343/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2497 - val_loss: 54.5675\n",
      "Epoch 344/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2485 - val_loss: 54.6063\n",
      "Epoch 345/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2475 - val_loss: 54.6433\n",
      "Epoch 346/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2465 - val_loss: 54.6788\n",
      "Epoch 347/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 26.2455 - val_loss: 54.7123\n",
      "Epoch 348/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2447 - val_loss: 54.7446\n",
      "Epoch 349/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2439 - val_loss: 54.7751\n",
      "Epoch 350/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2432 - val_loss: 54.8045\n",
      "Epoch 351/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2425 - val_loss: 54.8323\n",
      "Epoch 352/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2419 - val_loss: 54.8590\n",
      "Epoch 353/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2413 - val_loss: 54.8842\n",
      "Epoch 354/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2408 - val_loss: 54.9082\n",
      "Epoch 355/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2403 - val_loss: 54.9312\n",
      "Epoch 356/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 26.2399 - val_loss: 54.9529\n",
      "Epoch 357/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2395 - val_loss: 54.9737\n",
      "Epoch 358/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 26.2392 - val_loss: 54.9932\n",
      "Epoch 359/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2389 - val_loss: 55.0119\n",
      "Epoch 360/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 26.2385 - val_loss: 55.0296\n",
      "Epoch 361/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 26.2383 - val_loss: 55.0465\n",
      "Epoch 362/500\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 26.2381 - val_loss: 55.0625\n",
      "Epoch 363/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 26.2379 - val_loss: 55.0776\n",
      "Epoch 364/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 26.2378 - val_loss: 55.0920\n",
      "Epoch 365/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 26.2376 - val_loss: 55.1055\n",
      "Epoch 366/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2375 - val_loss: 55.1183\n",
      "Epoch 367/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2374 - val_loss: 55.1305\n",
      "Epoch 368/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2374 - val_loss: 55.1420\n",
      "Epoch 369/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2373 - val_loss: 55.1528\n",
      "Epoch 370/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2373 - val_loss: 55.1632\n",
      "Epoch 371/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2373 - val_loss: 55.1729\n",
      "Epoch 372/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2373 - val_loss: 55.1818\n",
      "Epoch 373/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2374 - val_loss: 55.1905\n",
      "Epoch 374/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 26.2374 - val_loss: 55.1988\n",
      "Epoch 375/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 26.2375 - val_loss: 55.2065\n",
      "Epoch 376/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 26.2376 - val_loss: 55.2138\n",
      "Epoch 377/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 26.2377 - val_loss: 55.2207\n",
      "Epoch 378/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 26.2378 - val_loss: 55.2273\n",
      "Epoch 379/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 26.2379 - val_loss: 55.2333\n",
      "Epoch 380/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 26.2381 - val_loss: 55.2393\n",
      "Epoch 381/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 26.2381 - val_loss: 55.2447\n",
      "Epoch 382/500\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 26.2382 - val_loss: 55.2496\n",
      "Epoch 383/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2384 - val_loss: 55.2542\n",
      "Epoch 384/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2386 - val_loss: 55.2586\n",
      "Epoch 385/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2388 - val_loss: 55.2627\n",
      "Epoch 386/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2390 - val_loss: 55.2667\n",
      "Epoch 387/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2392 - val_loss: 55.2703\n",
      "Epoch 388/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2393 - val_loss: 55.2736\n",
      "Epoch 389/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2395 - val_loss: 55.2766\n",
      "Epoch 390/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2398 - val_loss: 55.2796\n",
      "Epoch 391/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2400 - val_loss: 55.2824\n",
      "Epoch 392/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2402 - val_loss: 55.2849\n",
      "Epoch 393/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2404 - val_loss: 55.2875\n",
      "Epoch 394/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2407 - val_loss: 55.2897\n",
      "Epoch 395/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2409 - val_loss: 55.2919\n",
      "Epoch 396/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2411 - val_loss: 55.2938\n",
      "Epoch 397/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2413 - val_loss: 55.2955\n",
      "Epoch 398/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2415 - val_loss: 55.2972\n",
      "Epoch 399/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 26.2418 - val_loss: 55.2986\n",
      "Epoch 400/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2420 - val_loss: 55.2997\n",
      "Epoch 401/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2423 - val_loss: 55.3011\n",
      "Epoch 402/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2425 - val_loss: 55.3021\n",
      "Epoch 403/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 26.2428 - val_loss: 55.3031\n",
      "Epoch 404/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2431 - val_loss: 55.3040\n",
      "Epoch 405/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2433 - val_loss: 55.3051\n",
      "Epoch 406/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2435 - val_loss: 55.3056\n",
      "Epoch 407/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2438 - val_loss: 55.3065\n",
      "Epoch 408/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2441 - val_loss: 55.3072\n",
      "Epoch 409/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2443 - val_loss: 55.3077\n",
      "Epoch 410/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2445 - val_loss: 55.3081\n",
      "Epoch 411/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2448 - val_loss: 55.3086\n",
      "Epoch 412/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2450 - val_loss: 55.3089\n",
      "Epoch 413/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2454 - val_loss: 55.3092\n",
      "Epoch 414/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2456 - val_loss: 55.3093\n",
      "Epoch 415/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2458 - val_loss: 55.3097\n",
      "Epoch 416/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2461 - val_loss: 55.3098\n",
      "Epoch 417/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2463 - val_loss: 55.3099\n",
      "Epoch 418/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2466 - val_loss: 55.3102\n",
      "Epoch 419/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2468 - val_loss: 55.3103\n",
      "Epoch 420/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2471 - val_loss: 55.3101\n",
      "Epoch 421/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2473 - val_loss: 55.3101\n",
      "Epoch 422/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2476 - val_loss: 55.3099\n",
      "Epoch 423/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2478 - val_loss: 55.3099\n",
      "Epoch 424/500\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 26.2481 - val_loss: 55.3101\n",
      "Epoch 425/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 26.2483 - val_loss: 55.3100\n",
      "Epoch 426/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2486 - val_loss: 55.3100\n",
      "Epoch 427/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 26.2488 - val_loss: 55.3098\n",
      "Epoch 428/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2491 - val_loss: 55.3095\n",
      "Epoch 429/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2493 - val_loss: 55.3093\n",
      "Epoch 430/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2495 - val_loss: 55.3091\n",
      "Epoch 431/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2498 - val_loss: 55.3090\n",
      "Epoch 432/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 26.2500 - val_loss: 55.3085\n",
      "Epoch 433/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 26.2502 - val_loss: 55.3085\n",
      "Epoch 434/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2505 - val_loss: 55.3083\n",
      "Epoch 435/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2507 - val_loss: 55.3078\n",
      "Epoch 436/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2509 - val_loss: 55.3077\n",
      "Epoch 437/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2512 - val_loss: 55.3075\n",
      "Epoch 438/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2514 - val_loss: 55.3073\n",
      "Epoch 439/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2516 - val_loss: 55.3071\n",
      "Epoch 440/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2518 - val_loss: 55.3065\n",
      "Epoch 441/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2520 - val_loss: 55.3063\n",
      "Epoch 442/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2522 - val_loss: 55.3059\n",
      "Epoch 443/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2525 - val_loss: 55.3057\n",
      "Epoch 444/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2527 - val_loss: 55.3056\n",
      "Epoch 445/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2529 - val_loss: 55.3054\n",
      "Epoch 446/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2531 - val_loss: 55.3050\n",
      "Epoch 447/500\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 26.2533 - val_loss: 55.3047\n",
      "Epoch 448/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2535 - val_loss: 55.3040\n",
      "Epoch 449/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2538 - val_loss: 55.3040\n",
      "Epoch 450/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2539 - val_loss: 55.3038\n",
      "Epoch 451/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2541 - val_loss: 55.3036\n",
      "Epoch 452/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2543 - val_loss: 55.3032\n",
      "Epoch 453/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2545 - val_loss: 55.3026\n",
      "Epoch 454/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2547 - val_loss: 55.3023\n",
      "Epoch 455/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2549 - val_loss: 55.3019\n",
      "Epoch 456/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2551 - val_loss: 55.3016\n",
      "Epoch 457/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2553 - val_loss: 55.3015\n",
      "Epoch 458/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2555 - val_loss: 55.3013\n",
      "Epoch 459/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2556 - val_loss: 55.3006\n",
      "Epoch 460/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2559 - val_loss: 55.3003\n",
      "Epoch 461/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2560 - val_loss: 55.3000\n",
      "Epoch 462/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2562 - val_loss: 55.2997\n",
      "Epoch 463/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2564 - val_loss: 55.2995\n",
      "Epoch 464/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2566 - val_loss: 55.2995\n",
      "Epoch 465/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2567 - val_loss: 55.2993\n",
      "Epoch 466/500\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 26.2569 - val_loss: 55.2990\n",
      "Epoch 467/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2571 - val_loss: 55.2988\n",
      "Epoch 468/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2572 - val_loss: 55.2982\n",
      "Epoch 469/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2574 - val_loss: 55.2979\n",
      "Epoch 470/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2575 - val_loss: 55.2976\n",
      "Epoch 471/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2577 - val_loss: 55.2976\n",
      "Epoch 472/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2579 - val_loss: 55.2974\n",
      "Epoch 473/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2580 - val_loss: 55.2970\n",
      "Epoch 474/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2582 - val_loss: 55.2968\n",
      "Epoch 475/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2584 - val_loss: 55.2963\n",
      "Epoch 476/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2585 - val_loss: 55.2962\n",
      "Epoch 477/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2586 - val_loss: 55.2959\n",
      "Epoch 478/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2588 - val_loss: 55.2956\n",
      "Epoch 479/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2589 - val_loss: 55.2954\n",
      "Epoch 480/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2591 - val_loss: 55.2951\n",
      "Epoch 481/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2592 - val_loss: 55.2948\n",
      "Epoch 482/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2594 - val_loss: 55.2947\n",
      "Epoch 483/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2595 - val_loss: 55.2942\n",
      "Epoch 484/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 26.2597 - val_loss: 55.2939\n",
      "Epoch 485/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2598 - val_loss: 55.2937\n",
      "Epoch 486/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2599 - val_loss: 55.2934\n",
      "Epoch 487/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2600 - val_loss: 55.2928\n",
      "Epoch 488/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2602 - val_loss: 55.2926\n",
      "Epoch 489/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2603 - val_loss: 55.2922\n",
      "Epoch 490/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2605 - val_loss: 55.2920\n",
      "Epoch 491/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2606 - val_loss: 55.2919\n",
      "Epoch 492/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2607 - val_loss: 55.2918\n",
      "Epoch 493/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2608 - val_loss: 55.2917\n",
      "Epoch 494/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2610 - val_loss: 55.2917\n",
      "Epoch 495/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2611 - val_loss: 55.2913\n",
      "Epoch 496/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2612 - val_loss: 55.2910\n",
      "Epoch 497/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2613 - val_loss: 55.2906\n",
      "Epoch 498/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2614 - val_loss: 55.2905\n",
      "Epoch 499/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2615 - val_loss: 55.2904\n",
      "Epoch 500/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2616 - val_loss: 55.2901\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(82.7524, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0031, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0003, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0171, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(3.0230, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 336ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.72713119e+01, 5.72573063e+01, 5.72433007e+01, 5.72292950e+01,\n",
       "        5.72152895e+01, 5.72012839e+01, 5.71872782e+01, 5.71732726e+01,\n",
       "        5.71592670e+01, 5.71452614e+01, 5.71312558e+01, 5.71172502e+01,\n",
       "        5.71032446e+01, 5.70892390e+01, 5.70752334e+01, 5.70612278e+01,\n",
       "        5.70472222e+01, 5.70332166e+01, 5.70192110e+01, 5.70052054e+01,\n",
       "        5.69911998e+01, 5.69771942e+01, 5.69631886e+01, 5.69491830e+01,\n",
       "        5.69351774e+01, 5.69211718e+01, 5.69071662e+01, 5.68931606e+01,\n",
       "        5.68791550e+01, 5.68651494e+01, 5.68511438e+01, 5.68371382e+01,\n",
       "        5.68231326e+01, 5.68091270e+01, 5.67951214e+01, 5.67811158e+01,\n",
       "        5.67671102e+01, 5.67531046e+01, 5.67390990e+01, 5.67250934e+01,\n",
       "        5.67110878e+01, 5.66935808e+01, 5.66627684e+01, 5.66319561e+01,\n",
       "        5.66011438e+01, 5.65703315e+01, 5.65395191e+01, 5.65087068e+01,\n",
       "        5.64778945e+01, 5.64470822e+01, 5.64162698e+01, 5.63854575e+01,\n",
       "        5.63546452e+01, 5.63238329e+01, 5.62930205e+01, 5.62622082e+01,\n",
       "        5.62313959e+01, 5.62005836e+01, 5.61697712e+01, 5.61389589e+01,\n",
       "        5.61081466e+01, 5.60773343e+01, 5.60465219e+01, 5.60157096e+01,\n",
       "        5.59848973e+01, 5.59540850e+01, 5.59232726e+01, 5.58924603e+01,\n",
       "        5.58616480e+01, 5.58308357e+01, 5.58000233e+01, 5.57692110e+01,\n",
       "        5.57383987e+01, 5.57075864e+01, 5.56767740e+01, 5.56459617e+01,\n",
       "        5.56151494e+01, 5.55900327e+01, 5.55704248e+01, 5.55508170e+01,\n",
       "        6.40110703e+01, 1.29810739e+00, 0.00000000e+00, 4.24146205e-02,\n",
       "        0.00000000e+00, 6.38354957e-01, 8.74598384e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.56191289e-01, 8.68838131e-01, 0.00000000e+00,\n",
       "        1.23838449e+00, 4.33232427e-01, 3.15409720e-01, 5.82692683e-01,\n",
       "        2.79132515e-01, 0.00000000e+00, 0.00000000e+00, 3.72437805e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([54.32619048, 54.32058824, 54.31498599, 54.30938375, 54.30378151,\n",
       "       54.29726891, 54.28886555, 54.28046218, 54.27205882, 54.26365546,\n",
       "       54.2552521 , 54.24684874, 54.23844538, 54.23004202, 54.22163866,\n",
       "       54.21323529, 54.20483193, 54.19642857, 54.18802521, 54.17962185,\n",
       "       54.17121849, 54.16281513, 54.15441176, 54.1460084 , 54.13760504,\n",
       "       54.12920168, 54.12079832, 54.11239496, 54.1039916 , 54.09558824,\n",
       "       54.08718487, 54.07878151, 54.07037815, 54.06197479, 54.05357143,\n",
       "       54.04516807, 54.03676471, 54.02836134, 54.01995798, 54.01155462,\n",
       "       54.00315126, 53.9947479 , 53.98634454, 53.97794118, 53.96953782,\n",
       "       53.96113445, 53.95273109, 53.94432773, 53.93592437, 53.92752101,\n",
       "       53.91911765, 53.91071429, 53.90231092, 53.89390756, 53.8855042 ,\n",
       "       53.87710084, 53.86869748, 53.86029412, 53.85189076, 53.84348739,\n",
       "       53.83508403, 53.82668067, 53.81827731, 53.80987395, 53.80147059,\n",
       "       53.79306723, 53.78466387, 53.7762605 , 53.76785714, 53.75945378,\n",
       "       53.75105042, 53.74264706, 53.7342437 , 53.72584034, 53.71743697,\n",
       "       53.70903361, 53.70063025, 53.69222689, 53.68382353, 53.67542017,\n",
       "       53.66701681, 53.65861345, 53.65021008, 53.64180672, 53.63340336,\n",
       "       53.625     , 53.61659664, 53.60819328, 53.59978992, 53.59138655,\n",
       "       53.58298319, 53.57457983, 53.56617647, 53.55777311, 53.54936975,\n",
       "       53.54096639, 53.53256303, 53.52415966, 53.5157563 , 53.50735294])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.319760057196042\n",
      "12.251777791487461\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
