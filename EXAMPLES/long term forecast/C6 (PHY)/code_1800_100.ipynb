{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       84.600000\n",
       "1       84.431933\n",
       "2       84.263866\n",
       "3       84.095798\n",
       "4       83.927731\n",
       "          ...    \n",
       "1895    52.078968\n",
       "1896    52.063095\n",
       "1897    52.047222\n",
       "1898    52.031349\n",
       "1899    52.015476\n",
       "Name: C6, Length: 1900, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c6_interpolated_1800_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       84.600000\n",
       "1       84.431933\n",
       "2       84.263866\n",
       "3       84.095798\n",
       "4       83.927731\n",
       "          ...    \n",
       "1795     0.582693\n",
       "1796     0.279133\n",
       "1797     0.000000\n",
       "1798     0.000000\n",
       "1799     0.372438\n",
       "Name: C6, Length: 1800, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1800)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.431933</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84.263866</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84.095798</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83.927731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     84.600000  0.000298\n",
       "1     84.431933  0.000298\n",
       "2     84.263866  0.000297\n",
       "3     84.095798  0.000297\n",
       "4     83.927731  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 82.7524  # Value for C0\n",
    "K0 = -0.0031  # Value for K0\n",
    "K1 = -0.0003  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0171    # Value for b\n",
    "c = 3.0230    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    84.600000    84.431933    84.263866    84.095798    83.927731   \n",
      "351    84.431933    84.263866    84.095798    83.927731    83.759664   \n",
      "352    84.263866    84.095798    83.927731    83.759664    83.591597   \n",
      "353    84.095798    83.927731    83.759664    83.591597    83.423529   \n",
      "354    83.927731    83.759664    83.591597    83.423529    83.255462   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    83.759664    83.591597    83.423529    83.255462    83.092437  ...   \n",
      "351    83.591597    83.423529    83.255462    83.092437    82.991597  ...   \n",
      "352    83.423529    83.255462    83.092437    82.991597    82.890756  ...   \n",
      "353    83.255462    83.092437    82.991597    82.890756    82.789916  ...   \n",
      "354    83.092437    82.991597    82.890756    82.789916    82.689076  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   70.003688    0.000263   69.960738    0.000263   69.917787    0.000263   \n",
      "351   69.960738    0.000263   69.917787    0.000263   69.874837    0.000262   \n",
      "352   69.917787    0.000263   69.874837    0.000262   69.831886    0.000262   \n",
      "353   69.874837    0.000262   69.831886    0.000262   69.788936    0.000262   \n",
      "354   69.831886    0.000262   69.788936    0.000262   69.745985    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   69.874837    0.000262   69.831886    0.000262  \n",
      "351   69.831886    0.000262   69.788936    0.000262  \n",
      "352   69.788936    0.000262   69.745985    0.000262  \n",
      "353   69.745985    0.000262   69.703035    0.000262  \n",
      "354   69.703035    0.000262   69.660084    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1350, 1, 251) (1350, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "17/17 [==============================] - 2s 31ms/step - loss: 3919.2686 - val_loss: 2811.1902\n",
      "Epoch 2/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3828.6216 - val_loss: 2765.4028\n",
      "Epoch 3/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3773.8655 - val_loss: 2721.4749\n",
      "Epoch 4/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3720.6843 - val_loss: 2678.0974\n",
      "Epoch 5/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3668.2488 - val_loss: 2635.4395\n",
      "Epoch 6/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3616.5784 - val_loss: 2593.4375\n",
      "Epoch 7/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3565.5989 - val_loss: 2541.5872\n",
      "Epoch 8/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3500.4858 - val_loss: 2495.1184\n",
      "Epoch 9/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3442.9050 - val_loss: 2449.6208\n",
      "Epoch 10/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3387.6299 - val_loss: 2405.3423\n",
      "Epoch 11/500\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 3333.8113 - val_loss: 2362.2546\n",
      "Epoch 12/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3281.2371 - val_loss: 2320.1423\n",
      "Epoch 13/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3229.6873 - val_loss: 2278.8650\n",
      "Epoch 14/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3179.0227 - val_loss: 2238.3364\n",
      "Epoch 15/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3129.1580 - val_loss: 2198.4988\n",
      "Epoch 16/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3079.2434 - val_loss: 2155.7996\n",
      "Epoch 17/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3025.3076 - val_loss: 2113.7415\n",
      "Epoch 18/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2973.3682 - val_loss: 2072.7542\n",
      "Epoch 19/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2922.7791 - val_loss: 2032.9006\n",
      "Epoch 20/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2873.3936 - val_loss: 1993.9935\n",
      "Epoch 21/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2825.0146 - val_loss: 1955.9076\n",
      "Epoch 22/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2777.5156 - val_loss: 1918.5637\n",
      "Epoch 23/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2730.8176 - val_loss: 1881.9100\n",
      "Epoch 24/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2684.8655 - val_loss: 1845.9080\n",
      "Epoch 25/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2639.6194 - val_loss: 1810.5304\n",
      "Epoch 26/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2595.0500 - val_loss: 1775.7540\n",
      "Epoch 27/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2551.1321 - val_loss: 1741.5607\n",
      "Epoch 28/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2507.8462 - val_loss: 1707.9346\n",
      "Epoch 29/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2465.1750 - val_loss: 1674.8627\n",
      "Epoch 30/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2423.1050 - val_loss: 1642.3329\n",
      "Epoch 31/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2381.6221 - val_loss: 1610.3341\n",
      "Epoch 32/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2340.6494 - val_loss: 1577.5737\n",
      "Epoch 33/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2296.7605 - val_loss: 1542.8273\n",
      "Epoch 34/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2252.1821 - val_loss: 1508.8776\n",
      "Epoch 35/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2208.9180 - val_loss: 1476.1066\n",
      "Epoch 36/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2166.9292 - val_loss: 1444.3040\n",
      "Epoch 37/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2125.9819 - val_loss: 1413.3203\n",
      "Epoch 38/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2085.9248 - val_loss: 1383.0640\n",
      "Epoch 39/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2046.6635 - val_loss: 1353.4744\n",
      "Epoch 40/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2008.1335 - val_loss: 1324.5083\n",
      "Epoch 41/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1970.2913 - val_loss: 1296.1346\n",
      "Epoch 42/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1933.1014 - val_loss: 1268.3295\n",
      "Epoch 43/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1896.5375 - val_loss: 1241.0713\n",
      "Epoch 44/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1860.5770 - val_loss: 1214.3435\n",
      "Epoch 45/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1825.2013 - val_loss: 1188.1322\n",
      "Epoch 46/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1790.3951 - val_loss: 1162.4233\n",
      "Epoch 47/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 1756.1439 - val_loss: 1137.2073\n",
      "Epoch 48/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1722.4349 - val_loss: 1112.4722\n",
      "Epoch 49/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1689.2578 - val_loss: 1088.2096\n",
      "Epoch 50/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1656.6016 - val_loss: 1064.4103\n",
      "Epoch 51/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1624.4570 - val_loss: 1041.0660\n",
      "Epoch 52/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1592.8154 - val_loss: 1018.1692\n",
      "Epoch 53/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1561.6682 - val_loss: 995.7131\n",
      "Epoch 54/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1531.0078 - val_loss: 973.6899\n",
      "Epoch 55/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1500.8268 - val_loss: 952.0932\n",
      "Epoch 56/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1471.1177 - val_loss: 930.9169\n",
      "Epoch 57/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1441.8745 - val_loss: 910.1548\n",
      "Epoch 58/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1413.0903 - val_loss: 889.8008\n",
      "Epoch 59/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1384.7595 - val_loss: 869.8496\n",
      "Epoch 60/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1356.8752 - val_loss: 850.2953\n",
      "Epoch 61/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1329.4320 - val_loss: 831.1323\n",
      "Epoch 62/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1302.4244 - val_loss: 812.3556\n",
      "Epoch 63/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1275.8468 - val_loss: 793.9600\n",
      "Epoch 64/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1249.6931 - val_loss: 775.9406\n",
      "Epoch 65/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1223.9595 - val_loss: 758.2920\n",
      "Epoch 66/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1198.6395 - val_loss: 741.0096\n",
      "Epoch 67/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1173.7290 - val_loss: 724.0883\n",
      "Epoch 68/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1149.2229 - val_loss: 707.5239\n",
      "Epoch 69/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1125.1158 - val_loss: 691.3115\n",
      "Epoch 70/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1101.4034 - val_loss: 675.4468\n",
      "Epoch 71/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1078.0812 - val_loss: 659.9243\n",
      "Epoch 72/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1055.1442 - val_loss: 644.7405\n",
      "Epoch 73/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1032.5879 - val_loss: 629.8907\n",
      "Epoch 74/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1010.4082 - val_loss: 615.3711\n",
      "Epoch 75/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 988.6005 - val_loss: 601.1763\n",
      "Epoch 76/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 967.1603 - val_loss: 587.3026\n",
      "Epoch 77/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 946.0834 - val_loss: 573.7462\n",
      "Epoch 78/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 925.3654 - val_loss: 560.5024\n",
      "Epoch 79/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 905.0024 - val_loss: 547.5670\n",
      "Epoch 80/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 884.9899 - val_loss: 534.9360\n",
      "Epoch 81/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 865.3242 - val_loss: 522.6058\n",
      "Epoch 82/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 846.0006 - val_loss: 510.5722\n",
      "Epoch 83/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 827.0158 - val_loss: 498.8307\n",
      "Epoch 84/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 808.3655 - val_loss: 487.3780\n",
      "Epoch 85/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 790.0453 - val_loss: 476.2096\n",
      "Epoch 86/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 772.0519 - val_loss: 465.3219\n",
      "Epoch 87/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 754.3807 - val_loss: 454.7111\n",
      "Epoch 88/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 737.0289 - val_loss: 444.3732\n",
      "Epoch 89/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 719.9918 - val_loss: 434.3047\n",
      "Epoch 90/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 703.2660 - val_loss: 424.5012\n",
      "Epoch 91/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 686.8474 - val_loss: 414.9597\n",
      "Epoch 92/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 670.7325 - val_loss: 405.6760\n",
      "Epoch 93/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 654.9175 - val_loss: 396.6464\n",
      "Epoch 94/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 639.3990 - val_loss: 387.8671\n",
      "Epoch 95/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 624.1727 - val_loss: 379.3348\n",
      "Epoch 96/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 609.2357 - val_loss: 371.0456\n",
      "Epoch 97/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 594.5836 - val_loss: 362.9960\n",
      "Epoch 98/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 580.2136 - val_loss: 355.1823\n",
      "Epoch 99/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 566.1215 - val_loss: 347.6011\n",
      "Epoch 100/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 552.3041 - val_loss: 340.2484\n",
      "Epoch 101/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 538.7580 - val_loss: 333.1211\n",
      "Epoch 102/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 525.4794 - val_loss: 326.2156\n",
      "Epoch 103/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 512.4648 - val_loss: 319.5284\n",
      "Epoch 104/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 499.7112 - val_loss: 313.0560\n",
      "Epoch 105/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 487.2147 - val_loss: 306.7948\n",
      "Epoch 106/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 474.9720 - val_loss: 300.7415\n",
      "Epoch 107/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 462.9796 - val_loss: 294.8927\n",
      "Epoch 108/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 451.2344 - val_loss: 289.2448\n",
      "Epoch 109/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 439.7330 - val_loss: 283.7946\n",
      "Epoch 110/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 428.4717 - val_loss: 278.5388\n",
      "Epoch 111/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 417.4476 - val_loss: 273.4739\n",
      "Epoch 112/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 406.6572 - val_loss: 268.5965\n",
      "Epoch 113/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 396.0974 - val_loss: 263.9036\n",
      "Epoch 114/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 385.7647 - val_loss: 259.3915\n",
      "Epoch 115/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 375.6561 - val_loss: 255.0575\n",
      "Epoch 116/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 365.7682 - val_loss: 250.8979\n",
      "Epoch 117/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 356.0980 - val_loss: 246.9095\n",
      "Epoch 118/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 346.6421 - val_loss: 243.0890\n",
      "Epoch 119/500\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 337.3976 - val_loss: 239.4338\n",
      "Epoch 120/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 328.3612 - val_loss: 235.9400\n",
      "Epoch 121/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 319.5298 - val_loss: 232.6049\n",
      "Epoch 122/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 310.9005 - val_loss: 229.4250\n",
      "Epoch 123/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 302.4696 - val_loss: 226.3976\n",
      "Epoch 124/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 294.2346 - val_loss: 223.5193\n",
      "Epoch 125/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 286.1923 - val_loss: 220.7870\n",
      "Epoch 126/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 278.3397 - val_loss: 218.1979\n",
      "Epoch 127/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 270.6737 - val_loss: 215.7488\n",
      "Epoch 128/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 263.1914 - val_loss: 213.4366\n",
      "Epoch 129/500\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 255.8897 - val_loss: 211.2583\n",
      "Epoch 130/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 248.7657 - val_loss: 209.2113\n",
      "Epoch 131/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 241.8167 - val_loss: 207.2922\n",
      "Epoch 132/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 235.0395 - val_loss: 205.4982\n",
      "Epoch 133/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 228.4313 - val_loss: 203.8264\n",
      "Epoch 134/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 221.9892 - val_loss: 202.2738\n",
      "Epoch 135/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 215.7103 - val_loss: 200.8380\n",
      "Epoch 136/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 209.5920 - val_loss: 199.5155\n",
      "Epoch 137/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 203.6311 - val_loss: 198.3037\n",
      "Epoch 138/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 197.8253 - val_loss: 197.2000\n",
      "Epoch 139/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 192.1715 - val_loss: 196.2015\n",
      "Epoch 140/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 186.6670 - val_loss: 195.3055\n",
      "Epoch 141/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 181.3090 - val_loss: 194.5090\n",
      "Epoch 142/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 176.0948 - val_loss: 193.8097\n",
      "Epoch 143/500\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 171.0219 - val_loss: 193.2045\n",
      "Epoch 144/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 166.0874 - val_loss: 192.6911\n",
      "Epoch 145/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 161.2890 - val_loss: 192.2667\n",
      "Epoch 146/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 156.6238 - val_loss: 191.9287\n",
      "Epoch 147/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 152.0893 - val_loss: 191.6745\n",
      "Epoch 148/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 147.6829 - val_loss: 191.5015\n",
      "Epoch 149/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 143.4019 - val_loss: 191.4072\n",
      "Epoch 150/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 139.2440 - val_loss: 191.3892\n",
      "Epoch 151/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 135.2065 - val_loss: 191.4450\n",
      "Epoch 152/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 131.2871 - val_loss: 191.5720\n",
      "Epoch 153/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 127.4832 - val_loss: 191.7680\n",
      "Epoch 154/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 123.7925 - val_loss: 192.0304\n",
      "Epoch 155/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 120.2124 - val_loss: 192.3570\n",
      "Epoch 156/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 116.7407 - val_loss: 192.7453\n",
      "Epoch 157/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 113.3748 - val_loss: 193.1931\n",
      "Epoch 158/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 110.1127 - val_loss: 193.6982\n",
      "Epoch 159/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 106.9518 - val_loss: 194.2582\n",
      "Epoch 160/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 103.8901 - val_loss: 194.8710\n",
      "Epoch 161/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 100.9251 - val_loss: 195.5342\n",
      "Epoch 162/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 98.0546 - val_loss: 196.2460\n",
      "Epoch 163/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 95.2765 - val_loss: 197.0040\n",
      "Epoch 164/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 92.5885 - val_loss: 197.8063\n",
      "Epoch 165/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 89.9884 - val_loss: 198.6506\n",
      "Epoch 166/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 87.4742 - val_loss: 199.5352\n",
      "Epoch 167/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 85.0439 - val_loss: 200.4578\n",
      "Epoch 168/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 82.6951 - val_loss: 201.4167\n",
      "Epoch 169/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 80.4259 - val_loss: 202.4098\n",
      "Epoch 170/500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 78.2345 - val_loss: 203.4353\n",
      "Epoch 171/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 76.1187 - val_loss: 204.4913\n",
      "Epoch 172/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 74.0765 - val_loss: 205.5760\n",
      "Epoch 173/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 72.1061 - val_loss: 206.6876\n",
      "Epoch 174/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 70.2055 - val_loss: 207.8243\n",
      "Epoch 175/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 68.3730 - val_loss: 208.9845\n",
      "Epoch 176/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 66.6065 - val_loss: 210.1664\n",
      "Epoch 177/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 64.9044 - val_loss: 211.3684\n",
      "Epoch 178/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 63.2647 - val_loss: 212.5889\n",
      "Epoch 179/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 61.6858 - val_loss: 213.8263\n",
      "Epoch 180/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 60.1659 - val_loss: 215.0793\n",
      "Epoch 181/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 58.7033 - val_loss: 216.3460\n",
      "Epoch 182/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 57.2964 - val_loss: 217.6251\n",
      "Epoch 183/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 55.9436 - val_loss: 218.9152\n",
      "Epoch 184/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 54.6431 - val_loss: 220.2149\n",
      "Epoch 185/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 53.3934 - val_loss: 221.5229\n",
      "Epoch 186/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 52.1930 - val_loss: 222.8377\n",
      "Epoch 187/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 51.0404 - val_loss: 224.1583\n",
      "Epoch 188/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 49.9340 - val_loss: 225.4831\n",
      "Epoch 189/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 48.8724 - val_loss: 226.8111\n",
      "Epoch 190/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 47.8542 - val_loss: 228.1413\n",
      "Epoch 191/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 46.8779 - val_loss: 229.4721\n",
      "Epoch 192/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 45.9423 - val_loss: 230.8029\n",
      "Epoch 193/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 45.0458 - val_loss: 232.1323\n",
      "Epoch 194/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 44.1873 - val_loss: 233.4593\n",
      "Epoch 195/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 43.3654 - val_loss: 234.7831\n",
      "Epoch 196/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 42.5789 - val_loss: 236.1026\n",
      "Epoch 197/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 41.8266 - val_loss: 237.4166\n",
      "Epoch 198/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 41.1072 - val_loss: 238.7248\n",
      "Epoch 199/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 40.4197 - val_loss: 240.0260\n",
      "Epoch 200/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 39.7627 - val_loss: 241.3196\n",
      "Epoch 201/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 39.1352 - val_loss: 242.6045\n",
      "Epoch 202/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 38.5363 - val_loss: 243.8801\n",
      "Epoch 203/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 37.9648 - val_loss: 245.1457\n",
      "Epoch 204/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 37.4196 - val_loss: 246.4005\n",
      "Epoch 205/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 36.8998 - val_loss: 247.6442\n",
      "Epoch 206/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 36.4044 - val_loss: 248.8759\n",
      "Epoch 207/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 35.9324 - val_loss: 250.0949\n",
      "Epoch 208/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 35.4829 - val_loss: 251.3009\n",
      "Epoch 209/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 35.0552 - val_loss: 252.4932\n",
      "Epoch 210/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 34.6482 - val_loss: 253.6714\n",
      "Epoch 211/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 34.2611 - val_loss: 254.8351\n",
      "Epoch 212/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 33.8931 - val_loss: 255.9841\n",
      "Epoch 213/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 33.5434 - val_loss: 257.1174\n",
      "Epoch 214/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 33.2113 - val_loss: 258.2351\n",
      "Epoch 215/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 32.8959 - val_loss: 259.3366\n",
      "Epoch 216/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 32.5967 - val_loss: 260.4218\n",
      "Epoch 217/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 32.3128 - val_loss: 261.4904\n",
      "Epoch 218/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 32.0436 - val_loss: 262.5419\n",
      "Epoch 219/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 31.7886 - val_loss: 263.5764\n",
      "Epoch 220/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 31.5469 - val_loss: 264.5932\n",
      "Epoch 221/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 31.3181 - val_loss: 265.5924\n",
      "Epoch 222/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 31.1016 - val_loss: 266.5742\n",
      "Epoch 223/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 30.8967 - val_loss: 267.5380\n",
      "Epoch 224/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 30.7029 - val_loss: 268.4838\n",
      "Epoch 225/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 30.5198 - val_loss: 269.4116\n",
      "Epoch 226/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 30.3468 - val_loss: 270.3210\n",
      "Epoch 227/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 30.1834 - val_loss: 271.2123\n",
      "Epoch 228/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 30.0292 - val_loss: 272.0854\n",
      "Epoch 229/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 29.8837 - val_loss: 272.9405\n",
      "Epoch 230/500\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 29.7464 - val_loss: 273.7771\n",
      "Epoch 231/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 29.6170 - val_loss: 274.5957\n",
      "Epoch 232/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 29.4952 - val_loss: 275.3959\n",
      "Epoch 233/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 29.3803 - val_loss: 276.1785\n",
      "Epoch 234/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 29.2722 - val_loss: 276.9424\n",
      "Epoch 235/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 29.1705 - val_loss: 277.6888\n",
      "Epoch 236/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 29.0749 - val_loss: 278.4175\n",
      "Epoch 237/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 28.9849 - val_loss: 279.1281\n",
      "Epoch 238/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 28.9004 - val_loss: 279.8216\n",
      "Epoch 239/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 28.8210 - val_loss: 280.4976\n",
      "Epoch 240/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 28.7464 - val_loss: 281.1565\n",
      "Epoch 241/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 28.6764 - val_loss: 281.7983\n",
      "Epoch 242/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 28.6107 - val_loss: 282.4232\n",
      "Epoch 243/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 28.5492 - val_loss: 283.0311\n",
      "Epoch 244/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 28.4915 - val_loss: 283.6228\n",
      "Epoch 245/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 28.4374 - val_loss: 284.1983\n",
      "Epoch 246/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 28.3867 - val_loss: 284.7575\n",
      "Epoch 247/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 28.3393 - val_loss: 285.3009\n",
      "Epoch 248/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 28.2950 - val_loss: 285.8284\n",
      "Epoch 249/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 28.2535 - val_loss: 286.3409\n",
      "Epoch 250/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 28.2147 - val_loss: 286.8382\n",
      "Epoch 251/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 28.1784 - val_loss: 287.3204\n",
      "Epoch 252/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 28.1446 - val_loss: 287.7885\n",
      "Epoch 253/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 28.1129 - val_loss: 288.2417\n",
      "Epoch 254/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 28.0834 - val_loss: 288.6811\n",
      "Epoch 255/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 28.0559 - val_loss: 289.1068\n",
      "Epoch 256/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 28.0301 - val_loss: 289.5187\n",
      "Epoch 257/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 28.0061 - val_loss: 289.9174\n",
      "Epoch 258/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.9838 - val_loss: 290.3033\n",
      "Epoch 259/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.9629 - val_loss: 290.6761\n",
      "Epoch 260/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.9435 - val_loss: 291.0365\n",
      "Epoch 261/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.9255 - val_loss: 291.3851\n",
      "Epoch 262/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.9086 - val_loss: 291.7212\n",
      "Epoch 263/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.8930 - val_loss: 292.0463\n",
      "Epoch 264/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.8784 - val_loss: 292.3595\n",
      "Epoch 265/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.8649 - val_loss: 292.6620\n",
      "Epoch 266/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.8523 - val_loss: 292.9534\n",
      "Epoch 267/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.8406 - val_loss: 293.2345\n",
      "Epoch 268/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.8298 - val_loss: 293.5052\n",
      "Epoch 269/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.8197 - val_loss: 293.7664\n",
      "Epoch 270/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.8103 - val_loss: 294.0175\n",
      "Epoch 271/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.8016 - val_loss: 294.2592\n",
      "Epoch 272/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7936 - val_loss: 294.4918\n",
      "Epoch 273/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7861 - val_loss: 294.7157\n",
      "Epoch 274/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7791 - val_loss: 294.9305\n",
      "Epoch 275/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7728 - val_loss: 295.1370\n",
      "Epoch 276/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7668 - val_loss: 295.3351\n",
      "Epoch 277/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7613 - val_loss: 295.5255\n",
      "Epoch 278/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7562 - val_loss: 295.7084\n",
      "Epoch 279/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7515 - val_loss: 295.8839\n",
      "Epoch 280/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7471 - val_loss: 296.0521\n",
      "Epoch 281/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 27.7431 - val_loss: 296.2134\n",
      "Epoch 282/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 27.7393 - val_loss: 296.3680\n",
      "Epoch 283/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 27.7359 - val_loss: 296.5161\n",
      "Epoch 284/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 27.7327 - val_loss: 296.6576\n",
      "Epoch 285/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7298 - val_loss: 296.7931\n",
      "Epoch 286/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7271 - val_loss: 296.9227\n",
      "Epoch 287/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7247 - val_loss: 297.0471\n",
      "Epoch 288/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7223 - val_loss: 297.1658\n",
      "Epoch 289/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7202 - val_loss: 297.2790\n",
      "Epoch 290/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7183 - val_loss: 297.3874\n",
      "Epoch 291/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7165 - val_loss: 297.4907\n",
      "Epoch 292/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7149 - val_loss: 297.5892\n",
      "Epoch 293/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7134 - val_loss: 297.6832\n",
      "Epoch 294/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7121 - val_loss: 297.7731\n",
      "Epoch 295/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7108 - val_loss: 297.8587\n",
      "Epoch 296/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7097 - val_loss: 297.9404\n",
      "Epoch 297/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7087 - val_loss: 298.0181\n",
      "Epoch 298/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7078 - val_loss: 298.0918\n",
      "Epoch 299/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7070 - val_loss: 298.1621\n",
      "Epoch 300/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7062 - val_loss: 298.2292\n",
      "Epoch 301/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7055 - val_loss: 298.2926\n",
      "Epoch 302/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7050 - val_loss: 298.3534\n",
      "Epoch 303/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7044 - val_loss: 298.4105\n",
      "Epoch 304/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 27.7039 - val_loss: 298.4649\n",
      "Epoch 305/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7035 - val_loss: 298.5166\n",
      "Epoch 306/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7032 - val_loss: 298.5658\n",
      "Epoch 307/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7028 - val_loss: 298.6122\n",
      "Epoch 308/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7026 - val_loss: 298.6563\n",
      "Epoch 309/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7024 - val_loss: 298.6981\n",
      "Epoch 310/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7023 - val_loss: 298.7381\n",
      "Epoch 311/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7021 - val_loss: 298.7754\n",
      "Epoch 312/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7020 - val_loss: 298.8109\n",
      "Epoch 313/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7019 - val_loss: 298.8442\n",
      "Epoch 314/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7020 - val_loss: 298.8760\n",
      "Epoch 315/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7020 - val_loss: 298.9059\n",
      "Epoch 316/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7020 - val_loss: 298.9339\n",
      "Epoch 317/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7021 - val_loss: 298.9605\n",
      "Epoch 318/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7021 - val_loss: 298.9857\n",
      "Epoch 319/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7022 - val_loss: 299.0098\n",
      "Epoch 320/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7024 - val_loss: 299.0323\n",
      "Epoch 321/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7025 - val_loss: 299.0535\n",
      "Epoch 322/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7027 - val_loss: 299.0729\n",
      "Epoch 323/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7029 - val_loss: 299.0918\n",
      "Epoch 324/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7031 - val_loss: 299.1096\n",
      "Epoch 325/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7033 - val_loss: 299.1260\n",
      "Epoch 326/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7035 - val_loss: 299.1415\n",
      "Epoch 327/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 27.7038 - val_loss: 299.1562\n",
      "Epoch 328/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7040 - val_loss: 299.1700\n",
      "Epoch 329/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7043 - val_loss: 299.1829\n",
      "Epoch 330/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7045 - val_loss: 299.1949\n",
      "Epoch 331/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7048 - val_loss: 299.2060\n",
      "Epoch 332/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7051 - val_loss: 299.2164\n",
      "Epoch 333/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7054 - val_loss: 299.2264\n",
      "Epoch 334/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7057 - val_loss: 299.2355\n",
      "Epoch 335/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7061 - val_loss: 299.2440\n",
      "Epoch 336/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7064 - val_loss: 299.2520\n",
      "Epoch 337/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7067 - val_loss: 299.2590\n",
      "Epoch 338/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7071 - val_loss: 299.2664\n",
      "Epoch 339/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7074 - val_loss: 299.2726\n",
      "Epoch 340/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7077 - val_loss: 299.2786\n",
      "Epoch 341/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7081 - val_loss: 299.2838\n",
      "Epoch 342/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7085 - val_loss: 299.2887\n",
      "Epoch 343/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7088 - val_loss: 299.2934\n",
      "Epoch 344/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7092 - val_loss: 299.2976\n",
      "Epoch 345/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7096 - val_loss: 299.3018\n",
      "Epoch 346/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7099 - val_loss: 299.3055\n",
      "Epoch 347/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7103 - val_loss: 299.3088\n",
      "Epoch 348/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7107 - val_loss: 299.3120\n",
      "Epoch 349/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 27.7110 - val_loss: 299.3148\n",
      "Epoch 350/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7114 - val_loss: 299.3173\n",
      "Epoch 351/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7118 - val_loss: 299.3199\n",
      "Epoch 352/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7122 - val_loss: 299.3220\n",
      "Epoch 353/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7126 - val_loss: 299.3236\n",
      "Epoch 354/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7129 - val_loss: 299.3257\n",
      "Epoch 355/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7133 - val_loss: 299.3268\n",
      "Epoch 356/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7137 - val_loss: 299.3285\n",
      "Epoch 357/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7140 - val_loss: 299.3292\n",
      "Epoch 358/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7144 - val_loss: 299.3302\n",
      "Epoch 359/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7148 - val_loss: 299.3313\n",
      "Epoch 360/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7152 - val_loss: 299.3318\n",
      "Epoch 361/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7156 - val_loss: 299.3326\n",
      "Epoch 362/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7160 - val_loss: 299.3332\n",
      "Epoch 363/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7163 - val_loss: 299.3333\n",
      "Epoch 364/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7167 - val_loss: 299.3339\n",
      "Epoch 365/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7171 - val_loss: 299.3343\n",
      "Epoch 366/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7174 - val_loss: 299.3345\n",
      "Epoch 367/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7178 - val_loss: 299.3348\n",
      "Epoch 368/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7182 - val_loss: 299.3348\n",
      "Epoch 369/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7186 - val_loss: 299.3349\n",
      "Epoch 370/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7189 - val_loss: 299.3349\n",
      "Epoch 371/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7193 - val_loss: 299.3349\n",
      "Epoch 372/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 27.7196 - val_loss: 299.3348\n",
      "Epoch 373/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7200 - val_loss: 299.3345\n",
      "Epoch 374/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7203 - val_loss: 299.3342\n",
      "Epoch 375/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7207 - val_loss: 299.3340\n",
      "Epoch 376/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7210 - val_loss: 299.3333\n",
      "Epoch 377/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7214 - val_loss: 299.3329\n",
      "Epoch 378/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7218 - val_loss: 299.3327\n",
      "Epoch 379/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7221 - val_loss: 299.3323\n",
      "Epoch 380/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7224 - val_loss: 299.3317\n",
      "Epoch 381/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7228 - val_loss: 299.3314\n",
      "Epoch 382/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7231 - val_loss: 299.3309\n",
      "Epoch 383/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7234 - val_loss: 299.3302\n",
      "Epoch 384/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7238 - val_loss: 299.3295\n",
      "Epoch 385/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7241 - val_loss: 299.3288\n",
      "Epoch 386/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7244 - val_loss: 299.3282\n",
      "Epoch 387/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7248 - val_loss: 299.3272\n",
      "Epoch 388/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7251 - val_loss: 299.3267\n",
      "Epoch 389/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7254 - val_loss: 299.3263\n",
      "Epoch 390/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7257 - val_loss: 299.3254\n",
      "Epoch 391/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7260 - val_loss: 299.3248\n",
      "Epoch 392/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7263 - val_loss: 299.3239\n",
      "Epoch 393/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 27.7266 - val_loss: 299.3234\n",
      "Epoch 394/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7270 - val_loss: 299.3228\n",
      "Epoch 395/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7273 - val_loss: 299.3222\n",
      "Epoch 396/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7275 - val_loss: 299.3216\n",
      "Epoch 397/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7279 - val_loss: 299.3207\n",
      "Epoch 398/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7281 - val_loss: 299.3200\n",
      "Epoch 399/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7284 - val_loss: 299.3194\n",
      "Epoch 400/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7287 - val_loss: 299.3188\n",
      "Epoch 401/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7290 - val_loss: 299.3177\n",
      "Epoch 402/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7292 - val_loss: 299.3170\n",
      "Epoch 403/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 27.7295 - val_loss: 299.3163\n",
      "Epoch 404/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 27.7298 - val_loss: 299.3153\n",
      "Epoch 405/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 27.7302 - val_loss: 299.3148\n",
      "Epoch 406/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 27.7304 - val_loss: 299.3142\n",
      "Epoch 407/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 27.7307 - val_loss: 299.3141\n",
      "Epoch 408/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7309 - val_loss: 299.3133\n",
      "Epoch 409/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 27.7312 - val_loss: 299.3127\n",
      "Epoch 410/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 27.7315 - val_loss: 299.3119\n",
      "Epoch 411/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7317 - val_loss: 299.3115\n",
      "Epoch 412/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7320 - val_loss: 299.3109\n",
      "Epoch 413/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7322 - val_loss: 299.3103\n",
      "Epoch 414/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7324 - val_loss: 299.3099\n",
      "Epoch 415/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7327 - val_loss: 299.3088\n",
      "Epoch 416/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7330 - val_loss: 299.3083\n",
      "Epoch 417/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7332 - val_loss: 299.3081\n",
      "Epoch 418/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7334 - val_loss: 299.3075\n",
      "Epoch 419/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 27.7336 - val_loss: 299.3071\n",
      "Epoch 420/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7338 - val_loss: 299.3065\n",
      "Epoch 421/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7341 - val_loss: 299.3055\n",
      "Epoch 422/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7343 - val_loss: 299.3051\n",
      "Epoch 423/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7345 - val_loss: 299.3045\n",
      "Epoch 424/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7348 - val_loss: 299.3038\n",
      "Epoch 425/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7350 - val_loss: 299.3032\n",
      "Epoch 426/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7352 - val_loss: 299.3026\n",
      "Epoch 427/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7354 - val_loss: 299.3022\n",
      "Epoch 428/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7356 - val_loss: 299.3016\n",
      "Epoch 429/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7357 - val_loss: 299.3007\n",
      "Epoch 430/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7360 - val_loss: 299.2999\n",
      "Epoch 431/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7362 - val_loss: 299.2993\n",
      "Epoch 432/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7364 - val_loss: 299.2990\n",
      "Epoch 433/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7366 - val_loss: 299.2987\n",
      "Epoch 434/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7368 - val_loss: 299.2984\n",
      "Epoch 435/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7370 - val_loss: 299.2977\n",
      "Epoch 436/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7372 - val_loss: 299.2969\n",
      "Epoch 437/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7374 - val_loss: 299.2964\n",
      "Epoch 438/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7376 - val_loss: 299.2961\n",
      "Epoch 439/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 27.7378 - val_loss: 299.2959\n",
      "Epoch 440/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7379 - val_loss: 299.2956\n",
      "Epoch 441/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7381 - val_loss: 299.2949\n",
      "Epoch 442/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7383 - val_loss: 299.2939\n",
      "Epoch 443/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7385 - val_loss: 299.2935\n",
      "Epoch 444/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7386 - val_loss: 299.2932\n",
      "Epoch 445/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7388 - val_loss: 299.2927\n",
      "Epoch 446/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7389 - val_loss: 299.2922\n",
      "Epoch 447/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7391 - val_loss: 299.2915\n",
      "Epoch 448/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7393 - val_loss: 299.2910\n",
      "Epoch 449/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7394 - val_loss: 299.2905\n",
      "Epoch 450/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7396 - val_loss: 299.2904\n",
      "Epoch 451/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7398 - val_loss: 299.2901\n",
      "Epoch 452/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7399 - val_loss: 299.2897\n",
      "Epoch 453/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7401 - val_loss: 299.2893\n",
      "Epoch 454/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7402 - val_loss: 299.2891\n",
      "Epoch 455/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7403 - val_loss: 299.2882\n",
      "Epoch 456/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7405 - val_loss: 299.2878\n",
      "Epoch 457/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7406 - val_loss: 299.2874\n",
      "Epoch 458/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7408 - val_loss: 299.2871\n",
      "Epoch 459/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7409 - val_loss: 299.2866\n",
      "Epoch 460/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7411 - val_loss: 299.2863\n",
      "Epoch 461/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7412 - val_loss: 299.2859\n",
      "Epoch 462/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7413 - val_loss: 299.2855\n",
      "Epoch 463/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7415 - val_loss: 299.2851\n",
      "Epoch 464/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7416 - val_loss: 299.2849\n",
      "Epoch 465/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7417 - val_loss: 299.2844\n",
      "Epoch 466/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7418 - val_loss: 299.2842\n",
      "Epoch 467/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7419 - val_loss: 299.2837\n",
      "Epoch 468/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7421 - val_loss: 299.2836\n",
      "Epoch 469/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 27.7422 - val_loss: 299.2835\n",
      "Epoch 470/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7423 - val_loss: 299.2833\n",
      "Epoch 471/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7424 - val_loss: 299.2831\n",
      "Epoch 472/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7425 - val_loss: 299.2830\n",
      "Epoch 473/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7426 - val_loss: 299.2822\n",
      "Epoch 474/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7429 - val_loss: 299.2821\n",
      "Epoch 475/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7429 - val_loss: 299.2818\n",
      "Epoch 476/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7430 - val_loss: 299.2816\n",
      "Epoch 477/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7432 - val_loss: 299.2814\n",
      "Epoch 478/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7432 - val_loss: 299.2811\n",
      "Epoch 479/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7433 - val_loss: 299.2810\n",
      "Epoch 480/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7434 - val_loss: 299.2808\n",
      "Epoch 481/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7435 - val_loss: 299.2805\n",
      "Epoch 482/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7437 - val_loss: 299.2805\n",
      "Epoch 483/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7437 - val_loss: 299.2802\n",
      "Epoch 484/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7438 - val_loss: 299.2794\n",
      "Epoch 485/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7439 - val_loss: 299.2792\n",
      "Epoch 486/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7440 - val_loss: 299.2790\n",
      "Epoch 487/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7441 - val_loss: 299.2789\n",
      "Epoch 488/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7442 - val_loss: 299.2788\n",
      "Epoch 489/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7442 - val_loss: 299.2784\n",
      "Epoch 490/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7444 - val_loss: 299.2784\n",
      "Epoch 491/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7444 - val_loss: 299.2782\n",
      "Epoch 492/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7445 - val_loss: 299.2780\n",
      "Epoch 493/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7446 - val_loss: 299.2776\n",
      "Epoch 494/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7447 - val_loss: 299.2774\n",
      "Epoch 495/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7448 - val_loss: 299.2771\n",
      "Epoch 496/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7449 - val_loss: 299.2765\n",
      "Epoch 497/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7449 - val_loss: 299.2763\n",
      "Epoch 498/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 27.7450 - val_loss: 299.2761\n",
      "Epoch 499/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 27.7451 - val_loss: 299.2760\n",
      "Epoch 500/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.7452 - val_loss: 299.2760\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(82.7524, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0031, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0003, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0171, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(3.0230, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 362ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56.80445845, 56.79045285, 56.77644725, 56.76244164, 56.74843604,\n",
       "        56.73443044, 56.72042484, 56.70641923, 56.68330999, 56.65249767,\n",
       "        56.62168534, 56.59087302, 56.56006069, 56.52924837, 56.49843604,\n",
       "        56.46762372, 56.43681139, 56.40599907, 56.37518674, 56.34437442,\n",
       "        56.31356209, 56.28274977, 56.25193744, 56.22112512, 56.19031279,\n",
       "        56.15950047, 56.12868814, 56.09787582, 56.06706349, 56.03625117,\n",
       "        56.00543884, 55.97462652, 55.94381419, 55.91300187, 55.88218954,\n",
       "        55.85137722, 55.82056489, 55.78975257, 55.75894024, 55.72812792,\n",
       "        55.69731559, 55.66650327, 55.63569094, 55.60487862, 55.58349673,\n",
       "        55.56388889, 55.54428105, 58.17700747, 58.13499066, 58.09297386,\n",
       "        58.05095705, 58.00894024, 57.96692344, 57.92490663, 57.88288982,\n",
       "        57.84087302, 57.79885621, 57.7568394 , 57.7148226 , 57.67280579,\n",
       "        57.63078898, 57.58877218, 57.54675537, 57.50473856, 57.46272176,\n",
       "        57.42070495, 57.37868814, 57.33667134, 57.29465453, 57.25263772,\n",
       "        57.21062092, 57.16860411, 57.1265873 , 64.95271301,  0.66271698,\n",
       "         1.04276955,  0.        ,  0.13191691,  0.        ,  0.85314274,\n",
       "        61.80131149,  0.1296823 ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.16598321,  0.        ,  0.        ,  0.74634004,\n",
       "         0.        ,  0.18516281,  0.40364397,  0.        ,  0.        ,\n",
       "         0.        ,  0.41931009,  0.        ,  0.24974748,  0.1297501 ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([53.49894958, 53.49054622, 53.48214286, 53.4737395 , 53.46533613,\n",
       "       53.45693277, 53.44852941, 53.44012605, 53.43172269, 53.42331933,\n",
       "       53.41491597, 53.40651261, 53.39642857, 53.38055556, 53.36468254,\n",
       "       53.34880952, 53.33293651, 53.31706349, 53.30119048, 53.28531746,\n",
       "       53.26944444, 53.25357143, 53.23769841, 53.2218254 , 53.20595238,\n",
       "       53.19007937, 53.17420635, 53.15833333, 53.14246032, 53.1265873 ,\n",
       "       53.11071429, 53.09484127, 53.07896825, 53.06309524, 53.04722222,\n",
       "       53.03134921, 53.01547619, 52.99960317, 52.98373016, 52.96785714,\n",
       "       52.95198413, 52.93611111, 52.9202381 , 52.90436508, 52.88849206,\n",
       "       52.87261905, 52.85674603, 52.84087302, 52.825     , 52.80912698,\n",
       "       52.79325397, 52.77738095, 52.76150794, 52.74563492, 52.7297619 ,\n",
       "       52.71388889, 52.69801587, 52.68214286, 52.66626984, 52.65039683,\n",
       "       52.63452381, 52.61865079, 52.60277778, 52.58690476, 52.57103175,\n",
       "       52.55515873, 52.53928571, 52.5234127 , 52.50753968, 52.49166667,\n",
       "       52.47579365, 52.45992063, 52.44404762, 52.4281746 , 52.41230159,\n",
       "       52.39642857, 52.38055556, 52.36468254, 52.34880952, 52.33293651,\n",
       "       52.31706349, 52.30119048, 52.28531746, 52.26944444, 52.25357143,\n",
       "       52.23769841, 52.2218254 , 52.20595238, 52.19007937, 52.17420635,\n",
       "       52.15833333, 52.14246032, 52.1265873 , 52.11071429, 52.09484127,\n",
       "       52.07896825, 52.06309524, 52.04722222, 52.03134921, 52.01547619])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.256519175849675\n",
      "15.93434675360279\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
