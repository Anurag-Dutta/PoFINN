{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       84.600000\n",
       "1       84.431933\n",
       "2       84.263866\n",
       "3       84.095798\n",
       "4       83.927731\n",
       "          ...    \n",
       "1595    54.916830\n",
       "1596    54.910294\n",
       "1597    54.903758\n",
       "1598    54.897619\n",
       "1599    54.892017\n",
       "Name: C6, Length: 1600, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c6_interpolated_1500_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       84.600000\n",
       "1       84.431933\n",
       "2       84.263866\n",
       "3       84.095798\n",
       "4       83.927731\n",
       "          ...    \n",
       "1495    55.570425\n",
       "1496    55.563889\n",
       "1497    55.557353\n",
       "1498    55.550817\n",
       "1499    55.544281\n",
       "Name: C6, Length: 1500, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1500)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.431933</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84.263866</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84.095798</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83.927731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     84.600000  0.000298\n",
       "1     84.431933  0.000298\n",
       "2     84.263866  0.000297\n",
       "3     84.095798  0.000297\n",
       "4     83.927731  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAAsTAAALEwEAmpwYAAAitElEQVR4nO3dd3yUZb7+8c83CQkkhCQkARMghCZNIEAUEdQVrIigrgWPR9HV1d0VXT2u57j6O0f3nO1rX9ti742VVbGsih1XJID03ksgoQUJJe3+/TFPIGKQScjMM5O53q9XXjNzzwxz+eBcPLmfZs45REQk+sT5HUBERBpHBS4iEqVU4CIiUUoFLiISpVTgIiJRKiGcH5aVleXy8/PD+ZEiIlFv5syZW5xz2QePh7XA8/PzKSoqCudHiohEPTNbU9+4plBERKKUClxEJEoFVeBmdpOZLTCz+Wb2kpm1NLOnzWyVmX3j/RSEOKuIiNRx2DlwM+sA3AD0cc7tMbNXgXHe07c45yaFMqCIiNQv2CmUBKCVmSUAycDG0EUSEZFgHLbAnXMbgLuAtUAxUOace997+ndmNtfM7jWzpBDmFBGRgxy2wM0sAxgLdAFygRQz+3fg10Av4FigLfBfh3j/NWZWZGZFpaWlTRZcRCTWBTOFciqwyjlX6pyrBF4HTnDOFbuAfcBTwHH1vdk5N9E5V+icK8zO/t5+6EH5eHEJD3+yvFHvFRFproIp8LXA8WaWbGYGjAQWmVkOgDd2LjA/VCGnLd/C/R8uo6q6JlQfISISdYKZA58OTAJmAfO890wEXjCzed5YFvDbUIXsndOGfVU1rNpSHqqPEBGJOkEdSu+cuwO446DhEU0fp359ctsAsLB4Jz3ap4brY0VEIlpUHInZLbs1LeKNhcU7/Y4iIhIxoqLAExPi6NEulUXF3/odRUQkYkRFgUNgGmXhRq2Bi4jUipoC753Thi279lHy7V6/o4iIRISoKfA+OYENmZpGEREJiLoC1zSKiEhA1BR4WnILOqS3YpH2RBERAaKowAF656RqV0IREU9UFfiAjumsKN3Fll37/I4iIuK7qCrwU3q1w7nAya1ERGJdVBV439w2tG+TxEcqcBGR6CpwM2NEr/Z8trSUfVXVfscREfFVVBU4wKm921FeUc3Xq7b5HUVExFdRV+AndMsiKSGOqYs0jSIisS3qCrxVYjzDu2cxdfFmnHN+xxER8U3UFTjAiN7tWLdtD8tLdvkdRUTEN1FZ4CN7tQfgQ02jiEgMi8oCPyqtJX1z2/DR4s1+RxER8U1UFjjAyN7tmblmO9vLK/yOIiLii+gt8F7tqHHw4tdr/Y4iIuKLqC3w/h3TOLPvUdz1/hLem7/J7zgiImEXtQVuZtx7cQEFndL55cuzmbV2u9+RRETCKmoLHAL7hD9+eSFHpbXk6meKWL2l3O9IIiJhE9UFDpDZOomnrjgW5xxXPj2DbdqoKSIxIuoLHKBrdmseH1/Ihh17+OmzReyt1ImuRKT5C6rAzewmM1tgZvPN7CUza2lmXcxsupktN7NXzCwx1GF/yODObbnv4gJmrd3OTa98Q02NDrMXkebtsAVuZh2AG4BC59wxQDwwDvgTcK9zrjuwHbgqlEGDMapfDreP6s278zfxh3cX+R1HRCSkgp1CSQBamVkCkAwUAyOASd7zzwDnNnm6RrhqeBeuOCGfxz5fxdPTVvkdR0QkZA5b4M65DcBdwFoCxV0GzAR2OOeqvJetBzqEKmRDmBn/PboPp/Vpz2+mLOT9BdpHXESap2CmUDKAsUAXIBdIAc4M9gPM7BozKzKzotLS0kYHbYj4OOOBcQPp3zGdG16ezdz1O8LyuSIi4RTMFMqpwCrnXKlzrhJ4HRgGpHtTKgAdgQ31vdk5N9E5V+icK8zOzm6S0MFolRjPE+MLyUxJ4trnZlL6ra5kLyLNSzAFvhY43sySzcyAkcBC4GPgAu8144E3QhOx8bJaJzHx8sFs313Bz5+fSUVVjd+RRESaTDBz4NMJbKycBczz3jMR+C/gP8xsOZAJPBHCnI3WNzeNv1wwgKI127nzrQV+xxERaTIJh38JOOfuAO44aHglcFyTJwqBcwbksrB4J498soK+uW24dEhnvyOJiByxZnEkZjB+dXpPftQzmzveWMCM1bqivYhEv5gp8Pg44/5xA+nUNpmfPz+TjTv2+B1JROSIxEyBA6S1asFjlw9mb2UNP3t+JlXV2qgpItErpgocoHu7VP74437MXV+mq/mISFSLuQIHOLtfDkO7ZnLPB0sp213pdxwRkUaJyQI3M/7nnD7s3FPJ/VOX+R1HRKRRYrLAAXrntOHiY/N49l+rWVG6y+84IiINFrMFDnDz6UfTskU8v39bp54VkegT0wWe1TqJ60d0Z+riEj5bGp4TbYmINJWYLnCAK4blk9c2md++vVC7FYpIVIn5Ak9KiOe2Ub1ZunkXL81Y53ccEZGgxXyBA5zRtz3Hd23LPe8v0W6FIhI1VOAcuIrPjj2VPPCRdisUkeigAvf0zU3j4sJOPPPlalZqt0IRiQIq8DpuPr1nYLfCd7RboYhEPhV4HdmpSUwY0Z0PF5Xw5BercM75HUlE5JBU4Ae5clg+I3u143+nLOTm1+awt7La70giIvVSgR8kKSGexy4v5Jcje/D6rA1c8OiXrN++2+9YIiLfowKvR1yccdNpR/PE+ELWbN3NOX/9gi+WbfE7lojId6jAf8DI3u15c8JwslOTuPzJ6Tz66QrNi4tIxFCBH0aXrBQm/2IYZ/XL4Y/vLmbCi7Mp31fldywRERV4MFKSEnjwkoHcNqoX784v5tyHpmlfcRHxnQo8SGbGNSd147mrhrBl1z7GPjiNDxdu9juWiMQwFXgDDeuexVvXD6dzVjJXP1vEPR8spaZG8+IiEn4q8EbomJHMpJ+dwAWDO/LA1GVc/WwRZXt0EiwRCa/DFriZ9TSzb+r87DSzG83sTjPbUGd8VDgCR4qWLeL5ywX9+b+xfflsaSljH/yCJZu+9TuWiMSQwxa4c26Jc67AOVcADAZ2A5O9p++tfc45904Ic0YkM+Oyofm8cu3xlFdUc+5D05gyd6PfsUQkRjR0CmUksMI5tyYUYaLV4M5tefv64fTJbcOEF2fz+3cW6eo+IhJyDS3wccBLdR5PMLO5ZvakmWU0Ya6o065NS1766fFcPrQzEz9byeVPfs3WXfv8jiUizVjQBW5micAY4DVv6BGgG1AAFAN3H+J915hZkZkVlZY27wsHJybE8b9jj+GuCwdQtGY7Yx6cxrz1ZX7HEpFmqiFr4GcBs5xzmwGcc5udc9XOuRrgMeC4+t7knJvonCt0zhVmZ2cfeeIocMHgjvz9ZycA8ONHv+S1Il1rU0SaXkMK/BLqTJ+YWU6d584D5jdVqOagX8c03rp+OMfmZ3DLpLn89z/mU1GleXERaTpBFbiZpQCnAa/XGf6zmc0zs7nAKcBNIcgX1dqmJPLMlcdx7Uldee6rNVzy2Fds3rnX71gi0kxYOM+uV1hY6IqKisL2eZHk7bnF3DJpDilJCTxy6SAK89v6HUlEooSZzXTOFR48riMxw+Ts/jlM/sUwUhLjGTfxK12yTUSOmAo8jHoelcqb1w9nhHfJtgkvzWaXTk0rIo2kAg+zNi1b8LfLBnPrWb14d17g1LTLS3QIvog0nArcB2bGz07uxvNXD2HH7grGPKhD8EWk4VTgPjqhWxZTrj+R3jmBQ/B/89YCKnUIvogESQXus6PSWvLyNcdz5bB8npq2mksmfsWmMu1qKCKHpwKPAC3i47jjnL48cMlAFhbvZPRfP+fLFVv8jiUiEU4FHkHGDMjljeuGkdaqBf/++HQe/XSFdjUUkUNSgUeYHu1TeWPCcM46Joc/vruYa5+byc69utqPiHyfCjwCtU5K4MF/G8h/j+7DR4tLGPPXL5i7foffsUQkwqjAI5SZcdXwLrx0zfHsrqhmzIPTuPa5IhYV7/Q7mohECBV4hDs2vy0f3nwyN57agy+Xb+Ws+z/n58/P1PU3RUQns4omZbsreeKLlTw5bTXlFVWM6pfDjSN70KN9qt/RRCSEDnUyKxV4FNqxu4LHPl/J09NWs7uymnP653LDyB50b9fa72giEgIq8GZoW3mgyJ/5cjV7K6sZW9CB60d0p2u2ilykOVGBN2Nbd+1j4mcrefZfa9hXVc25Aztww4ge5Gel+B1NRJqACjwGlH67j799uoLnvlpDVY3j/IEduH5ED/Iyk/2OJiJHQAUeQ0q+3cujn6zk+elrqKlxXDC4I9ed0p1ObVXkItFIBR6DNu/cyyOfrODF6WupcY4LCzsxYUR3OqS38juaiDSACjyGFZft4eGPV/DKjHU4HBcf24nrTulOTpqKXCQaqMCFjTv28NDHy3m1aB2G8W9D8vj1qF4kJcT7HU1EfoAuaizkprfid+f14+Nf/YjzB3Xg6S9Xc8/7S/2OJSKNlOB3AAm/jhnJ/PHH/YmPMyZ+vpJTerXj+K6ZfscSkQbSGngMu/3s3uRnpnDzq3N0ylqRKKQCj2HJiQncc9EANu3cy51vLPA7jog00GEL3Mx6mtk3dX52mtmNZtbWzD4ws2XebUY4AkvTGpiXwYRTuvP67A28PbfY7zgi0gCHLXDn3BLnXIFzrgAYDOwGJgO3AlOdcz2Aqd5jiUITRnRnQKd0bv/HPDbv1AWVRaJFQ6dQRgIrnHNrgLHAM974M8C5TZhLwqhFfBz3XjSAfZU1/Oq1OdTU6DqcItGgoQU+DnjJu9/eOVf7O/cmoH2TpZKw65rdmtvP7s3ny7bw3Fdr/I4jIkEIusDNLBEYA7x28HMucDRQvattZnaNmRWZWVFpaWmjg0roXTokj1N6ZvP7dxaxvERX/BGJdA1ZAz8LmOWc2+w93mxmOQDebUl9b3LOTXTOFTrnCrOzs48srYSUmfGnC/qTnBjPja98Q0VVjd+RROQHNKTAL+HA9AnAm8B47/544I2mCiX+aZfakj+c35/5G3bywNRlfscRkR8QVIGbWQpwGvB6neE/AqeZ2TLgVO+xNANnHnMUFw7uyMOfLGfmmm1+xxGRQwiqwJ1z5c65TOdcWZ2xrc65kc65Hs65U51z+qY3I/9zTh9y01tx0ytz2LWvyu84IlIPHYkp9Upt2YJ7Ly5g3fbd/HbKQr/jiEg9VOBySMfmt+VnJ3fj5Rnr+GDh5sO/QUTCSgUuP+imU4+mT04bbv37XEq/3ed3HBGpQwUuPygxIY77xhXw7b4qbv37XMJ5ARAR+WEqcDmso9uncuuZvZi6uISXZ6zzO46IeFTgEpQrTshnWPdM/m/KQlZvKfc7joigApcgxcUZd104gIQ446ZXv6GqWkdpivhNBS5By0lrxW/P68fstTt4+JMVfscRiXkqcGmQMQNyGVuQy/1TlzFn3Q6/44jENBW4NNj/jjmGdqlJ3PTqN+ypqPY7jkjMUoFLg6Ult+DuCwewsrScP7y7yO84IjFLBS6NckL3LK4a3oVn/7WGT5bUeyZhEQkxFbg02i1n9OTo9q25ZdJctpdX+B1HJOaowKXRWraI576LB7JjdwW3TZ6nozRFwkwFLkekT24bbj69J+/O38Trszb4HUckpqjA5Yj99MSuHJffljveXMC6bbv9jiMSM1TgcsTi44y7LxoAwM2vzqG6RlMpIuGgApcm0altMneO6cvXq7dx9/tL2LpLp54VCbUEvwNI8/HjQR34eEkJD3+ygoc/WUF+ZjID8zIYlJfOwLwMeh2VSkK81hlEmoqFc8+BwsJCV1RUFLbPk/CrrnEUrd7G7HU7mL12O7PW7th/IYhWLeLp1zGNQXkZDMxLZ2BeOu1SW/qcWCTymdlM51zhweNaA5cmFR9nDOmayZCumQA459iwYw+z1h4o9Ce+WElldWDFoWNGq++spffJaUNigtbSRYKhApeQMjM6ZiTTMSOZMQNyAdhbWc2CjTuZvXY7s9fuoGj1Nt6asxEIXAGoX4e0/YU+KC+Do9K0li5SH02hSEQoLtvD7Dpr6fM2lFFRFTjneE5aSwblZXDB4I6c0qudz0lFwk9TKBLRctJakdOvFaP65QBQUVXDwuIDa+lfr9rG2/OKuXRIHv/v7D60Soz3ObGI/1TgEpESE+Io6JROQad0rhwG+6qqufv9pUz8bCXTV23j/nEF9M1N8zumiK+C2lpkZulmNsnMFpvZIjMbamZ3mtkGM/vG+xkV6rASu5IS4rltVG+ev2oIO/dUct5DX/L45yup0UFDEsOC3dx/P/Cec64XMACoPQn0vc65Au/nnZAkFKljeI8s3rvxJE7umc1v317E+Ke+pmTnXr9jifjisAVuZmnAScATAM65CufcjhDnEjmktimJTLxsML877xhmrN7GGfd9xgcLN/sdSyTsglkD7wKUAk+Z2Wwze9zMUrznJpjZXDN70swyQhdT5LvMjEuHdGbK9cPJSWvFT58t4vbJ83SJN4kpwRR4AjAIeMQ5NxAoB24FHgG6AQVAMXB3fW82s2vMrMjMikpLS5sktEit7u1SmXzdCVxzUldemL6Wcx78ggUby/yOJRIWwRT4emC9c26693gSMMg5t9k5V+2cqwEeA46r783OuYnOuULnXGF2dnbTpBapQxs4JVYdtsCdc5uAdWbW0xsaCSw0s5w6LzsPmB+CfCJB0wZOiTXB7oVyPfCCmc0lMGXye+DPZjbPGzsFuCk0EUWCpw2cEkt0KL00W8tLvuWGl75hYfFOHcEpUe1Qh9LrtG/SbGkDpzR3KnBp1g7ewHnuQ9N47DNt4JTmQQUuMaF2A+ePerbjd+8ENnBu1gZOiXIqcIkZB2/gPFMbOCXK6WyEElNqj+Ac0qUtN7z0DT99tohj8zMY2i2LoV0zGZiXTssW2tAp0UF7oUjM2ldVzcRPV/L+ws0s2FhGjQucxnZgp3SO75rJ0G6ZFHRSoYv/DrUXigpcBCjbU0nR6m18tXIrX63cxvyNZTiv0AfleYXeNZOCvHSSElToEl4qcJEGKNtTyYxVXqGv2sqCjTtxDpIS4hiUl7F/DX1ApzQVuoScClzkCJTtruTr/WvoW1lYfKDQB3c+UOj9O6rQpempwEWaUG2h/2tFoNAXbQoUessWgUI/+ehsxp+QrzKXJqECFwmhHbsr+HrVNr5auW3/GnpBp3QevnQQuemt/I4nUU4FLhJG784r5pZJc0lMiOOvlwxkWPcsvyNJFNO5UETC6Kx+ObwxYRiZKYlc9sR0Hvp4uQ7flyanAhcJkW7ZrfnHdcMY3T+Xv/xzCdc8V0TZnkq/Y0kzogIXCaGUpATuH1fAnef04ZMlpYx58AsWbtzpdyxpJlTgIiFmZlwxrAuvXHs8eyurOe/haUyaud7vWNIMqMBFwmRw57a8fcOJDMrL4FevzeG2yfPYV1XtdyyJYipwkTDKap3Ec1cdx89O7saL09dy0aP/YsOOPX7HkiilAhcJs4T4OG49qxd/u2wwK0vLGf3A53y2tNTvWBKFVOAiPjmj71G8ef1w2qW2ZPxTX/PA1GXa1VAaRAUu4qMuWSlMvu4Ezi3owD0fLOXqZ4so261dDSU4KnARnyUnJnDPRQP4v7F9+XxZKaMf/Jz5G3TxZTk8FbhIBDAzLhuazyvXDqWq2nH+I1/y6ox1fseSCKcCF4kgg/IymHL9cI7Nz+A//z6XW/8+l72V2tVQ6qcCF4kwma2TePYnQ7julG68PGMdFz76L9Zt2+13LIlAQRW4maWb2SQzW2xmi8xsqJm1NbMPzGyZd5sR6rAisSI+zrjljF48dnkhq7eWM/qvX/DxkhK/Y0mECXYN/H7gPedcL2AAsAi4FZjqnOsBTPUei0gTOq1Pe96aMJyctJb85OkZ3PvBUu1qKPsdtsDNLA04CXgCwDlX4ZzbAYwFnvFe9gxwbmgiisS2/KwUJv9iGOcN7MD9U5cx7rGvWF6yy+9YEgGCWQPvApQCT5nZbDN73MxSgPbOuWLvNZuA9qEKKRLrWiXGc/eFA/jzj/uzuHgnZ93/Gfe8v0QbOGNcMAWeAAwCHnHODQTKOWi6xAUu61Pv73Vmdo2ZFZlZUWmpDhcWaSwz46JjOzH15h9xdr8cHvhoOWfe9xlfLNvidzTxSTAFvh5Y75yb7j2eRKDQN5tZDoB3W+8WFufcROdcoXOuMDs7uykyi8S07NQk7hs3kOevGgLAvz8xnV++PJvSb/f5nEzC7bAF7pzbBKwzs57e0EhgIfAmMN4bGw+8EZKEIlKv4T2yeO/Gk7hhZA/enbeJkXd/wgvT12gjZwwJ6qLGZlYAPA4kAiuBKwmU/6tAHrAGuMg5t+2H/hxd1FgkNJaX7OL//WMeX63cxqC8dH5/fj96HdXG71jSRHRVepFmzjnH67M28Lt3FrFzTyVXndiFX47sQXJigt/R5AjpqvQizZyZ8ePBHZn6Hydz/qAO/O3TlZx2z2d8tHiz39EkRFTgIs1MRkoif75gAK9eO5RWifH85Okifv78TDaV7fU7mjQxFbhIM3Vcl7a8c8OJ3HJGTz5aXMKp93zKU9NWUa2NnM2GClykGUtMiOO6U7rz/k0nMahzBr95ayHnPjSNeet1vvHmQAUuEgM6Z6bwzJXH8sAlAyku28vYh77gN28tYNe+Kr+jyRFQgYvECDNjzIBcpt58Mv82JI+nv1zNqXd/ynvziwnn3mjSdLQboUiMmr12O7dNns+i4p30bJ9Kz6NS6Zbdmu7tWtOtXQr5mSm0bBHvd0xB+4GLSD2qqmt47qs1fLKklBWlu9iwYw+1lRBn0KltMt2zW9OtXWvvNoXu2amkJbfwN3iMUYGLyGHtqahm5ZZdrCgtZ3nJLlaU7mJFyS5Wbimnoqpm/+uyWifStXZtff9tCrlprYiLMx//C5qnQxW4DtESkf1aJcbTNzeNvrlp3xmvrnGs376bFaW7AsVeUs6K0l28M6+YHbsrD7y/RTxds1PqlHrgNj8rmaQETcc0NRW4iBxWfJzROTOFzpkpjOh14NT/zjm2lVd4a+sH1tpnrd3Om3M27n9dnEFe22S6aTqmSanARaTRzIzM1klktk5iSNfM7zxXOx1TW+4rvHL/fPmW703HfLfYNR0TLBW4iITE4aZjDsyxl7O8dBdvzy2mbM/3p2O+O8+u6Zi6VOAiElZ1p2NG9v7udMzW8gpWlOxieZ1iL1q9nTe+qX86prbUa9feY206RgUuIhHBzMhqnURWPdMxuyuqWFlavn+vmNr59s+XbaGiuu50TBLdslO+Mx3TvV1rctq0bJbTMSpwEYl4yYkJHNMhjWM6fH86Zt22OnvHlAbKvb7pmG7tAnvHNKfpGBW4iESt+DgjPyuF/Kz6p2NqS712Q+qhpmMCR5+25kdHt+O4Lm2Jj5K1dR3IIyIx5eDpmNr59lVbyqmorqFdahKj+uUwun8Og/IyImLqRQfyiIhw6OmY3RVVTF1UwpS5G3nx67U8/eVqctJa7i/zgk7pmPlf5nVpDVxE5CC79lXx4cLNTJm7kU+XllJZ7eiY0Yqz++dwTv9c+ua2CWuZ61woIiKNULankvcXbOLtecV8sWwLVTWO/Mxkzu6fw+j+ufQ6KjXkZa4CFxE5QtvLK/inV+ZfrthKdY2jW3YKZ/fP5Zz+OfRonxqSz1WBi4g0oa279vHu/E1MmbuR6au24Rz0bJ/K6P45nN0/h67ZrZvss1TgIiIhUrJz7/4yn7F6OwB9ctowekAOo/vlkpeZfER/vgpcRCQMisv28M68QJnPXrsDgM6Zyfzh/H6c0C2rUX/mEe1GaGargW+BaqDKOVdoZncCPwVKvZfd5px7p1HpRESaiZy0Vlw1vAtXDe/C+u27eW/+JopWb6ddassm/6yG7Ad+inNuy0Fj9zrn7mrKQCIizUXHjGSuPrErV58Ymj9fV6UXEYlSwRa4A943s5lmdk2d8QlmNtfMnjSzjBDkExGRQwi2wIc75wYBZwHXmdlJwCNAN6AAKAburu+NZnaNmRWZWVFpaWl9LxERkUYIqsCdcxu82xJgMnCcc26zc67aOVcDPAYcd4j3TnTOFTrnCrOzs5sqt4hIzDtsgZtZipml1t4HTgfmm1lOnZedB8wPTUQREalPMHuhtAcme8f6JwAvOufeM7PnzKyAwPz4auDaUIUUEZHvO2yBO+dWAgPqGb8sJIlERCQo2o1QRCRKhfVQejMrBdY08u1ZwMEHEkWaSM8Y6flAGZtCpOeDyM8Yafk6O+e+txdIWAv8SJhZUX3nAogkkZ4x0vOBMjaFSM8HkZ8x0vPV0hSKiEiUUoGLiESpaCrwiX4HCEKkZ4z0fKCMTSHS80HkZ4z0fEAUzYGLiMh3RdMauIiI1KECFxGJUlFR4GZ2ppktMbPlZnarTxk6mdnHZrbQzBaY2S+98bZm9oGZLfNuM7xxM7MHvMxzzWxQmHLGm9lsM5viPe5iZtO9HK+YWaI3nuQ9Xu49nx+mfOlmNsnMFpvZIjMbGoHL8Cbv73i+mb1kZi39Xo7eKZtLzGx+nbEGLzczG++9fpmZjQ9xvr94f89zzWyymaXXee7XXr4lZnZGnfGQfdfry1jnuZvNzJlZlvc47MuwUZxzEf0DxAMrgK5AIjAH6ONDjhxgkHc/FVgK9AH+DNzqjd8K/Mm7Pwp4FzDgeGB6mHL+B/AiMMV7/Cowzrv/KPBz7/4vgEe9++OAV8KU7xngau9+IpAeScsQ6ACsAlrVWX5X+L0cgZOAQcD8OmMNWm5AW2Cld5vh3c8IYb7TgQTv/p/q5OvjfY+TgC7e9zs+1N/1+jJ6452AfxI4yDDLr2XYqP8mvz64AQt9KPDPOo9/Dfw6AnK9AZwGLAFyvLEcYIl3/2/AJXVev/91IczUEZgKjACmeP/zbanzJdq/LL3/YYd69xO811mI86V55WgHjUfSMuwArPO+oAnecjwjEpYjkH9QQTZouQGXAH+rM/6d1zV1voOeOw94wbv/ne9w7TIMx3e9vozAJALne1rNgQL3ZRk29CcaplBqv1C11ntjvvF+TR4ITAfaO+eKvac2ETh7I/iT+z7gP4Ea73EmsMM5V1VPhv35vOfLvNeHUhcCF8F+ypvmedwCpyiOmGXoAue+vwtYS+BCJWXATCJrOdZq6HLz87v0EwJrtPxAjrDnM7OxwAbn3JyDnoqYjD8kGgo8ophZa+DvwI3OuZ11n3OBf5J92S/TzEYDJc65mX58fpASCPwK+4hzbiBQTuBX//38XIYA3jzyWAL/2OQCKcCZfuUJlt/L7YeY2e1AFfCC31nqMrNk4Dbgf/zO0ljRUOAbCMxR1erojYWdmbUgUN4vOOde94Y3m3dxC++2xBsPd+5hwBgzWw28TGAa5X4g3cxqTxtcN8P+fN7zacDWEOaDwNrKeufcdO/xJAKFHinLEOBUYJVzrtQ5Vwm8TmDZRtJyrNXQ5Rb25WlmVwCjgUu9f2QiKV83Av9Qz/G+Nx2BWWZ2VARl/EHRUOAzgB7eXgCJBDYUvRnuEGZmwBPAIufcPXWeehOo3RI9nsDceO345d7W7OOBsjq/7jY559yvnXMdnXP5BJbRR865S4GPgQsOka829wXe60O6Buec2wSsM7Oe3tBIYCERsgw9a4HjzSzZ+zuvzRgxy7GOhi63fwKnm1mG95vG6d5YSJjZmQSm9MY453YflHuctwdPF6AH8DVh/q475+Y559o55/K97816AjsqbCJCluFh+TX53sAND6MI7PWxArjdpwzDCfyKOhf4xvsZRWC+cyqwDPgQaOu93oCHvMzzgMIwZv0RB/ZC6Urgy7EceA1I8sZbeo+Xe893DVO2AqDIW47/ILAlP6KWIfAbYDGBywQ+R2BvCV+XI/ASgTn5SgJFc1VjlhuBuejl3s+VIc63nMB8ce335dE6r7/dy7cEOKvOeMi+6/VlPOj51RzYiBn2ZdiYHx1KLyISpaJhCkVEROqhAhcRiVIqcBGRKKUCFxGJUipwEZEopQIXEYlSKnARkSj1/wH0u3IsiKmIwAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 82.7524  # Value for C0\n",
    "K0 = -0.0031  # Value for K0\n",
    "K1 = -0.0003  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0171    # Value for b\n",
    "c = 3.0230    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    84.600000    84.431933    84.263866    84.095798    83.927731   \n",
      "351    84.431933    84.263866    84.095798    83.927731    83.759664   \n",
      "352    84.263866    84.095798    83.927731    83.759664    83.591597   \n",
      "353    84.095798    83.927731    83.759664    83.591597    83.423529   \n",
      "354    83.927731    83.759664    83.591597    83.423529    83.255462   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    83.759664    83.591597    83.423529    83.255462    83.092437  ...   \n",
      "351    83.591597    83.423529    83.255462    83.092437    82.991597  ...   \n",
      "352    83.423529    83.255462    83.092437    82.991597    82.890756  ...   \n",
      "353    83.255462    83.092437    82.991597    82.890756    82.789916  ...   \n",
      "354    83.092437    82.991597    82.890756    82.789916    82.689076  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   70.003688    0.000263   69.960738    0.000263   69.917787    0.000263   \n",
      "351   69.960738    0.000263   69.917787    0.000263   69.874837    0.000262   \n",
      "352   69.917787    0.000263   69.874837    0.000262   69.831886    0.000262   \n",
      "353   69.874837    0.000262   69.831886    0.000262   69.788936    0.000262   \n",
      "354   69.831886    0.000262   69.788936    0.000262   69.745985    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   69.874837    0.000262   69.831886    0.000262  \n",
      "351   69.831886    0.000262   69.788936    0.000262  \n",
      "352   69.788936    0.000262   69.745985    0.000262  \n",
      "353   69.745985    0.000262   69.703035    0.000262  \n",
      "354   69.703035    0.000262   69.660084    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050, 1, 251) (1050, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "14/14 [==============================] - 3s 52ms/step - loss: 4175.2056 - val_loss: 3172.8586\n",
      "Epoch 2/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 4124.9790 - val_loss: 3143.1226\n",
      "Epoch 3/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4091.5681 - val_loss: 3113.8081\n",
      "Epoch 4/500\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 4058.1140 - val_loss: 3073.6111\n",
      "Epoch 5/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 4011.7495 - val_loss: 3043.1050\n",
      "Epoch 6/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 3976.8867 - val_loss: 3012.7212\n",
      "Epoch 7/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3942.3589 - val_loss: 2982.7456\n",
      "Epoch 8/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3908.2627 - val_loss: 2953.1401\n",
      "Epoch 9/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3874.5471 - val_loss: 2923.8577\n",
      "Epoch 10/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3840.9492 - val_loss: 2893.6899\n",
      "Epoch 11/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3805.4475 - val_loss: 2862.2422\n",
      "Epoch 12/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 3769.5652 - val_loss: 2831.2502\n",
      "Epoch 13/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3734.2876 - val_loss: 2800.8252\n",
      "Epoch 14/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 3699.5947 - val_loss: 2770.8777\n",
      "Epoch 15/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 3665.3872 - val_loss: 2741.3328\n",
      "Epoch 16/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3631.5935 - val_loss: 2712.1404\n",
      "Epoch 17/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 3598.1646 - val_loss: 2683.2668\n",
      "Epoch 18/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3565.0667 - val_loss: 2653.2131\n",
      "Epoch 19/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3516.4963 - val_loss: 2609.8040\n",
      "Epoch 20/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 3479.1272 - val_loss: 2578.6748\n",
      "Epoch 21/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 3443.5393 - val_loss: 2548.2607\n",
      "Epoch 22/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 3401.6445 - val_loss: 2509.9651\n",
      "Epoch 23/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 3363.0693 - val_loss: 2477.3896\n",
      "Epoch 24/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 3325.8147 - val_loss: 2445.7356\n",
      "Epoch 25/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 3289.5391 - val_loss: 2414.8665\n",
      "Epoch 26/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 3254.0676 - val_loss: 2384.6357\n",
      "Epoch 27/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 3219.2556 - val_loss: 2354.9448\n",
      "Epoch 28/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 3185.0042 - val_loss: 2325.7251\n",
      "Epoch 29/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 3151.2456 - val_loss: 2296.9290\n",
      "Epoch 30/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 3117.9333 - val_loss: 2268.5217\n",
      "Epoch 31/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3085.0310 - val_loss: 2240.4766\n",
      "Epoch 32/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3052.5112 - val_loss: 2212.7729\n",
      "Epoch 33/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3020.3535 - val_loss: 2185.3950\n",
      "Epoch 34/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 2988.5381 - val_loss: 2158.3274\n",
      "Epoch 35/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2957.0532 - val_loss: 2131.5596\n",
      "Epoch 36/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 2925.8848 - val_loss: 2105.0813\n",
      "Epoch 37/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2895.0227 - val_loss: 2078.8843\n",
      "Epoch 38/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2864.4587 - val_loss: 2052.9609\n",
      "Epoch 39/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2834.1836 - val_loss: 2027.3049\n",
      "Epoch 40/500\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 2804.1914 - val_loss: 2001.9098\n",
      "Epoch 41/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2769.8381 - val_loss: 1966.5142\n",
      "Epoch 42/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2731.0349 - val_loss: 1937.7092\n",
      "Epoch 43/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2697.4482 - val_loss: 1909.6870\n",
      "Epoch 44/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2664.8086 - val_loss: 1882.4658\n",
      "Epoch 45/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2633.0022 - val_loss: 1855.8920\n",
      "Epoch 46/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2601.8660 - val_loss: 1829.8524\n",
      "Epoch 47/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2571.2883 - val_loss: 1804.2712\n",
      "Epoch 48/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2541.1941 - val_loss: 1779.0991\n",
      "Epoch 49/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2511.5334 - val_loss: 1754.3002\n",
      "Epoch 50/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2482.2690 - val_loss: 1729.8484\n",
      "Epoch 51/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2453.3748 - val_loss: 1705.7238\n",
      "Epoch 52/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2424.8281 - val_loss: 1681.9102\n",
      "Epoch 53/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2396.6145 - val_loss: 1658.3945\n",
      "Epoch 54/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2368.7180 - val_loss: 1635.1663\n",
      "Epoch 55/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2341.1282 - val_loss: 1612.2159\n",
      "Epoch 56/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2313.8352 - val_loss: 1589.5358\n",
      "Epoch 57/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2286.8293 - val_loss: 1567.1184\n",
      "Epoch 58/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2260.1045 - val_loss: 1544.9585\n",
      "Epoch 59/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2233.6528 - val_loss: 1523.0494\n",
      "Epoch 60/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2207.4690 - val_loss: 1501.3866\n",
      "Epoch 61/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2181.5481 - val_loss: 1479.9663\n",
      "Epoch 62/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 2155.8843 - val_loss: 1458.7831\n",
      "Epoch 63/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2130.4731 - val_loss: 1437.8336\n",
      "Epoch 64/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2105.3115 - val_loss: 1417.1146\n",
      "Epoch 65/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2080.3950 - val_loss: 1396.6223\n",
      "Epoch 66/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2055.7197 - val_loss: 1376.3538\n",
      "Epoch 67/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2031.2821 - val_loss: 1356.3059\n",
      "Epoch 68/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2007.0800 - val_loss: 1336.4767\n",
      "Epoch 69/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 1983.1099 - val_loss: 1316.8621\n",
      "Epoch 70/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1959.3683 - val_loss: 1297.4606\n",
      "Epoch 71/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1935.8533 - val_loss: 1278.2689\n",
      "Epoch 72/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1912.5627 - val_loss: 1259.2861\n",
      "Epoch 73/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1889.4930 - val_loss: 1240.5087\n",
      "Epoch 74/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1866.6427 - val_loss: 1221.9349\n",
      "Epoch 75/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1844.0093 - val_loss: 1203.5631\n",
      "Epoch 76/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1821.5903 - val_loss: 1185.3909\n",
      "Epoch 77/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1799.3839 - val_loss: 1167.4160\n",
      "Epoch 78/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1777.3879 - val_loss: 1149.6375\n",
      "Epoch 79/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1755.6010 - val_loss: 1132.0527\n",
      "Epoch 80/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1734.0204 - val_loss: 1114.6606\n",
      "Epoch 81/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1712.6445 - val_loss: 1097.4586\n",
      "Epoch 82/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1691.4719 - val_loss: 1080.4456\n",
      "Epoch 83/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1670.5002 - val_loss: 1063.6198\n",
      "Epoch 84/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1649.7288 - val_loss: 1046.9799\n",
      "Epoch 85/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1629.1549 - val_loss: 1030.5238\n",
      "Epoch 86/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1608.7772 - val_loss: 1014.2506\n",
      "Epoch 87/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1588.5940 - val_loss: 998.1578\n",
      "Epoch 88/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1568.6044 - val_loss: 982.2452\n",
      "Epoch 89/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1548.8064 - val_loss: 966.5103\n",
      "Epoch 90/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1529.1984 - val_loss: 950.9518\n",
      "Epoch 91/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1509.7787 - val_loss: 935.5688\n",
      "Epoch 92/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1490.5464 - val_loss: 920.3598\n",
      "Epoch 93/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1471.4999 - val_loss: 905.3229\n",
      "Epoch 94/500\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 1452.6377 - val_loss: 890.4573\n",
      "Epoch 95/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1433.9585 - val_loss: 875.7618\n",
      "Epoch 96/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1415.4608 - val_loss: 861.2344\n",
      "Epoch 97/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1397.1434 - val_loss: 846.8745\n",
      "Epoch 98/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1379.0055 - val_loss: 832.6807\n",
      "Epoch 99/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1361.0448 - val_loss: 818.6514\n",
      "Epoch 100/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1343.2606 - val_loss: 804.7853\n",
      "Epoch 101/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1325.6514 - val_loss: 791.0815\n",
      "Epoch 102/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1308.2161 - val_loss: 777.5386\n",
      "Epoch 103/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1287.4152 - val_loss: 757.4752\n",
      "Epoch 104/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1263.6085 - val_loss: 741.0842\n",
      "Epoch 105/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1242.8240 - val_loss: 725.3533\n",
      "Epoch 106/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1222.8654 - val_loss: 710.2571\n",
      "Epoch 107/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1203.6106 - val_loss: 695.6664\n",
      "Epoch 108/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1184.9120 - val_loss: 681.4873\n",
      "Epoch 109/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1166.6696 - val_loss: 667.6585\n",
      "Epoch 110/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1148.8175 - val_loss: 654.1396\n",
      "Epoch 111/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1131.3109 - val_loss: 640.9012\n",
      "Epoch 112/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1114.1182 - val_loss: 627.9231\n",
      "Epoch 113/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1097.2156 - val_loss: 615.1879\n",
      "Epoch 114/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1080.5844 - val_loss: 602.6833\n",
      "Epoch 115/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1064.2101 - val_loss: 590.3987\n",
      "Epoch 116/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1048.0807 - val_loss: 578.3250\n",
      "Epoch 117/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1032.1863 - val_loss: 566.4552\n",
      "Epoch 118/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1016.5181 - val_loss: 554.7823\n",
      "Epoch 119/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 1001.0690 - val_loss: 543.3012\n",
      "Epoch 120/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 985.8325 - val_loss: 532.0064\n",
      "Epoch 121/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 970.8027 - val_loss: 520.8937\n",
      "Epoch 122/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 955.9749 - val_loss: 509.9587\n",
      "Epoch 123/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 941.3441 - val_loss: 499.1985\n",
      "Epoch 124/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 926.9065 - val_loss: 488.6087\n",
      "Epoch 125/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 912.6578 - val_loss: 478.1865\n",
      "Epoch 126/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 898.5947 - val_loss: 467.9287\n",
      "Epoch 127/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 884.7141 - val_loss: 457.8331\n",
      "Epoch 128/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 871.0125 - val_loss: 447.8970\n",
      "Epoch 129/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 857.4874 - val_loss: 438.1176\n",
      "Epoch 130/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 844.1361 - val_loss: 428.4927\n",
      "Epoch 131/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 830.9558 - val_loss: 419.0202\n",
      "Epoch 132/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 817.9443 - val_loss: 409.6978\n",
      "Epoch 133/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 805.0989 - val_loss: 400.5236\n",
      "Epoch 134/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 792.4180 - val_loss: 391.4956\n",
      "Epoch 135/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 779.8987 - val_loss: 382.6116\n",
      "Epoch 136/500\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 767.5397 - val_loss: 373.8701\n",
      "Epoch 137/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 755.3386 - val_loss: 365.2693\n",
      "Epoch 138/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 743.2938 - val_loss: 356.8076\n",
      "Epoch 139/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 731.4032 - val_loss: 348.4829\n",
      "Epoch 140/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 719.6652 - val_loss: 340.2941\n",
      "Epoch 141/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 708.0778 - val_loss: 332.2393\n",
      "Epoch 142/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 696.6397 - val_loss: 324.3167\n",
      "Epoch 143/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 683.1895 - val_loss: 313.6808\n",
      "Epoch 144/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 668.9235 - val_loss: 304.0024\n",
      "Epoch 145/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 655.1865 - val_loss: 294.8195\n",
      "Epoch 146/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 642.0727 - val_loss: 286.0609\n",
      "Epoch 147/500\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 629.4683 - val_loss: 277.6464\n",
      "Epoch 148/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 617.2758 - val_loss: 269.5218\n",
      "Epoch 149/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 605.4294 - val_loss: 261.6502\n",
      "Epoch 150/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 593.8853 - val_loss: 254.0053\n",
      "Epoch 151/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 582.6123 - val_loss: 246.5697\n",
      "Epoch 152/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 571.5878 - val_loss: 239.3287\n",
      "Epoch 153/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 560.7950 - val_loss: 232.2715\n",
      "Epoch 154/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 550.2195 - val_loss: 225.3883\n",
      "Epoch 155/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 539.8502 - val_loss: 218.6723\n",
      "Epoch 156/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 529.6776 - val_loss: 212.1167\n",
      "Epoch 157/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 519.6939 - val_loss: 205.7152\n",
      "Epoch 158/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 509.8924 - val_loss: 199.4637\n",
      "Epoch 159/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 500.2668 - val_loss: 193.3576\n",
      "Epoch 160/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 490.8117 - val_loss: 187.3924\n",
      "Epoch 161/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 481.5226 - val_loss: 181.5648\n",
      "Epoch 162/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 472.3950 - val_loss: 175.8714\n",
      "Epoch 163/500\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 463.4248 - val_loss: 170.3090\n",
      "Epoch 164/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 454.6084 - val_loss: 164.8745\n",
      "Epoch 165/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 445.9422 - val_loss: 159.5657\n",
      "Epoch 166/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 437.4235 - val_loss: 154.3793\n",
      "Epoch 167/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 429.0489 - val_loss: 149.3135\n",
      "Epoch 168/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 420.8158 - val_loss: 144.3660\n",
      "Epoch 169/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 412.7215 - val_loss: 139.5339\n",
      "Epoch 170/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 404.7634 - val_loss: 134.8156\n",
      "Epoch 171/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 396.9391 - val_loss: 130.2088\n",
      "Epoch 172/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 389.2465 - val_loss: 125.7120\n",
      "Epoch 173/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 381.6835 - val_loss: 121.3226\n",
      "Epoch 174/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 374.2478 - val_loss: 117.0394\n",
      "Epoch 175/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 366.9375 - val_loss: 112.8601\n",
      "Epoch 176/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 359.7501 - val_loss: 108.7832\n",
      "Epoch 177/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 352.6844 - val_loss: 104.8071\n",
      "Epoch 178/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 345.7384 - val_loss: 100.9299\n",
      "Epoch 179/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 338.9101 - val_loss: 97.1506\n",
      "Epoch 180/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 332.1981 - val_loss: 93.4668\n",
      "Epoch 181/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 325.6004 - val_loss: 89.8776\n",
      "Epoch 182/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 319.1155 - val_loss: 86.3810\n",
      "Epoch 183/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 312.7419 - val_loss: 82.9761\n",
      "Epoch 184/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 306.4779 - val_loss: 79.6609\n",
      "Epoch 185/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 300.3219 - val_loss: 76.4342\n",
      "Epoch 186/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 294.2725 - val_loss: 73.2947\n",
      "Epoch 187/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 288.3283 - val_loss: 70.2409\n",
      "Epoch 188/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 282.4878 - val_loss: 67.2714\n",
      "Epoch 189/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 276.7495 - val_loss: 64.3851\n",
      "Epoch 190/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 271.1121 - val_loss: 61.5802\n",
      "Epoch 191/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 265.5742 - val_loss: 58.8560\n",
      "Epoch 192/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 260.1343 - val_loss: 56.2109\n",
      "Epoch 193/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 254.7914 - val_loss: 53.6438\n",
      "Epoch 194/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 249.5440 - val_loss: 51.1532\n",
      "Epoch 195/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 244.3908 - val_loss: 48.7379\n",
      "Epoch 196/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 239.3305 - val_loss: 46.3968\n",
      "Epoch 197/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 234.3618 - val_loss: 44.1287\n",
      "Epoch 198/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 229.4839 - val_loss: 41.9324\n",
      "Epoch 199/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 224.6951 - val_loss: 39.8068\n",
      "Epoch 200/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 219.9944 - val_loss: 37.7505\n",
      "Epoch 201/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 215.3806 - val_loss: 35.7626\n",
      "Epoch 202/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 210.8525 - val_loss: 33.8418\n",
      "Epoch 203/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 206.4089 - val_loss: 31.9870\n",
      "Epoch 204/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 202.0488 - val_loss: 30.1970\n",
      "Epoch 205/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 197.7706 - val_loss: 28.4709\n",
      "Epoch 206/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 193.5738 - val_loss: 26.8074\n",
      "Epoch 207/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 189.4569 - val_loss: 25.2055\n",
      "Epoch 208/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 185.4189 - val_loss: 23.6642\n",
      "Epoch 209/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 181.4588 - val_loss: 22.1822\n",
      "Epoch 210/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 177.5754 - val_loss: 20.7586\n",
      "Epoch 211/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 173.7676 - val_loss: 19.3924\n",
      "Epoch 212/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 170.0343 - val_loss: 18.0824\n",
      "Epoch 213/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 166.3747 - val_loss: 16.8276\n",
      "Epoch 214/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 162.7878 - val_loss: 15.6271\n",
      "Epoch 215/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 159.2722 - val_loss: 14.4798\n",
      "Epoch 216/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 155.8270 - val_loss: 13.3847\n",
      "Epoch 217/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 152.4512 - val_loss: 12.3407\n",
      "Epoch 218/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 149.1440 - val_loss: 11.3471\n",
      "Epoch 219/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 145.9041 - val_loss: 10.4026\n",
      "Epoch 220/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 142.7307 - val_loss: 9.5063\n",
      "Epoch 221/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 139.6227 - val_loss: 8.6573\n",
      "Epoch 222/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 136.5793 - val_loss: 7.8547\n",
      "Epoch 223/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 133.5995 - val_loss: 7.0973\n",
      "Epoch 224/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 130.6821 - val_loss: 6.3845\n",
      "Epoch 225/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 127.8265 - val_loss: 5.7152\n",
      "Epoch 226/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 125.0316 - val_loss: 5.0885\n",
      "Epoch 227/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 122.2964 - val_loss: 4.5033\n",
      "Epoch 228/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 119.6202 - val_loss: 3.9590\n",
      "Epoch 229/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 117.0018 - val_loss: 3.4545\n",
      "Epoch 230/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 114.4407 - val_loss: 2.9890\n",
      "Epoch 231/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 111.9356 - val_loss: 2.5615\n",
      "Epoch 232/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 109.4857 - val_loss: 2.1712\n",
      "Epoch 233/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 107.0902 - val_loss: 1.8172\n",
      "Epoch 234/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 104.7482 - val_loss: 1.4987\n",
      "Epoch 235/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 102.4588 - val_loss: 1.2148\n",
      "Epoch 236/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 100.2214 - val_loss: 0.9647\n",
      "Epoch 237/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 98.0347 - val_loss: 0.7475\n",
      "Epoch 238/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 95.8982 - val_loss: 0.5624\n",
      "Epoch 239/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 93.8109 - val_loss: 0.4086\n",
      "Epoch 240/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 91.7719 - val_loss: 0.2852\n",
      "Epoch 241/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 89.7805 - val_loss: 0.1915\n",
      "Epoch 242/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 87.8359 - val_loss: 0.1266\n",
      "Epoch 243/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 85.9373 - val_loss: 0.0899\n",
      "Epoch 244/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 84.0840 - val_loss: 0.0804\n",
      "Epoch 245/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 82.2749 - val_loss: 0.0974\n",
      "Epoch 246/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 80.5094 - val_loss: 0.1402\n",
      "Epoch 247/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 78.7868 - val_loss: 0.2081\n",
      "Epoch 248/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 77.1061 - val_loss: 0.3001\n",
      "Epoch 249/500\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 75.4667 - val_loss: 0.4158\n",
      "Epoch 250/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 73.8678 - val_loss: 0.5541\n",
      "Epoch 251/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 72.3089 - val_loss: 0.7146\n",
      "Epoch 252/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 70.7889 - val_loss: 0.8964\n",
      "Epoch 253/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 69.3073 - val_loss: 1.0989\n",
      "Epoch 254/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 67.8632 - val_loss: 1.3214\n",
      "Epoch 255/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 66.4560 - val_loss: 1.5631\n",
      "Epoch 256/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 65.0850 - val_loss: 1.8235\n",
      "Epoch 257/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 63.7494 - val_loss: 2.1018\n",
      "Epoch 258/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 62.4485 - val_loss: 2.3973\n",
      "Epoch 259/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 61.1818 - val_loss: 2.7095\n",
      "Epoch 260/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 59.9485 - val_loss: 3.0377\n",
      "Epoch 261/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 58.7477 - val_loss: 3.3812\n",
      "Epoch 262/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 57.5791 - val_loss: 3.7395\n",
      "Epoch 263/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 56.4420 - val_loss: 4.1119\n",
      "Epoch 264/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 55.3355 - val_loss: 4.4978\n",
      "Epoch 265/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 54.2592 - val_loss: 4.8966\n",
      "Epoch 266/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 53.2124 - val_loss: 5.3078\n",
      "Epoch 267/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 52.1944 - val_loss: 5.7308\n",
      "Epoch 268/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 51.2046 - val_loss: 6.1650\n",
      "Epoch 269/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 50.2425 - val_loss: 6.6097\n",
      "Epoch 270/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 49.3075 - val_loss: 7.0646\n",
      "Epoch 271/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 48.3990 - val_loss: 7.5291\n",
      "Epoch 272/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 47.5162 - val_loss: 8.0025\n",
      "Epoch 273/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 46.6588 - val_loss: 8.4846\n",
      "Epoch 274/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 45.8260 - val_loss: 8.9745\n",
      "Epoch 275/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 45.0174 - val_loss: 9.4720\n",
      "Epoch 276/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 44.2324 - val_loss: 9.9764\n",
      "Epoch 277/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 43.4705 - val_loss: 10.4875\n",
      "Epoch 278/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 42.7311 - val_loss: 11.0045\n",
      "Epoch 279/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 42.0136 - val_loss: 11.5272\n",
      "Epoch 280/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 41.3176 - val_loss: 12.0550\n",
      "Epoch 281/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 40.6426 - val_loss: 12.5876\n",
      "Epoch 282/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 39.9881 - val_loss: 13.1243\n",
      "Epoch 283/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 39.3535 - val_loss: 13.6649\n",
      "Epoch 284/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 38.7383 - val_loss: 14.2090\n",
      "Epoch 285/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 38.1421 - val_loss: 14.7561\n",
      "Epoch 286/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 37.5645 - val_loss: 15.3057\n",
      "Epoch 287/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 37.0049 - val_loss: 15.8576\n",
      "Epoch 288/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 36.4628 - val_loss: 16.4113\n",
      "Epoch 289/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 35.9380 - val_loss: 16.9667\n",
      "Epoch 290/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 35.4297 - val_loss: 17.5231\n",
      "Epoch 291/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 34.9378 - val_loss: 18.0803\n",
      "Epoch 292/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 34.4616 - val_loss: 18.6381\n",
      "Epoch 293/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 34.0008 - val_loss: 19.1960\n",
      "Epoch 294/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 33.5550 - val_loss: 19.7536\n",
      "Epoch 295/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 33.1239 - val_loss: 20.3108\n",
      "Epoch 296/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 32.7069 - val_loss: 20.8670\n",
      "Epoch 297/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 32.3037 - val_loss: 21.4223\n",
      "Epoch 298/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 31.9140 - val_loss: 21.9762\n",
      "Epoch 299/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 31.5373 - val_loss: 22.5282\n",
      "Epoch 300/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 31.1734 - val_loss: 23.0786\n",
      "Epoch 301/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 30.8217 - val_loss: 23.6266\n",
      "Epoch 302/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 30.4820 - val_loss: 24.1722\n",
      "Epoch 303/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 30.1540 - val_loss: 24.7154\n",
      "Epoch 304/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 29.8372 - val_loss: 25.2553\n",
      "Epoch 305/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 29.5313 - val_loss: 25.7924\n",
      "Epoch 306/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 29.2361 - val_loss: 26.3261\n",
      "Epoch 307/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 28.9513 - val_loss: 26.8562\n",
      "Epoch 308/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 28.6764 - val_loss: 27.3826\n",
      "Epoch 309/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 28.4113 - val_loss: 27.9052\n",
      "Epoch 310/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 28.1555 - val_loss: 28.4235\n",
      "Epoch 311/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 27.9089 - val_loss: 28.9377\n",
      "Epoch 312/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 27.6711 - val_loss: 29.4474\n",
      "Epoch 313/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 27.4419 - val_loss: 29.9524\n",
      "Epoch 314/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 27.2211 - val_loss: 30.4529\n",
      "Epoch 315/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 27.0081 - val_loss: 30.9484\n",
      "Epoch 316/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 26.8030 - val_loss: 31.4389\n",
      "Epoch 317/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 26.6055 - val_loss: 31.9244\n",
      "Epoch 318/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 26.4152 - val_loss: 32.4045\n",
      "Epoch 319/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 26.2319 - val_loss: 32.8793\n",
      "Epoch 320/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 26.0556 - val_loss: 33.3484\n",
      "Epoch 321/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 25.8858 - val_loss: 33.8122\n",
      "Epoch 322/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 25.7224 - val_loss: 34.2703\n",
      "Epoch 323/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 25.5650 - val_loss: 34.7228\n",
      "Epoch 324/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 25.4137 - val_loss: 35.1694\n",
      "Epoch 325/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 25.2681 - val_loss: 35.6100\n",
      "Epoch 326/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 25.1281 - val_loss: 36.0447\n",
      "Epoch 327/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 24.9934 - val_loss: 36.4735\n",
      "Epoch 328/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 24.8639 - val_loss: 36.8960\n",
      "Epoch 329/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 24.7395 - val_loss: 37.3126\n",
      "Epoch 330/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 24.6198 - val_loss: 37.7229\n",
      "Epoch 331/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 24.5048 - val_loss: 38.1272\n",
      "Epoch 332/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 24.3943 - val_loss: 38.5251\n",
      "Epoch 333/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 24.2881 - val_loss: 38.9167\n",
      "Epoch 334/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 24.1861 - val_loss: 39.3021\n",
      "Epoch 335/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 24.0881 - val_loss: 39.6814\n",
      "Epoch 336/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 23.9940 - val_loss: 40.0542\n",
      "Epoch 337/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 23.9036 - val_loss: 40.4206\n",
      "Epoch 338/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 23.8168 - val_loss: 40.7810\n",
      "Epoch 339/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 23.7335 - val_loss: 41.1350\n",
      "Epoch 340/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 23.6535 - val_loss: 41.4824\n",
      "Epoch 341/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 23.5767 - val_loss: 41.8239\n",
      "Epoch 342/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 23.5030 - val_loss: 42.1590\n",
      "Epoch 343/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 23.4323 - val_loss: 42.4879\n",
      "Epoch 344/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 23.3645 - val_loss: 42.8105\n",
      "Epoch 345/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 23.2994 - val_loss: 43.1268\n",
      "Epoch 346/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 23.2369 - val_loss: 43.4372\n",
      "Epoch 347/500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 23.1770 - val_loss: 43.7413\n",
      "Epoch 348/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 23.1195 - val_loss: 44.0397\n",
      "Epoch 349/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 23.0644 - val_loss: 44.3315\n",
      "Epoch 350/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 23.0116 - val_loss: 44.6176\n",
      "Epoch 351/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.9609 - val_loss: 44.8975\n",
      "Epoch 352/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 22.9123 - val_loss: 45.1719\n",
      "Epoch 353/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.8656 - val_loss: 45.4401\n",
      "Epoch 354/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.8210 - val_loss: 45.7025\n",
      "Epoch 355/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.7781 - val_loss: 45.9594\n",
      "Epoch 356/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.7371 - val_loss: 46.2102\n",
      "Epoch 357/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.6977 - val_loss: 46.4557\n",
      "Epoch 358/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.6600 - val_loss: 46.6955\n",
      "Epoch 359/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.6238 - val_loss: 46.9299\n",
      "Epoch 360/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 22.5891 - val_loss: 47.1588\n",
      "Epoch 361/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.5559 - val_loss: 47.3823\n",
      "Epoch 362/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 22.5241 - val_loss: 47.6004\n",
      "Epoch 363/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 22.4936 - val_loss: 47.8134\n",
      "Epoch 364/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.4643 - val_loss: 48.0213\n",
      "Epoch 365/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.4363 - val_loss: 48.2241\n",
      "Epoch 366/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.4095 - val_loss: 48.4215\n",
      "Epoch 367/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 22.3838 - val_loss: 48.6144\n",
      "Epoch 368/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.3591 - val_loss: 48.8021\n",
      "Epoch 369/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 22.3355 - val_loss: 48.9853\n",
      "Epoch 370/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.3129 - val_loss: 49.1637\n",
      "Epoch 371/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.2912 - val_loss: 49.3374\n",
      "Epoch 372/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 22.2704 - val_loss: 49.5066\n",
      "Epoch 373/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 22.2505 - val_loss: 49.6712\n",
      "Epoch 374/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.2314 - val_loss: 49.8314\n",
      "Epoch 375/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.2131 - val_loss: 49.9872\n",
      "Epoch 376/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.1957 - val_loss: 50.1391\n",
      "Epoch 377/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 22.1789 - val_loss: 50.2865\n",
      "Epoch 378/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.1628 - val_loss: 50.4300\n",
      "Epoch 379/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 22.1473 - val_loss: 50.5691\n",
      "Epoch 380/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 22.1326 - val_loss: 50.7045\n",
      "Epoch 381/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.1185 - val_loss: 50.8361\n",
      "Epoch 382/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 22.1050 - val_loss: 50.9638\n",
      "Epoch 383/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 22.0920 - val_loss: 51.0878\n",
      "Epoch 384/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 22.0796 - val_loss: 51.2082\n",
      "Epoch 385/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.0677 - val_loss: 51.3251\n",
      "Epoch 386/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 22.0563 - val_loss: 51.4385\n",
      "Epoch 387/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 22.0454 - val_loss: 51.5484\n",
      "Epoch 388/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 22.0349 - val_loss: 51.6549\n",
      "Epoch 389/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 22.0249 - val_loss: 51.7584\n",
      "Epoch 390/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.0153 - val_loss: 51.8586\n",
      "Epoch 391/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 22.0061 - val_loss: 51.9558\n",
      "Epoch 392/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.9973 - val_loss: 52.0496\n",
      "Epoch 393/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.9888 - val_loss: 52.1407\n",
      "Epoch 394/500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 21.9808 - val_loss: 52.2289\n",
      "Epoch 395/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.9730 - val_loss: 52.3144\n",
      "Epoch 396/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.9656 - val_loss: 52.3969\n",
      "Epoch 397/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.9585 - val_loss: 52.4770\n",
      "Epoch 398/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 21.9517 - val_loss: 52.5543\n",
      "Epoch 399/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.9452 - val_loss: 52.6292\n",
      "Epoch 400/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.9390 - val_loss: 52.7013\n",
      "Epoch 401/500\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 21.9330 - val_loss: 52.7712\n",
      "Epoch 402/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.9273 - val_loss: 52.8388\n",
      "Epoch 403/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 21.9218 - val_loss: 52.9041\n",
      "Epoch 404/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.9166 - val_loss: 52.9671\n",
      "Epoch 405/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 21.9116 - val_loss: 53.0281\n",
      "Epoch 406/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.9068 - val_loss: 53.0868\n",
      "Epoch 407/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.9022 - val_loss: 53.1436\n",
      "Epoch 408/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8978 - val_loss: 53.1982\n",
      "Epoch 409/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8936 - val_loss: 53.2511\n",
      "Epoch 410/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8895 - val_loss: 53.3018\n",
      "Epoch 411/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8857 - val_loss: 53.3509\n",
      "Epoch 412/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8820 - val_loss: 53.3982\n",
      "Epoch 413/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 21.8785 - val_loss: 53.4437\n",
      "Epoch 414/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 21.8751 - val_loss: 53.4879\n",
      "Epoch 415/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8719 - val_loss: 53.5301\n",
      "Epoch 416/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8689 - val_loss: 53.5710\n",
      "Epoch 417/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8659 - val_loss: 53.6104\n",
      "Epoch 418/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8631 - val_loss: 53.6481\n",
      "Epoch 419/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 21.8604 - val_loss: 53.6845\n",
      "Epoch 420/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 21.8578 - val_loss: 53.7195\n",
      "Epoch 421/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 21.8554 - val_loss: 53.7530\n",
      "Epoch 422/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8530 - val_loss: 53.7856\n",
      "Epoch 423/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 21.8508 - val_loss: 53.8165\n",
      "Epoch 424/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 21.8487 - val_loss: 53.8465\n",
      "Epoch 425/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 21.8466 - val_loss: 53.8752\n",
      "Epoch 426/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8447 - val_loss: 53.9028\n",
      "Epoch 427/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8429 - val_loss: 53.9292\n",
      "Epoch 428/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8411 - val_loss: 53.9547\n",
      "Epoch 429/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 21.8394 - val_loss: 53.9789\n",
      "Epoch 430/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8379 - val_loss: 54.0024\n",
      "Epoch 431/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 21.8364 - val_loss: 54.0248\n",
      "Epoch 432/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8349 - val_loss: 54.0464\n",
      "Epoch 433/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8335 - val_loss: 54.0669\n",
      "Epoch 434/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 21.8322 - val_loss: 54.0869\n",
      "Epoch 435/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 21.8310 - val_loss: 54.1059\n",
      "Epoch 436/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 21.8298 - val_loss: 54.1242\n",
      "Epoch 437/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 21.8287 - val_loss: 54.1415\n",
      "Epoch 438/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8276 - val_loss: 54.1584\n",
      "Epoch 439/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8266 - val_loss: 54.1742\n",
      "Epoch 440/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8257 - val_loss: 54.1896\n",
      "Epoch 441/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8247 - val_loss: 54.2041\n",
      "Epoch 442/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8239 - val_loss: 54.2181\n",
      "Epoch 443/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8231 - val_loss: 54.2314\n",
      "Epoch 444/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8223 - val_loss: 54.2443\n",
      "Epoch 445/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8216 - val_loss: 54.2566\n",
      "Epoch 446/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8209 - val_loss: 54.2683\n",
      "Epoch 447/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8203 - val_loss: 54.2794\n",
      "Epoch 448/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 21.8196 - val_loss: 54.2901\n",
      "Epoch 449/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 21.8191 - val_loss: 54.3002\n",
      "Epoch 450/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 21.8186 - val_loss: 54.3098\n",
      "Epoch 451/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 21.8181 - val_loss: 54.3192\n",
      "Epoch 452/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8176 - val_loss: 54.3278\n",
      "Epoch 453/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 21.8172 - val_loss: 54.3365\n",
      "Epoch 454/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 21.8167 - val_loss: 54.3446\n",
      "Epoch 455/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 21.8164 - val_loss: 54.3520\n",
      "Epoch 456/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 21.8160 - val_loss: 54.3594\n",
      "Epoch 457/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 21.8157 - val_loss: 54.3663\n",
      "Epoch 458/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8154 - val_loss: 54.3730\n",
      "Epoch 459/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 21.8151 - val_loss: 54.3793\n",
      "Epoch 460/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8148 - val_loss: 54.3851\n",
      "Epoch 461/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 21.8146 - val_loss: 54.3910\n",
      "Epoch 462/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 21.8143 - val_loss: 54.3962\n",
      "Epoch 463/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 21.8141 - val_loss: 54.4013\n",
      "Epoch 464/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8140 - val_loss: 54.4065\n",
      "Epoch 465/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 21.8138 - val_loss: 54.4109\n",
      "Epoch 466/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8136 - val_loss: 54.4154\n",
      "Epoch 467/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8135 - val_loss: 54.4197\n",
      "Epoch 468/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 21.8133 - val_loss: 54.4234\n",
      "Epoch 469/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 21.8133 - val_loss: 54.4272\n",
      "Epoch 470/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 21.8132 - val_loss: 54.4308\n",
      "Epoch 471/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 21.8131 - val_loss: 54.4341\n",
      "Epoch 472/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 21.8130 - val_loss: 54.4373\n",
      "Epoch 473/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 21.8130 - val_loss: 54.4404\n",
      "Epoch 474/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 21.8129 - val_loss: 54.4433\n",
      "Epoch 475/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8129 - val_loss: 54.4460\n",
      "Epoch 476/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 21.8129 - val_loss: 54.4485\n",
      "Epoch 477/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 21.8129 - val_loss: 54.4511\n",
      "Epoch 478/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 21.8128 - val_loss: 54.4532\n",
      "Epoch 479/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 21.8129 - val_loss: 54.4553\n",
      "Epoch 480/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8128 - val_loss: 54.4574\n",
      "Epoch 481/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8129 - val_loss: 54.4592\n",
      "Epoch 482/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 21.8129 - val_loss: 54.4610\n",
      "Epoch 483/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8130 - val_loss: 54.4625\n",
      "Epoch 484/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 21.8130 - val_loss: 54.4643\n",
      "Epoch 485/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8131 - val_loss: 54.4658\n",
      "Epoch 486/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8131 - val_loss: 54.4674\n",
      "Epoch 487/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 21.8132 - val_loss: 54.4686\n",
      "Epoch 488/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 21.8132 - val_loss: 54.4697\n",
      "Epoch 489/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8133 - val_loss: 54.4709\n",
      "Epoch 490/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 21.8134 - val_loss: 54.4720\n",
      "Epoch 491/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8134 - val_loss: 54.4731\n",
      "Epoch 492/500\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 21.8135 - val_loss: 54.4739\n",
      "Epoch 493/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8136 - val_loss: 54.4749\n",
      "Epoch 494/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8137 - val_loss: 54.4755\n",
      "Epoch 495/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8138 - val_loss: 54.4765\n",
      "Epoch 496/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8139 - val_loss: 54.4772\n",
      "Epoch 497/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.8140 - val_loss: 54.4779\n",
      "Epoch 498/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 21.8141 - val_loss: 54.4785\n",
      "Epoch 499/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 21.8142 - val_loss: 54.4790\n",
      "Epoch 500/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 21.8143 - val_loss: 54.4799\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(82.7524, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0031, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0003, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0171, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(3.0230, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 382ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[58.2200747 , 58.19101307, 58.17700747, 58.16300187, 58.14899627,\n",
       "        58.13499066, 58.12098506, 58.10697946, 58.09297386, 58.07896825,\n",
       "        58.06496265, 58.05095705, 58.03695145, 58.02294585, 58.00894024,\n",
       "        57.99493464, 57.98092904, 57.96692344, 57.95291783, 57.93891223,\n",
       "        57.92490663, 57.91090103, 57.89689542, 57.88288982, 57.86888422,\n",
       "        57.85487862, 57.84087302, 57.82686741, 57.81286181, 57.79885621,\n",
       "        57.78485061, 57.770845  , 57.7568394 , 57.7428338 , 57.7288282 ,\n",
       "        57.7148226 , 57.70081699, 57.68681139, 57.67280579, 57.65880019,\n",
       "        57.64479458, 57.63078898, 57.61678338, 57.60277778, 57.58877218,\n",
       "        57.57476657, 57.56076097, 57.54675537, 57.53274977, 57.51874416,\n",
       "        57.50473856, 57.49073296, 57.47672736, 57.46272176, 57.44871615,\n",
       "        57.43471055, 57.42070495, 57.40669935, 57.39269374, 57.37868814,\n",
       "        57.36468254, 57.35067694, 57.33667134, 57.32266573, 57.30866013,\n",
       "        57.29465453, 57.28064893, 57.26664332, 57.25263772, 57.23863212,\n",
       "        57.22462652, 57.21062092, 57.19661531, 57.18260971, 57.16860411,\n",
       "        57.15459851, 57.1405929 , 57.1265873 , 57.1125817 , 57.0985761 ,\n",
       "        64.95271301,  0.        ,  0.        ,  0.66271698,  0.        ,\n",
       "         0.        ,  1.04276955,  0.65397727,  0.17152965,  0.        ,\n",
       "         0.24789816,  0.44096228,  0.13191691,  0.        ,  0.79818988,\n",
       "         0.        ,  0.2817767 ,  0.20957324,  0.85314274,  0.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([55.5377451 , 55.53120915, 55.5246732 , 55.51813725, 55.51160131,\n",
       "       55.50506536, 55.49852941, 55.49199346, 55.48545752, 55.47892157,\n",
       "       55.47238562, 55.46584967, 55.45931373, 55.45277778, 55.44624183,\n",
       "       55.43970588, 55.43316993, 55.42663399, 55.42009804, 55.41356209,\n",
       "       55.40702614, 55.4004902 , 55.39395425, 55.3874183 , 55.38088235,\n",
       "       55.37434641, 55.36781046, 55.36127451, 55.35473856, 55.34820261,\n",
       "       55.34166667, 55.33513072, 55.32859477, 55.32205882, 55.31552288,\n",
       "       55.30898693, 55.30245098, 55.29591503, 55.28937908, 55.28284314,\n",
       "       55.27630719, 55.26977124, 55.26323529, 55.25669935, 55.2501634 ,\n",
       "       55.24362745, 55.2370915 , 55.23055556, 55.22401961, 55.21748366,\n",
       "       55.21094771, 55.20441176, 55.19787582, 55.19133987, 55.18480392,\n",
       "       55.17826797, 55.17173203, 55.16519608, 55.15866013, 55.15212418,\n",
       "       55.14558824, 55.13905229, 55.13251634, 55.12598039, 55.11944444,\n",
       "       55.1129085 , 55.10637255, 55.0998366 , 55.09330065, 55.08676471,\n",
       "       55.08022876, 55.07369281, 55.06715686, 55.06062092, 55.05408497,\n",
       "       55.04754902, 55.04101307, 55.03447712, 55.02794118, 55.02140523,\n",
       "       55.01486928, 55.00833333, 55.00179739, 54.99526144, 54.98872549,\n",
       "       54.98218954, 54.97565359, 54.96911765, 54.9625817 , 54.95604575,\n",
       "       54.9495098 , 54.94297386, 54.93643791, 54.92990196, 54.92336601,\n",
       "       54.91683007, 54.91029412, 54.90375817, 54.89761905, 54.89201681])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.941825366863444\n",
      "12.382786378221242\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
