{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "1595    67.812021\n",
       "1596    67.807353\n",
       "1597    67.802684\n",
       "1598    67.796032\n",
       "1599    67.786695\n",
       "Name: C2, Length: 1600, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c2_interpolated_1500_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "1495    68.278875\n",
       "1496    68.274206\n",
       "1497    68.269538\n",
       "1498    68.264869\n",
       "1499    68.260201\n",
       "Name: C2, Length: 1500, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1500)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.500000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.275910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.051821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.827731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.603641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     90.500000  0.000298\n",
       "1     90.275910  0.000298\n",
       "2     90.051821  0.000297\n",
       "3     89.827731  0.000297\n",
       "4     89.603641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAAsTAAALEwEAmpwYAAAk5klEQVR4nO3deXxU9b3/8dcnewghgSRsCRB2RZEtouxYBaxaUasVtYpe3EVtuXprf97eVuu9bdW6I0q1St1wr4gLghJEQCTsm0ACGBK2sAWQNeT7+2MONKQJWyY5k8z7+XjMI2fOOcO8c3TynrObcw4REQlfEX4HEBERf6kIRETCnIpARCTMqQhERMKcikBEJMxF+R3gVKSmprrMzEy/Y4iI1Cpz587d4pxLKz++VhZBZmYmOTk5fscQEalVzOyHisZr05CISJhTEYiIhDkVgYhImFMRiIiEORWBiEiYUxGIiIQ5FYGISJgLqyIYN3MtExau9zuGiEhICasieOu7fCYsKPQ7hohISAmrIkhLjKVo9wG/Y4iIhJTwKoL6sWzZtd/vGCIiISW8iiAxlqLd+9HtOUVE/iUoRWBmF5rZCjPLNbMHKpje38zmmVmJmV1ZbtohM1vgPSYEI09lUuvHcqCklF37S6rzbUREapUqX33UzCKB0cAgoACYY2YTnHPLysyWD9wI3FfBP7HXOde1qjlORGpiDABFu/bTIC66Jt5SRCTkBWONoCeQ65xb7Zw7AIwHhpadwTm31jm3CCgNwvudsrT6cQDaTyAiUkYwiiAdWFfmeYE37kTFmVmOmX1rZpdVNpOZ3erNl1NUVHRKQY+sEexWEYiIHBYKO4tbOeeygGuBp8ysbUUzOefGOueynHNZaWn/doOdE5JWPxbQGoGISFnBKIJCoEWZ5xneuBPinCv0fq4GsoFuQchUoYb1YoiMMK0RiIiUEYwimAO0N7PWZhYDDANO6OgfM2toZrHecCrQB1h27FeduogIIyUhhi27dFKZiMhhVS4C51wJMBKYBCwH3nHOLTWzh83sUgAzO9vMCoCrgBfNbKn38tOBHDNbCEwF/lzuaKOgS60fyxatEYiIHBGUm9c75z4FPi037n/KDM8hsMmo/OtmAp2DkeFEHT6pTEREAkJhZ3GNStVlJkREjhJ2RZCWGMuW3Qd0mQkREU/YFUFq/RgOHCpl515dZkJEBMKwCNISA+cSaD+BiEhA+BWBd1JZkfYTiIgA4VgE3hqBDiEVEQkIuyJI1RqBiMhRwq4IkutFkxQfzbINO/2OIiISEsKuCMyM8zqm8eXyTZQc8vWq2CIiISHsigBg8BlN2b7nIHN/2O53FBER34VlEfTvkEZMZASTl23yO4qIiO/Csgjqx0bRu10KXyzbpDOMRSTshWURAAzu1JT8bXtYuWm331FERHwVtkVwwemNAfhi6Uafk4iI+Ctsi6Bxgzi6tUxm8nLtJxCR8Ba2RQAwqFMTFhUUs6F4r99RRER8E9ZFMLhTUwCm6OghEQljYV0E7RrXp01qAl+oCEQkjIV1EUBg89CsvK269pCIhK2wL4KrsjKIiDBGvbOAQ6U6p0BEwk/YF0G7xok8dOkZTF+1hdFTc/2OIyJS48K+CACGnd2Cy7ul8+SUlczI3eJ3HBGRGqUiIHBF0kcuO5M2qQncO34+m3fu8zuSiEiNURF4EmKjGPPLHuzeX8Ldb83XJapFJGyoCMro0CSRRy7rzOw123hqyiq/44iI1AgVQTlX9sjg6qwWPDc1l6krNvsdR0Sk2qkIKvDQ0DM4rWkio95ewPoduvyEiNRtKoIKxEVHMvq67hwoKWXkm/M4qP0FIlKHqQgq0TatPn/++VnMy9/BY5NW+B1HRKTaBKUIzOxCM1thZrlm9kAF0/ub2TwzKzGzK8tNG25mq7zH8GDkCZafdWnO9ee2YuzXq/ls8Qa/44iIVIsqF4GZRQKjgZ8CnYBrzKxTudnygRuBN8u9thHwe+AcoCfwezNrWNVMwfTfl5xOt5bJjHpnIcvW7/Q7johI0AVjjaAnkOucW+2cOwCMB4aWncE5t9Y5twgov7F9CDDZObfNObcdmAxcGIRMQRMbFcmLv+xBg/gobvlHDlt36+J0IlK3BKMI0oF1ZZ4XeOOC+lozu9XMcswsp6io6JSCnqrGDeIYe30WW3bv54435nGgRDuPRaTuqDU7i51zY51zWc65rLS0tBp//y4tknn0yrP4bs02Hvp4aY2/v4hIdQlGERQCLco8z/DGVfdra9zQruncPqAtb8zO57Vvf/A7johIUASjCOYA7c2stZnFAMOACSf42knAYDNr6O0kHuyNC1n3D+nIT05rzEMTljIrb6vfcUREqqzKReCcKwFGEvgDvhx4xzm31MweNrNLAczsbDMrAK4CXjSzpd5rtwF/JFAmc4CHvXEhKzLCeGpYV1ql1OPON+aybtsevyOJiFSJOVf77sqVlZXlcnJyfM2wZsuPDH3uG05r2oB3bu/laxYRkRNhZnOdc1nlx9eancWhpnVqAvde0IHv1m5jcUGx33FERE6ZiqAKrsrKoF5MJK/OXOt3FBGRU6YiqIIGcdH8vHsGHy9czxadaCYitZSKoIqG927FgUOljP8u3+8oIiKnREVQRe0aJ9KvfSqvf5uvy1WLSK2kIgiC4b0y2bhzH5OWbvQ7iojISVMRBMF5pzWmZaN6jNNOYxGphVQEQRAZYdzQqxVz1m5nSaEOJRWR2kVFECRXZbUgPjpSawUiUuuoCIIkKT6aK7qn89HC9Wz78YDfcURETpiKIIiG987kQEkp4+foUFIRqT1UBEHUoUkifdql8PqsHyjRoaQiUkuoCIJseK9M1hfvY/KyTX5HERE5ISqCIDv/9CZkNIzX9YdEpNZQEQTZ4UNJZ6/ZxvINO/2OIyJyXCqCavCLrBbERUfw1y9WsGDdDvYdPOR3JBGRSkX5HaAuSq4Xw019WjMmO48pyzcTFWF0aJJI5/QkzsxI4qz0JDo2TSQuOtLvqCIiukNZdXHOUbB9L0sKi1lc5rFjz0GAo8qhc0YSndOTOK1ZIrFRKgcRqR6V3aFMRVCDypbDosLiIyVRthw6Nk3krIwkbuzdmo5NE31OLCJ1iYogRB0uh8NrDEsKi5mfvwPnHM9e242fnNbE74giUkeoCGqRTTv3MWLcHJat38nvLunEjb0zMTO/Y4lILaeb19ciTRrE8c5tvbjg9CY89PEy/uejpTpTWUSqjYogRNWLieKFX/bg1v5teO3bHxgxLodd+w76HUtE6iAVQQiLiDD+30Wn83+Xd+ab3C1cOWYWBdv3+B1LROoYFUEtcO05LRl3U0/WF+/lstEzmJ+/3e9IIlKHqAhqib7tU/nwzt7Ex0QybOy3TFy03u9IIlJHqAhqkXaNE/nnnX04Mz2JkW/OZ/TUXGrjUV8iElpUBLVMSv1Y3rj5HIZ2bc5jk1Zw37uL2F+iaxmJyKnTtYZqobjoSJ66uiutUxN4asoq1m3fw4u/7EHDhBi/o4lILaQ1glrKzPjVBR146uquLMjfweXPz2B10W6/Y4lILRSUIjCzC81shZnlmtkDFUyPNbO3vemzzSzTG59pZnvNbIH3eCEYecLJZd3SefOWc9i5r4TLn5/JrLytfkcSkVqmykVgZpHAaOCnQCfgGjPrVG62EcB251w74EngL2Wm5TnnunqP26uaJxxlZTbin3f2IbV+DDf8fTbv5KzzO5KI1CLBWCPoCeQ651Y75w4A44Gh5eYZCozzht8DzjddPCeoWqbU44M7+9CzdSP+671FPPr595SW6ogiETm+YBRBOlD2K2iBN67CeZxzJUAxkOJNa21m881smpn1q+xNzOxWM8sxs5yioqIgxK57kuKjefWmnlzTswXPZ+cx8q157D2gI4pE5Nj83lm8AWjpnOsGjALeNLMGFc3onBvrnMtyzmWlpaXVaMjaJDoygv+7vDMPXnQ6ny3ZyC9enMX6HXv9jiUiISwYRVAItCjzPMMbV+E8ZhYFJAFbnXP7nXNbAZxzc4E8oEMQMoU1M+OW/m342/VZrC7azaXPzSBn7Ta/Y4lIiApGEcwB2ptZazOLAYYBE8rNMwEY7g1fCXzlnHNmlubtbMbM2gDtgdVByCTABZ2a8OFdfUiIjeSav33L+O/y/Y4kIiGoykXgbfMfCUwClgPvOOeWmtnDZnapN9vLQIqZ5RLYBHT4ENP+wCIzW0BgJ/Ltzjl9dQ2iDk0S+eiuPpzbJoUHPljM7z9awkHd20BEytAdysJEyaFS/vTZ97z8zRp6ZjbimWu60TQpzu9YIlKDdIeyMBcVGcHvLunEk1d3YXFhMRc9M53sFZv9jiUiIUBFEGYu75bBx3f3Ja1+LDe+Moe/fP69boMpEuZUBGGoXeP6fDSyD9f0bMGY7DyGjf1Wh5iKhDEVQZiKi47kT1ecxdPDurJ8w04ufmY6U7/XpiKRcKQiCHNDu6bz8d19aZoUz02vzuFPny7XUUUiYUZFILRJq8+Hd/bmunNa8uLXq7n6xVkUalORSNhQEQgQ2FT0v5d35tlrurFy024ueno6U5Zt8juWiNQAFYEc5WddmvPx3X3JaBjPzf/I4ZGJyzhQok1FInWZikD+TevUBN6/ozc39GrFS9+s4aoXZ7Fu2x6/Y4lINVERSIXioiN5eOiZPH9dd1Zv3s3Fz0xn0tKNfscSkWqgIpBjuqhzMybe05dWKQnc9tpcHvp4qTYVidQxKgI5rlYpCbx3Ry9u7J3JKzPWcuULM8nfqk1FInWFikBOSGxUJH+49Axe+GV31mz5kYufnc7nSzb4HUtEgkBFICflwjOb8ek9/WiTmsDtr8/j9x8tYd9B3Q5TpDZTEchJa9GoHu/e3psRfVszbtYPXP78TPKKdvsdS0ROkYpATklMVOCy1i8Pz2Jj8V4ueeYb3s1ZR228v4VIuFMRSJWcf3oTPru3P2dlJHH/e4v49dsL2L2/xO9YInISVARSZU2T4njzlnMZNagDExau5+JnprOoYIffsUTkBKkIJCgiI4x7zm/P+Ft7caCklJ+PmclL01dTWqpNRSKhTkUgQdWzdSM+u7cfAzs25pFPljNi3By27t7vdywROQYVgQRdcr0Yxl7fg4eHnsGMvK389OnpzMzd4ncsEamEikCqhZlxQ69M/nlnH+rHRXHdy7N5fNIK3R9ZJASpCKRadWregIl39+XK7hk8NzWXYWO/1U1vREKMikCqXb2YKB67qgtPD+vK9xt38dOnvtblKURCiIpAaszQrul8ck9fMr3LU/z3Pxfr8hQiIUBFIDWqVUoC793em1v6teb1b/O5bPQMcjfv8juWSFhTEUiNi4mK4MGLO/HKTWdTtGs/lzz7DW/PydflKUR8oiIQ35zXsTGf3duP7i0b8pv3F3P3W/Mp3nPQ71giYUdFIL5q3CCO10acw/1DOvL5ko0Meeprvlmlcw5EapKKQHwXGWHcdV47PrizNwmxkfzy5dn8YcJS9h7QjmSRmhCUIjCzC81shZnlmtkDFUyPNbO3vemzzSyzzLTfeuNXmNmQYOSR2umsjGQ+uacfN/XJ5NWZa+n36FTuf3chnyzaoE1GItUoqqr/gJlFAqOBQUABMMfMJjjnlpWZbQSw3TnXzsyGAX8BrjazTsAw4AygOTDFzDo45/RVMEzFRUfy+5+dweBOTXl99g9MWrqRd+cWEBlhdGuRzMCOaQzs2JhOzRoQEWF+xxWpE6pcBEBPINc5txrAzMYDQ4GyRTAU+IM3/B7wnJmZN368c24/sMbMcr1/b1YQckkt1qttCr3aplByqJQF63aQvaKIaSuLePyLlTz+xUpS68fSv0MqAzs2pn/7VJLrxfgdWaTWCkYRpAPryjwvAM6pbB7nXImZFQMp3vhvy702vaI3MbNbgVsBWrZsGYTYUhtERUaQldmIrMxG3DekI0W79vP1yiKyVxbx1feb+WBeIREGXVskM7BjYwZ0SKNzepLWFkROQjCKoEY458YCYwGysrJ0wHmYSkuM5ec9Mvh5jwwOlToWFnhrCys28+SUlTwxeSUpCTH075DGwI5p9GufRqMErS2IHEswiqAQaFHmeYY3rqJ5CswsCkgCtp7ga0UqFBlhdG/ZkO4tGzJqUAe27t7P16uKmOZtRvpwfiFmgZ3QA71iOCsjmUitLYgcxap6Nqf3h30lcD6BP+JzgGudc0vLzHMX0Nk5d7u3s/gK59wvzOwM4E0C+wWaA18C7Y+3szgrK8vl5ORUKbfUbYdKHYsLi8lesZnsFUUsLNiBc9CwXjTX9GzJPee3Jy460u+YIjXKzOY657LKj6/yGoG3zX8kMAmIBP7unFtqZg8DOc65CcDLwGvezuBtBI4UwpvvHQI7lkuAu3TEkARDZITRtUUyXVsk86sLOrD9xwN8vaqISUs38nx2HhMXbeB/Lz+Tfu3T/I4q4rsqrxH4QWsEUhUz87bw4IdLWLPlRy7vls5/X3w6KfVj/Y4lUu0qWyPQmcUSdnq3TeWze/txz0/aMXHRes5/Yhrv5KzTRe8kbKkIJCzFRUcyanBHPr2nH+3S6vNf7y3i2r/NZnXRbr+jidQ4FYGEtfZNEnnntl783+WdWbK+mAufns6zX67iQInurSzhQ0UgYS8iwrj2nJZ8OWoAgzo14a+TV3LxM9OZs3ab39FEaoSKQMTTuEEco6/tzis3ns2eA4e46oVZ/PaDxRTv1QXvpG5TEYiUc95pjZk8qj+39GvN23PyueCJaUxctF47k6XOUhGIVKBeTBQPXtyJCSP70rRBHCPfnM+IcTkUbN/jdzSRoFMRiBzDmelJfHhnb353SSe+Xb2VQU98zUvTV1NySDuTpe5QEYgcR1RkBCP6tmbyqAH0bpvCI58s57LnZ7C4oNjvaCJBoSIQOUHpyfG8NDyL56/rzuad+xk6+hv+OHEZP+4v8TuaSJWoCEROgplxUedmTPnPAVx7Tkv+PmMNg5/8mo8Xamey1F4qApFT0CAumkcu68x7t/eiQXw0d781n0ue/YZPFm3gUKkKQWoXFYFIFfRo1YiJd/fl8au6sOfAIe56cx6DnpjG23Py2V+iC+lK7aCrj4oEyaFSx+dLNvJ8di5L1++kaYM4bu7Xmmt6tiQhttbcDFDqsMquPqoiEAky5xzTV21hTHYes1ZvJSk+muG9M7mpdyYNddtM8ZGKQMQH8/K3MyY7j8nLNhEfHck1PVtyc7/WNE+O9zuahCEVgYiPVm3axZhpeXy0YD0RBpd1Tee2AW1p17i+39EkjKgIREJAwfY9vDR9DePn5LO/pJQhnZpy53ltOSsj2e9oEgZUBCIhZMvu/bw6Yy3jZq1l174S+rRL4c6B7ejdNgUz8zue1FEqApEQtGvfQd6cnc9L36yhaNd+umQkccfAdgzu1ISICBWCBJeKQCSE7Tt4iPfnFfDitNXkb9tD27QEbh/QlqFd04mJ0uk+EhwqApFaoORQKZ8u2ciY7DyWb9hJ86Q4bu7XhmE9W1AvRuciSNWoCERqEecc2SuLGDM1j+/WbqNhvWhu6tOaG3q1IrmezkWQU6MiEKmlctZuY0x2Hl9+v5mEmEiuPaclI/q2oWlSnN/RpJZREYjUcss37OSFaXl8vHA9URERXJmVwV3ntSNdJ6fJCVIRiNQR+Vv38MLXebybsw6Aq89uwZ0D2+lsZTkuFYFIHVO4Yy+jp+bybs46DAsUwnltaZakQpCKqQhE6qiC7XsYPTWwhhBhxrCegTUE7UOQ8lQEInXcum17eD47l3dzCoiIMK7t2ZI7BralSQMVggRUVgRVOlPFzBqZ2WQzW+X9bFjJfMO9eVaZ2fAy47PNbIWZLfAejauSRySctWhUjz9dcRZT7xvIFd3Sef3bH+j36FT+MGEpm3fu8zuehLAqrRGY2aPANufcn83sAaChc+435eZpBOQAWYAD5gI9nHPbzSwbuM85d1Jf77VGIHJ8+Vv38NzUVbw/r5CoCOPac1pyx4C2NNYaQtiqljUCYCgwzhseB1xWwTxDgMnOuW3Oue3AZODCKr6viBxHy5R6PHplF6b+50Au7dKcf8wKrCH8ceIyNu/SGoL8S1WLoIlzboM3vBFoUsE86cC6Ms8LvHGHveJtFvqdHeOyi2Z2q5nlmFlOUVFRFWOLhI+WKfV47KoufDlqAD/r0pxXZ66l/6NT+fNn37PnQInf8SQEHLcIzGyKmS2p4DG07HwusI3pZLczXeec6wz08x7XVzajc26scy7LOZeVlpZ2km8jIpmpCTzuFcJFZzbjhWl5DHria75cvsnvaOKz4xaBc+4C59yZFTw+AjaZWTMA7+fmCv6JQqBFmecZ3jicc4d/7gLeBHpW7dcRkePJTE3giau78u7tvagXE8mIcTnc8fpcNhZrc1G4quqmoQnA4aOAhgMfVTDPJGCwmTX0jioaDEwysygzSwUws2jgEmBJFfOIyAk6O7MRn9zTj/uHdOSr7zdzwRPTeHXGGg6V1r5DyqVqqloEfwYGmdkq4ALvOWaWZWYvATjntgF/BOZ4j4e9cbEECmERsIDAWsLfqphHRE5CTFQEd53Xji9+3Z9uLZP5w8fLuPz5GSwpLPY7mtQgnVAmIkDg0tcTFq7njxOXse3HA/xHn9b8elAHEmJ1H4S6oroOHxWROsLMGNo1nS9HDeTqs1vy0jdrGPTENCYv087kuk5FICJHSaoXzZ+u6Mx7t/ciMS6aW/6Rw22v5bCheK/f0aSaqAhEpEJZmY2YeE9ffnPhaUxbWcQFf53GK9qZXCepCESkUtGREdwxsC1f/GoAWZmNeOjjZVw2egaLC7QzuS5REYjIcbVMqcerN53Nc9d2Y+POfQwd/Q0Pf7yM3ft1ZnJdoCIQkRNiZlxyVnOmjBrAtee05JWZgZ3Jk5Zu9DuaVJGKQEROSlJ8NI9c1pn37+hNUnw0t702lxGvziGvaLff0eQU6TwCETllBw+V8vdv1vDsV7nsO3iI05ol0iwpnvTkeJolxdEsOZ705DiaJcXTODGWqEh99/RTZecR6EwRETll0ZER3DagLVd0z+Cl6av5fuMuftj6I7Pytv7b/oMIgyYN4mjulUTZn82T4mmWHEdKQgzHuAixVBMVgYhUWVpiLL+96PSjxu3cd5ANO/axvngv63fsPTK8Ycc+FhcW88WyTRwoKT3qNTFRETRPCqxBNEuO89YsAsPNk+JpnhxHYlx0Tf5qYUFFICLVokFcNA2aRtOxaWKF051zbP3xABt27KNwx142FO9lQ/E+1u8IFMesvK1s2rmP8qctJMZG0czb3BRYmwhsgmrulUXTpDjioiNr4DesO1QEIuILMyO1fiyp9WPpnJFU4Twlh0rZvGs/G4r3UrhjHxt2lCmL4r0sKSxm648H/u11qfVjaJ4c2FeRnhxPesN//cxIrkeD+ChtgipDRSAiISsqMiLwrT85nh6tKp5n38FDbDxSDoGyWF+8l4Lte1mxaRdTV2xm38GjN0HVj40iPTmeM9Ib0LttKr3bptA8Ob4GfqPQpCIQkVotLjqSzNQEMlMTKpx+eBNU4fa9FO7Ye+Tnum17yF5RxAfzCgHITKlHL68UerVNIbV+bE3+Gr5SEYhInVZ2E1SXFslHTSstdazYtIuZeVuZlbeFiQvX89Z3+QB0bJJIr7Yp9G6bwjltUkiKr7s7qXUegYiIp+RQKUvW72Rm3hZm5W1lztpt7DtYSoTBmelJXjGkcnZmQ+rF1L7v0ZWdR6AiEBGpxP6SQyzI3+GtMWxl/rrtHDzkiI40urZIPrIpqVvLZGKjQv9IJRWBiEgV7TlQQs7a7Uc2JS0uLKbUQVx0BFmtGtGrbQqDOjWhQ5OKD5n1m4pARCTIivce5Ls1245sSvp+4y7M4OfdM7h/SEeaNIjzO+JRVAQiItWsaNd+Xpq+mr/PWBO4l8OAttzSv03InOCmexaLiFSzw5famDJqAP3bp/HXySv5yePZfLSgkFD+0q0iEBEJslYpCbxwfQ/G33ouDRNiuHf8Aq4YM5N5+dv9jlYhFYGISDU5t00KE0b25dErz6Jg+16ueH4m946fT+GOvX5HO4qKQESkGkVGGL/IakH2fQMZeV47Pl+ykZ88ns1fv1jBjyFyq08VgYhIDUiIjeK+IR356r6BDDmjKc9+lct5j2fzbs46SstfYrWGqQhERGpQenI8z1zTjffv6E3z5Hjuf28RQ0fP4Ls123zLpCIQEfFBj1YN+eCO3jw9rCtbd+/nFy/O4o7X55K/dU+NZ6l9F8sQEakjIiKMoV3TGdypKX+bvpox2Xl8uXwzN/XNZOR57WrsbmxaIxAR8Vl8TCT3nN+e7PsHcmnX5rw4bTUDH8vmjdk/UHKo9Pj/QBWpCEREQkSTBnE8flUXPh7Zl7Zp9XnwwyUMevJrPpxfUK2FUKUiMLNGZjbZzFZ5PxtWMt/nZrbDzCaWG9/azGabWa6ZvW1mMVXJIyJSF3TOSOLt285l7PU9iI2K4NdvL6TT7ydx0dPTKd5zMOjvV9U1ggeAL51z7YEvvecVeQy4voLxfwGedM61A7YDI6qYR0SkTjAzBp/RlE/v6cdLN2RxY+9MMhrG0yA++Lt2q3TROTNbAQx0zm0ws2ZAtnOuYyXzDgTuc85d4j03oAho6pwrMbNewB+cc0OO97666JyIyMmrrovONXHObfCGNwJNTuK1KcAO59zhU+sKgPTKZjazW80sx8xyioqKTi2tiIj8m+OuY5jZFKBpBZMeLPvEOefMrNpOj3POjQXGQmCNoLreR0Qk3By3CJxzF1Q2zcw2mVmzMpuGNp/Ee28Fks0sylsryAAKT+L1IiISBFXdNDQBGO4NDwc+OtEXusDOianAlafyehERCY6qFsGfgUFmtgq4wHuOmWWZ2UuHZzKz6cC7wPlmVmBmh3cI/wYYZWa5BPYZvFzFPCIicpKqdBySc24rcH4F43OAm8s871fJ61cDPauSQUREqkZnFouIhDkVgYhImKvSCWV+MbMi4IdTfHkqsCWIcapDqGcM9XwQ+hlDPR8oYzCEWr5Wzrm08iNrZRFUhZnlVHRmXSgJ9Yyhng9CP2Oo5wNlDIZQz3eYNg2JiIQ5FYGISJgLxyIY63eAExDqGUM9H4R+xlDPB8oYDKGeDwjDfQQiInK0cFwjEBGRMlQEIiJhLmyKwMwuNLMV3m0xK7uTWk3kaGFmU81smZktNbN7vfEV3vbTAp7xci8ys+41lDPSzOYfvr1oZbcVNbNY73muNz2zhvIlm9l7Zva9mS03s14huAx/7f03XmJmb5lZnN/L0cz+bmabzWxJmXEnvdzMbLg3/yozG17RewUx32Pef+dFZvahmSWXmfZbL9+KMtcwq9bPe0UZy0z7TzNzZpbqPa/xZXhKnHN1/gFEAnlAGyAGWAh08ilLM6C7N5wIrAQ6AY8CD3jjHwD+4g1fBHwGGHAuMLuGco4C3gQmes/fAYZ5wy8Ad3jDdwIveMPDgLdrKN844GZvOAZIDqVlSOAmS2uA+DLL70a/lyPQH+gOLCkz7qSWG9AIWO39bOgNN6zGfIOBKG/4L2XydfI+y7FAa+8zHlndn/eKMnrjWwCTCJzsmurXMjyl38mvN67RXxJ6AZPKPP8t8Fu/c3lZPgIGASuAZt64ZsAKb/hF4Joy8x+ZrxozZRC4B/VPgIne/8RbynwYjyxP73/8Xt5wlDefVXO+JO+PrJUbH0rLMB1Y533Qo7zlOCQUliOQWe4P7UktN+Aa4MUy44+aL9j5yk27HHjDGz7qc3x4GdbE572ijMB7QBdgLf8qAl+W4ck+wmXT0OEP5WHHvC1mTfFW/7sBs6n8tp9+ZH8K+C+g1Ht+rNuKHsnnTS/25q9OrQnc7/oVb/PVS2aWQAgtQ+dcIfA4kA9sILBc5hJay/Gwk11ufn6e/oPAN2yOkaPG85nZUKDQObew3KSQyXgs4VIEIcfM6gPvA79yzu0sO80FviL4clyvmV0CbHbOzfXj/U9QFIFV8zHOuW7AjwQ2aRzh5zIE8LazDyVQWs2BBOBCv/KcKL+X27GY2YNACfCG31nKMrN6wP8D/sfvLKcqXIqgkMD2u8N8vS2mmUUTKIE3nHMfeKM3WeB2n9jRt/2s6ex9gEvNbC0wnsDmoafxbitaQYYj+bzpSQRuQ1qdCoAC59xs7/l7BIohVJYhBG7UtMY5V+ScOwh8QGDZhtJyPOxkl1uNL08zuxG4BLjOK6tQyteWQOEv9D43GcA8M2saQhmPKVyKYA7Q3jtiI4bAzrgJfgQxMyNwJ7blzrknykyq7LafE4AbvKMPzgWKy6zGB51z7rfOuQznXCaB5fSVc+46Kr+taNncV3rzV+s3SufcRmCdmXX0Rp0PLCNElqEnHzjXzOp5/80PZwyZ5VjGyS63ScBgM2vorfkM9sZVCzO7kMCmykudc3vK5R7mHXHVGmgPfEcNf96dc4udc42dc5ne56aAwAEhGwmRZXhcfu2cqOkHgb33KwkcTfCgjzn6Elj1XgQs8B4XEdge/CWwCpgCNPLmN2C0l3sxkFWDWQfyr6OG2hD4kOUSuO1orDc+znue601vU0PZugI53nL8J4EjL0JqGQIPAd8DS4DXCBzd4utyBN4isM/iIIE/WCNOZbkR2Faf6z1uquZ8uQS2px/+vLxQZv4HvXwrgJ+WGV9tn/eKMpabvpZ/7Syu8WV4Kg9dYkJEJMyFy6YhERGphIpARCTMqQhERMKcikBEJMypCEREwpyKQEQkzKkIRETC3P8HdxZvSv/g0CEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.1552  # Value for C0\n",
    "K0 = -0.0026  # Value for K0\n",
    "K1 = -0.0004  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0102    # Value for b\n",
    "c = 2.8734    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    90.500000    90.275910    90.051821    89.827731    89.603641   \n",
      "351    90.275910    90.051821    89.827731    89.603641    89.379552   \n",
      "352    90.051821    89.827731    89.603641    89.379552    89.155462   \n",
      "353    89.827731    89.603641    89.379552    89.155462    88.931373   \n",
      "354    89.603641    89.379552    89.155462    88.931373    88.707283   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    89.379552    89.155462    88.931373    88.707283    88.494958  ...   \n",
      "351    89.155462    88.931373    88.707283    88.494958    88.427731  ...   \n",
      "352    88.931373    88.707283    88.494958    88.427731    88.360504  ...   \n",
      "353    88.707283    88.494958    88.427731    88.360504    88.293277  ...   \n",
      "354    88.494958    88.427731    88.360504    88.293277    88.226050  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   79.071008    0.000263   79.054202    0.000263   79.037395    0.000263   \n",
      "351   79.054202    0.000263   79.037395    0.000263   79.020588    0.000262   \n",
      "352   79.037395    0.000263   79.020588    0.000262   79.003782    0.000262   \n",
      "353   79.020588    0.000262   79.003782    0.000262   78.986975    0.000262   \n",
      "354   79.003782    0.000262   78.986975    0.000262   78.970168    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   79.020588    0.000262   79.003782    0.000262  \n",
      "351   79.003782    0.000262   78.986975    0.000262  \n",
      "352   78.986975    0.000262   78.970168    0.000262  \n",
      "353   78.970168    0.000262   78.953361    0.000262  \n",
      "354   78.953361    0.000262   78.936555    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050, 1, 251) (1050, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "14/14 [==============================] - 3s 37ms/step - loss: 5703.7695 - val_loss: 4843.4399\n",
      "Epoch 2/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5638.2441 - val_loss: 4797.3374\n",
      "Epoch 3/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 5588.5029 - val_loss: 4751.4136\n",
      "Epoch 4/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 5535.9243 - val_loss: 4701.5732\n",
      "Epoch 5/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5484.2451 - val_loss: 4654.0576\n",
      "Epoch 6/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 5433.0649 - val_loss: 4607.0186\n",
      "Epoch 7/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5382.3916 - val_loss: 4560.4570\n",
      "Epoch 8/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5332.1992 - val_loss: 4514.3340\n",
      "Epoch 9/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5268.2529 - val_loss: 4442.0781\n",
      "Epoch 10/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 5201.6411 - val_loss: 4390.7510\n",
      "Epoch 11/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5146.2471 - val_loss: 4340.1899\n",
      "Epoch 12/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5091.9204 - val_loss: 4290.6973\n",
      "Epoch 13/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5038.6504 - val_loss: 4242.0962\n",
      "Epoch 14/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 4986.2480 - val_loss: 4194.2319\n",
      "Epoch 15/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 4934.5728 - val_loss: 4147.0054\n",
      "Epoch 16/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 4883.5337 - val_loss: 4100.3481\n",
      "Epoch 17/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 4833.0679 - val_loss: 4054.2139\n",
      "Epoch 18/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 4783.1309 - val_loss: 4008.5703\n",
      "Epoch 19/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 4733.6924 - val_loss: 3963.3911\n",
      "Epoch 20/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4684.7285 - val_loss: 3918.6567\n",
      "Epoch 21/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4636.2188 - val_loss: 3874.3533\n",
      "Epoch 22/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 4588.1489 - val_loss: 3830.4666\n",
      "Epoch 23/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 4540.5059 - val_loss: 3786.9863\n",
      "Epoch 24/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4493.2803 - val_loss: 3743.9045\n",
      "Epoch 25/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 4446.4614 - val_loss: 3701.2119\n",
      "Epoch 26/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4400.0430 - val_loss: 3658.9021\n",
      "Epoch 27/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4354.0171 - val_loss: 3616.9697\n",
      "Epoch 28/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4308.3784 - val_loss: 3575.4080\n",
      "Epoch 29/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4263.1201 - val_loss: 3534.2122\n",
      "Epoch 30/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4218.2378 - val_loss: 3493.3789\n",
      "Epoch 31/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4173.7271 - val_loss: 3452.9026\n",
      "Epoch 32/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4129.5835 - val_loss: 3412.7808\n",
      "Epoch 33/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 4085.8027 - val_loss: 3373.0073\n",
      "Epoch 34/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 4042.3813 - val_loss: 3333.5811\n",
      "Epoch 35/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3999.3152 - val_loss: 3294.4976\n",
      "Epoch 36/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 3956.6023 - val_loss: 3255.7539\n",
      "Epoch 37/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3914.2380 - val_loss: 3217.3474\n",
      "Epoch 38/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3872.2209 - val_loss: 3179.2749\n",
      "Epoch 39/500\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 3830.5461 - val_loss: 3141.5334\n",
      "Epoch 40/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3789.2122 - val_loss: 3104.1208\n",
      "Epoch 41/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3748.2158 - val_loss: 3067.0339\n",
      "Epoch 42/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 3707.5542 - val_loss: 3030.2708\n",
      "Epoch 43/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 3667.2256 - val_loss: 2993.8284\n",
      "Epoch 44/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 3627.2278 - val_loss: 2957.7048\n",
      "Epoch 45/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 3587.5569 - val_loss: 2921.8972\n",
      "Epoch 46/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 3548.2114 - val_loss: 2886.4041\n",
      "Epoch 47/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3509.1892 - val_loss: 2851.2229\n",
      "Epoch 48/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3470.4875 - val_loss: 2816.3516\n",
      "Epoch 49/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3432.1050 - val_loss: 2781.7881\n",
      "Epoch 50/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3392.6694 - val_loss: 2738.6572\n",
      "Epoch 51/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3344.2451 - val_loss: 2699.7998\n",
      "Epoch 52/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3301.4189 - val_loss: 2661.6306\n",
      "Epoch 53/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3259.6394 - val_loss: 2624.5132\n",
      "Epoch 54/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3218.9148 - val_loss: 2588.2634\n",
      "Epoch 55/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 3179.0457 - val_loss: 2552.7229\n",
      "Epoch 56/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3139.8833 - val_loss: 2517.7886\n",
      "Epoch 57/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3101.3328 - val_loss: 2483.3909\n",
      "Epoch 58/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 3063.3279 - val_loss: 2449.4810\n",
      "Epoch 59/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3025.8242 - val_loss: 2416.0261\n",
      "Epoch 60/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2988.7866 - val_loss: 2382.9990\n",
      "Epoch 61/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2952.1899 - val_loss: 2350.3804\n",
      "Epoch 62/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2916.0146 - val_loss: 2318.1533\n",
      "Epoch 63/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2880.2429 - val_loss: 2286.3042\n",
      "Epoch 64/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2844.8623 - val_loss: 2254.8210\n",
      "Epoch 65/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2809.8604 - val_loss: 2223.6951\n",
      "Epoch 66/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2775.2280 - val_loss: 2192.9167\n",
      "Epoch 67/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2740.9556 - val_loss: 2162.4792\n",
      "Epoch 68/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2707.0356 - val_loss: 2132.3765\n",
      "Epoch 69/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2673.4619 - val_loss: 2102.6013\n",
      "Epoch 70/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2640.2283 - val_loss: 2073.1484\n",
      "Epoch 71/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2607.3284 - val_loss: 2044.0129\n",
      "Epoch 72/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2574.7568 - val_loss: 2015.1908\n",
      "Epoch 73/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2542.5100 - val_loss: 1986.6769\n",
      "Epoch 74/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2510.5825 - val_loss: 1958.4674\n",
      "Epoch 75/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2478.9709 - val_loss: 1930.5588\n",
      "Epoch 76/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2447.6709 - val_loss: 1902.9473\n",
      "Epoch 77/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2416.6782 - val_loss: 1875.6295\n",
      "Epoch 78/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 2385.9907 - val_loss: 1848.6022\n",
      "Epoch 79/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2355.6042 - val_loss: 1821.8628\n",
      "Epoch 80/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2325.5159 - val_loss: 1795.4075\n",
      "Epoch 81/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2295.7227 - val_loss: 1769.2338\n",
      "Epoch 82/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2266.2217 - val_loss: 1743.3401\n",
      "Epoch 83/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2237.0098 - val_loss: 1717.7208\n",
      "Epoch 84/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2208.0845 - val_loss: 1692.3770\n",
      "Epoch 85/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2179.4431 - val_loss: 1667.3030\n",
      "Epoch 86/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2151.0837 - val_loss: 1642.4989\n",
      "Epoch 87/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2123.0032 - val_loss: 1617.9612\n",
      "Epoch 88/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2095.1990 - val_loss: 1593.6877\n",
      "Epoch 89/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2067.6697 - val_loss: 1569.6759\n",
      "Epoch 90/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 2040.4122 - val_loss: 1545.9248\n",
      "Epoch 91/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 2013.4246 - val_loss: 1522.4305\n",
      "Epoch 92/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 1986.7045 - val_loss: 1499.1920\n",
      "Epoch 93/500\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 1960.2502 - val_loss: 1476.2078\n",
      "Epoch 94/500\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 1934.0596 - val_loss: 1453.4744\n",
      "Epoch 95/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 1908.1302 - val_loss: 1430.9906\n",
      "Epoch 96/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 1882.4606 - val_loss: 1408.7543\n",
      "Epoch 97/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 1857.0482 - val_loss: 1386.7645\n",
      "Epoch 98/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 1831.8912 - val_loss: 1365.0171\n",
      "Epoch 99/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1806.9880 - val_loss: 1343.5134\n",
      "Epoch 100/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1782.3372 - val_loss: 1322.2480\n",
      "Epoch 101/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1757.9359 - val_loss: 1301.2222\n",
      "Epoch 102/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 1733.7825 - val_loss: 1280.4316\n",
      "Epoch 103/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1709.8751 - val_loss: 1259.8770\n",
      "Epoch 104/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1686.2128 - val_loss: 1239.5546\n",
      "Epoch 105/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1662.7932 - val_loss: 1219.4645\n",
      "Epoch 106/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1639.6146 - val_loss: 1199.6028\n",
      "Epoch 107/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1616.6750 - val_loss: 1179.9686\n",
      "Epoch 108/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1593.9728 - val_loss: 1160.5612\n",
      "Epoch 109/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1571.5067 - val_loss: 1141.3782\n",
      "Epoch 110/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1549.2750 - val_loss: 1122.4182\n",
      "Epoch 111/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 1527.2758 - val_loss: 1103.6785\n",
      "Epoch 112/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1505.5074 - val_loss: 1085.1597\n",
      "Epoch 113/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1483.9685 - val_loss: 1066.8579\n",
      "Epoch 114/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1462.6569 - val_loss: 1048.7723\n",
      "Epoch 115/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1441.5714 - val_loss: 1030.9022\n",
      "Epoch 116/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1420.7106 - val_loss: 1013.2451\n",
      "Epoch 117/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1400.0729 - val_loss: 995.7993\n",
      "Epoch 118/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1379.6562 - val_loss: 978.5636\n",
      "Epoch 119/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1359.4597 - val_loss: 961.5367\n",
      "Epoch 120/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1339.4817 - val_loss: 944.7175\n",
      "Epoch 121/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1319.7202 - val_loss: 928.1025\n",
      "Epoch 122/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1300.1740 - val_loss: 911.6921\n",
      "Epoch 123/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1280.8416 - val_loss: 895.4839\n",
      "Epoch 124/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1261.7214 - val_loss: 879.4769\n",
      "Epoch 125/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1242.8118 - val_loss: 863.6696\n",
      "Epoch 126/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1224.1117 - val_loss: 848.0600\n",
      "Epoch 127/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1205.6191 - val_loss: 832.6466\n",
      "Epoch 128/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1187.3334 - val_loss: 817.4290\n",
      "Epoch 129/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1169.2523 - val_loss: 802.4049\n",
      "Epoch 130/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1151.3748 - val_loss: 787.5724\n",
      "Epoch 131/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 1133.6995 - val_loss: 772.9304\n",
      "Epoch 132/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 1116.2241 - val_loss: 758.4784\n",
      "Epoch 133/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1098.9484 - val_loss: 744.2149\n",
      "Epoch 134/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1081.8702 - val_loss: 730.1370\n",
      "Epoch 135/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1064.9889 - val_loss: 716.2438\n",
      "Epoch 136/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 1048.3020 - val_loss: 702.5347\n",
      "Epoch 137/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1031.8087 - val_loss: 689.0078\n",
      "Epoch 138/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 1015.5076 - val_loss: 675.6619\n",
      "Epoch 139/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 999.3975 - val_loss: 662.4949\n",
      "Epoch 140/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 983.4766 - val_loss: 649.5063\n",
      "Epoch 141/500\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 967.7440 - val_loss: 636.6946\n",
      "Epoch 142/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 952.1980 - val_loss: 624.0580\n",
      "Epoch 143/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 936.8372 - val_loss: 611.5951\n",
      "Epoch 144/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 921.6605 - val_loss: 599.3048\n",
      "Epoch 145/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 906.6661 - val_loss: 587.1855\n",
      "Epoch 146/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 891.8530 - val_loss: 575.2360\n",
      "Epoch 147/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 877.2197 - val_loss: 563.4554\n",
      "Epoch 148/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 862.7652 - val_loss: 551.8411\n",
      "Epoch 149/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 848.4874 - val_loss: 540.3928\n",
      "Epoch 150/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 834.3860 - val_loss: 529.1093\n",
      "Epoch 151/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 820.4592 - val_loss: 517.9889\n",
      "Epoch 152/500\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 806.7055 - val_loss: 507.0295\n",
      "Epoch 153/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 793.1236 - val_loss: 496.2315\n",
      "Epoch 154/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 779.7128 - val_loss: 485.5919\n",
      "Epoch 155/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 766.4713 - val_loss: 475.1102\n",
      "Epoch 156/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 753.3974 - val_loss: 464.7849\n",
      "Epoch 157/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 740.4904 - val_loss: 454.6153\n",
      "Epoch 158/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 727.7490 - val_loss: 444.5984\n",
      "Epoch 159/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 715.1716 - val_loss: 434.7349\n",
      "Epoch 160/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 702.7571 - val_loss: 425.0225\n",
      "Epoch 161/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 690.5043 - val_loss: 415.4597\n",
      "Epoch 162/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 678.4118 - val_loss: 406.0456\n",
      "Epoch 163/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 666.4786 - val_loss: 396.7788\n",
      "Epoch 164/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 654.7029 - val_loss: 387.6582\n",
      "Epoch 165/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 643.0837 - val_loss: 378.6823\n",
      "Epoch 166/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 631.6199 - val_loss: 369.8499\n",
      "Epoch 167/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 620.3100 - val_loss: 361.1595\n",
      "Epoch 168/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 609.1529 - val_loss: 352.6101\n",
      "Epoch 169/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 598.1472 - val_loss: 344.2005\n",
      "Epoch 170/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 587.2917 - val_loss: 335.9289\n",
      "Epoch 171/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 576.5852 - val_loss: 327.7942\n",
      "Epoch 172/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 566.0266 - val_loss: 319.7958\n",
      "Epoch 173/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 555.6143 - val_loss: 311.9316\n",
      "Epoch 174/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 545.3472 - val_loss: 304.2007\n",
      "Epoch 175/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 535.2241 - val_loss: 296.6015\n",
      "Epoch 176/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 525.2437 - val_loss: 289.1334\n",
      "Epoch 177/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 515.4048 - val_loss: 281.7946\n",
      "Epoch 178/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 505.7062 - val_loss: 274.5840\n",
      "Epoch 179/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 496.1471 - val_loss: 267.5007\n",
      "Epoch 180/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 486.7255 - val_loss: 260.5428\n",
      "Epoch 181/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 477.4407 - val_loss: 253.7100\n",
      "Epoch 182/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 468.2913 - val_loss: 247.0001\n",
      "Epoch 183/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 459.2763 - val_loss: 240.4120\n",
      "Epoch 184/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 450.3943 - val_loss: 233.9451\n",
      "Epoch 185/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 441.6441 - val_loss: 227.5975\n",
      "Epoch 186/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 433.0244 - val_loss: 221.3680\n",
      "Epoch 187/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 424.5341 - val_loss: 215.2559\n",
      "Epoch 188/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 416.1722 - val_loss: 209.2597\n",
      "Epoch 189/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 407.9371 - val_loss: 203.3779\n",
      "Epoch 190/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 399.8277 - val_loss: 197.6098\n",
      "Epoch 191/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 391.8431 - val_loss: 191.9536\n",
      "Epoch 192/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 383.9820 - val_loss: 186.4088\n",
      "Epoch 193/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 376.2431 - val_loss: 180.9737\n",
      "Epoch 194/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 368.6254 - val_loss: 175.6472\n",
      "Epoch 195/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 361.1276 - val_loss: 170.4281\n",
      "Epoch 196/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 353.7484 - val_loss: 165.3151\n",
      "Epoch 197/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 346.4870 - val_loss: 160.3071\n",
      "Epoch 198/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 339.3417 - val_loss: 155.4026\n",
      "Epoch 199/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 332.3118 - val_loss: 150.6011\n",
      "Epoch 200/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 325.3960 - val_loss: 145.9008\n",
      "Epoch 201/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 318.5931 - val_loss: 141.3010\n",
      "Epoch 202/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 311.9019 - val_loss: 136.7998\n",
      "Epoch 203/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 305.3211 - val_loss: 132.3965\n",
      "Epoch 204/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 298.8497 - val_loss: 128.0901\n",
      "Epoch 205/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 291.9842 - val_loss: 120.8845\n",
      "Epoch 206/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 280.9694 - val_loss: 115.4567\n",
      "Epoch 207/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 272.8483 - val_loss: 110.2886\n",
      "Epoch 208/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 265.1611 - val_loss: 105.4633\n",
      "Epoch 209/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 257.9041 - val_loss: 100.9172\n",
      "Epoch 210/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 250.9909 - val_loss: 96.5998\n",
      "Epoch 211/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 244.3580 - val_loss: 92.4767\n",
      "Epoch 212/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 237.9638 - val_loss: 88.5249\n",
      "Epoch 213/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 231.7796 - val_loss: 84.7282\n",
      "Epoch 214/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 225.7848 - val_loss: 81.0743\n",
      "Epoch 215/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 219.9642 - val_loss: 77.5538\n",
      "Epoch 216/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 214.3061 - val_loss: 74.1587\n",
      "Epoch 217/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 208.8009 - val_loss: 70.8831\n",
      "Epoch 218/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 203.4407 - val_loss: 67.7215\n",
      "Epoch 219/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 198.2186 - val_loss: 64.6686\n",
      "Epoch 220/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 193.1287 - val_loss: 61.7206\n",
      "Epoch 221/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 188.1662 - val_loss: 58.8735\n",
      "Epoch 222/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 183.3262 - val_loss: 56.1241\n",
      "Epoch 223/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 178.6048 - val_loss: 53.4693\n",
      "Epoch 224/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 173.9984 - val_loss: 50.9059\n",
      "Epoch 225/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 169.5033 - val_loss: 48.4313\n",
      "Epoch 226/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 165.1164 - val_loss: 46.0430\n",
      "Epoch 227/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 160.8348 - val_loss: 43.7388\n",
      "Epoch 228/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 156.6561 - val_loss: 41.5160\n",
      "Epoch 229/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 152.5771 - val_loss: 39.3730\n",
      "Epoch 230/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 148.5959 - val_loss: 37.3071\n",
      "Epoch 231/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 144.7100 - val_loss: 35.3169\n",
      "Epoch 232/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 140.9171 - val_loss: 33.4003\n",
      "Epoch 233/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 137.2152 - val_loss: 31.5551\n",
      "Epoch 234/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 133.6020 - val_loss: 29.7801\n",
      "Epoch 235/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 130.0757 - val_loss: 28.0731\n",
      "Epoch 236/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 126.6346 - val_loss: 26.4328\n",
      "Epoch 237/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 123.2769 - val_loss: 24.8574\n",
      "Epoch 238/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 120.0005 - val_loss: 23.3453\n",
      "Epoch 239/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 116.8039 - val_loss: 21.8951\n",
      "Epoch 240/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 113.6855 - val_loss: 20.5049\n",
      "Epoch 241/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 110.6433 - val_loss: 19.1737\n",
      "Epoch 242/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 107.6760 - val_loss: 17.8997\n",
      "Epoch 243/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 104.7821 - val_loss: 16.6818\n",
      "Epoch 244/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 101.9601 - val_loss: 15.5184\n",
      "Epoch 245/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 99.2085 - val_loss: 14.4084\n",
      "Epoch 246/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 96.5258 - val_loss: 13.3500\n",
      "Epoch 247/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 93.9106 - val_loss: 12.3425\n",
      "Epoch 248/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 91.3616 - val_loss: 11.3843\n",
      "Epoch 249/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 88.8776 - val_loss: 10.4742\n",
      "Epoch 250/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 86.4570 - val_loss: 9.6109\n",
      "Epoch 251/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 84.0986 - val_loss: 8.7933\n",
      "Epoch 252/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 81.8012 - val_loss: 8.0201\n",
      "Epoch 253/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 79.5633 - val_loss: 7.2903\n",
      "Epoch 254/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 77.3841 - val_loss: 6.6025\n",
      "Epoch 255/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 75.2621 - val_loss: 5.9560\n",
      "Epoch 256/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 73.1962 - val_loss: 5.3492\n",
      "Epoch 257/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 71.1850 - val_loss: 4.7812\n",
      "Epoch 258/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 69.2277 - val_loss: 4.2509\n",
      "Epoch 259/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 67.3230 - val_loss: 3.7574\n",
      "Epoch 260/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 65.4697 - val_loss: 3.2995\n",
      "Epoch 261/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 63.6670 - val_loss: 2.8763\n",
      "Epoch 262/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 61.9137 - val_loss: 2.4866\n",
      "Epoch 263/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 60.2086 - val_loss: 2.1296\n",
      "Epoch 264/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 58.5507 - val_loss: 1.8042\n",
      "Epoch 265/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 56.9390 - val_loss: 1.5095\n",
      "Epoch 266/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 55.3726 - val_loss: 1.2445\n",
      "Epoch 267/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 53.8502 - val_loss: 1.0084\n",
      "Epoch 268/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 52.3713 - val_loss: 0.8001\n",
      "Epoch 269/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 50.9346 - val_loss: 0.6189\n",
      "Epoch 270/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 49.5393 - val_loss: 0.4638\n",
      "Epoch 271/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 48.1843 - val_loss: 0.3339\n",
      "Epoch 272/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 46.8688 - val_loss: 0.2285\n",
      "Epoch 273/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 45.5918 - val_loss: 0.1466\n",
      "Epoch 274/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 44.3527 - val_loss: 0.0874\n",
      "Epoch 275/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 43.1503 - val_loss: 0.0502\n",
      "Epoch 276/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 41.9839 - val_loss: 0.0340\n",
      "Epoch 277/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 40.8525 - val_loss: 0.0383\n",
      "Epoch 278/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 39.7555 - val_loss: 0.0621\n",
      "Epoch 279/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 38.6919 - val_loss: 0.1048\n",
      "Epoch 280/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 37.6609 - val_loss: 0.1655\n",
      "Epoch 281/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 36.6617 - val_loss: 0.2436\n",
      "Epoch 282/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 35.6936 - val_loss: 0.3384\n",
      "Epoch 283/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 34.7558 - val_loss: 0.4491\n",
      "Epoch 284/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 33.8475 - val_loss: 0.5751\n",
      "Epoch 285/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 32.9681 - val_loss: 0.7157\n",
      "Epoch 286/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 32.1166 - val_loss: 0.8702\n",
      "Epoch 287/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 31.2925 - val_loss: 1.0380\n",
      "Epoch 288/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 30.4951 - val_loss: 1.2185\n",
      "Epoch 289/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 29.7236 - val_loss: 1.4111\n",
      "Epoch 290/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 28.9775 - val_loss: 1.6152\n",
      "Epoch 291/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 28.2557 - val_loss: 1.8300\n",
      "Epoch 292/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 27.5580 - val_loss: 2.0552\n",
      "Epoch 293/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 26.8836 - val_loss: 2.2901\n",
      "Epoch 294/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 26.2319 - val_loss: 2.5342\n",
      "Epoch 295/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 25.6022 - val_loss: 2.7870\n",
      "Epoch 296/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 24.9940 - val_loss: 3.0479\n",
      "Epoch 297/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 24.4066 - val_loss: 3.3163\n",
      "Epoch 298/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 23.8394 - val_loss: 3.5919\n",
      "Epoch 299/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 23.2919 - val_loss: 3.8741\n",
      "Epoch 300/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.7636 - val_loss: 4.1624\n",
      "Epoch 301/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.2538 - val_loss: 4.4564\n",
      "Epoch 302/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.7620 - val_loss: 4.7556\n",
      "Epoch 303/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.2878 - val_loss: 5.0598\n",
      "Epoch 304/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 20.8306 - val_loss: 5.3682\n",
      "Epoch 305/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 20.3898 - val_loss: 5.6806\n",
      "Epoch 306/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 19.9650 - val_loss: 5.9966\n",
      "Epoch 307/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 19.5557 - val_loss: 6.3158\n",
      "Epoch 308/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 19.1615 - val_loss: 6.6378\n",
      "Epoch 309/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 18.7817 - val_loss: 6.9623\n",
      "Epoch 310/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 18.4161 - val_loss: 7.2887\n",
      "Epoch 311/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 18.0642 - val_loss: 7.6170\n",
      "Epoch 312/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.7256 - val_loss: 7.9466\n",
      "Epoch 313/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.3998 - val_loss: 8.2774\n",
      "Epoch 314/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.0864 - val_loss: 8.6091\n",
      "Epoch 315/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.7849 - val_loss: 8.9411\n",
      "Epoch 316/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 16.4950 - val_loss: 9.2732\n",
      "Epoch 317/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.2165 - val_loss: 9.6055\n",
      "Epoch 318/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 15.9488 - val_loss: 9.9373\n",
      "Epoch 319/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 15.6915 - val_loss: 10.2685\n",
      "Epoch 320/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 15.4444 - val_loss: 10.5990\n",
      "Epoch 321/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 15.2071 - val_loss: 10.9282\n",
      "Epoch 322/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.9794 - val_loss: 11.2562\n",
      "Epoch 323/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.7607 - val_loss: 11.5827\n",
      "Epoch 324/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.5508 - val_loss: 11.9075\n",
      "Epoch 325/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.3494 - val_loss: 12.2304\n",
      "Epoch 326/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1563 - val_loss: 12.5512\n",
      "Epoch 327/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 13.9710 - val_loss: 12.8697\n",
      "Epoch 328/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.7933 - val_loss: 13.1858\n",
      "Epoch 329/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.6231 - val_loss: 13.4993\n",
      "Epoch 330/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.4598 - val_loss: 13.8099\n",
      "Epoch 331/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.3035 - val_loss: 14.1177\n",
      "Epoch 332/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 13.1537 - val_loss: 14.4225\n",
      "Epoch 333/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.0102 - val_loss: 14.7239\n",
      "Epoch 334/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 12.8729 - val_loss: 15.0223\n",
      "Epoch 335/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 12.7412 - val_loss: 15.3171\n",
      "Epoch 336/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 12.6153 - val_loss: 15.6083\n",
      "Epoch 337/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 12.4949 - val_loss: 15.8960\n",
      "Epoch 338/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 12.3796 - val_loss: 16.1801\n",
      "Epoch 339/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 12.2694 - val_loss: 16.4602\n",
      "Epoch 340/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 12.1639 - val_loss: 16.7366\n",
      "Epoch 341/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 12.0631 - val_loss: 17.0090\n",
      "Epoch 342/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 11.9667 - val_loss: 17.2775\n",
      "Epoch 343/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 11.8745 - val_loss: 17.5418\n",
      "Epoch 344/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 11.7865 - val_loss: 17.8020\n",
      "Epoch 345/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 11.7024 - val_loss: 18.0582\n",
      "Epoch 346/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 11.6220 - val_loss: 18.3100\n",
      "Epoch 347/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 11.5453 - val_loss: 18.5577\n",
      "Epoch 348/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 11.4719 - val_loss: 18.8012\n",
      "Epoch 349/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 11.4019 - val_loss: 19.0402\n",
      "Epoch 350/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 11.3351 - val_loss: 19.2752\n",
      "Epoch 351/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 11.2713 - val_loss: 19.5057\n",
      "Epoch 352/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 11.2104 - val_loss: 19.7320\n",
      "Epoch 353/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 11.1523 - val_loss: 19.9539\n",
      "Epoch 354/500\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 11.0969 - val_loss: 20.1716\n",
      "Epoch 355/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 11.0439 - val_loss: 20.3848\n",
      "Epoch 356/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.9935 - val_loss: 20.5938\n",
      "Epoch 357/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.9454 - val_loss: 20.7985\n",
      "Epoch 358/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 10.8995 - val_loss: 20.9988\n",
      "Epoch 359/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 10.8558 - val_loss: 21.1947\n",
      "Epoch 360/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 10.8141 - val_loss: 21.3866\n",
      "Epoch 361/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 10.7743 - val_loss: 21.5740\n",
      "Epoch 362/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 10.7364 - val_loss: 21.7575\n",
      "Epoch 363/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 10.7003 - val_loss: 21.9365\n",
      "Epoch 364/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 10.6660 - val_loss: 22.1115\n",
      "Epoch 365/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 10.6331 - val_loss: 22.2826\n",
      "Epoch 366/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 10.6019 - val_loss: 22.4494\n",
      "Epoch 367/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.5721 - val_loss: 22.6121\n",
      "Epoch 368/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 10.5438 - val_loss: 22.7708\n",
      "Epoch 369/500\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 10.5167 - val_loss: 22.9257\n",
      "Epoch 370/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.4910 - val_loss: 23.0767\n",
      "Epoch 371/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 10.4665 - val_loss: 23.2238\n",
      "Epoch 372/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.4432 - val_loss: 23.3669\n",
      "Epoch 373/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.4210 - val_loss: 23.5066\n",
      "Epoch 374/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.3998 - val_loss: 23.6424\n",
      "Epoch 375/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.3797 - val_loss: 23.7744\n",
      "Epoch 376/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.3605 - val_loss: 23.9031\n",
      "Epoch 377/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.3422 - val_loss: 24.0281\n",
      "Epoch 378/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.3248 - val_loss: 24.1498\n",
      "Epoch 379/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 10.3082 - val_loss: 24.2679\n",
      "Epoch 380/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 10.2925 - val_loss: 24.3827\n",
      "Epoch 381/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 10.2774 - val_loss: 24.4944\n",
      "Epoch 382/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.2631 - val_loss: 24.6024\n",
      "Epoch 383/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 10.2495 - val_loss: 24.7075\n",
      "Epoch 384/500\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 10.2365 - val_loss: 24.8094\n",
      "Epoch 385/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.2242 - val_loss: 24.9081\n",
      "Epoch 386/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.2125 - val_loss: 25.0042\n",
      "Epoch 387/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 10.2013 - val_loss: 25.0969\n",
      "Epoch 388/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.1906 - val_loss: 25.1871\n",
      "Epoch 389/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 10.1805 - val_loss: 25.2741\n",
      "Epoch 390/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 10.1708 - val_loss: 25.3586\n",
      "Epoch 391/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 10.1617 - val_loss: 25.4404\n",
      "Epoch 392/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 10.1529 - val_loss: 25.5195\n",
      "Epoch 393/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.1446 - val_loss: 25.5959\n",
      "Epoch 394/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.1366 - val_loss: 25.6700\n",
      "Epoch 395/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.1291 - val_loss: 25.7413\n",
      "Epoch 396/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 10.1219 - val_loss: 25.8105\n",
      "Epoch 397/500\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 10.1151 - val_loss: 25.8774\n",
      "Epoch 398/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.1086 - val_loss: 25.9419\n",
      "Epoch 399/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 10.1024 - val_loss: 26.0041\n",
      "Epoch 400/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.0964 - val_loss: 26.0642\n",
      "Epoch 401/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 10.0909 - val_loss: 26.1221\n",
      "Epoch 402/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 10.0855 - val_loss: 26.1781\n",
      "Epoch 403/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 10.0804 - val_loss: 26.2320\n",
      "Epoch 404/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.0756 - val_loss: 26.2839\n",
      "Epoch 405/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.0710 - val_loss: 26.3339\n",
      "Epoch 406/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.0666 - val_loss: 26.3822\n",
      "Epoch 407/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.0624 - val_loss: 26.4288\n",
      "Epoch 408/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.0584 - val_loss: 26.4735\n",
      "Epoch 409/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.0547 - val_loss: 26.5163\n",
      "Epoch 410/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.0511 - val_loss: 26.5578\n",
      "Epoch 411/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.0476 - val_loss: 26.5976\n",
      "Epoch 412/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.0444 - val_loss: 26.6358\n",
      "Epoch 413/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 10.0413 - val_loss: 26.6725\n",
      "Epoch 414/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 10.0384 - val_loss: 26.7079\n",
      "Epoch 415/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 10.0356 - val_loss: 26.7418\n",
      "Epoch 416/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 10.0329 - val_loss: 26.7744\n",
      "Epoch 417/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 10.0304 - val_loss: 26.8055\n",
      "Epoch 418/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.0280 - val_loss: 26.8355\n",
      "Epoch 419/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.0257 - val_loss: 26.8642\n",
      "Epoch 420/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 10.0236 - val_loss: 26.8917\n",
      "Epoch 421/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.0215 - val_loss: 26.9181\n",
      "Epoch 422/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 10.0196 - val_loss: 26.9433\n",
      "Epoch 423/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.0177 - val_loss: 26.9676\n",
      "Epoch 424/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.0160 - val_loss: 26.9907\n",
      "Epoch 425/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.0143 - val_loss: 27.0129\n",
      "Epoch 426/500\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 10.0127 - val_loss: 27.0340\n",
      "Epoch 427/500\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 10.0113 - val_loss: 27.0543\n",
      "Epoch 428/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 10.0098 - val_loss: 27.0737\n",
      "Epoch 429/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 10.0085 - val_loss: 27.0921\n",
      "Epoch 430/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 10.0073 - val_loss: 27.1099\n",
      "Epoch 431/500\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 10.0061 - val_loss: 27.1270\n",
      "Epoch 432/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 10.0049 - val_loss: 27.1431\n",
      "Epoch 433/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 10.0038 - val_loss: 27.1584\n",
      "Epoch 434/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 10.0028 - val_loss: 27.1731\n",
      "Epoch 435/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 10.0019 - val_loss: 27.1871\n",
      "Epoch 436/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.0010 - val_loss: 27.2006\n",
      "Epoch 437/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 10.0001 - val_loss: 27.2134\n",
      "Epoch 438/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 9.9993 - val_loss: 27.2253\n",
      "Epoch 439/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 9.9986 - val_loss: 27.2366\n",
      "Epoch 440/500\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 9.9979 - val_loss: 27.2476\n",
      "Epoch 441/500\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 9.9973 - val_loss: 27.2581\n",
      "Epoch 442/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 9.9966 - val_loss: 27.2682\n",
      "Epoch 443/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 9.9960 - val_loss: 27.2775\n",
      "Epoch 444/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 9.9955 - val_loss: 27.2865\n",
      "Epoch 445/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 9.9950 - val_loss: 27.2950\n",
      "Epoch 446/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9945 - val_loss: 27.3031\n",
      "Epoch 447/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9941 - val_loss: 27.3107\n",
      "Epoch 448/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9936 - val_loss: 27.3179\n",
      "Epoch 449/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 9.9933 - val_loss: 27.3249\n",
      "Epoch 450/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 9.9929 - val_loss: 27.3314\n",
      "Epoch 451/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 9.9926 - val_loss: 27.3375\n",
      "Epoch 452/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 9.9923 - val_loss: 27.3433\n",
      "Epoch 453/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 9.9920 - val_loss: 27.3490\n",
      "Epoch 454/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 9.9918 - val_loss: 27.3542\n",
      "Epoch 455/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 9.9916 - val_loss: 27.3590\n",
      "Epoch 456/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9913 - val_loss: 27.3637\n",
      "Epoch 457/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9912 - val_loss: 27.3681\n",
      "Epoch 458/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 9.9910 - val_loss: 27.3722\n",
      "Epoch 459/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9909 - val_loss: 27.3763\n",
      "Epoch 460/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9907 - val_loss: 27.3800\n",
      "Epoch 461/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9906 - val_loss: 27.3833\n",
      "Epoch 462/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9905 - val_loss: 27.3868\n",
      "Epoch 463/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 9.9904 - val_loss: 27.3898\n",
      "Epoch 464/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 9.9904 - val_loss: 27.3927\n",
      "Epoch 465/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9903 - val_loss: 27.3956\n",
      "Epoch 466/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 9.9902 - val_loss: 27.3981\n",
      "Epoch 467/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9902 - val_loss: 27.4005\n",
      "Epoch 468/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 9.9902 - val_loss: 27.4026\n",
      "Epoch 469/500\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 9.9902 - val_loss: 27.4047\n",
      "Epoch 470/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 9.9902 - val_loss: 27.4066\n",
      "Epoch 471/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9902 - val_loss: 27.4083\n",
      "Epoch 472/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9902 - val_loss: 27.4101\n",
      "Epoch 473/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9902 - val_loss: 27.4115\n",
      "Epoch 474/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9903 - val_loss: 27.4130\n",
      "Epoch 475/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9903 - val_loss: 27.4144\n",
      "Epoch 476/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9904 - val_loss: 27.4156\n",
      "Epoch 477/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.9904 - val_loss: 27.4168\n",
      "Epoch 478/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.9905 - val_loss: 27.4179\n",
      "Epoch 479/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.9906 - val_loss: 27.4189\n",
      "Epoch 480/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.9907 - val_loss: 27.4199\n",
      "Epoch 481/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9907 - val_loss: 27.4209\n",
      "Epoch 482/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9908 - val_loss: 27.4216\n",
      "Epoch 483/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.9909 - val_loss: 27.4223\n",
      "Epoch 484/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.9910 - val_loss: 27.4228\n",
      "Epoch 485/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 9.9911 - val_loss: 27.4232\n",
      "Epoch 486/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9913 - val_loss: 27.4240\n",
      "Epoch 487/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9913 - val_loss: 27.4245\n",
      "Epoch 488/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9915 - val_loss: 27.4247\n",
      "Epoch 489/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.9916 - val_loss: 27.4254\n",
      "Epoch 490/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.9917 - val_loss: 27.4256\n",
      "Epoch 491/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 9.9918 - val_loss: 27.4256\n",
      "Epoch 492/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9920 - val_loss: 27.4260\n",
      "Epoch 493/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.9921 - val_loss: 27.4262\n",
      "Epoch 494/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9922 - val_loss: 27.4264\n",
      "Epoch 495/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.9924 - val_loss: 27.4268\n",
      "Epoch 496/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.9925 - val_loss: 27.4268\n",
      "Epoch 497/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9926 - val_loss: 27.4269\n",
      "Epoch 498/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.9928 - val_loss: 27.4269\n",
      "Epoch 499/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.9929 - val_loss: 27.4269\n",
      "Epoch 500/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9931 - val_loss: 27.4271\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.1552, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0026, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0004, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0102, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.8734, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1e-10\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 456ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.12368628e+01, 7.12142484e+01, 7.12052848e+01, 7.11963212e+01,\n",
       "        7.11873576e+01, 7.11783940e+01, 7.11694304e+01, 7.11604669e+01,\n",
       "        7.11515033e+01, 7.11425397e+01, 7.11335761e+01, 7.11246125e+01,\n",
       "        7.11156489e+01, 7.11066853e+01, 7.10977218e+01, 7.10887582e+01,\n",
       "        7.10797946e+01, 7.10708310e+01, 7.10618674e+01, 7.10529038e+01,\n",
       "        7.10439402e+01, 7.10349767e+01, 7.10260131e+01, 7.10170495e+01,\n",
       "        7.10080859e+01, 7.09991223e+01, 7.09901587e+01, 7.09811951e+01,\n",
       "        7.09722316e+01, 7.09632680e+01, 7.09543044e+01, 7.09453408e+01,\n",
       "        7.09363772e+01, 7.09274136e+01, 7.09184501e+01, 7.09094865e+01,\n",
       "        7.09005229e+01, 7.08920868e+01, 7.08836835e+01, 7.08752801e+01,\n",
       "        7.08668768e+01, 7.08584734e+01, 7.08500700e+01, 7.08416667e+01,\n",
       "        7.08332633e+01, 7.08248599e+01, 7.08164566e+01, 7.08080532e+01,\n",
       "        7.07996499e+01, 7.07912465e+01, 7.07828431e+01, 7.07744398e+01,\n",
       "        7.07660364e+01, 7.07576330e+01, 7.07492297e+01, 7.07408263e+01,\n",
       "        7.07324230e+01, 7.07240196e+01, 7.07156162e+01, 7.07072129e+01,\n",
       "        7.06988095e+01, 7.06904062e+01, 7.06820028e+01, 7.06735994e+01,\n",
       "        7.06651961e+01, 7.06567927e+01, 7.06483894e+01, 7.06399860e+01,\n",
       "        7.06315826e+01, 7.06231793e+01, 7.06147759e+01, 7.06063726e+01,\n",
       "        7.05952614e+01, 7.05756536e+01, 7.05560457e+01, 7.05364379e+01,\n",
       "        7.05168301e+01, 7.04972222e+01, 7.04776144e+01, 7.04580065e+01,\n",
       "        7.60577087e+01, 4.10622284e-02, 0.00000000e+00, 7.18524694e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.08696127e-01,\n",
       "        2.94023246e-01, 0.00000000e+00, 1.05459578e-01, 8.72619152e-02,\n",
       "        6.36032522e-01, 8.34656477e-01, 1.57308318e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.65577948e-02, 0.00000000e+00, 3.19186717e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([68.25553221, 68.25086368, 68.24619514, 68.24152661, 68.23685808,\n",
       "       68.23218954, 68.22752101, 68.22285247, 68.21818394, 68.21351541,\n",
       "       68.20884687, 68.20417834, 68.1995098 , 68.19484127, 68.19017274,\n",
       "       68.1855042 , 68.18083567, 68.17616713, 68.1714986 , 68.16683007,\n",
       "       68.16216153, 68.157493  , 68.15282446, 68.14815593, 68.14348739,\n",
       "       68.13881886, 68.13415033, 68.12948179, 68.12481326, 68.12014472,\n",
       "       68.11547619, 68.11080766, 68.10613912, 68.10147059, 68.09680205,\n",
       "       68.09213352, 68.08746499, 68.08279645, 68.07812792, 68.07345938,\n",
       "       68.06879085, 68.06412232, 68.05945378, 68.05478525, 68.05011671,\n",
       "       68.04544818, 68.04077965, 68.03611111, 68.03144258, 68.02677404,\n",
       "       68.02210551, 68.01743697, 68.01276844, 68.00809991, 68.00343137,\n",
       "       67.99876284, 67.9940943 , 67.98942577, 67.98475724, 67.9800887 ,\n",
       "       67.97542017, 67.97075163, 67.9660831 , 67.96141457, 67.95674603,\n",
       "       67.9520775 , 67.94740896, 67.94274043, 67.9380719 , 67.93340336,\n",
       "       67.92873483, 67.92406629, 67.91939776, 67.91472923, 67.91006069,\n",
       "       67.90539216, 67.90072362, 67.89605509, 67.89138655, 67.88671802,\n",
       "       67.88204949, 67.87738095, 67.87271242, 67.86804388, 67.86337535,\n",
       "       67.85870682, 67.85403828, 67.84936975, 67.84470121, 67.84003268,\n",
       "       67.83536415, 67.83069561, 67.82602708, 67.82135854, 67.81669001,\n",
       "       67.81202148, 67.80735294, 67.80268441, 67.79603175, 67.78669468])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.59867219590584\n",
      "15.172605959988045\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
