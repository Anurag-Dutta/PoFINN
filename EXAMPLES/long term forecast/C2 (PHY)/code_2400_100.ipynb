{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "2495    52.162308\n",
       "2496    52.146731\n",
       "2497    52.131154\n",
       "2498    52.115577\n",
       "2499    52.100000\n",
       "Name: C2, Length: 2500, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c2_interpolated_2400_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "2395     0.000000\n",
       "2396     0.000000\n",
       "2397     0.295087\n",
       "2398     0.356222\n",
       "2399     0.510593\n",
       "Name: C2, Length: 2400, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2400)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.500000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.275910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.051821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.827731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.603641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     90.500000  0.000298\n",
       "1     90.275910  0.000298\n",
       "2     90.051821  0.000297\n",
       "3     89.827731  0.000297\n",
       "4     89.603641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.1552  # Value for C0\n",
    "K0 = -0.0026  # Value for K0\n",
    "K1 = -0.0004  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0102    # Value for b\n",
    "c = 2.8734    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    90.500000    90.275910    90.051821    89.827731    89.603641   \n",
      "351    90.275910    90.051821    89.827731    89.603641    89.379552   \n",
      "352    90.051821    89.827731    89.603641    89.379552    89.155462   \n",
      "353    89.827731    89.603641    89.379552    89.155462    88.931373   \n",
      "354    89.603641    89.379552    89.155462    88.931373    88.707283   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    89.379552    89.155462    88.931373    88.707283    88.494958  ...   \n",
      "351    89.155462    88.931373    88.707283    88.494958    88.427731  ...   \n",
      "352    88.931373    88.707283    88.494958    88.427731    88.360504  ...   \n",
      "353    88.707283    88.494958    88.427731    88.360504    88.293277  ...   \n",
      "354    88.494958    88.427731    88.360504    88.293277    88.226050  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   79.071008    0.000263   79.054202    0.000263   79.037395    0.000263   \n",
      "351   79.054202    0.000263   79.037395    0.000263   79.020588    0.000262   \n",
      "352   79.037395    0.000263   79.020588    0.000262   79.003782    0.000262   \n",
      "353   79.020588    0.000262   79.003782    0.000262   78.986975    0.000262   \n",
      "354   79.003782    0.000262   78.986975    0.000262   78.970168    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   79.020588    0.000262   79.003782    0.000262  \n",
      "351   79.003782    0.000262   78.986975    0.000262  \n",
      "352   78.986975    0.000262   78.970168    0.000262  \n",
      "353   78.970168    0.000262   78.953361    0.000262  \n",
      "354   78.953361    0.000262   78.936555    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1950, 1, 251) (1950, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "25/25 [==============================] - 2s 22ms/step - loss: 4882.0938 - val_loss: 2760.6843\n",
      "Epoch 2/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 4687.7974 - val_loss: 2639.8967\n",
      "Epoch 3/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 4534.8672 - val_loss: 2555.8855\n",
      "Epoch 4/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4403.1880 - val_loss: 2478.6099\n",
      "Epoch 5/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 4271.9526 - val_loss: 2404.3757\n",
      "Epoch 6/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 4153.5303 - val_loss: 2338.6169\n",
      "Epoch 7/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 4040.2195 - val_loss: 2275.7339\n",
      "Epoch 8/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3930.7173 - val_loss: 2215.5178\n",
      "Epoch 9/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 3824.4395 - val_loss: 2157.6018\n",
      "Epoch 10/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 3721.0500 - val_loss: 2101.8181\n",
      "Epoch 11/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 3620.3289 - val_loss: 2048.0476\n",
      "Epoch 12/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 3522.1243 - val_loss: 1996.1993\n",
      "Epoch 13/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 3426.3193 - val_loss: 1946.2003\n",
      "Epoch 14/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 3332.8196 - val_loss: 1897.9894\n",
      "Epoch 15/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 3241.5503 - val_loss: 1851.5125\n",
      "Epoch 16/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3152.4443 - val_loss: 1806.7214\n",
      "Epoch 17/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 3065.4429 - val_loss: 1763.5729\n",
      "Epoch 18/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 2980.4951 - val_loss: 1722.0258\n",
      "Epoch 19/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2897.5535 - val_loss: 1682.0424\n",
      "Epoch 20/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2816.5728 - val_loss: 1643.5854\n",
      "Epoch 21/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2737.5129 - val_loss: 1606.6215\n",
      "Epoch 22/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2660.3337 - val_loss: 1571.1169\n",
      "Epoch 23/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 2584.9988 - val_loss: 1537.0396\n",
      "Epoch 24/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2511.4731 - val_loss: 1504.3582\n",
      "Epoch 25/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2439.7212 - val_loss: 1473.0419\n",
      "Epoch 26/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2369.7109 - val_loss: 1443.0599\n",
      "Epoch 27/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2301.4097 - val_loss: 1414.3802\n",
      "Epoch 28/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 2234.7871 - val_loss: 1386.9166\n",
      "Epoch 29/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 2169.2329 - val_loss: 1355.6266\n",
      "Epoch 30/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2090.6838 - val_loss: 1328.1746\n",
      "Epoch 31/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 2022.9022 - val_loss: 1302.7355\n",
      "Epoch 32/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1958.3989 - val_loss: 1279.0786\n",
      "Epoch 33/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1896.5103 - val_loss: 1256.9476\n",
      "Epoch 34/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1836.8136 - val_loss: 1236.2000\n",
      "Epoch 35/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1779.0801 - val_loss: 1216.7449\n",
      "Epoch 36/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1723.1669 - val_loss: 1198.5156\n",
      "Epoch 37/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1668.9723 - val_loss: 1181.4594\n",
      "Epoch 38/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1616.4194 - val_loss: 1165.5299\n",
      "Epoch 39/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1565.4441 - val_loss: 1150.6871\n",
      "Epoch 40/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1515.9927 - val_loss: 1136.8940\n",
      "Epoch 41/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1468.0170 - val_loss: 1124.1163\n",
      "Epoch 42/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1421.4745 - val_loss: 1112.3215\n",
      "Epoch 43/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1376.3257 - val_loss: 1101.4791\n",
      "Epoch 44/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1332.5337 - val_loss: 1091.5594\n",
      "Epoch 45/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1290.0643 - val_loss: 1082.5339\n",
      "Epoch 46/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1248.8846 - val_loss: 1074.3749\n",
      "Epoch 47/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1208.9631 - val_loss: 1067.0549\n",
      "Epoch 48/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1170.2697 - val_loss: 1060.5482\n",
      "Epoch 49/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1132.7751 - val_loss: 1054.8289\n",
      "Epoch 50/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1096.4514 - val_loss: 1049.8716\n",
      "Epoch 51/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1061.2705 - val_loss: 1045.6515\n",
      "Epoch 52/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1027.2065 - val_loss: 1042.1444\n",
      "Epoch 53/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 994.2326 - val_loss: 1039.3263\n",
      "Epoch 54/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 962.3235 - val_loss: 1037.1735\n",
      "Epoch 55/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 931.4539 - val_loss: 1035.6628\n",
      "Epoch 56/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 901.5999 - val_loss: 1034.7717\n",
      "Epoch 57/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 872.7365 - val_loss: 1034.4771\n",
      "Epoch 58/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 844.8401 - val_loss: 1034.7570\n",
      "Epoch 59/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 817.8881 - val_loss: 1035.5895\n",
      "Epoch 60/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 791.8571 - val_loss: 1036.9530\n",
      "Epoch 61/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 766.7241 - val_loss: 1038.8259\n",
      "Epoch 62/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 742.4675 - val_loss: 1041.1877\n",
      "Epoch 63/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 719.0652 - val_loss: 1044.0173\n",
      "Epoch 64/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 696.4955 - val_loss: 1047.2944\n",
      "Epoch 65/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 674.7369 - val_loss: 1050.9987\n",
      "Epoch 66/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 653.7693 - val_loss: 1055.1104\n",
      "Epoch 67/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 633.5708 - val_loss: 1059.6102\n",
      "Epoch 68/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 614.1221 - val_loss: 1064.4783\n",
      "Epoch 69/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 595.4025 - val_loss: 1069.6962\n",
      "Epoch 70/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 577.3925 - val_loss: 1075.2450\n",
      "Epoch 71/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 560.0723 - val_loss: 1081.1063\n",
      "Epoch 72/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 543.4230 - val_loss: 1087.2621\n",
      "Epoch 73/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 527.4252 - val_loss: 1093.6946\n",
      "Epoch 74/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 512.0609 - val_loss: 1100.3866\n",
      "Epoch 75/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 497.3112 - val_loss: 1107.3203\n",
      "Epoch 76/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 483.1581 - val_loss: 1114.4796\n",
      "Epoch 77/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 469.5836 - val_loss: 1121.8478\n",
      "Epoch 78/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 456.5705 - val_loss: 1129.4084\n",
      "Epoch 79/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 444.1015 - val_loss: 1137.1461\n",
      "Epoch 80/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 432.1595 - val_loss: 1145.0449\n",
      "Epoch 81/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 420.7281 - val_loss: 1153.0897\n",
      "Epoch 82/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 409.7907 - val_loss: 1161.2660\n",
      "Epoch 83/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 399.3311 - val_loss: 1169.5591\n",
      "Epoch 84/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 389.3335 - val_loss: 1177.9551\n",
      "Epoch 85/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 379.7825 - val_loss: 1186.4398\n",
      "Epoch 86/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 370.6629 - val_loss: 1195.0006\n",
      "Epoch 87/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 361.9595 - val_loss: 1203.6235\n",
      "Epoch 88/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 353.6581 - val_loss: 1212.2968\n",
      "Epoch 89/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 345.7441 - val_loss: 1221.0073\n",
      "Epoch 90/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 338.2033 - val_loss: 1229.7448\n",
      "Epoch 91/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 331.0222 - val_loss: 1238.4955\n",
      "Epoch 92/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 324.1874 - val_loss: 1247.2499\n",
      "Epoch 93/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 317.6857 - val_loss: 1255.9966\n",
      "Epoch 94/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 311.5042 - val_loss: 1264.7258\n",
      "Epoch 95/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 305.6304 - val_loss: 1273.4272\n",
      "Epoch 96/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 300.0522 - val_loss: 1282.0918\n",
      "Epoch 97/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 294.7574 - val_loss: 1290.7091\n",
      "Epoch 98/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 289.7349 - val_loss: 1299.2719\n",
      "Epoch 99/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 284.9730 - val_loss: 1307.7711\n",
      "Epoch 100/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 280.4608 - val_loss: 1316.1986\n",
      "Epoch 101/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 276.1876 - val_loss: 1324.5472\n",
      "Epoch 102/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 272.1431 - val_loss: 1332.8098\n",
      "Epoch 103/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 268.3172 - val_loss: 1340.9794\n",
      "Epoch 104/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 264.7000 - val_loss: 1349.0503\n",
      "Epoch 105/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 261.2822 - val_loss: 1357.0155\n",
      "Epoch 106/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 258.0545 - val_loss: 1364.8700\n",
      "Epoch 107/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 255.0083 - val_loss: 1372.6085\n",
      "Epoch 108/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 252.1345 - val_loss: 1380.2263\n",
      "Epoch 109/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 249.4253 - val_loss: 1387.7184\n",
      "Epoch 110/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 246.8724 - val_loss: 1395.0820\n",
      "Epoch 111/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 244.4682 - val_loss: 1402.3119\n",
      "Epoch 112/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 242.2052 - val_loss: 1409.4056\n",
      "Epoch 113/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 240.0763 - val_loss: 1416.3596\n",
      "Epoch 114/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 238.0747 - val_loss: 1423.1716\n",
      "Epoch 115/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 236.1936 - val_loss: 1429.8397\n",
      "Epoch 116/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 234.4266 - val_loss: 1436.3608\n",
      "Epoch 117/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 232.7678 - val_loss: 1442.7346\n",
      "Epoch 118/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 231.2112 - val_loss: 1448.9575\n",
      "Epoch 119/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 229.7514 - val_loss: 1455.0309\n",
      "Epoch 120/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 228.3827 - val_loss: 1460.9532\n",
      "Epoch 121/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 227.1003 - val_loss: 1466.7227\n",
      "Epoch 122/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 225.8993 - val_loss: 1472.3412\n",
      "Epoch 123/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 224.7747 - val_loss: 1477.8063\n",
      "Epoch 124/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 223.7225 - val_loss: 1483.1204\n",
      "Epoch 125/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.7382 - val_loss: 1488.2838\n",
      "Epoch 126/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 221.8178 - val_loss: 1493.2958\n",
      "Epoch 127/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 220.9576 - val_loss: 1498.1589\n",
      "Epoch 128/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 220.1537 - val_loss: 1502.8735\n",
      "Epoch 129/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 219.4029 - val_loss: 1507.4396\n",
      "Epoch 130/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.7020 - val_loss: 1511.8622\n",
      "Epoch 131/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 218.0477 - val_loss: 1516.1400\n",
      "Epoch 132/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 217.4370 - val_loss: 1520.2756\n",
      "Epoch 133/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 216.8674 - val_loss: 1524.2716\n",
      "Epoch 134/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 216.3362 - val_loss: 1528.1298\n",
      "Epoch 135/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 215.8408 - val_loss: 1531.8525\n",
      "Epoch 136/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 215.3790 - val_loss: 1535.4420\n",
      "Epoch 137/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 214.9486 - val_loss: 1538.9008\n",
      "Epoch 138/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 214.5474 - val_loss: 1542.2314\n",
      "Epoch 139/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 214.1737 - val_loss: 1545.4365\n",
      "Epoch 140/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 213.8255 - val_loss: 1548.5201\n",
      "Epoch 141/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 213.5012 - val_loss: 1551.4822\n",
      "Epoch 142/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 213.1992 - val_loss: 1554.3287\n",
      "Epoch 143/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 212.9178 - val_loss: 1557.0603\n",
      "Epoch 144/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 212.6559 - val_loss: 1559.6807\n",
      "Epoch 145/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 212.4119 - val_loss: 1562.1936\n",
      "Epoch 146/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 212.1848 - val_loss: 1564.6000\n",
      "Epoch 147/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 211.9733 - val_loss: 1566.9058\n",
      "Epoch 148/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 211.7763 - val_loss: 1569.1117\n",
      "Epoch 149/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 211.5929 - val_loss: 1571.2209\n",
      "Epoch 150/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 211.4221 - val_loss: 1573.2367\n",
      "Epoch 151/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 211.2631 - val_loss: 1575.1630\n",
      "Epoch 152/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 211.1151 - val_loss: 1577.0016\n",
      "Epoch 153/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 210.9772 - val_loss: 1578.7562\n",
      "Epoch 154/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 210.8487 - val_loss: 1580.4294\n",
      "Epoch 155/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 210.7290 - val_loss: 1582.0242\n",
      "Epoch 156/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 210.6178 - val_loss: 1583.5420\n",
      "Epoch 157/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 210.5139 - val_loss: 1584.9883\n",
      "Epoch 158/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 210.4172 - val_loss: 1586.3639\n",
      "Epoch 159/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 210.3271 - val_loss: 1587.6721\n",
      "Epoch 160/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 210.2433 - val_loss: 1588.9154\n",
      "Epoch 161/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 210.1651 - val_loss: 1590.0963\n",
      "Epoch 162/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 210.0922 - val_loss: 1591.2174\n",
      "Epoch 163/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 210.0243 - val_loss: 1592.2806\n",
      "Epoch 164/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 209.9611 - val_loss: 1593.2899\n",
      "Epoch 165/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 209.9021 - val_loss: 1594.2457\n",
      "Epoch 166/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.8471 - val_loss: 1595.1515\n",
      "Epoch 167/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.7960 - val_loss: 1596.0090\n",
      "Epoch 168/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.7482 - val_loss: 1596.8207\n",
      "Epoch 169/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.7038 - val_loss: 1597.5880\n",
      "Epoch 170/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.6624 - val_loss: 1598.3138\n",
      "Epoch 171/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.6237 - val_loss: 1598.9999\n",
      "Epoch 172/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.5877 - val_loss: 1599.6482\n",
      "Epoch 173/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.5542 - val_loss: 1600.2593\n",
      "Epoch 174/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.5230 - val_loss: 1600.8369\n",
      "Epoch 175/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.4940 - val_loss: 1601.3817\n",
      "Epoch 176/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.4668 - val_loss: 1601.8945\n",
      "Epoch 177/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.4416 - val_loss: 1602.3779\n",
      "Epoch 178/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.4182 - val_loss: 1602.8329\n",
      "Epoch 179/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.3963 - val_loss: 1603.2616\n",
      "Epoch 180/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.3760 - val_loss: 1603.6650\n",
      "Epoch 181/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.3571 - val_loss: 1604.0441\n",
      "Epoch 182/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.3396 - val_loss: 1604.4017\n",
      "Epoch 183/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.3232 - val_loss: 1604.7361\n",
      "Epoch 184/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.3080 - val_loss: 1605.0518\n",
      "Epoch 185/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.2939 - val_loss: 1605.3472\n",
      "Epoch 186/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.2809 - val_loss: 1605.6246\n",
      "Epoch 187/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.2688 - val_loss: 1605.8848\n",
      "Epoch 188/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.2575 - val_loss: 1606.1295\n",
      "Epoch 189/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.2470 - val_loss: 1606.3580\n",
      "Epoch 190/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.2374 - val_loss: 1606.5728\n",
      "Epoch 191/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.2284 - val_loss: 1606.7732\n",
      "Epoch 192/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 209.2202 - val_loss: 1606.9619\n",
      "Epoch 193/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.2126 - val_loss: 1607.1375\n",
      "Epoch 194/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 209.2055 - val_loss: 1607.3024\n",
      "Epoch 195/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 209.1990 - val_loss: 1607.4569\n",
      "Epoch 196/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1929 - val_loss: 1607.6006\n",
      "Epoch 197/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1874 - val_loss: 1607.7355\n",
      "Epoch 198/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1823 - val_loss: 1607.8612\n",
      "Epoch 199/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1777 - val_loss: 1607.9789\n",
      "Epoch 200/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1734 - val_loss: 1608.0890\n",
      "Epoch 201/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1694 - val_loss: 1608.1910\n",
      "Epoch 202/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1658 - val_loss: 1608.2869\n",
      "Epoch 203/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1626 - val_loss: 1608.3759\n",
      "Epoch 204/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1596 - val_loss: 1608.4590\n",
      "Epoch 205/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1569 - val_loss: 1608.5365\n",
      "Epoch 206/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1544 - val_loss: 1608.6088\n",
      "Epoch 207/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1522 - val_loss: 1608.6763\n",
      "Epoch 208/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1501 - val_loss: 1608.7377\n",
      "Epoch 209/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1483 - val_loss: 1608.7966\n",
      "Epoch 210/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1467 - val_loss: 1608.8506\n",
      "Epoch 211/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1453 - val_loss: 1608.9020\n",
      "Epoch 212/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1439 - val_loss: 1608.9479\n",
      "Epoch 213/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 209.1428 - val_loss: 1608.9915\n",
      "Epoch 214/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1418 - val_loss: 1609.0317\n",
      "Epoch 215/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1410 - val_loss: 1609.0696\n",
      "Epoch 216/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1401 - val_loss: 1609.1040\n",
      "Epoch 217/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1395 - val_loss: 1609.1372\n",
      "Epoch 218/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1389 - val_loss: 1609.1669\n",
      "Epoch 219/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1385 - val_loss: 1609.1956\n",
      "Epoch 220/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1381 - val_loss: 1609.2214\n",
      "Epoch 221/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1378 - val_loss: 1609.2454\n",
      "Epoch 222/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1376 - val_loss: 1609.2676\n",
      "Epoch 223/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1374 - val_loss: 1609.2883\n",
      "Epoch 224/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1373 - val_loss: 1609.3079\n",
      "Epoch 225/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 209.1372 - val_loss: 1609.3257\n",
      "Epoch 226/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1373 - val_loss: 1609.3420\n",
      "Epoch 227/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1373 - val_loss: 1609.3572\n",
      "Epoch 228/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1374 - val_loss: 1609.3715\n",
      "Epoch 229/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1375 - val_loss: 1609.3849\n",
      "Epoch 230/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1376 - val_loss: 1609.3966\n",
      "Epoch 231/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1378 - val_loss: 1609.4078\n",
      "Epoch 232/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1380 - val_loss: 1609.4187\n",
      "Epoch 233/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1382 - val_loss: 1609.4281\n",
      "Epoch 234/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1385 - val_loss: 1609.4369\n",
      "Epoch 235/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1387 - val_loss: 1609.4451\n",
      "Epoch 236/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1391 - val_loss: 1609.4532\n",
      "Epoch 237/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1394 - val_loss: 1609.4595\n",
      "Epoch 238/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1398 - val_loss: 1609.4667\n",
      "Epoch 239/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1400 - val_loss: 1609.4724\n",
      "Epoch 240/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1404 - val_loss: 1609.4780\n",
      "Epoch 241/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1408 - val_loss: 1609.4830\n",
      "Epoch 242/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1411 - val_loss: 1609.4882\n",
      "Epoch 243/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1415 - val_loss: 1609.4924\n",
      "Epoch 244/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1418 - val_loss: 1609.4965\n",
      "Epoch 245/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1422 - val_loss: 1609.5000\n",
      "Epoch 246/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1427 - val_loss: 1609.5032\n",
      "Epoch 247/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1431 - val_loss: 1609.5068\n",
      "Epoch 248/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1434 - val_loss: 1609.5101\n",
      "Epoch 249/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1438 - val_loss: 1609.5129\n",
      "Epoch 250/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1441 - val_loss: 1609.5149\n",
      "Epoch 251/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 209.1445 - val_loss: 1609.5178\n",
      "Epoch 252/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 209.1450 - val_loss: 1609.5197\n",
      "Epoch 253/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1452 - val_loss: 1609.5221\n",
      "Epoch 254/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1457 - val_loss: 1609.5234\n",
      "Epoch 255/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1461 - val_loss: 1609.5256\n",
      "Epoch 256/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1463 - val_loss: 1609.5271\n",
      "Epoch 257/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1467 - val_loss: 1609.5282\n",
      "Epoch 258/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1471 - val_loss: 1609.5288\n",
      "Epoch 259/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1475 - val_loss: 1609.5302\n",
      "Epoch 260/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1479 - val_loss: 1609.5314\n",
      "Epoch 261/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1482 - val_loss: 1609.5326\n",
      "Epoch 262/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1485 - val_loss: 1609.5334\n",
      "Epoch 263/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1489 - val_loss: 1609.5343\n",
      "Epoch 264/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1492 - val_loss: 1609.5354\n",
      "Epoch 265/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1496 - val_loss: 1609.5360\n",
      "Epoch 266/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1499 - val_loss: 1609.5365\n",
      "Epoch 267/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1502 - val_loss: 1609.5374\n",
      "Epoch 268/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1505 - val_loss: 1609.5375\n",
      "Epoch 269/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1508 - val_loss: 1609.5378\n",
      "Epoch 270/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1511 - val_loss: 1609.5381\n",
      "Epoch 271/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1514 - val_loss: 1609.5389\n",
      "Epoch 272/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1517 - val_loss: 1609.5393\n",
      "Epoch 273/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1520 - val_loss: 1609.5398\n",
      "Epoch 274/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1523 - val_loss: 1609.5404\n",
      "Epoch 275/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1526 - val_loss: 1609.5406\n",
      "Epoch 276/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1528 - val_loss: 1609.5409\n",
      "Epoch 277/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1531 - val_loss: 1609.5410\n",
      "Epoch 278/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1534 - val_loss: 1609.5416\n",
      "Epoch 279/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 209.1536 - val_loss: 1609.5420\n",
      "Epoch 280/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1538 - val_loss: 1609.5420\n",
      "Epoch 281/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1541 - val_loss: 1609.5421\n",
      "Epoch 282/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 209.1543 - val_loss: 1609.5425\n",
      "Epoch 283/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1546 - val_loss: 1609.5425\n",
      "Epoch 284/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1548 - val_loss: 1609.5420\n",
      "Epoch 285/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1551 - val_loss: 1609.5421\n",
      "Epoch 286/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1552 - val_loss: 1609.5425\n",
      "Epoch 287/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1555 - val_loss: 1609.5425\n",
      "Epoch 288/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1557 - val_loss: 1609.5430\n",
      "Epoch 289/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1559 - val_loss: 1609.5425\n",
      "Epoch 290/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1561 - val_loss: 1609.5431\n",
      "Epoch 291/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1563 - val_loss: 1609.5431\n",
      "Epoch 292/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1564 - val_loss: 1609.5431\n",
      "Epoch 293/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1567 - val_loss: 1609.5431\n",
      "Epoch 294/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1568 - val_loss: 1609.5431\n",
      "Epoch 295/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1570 - val_loss: 1609.5431\n",
      "Epoch 296/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1572 - val_loss: 1609.5436\n",
      "Epoch 297/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1574 - val_loss: 1609.5437\n",
      "Epoch 298/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1575 - val_loss: 1609.5437\n",
      "Epoch 299/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1578 - val_loss: 1609.5441\n",
      "Epoch 300/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1579 - val_loss: 1609.5442\n",
      "Epoch 301/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1581 - val_loss: 1609.5437\n",
      "Epoch 302/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1582 - val_loss: 1609.5437\n",
      "Epoch 303/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1583 - val_loss: 1609.5436\n",
      "Epoch 304/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1585 - val_loss: 1609.5437\n",
      "Epoch 305/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 209.1586 - val_loss: 1609.5437\n",
      "Epoch 306/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 209.1588 - val_loss: 1609.5437\n",
      "Epoch 307/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1589 - val_loss: 1609.5436\n",
      "Epoch 308/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1591 - val_loss: 1609.5437\n",
      "Epoch 309/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1592 - val_loss: 1609.5437\n",
      "Epoch 310/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1593 - val_loss: 1609.5436\n",
      "Epoch 311/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1595 - val_loss: 1609.5437\n",
      "Epoch 312/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1596 - val_loss: 1609.5437\n",
      "Epoch 313/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1597 - val_loss: 1609.5442\n",
      "Epoch 314/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1598 - val_loss: 1609.5442\n",
      "Epoch 315/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1599 - val_loss: 1609.5442\n",
      "Epoch 316/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1600 - val_loss: 1609.5442\n",
      "Epoch 317/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1601 - val_loss: 1609.5437\n",
      "Epoch 318/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1603 - val_loss: 1609.5437\n",
      "Epoch 319/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1603 - val_loss: 1609.5437\n",
      "Epoch 320/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1605 - val_loss: 1609.5437\n",
      "Epoch 321/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1605 - val_loss: 1609.5437\n",
      "Epoch 322/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1607 - val_loss: 1609.5442\n",
      "Epoch 323/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1607 - val_loss: 1609.5442\n",
      "Epoch 324/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1608 - val_loss: 1609.5442\n",
      "Epoch 325/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1609 - val_loss: 1609.5442\n",
      "Epoch 326/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1610 - val_loss: 1609.5442\n",
      "Epoch 327/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1611 - val_loss: 1609.5442\n",
      "Epoch 328/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1611 - val_loss: 1609.5442\n",
      "Epoch 329/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1612 - val_loss: 1609.5447\n",
      "Epoch 330/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1613 - val_loss: 1609.5449\n",
      "Epoch 331/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1614 - val_loss: 1609.5447\n",
      "Epoch 332/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1614 - val_loss: 1609.5447\n",
      "Epoch 333/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1615 - val_loss: 1609.5449\n",
      "Epoch 334/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1616 - val_loss: 1609.5447\n",
      "Epoch 335/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1617 - val_loss: 1609.5447\n",
      "Epoch 336/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1617 - val_loss: 1609.5446\n",
      "Epoch 337/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 209.1618 - val_loss: 1609.5442\n",
      "Epoch 338/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 209.1619 - val_loss: 1609.5447\n",
      "Epoch 339/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1619 - val_loss: 1609.5446\n",
      "Epoch 340/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1620 - val_loss: 1609.5446\n",
      "Epoch 341/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1621 - val_loss: 1609.5441\n",
      "Epoch 342/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1621 - val_loss: 1609.5441\n",
      "Epoch 343/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1622 - val_loss: 1609.5446\n",
      "Epoch 344/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1622 - val_loss: 1609.5441\n",
      "Epoch 345/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1623 - val_loss: 1609.5441\n",
      "Epoch 346/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1623 - val_loss: 1609.5439\n",
      "Epoch 347/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1624 - val_loss: 1609.5437\n",
      "Epoch 348/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1624 - val_loss: 1609.5436\n",
      "Epoch 349/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1625 - val_loss: 1609.5437\n",
      "Epoch 350/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1625 - val_loss: 1609.5437\n",
      "Epoch 351/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1626 - val_loss: 1609.5435\n",
      "Epoch 352/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1627 - val_loss: 1609.5437\n",
      "Epoch 353/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1627 - val_loss: 1609.5437\n",
      "Epoch 354/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1627 - val_loss: 1609.5432\n",
      "Epoch 355/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1628 - val_loss: 1609.5432\n",
      "Epoch 356/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1628 - val_loss: 1609.5436\n",
      "Epoch 357/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1629 - val_loss: 1609.5436\n",
      "Epoch 358/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1629 - val_loss: 1609.5437\n",
      "Epoch 359/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1629 - val_loss: 1609.5437\n",
      "Epoch 360/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 209.1629 - val_loss: 1609.5442\n",
      "Epoch 361/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 209.1629 - val_loss: 1609.5437\n",
      "Epoch 362/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 209.1630 - val_loss: 1609.5437\n",
      "Epoch 363/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1630 - val_loss: 1609.5437\n",
      "Epoch 364/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1631 - val_loss: 1609.5437\n",
      "Epoch 365/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1631 - val_loss: 1609.5437\n",
      "Epoch 366/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1632 - val_loss: 1609.5437\n",
      "Epoch 367/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1632 - val_loss: 1609.5437\n",
      "Epoch 368/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1632 - val_loss: 1609.5437\n",
      "Epoch 369/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1632 - val_loss: 1609.5436\n",
      "Epoch 370/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1633 - val_loss: 1609.5437\n",
      "Epoch 371/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1633 - val_loss: 1609.5437\n",
      "Epoch 372/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1633 - val_loss: 1609.5436\n",
      "Epoch 373/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1634 - val_loss: 1609.5436\n",
      "Epoch 374/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1634 - val_loss: 1609.5439\n",
      "Epoch 375/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1634 - val_loss: 1609.5439\n",
      "Epoch 376/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1634 - val_loss: 1609.5439\n",
      "Epoch 377/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1635 - val_loss: 1609.5439\n",
      "Epoch 378/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1635 - val_loss: 1609.5437\n",
      "Epoch 379/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1635 - val_loss: 1609.5439\n",
      "Epoch 380/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1635 - val_loss: 1609.5437\n",
      "Epoch 381/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1636 - val_loss: 1609.5436\n",
      "Epoch 382/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1636 - val_loss: 1609.5437\n",
      "Epoch 383/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1636 - val_loss: 1609.5437\n",
      "Epoch 384/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1637 - val_loss: 1609.5441\n",
      "Epoch 385/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1637 - val_loss: 1609.5441\n",
      "Epoch 386/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1637 - val_loss: 1609.5447\n",
      "Epoch 387/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1637 - val_loss: 1609.5447\n",
      "Epoch 388/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1637 - val_loss: 1609.5441\n",
      "Epoch 389/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 209.1637 - val_loss: 1609.5441\n",
      "Epoch 390/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1638 - val_loss: 1609.5447\n",
      "Epoch 391/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1637 - val_loss: 1609.5449\n",
      "Epoch 392/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1637 - val_loss: 1609.5450\n",
      "Epoch 393/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1637 - val_loss: 1609.5450\n",
      "Epoch 394/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1638 - val_loss: 1609.5450\n",
      "Epoch 395/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1638 - val_loss: 1609.5447\n",
      "Epoch 396/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1638 - val_loss: 1609.5446\n",
      "Epoch 397/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1638 - val_loss: 1609.5442\n",
      "Epoch 398/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1638 - val_loss: 1609.5437\n",
      "Epoch 399/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1639 - val_loss: 1609.5437\n",
      "Epoch 400/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1639 - val_loss: 1609.5439\n",
      "Epoch 401/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1639 - val_loss: 1609.5437\n",
      "Epoch 402/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1639 - val_loss: 1609.5437\n",
      "Epoch 403/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1640 - val_loss: 1609.5439\n",
      "Epoch 404/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1639 - val_loss: 1609.5439\n",
      "Epoch 405/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1640 - val_loss: 1609.5439\n",
      "Epoch 406/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1640 - val_loss: 1609.5442\n",
      "Epoch 407/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1640 - val_loss: 1609.5446\n",
      "Epoch 408/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1640 - val_loss: 1609.5447\n",
      "Epoch 409/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1640 - val_loss: 1609.5447\n",
      "Epoch 410/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1640 - val_loss: 1609.5450\n",
      "Epoch 411/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1640 - val_loss: 1609.5450\n",
      "Epoch 412/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1640 - val_loss: 1609.5450\n",
      "Epoch 413/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 209.1640 - val_loss: 1609.5449\n",
      "Epoch 414/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1640 - val_loss: 1609.5447\n",
      "Epoch 415/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1640 - val_loss: 1609.5447\n",
      "Epoch 416/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1640 - val_loss: 1609.5441\n",
      "Epoch 417/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1641 - val_loss: 1609.5441\n",
      "Epoch 418/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1641 - val_loss: 1609.5442\n",
      "Epoch 419/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1642 - val_loss: 1609.5442\n",
      "Epoch 420/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1641 - val_loss: 1609.5441\n",
      "Epoch 421/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1641 - val_loss: 1609.5437\n",
      "Epoch 422/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1642 - val_loss: 1609.5435\n",
      "Epoch 423/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1642 - val_loss: 1609.5431\n",
      "Epoch 424/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1642 - val_loss: 1609.5426\n",
      "Epoch 425/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1642 - val_loss: 1609.5425\n",
      "Epoch 426/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1642 - val_loss: 1609.5425\n",
      "Epoch 427/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1642 - val_loss: 1609.5425\n",
      "Epoch 428/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1642 - val_loss: 1609.5426\n",
      "Epoch 429/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5428\n",
      "Epoch 430/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5431\n",
      "Epoch 431/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5435\n",
      "Epoch 432/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5436\n",
      "Epoch 433/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5437\n",
      "Epoch 434/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5437\n",
      "Epoch 435/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 209.1643 - val_loss: 1609.5439\n",
      "Epoch 436/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1642 - val_loss: 1609.5439\n",
      "Epoch 437/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1642 - val_loss: 1609.5439\n",
      "Epoch 438/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1642 - val_loss: 1609.5436\n",
      "Epoch 439/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5436\n",
      "Epoch 440/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1643 - val_loss: 1609.5436\n",
      "Epoch 441/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5436\n",
      "Epoch 442/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5436\n",
      "Epoch 443/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5435\n",
      "Epoch 444/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5431\n",
      "Epoch 445/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5426\n",
      "Epoch 446/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1644 - val_loss: 1609.5421\n",
      "Epoch 447/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1644 - val_loss: 1609.5420\n",
      "Epoch 448/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1644 - val_loss: 1609.5420\n",
      "Epoch 449/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1644 - val_loss: 1609.5420\n",
      "Epoch 450/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1644 - val_loss: 1609.5420\n",
      "Epoch 451/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1644 - val_loss: 1609.5420\n",
      "Epoch 452/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1644 - val_loss: 1609.5420\n",
      "Epoch 453/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1644 - val_loss: 1609.5420\n",
      "Epoch 454/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1644 - val_loss: 1609.5420\n",
      "Epoch 455/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1644 - val_loss: 1609.5424\n",
      "Epoch 456/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1644 - val_loss: 1609.5426\n",
      "Epoch 457/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1644 - val_loss: 1609.5437\n",
      "Epoch 458/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1644 - val_loss: 1609.5437\n",
      "Epoch 459/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1644 - val_loss: 1609.5442\n",
      "Epoch 460/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1643 - val_loss: 1609.5442\n",
      "Epoch 461/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 209.1643 - val_loss: 1609.5447\n",
      "Epoch 462/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5447\n",
      "Epoch 463/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5450\n",
      "Epoch 464/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5455\n",
      "Epoch 465/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5455\n",
      "Epoch 466/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 209.1643 - val_loss: 1609.5455\n",
      "Epoch 467/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1643 - val_loss: 1609.5460\n",
      "Epoch 468/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5460\n",
      "Epoch 469/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5460\n",
      "Epoch 470/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1643 - val_loss: 1609.5460\n",
      "Epoch 471/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5458\n",
      "Epoch 472/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5458\n",
      "Epoch 473/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5458\n",
      "Epoch 474/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5458\n",
      "Epoch 475/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5458\n",
      "Epoch 476/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5458\n",
      "Epoch 477/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5458\n",
      "Epoch 478/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5458\n",
      "Epoch 479/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5458\n",
      "Epoch 480/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5454\n",
      "Epoch 481/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5450\n",
      "Epoch 482/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1643 - val_loss: 1609.5449\n",
      "Epoch 483/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1643 - val_loss: 1609.5442\n",
      "Epoch 484/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1643 - val_loss: 1609.5441\n",
      "Epoch 485/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 209.1643 - val_loss: 1609.5441\n",
      "Epoch 486/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 209.1644 - val_loss: 1609.5439\n",
      "Epoch 487/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1644 - val_loss: 1609.5437\n",
      "Epoch 488/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1644 - val_loss: 1609.5437\n",
      "Epoch 489/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1644 - val_loss: 1609.5437\n",
      "Epoch 490/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1644 - val_loss: 1609.5436\n",
      "Epoch 491/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1644 - val_loss: 1609.5432\n",
      "Epoch 492/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1644 - val_loss: 1609.5430\n",
      "Epoch 493/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1644 - val_loss: 1609.5428\n",
      "Epoch 494/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1644 - val_loss: 1609.5428\n",
      "Epoch 495/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1644 - val_loss: 1609.5430\n",
      "Epoch 496/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1645 - val_loss: 1609.5428\n",
      "Epoch 497/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 209.1645 - val_loss: 1609.5428\n",
      "Epoch 498/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1645 - val_loss: 1609.5430\n",
      "Epoch 499/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1645 - val_loss: 1609.5432\n",
      "Epoch 500/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 209.1645 - val_loss: 1609.5432\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.1552, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0026, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0004, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0102, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.8734, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 336ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.04906863e+01, 7.03142157e+01, 7.01377451e+01, 6.99612745e+01,\n",
       "        7.57085190e+01, 0.00000000e+00, 6.86436070e-02, 0.00000000e+00,\n",
       "        3.50399256e-01, 4.01462406e-01, 4.23700124e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.26712185e+01, 6.24443277e+01, 6.23797152e+01,\n",
       "        0.00000000e+00, 9.36131420e-01, 7.08724790e+01, 7.07968487e+01,\n",
       "        7.07212185e+01, 7.06455882e+01, 7.05299020e+01, 7.03534314e+01,\n",
       "        7.01769608e+01, 7.00004902e+01, 6.97263305e+01, 0.00000000e+00,\n",
       "        1.71475142e-01, 7.06007703e+01, 7.04253268e+01, 7.02488562e+01,\n",
       "        7.00723856e+01, 6.98906629e+01, 6.94873016e+01, 6.90839402e+01,\n",
       "        6.86805789e+01, 6.82928805e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.80321479e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.44669490e-02, 3.76224637e-01, 7.00397059e+01,\n",
       "        6.98159664e+01, 2.43531480e-01, 7.60643260e-02, 7.06175770e+01,\n",
       "        7.04645425e+01, 7.02880719e+01, 7.01116013e+01, 6.99351307e+01,\n",
       "        6.95769374e+01, 6.91735761e+01, 6.87702147e+01, 6.83668534e+01,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.00070261e+01, 6.97412698e+01,\n",
       "        6.93379085e+01, 6.89345472e+01, 6.85311858e+01, 6.39678105e+01,\n",
       "        6.34383987e+01, 6.27972689e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.95178750e-01, 3.83822441e+01, 2.08030969e-01, 0.00000000e+00,\n",
       "        2.87644804e-01, 0.00000000e+00, 5.69434285e-01, 3.52534056e-01,\n",
       "        6.37506294e+01, 9.36524644e-02, 7.92258084e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.99641728e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.73973656e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.51456261e-01, 4.70111668e-01, 0.00000000e+00,\n",
       "        7.63209403e-01, 0.00000000e+00, 2.36043736e-01, 3.71492296e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([53.64211929, 53.62654232, 53.61096536, 53.5953884 , 53.57981144,\n",
       "       53.56423447, 53.54865751, 53.53308055, 53.51750359, 53.50192662,\n",
       "       53.48634966, 53.4707727 , 53.45519574, 53.43961877, 53.42404181,\n",
       "       53.40846485, 53.39288789, 53.37731092, 53.36173396, 53.346157  ,\n",
       "       53.33058004, 53.31500307, 53.29942611, 53.28384915, 53.26827219,\n",
       "       53.25269522, 53.23711826, 53.2215413 , 53.20596434, 53.19038737,\n",
       "       53.17481041, 53.15923345, 53.14365649, 53.12807952, 53.11250256,\n",
       "       53.0969256 , 53.08134864, 53.06577167, 53.05019471, 53.03461775,\n",
       "       53.01904079, 53.00346382, 52.98788686, 52.9723099 , 52.95673294,\n",
       "       52.94115597, 52.92557901, 52.91000205, 52.89442509, 52.87884812,\n",
       "       52.86327116, 52.8476942 , 52.83211724, 52.81654027, 52.80096331,\n",
       "       52.78538635, 52.76980939, 52.75423242, 52.73865546, 52.7230785 ,\n",
       "       52.70750154, 52.69192457, 52.67634761, 52.66077065, 52.64519369,\n",
       "       52.62961672, 52.61403976, 52.5984628 , 52.58288584, 52.56730887,\n",
       "       52.55173191, 52.53615495, 52.52057799, 52.50500102, 52.48942406,\n",
       "       52.4738471 , 52.45827014, 52.44269317, 52.42711621, 52.41153925,\n",
       "       52.39596229, 52.38038532, 52.36480836, 52.3492314 , 52.33365444,\n",
       "       52.31807747, 52.30250051, 52.28692355, 52.27134659, 52.25576962,\n",
       "       52.24019266, 52.2246157 , 52.20903874, 52.19346177, 52.17788481,\n",
       "       52.16230785, 52.14673089, 52.13115392, 52.11557696, 52.1       ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.82346126175257\n",
      "35.316735550439674\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
