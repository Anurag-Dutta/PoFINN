{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "1695    66.890336\n",
       "1696    66.880999\n",
       "1697    66.871662\n",
       "1698    66.862325\n",
       "1699    66.852988\n",
       "Name: C2, Length: 1700, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c2_interpolated_1600_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "1595     0.000000\n",
       "1596     0.000000\n",
       "1597     0.936131\n",
       "1598     0.000000\n",
       "1599     0.227903\n",
       "Name: C2, Length: 1600, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1600)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.500000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.275910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.051821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.827731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.603641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     90.500000  0.000298\n",
       "1     90.275910  0.000298\n",
       "2     90.051821  0.000297\n",
       "3     89.827731  0.000297\n",
       "4     89.603641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAAsTAAALEwEAmpwYAAAm2UlEQVR4nO3dd3yW9b3/8dcnm5EFgTBDQEBAUIGwZChTHBWtWLUOXKVWcRzrwNPza8/x2FrtqbYetYqoWHFgra0UB0UqyJARhsgQCTOsMEISZiDk+/vjvoIxJxGSeyW538/H437kWneuj5fc9zvf7/ca5pxDREQiV1S4CxARkfBSEIiIRDgFgYhIhFMQiIhEOAWBiEiEiwl3ATWRlpbmMjMzw12GiEidsmzZsn3OuWYVl9fJIMjMzCQ7OzvcZYiI1ClmtrWy5eoaEhGJcAoCEZEIpyAQEYlwCgIRkQinIBARiXAKAhGRCKcgEBGJcBEVBH9bsZ2piyo9jVZEJGJFVBB89NVu3vhCQSAiUl5EBUGLpATyDh4LdxkiIrVKQILAzEab2XozyzGziZWsH2Jmy82sxMzGVlh30sxWeq/pgainKulJ8RQcOcGxEyeDuRsRkTrF73sNmVk08DwwEtgOLDWz6c65teU22wbcAjxYya846pw73986zkR6UgIAeUXHaNe0USh2KSJS6wWiRdAXyHHObXLOHQfeAcaU38A5t8U5twooDcD+aqxFclkQFIezDBGRWiUQQdAayC03v91bdqYSzCzbzBaZ2ZUBqKdKZS2C3UUaJxARKVMbbkPdzjm3w8w6AP8ys6+ccxsrbmRm44HxABkZGTXa0amuoUIFgYhImUC0CHYAbcvNt/GWnRHn3A7v5yZgDtCziu0mOeeynHNZzZr9n+cqnJGkhBgaxEaTpxaBiMgpgQiCpUAnM2tvZnHAdcAZnf1jZqlmFu9NpwEDgbXf/66aMzNaJCeoa0hEpBy/g8A5VwJMAGYC64B3nXNrzOwxM7sCwMz6mNl24BrgJTNb4729K5BtZl8CnwG/rXC2UcA1T4xXi0BEpJyAjBE45z4CPqqw7Jflppfi6zKq+L6FQI9A1HCmWiQnsGJbQSh3KSJSq0XUlcXgu7p4d9ExnHPhLkVEpFaIuCBonpTA8ZJSCo6cCHcpIiK1QsQFQYuyU0h1zyERESASgyA5HoDdupZARASIwCAof78hERGJwCBonpiAGWzLPxLuUkREaoWIC4K4mCj6Zjbhk9W7deaQiAgRGAQAY85vzca9h1mzsyjcpYiIhF1EBsGlPVoQG218sPKMb4kkIlJvRWQQpDSM48LOzZn+5U5Olqp7SEQiW0QGAcCVPVuRV1TM4s37w12KiEhYRWwQDO+STqO4aD5YsTPcpYiIhFXEBkGDuGgu7t6Cj1bvorhED7MXkcgVsUEAvrOHDh4r4bOv94a7FBGRsInoIBh4VlPSGsfp7CERiWgRHQQx0VFceX5rZq7Zzex1eeEuR0QkLCI6CAAeGNWZbq2SuOftFazeURjuckREQi7ig6BhXAyvjutDSoNYbpuylJ0FR8NdkohISEV8EIDvYTWv3tqHI8dPctuUpRw8pofWiEjkUBB4urRI4oUberFhzyEmvLWCkpOl4S5JRCQkFATlDOncjMev7M7cb/byy+lrdHdSEYkIMeEuoLa5vm8GW/cf4cW5G8ls2pDxQ84Kd0kiIkGlIKjEwxefTW7+EX7z0de0TW3IJT1ahrskEZGgCUjXkJmNNrP1ZpZjZhMrWT/EzJabWYmZja2wbpyZbfBe4wJRj7+ioozf/+g8emWkcP+0lSzfdiDcJYmIBI3fQWBm0cDzwCVAN+B6M+tWYbNtwC3AWxXe2wT4FdAP6Av8ysxS/a0pEBJio3n55izSkxK4bcpS1u8+GO6SRESCIhAtgr5AjnNuk3PuOPAOMKb8Bs65Lc65VUDFU3EuBmY55/KdcweAWcDoANQUEE0bxzP19n7ERUdx0yuL2bZfzzkWkfonEEHQGsgtN7/dWxbQ95rZeDPLNrPsvXtDd5O4jKYNeeP2fhSXlHLjK4vZU3QsZPsWEQmFOnP6qHNuknMuyzmX1axZs5Du++wWiUy5tQ/7DhVz0ytLKDhyPKT7FxEJpkAEwQ6gbbn5Nt6yYL83pHpmpPLyzVls3neYW15byuHiknCXJCISEIEIgqVAJzNrb2ZxwHXA9DN870xglJmleoPEo7xltdLAjmk8e31PVm0v4KdvLNMDbUSkXvA7CJxzJcAEfF/g64B3nXNrzOwxM7sCwMz6mNl24BrgJTNb4703H/hvfGGyFHjMW1Zrje7egievPpf5Oft4fMa6cJcjIuI3q4u3UcjKynLZ2dlhreE//v4V05bmMuehobROaRDWWkREzoSZLXPOZVVcXmcGi2ubn13UEedg0tyN4S5FRMQvCoIaap3SgKt7teHtpbk6pVRE6jQFgR/uGnoWJSdLeXnepnCXIiJSYwoCP7Rr2ogx57dm6qJt7D9UHO5yRERqREHgp7uHnsWxkpO8umBzuEsREakRBYGfOjZP5NLuLXl94VYKj+gRlyJS9ygIAmDCsI4cKi5hysIt4S5FRKTaFAQB0LVlEiO6pvPqgs168L2I1DkKggC5d3hHCo+eYOqibeEuRUSkWhQEAXJumxQu7NyMyfM2ceS4bkgnInWHgiCA7hnWkf2Hj/P2ktzTbywiUksoCAIoK7MJAzo05aW5G8nZc5CSkxUfyCYiUvvEhLuA+ua+EZ348cuLGPH058TFRNGxWWO6tEjkbO/VpUUS6UnxmFm4SxURAXT30aDI2XOIL3MLWJ93kK93H2T97iLyir698jilYSyd0xNPBUSXFol0Tk8kMSE2jFWLSH1X1d1H1SIIgo7NG9OxeePvLDtw+Djr8w6yfrcvHL7eXcRfl23n8PFvH27TrWUST409l+6tk0NdsohEMLUIwqi01LGj4OipVsPURdvIP3Kc/3d5N27sl6HuIxEJqKpaBAqCWmT/oWIeePdL5n6zl8t6tOSJq3uQpO4iEQkQPZimDmjaOJ7XbunDI6O78Mma3Vz+7Hy+2l4Y7rJEpJ5TENQyUVHGzy46i2nj+3PiZClX/2khUxZspi623ESkblAQ1FJZmU346N7BDO6Uxn/+Yy0/m7qcwqO6j5GIBJ6CoBZLbRTH5HFZ/OLSrny6Lo/Lnp3HytyCcJclIvWMgqCWMzN+MqQD7945AOfgmhcXMnneJnUViUjAKAjqiF4ZqXx072AuOrs5j3+4jp/8eRkFR46HuywRqQcCEgRmNtrM1ptZjplNrGR9vJlN89YvNrNMb3mmmR01s5Xe68VA1FNfJTeMZdJNvfnl5d2Y+80eLnt2Psu2Hgh3WSJSx/kdBGYWDTwPXAJ0A643s24VNrsdOOCc6wg8AzxZbt1G59z53utOf+up78yM2wa15707LyAqCq596QtemruR0lJ1FYlIzQSiRdAXyHHObXLOHQfeAcZU2GYM8Lo3/R4w3HTZrF/Oa5vCjHsGM7JbOk98/DV3/Dmb/MPqKhKR6gtEELQGyt+Af7u3rNJtnHMlQCHQ1FvX3sxWmNlcMxtc1U7MbLyZZZtZ9t69ewNQdt2X3CCWF27oxWNjzmH+hn1c9uw8lm7JD3dZIlLHhHuweBeQ4ZzrCTwAvGVmSZVt6Jyb5JzLcs5lNWvWLKRF1mZmxs0DMnn/rguIi4niukmLeGFOjrqKROSMBSIIdgBty8238ZZVuo2ZxQDJwH7nXLFzbj+Ac24ZsBHoHICaIk731snMuGcQo7u34KlP1nPjK4vZVXg03GWJSB0QiCBYCnQys/ZmFgdcB0yvsM10YJw3PRb4l3POmVkzb7AZM+sAdAI2BaCmiJSYEMtz1/fkyat7sDK3gIuf+ZwZq3aGuywRqeX8DgKvz38CMBNYB7zrnFtjZo+Z2RXeZq8ATc0sB18XUNkppkOAVWa2Et8g8p3OOXVy+8HMuLZPBh/dO5gOzRoz4a0V3P/OCl1zICJV0m2o67GSk6U891kOz/0rh5SGcTx+5TmM7t4y3GWJSJjoNtQRKCY6ivtHdGb6hEGkJ8Vz59Tl3P3mcvYeLD79m0UkYigIIkC3Vkn8/e6BPHTx2cxam8eoZ+bywcodul+RiAAKgogRGx3F3UM78tF9g8hMa8R976zkjtez2V14LNyliUiYKQgiTMfmibx35wX8v8u7sWDjPkY+PZdpS7epdSASwRQEESg6yrh9UHtm3j+Ec1on8chfv+KmV5aQm38k3KWJSBgoCCJYu6aNeOuO/jx+ZXffdQd/+JzXF27RVckiEUZBEOGioowb+7dj5r8NoU9mE341fQ3XTVrEpr2Hwl2aiISIgkAAaJ3SgCm39uF/rjmPr3cXcckf5/HS3I2UnCwNd2kiEmQKAjnFzBjbuw2fPnAhF3ZuxhMff83Vf1rI+t0Hw12aiASRgkD+j+ZJCbx0U2+e+3FPcg8c5fL/ncezszdwQq0DkXpJQSCVMjMuP7cVs/5tCJd0b8nTs77hiucWsHpHYbhLE5EAUxDI92raOJ5nr+/Jyzdnsf9QMWOeX8CTn3zNsRMnw12aiASIgkDOyMhu6cx64EKu7tWaP83ZyCV/nMeiTfvDXZaIBICCQM5YcoNYnhp7Hm/e0Y+TpY7rJi3i0fdXUXj0RLhLExE/KAik2gZ2TGPm/UP46ZAOTFuay8in5/LJ6t3hLktEakhBIDXSIC6aRy/tyvQJg0hrHM+dU5dx5xvLyCvSTexE6hoFgfile+tkPpgwkImXdOGz9XsY8fRc3l6im9iJ1CUKAvFbbHQUd154Fp/cP4RzWiXx6Ptfcf3Li9i873C4SxORM6AgkIBpn9aIt3/Snyev7sGanUVc/IfPeWFOji5EE6nlFAQSUGbGtX0ymP3AhQzv0pynPlnPFc8tYNX2gnCXJiJVUBBIUDRPSuBPN/bmxRt7s/9QMVc+v4Bff7iWI8dLwl2aiFSgIJCgGt29BbMeuJDr+mbw8rzNXPyHz5m/YV+4yxKRchQEEnTJDWL5zVU9eGd8f2KiorjxlcU8+JcvKThyPNyliQgBCgIzG21m680sx8wmVrI+3symeesXm1lmuXWPesvXm9nFgahHaqf+HZry8X2Dueuis/jbih2MeHou72bnUnhEVyaLhJP5e763mUUD3wAjge3AUuB659zactvcBZzrnLvTzK4DrnLOXWtm3YC3gb5AK+BToLNz7nvvaJaVleWys7P9qlvCa+3OIia+v4pV2wsxg64tkujfoSn9OzShb/smpDSMC3eJIvWOmS1zzmVVXB4TgN/dF8hxzm3ydvQOMAZYW26bMcB/etPvAc+ZmXnL33HOFQObzSzH+31fBKAuqcW6tUri73cNZPm2A3yxcT+LNu/nzcVbeXXBZgWDSIgFIghaA7nl5rcD/araxjlXYmaFQFNv+aIK721d2U7MbDwwHiAjIyMAZUu4RUUZWZlNyMpswj10orjkJKu2F7KokmDo0iKJ/h2a0L9DU/opGEQCKhBBEBLOuUnAJPB1DYW5HAmC+Jho+mQ2oU8VwfD2km28tmCLgkEkwAIRBDuAtuXm23jLKttmu5nFAMnA/jN8r0So7wuGxZvzqwyGgR3TaBxfZ/7GETkt5xy+3vTgCMRgcQy+weLh+L7ElwI/ds6tKbfN3UCPcoPFP3TO/cjMzgHe4tvB4tlAJw0Wy5k4XlLKqu0FLNq0n0Wb8snems+xE6U0aRTH/SM6cX3fDGKjdYa01H2ZEz9kcKc03ri9Yq979QRtsNjr858AzASigVedc2vM7DEg2zk3HXgFeMMbDM4HrvPeu8bM3sU3sFwC3H26EBApExcTdWqMYcIwXzBkb83n2dkb+OUHa5iyYAuPXNKFUd3Sg/rXlEgozAvihZh+twjCQS0C+T7OOf719R5+89E6Nu49TJ/MVP790q70zEgNd2ki1eKc486py5i5Jg+ALb+9zK/fV1WLQO1mqXfMjOFd05l5/xB+fVV3Nu87zFUvLGTCW8vJzT8S7vJEztjB4pJTIRBMCgKpt2Kio7ihXzvmPDSUe4d15NN1eQz//Vwen7FWt7eQOiFUV90rCKTeaxwfwwOjzmbOg0O5smcrXlmwmQt/N4fJ8zZRXKIhKam9Co8qCEQCqkVyAk+NPY+P7h3MeW1TePzDdYx4ei4zVu3UozWlVmqRnBCS/SgIJOJ0bZnEn2/ry59v60ujuBgmvLWCq15YyNIt+eEuTeQ70hrHf2e+JEhP+1MQSMQa0rkZH947mKfGnsuuwqNc8+IX/PSNbDbtPRTu0kROSYj99ms6KkinQSsIJKJFRxk/ymrLZw9exM9Hdmb+hn2MeuZzfvXBavYfKg53eSIkN4g9NR0VpSAQCZqGcTHcM7wTcx4ayrV92jJ18TYu+t0cXpiTw7ETGlCW8ImJCv7XtIJApJxmifH8+qoezLx/MP06NOGpT9Yz6MnP+P0/17Oz4Gi4y5MIFBMd/KvidWcukUp0bJ7I5HF9+GLjfibP28Rzn+Xw/Gc5jOiazk0D2jHwrLSgNdNFyosOwb8zBYHI9xhwVlMGnNWU3PwjvLVkG9OW5vLPtXl0SGvEDf3bMbZXG5Ibxp7+F4nUUKy6hkRqh7ZNGvLI6C4snDiMZ649j5SGsfz3jLX0e+JTHnlvFat3FIa7RKmnyp81FCxqEYhUQ0JsNFf1bMNVPduwekchby7eyt9X7GRadi7nt03hpv7tuOzcliTERoe7VKknGsQF/9+SWgQiNdS9dTJP/PBcFv37cH71g24UHTvBz//yJQOemM0TH6/TDe4kIBrGBf/vdbUIRPyU3CCWWwe255YLMlm4cT9vfLGVyfM2M+nzTVzUuRk3DWjHhZ2bh2TQT+qfULQIFAQiAWJmDOyYxsCOaewqPMrbS3J5e8k2bpuSTdsmDbihXzt+lNWWJo30fGU5c80T40+/kZ/0YBqRIDpxspR/rsnjjUVbWLQpn7iYKC7v0ZIbB7SjZ9sUPTlNzkjmxA+B4D2YRi0CkSCKjY7isnNbctm5Lfkm7yBTF23l/eU7eH/FDs5plcTNA9pxxXmtQ9L8F6mKBotFQqRzeiKPjenOon8fzuNXdqfkpOORv35Fv998ymP/WKub3UnYqEUgEmKN42O4sX87buiXQfbWA7zxxVbeWLSFVxdsZlDHNG65IJNhXZrrymUJGQWBSJiYGX0ym9Answl7Dnbl3aW5vLl4G3f8OZv2aY24dWAmY3u3Ccnpg1K7DejQlDU7g3fRogaLRWqREydL+WT1bl6Zv5mVuQUkJcRwfb8Mxg3IpFVKg3CXJ3VcVYPFCgKRWmrZ1gO8On8zH6/ehZlxaY+W3D6oPee3TQl3aVJHBeWsITNrAkwDMoEtwI+ccwcq2W4c8B/e7OPOude95XOAlkDZ/X1HOef2+FOTSH3Ru10qvdulkpt/hNcXbmHa0lz+8eVOstqlcvug9ow6p4UuUpOA8KtFYGZPAfnOud+a2UQg1Tn3SIVtmgDZQBbggGVAb+fcAS8IHnTOVevPe7UIJBIdKi7h3aW5vLZwM7n5R2mT2oBbLsjk2j5tSUzQHVDl9KpqEfh7+ugY4HVv+nXgykq2uRiY5ZzL91oLs4DRfu5XJOI0jo/htkHtmfPgUF68sTetkhvw+IfrGPDEv3jsH2vZfkD3NpKa8fd0hHTn3C5vejeQXsk2rYHccvPbvWVlXjOzk8Bf8XUbVdpEMbPxwHiAjIwMP8sWqbuio4zR3VswunsLvtpeyCvzN/HnL7YwddFWxl3QjruHdiSloW5jIWfutC0CM/vUzFZX8hpTfjvvC7y6/Uw3OOd6AIO9101Vbeicm+Scy3LOZTVr1qyauxGpn3q0SeYP1/Xk84eHMub8Vkyev5kLfzeHlz/fpGctyxk7bRA450Y457pX8voAyDOzlgDez8oGencAbcvNt/GW4Zwr+3kQeAvo699/jkhkapXSgN9dcx4f3zeYnhkp/PqjdQz//Vz+vmIHpaV178xACS1/xwimA+O86XHAB5VsMxMYZWapZpYKjAJmmlmMmaUBmFkscDmw2s96RCJalxZJTLm1L2/e0Y/URrHcP20lP3huPgty9oW7NKnF/A2C3wIjzWwDMMKbx8yyzGwygHMuH/hvYKn3esxbFo8vEFYBK/G1El72sx4RAQZ2TGP63YP443XnU3DkBDdMXsy4V5ewbldRuEuTWkgXlInUc8dOnGTqoq38779yKDp2gqt7teHnozrTMllXKkcaXVksEuEKj5zg+Tk5TFmwBTO4bVB7fnbRWSTpGoSIoSAQEQBy84/w9Kxv+NuKHaQ2jOXe4Z24oV874mJ0V/r6LlgXlIlIHdO2SUOeufZ8ZtwziG6tkvivf6xlxNNzmbFqJ3XxD0Pxn4JAJEJ1b53M1Nv7MeXWPjSMi2bCWyu48oWFLN60P9ylSYipa0hEOFnqeH/5dn7/z2/YXXSM3u1Sade0IS2SEmiRnEDzxASaJ8WTnpRAs8bx6kaqo/TMYhGpUnSUcU1WWy4/txWvLdzMrLV5LNq4nz0Hiymp5IK0Jo3iaJ4YT/OkBNIT40+FRNmy5onxNEuMJz5Gz2KuC9QiEJEqlZY69h0uZk9RMXsOHmNPUTF53nReUTF7y34eKuZkJYGR2jDW14pI/DYoKgZG8yQFRqioRSAi1RYVZb5uocQEILnK7U6WOvIPHyev6Bh7DxaTV3SMPeV+7ik6Rs6eQ+w5WHlgpDSMJd3rfmqemEB6Uvy3oeG1NtKTEoiNVpdUMCgIRMRv0VFGM6876PuUljryjxz/TkDsKSomr6y1cbCYnD372FtJl5QZNGscT8uUBrRMSqBlSgLntEqmd7tUMps2xEwP6akpBYGIhExUlJHWOJ60xvGc8z3blQVGWUjkFR5jZ+ExdhceZVfhMXL2HuLzDXs5cnwL4Buz6JWReuqpbue2SSYhVt1NZ0pBICK1TvnA6EZSpduUljpy9h5i2dYDLNt6gOVbD/DpujwAYqKMc1on09sLh17tUnRLje+hwWIRqTfyDx9n+dYDLNvmC4cvcwsoLikFoFVyAr3afdtq6NoyKeLGHDRYLCL1XpNGcYzols6Ibr6HJR4vKWXdriJfq8ELhxmrfA9VTIiN4rw2KaeCoVdGKqmNIvPJbmoRiEhE2VlwlOXbvu1OWrOz6NTA9Mhu6dw3vBPdW1d9hlRdphaBiAi+p7m1SmnA5ee2AuDo8ZOs2l7A5xv28sYXW7l8bR4juqZz/4j6GwgVqUUgIuIpOnaCKQu2MHneJoqOlTCia3PuG96ZHm3CEwgjnp7LD3u15q6LOgbk9+nuoyIip5GU4Lst9/yJw3hgZGeWbM7nB8/N5/YpS1m1vSDk9eTmH6Hw6Img70dBICJSQflA+PnIzmRvPcAVzy3gtilL+TK3IGR1OAdG8C+UUxCIiFQhKSGWe4Z3Yv4jQ3lwVGeWbzvAmOdDFwgORygumFYQiIicRmJCLBOGdWLew0N56OKzTwXCra8tYWUQA8HXIgg+BYGIyBlKTIjl7qEdmf/IMB66+GxW5hZw5fMLuOW1JazYdiDg+3OgFoGISG3UOD6Gu4d2ZN4jw3h49Nl8mVvAVS8sZNyrS1gewEBwzmmMQESkNmscH8NdF/laCI+M7sJXOwr54QsLufnVJSzbmu/3M6DrRIvAzJqY2Swz2+D9TK1iu0/MrMDMZlRY3t7MFptZjplNM7PIvL5bROq0RvEx/Oyis5j38FAmXtKF1TsKufpPX9Dn159y+5SlHPfud1RddWWMYCIw2znXCZjtzVfmd8BNlSx/EnjGOdcROADc7mc9IiJh0yg+hjsv9AXCb67qQafmicz+eg95Rceq/bvKWhOheM6Cv0EwBnjdm34duLKyjZxzs4GD5ZeZ779uGPDe6d4vIlKXNIqP4cf9Mhjbu02Nf0dZr1Kt7xoC0p1zu7zp3UB6Nd7bFChwzpV489uB1lVtbGbjzSzbzLL37t1bs2pFREIoyvuGLa3BWEHZO0IxWHzam86Z2adAi0pW/aL8jHPOmVnQblzknJsETALfvYaCtR8RkUCJ8v6cr+Qxzaf1bddQICuq3GmDwDk3oqp1ZpZnZi2dc7vMrCWwpxr73g+kmFmM1ypoA+yoxvtFRGo1OxUE/rQIgs/frqHpwDhvehzwwZm+0fni7jNgbE3eLyJS20V53+I1OY20Lo0R/BYYaWYbgBHePGaWZWaTyzYys3nAX4DhZrbdzC72Vj0CPGBmOfjGDF7xsx4RkVrDr64hQnfWkF8PpnHO7QeGV7I8G7ij3PzgKt6/CejrTw0iIrVV2Vd4jbqGQjgSqiuLRUSC5NQYQQ2uJ6tLXUMiIlKFsjGCmg0WO+931P4LykREpAplX+I16eY51SIIYD1VURCIiARJQC4oU9eQiEjd5dd1BGUXlOk21CIiddeprqEavFctAhGResCvC8pqdufqGlEQiIgESVm3Tm2/oExBICISJKdOH61BEuisIRGResD8usWET5TGCERE6i7/bjqnriERkTovKsr/FoHOGhIRqcP8usWExghEROq+QFxQFoomgYJARCRI/LrXkPdTLQIRkTosIF1DGiMQEam7AnJBme41JCJSdzVO8D0E8oU5OewoOFqt95a1CHQdgYhIHdY+rRH/+YNufL3rICN+P5eX5m7kxMkzu4mQTh8VEaknbhnYnlkPDGFgxzSe+PhrLn92Pku35J/2fboNtYhIPdImtSGTx2Xx8s1ZHCou4ZoXv+Dh974k//DxKt/jQnjakIJARCRERnZLZ9YDQ/jphR14f/kOhv1+Du8uza30FhR15oIyM2tiZrPMbIP3M7WK7T4xswIzm1Fh+RQz22xmK73X+f7UIyJS2zWMi+HRS7ry4b2D6dw8kYf/uoo7py6j8MiJ72xXl25DPRGY7ZzrBMz25ivzO+CmKtY95Jw733ut9LMeEZE64ewWibwzvj+/uLQrs9ft4bL/ncfK3IJT6+tMiwAYA7zuTb8OXFnZRs652cBBP/clIlKvREUZPxnSgb/cOQDnYOyfFjJ53iacc3XqrKF059wub3o3kF6D3/FrM1tlZs+YWbyf9YiI1Dk9M1L56N7BDO/anMc/XMcdr2dz4IhvIDkUQRBzug3M7FOgRSWrflF+xjnnzKy61889ii9A4oBJwCPAY1XUMR4YD5CRkVHN3YiI1G7JDWN58cbe/PmLrfz6w3XMy9kHhOb00dMGgXNuRFXrzCzPzFo653aZWUtgT3V2Xq41UWxmrwEPfs+2k/CFBVlZWTW4YFtEpHYzM8ZdkEnvdqn8JTuXI8dPMuCspkHf72mD4DSmA+OA33o/P6jOm8uFiOEbX1jtZz0iInVe99bJdG+dHLL9+TtG8FtgpJltAEZ485hZlplNLtvIzOYBfwGGm9l2M7vYW/WmmX0FfAWkAY/7WY+IiFSTXy0C59x+YHgly7OBO8rND67i/cP82b+IiPhPVxaLiEQ4BYGISIRTEIiIRDgFgYhIhFMQiIhEOAWBiEiEs8rug13bmdleYGsN354G7AtgOYGiuqpHdVWP6qq+2lqbP3W1c841q7iwTgaBP8ws2zmXFe46KlJd1aO6qkd1VV9trS0YdalrSEQkwikIREQiXCQGwaRwF1AF1VU9qqt6VFf11dbaAl5XxI0RiIjId0Vii0BERMpREIiIRLiICgIzG21m680sx8wmhnC/bc3sMzNba2ZrzOw+b3kTM5tlZhu8n6necjOzZ706V5lZryDXF21mK8xshjff3swWe/ufZmZx3vJ4bz7HW58Z5LpSzOw9M/vazNaZ2YDacMzM7N+8/4+rzextM0sIxzEzs1fNbI+ZrS63rNrHx8zGedtvMLNxQarrd97/x1Vm9jczSym37lGvrvXlnlUS8M9rZXWVW/dzM3NmlubNh/V4ecvv8Y7ZGjN7qtzywB8v51xEvIBoYCPQAd8zkr8EuoVo3y2BXt50IvAN0A14CpjoLZ8IPOlNXwp8DBjQH1gc5PoeAN4CZnjz7wLXedMvAj/zpu8CXvSmrwOmBbmu14E7vOk4ICXcxwxoDWwGGpQ7VreE45gBQ4BewOpyy6p1fIAmwCbvZ6o3nRqEukYBMd70k+Xq6uZ9FuOB9t5nNDoYn9fK6vKWtwVm4rtINa2WHK+hwKdAvDffPJjHK2gf4tr2AgYAM8vNPwo8GqZaPgBGAuuBlt6ylsB6b/ol4Ppy25/aLgi1tAFmA8OAGd4//H3lPrSnjpv3YRngTcd421mQ6krG94VrFZaH9ZjhC4Jc74sgxjtmF4frmAGZFb5AqnV8gOuBl8ot/852gaqrwrqrgDe96e98DsuOV7A+r5XVBbwHnAds4dsgCOvxwveHxYhKtgvK8YqkrqGyD3CZ7d6ykPK6BnoCi4F059wub9VuIN2bDmWtfwAeBkq9+aZAgXOupJJ9n6rLW1/obR8M7YG9wGtet9VkM2tEmI+Zc24H8D/ANmAXvmOwjNpxzKD6xyccn4vb8P21Hfa6zGwMsMM592WFVeE+Xp2BwV534lwz6xPMuiIpCMLOzBoDfwXud84VlV/nfDEe0nN5zexyYI9zblko93uGYvA1l//knOsJHMbX1XFKmI5ZKjAGX1C1AhoBo0NZw5kKx/E5HTP7BVACvFkLamkI/Dvwy3DXUokYfK3O/sBDwLtmZsHaWSQFwQ58fYFl2njLQsLMYvGFwJvOufe9xXlm1tJb3xLYE+JaBwJXmNkW4B183UN/BFLMrOx51uX3faoub30ysD8IdYHvL5rtzrnF3vx7+IIh3MdsBLDZObfXOXcCeB/fcawNxwyqf3xC9rkws1uAy4EbvJAKd11n4Qv0L73PQBtguZm1CHNd4Pv3/77zWYKvxZ4WrLoiKQiWAp28szvi8A3cTQ/Fjr0kfwVY55x7utyq6UDZWQfj8I0dlC2/2TtzoT9QWK65HzDOuUedc22cc5n4jse/nHM3AJ8BY6uoq6zesd72QfmL0zm3G8g1s7O9RcOBtYT5mOHrEupvZg29/69ldYX9mFWyvzM5PjOBUWaW6rV2RnnLAsrMRuPrgrzCOXekQr3Xme/sqvZAJ2AJIfi8Oue+cs41d85lep+B7fhO6thNmI8X8Hd8A8aYWWd8A8D7CNbx8neQoy698J0J8A2+0fVfhHC/g/A10VcBK73Xpfj6imcDG/CdIdDE296A5706vwKyQlDjRXx71lAH7x9XDvAXvj1zIcGbz/HWdwhyTecD2d5x+zu+szTCfsyA/wK+BlYDb+A7gyPkxwx4G984xQl8X2K31+T44Ouzz/Fetwaprhx8fdhl//5fLLf9L7y61gOXlFse0M9rZXVVWL+FbweLw3284oCp3r+x5cCwYB4v3WJCRCTCRVLXkIiIVEJBICIS4RQEIiIRTkEgIhLhFAQiIhFOQSAiEuEUBCIiEe7/A1kxbkTgeFaeAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.1552  # Value for C0\n",
    "K0 = -0.0026  # Value for K0\n",
    "K1 = -0.0004  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0102    # Value for b\n",
    "c = 2.8734    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    90.500000    90.275910    90.051821    89.827731    89.603641   \n",
      "351    90.275910    90.051821    89.827731    89.603641    89.379552   \n",
      "352    90.051821    89.827731    89.603641    89.379552    89.155462   \n",
      "353    89.827731    89.603641    89.379552    89.155462    88.931373   \n",
      "354    89.603641    89.379552    89.155462    88.931373    88.707283   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    89.379552    89.155462    88.931373    88.707283    88.494958  ...   \n",
      "351    89.155462    88.931373    88.707283    88.494958    88.427731  ...   \n",
      "352    88.931373    88.707283    88.494958    88.427731    88.360504  ...   \n",
      "353    88.707283    88.494958    88.427731    88.360504    88.293277  ...   \n",
      "354    88.494958    88.427731    88.360504    88.293277    88.226050  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   79.071008    0.000263   79.054202    0.000263   79.037395    0.000263   \n",
      "351   79.054202    0.000263   79.037395    0.000263   79.020588    0.000262   \n",
      "352   79.037395    0.000263   79.020588    0.000262   79.003782    0.000262   \n",
      "353   79.020588    0.000262   79.003782    0.000262   78.986975    0.000262   \n",
      "354   79.003782    0.000262   78.986975    0.000262   78.970168    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   79.020588    0.000262   79.003782    0.000262  \n",
      "351   79.003782    0.000262   78.986975    0.000262  \n",
      "352   78.986975    0.000262   78.970168    0.000262  \n",
      "353   78.970168    0.000262   78.953361    0.000262  \n",
      "354   78.953361    0.000262   78.936555    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1150, 1, 251) (1150, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "15/15 [==============================] - 2s 35ms/step - loss: 5628.4634 - val_loss: 4787.0000\n",
      "Epoch 2/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5560.8379 - val_loss: 4728.9053\n",
      "Epoch 3/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5499.3252 - val_loss: 4676.4946\n",
      "Epoch 4/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5442.8379 - val_loss: 4624.4331\n",
      "Epoch 5/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5386.8301 - val_loss: 4572.8955\n",
      "Epoch 6/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5331.3579 - val_loss: 4521.8618\n",
      "Epoch 7/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5265.5381 - val_loss: 4454.4404\n",
      "Epoch 8/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5202.2402 - val_loss: 4401.3594\n",
      "Epoch 9/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5145.1426 - val_loss: 4349.0723\n",
      "Epoch 10/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5088.8794 - val_loss: 4297.5415\n",
      "Epoch 11/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5033.3604 - val_loss: 4246.6641\n",
      "Epoch 12/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4978.4893 - val_loss: 4196.3706\n",
      "Epoch 13/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4924.2026 - val_loss: 4146.6157\n",
      "Epoch 14/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4870.4595 - val_loss: 4097.3672\n",
      "Epoch 15/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4817.2295 - val_loss: 4048.6047\n",
      "Epoch 16/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4764.4941 - val_loss: 4000.3123\n",
      "Epoch 17/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4712.2388 - val_loss: 3952.4773\n",
      "Epoch 18/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4660.4492 - val_loss: 3905.0891\n",
      "Epoch 19/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4609.1187 - val_loss: 3858.1409\n",
      "Epoch 20/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4558.2363 - val_loss: 3811.6233\n",
      "Epoch 21/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4507.7964 - val_loss: 3765.5320\n",
      "Epoch 22/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4457.7930 - val_loss: 3719.8616\n",
      "Epoch 23/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4408.2192 - val_loss: 3674.6057\n",
      "Epoch 24/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4359.0708 - val_loss: 3629.7603\n",
      "Epoch 25/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4310.3442 - val_loss: 3585.3218\n",
      "Epoch 26/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4262.0332 - val_loss: 3541.2854\n",
      "Epoch 27/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4214.1353 - val_loss: 3497.6475\n",
      "Epoch 28/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4166.6455 - val_loss: 3454.4060\n",
      "Epoch 29/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4119.5615 - val_loss: 3411.5552\n",
      "Epoch 30/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4072.8799 - val_loss: 3369.0940\n",
      "Epoch 31/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4026.5967 - val_loss: 3327.0178\n",
      "Epoch 32/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3980.7095 - val_loss: 3285.3240\n",
      "Epoch 33/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3935.2146 - val_loss: 3244.0103\n",
      "Epoch 34/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3890.1089 - val_loss: 3203.0737\n",
      "Epoch 35/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3845.3904 - val_loss: 3162.5115\n",
      "Epoch 36/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3801.0559 - val_loss: 3122.3198\n",
      "Epoch 37/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3757.1033 - val_loss: 3082.4968\n",
      "Epoch 38/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3713.5286 - val_loss: 3043.0403\n",
      "Epoch 39/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3670.3301 - val_loss: 3003.9478\n",
      "Epoch 40/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3627.5049 - val_loss: 2965.2158\n",
      "Epoch 41/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3585.0500 - val_loss: 2926.8428\n",
      "Epoch 42/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3542.9646 - val_loss: 2888.8262\n",
      "Epoch 43/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3501.2449 - val_loss: 2851.1631\n",
      "Epoch 44/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3459.8887 - val_loss: 2813.8513\n",
      "Epoch 45/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3418.8936 - val_loss: 2776.8892\n",
      "Epoch 46/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3378.2576 - val_loss: 2740.2734\n",
      "Epoch 47/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3337.9775 - val_loss: 2704.0027\n",
      "Epoch 48/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3298.0532 - val_loss: 2668.0745\n",
      "Epoch 49/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3258.4797 - val_loss: 2632.4858\n",
      "Epoch 50/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3219.2559 - val_loss: 2597.2358\n",
      "Epoch 51/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3180.3806 - val_loss: 2562.3206\n",
      "Epoch 52/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3141.8503 - val_loss: 2527.7395\n",
      "Epoch 53/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3103.6628 - val_loss: 2493.4897\n",
      "Epoch 54/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3065.8159 - val_loss: 2459.5684\n",
      "Epoch 55/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3028.3086 - val_loss: 2425.9756\n",
      "Epoch 56/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2991.1379 - val_loss: 2392.7065\n",
      "Epoch 57/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 2954.3015 - val_loss: 2359.7610\n",
      "Epoch 58/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2917.7981 - val_loss: 2327.1362\n",
      "Epoch 59/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2881.6245 - val_loss: 2294.8303\n",
      "Epoch 60/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2845.7798 - val_loss: 2262.8413\n",
      "Epoch 61/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 2810.2615 - val_loss: 2231.1665\n",
      "Epoch 62/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2775.0674 - val_loss: 2199.8054\n",
      "Epoch 63/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2740.1960 - val_loss: 2168.7546\n",
      "Epoch 64/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2705.6443 - val_loss: 2138.0129\n",
      "Epoch 65/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2671.4116 - val_loss: 2107.5779\n",
      "Epoch 66/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2637.4954 - val_loss: 2077.4473\n",
      "Epoch 67/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2603.8933 - val_loss: 2047.6204\n",
      "Epoch 68/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2570.6038 - val_loss: 2018.0945\n",
      "Epoch 69/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2537.6248 - val_loss: 1988.8671\n",
      "Epoch 70/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 2504.9543 - val_loss: 1959.9374\n",
      "Epoch 71/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2472.5913 - val_loss: 1931.3027\n",
      "Epoch 72/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2440.5327 - val_loss: 1902.9619\n",
      "Epoch 73/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2408.7771 - val_loss: 1874.9120\n",
      "Epoch 74/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2377.3223 - val_loss: 1847.1525\n",
      "Epoch 75/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 2346.1675 - val_loss: 1819.6803\n",
      "Epoch 76/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2315.3093 - val_loss: 1792.4943\n",
      "Epoch 77/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2284.7471 - val_loss: 1765.5928\n",
      "Epoch 78/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2254.4785 - val_loss: 1738.9734\n",
      "Epoch 79/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2224.5020 - val_loss: 1712.6348\n",
      "Epoch 80/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2194.8154 - val_loss: 1686.5742\n",
      "Epoch 81/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2165.4172 - val_loss: 1660.7913\n",
      "Epoch 82/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2136.3054 - val_loss: 1635.2828\n",
      "Epoch 83/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 2107.4785 - val_loss: 1610.0477\n",
      "Epoch 84/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2078.9343 - val_loss: 1585.0848\n",
      "Epoch 85/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2050.6714 - val_loss: 1560.3915\n",
      "Epoch 86/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 2022.6877 - val_loss: 1535.9661\n",
      "Epoch 87/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1994.9819 - val_loss: 1511.8077\n",
      "Epoch 88/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1967.5518 - val_loss: 1487.9124\n",
      "Epoch 89/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1940.3959 - val_loss: 1464.2814\n",
      "Epoch 90/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1913.5134 - val_loss: 1440.9111\n",
      "Epoch 91/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1886.9011 - val_loss: 1417.8003\n",
      "Epoch 92/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1860.5574 - val_loss: 1394.9471\n",
      "Epoch 93/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1834.4814 - val_loss: 1372.3497\n",
      "Epoch 94/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1808.6710 - val_loss: 1350.0071\n",
      "Epoch 95/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1783.1249 - val_loss: 1327.9171\n",
      "Epoch 96/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1757.8411 - val_loss: 1306.0776\n",
      "Epoch 97/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 1732.8174 - val_loss: 1284.4879\n",
      "Epoch 98/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1708.0536 - val_loss: 1263.1458\n",
      "Epoch 99/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1683.5465 - val_loss: 1242.0494\n",
      "Epoch 100/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1659.2954 - val_loss: 1221.1973\n",
      "Epoch 101/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1635.2981 - val_loss: 1200.5881\n",
      "Epoch 102/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1611.5532 - val_loss: 1180.2203\n",
      "Epoch 103/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1588.0593 - val_loss: 1160.0918\n",
      "Epoch 104/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1564.8147 - val_loss: 1140.2007\n",
      "Epoch 105/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1541.8171 - val_loss: 1120.5457\n",
      "Epoch 106/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1519.0658 - val_loss: 1101.1254\n",
      "Epoch 107/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1496.5590 - val_loss: 1081.9381\n",
      "Epoch 108/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1474.2949 - val_loss: 1062.9825\n",
      "Epoch 109/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1452.2719 - val_loss: 1044.2563\n",
      "Epoch 110/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1430.4886 - val_loss: 1025.7581\n",
      "Epoch 111/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1408.9434 - val_loss: 1007.4868\n",
      "Epoch 112/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1387.6342 - val_loss: 989.4401\n",
      "Epoch 113/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1366.5599 - val_loss: 971.6170\n",
      "Epoch 114/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1345.7196 - val_loss: 954.0164\n",
      "Epoch 115/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1325.1108 - val_loss: 936.6354\n",
      "Epoch 116/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1304.7324 - val_loss: 919.4732\n",
      "Epoch 117/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1284.5824 - val_loss: 902.5283\n",
      "Epoch 118/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1264.6595 - val_loss: 885.7992\n",
      "Epoch 119/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1244.9625 - val_loss: 869.2838\n",
      "Epoch 120/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1225.4894 - val_loss: 852.9812\n",
      "Epoch 121/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1206.2390 - val_loss: 836.8893\n",
      "Epoch 122/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1187.2095 - val_loss: 821.0067\n",
      "Epoch 123/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1168.3995 - val_loss: 805.3329\n",
      "Epoch 124/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1149.8076 - val_loss: 789.8649\n",
      "Epoch 125/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 1131.4325 - val_loss: 774.6017\n",
      "Epoch 126/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1113.2721 - val_loss: 759.5421\n",
      "Epoch 127/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1095.3250 - val_loss: 744.6839\n",
      "Epoch 128/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1077.5902 - val_loss: 730.0269\n",
      "Epoch 129/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1060.0658 - val_loss: 715.5674\n",
      "Epoch 130/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1042.7501 - val_loss: 701.3061\n",
      "Epoch 131/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1025.6423 - val_loss: 687.2407\n",
      "Epoch 132/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1008.7408 - val_loss: 673.3694\n",
      "Epoch 133/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 992.0438 - val_loss: 659.6907\n",
      "Epoch 134/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 975.5497 - val_loss: 646.2038\n",
      "Epoch 135/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 959.2573 - val_loss: 632.9066\n",
      "Epoch 136/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 943.1653 - val_loss: 619.7974\n",
      "Epoch 137/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 927.2721 - val_loss: 606.8757\n",
      "Epoch 138/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 911.5759 - val_loss: 594.1390\n",
      "Epoch 139/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 896.0758 - val_loss: 581.5861\n",
      "Epoch 140/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 880.7701 - val_loss: 569.2162\n",
      "Epoch 141/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 865.6577 - val_loss: 557.0272\n",
      "Epoch 142/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 850.7366 - val_loss: 545.0181\n",
      "Epoch 143/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 836.0055 - val_loss: 533.1868\n",
      "Epoch 144/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 821.4635 - val_loss: 521.5324\n",
      "Epoch 145/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 807.1084 - val_loss: 510.0525\n",
      "Epoch 146/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 792.9392 - val_loss: 498.7470\n",
      "Epoch 147/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 778.9546 - val_loss: 487.6142\n",
      "Epoch 148/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 765.1528 - val_loss: 476.6515\n",
      "Epoch 149/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 751.5323 - val_loss: 465.8581\n",
      "Epoch 150/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 738.0921 - val_loss: 455.2331\n",
      "Epoch 151/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 724.8309 - val_loss: 444.7746\n",
      "Epoch 152/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 711.7469 - val_loss: 434.4811\n",
      "Epoch 153/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 698.8387 - val_loss: 424.3510\n",
      "Epoch 154/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 686.1050 - val_loss: 414.3835\n",
      "Epoch 155/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 673.5446 - val_loss: 404.5768\n",
      "Epoch 156/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 661.1561 - val_loss: 394.9297\n",
      "Epoch 157/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 648.9377 - val_loss: 385.4406\n",
      "Epoch 158/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 636.8883 - val_loss: 376.1079\n",
      "Epoch 159/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 625.0066 - val_loss: 366.9301\n",
      "Epoch 160/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 613.2911 - val_loss: 357.9063\n",
      "Epoch 161/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 601.7404 - val_loss: 349.0351\n",
      "Epoch 162/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 590.3530 - val_loss: 340.3144\n",
      "Epoch 163/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 579.1279 - val_loss: 331.7432\n",
      "Epoch 164/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 568.0633 - val_loss: 323.3203\n",
      "Epoch 165/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 557.1582 - val_loss: 315.0447\n",
      "Epoch 166/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 546.4112 - val_loss: 306.9137\n",
      "Epoch 167/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 535.8206 - val_loss: 298.9269\n",
      "Epoch 168/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 525.3853 - val_loss: 291.0826\n",
      "Epoch 169/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 515.1037 - val_loss: 283.3792\n",
      "Epoch 170/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 504.9745 - val_loss: 275.8157\n",
      "Epoch 171/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 494.9966 - val_loss: 268.3906\n",
      "Epoch 172/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 485.1685 - val_loss: 261.1022\n",
      "Epoch 173/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 475.4888 - val_loss: 253.9498\n",
      "Epoch 174/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 465.9562 - val_loss: 246.9313\n",
      "Epoch 175/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 456.5691 - val_loss: 240.0458\n",
      "Epoch 176/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 447.3266 - val_loss: 233.2911\n",
      "Epoch 177/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 438.2273 - val_loss: 226.6672\n",
      "Epoch 178/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 429.2696 - val_loss: 220.1720\n",
      "Epoch 179/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 420.4522 - val_loss: 213.8037\n",
      "Epoch 180/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 411.7740 - val_loss: 207.5618\n",
      "Epoch 181/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 403.2336 - val_loss: 201.4443\n",
      "Epoch 182/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 394.8295 - val_loss: 195.4502\n",
      "Epoch 183/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 386.5606 - val_loss: 189.5779\n",
      "Epoch 184/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 378.4256 - val_loss: 183.8263\n",
      "Epoch 185/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 370.4230 - val_loss: 178.1937\n",
      "Epoch 186/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 362.5516 - val_loss: 172.6792\n",
      "Epoch 187/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 354.8098 - val_loss: 167.2808\n",
      "Epoch 188/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 347.1965 - val_loss: 161.9977\n",
      "Epoch 189/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 339.7102 - val_loss: 156.8281\n",
      "Epoch 190/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 332.3496 - val_loss: 151.7709\n",
      "Epoch 191/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 325.1137 - val_loss: 146.8251\n",
      "Epoch 192/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 318.0013 - val_loss: 141.9889\n",
      "Epoch 193/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 311.0105 - val_loss: 137.2614\n",
      "Epoch 194/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 304.1406 - val_loss: 132.6406\n",
      "Epoch 195/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 297.3899 - val_loss: 128.1258\n",
      "Epoch 196/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 290.7575 - val_loss: 123.7155\n",
      "Epoch 197/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 284.2420 - val_loss: 119.4083\n",
      "Epoch 198/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 277.8419 - val_loss: 115.2030\n",
      "Epoch 199/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 271.5560 - val_loss: 111.0982\n",
      "Epoch 200/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 265.3832 - val_loss: 107.0924\n",
      "Epoch 201/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 259.3218 - val_loss: 103.1846\n",
      "Epoch 202/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 253.3711 - val_loss: 99.3735\n",
      "Epoch 203/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 247.5294 - val_loss: 95.6575\n",
      "Epoch 204/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 241.7957 - val_loss: 92.0356\n",
      "Epoch 205/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 236.1687 - val_loss: 88.5064\n",
      "Epoch 206/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 230.6468 - val_loss: 85.0684\n",
      "Epoch 207/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 225.2291 - val_loss: 81.7208\n",
      "Epoch 208/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 219.9144 - val_loss: 78.4620\n",
      "Epoch 209/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 214.7012 - val_loss: 75.2907\n",
      "Epoch 210/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 209.5885 - val_loss: 72.2055\n",
      "Epoch 211/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 204.5747 - val_loss: 69.2055\n",
      "Epoch 212/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 199.6590 - val_loss: 66.2893\n",
      "Epoch 213/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 194.8400 - val_loss: 63.4557\n",
      "Epoch 214/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 190.1165 - val_loss: 60.7031\n",
      "Epoch 215/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 185.4869 - val_loss: 58.0305\n",
      "Epoch 216/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 180.9504 - val_loss: 55.4366\n",
      "Epoch 217/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 176.5055 - val_loss: 52.9200\n",
      "Epoch 218/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 172.1513 - val_loss: 50.4800\n",
      "Epoch 219/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 167.8864 - val_loss: 48.1147\n",
      "Epoch 220/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 163.7097 - val_loss: 45.8234\n",
      "Epoch 221/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 159.6199 - val_loss: 43.6045\n",
      "Epoch 222/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 155.6159 - val_loss: 41.4570\n",
      "Epoch 223/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 151.6965 - val_loss: 39.3797\n",
      "Epoch 224/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 147.8605 - val_loss: 37.3713\n",
      "Epoch 225/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 144.1067 - val_loss: 35.4306\n",
      "Epoch 226/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 140.4340 - val_loss: 33.5564\n",
      "Epoch 227/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 136.8411 - val_loss: 31.7478\n",
      "Epoch 228/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 133.3270 - val_loss: 30.0031\n",
      "Epoch 229/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 129.8904 - val_loss: 28.3213\n",
      "Epoch 230/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 126.5301 - val_loss: 26.7015\n",
      "Epoch 231/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 123.2450 - val_loss: 25.1425\n",
      "Epoch 232/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 120.0344 - val_loss: 23.6428\n",
      "Epoch 233/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 116.8966 - val_loss: 22.2014\n",
      "Epoch 234/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 113.8307 - val_loss: 20.8174\n",
      "Epoch 235/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 110.8355 - val_loss: 19.4894\n",
      "Epoch 236/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 107.9100 - val_loss: 18.2164\n",
      "Epoch 237/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 105.0531 - val_loss: 16.9970\n",
      "Epoch 238/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 102.2634 - val_loss: 15.8306\n",
      "Epoch 239/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 99.5400 - val_loss: 14.7155\n",
      "Epoch 240/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 96.8820 - val_loss: 13.6511\n",
      "Epoch 241/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 94.2880 - val_loss: 12.6361\n",
      "Epoch 242/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 91.7571 - val_loss: 11.6693\n",
      "Epoch 243/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 89.2881 - val_loss: 10.7498\n",
      "Epoch 244/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 86.8802 - val_loss: 9.8766\n",
      "Epoch 245/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 84.5322 - val_loss: 9.0485\n",
      "Epoch 246/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 82.2432 - val_loss: 8.2645\n",
      "Epoch 247/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 80.0117 - val_loss: 7.5235\n",
      "Epoch 248/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 77.8372 - val_loss: 6.8246\n",
      "Epoch 249/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 75.7184 - val_loss: 6.1667\n",
      "Epoch 250/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 73.6546 - val_loss: 5.5488\n",
      "Epoch 251/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 71.6444 - val_loss: 4.9698\n",
      "Epoch 252/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 69.6869 - val_loss: 4.4288\n",
      "Epoch 253/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 67.7812 - val_loss: 3.9249\n",
      "Epoch 254/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 65.9264 - val_loss: 3.4571\n",
      "Epoch 255/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 64.1214 - val_loss: 3.0243\n",
      "Epoch 256/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 62.3652 - val_loss: 2.6256\n",
      "Epoch 257/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 60.6568 - val_loss: 2.2600\n",
      "Epoch 258/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 58.9955 - val_loss: 1.9267\n",
      "Epoch 259/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 57.3801 - val_loss: 1.6248\n",
      "Epoch 260/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 55.8097 - val_loss: 1.3532\n",
      "Epoch 261/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 54.2835 - val_loss: 1.1111\n",
      "Epoch 262/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 52.8007 - val_loss: 0.8977\n",
      "Epoch 263/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 51.3601 - val_loss: 0.7120\n",
      "Epoch 264/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 49.9610 - val_loss: 0.5531\n",
      "Epoch 265/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 48.6024 - val_loss: 0.4203\n",
      "Epoch 266/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 47.2835 - val_loss: 0.3126\n",
      "Epoch 267/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 46.0035 - val_loss: 0.2292\n",
      "Epoch 268/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 44.7614 - val_loss: 0.1694\n",
      "Epoch 269/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 43.5566 - val_loss: 0.1322\n",
      "Epoch 270/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 42.3881 - val_loss: 0.1170\n",
      "Epoch 271/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 41.2551 - val_loss: 0.1228\n",
      "Epoch 272/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 40.1567 - val_loss: 0.1490\n",
      "Epoch 273/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 39.0922 - val_loss: 0.1947\n",
      "Epoch 274/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 38.0607 - val_loss: 0.2592\n",
      "Epoch 275/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 37.0616 - val_loss: 0.3418\n",
      "Epoch 276/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 36.0939 - val_loss: 0.4418\n",
      "Epoch 277/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 35.1572 - val_loss: 0.5584\n",
      "Epoch 278/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 34.2503 - val_loss: 0.6909\n",
      "Epoch 279/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 33.3727 - val_loss: 0.8386\n",
      "Epoch 280/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 32.5238 - val_loss: 1.0009\n",
      "Epoch 281/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 31.7026 - val_loss: 1.1771\n",
      "Epoch 282/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 30.9086 - val_loss: 1.3666\n",
      "Epoch 283/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 30.1409 - val_loss: 1.5686\n",
      "Epoch 284/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 29.3991 - val_loss: 1.7826\n",
      "Epoch 285/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 28.6823 - val_loss: 2.0079\n",
      "Epoch 286/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 27.9899 - val_loss: 2.2440\n",
      "Epoch 287/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 27.3213 - val_loss: 2.4903\n",
      "Epoch 288/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 26.6758 - val_loss: 2.7462\n",
      "Epoch 289/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 26.0529 - val_loss: 3.0110\n",
      "Epoch 290/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.4517 - val_loss: 3.2844\n",
      "Epoch 291/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.8718 - val_loss: 3.5657\n",
      "Epoch 292/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3126 - val_loss: 3.8543\n",
      "Epoch 293/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 23.7735 - val_loss: 4.1498\n",
      "Epoch 294/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 23.2539 - val_loss: 4.4517\n",
      "Epoch 295/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 22.7532 - val_loss: 4.7595\n",
      "Epoch 296/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 22.2709 - val_loss: 5.0728\n",
      "Epoch 297/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 21.8064 - val_loss: 5.3911\n",
      "Epoch 298/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 21.3592 - val_loss: 5.7138\n",
      "Epoch 299/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 20.9287 - val_loss: 6.0406\n",
      "Epoch 300/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 20.5146 - val_loss: 6.3711\n",
      "Epoch 301/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 20.1162 - val_loss: 6.7047\n",
      "Epoch 302/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 19.7331 - val_loss: 7.0412\n",
      "Epoch 303/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 19.3647 - val_loss: 7.3802\n",
      "Epoch 304/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 19.0107 - val_loss: 7.7212\n",
      "Epoch 305/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 18.6706 - val_loss: 8.0640\n",
      "Epoch 306/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.3438 - val_loss: 8.4080\n",
      "Epoch 307/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.0301 - val_loss: 8.7530\n",
      "Epoch 308/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 17.7289 - val_loss: 9.0987\n",
      "Epoch 309/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 17.4399 - val_loss: 9.4447\n",
      "Epoch 310/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 17.1627 - val_loss: 9.7909\n",
      "Epoch 311/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.8966 - val_loss: 10.1368\n",
      "Epoch 312/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.6415 - val_loss: 10.4823\n",
      "Epoch 313/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 16.3970 - val_loss: 10.8268\n",
      "Epoch 314/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.1627 - val_loss: 11.1703\n",
      "Epoch 315/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 15.9382 - val_loss: 11.5125\n",
      "Epoch 316/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.7232 - val_loss: 11.8531\n",
      "Epoch 317/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.5174 - val_loss: 12.1919\n",
      "Epoch 318/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.3203 - val_loss: 12.5287\n",
      "Epoch 319/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1318 - val_loss: 12.8634\n",
      "Epoch 320/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.9514 - val_loss: 13.1956\n",
      "Epoch 321/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.7789 - val_loss: 13.5252\n",
      "Epoch 322/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.6140 - val_loss: 13.8521\n",
      "Epoch 323/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 14.4563 - val_loss: 14.1759\n",
      "Epoch 324/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.3057 - val_loss: 14.4967\n",
      "Epoch 325/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.1618 - val_loss: 14.8142\n",
      "Epoch 326/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.0244 - val_loss: 15.1283\n",
      "Epoch 327/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8932 - val_loss: 15.4388\n",
      "Epoch 328/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.7680 - val_loss: 15.7455\n",
      "Epoch 329/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 13.6486 - val_loss: 16.0487\n",
      "Epoch 330/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.5346 - val_loss: 16.3479\n",
      "Epoch 331/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.4259 - val_loss: 16.6428\n",
      "Epoch 332/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.3224 - val_loss: 16.9339\n",
      "Epoch 333/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.2236 - val_loss: 17.2209\n",
      "Epoch 334/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.1295 - val_loss: 17.5035\n",
      "Epoch 335/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.0399 - val_loss: 17.7819\n",
      "Epoch 336/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.9545 - val_loss: 18.0559\n",
      "Epoch 337/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.8732 - val_loss: 18.3253\n",
      "Epoch 338/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 12.7958 - val_loss: 18.5904\n",
      "Epoch 339/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.7222 - val_loss: 18.8509\n",
      "Epoch 340/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.6521 - val_loss: 19.1068\n",
      "Epoch 341/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 12.5855 - val_loss: 19.3580\n",
      "Epoch 342/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.5221 - val_loss: 19.6047\n",
      "Epoch 343/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.4619 - val_loss: 19.8466\n",
      "Epoch 344/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.4047 - val_loss: 20.0838\n",
      "Epoch 345/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 12.3503 - val_loss: 20.3165\n",
      "Epoch 346/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.2986 - val_loss: 20.5444\n",
      "Epoch 347/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.2495 - val_loss: 20.7677\n",
      "Epoch 348/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.2029 - val_loss: 20.9863\n",
      "Epoch 349/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 12.1587 - val_loss: 21.2000\n",
      "Epoch 350/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 12.1167 - val_loss: 21.4093\n",
      "Epoch 351/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 12.0768 - val_loss: 21.6140\n",
      "Epoch 352/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 12.0390 - val_loss: 21.8141\n",
      "Epoch 353/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.0031 - val_loss: 22.0095\n",
      "Epoch 354/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.9691 - val_loss: 22.2002\n",
      "Epoch 355/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.9368 - val_loss: 22.3867\n",
      "Epoch 356/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.9062 - val_loss: 22.5686\n",
      "Epoch 357/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8772 - val_loss: 22.7459\n",
      "Epoch 358/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8497 - val_loss: 22.9189\n",
      "Epoch 359/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8237 - val_loss: 23.0876\n",
      "Epoch 360/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7990 - val_loss: 23.2518\n",
      "Epoch 361/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7756 - val_loss: 23.4118\n",
      "Epoch 362/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.7535 - val_loss: 23.5677\n",
      "Epoch 363/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7325 - val_loss: 23.7195\n",
      "Epoch 364/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7126 - val_loss: 23.8670\n",
      "Epoch 365/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.6937 - val_loss: 24.0105\n",
      "Epoch 366/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.6759 - val_loss: 24.1499\n",
      "Epoch 367/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.6591 - val_loss: 24.2855\n",
      "Epoch 368/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.6431 - val_loss: 24.4172\n",
      "Epoch 369/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.6280 - val_loss: 24.5450\n",
      "Epoch 370/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.6137 - val_loss: 24.6691\n",
      "Epoch 371/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.6002 - val_loss: 24.7896\n",
      "Epoch 372/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.5874 - val_loss: 24.9064\n",
      "Epoch 373/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.5752 - val_loss: 25.0198\n",
      "Epoch 374/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.5638 - val_loss: 25.1296\n",
      "Epoch 375/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.5529 - val_loss: 25.2360\n",
      "Epoch 376/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.5426 - val_loss: 25.3391\n",
      "Epoch 377/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.5329 - val_loss: 25.4390\n",
      "Epoch 378/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.5237 - val_loss: 25.5354\n",
      "Epoch 379/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.5150 - val_loss: 25.6287\n",
      "Epoch 380/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.5068 - val_loss: 25.7191\n",
      "Epoch 381/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.4990 - val_loss: 25.8065\n",
      "Epoch 382/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.4917 - val_loss: 25.8910\n",
      "Epoch 383/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.4847 - val_loss: 25.9724\n",
      "Epoch 384/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.4782 - val_loss: 26.0513\n",
      "Epoch 385/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.4719 - val_loss: 26.1273\n",
      "Epoch 386/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.4660 - val_loss: 26.2006\n",
      "Epoch 387/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.4605 - val_loss: 26.2714\n",
      "Epoch 388/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.4552 - val_loss: 26.3396\n",
      "Epoch 389/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.4503 - val_loss: 26.4055\n",
      "Epoch 390/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.4455 - val_loss: 26.4690\n",
      "Epoch 391/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.4411 - val_loss: 26.5300\n",
      "Epoch 392/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.4369 - val_loss: 26.5889\n",
      "Epoch 393/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 11.4329 - val_loss: 26.6456\n",
      "Epoch 394/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.4291 - val_loss: 26.7000\n",
      "Epoch 395/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.4256 - val_loss: 26.7525\n",
      "Epoch 396/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.4222 - val_loss: 26.8027\n",
      "Epoch 397/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.4191 - val_loss: 26.8510\n",
      "Epoch 398/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.4161 - val_loss: 26.8975\n",
      "Epoch 399/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.4132 - val_loss: 26.9423\n",
      "Epoch 400/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.4105 - val_loss: 26.9850\n",
      "Epoch 401/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.4080 - val_loss: 27.0263\n",
      "Epoch 402/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.4056 - val_loss: 27.0657\n",
      "Epoch 403/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.4034 - val_loss: 27.1036\n",
      "Epoch 404/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.4013 - val_loss: 27.1399\n",
      "Epoch 405/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3992 - val_loss: 27.1747\n",
      "Epoch 406/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3974 - val_loss: 27.2081\n",
      "Epoch 407/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3956 - val_loss: 27.2400\n",
      "Epoch 408/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3939 - val_loss: 27.2704\n",
      "Epoch 409/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3923 - val_loss: 27.2996\n",
      "Epoch 410/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.3908 - val_loss: 27.3276\n",
      "Epoch 411/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3894 - val_loss: 27.3542\n",
      "Epoch 412/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3881 - val_loss: 27.3798\n",
      "Epoch 413/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3868 - val_loss: 27.4043\n",
      "Epoch 414/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.3857 - val_loss: 27.4276\n",
      "Epoch 415/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3845 - val_loss: 27.4496\n",
      "Epoch 416/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.3835 - val_loss: 27.4709\n",
      "Epoch 417/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3825 - val_loss: 27.4911\n",
      "Epoch 418/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3816 - val_loss: 27.5102\n",
      "Epoch 419/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3808 - val_loss: 27.5286\n",
      "Epoch 420/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3800 - val_loss: 27.5459\n",
      "Epoch 421/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3793 - val_loss: 27.5626\n",
      "Epoch 422/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3786 - val_loss: 27.5786\n",
      "Epoch 423/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3779 - val_loss: 27.5937\n",
      "Epoch 424/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3773 - val_loss: 27.6081\n",
      "Epoch 425/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3767 - val_loss: 27.6217\n",
      "Epoch 426/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3763 - val_loss: 27.6347\n",
      "Epoch 427/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3757 - val_loss: 27.6470\n",
      "Epoch 428/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3753 - val_loss: 27.6586\n",
      "Epoch 429/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3749 - val_loss: 27.6697\n",
      "Epoch 430/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3745 - val_loss: 27.6801\n",
      "Epoch 431/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3742 - val_loss: 27.6901\n",
      "Epoch 432/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3738 - val_loss: 27.6996\n",
      "Epoch 433/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3736 - val_loss: 27.7086\n",
      "Epoch 434/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3733 - val_loss: 27.7171\n",
      "Epoch 435/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.3731 - val_loss: 27.7251\n",
      "Epoch 436/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3728 - val_loss: 27.7325\n",
      "Epoch 437/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3727 - val_loss: 27.7397\n",
      "Epoch 438/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3725 - val_loss: 27.7464\n",
      "Epoch 439/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3723 - val_loss: 27.7527\n",
      "Epoch 440/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3722 - val_loss: 27.7586\n",
      "Epoch 441/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3721 - val_loss: 27.7643\n",
      "Epoch 442/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3720 - val_loss: 27.7695\n",
      "Epoch 443/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3720 - val_loss: 27.7747\n",
      "Epoch 444/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3719 - val_loss: 27.7793\n",
      "Epoch 445/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3718 - val_loss: 27.7837\n",
      "Epoch 446/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3718 - val_loss: 27.7880\n",
      "Epoch 447/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3718 - val_loss: 27.7919\n",
      "Epoch 448/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3718 - val_loss: 27.7956\n",
      "Epoch 449/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3718 - val_loss: 27.7989\n",
      "Epoch 450/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3719 - val_loss: 27.8023\n",
      "Epoch 451/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.3719 - val_loss: 27.8052\n",
      "Epoch 452/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3719 - val_loss: 27.8080\n",
      "Epoch 453/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3720 - val_loss: 27.8107\n",
      "Epoch 454/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.3720 - val_loss: 27.8130\n",
      "Epoch 455/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3721 - val_loss: 27.8153\n",
      "Epoch 456/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3722 - val_loss: 27.8174\n",
      "Epoch 457/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3723 - val_loss: 27.8193\n",
      "Epoch 458/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3724 - val_loss: 27.8213\n",
      "Epoch 459/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3725 - val_loss: 27.8229\n",
      "Epoch 460/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3726 - val_loss: 27.8245\n",
      "Epoch 461/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.3727 - val_loss: 27.8260\n",
      "Epoch 462/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.3728 - val_loss: 27.8273\n",
      "Epoch 463/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3729 - val_loss: 27.8284\n",
      "Epoch 464/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3731 - val_loss: 27.8298\n",
      "Epoch 465/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3732 - val_loss: 27.8308\n",
      "Epoch 466/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3734 - val_loss: 27.8317\n",
      "Epoch 467/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3735 - val_loss: 27.8326\n",
      "Epoch 468/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 11.3737 - val_loss: 27.8333\n",
      "Epoch 469/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3738 - val_loss: 27.8342\n",
      "Epoch 470/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3740 - val_loss: 27.8348\n",
      "Epoch 471/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3741 - val_loss: 27.8356\n",
      "Epoch 472/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3743 - val_loss: 27.8361\n",
      "Epoch 473/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3745 - val_loss: 27.8365\n",
      "Epoch 474/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3746 - val_loss: 27.8369\n",
      "Epoch 475/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3748 - val_loss: 27.8372\n",
      "Epoch 476/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3750 - val_loss: 27.8377\n",
      "Epoch 477/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3751 - val_loss: 27.8381\n",
      "Epoch 478/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3753 - val_loss: 27.8383\n",
      "Epoch 479/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3755 - val_loss: 27.8385\n",
      "Epoch 480/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3757 - val_loss: 27.8388\n",
      "Epoch 481/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3759 - val_loss: 27.8390\n",
      "Epoch 482/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.3761 - val_loss: 27.8391\n",
      "Epoch 483/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3763 - val_loss: 27.8392\n",
      "Epoch 484/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3764 - val_loss: 27.8392\n",
      "Epoch 485/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 11.3766 - val_loss: 27.8391\n",
      "Epoch 486/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3768 - val_loss: 27.8391\n",
      "Epoch 487/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3770 - val_loss: 27.8390\n",
      "Epoch 488/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3772 - val_loss: 27.8390\n",
      "Epoch 489/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3774 - val_loss: 27.8390\n",
      "Epoch 490/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3776 - val_loss: 27.8390\n",
      "Epoch 491/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3777 - val_loss: 27.8388\n",
      "Epoch 492/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3779 - val_loss: 27.8385\n",
      "Epoch 493/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3781 - val_loss: 27.8383\n",
      "Epoch 494/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3783 - val_loss: 27.8382\n",
      "Epoch 495/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.3785 - val_loss: 27.8379\n",
      "Epoch 496/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3787 - val_loss: 27.8377\n",
      "Epoch 497/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3789 - val_loss: 27.8375\n",
      "Epoch 498/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3791 - val_loss: 27.8373\n",
      "Epoch 499/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3793 - val_loss: 27.8370\n",
      "Epoch 500/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3795 - val_loss: 27.8368\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.1552, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0026, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0004, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0102, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.8734, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1e-10\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 395ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.09244258e+01, 7.09154622e+01, 7.09064986e+01, 7.08976891e+01,\n",
       "        7.08892857e+01, 7.08808823e+01, 7.08724790e+01, 7.08640756e+01,\n",
       "        7.08556723e+01, 7.08472689e+01, 7.08388655e+01, 7.08304622e+01,\n",
       "        7.08220588e+01, 7.08136555e+01, 7.08052521e+01, 7.07968487e+01,\n",
       "        7.07884454e+01, 7.07800420e+01, 7.07716387e+01, 7.07632353e+01,\n",
       "        7.07548319e+01, 7.07464286e+01, 7.07380252e+01, 7.07296219e+01,\n",
       "        7.07212185e+01, 7.07128151e+01, 7.07044118e+01, 7.06960084e+01,\n",
       "        7.06876050e+01, 7.06792017e+01, 7.06707983e+01, 7.06623950e+01,\n",
       "        7.06539916e+01, 7.06455882e+01, 7.06371849e+01, 7.06287815e+01,\n",
       "        7.06203781e+01, 7.06119748e+01, 7.06035714e+01, 7.05887255e+01,\n",
       "        7.05691176e+01, 7.05495098e+01, 7.05299020e+01, 7.05102941e+01,\n",
       "        7.04906863e+01, 7.04710784e+01, 7.04514706e+01, 7.04318627e+01,\n",
       "        7.04122549e+01, 7.03926471e+01, 7.03730392e+01, 7.03534314e+01,\n",
       "        7.03338235e+01, 7.03142157e+01, 7.02946078e+01, 7.02750000e+01,\n",
       "        7.02553922e+01, 7.02357843e+01, 7.02161765e+01, 7.01965686e+01,\n",
       "        7.01769608e+01, 7.01573529e+01, 7.01377451e+01, 7.01181373e+01,\n",
       "        7.00985294e+01, 7.00789216e+01, 7.00593137e+01, 7.00397059e+01,\n",
       "        7.00200980e+01, 7.00004902e+01, 6.99808824e+01, 6.99612745e+01,\n",
       "        6.99416667e+01, 6.99220588e+01, 6.99024510e+01, 6.98607843e+01,\n",
       "        6.98159664e+01, 6.97711485e+01, 6.97263305e+01, 6.96815126e+01,\n",
       "        7.57085190e+01, 0.00000000e+00, 4.62659091e-01, 1.18381113e-01,\n",
       "        1.86991952e-02, 2.43531480e-01, 9.45959151e-01, 0.00000000e+00,\n",
       "        6.51802570e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.03351235e-01, 7.60643259e-02, 0.00000000e+00,\n",
       "        1.71475142e-01, 7.76654109e-04, 6.86436072e-02, 4.62394208e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([67.77735761, 67.76802054, 67.75868347, 67.74934641, 67.74000934,\n",
       "       67.73067227, 67.7213352 , 67.71199813, 67.70266106, 67.693324  ,\n",
       "       67.68398693, 67.67464986, 67.66531279, 67.65597572, 67.64663866,\n",
       "       67.63730159, 67.62796452, 67.61862745, 67.60929038, 67.59995331,\n",
       "       67.59061625, 67.58127918, 67.57194211, 67.56260504, 67.55326797,\n",
       "       67.54393091, 67.53459384, 67.52525677, 67.5159197 , 67.50658263,\n",
       "       67.49724556, 67.4879085 , 67.47857143, 67.46923436, 67.45989729,\n",
       "       67.45056022, 67.44122316, 67.43188609, 67.42254902, 67.41321195,\n",
       "       67.40387488, 67.39453782, 67.38520075, 67.37586368, 67.36652661,\n",
       "       67.35718954, 67.34785247, 67.33851541, 67.32917834, 67.31984127,\n",
       "       67.3105042 , 67.30116713, 67.29183007, 67.282493  , 67.27315593,\n",
       "       67.26381886, 67.25448179, 67.24514472, 67.23580766, 67.22647059,\n",
       "       67.21713352, 67.20779645, 67.19845938, 67.18912232, 67.17978525,\n",
       "       67.17044818, 67.16111111, 67.15177404, 67.14243697, 67.13309991,\n",
       "       67.12376284, 67.11442577, 67.1050887 , 67.09575163, 67.08641457,\n",
       "       67.0770775 , 67.06774043, 67.05840336, 67.04906629, 67.03972923,\n",
       "       67.03039216, 67.02105509, 67.01171802, 67.00238095, 66.99304388,\n",
       "       66.98370682, 66.97436975, 66.96503268, 66.95569561, 66.94635854,\n",
       "       66.93702148, 66.92768441, 66.91834734, 66.90901027, 66.8996732 ,\n",
       "       66.89033613, 66.88099907, 66.871662  , 66.86232493, 66.85298786])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.244669663832735\n",
      "15.220269403924169\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
