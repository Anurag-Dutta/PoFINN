{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "1795    66.125303\n",
       "1796    66.117834\n",
       "1797    66.110364\n",
       "1798    66.102894\n",
       "1799    66.095425\n",
       "Name: C2, Length: 1800, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c2_interpolated_1700_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "1695     0.000000\n",
       "1696     0.171475\n",
       "1697     0.000777\n",
       "1698     0.068644\n",
       "1699     0.462394\n",
       "Name: C2, Length: 1700, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1700)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.500000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.275910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.051821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.827731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.603641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     90.500000  0.000298\n",
       "1     90.275910  0.000298\n",
       "2     90.051821  0.000297\n",
       "3     89.827731  0.000297\n",
       "4     89.603641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.1552  # Value for C0\n",
    "K0 = -0.0026  # Value for K0\n",
    "K1 = -0.0004  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0102    # Value for b\n",
    "c = 2.8734    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    90.500000    90.275910    90.051821    89.827731    89.603641   \n",
      "351    90.275910    90.051821    89.827731    89.603641    89.379552   \n",
      "352    90.051821    89.827731    89.603641    89.379552    89.155462   \n",
      "353    89.827731    89.603641    89.379552    89.155462    88.931373   \n",
      "354    89.603641    89.379552    89.155462    88.931373    88.707283   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    89.379552    89.155462    88.931373    88.707283    88.494958  ...   \n",
      "351    89.155462    88.931373    88.707283    88.494958    88.427731  ...   \n",
      "352    88.931373    88.707283    88.494958    88.427731    88.360504  ...   \n",
      "353    88.707283    88.494958    88.427731    88.360504    88.293277  ...   \n",
      "354    88.494958    88.427731    88.360504    88.293277    88.226050  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   79.071008    0.000263   79.054202    0.000263   79.037395    0.000263   \n",
      "351   79.054202    0.000263   79.037395    0.000263   79.020588    0.000262   \n",
      "352   79.037395    0.000263   79.020588    0.000262   79.003782    0.000262   \n",
      "353   79.020588    0.000262   79.003782    0.000262   78.986975    0.000262   \n",
      "354   79.003782    0.000262   78.986975    0.000262   78.970168    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   79.020588    0.000262   79.003782    0.000262  \n",
      "351   79.003782    0.000262   78.986975    0.000262  \n",
      "352   78.986975    0.000262   78.970168    0.000262  \n",
      "353   78.970168    0.000262   78.953361    0.000262  \n",
      "354   78.953361    0.000262   78.936555    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1250, 1, 251) (1250, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "16/16 [==============================] - 2s 30ms/step - loss: 5567.9663 - val_loss: 4670.4399\n",
      "Epoch 2/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5485.9258 - val_loss: 4620.0596\n",
      "Epoch 3/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5431.2202 - val_loss: 4569.7969\n",
      "Epoch 4/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5376.8555 - val_loss: 4519.9941\n",
      "Epoch 5/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5322.9785 - val_loss: 4470.6582\n",
      "Epoch 6/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5269.5752 - val_loss: 4421.7671\n",
      "Epoch 7/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5216.6206 - val_loss: 4373.3018\n",
      "Epoch 8/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5164.1001 - val_loss: 4325.2495\n",
      "Epoch 9/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5112.0015 - val_loss: 4277.5996\n",
      "Epoch 10/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5060.3145 - val_loss: 4230.3467\n",
      "Epoch 11/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5009.0317 - val_loss: 4183.4814\n",
      "Epoch 12/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4953.8716 - val_loss: 4130.6943\n",
      "Epoch 13/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4898.8354 - val_loss: 4080.7102\n",
      "Epoch 14/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4844.7876 - val_loss: 4031.6985\n",
      "Epoch 15/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4791.7075 - val_loss: 3983.5068\n",
      "Epoch 16/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4739.4233 - val_loss: 3935.9966\n",
      "Epoch 17/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4687.8110 - val_loss: 3889.0803\n",
      "Epoch 18/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4636.7920 - val_loss: 3842.7043\n",
      "Epoch 19/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4586.3198 - val_loss: 3796.8303\n",
      "Epoch 20/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4536.3579 - val_loss: 3751.4333\n",
      "Epoch 21/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4486.8809 - val_loss: 3706.4932\n",
      "Epoch 22/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4437.8716 - val_loss: 3661.9961\n",
      "Epoch 23/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4389.3149 - val_loss: 3617.9280\n",
      "Epoch 24/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4341.1987 - val_loss: 3574.2795\n",
      "Epoch 25/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4293.5127 - val_loss: 3531.0420\n",
      "Epoch 26/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4246.2480 - val_loss: 3488.2087\n",
      "Epoch 27/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4199.3979 - val_loss: 3445.7732\n",
      "Epoch 28/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4152.9565 - val_loss: 3403.7290\n",
      "Epoch 29/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4106.9175 - val_loss: 3362.0715\n",
      "Epoch 30/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4061.2749 - val_loss: 3320.7954\n",
      "Epoch 31/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4016.0256 - val_loss: 3279.8967\n",
      "Epoch 32/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3971.1633 - val_loss: 3239.3721\n",
      "Epoch 33/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3926.6855 - val_loss: 3199.2166\n",
      "Epoch 34/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3882.5872 - val_loss: 3159.4270\n",
      "Epoch 35/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3838.8655 - val_loss: 3119.9998\n",
      "Epoch 36/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3795.5168 - val_loss: 3080.9326\n",
      "Epoch 37/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3752.5371 - val_loss: 3042.2207\n",
      "Epoch 38/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3709.9246 - val_loss: 3003.8625\n",
      "Epoch 39/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3667.6741 - val_loss: 2965.8550\n",
      "Epoch 40/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3625.7854 - val_loss: 2928.1941\n",
      "Epoch 41/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3584.2534 - val_loss: 2890.8789\n",
      "Epoch 42/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3543.0769 - val_loss: 2853.9055\n",
      "Epoch 43/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3502.2524 - val_loss: 2817.2720\n",
      "Epoch 44/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3461.7776 - val_loss: 2780.9753\n",
      "Epoch 45/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3421.6499 - val_loss: 2745.0137\n",
      "Epoch 46/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3381.8667 - val_loss: 2709.3843\n",
      "Epoch 47/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3342.4260 - val_loss: 2674.0842\n",
      "Epoch 48/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3303.3245 - val_loss: 2639.1133\n",
      "Epoch 49/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3264.5610 - val_loss: 2604.4666\n",
      "Epoch 50/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3226.1328 - val_loss: 2570.1436\n",
      "Epoch 51/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3188.0371 - val_loss: 2536.1414\n",
      "Epoch 52/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3150.2727 - val_loss: 2502.4575\n",
      "Epoch 53/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3112.8364 - val_loss: 2469.0911\n",
      "Epoch 54/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3075.7266 - val_loss: 2436.0388\n",
      "Epoch 55/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3038.9409 - val_loss: 2403.2993\n",
      "Epoch 56/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3002.4775 - val_loss: 2370.8691\n",
      "Epoch 57/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2966.3342 - val_loss: 2338.7483\n",
      "Epoch 58/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2930.5088 - val_loss: 2306.9343\n",
      "Epoch 59/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2894.9998 - val_loss: 2275.4238\n",
      "Epoch 60/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2859.8040 - val_loss: 2244.2158\n",
      "Epoch 61/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2824.9209 - val_loss: 2213.3088\n",
      "Epoch 62/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2790.3479 - val_loss: 2182.7000\n",
      "Epoch 63/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2756.0830 - val_loss: 2152.3879\n",
      "Epoch 64/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2722.1245 - val_loss: 2122.3701\n",
      "Epoch 65/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2688.4695 - val_loss: 2092.6460\n",
      "Epoch 66/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2655.1174 - val_loss: 2063.2122\n",
      "Epoch 67/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2622.0657 - val_loss: 2034.0676\n",
      "Epoch 68/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2589.3127 - val_loss: 2005.2104\n",
      "Epoch 69/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2556.8567 - val_loss: 1976.6385\n",
      "Epoch 70/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2524.6953 - val_loss: 1948.3503\n",
      "Epoch 71/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2492.8271 - val_loss: 1920.3439\n",
      "Epoch 72/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2461.2507 - val_loss: 1892.6182\n",
      "Epoch 73/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2429.9639 - val_loss: 1865.1704\n",
      "Epoch 74/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2398.9644 - val_loss: 1837.9989\n",
      "Epoch 75/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2368.2515 - val_loss: 1811.1028\n",
      "Epoch 76/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2337.8232 - val_loss: 1784.4790\n",
      "Epoch 77/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2307.6765 - val_loss: 1758.1272\n",
      "Epoch 78/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2277.8113 - val_loss: 1732.0448\n",
      "Epoch 79/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2248.2253 - val_loss: 1706.2306\n",
      "Epoch 80/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2218.9170 - val_loss: 1680.6830\n",
      "Epoch 81/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2189.8843 - val_loss: 1655.3998\n",
      "Epoch 82/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2161.1260 - val_loss: 1630.3800\n",
      "Epoch 83/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2132.6396 - val_loss: 1605.6205\n",
      "Epoch 84/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2104.4243 - val_loss: 1581.1208\n",
      "Epoch 85/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2076.4783 - val_loss: 1556.8796\n",
      "Epoch 86/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2048.7996 - val_loss: 1532.8948\n",
      "Epoch 87/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2021.3868 - val_loss: 1509.1646\n",
      "Epoch 88/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1994.2384 - val_loss: 1485.6876\n",
      "Epoch 89/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1967.3525 - val_loss: 1462.4619\n",
      "Epoch 90/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1940.7280 - val_loss: 1439.4868\n",
      "Epoch 91/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1914.3628 - val_loss: 1416.7594\n",
      "Epoch 92/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1888.2554 - val_loss: 1394.2791\n",
      "Epoch 93/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1862.4044 - val_loss: 1372.0438\n",
      "Epoch 94/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1836.8079 - val_loss: 1350.0522\n",
      "Epoch 95/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1811.4647 - val_loss: 1328.3030\n",
      "Epoch 96/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1786.3739 - val_loss: 1306.7948\n",
      "Epoch 97/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1761.5322 - val_loss: 1285.5243\n",
      "Epoch 98/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1736.9391 - val_loss: 1264.4917\n",
      "Epoch 99/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1712.5934 - val_loss: 1243.6952\n",
      "Epoch 100/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1688.4937 - val_loss: 1223.1335\n",
      "Epoch 101/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1664.6375 - val_loss: 1202.8043\n",
      "Epoch 102/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1641.0239 - val_loss: 1182.7065\n",
      "Epoch 103/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1617.6515 - val_loss: 1162.8389\n",
      "Epoch 104/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1594.5186 - val_loss: 1143.1995\n",
      "Epoch 105/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1571.6237 - val_loss: 1123.7872\n",
      "Epoch 106/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1548.9651 - val_loss: 1104.6005\n",
      "Epoch 107/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1526.5419 - val_loss: 1085.6372\n",
      "Epoch 108/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1504.3523 - val_loss: 1066.8967\n",
      "Epoch 109/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1482.3949 - val_loss: 1048.3771\n",
      "Epoch 110/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1460.6681 - val_loss: 1030.0770\n",
      "Epoch 111/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1439.1707 - val_loss: 1011.9951\n",
      "Epoch 112/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1417.9008 - val_loss: 994.1300\n",
      "Epoch 113/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1396.8578 - val_loss: 976.4800\n",
      "Epoch 114/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1376.0396 - val_loss: 959.0438\n",
      "Epoch 115/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1355.4447 - val_loss: 941.8195\n",
      "Epoch 116/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1335.0719 - val_loss: 924.8064\n",
      "Epoch 117/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1314.9199 - val_loss: 908.0025\n",
      "Epoch 118/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1294.9873 - val_loss: 891.4068\n",
      "Epoch 119/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1275.2725 - val_loss: 875.0182\n",
      "Epoch 120/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1255.7744 - val_loss: 858.8345\n",
      "Epoch 121/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1236.4908 - val_loss: 842.8545\n",
      "Epoch 122/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1217.4219 - val_loss: 827.0776\n",
      "Epoch 123/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1198.5649 - val_loss: 811.5009\n",
      "Epoch 124/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1179.9187 - val_loss: 796.1241\n",
      "Epoch 125/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1161.4823 - val_loss: 780.9455\n",
      "Epoch 126/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1143.2540 - val_loss: 765.9642\n",
      "Epoch 127/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1125.2329 - val_loss: 751.1774\n",
      "Epoch 128/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1107.4165 - val_loss: 736.5850\n",
      "Epoch 129/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1089.8044 - val_loss: 722.1854\n",
      "Epoch 130/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1072.3955 - val_loss: 707.9769\n",
      "Epoch 131/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1055.1879 - val_loss: 693.9589\n",
      "Epoch 132/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1038.1803 - val_loss: 680.1292\n",
      "Epoch 133/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1021.3713 - val_loss: 666.4862\n",
      "Epoch 134/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1004.7595 - val_loss: 653.0294\n",
      "Epoch 135/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 988.3441 - val_loss: 639.7574\n",
      "Epoch 136/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 972.1230 - val_loss: 626.6681\n",
      "Epoch 137/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 956.0955 - val_loss: 613.7608\n",
      "Epoch 138/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 940.2599 - val_loss: 601.0336\n",
      "Epoch 139/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 924.6150 - val_loss: 588.4858\n",
      "Epoch 140/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 909.1596 - val_loss: 576.1157\n",
      "Epoch 141/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 893.8921 - val_loss: 563.9219\n",
      "Epoch 142/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 878.8118 - val_loss: 551.9026\n",
      "Epoch 143/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 863.9168 - val_loss: 540.0582\n",
      "Epoch 144/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 849.2061 - val_loss: 528.3857\n",
      "Epoch 145/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 834.6780 - val_loss: 516.8842\n",
      "Epoch 146/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 820.3315 - val_loss: 505.5522\n",
      "Epoch 147/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 806.1651 - val_loss: 494.3885\n",
      "Epoch 148/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 792.1776 - val_loss: 483.3919\n",
      "Epoch 149/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 778.3677 - val_loss: 472.5613\n",
      "Epoch 150/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 764.7344 - val_loss: 461.8951\n",
      "Epoch 151/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 751.2759 - val_loss: 451.3918\n",
      "Epoch 152/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 737.9913 - val_loss: 441.0503\n",
      "Epoch 153/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 724.8791 - val_loss: 430.8694\n",
      "Epoch 154/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 711.9383 - val_loss: 420.8476\n",
      "Epoch 155/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 699.1674 - val_loss: 410.9838\n",
      "Epoch 156/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 686.5651 - val_loss: 401.2762\n",
      "Epoch 157/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 674.1300 - val_loss: 391.7236\n",
      "Epoch 158/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 661.8610 - val_loss: 382.3257\n",
      "Epoch 159/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 649.7568 - val_loss: 373.0794\n",
      "Epoch 160/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 637.8162 - val_loss: 363.9853\n",
      "Epoch 161/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 626.0379 - val_loss: 355.0403\n",
      "Epoch 162/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 614.4208 - val_loss: 346.2448\n",
      "Epoch 163/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 602.9633 - val_loss: 337.5966\n",
      "Epoch 164/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 591.6642 - val_loss: 329.0943\n",
      "Epoch 165/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 580.5223 - val_loss: 320.7366\n",
      "Epoch 166/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 569.5365 - val_loss: 312.5227\n",
      "Epoch 167/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 558.7053 - val_loss: 304.4508\n",
      "Epoch 168/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 548.0278 - val_loss: 296.5205\n",
      "Epoch 169/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 537.5024 - val_loss: 288.7292\n",
      "Epoch 170/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 527.1276 - val_loss: 281.0759\n",
      "Epoch 171/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 516.9027 - val_loss: 273.5604\n",
      "Epoch 172/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 506.8264 - val_loss: 266.1805\n",
      "Epoch 173/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 496.8972 - val_loss: 258.9346\n",
      "Epoch 174/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 487.1142 - val_loss: 251.8222\n",
      "Epoch 175/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 477.4752 - val_loss: 244.8416\n",
      "Epoch 176/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 467.9801 - val_loss: 237.9916\n",
      "Epoch 177/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 458.6273 - val_loss: 231.2715\n",
      "Epoch 178/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 449.4154 - val_loss: 224.6788\n",
      "Epoch 179/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 440.3433 - val_loss: 218.2133\n",
      "Epoch 180/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 431.4097 - val_loss: 211.8731\n",
      "Epoch 181/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 422.6135 - val_loss: 205.6573\n",
      "Epoch 182/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 413.9530 - val_loss: 199.5643\n",
      "Epoch 183/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 405.4273 - val_loss: 193.5927\n",
      "Epoch 184/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 397.0353 - val_loss: 187.7420\n",
      "Epoch 185/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 388.7755 - val_loss: 182.0103\n",
      "Epoch 186/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 380.6470 - val_loss: 176.3968\n",
      "Epoch 187/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 372.6485 - val_loss: 170.8992\n",
      "Epoch 188/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 364.7785 - val_loss: 165.5173\n",
      "Epoch 189/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 357.0359 - val_loss: 160.2495\n",
      "Epoch 190/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 349.4195 - val_loss: 155.0947\n",
      "Epoch 191/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 341.9283 - val_loss: 150.0512\n",
      "Epoch 192/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 334.5609 - val_loss: 145.1182\n",
      "Epoch 193/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 327.3162 - val_loss: 140.2943\n",
      "Epoch 194/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 320.1927 - val_loss: 135.5777\n",
      "Epoch 195/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 313.1891 - val_loss: 130.9681\n",
      "Epoch 196/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 306.3044 - val_loss: 126.4634\n",
      "Epoch 197/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 299.5377 - val_loss: 122.0626\n",
      "Epoch 198/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 292.8871 - val_loss: 117.7648\n",
      "Epoch 199/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 286.3520 - val_loss: 113.5684\n",
      "Epoch 200/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 279.9311 - val_loss: 109.4726\n",
      "Epoch 201/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 273.6231 - val_loss: 105.4755\n",
      "Epoch 202/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 267.4268 - val_loss: 101.5761\n",
      "Epoch 203/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 261.3406 - val_loss: 97.7731\n",
      "Epoch 204/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 255.3640 - val_loss: 94.0656\n",
      "Epoch 205/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 249.4956 - val_loss: 90.4519\n",
      "Epoch 206/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 243.7339 - val_loss: 86.9316\n",
      "Epoch 207/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 238.0779 - val_loss: 83.5022\n",
      "Epoch 208/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 232.5263 - val_loss: 80.1632\n",
      "Epoch 209/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 227.0780 - val_loss: 76.9137\n",
      "Epoch 210/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 221.7321 - val_loss: 73.7516\n",
      "Epoch 211/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 216.4870 - val_loss: 70.6765\n",
      "Epoch 212/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 211.3418 - val_loss: 67.6867\n",
      "Epoch 213/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 206.2952 - val_loss: 64.7812\n",
      "Epoch 214/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 201.3458 - val_loss: 61.9587\n",
      "Epoch 215/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 196.4929 - val_loss: 59.2178\n",
      "Epoch 216/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 191.7347 - val_loss: 56.5574\n",
      "Epoch 217/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 187.0707 - val_loss: 53.9765\n",
      "Epoch 218/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 182.4994 - val_loss: 51.4738\n",
      "Epoch 219/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 178.0195 - val_loss: 49.0478\n",
      "Epoch 220/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 173.6301 - val_loss: 46.6978\n",
      "Epoch 221/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 169.3301 - val_loss: 44.4224\n",
      "Epoch 222/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 165.1181 - val_loss: 42.2201\n",
      "Epoch 223/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 160.9930 - val_loss: 40.0900\n",
      "Epoch 224/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 156.9538 - val_loss: 38.0311\n",
      "Epoch 225/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 152.9992 - val_loss: 36.0419\n",
      "Epoch 226/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 149.1283 - val_loss: 34.1213\n",
      "Epoch 227/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 145.3395 - val_loss: 32.2683\n",
      "Epoch 228/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 141.6322 - val_loss: 30.4815\n",
      "Epoch 229/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 138.0048 - val_loss: 28.7598\n",
      "Epoch 230/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 134.4566 - val_loss: 27.1022\n",
      "Epoch 231/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 130.9865 - val_loss: 25.5074\n",
      "Epoch 232/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 127.5930 - val_loss: 23.9743\n",
      "Epoch 233/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 124.2752 - val_loss: 22.5016\n",
      "Epoch 234/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 121.0317 - val_loss: 21.0884\n",
      "Epoch 235/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 117.8618 - val_loss: 19.7335\n",
      "Epoch 236/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 114.7643 - val_loss: 18.4358\n",
      "Epoch 237/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 111.7381 - val_loss: 17.1940\n",
      "Epoch 238/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 108.7820 - val_loss: 16.0072\n",
      "Epoch 239/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 105.8949 - val_loss: 14.8741\n",
      "Epoch 240/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 103.0759 - val_loss: 13.7937\n",
      "Epoch 241/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 100.3238 - val_loss: 12.7649\n",
      "Epoch 242/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 97.6376 - val_loss: 11.7866\n",
      "Epoch 243/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 95.0163 - val_loss: 10.8579\n",
      "Epoch 244/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 92.4586 - val_loss: 9.9774\n",
      "Epoch 245/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 89.9636 - val_loss: 9.1442\n",
      "Epoch 246/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 87.5302 - val_loss: 8.3571\n",
      "Epoch 247/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 85.1575 - val_loss: 7.6154\n",
      "Epoch 248/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 82.8446 - val_loss: 6.9177\n",
      "Epoch 249/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 80.5901 - val_loss: 6.2631\n",
      "Epoch 250/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 78.3931 - val_loss: 5.6505\n",
      "Epoch 251/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 76.2528 - val_loss: 5.0789\n",
      "Epoch 252/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 74.1679 - val_loss: 4.5474\n",
      "Epoch 253/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 72.1378 - val_loss: 4.0548\n",
      "Epoch 254/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 70.1608 - val_loss: 3.6003\n",
      "Epoch 255/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 68.2366 - val_loss: 3.1827\n",
      "Epoch 256/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 66.3640 - val_loss: 2.8012\n",
      "Epoch 257/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 64.5420 - val_loss: 2.4547\n",
      "Epoch 258/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 62.7697 - val_loss: 2.1423\n",
      "Epoch 259/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 61.0459 - val_loss: 1.8630\n",
      "Epoch 260/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 59.3699 - val_loss: 1.6159\n",
      "Epoch 261/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 57.7407 - val_loss: 1.4000\n",
      "Epoch 262/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 56.1573 - val_loss: 1.2144\n",
      "Epoch 263/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 54.6189 - val_loss: 1.0583\n",
      "Epoch 264/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 53.1245 - val_loss: 0.9306\n",
      "Epoch 265/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 51.6733 - val_loss: 0.8306\n",
      "Epoch 266/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 50.2643 - val_loss: 0.7573\n",
      "Epoch 267/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 48.8967 - val_loss: 0.7098\n",
      "Epoch 268/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 47.5695 - val_loss: 0.6872\n",
      "Epoch 269/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 46.2818 - val_loss: 0.6889\n",
      "Epoch 270/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 45.0331 - val_loss: 0.7137\n",
      "Epoch 271/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 43.8221 - val_loss: 0.7611\n",
      "Epoch 272/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 42.6482 - val_loss: 0.8300\n",
      "Epoch 273/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 41.5105 - val_loss: 0.9198\n",
      "Epoch 274/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 40.4081 - val_loss: 1.0296\n",
      "Epoch 275/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 39.3403 - val_loss: 1.1586\n",
      "Epoch 276/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 38.3063 - val_loss: 1.3061\n",
      "Epoch 277/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 37.3052 - val_loss: 1.4713\n",
      "Epoch 278/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 36.3363 - val_loss: 1.6534\n",
      "Epoch 279/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 35.3987 - val_loss: 1.8518\n",
      "Epoch 280/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 34.4918 - val_loss: 2.0656\n",
      "Epoch 281/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 33.6148 - val_loss: 2.2943\n",
      "Epoch 282/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 32.7669 - val_loss: 2.5369\n",
      "Epoch 283/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 31.9475 - val_loss: 2.7930\n",
      "Epoch 284/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 31.1557 - val_loss: 3.0618\n",
      "Epoch 285/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 30.3910 - val_loss: 3.3426\n",
      "Epoch 286/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 29.6524 - val_loss: 3.6349\n",
      "Epoch 287/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 28.9394 - val_loss: 3.9379\n",
      "Epoch 288/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 28.2513 - val_loss: 4.2509\n",
      "Epoch 289/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 27.5875 - val_loss: 4.5737\n",
      "Epoch 290/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.9471 - val_loss: 4.9052\n",
      "Epoch 291/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.3297 - val_loss: 5.2451\n",
      "Epoch 292/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 25.7346 - val_loss: 5.5927\n",
      "Epoch 293/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 25.1612 - val_loss: 5.9476\n",
      "Epoch 294/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 24.6088 - val_loss: 6.3091\n",
      "Epoch 295/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 24.0769 - val_loss: 6.6768\n",
      "Epoch 296/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 23.5647 - val_loss: 7.0501\n",
      "Epoch 297/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 23.0718 - val_loss: 7.4284\n",
      "Epoch 298/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 22.5976 - val_loss: 7.8114\n",
      "Epoch 299/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 22.1415 - val_loss: 8.1985\n",
      "Epoch 300/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 21.7030 - val_loss: 8.5892\n",
      "Epoch 301/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 21.2815 - val_loss: 8.9832\n",
      "Epoch 302/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 20.8764 - val_loss: 9.3798\n",
      "Epoch 303/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.4874 - val_loss: 9.7789\n",
      "Epoch 304/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 20.1138 - val_loss: 10.1798\n",
      "Epoch 305/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 19.7553 - val_loss: 10.5823\n",
      "Epoch 306/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 19.4112 - val_loss: 10.9858\n",
      "Epoch 307/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 19.0811 - val_loss: 11.3901\n",
      "Epoch 308/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 18.7646 - val_loss: 11.7949\n",
      "Epoch 309/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 18.4612 - val_loss: 12.1995\n",
      "Epoch 310/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 18.1705 - val_loss: 12.6039\n",
      "Epoch 311/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 17.8919 - val_loss: 13.0075\n",
      "Epoch 312/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 17.6252 - val_loss: 13.4103\n",
      "Epoch 313/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.3698 - val_loss: 13.8120\n",
      "Epoch 314/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.1254 - val_loss: 14.2118\n",
      "Epoch 315/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 16.8917 - val_loss: 14.6099\n",
      "Epoch 316/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 16.6682 - val_loss: 15.0059\n",
      "Epoch 317/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 16.4545 - val_loss: 15.3995\n",
      "Epoch 318/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 16.2502 - val_loss: 15.7907\n",
      "Epoch 319/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 16.0551 - val_loss: 16.1788\n",
      "Epoch 320/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 15.8688 - val_loss: 16.5640\n",
      "Epoch 321/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.6909 - val_loss: 16.9459\n",
      "Epoch 322/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.5212 - val_loss: 17.3244\n",
      "Epoch 323/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.3593 - val_loss: 17.6991\n",
      "Epoch 324/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.2049 - val_loss: 18.0700\n",
      "Epoch 325/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0577 - val_loss: 18.4368\n",
      "Epoch 326/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 14.9175 - val_loss: 18.7996\n",
      "Epoch 327/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 14.7838 - val_loss: 19.1579\n",
      "Epoch 328/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 14.6566 - val_loss: 19.5120\n",
      "Epoch 329/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 14.5355 - val_loss: 19.8612\n",
      "Epoch 330/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 14.4203 - val_loss: 20.2059\n",
      "Epoch 331/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 14.3106 - val_loss: 20.5456\n",
      "Epoch 332/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 14.2064 - val_loss: 20.8806\n",
      "Epoch 333/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 14.1073 - val_loss: 21.2104\n",
      "Epoch 334/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 14.0131 - val_loss: 21.5353\n",
      "Epoch 335/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 13.9237 - val_loss: 21.8551\n",
      "Epoch 336/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 13.8387 - val_loss: 22.1691\n",
      "Epoch 337/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.7581 - val_loss: 22.4782\n",
      "Epoch 338/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 13.6816 - val_loss: 22.7820\n",
      "Epoch 339/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 13.6090 - val_loss: 23.0801\n",
      "Epoch 340/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.5402 - val_loss: 23.3732\n",
      "Epoch 341/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 13.4750 - val_loss: 23.6606\n",
      "Epoch 342/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.4131 - val_loss: 23.9423\n",
      "Epoch 343/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 13.3546 - val_loss: 24.2187\n",
      "Epoch 344/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.2991 - val_loss: 24.4896\n",
      "Epoch 345/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.2466 - val_loss: 24.7550\n",
      "Epoch 346/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.1969 - val_loss: 25.0147\n",
      "Epoch 347/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 13.1499 - val_loss: 25.2689\n",
      "Epoch 348/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.1055 - val_loss: 25.5176\n",
      "Epoch 349/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.0634 - val_loss: 25.7608\n",
      "Epoch 350/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 13.0237 - val_loss: 25.9987\n",
      "Epoch 351/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.9861 - val_loss: 26.2310\n",
      "Epoch 352/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.9507 - val_loss: 26.4577\n",
      "Epoch 353/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.9172 - val_loss: 26.6791\n",
      "Epoch 354/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.8856 - val_loss: 26.8952\n",
      "Epoch 355/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 12.8558 - val_loss: 27.1061\n",
      "Epoch 356/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.8276 - val_loss: 27.3116\n",
      "Epoch 357/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.8011 - val_loss: 27.5117\n",
      "Epoch 358/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.7760 - val_loss: 27.7068\n",
      "Epoch 359/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.7524 - val_loss: 27.8969\n",
      "Epoch 360/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.7302 - val_loss: 28.0820\n",
      "Epoch 361/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.7092 - val_loss: 28.2618\n",
      "Epoch 362/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.6895 - val_loss: 28.4371\n",
      "Epoch 363/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.6709 - val_loss: 28.6071\n",
      "Epoch 364/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.6534 - val_loss: 28.7726\n",
      "Epoch 365/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.6369 - val_loss: 28.9332\n",
      "Epoch 366/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.6214 - val_loss: 29.0893\n",
      "Epoch 367/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.6068 - val_loss: 29.2409\n",
      "Epoch 368/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.5931 - val_loss: 29.3878\n",
      "Epoch 369/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.5802 - val_loss: 29.5306\n",
      "Epoch 370/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.5680 - val_loss: 29.6688\n",
      "Epoch 371/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.5566 - val_loss: 29.8027\n",
      "Epoch 372/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.5458 - val_loss: 29.9325\n",
      "Epoch 373/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.5358 - val_loss: 30.0583\n",
      "Epoch 374/500\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 12.5263 - val_loss: 30.1801\n",
      "Epoch 375/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.5174 - val_loss: 30.2978\n",
      "Epoch 376/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.5090 - val_loss: 30.4120\n",
      "Epoch 377/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.5011 - val_loss: 30.5220\n",
      "Epoch 378/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.4937 - val_loss: 30.6287\n",
      "Epoch 379/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.4868 - val_loss: 30.7314\n",
      "Epoch 380/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.4803 - val_loss: 30.8306\n",
      "Epoch 381/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.4742 - val_loss: 30.9266\n",
      "Epoch 382/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.4685 - val_loss: 31.0190\n",
      "Epoch 383/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.4631 - val_loss: 31.1083\n",
      "Epoch 384/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.4581 - val_loss: 31.1947\n",
      "Epoch 385/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.4533 - val_loss: 31.2776\n",
      "Epoch 386/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.4489 - val_loss: 31.3577\n",
      "Epoch 387/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.4447 - val_loss: 31.4346\n",
      "Epoch 388/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.4408 - val_loss: 31.5089\n",
      "Epoch 389/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.4371 - val_loss: 31.5804\n",
      "Epoch 390/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.4337 - val_loss: 31.6489\n",
      "Epoch 391/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.4304 - val_loss: 31.7149\n",
      "Epoch 392/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.4274 - val_loss: 31.7786\n",
      "Epoch 393/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 12.4246 - val_loss: 31.8393\n",
      "Epoch 394/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.4219 - val_loss: 31.8980\n",
      "Epoch 395/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.4195 - val_loss: 31.9545\n",
      "Epoch 396/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.4171 - val_loss: 32.0084\n",
      "Epoch 397/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.4150 - val_loss: 32.0604\n",
      "Epoch 398/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.4129 - val_loss: 32.1100\n",
      "Epoch 399/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.4110 - val_loss: 32.1575\n",
      "Epoch 400/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.4093 - val_loss: 32.2032\n",
      "Epoch 401/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.4076 - val_loss: 32.2469\n",
      "Epoch 402/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.4060 - val_loss: 32.2889\n",
      "Epoch 403/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.4045 - val_loss: 32.3290\n",
      "Epoch 404/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.4032 - val_loss: 32.3671\n",
      "Epoch 405/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.4019 - val_loss: 32.4038\n",
      "Epoch 406/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.4007 - val_loss: 32.4390\n",
      "Epoch 407/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3996 - val_loss: 32.4723\n",
      "Epoch 408/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3986 - val_loss: 32.5043\n",
      "Epoch 409/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3976 - val_loss: 32.5349\n",
      "Epoch 410/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3967 - val_loss: 32.5639\n",
      "Epoch 411/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3959 - val_loss: 32.5917\n",
      "Epoch 412/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3952 - val_loss: 32.6185\n",
      "Epoch 413/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 12.3944 - val_loss: 32.6434\n",
      "Epoch 414/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3938 - val_loss: 32.6676\n",
      "Epoch 415/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3931 - val_loss: 32.6905\n",
      "Epoch 416/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3926 - val_loss: 32.7124\n",
      "Epoch 417/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.3921 - val_loss: 32.7333\n",
      "Epoch 418/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3916 - val_loss: 32.7528\n",
      "Epoch 419/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3912 - val_loss: 32.7717\n",
      "Epoch 420/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3908 - val_loss: 32.7893\n",
      "Epoch 421/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3905 - val_loss: 32.8065\n",
      "Epoch 422/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3901 - val_loss: 32.8228\n",
      "Epoch 423/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3898 - val_loss: 32.8378\n",
      "Epoch 424/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3896 - val_loss: 32.8524\n",
      "Epoch 425/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3893 - val_loss: 32.8661\n",
      "Epoch 426/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3891 - val_loss: 32.8787\n",
      "Epoch 427/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3889 - val_loss: 32.8911\n",
      "Epoch 428/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3888 - val_loss: 32.9029\n",
      "Epoch 429/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 12.3886 - val_loss: 32.9138\n",
      "Epoch 430/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3885 - val_loss: 32.9246\n",
      "Epoch 431/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3884 - val_loss: 32.9343\n",
      "Epoch 432/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3883 - val_loss: 32.9436\n",
      "Epoch 433/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3882 - val_loss: 32.9520\n",
      "Epoch 434/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3882 - val_loss: 32.9604\n",
      "Epoch 435/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3882 - val_loss: 32.9683\n",
      "Epoch 436/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3882 - val_loss: 32.9760\n",
      "Epoch 437/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3882 - val_loss: 32.9829\n",
      "Epoch 438/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3882 - val_loss: 32.9892\n",
      "Epoch 439/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3883 - val_loss: 32.9954\n",
      "Epoch 440/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3883 - val_loss: 33.0010\n",
      "Epoch 441/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3883 - val_loss: 33.0063\n",
      "Epoch 442/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.3884 - val_loss: 33.0116\n",
      "Epoch 443/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3885 - val_loss: 33.0164\n",
      "Epoch 444/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.3885 - val_loss: 33.0209\n",
      "Epoch 445/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3886 - val_loss: 33.0251\n",
      "Epoch 446/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3887 - val_loss: 33.0290\n",
      "Epoch 447/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3888 - val_loss: 33.0325\n",
      "Epoch 448/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.3890 - val_loss: 33.0361\n",
      "Epoch 449/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 12.3891 - val_loss: 33.0390\n",
      "Epoch 450/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3892 - val_loss: 33.0419\n",
      "Epoch 451/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3894 - val_loss: 33.0448\n",
      "Epoch 452/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3895 - val_loss: 33.0471\n",
      "Epoch 453/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.3897 - val_loss: 33.0496\n",
      "Epoch 454/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3898 - val_loss: 33.0519\n",
      "Epoch 455/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3900 - val_loss: 33.0539\n",
      "Epoch 456/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.3901 - val_loss: 33.0555\n",
      "Epoch 457/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3903 - val_loss: 33.0573\n",
      "Epoch 458/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3905 - val_loss: 33.0590\n",
      "Epoch 459/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3906 - val_loss: 33.0604\n",
      "Epoch 460/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3908 - val_loss: 33.0619\n",
      "Epoch 461/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3910 - val_loss: 33.0631\n",
      "Epoch 462/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3912 - val_loss: 33.0640\n",
      "Epoch 463/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3914 - val_loss: 33.0650\n",
      "Epoch 464/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3916 - val_loss: 33.0660\n",
      "Epoch 465/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3918 - val_loss: 33.0666\n",
      "Epoch 466/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 12.3920 - val_loss: 33.0677\n",
      "Epoch 467/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3922 - val_loss: 33.0685\n",
      "Epoch 468/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3924 - val_loss: 33.0689\n",
      "Epoch 469/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3926 - val_loss: 33.0693\n",
      "Epoch 470/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3928 - val_loss: 33.0701\n",
      "Epoch 471/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3930 - val_loss: 33.0703\n",
      "Epoch 472/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3932 - val_loss: 33.0706\n",
      "Epoch 473/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3934 - val_loss: 33.0711\n",
      "Epoch 474/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3936 - val_loss: 33.0715\n",
      "Epoch 475/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3938 - val_loss: 33.0716\n",
      "Epoch 476/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3940 - val_loss: 33.0717\n",
      "Epoch 477/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3942 - val_loss: 33.0718\n",
      "Epoch 478/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3945 - val_loss: 33.0718\n",
      "Epoch 479/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.3947 - val_loss: 33.0721\n",
      "Epoch 480/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3949 - val_loss: 33.0720\n",
      "Epoch 481/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3951 - val_loss: 33.0721\n",
      "Epoch 482/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.3953 - val_loss: 33.0721\n",
      "Epoch 483/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3955 - val_loss: 33.0720\n",
      "Epoch 484/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3958 - val_loss: 33.0720\n",
      "Epoch 485/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 12.3959 - val_loss: 33.0719\n",
      "Epoch 486/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3961 - val_loss: 33.0717\n",
      "Epoch 487/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3964 - val_loss: 33.0716\n",
      "Epoch 488/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3966 - val_loss: 33.0715\n",
      "Epoch 489/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3968 - val_loss: 33.0715\n",
      "Epoch 490/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3970 - val_loss: 33.0713\n",
      "Epoch 491/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3972 - val_loss: 33.0710\n",
      "Epoch 492/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3974 - val_loss: 33.0707\n",
      "Epoch 493/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3976 - val_loss: 33.0704\n",
      "Epoch 494/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3978 - val_loss: 33.0703\n",
      "Epoch 495/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3981 - val_loss: 33.0702\n",
      "Epoch 496/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3982 - val_loss: 33.0698\n",
      "Epoch 497/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3985 - val_loss: 33.0695\n",
      "Epoch 498/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3987 - val_loss: 33.0692\n",
      "Epoch 499/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.3989 - val_loss: 33.0689\n",
      "Epoch 500/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3991 - val_loss: 33.0687\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.1552, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0026, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0004, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0102, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.8734, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 328ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[70.64278711, 70.63438375, 70.62598039, 70.61757703, 70.60917367,\n",
       "        70.60077031, 70.58218954, 70.5625817 , 70.54297386, 70.52336601,\n",
       "        70.50375817, 70.48415033, 70.46454248, 70.44493464, 70.4253268 ,\n",
       "        70.40571895, 70.38611111, 70.36650327, 70.34689542, 70.32728758,\n",
       "        70.30767974, 70.2880719 , 70.26846405, 70.24885621, 70.22924837,\n",
       "        70.20964052, 70.19003268, 70.17042484, 70.15081699, 70.13120915,\n",
       "        70.11160131, 70.09199346, 70.07238562, 70.05277778, 70.03316993,\n",
       "        70.01356209, 69.99395425, 69.97434641, 69.95473856, 69.93513072,\n",
       "        69.91552288, 69.89066293, 69.845845  , 69.80102708, 69.75620915,\n",
       "        69.71139122, 69.6665733 , 69.62175537, 69.57693744, 69.53211951,\n",
       "        69.48730159, 69.44248366, 69.39766573, 69.35284781, 69.30802988,\n",
       "        69.26321195, 69.21839402, 69.1735761 , 69.12875817, 69.08394024,\n",
       "        69.03912232, 68.99430439, 68.94948646, 68.90466853, 68.85985061,\n",
       "        68.81503268, 68.77021475, 68.72539683, 68.6805789 , 68.63576097,\n",
       "        68.59094304, 68.54612512, 68.50130719, 68.45648926, 68.41167134,\n",
       "        68.36685341, 68.32203548, 68.29288049, 68.27887488, 68.26486928,\n",
       "        75.39174652,  0.        ,  0.07724502,  0.        ,  0.        ,\n",
       "         0.23195714,  0.        ,  0.08291335,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.13620821,  0.        ,  0.29906166,\n",
       "         0.        ,  0.07937889,  0.        ,  0.54589337,  0.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([66.84365079, 66.83431373, 66.82497666, 66.81563959, 66.80630252,\n",
       "       66.79757236, 66.79010271, 66.78263305, 66.7751634 , 66.76769374,\n",
       "       66.76022409, 66.75275444, 66.74528478, 66.73781513, 66.73034547,\n",
       "       66.72287582, 66.71540616, 66.70793651, 66.70046685, 66.6929972 ,\n",
       "       66.68552754, 66.67805789, 66.67058824, 66.66311858, 66.65564893,\n",
       "       66.64817927, 66.64070962, 66.63323996, 66.62577031, 66.61830065,\n",
       "       66.610831  , 66.60336134, 66.59589169, 66.58842204, 66.58095238,\n",
       "       66.57348273, 66.56601307, 66.55854342, 66.55107376, 66.54360411,\n",
       "       66.53613445, 66.5286648 , 66.52119514, 66.51372549, 66.50625584,\n",
       "       66.49878618, 66.49131653, 66.48384687, 66.47637722, 66.46890756,\n",
       "       66.46143791, 66.45396825, 66.4464986 , 66.43902894, 66.43155929,\n",
       "       66.42408964, 66.41661998, 66.40915033, 66.40168067, 66.39421102,\n",
       "       66.38674136, 66.37927171, 66.37180205, 66.3643324 , 66.35686275,\n",
       "       66.34939309, 66.34192344, 66.33445378, 66.32698413, 66.31951447,\n",
       "       66.31204482, 66.30457516, 66.29710551, 66.28963585, 66.2821662 ,\n",
       "       66.27469655, 66.26722689, 66.25975724, 66.25228758, 66.24481793,\n",
       "       66.23734827, 66.22987862, 66.22240896, 66.21493931, 66.20746965,\n",
       "       66.2       , 66.19253035, 66.18506069, 66.17759104, 66.17012138,\n",
       "       66.16265173, 66.15518207, 66.14771242, 66.14024276, 66.13277311,\n",
       "       66.12530345, 66.1178338 , 66.11036415, 66.10289449, 66.09542484])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.962673500290848\n",
      "15.166834399484662\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
