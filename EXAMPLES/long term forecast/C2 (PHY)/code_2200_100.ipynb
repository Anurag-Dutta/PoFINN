{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "2295    55.277700\n",
       "2296    55.262123\n",
       "2297    55.246546\n",
       "2298    55.230969\n",
       "2299    55.215393\n",
       "Name: C2, Length: 2300, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c2_interpolated_2200_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "2195     0.569434\n",
       "2196     0.344123\n",
       "2197     0.732501\n",
       "2198     0.352534\n",
       "2199     0.273458\n",
       "Name: C2, Length: 2200, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2200)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.500000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.275910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.051821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.827731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.603641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     90.500000  0.000298\n",
       "1     90.275910  0.000298\n",
       "2     90.051821  0.000297\n",
       "3     89.827731  0.000297\n",
       "4     89.603641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.1552  # Value for C0\n",
    "K0 = -0.0026  # Value for K0\n",
    "K1 = -0.0004  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0102    # Value for b\n",
    "c = 2.8734    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    90.500000    90.275910    90.051821    89.827731    89.603641   \n",
      "351    90.275910    90.051821    89.827731    89.603641    89.379552   \n",
      "352    90.051821    89.827731    89.603641    89.379552    89.155462   \n",
      "353    89.827731    89.603641    89.379552    89.155462    88.931373   \n",
      "354    89.603641    89.379552    89.155462    88.931373    88.707283   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    89.379552    89.155462    88.931373    88.707283    88.494958  ...   \n",
      "351    89.155462    88.931373    88.707283    88.494958    88.427731  ...   \n",
      "352    88.931373    88.707283    88.494958    88.427731    88.360504  ...   \n",
      "353    88.707283    88.494958    88.427731    88.360504    88.293277  ...   \n",
      "354    88.494958    88.427731    88.360504    88.293277    88.226050  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   79.071008    0.000263   79.054202    0.000263   79.037395    0.000263   \n",
      "351   79.054202    0.000263   79.037395    0.000263   79.020588    0.000262   \n",
      "352   79.037395    0.000263   79.020588    0.000262   79.003782    0.000262   \n",
      "353   79.020588    0.000262   79.003782    0.000262   78.986975    0.000262   \n",
      "354   79.003782    0.000262   78.986975    0.000262   78.970168    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   79.020588    0.000262   79.003782    0.000262  \n",
      "351   79.003782    0.000262   78.986975    0.000262  \n",
      "352   78.986975    0.000262   78.970168    0.000262  \n",
      "353   78.970168    0.000262   78.953361    0.000262  \n",
      "354   78.953361    0.000262   78.936555    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1750, 1, 251) (1750, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 2s 24ms/step - loss: 5223.1133 - val_loss: 3292.2649\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5067.7812 - val_loss: 3167.2964\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4972.4121 - val_loss: 3111.8042\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4891.5225 - val_loss: 3053.4087\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 4807.8252 - val_loss: 3003.5764\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4732.9517 - val_loss: 2955.3752\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 4660.0571 - val_loss: 2908.4714\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4588.7061 - val_loss: 2862.6377\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4518.6323 - val_loss: 2817.7463\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4449.6855 - val_loss: 2773.7178\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4381.7695 - val_loss: 2730.4985\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4314.8198 - val_loss: 2688.0515\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4248.7871 - val_loss: 2646.3467\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4183.6338 - val_loss: 2605.3625\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4119.3330 - val_loss: 2565.0835\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4055.8596 - val_loss: 2525.5745\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3987.6514 - val_loss: 2487.7322\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 3914.6450 - val_loss: 2435.4507\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3847.7263 - val_loss: 2394.6282\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3782.6790 - val_loss: 2355.0603\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3719.1567 - val_loss: 2316.5271\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3656.8958 - val_loss: 2278.9014\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3595.7410 - val_loss: 2242.1060\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3535.5950 - val_loss: 2206.0896\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3476.3953 - val_loss: 2170.8147\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3418.0918 - val_loss: 2136.2527\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3360.6482 - val_loss: 2102.3801\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3304.0356 - val_loss: 2069.1792\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3248.2300 - val_loss: 2036.6323\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3193.2092 - val_loss: 2004.7257\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3138.9570 - val_loss: 1973.4467\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3085.4568 - val_loss: 1942.7834\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3032.6943 - val_loss: 1912.7256\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2980.6572 - val_loss: 1883.2632\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2929.3330 - val_loss: 1854.3873\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2878.7112 - val_loss: 1826.0890\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2828.7815 - val_loss: 1798.3596\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2779.5334 - val_loss: 1771.1929\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2730.9590 - val_loss: 1744.5796\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2683.0493 - val_loss: 1718.5132\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2635.7949 - val_loss: 1692.9865\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2589.1892 - val_loss: 1667.9927\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2543.2244 - val_loss: 1643.5259\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2497.8921 - val_loss: 1619.5786\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2453.1863 - val_loss: 1596.1455\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2409.0991 - val_loss: 1573.2195\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2365.6240 - val_loss: 1550.7955\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2322.7549 - val_loss: 1528.8669\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2280.4849 - val_loss: 1507.4282\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2238.8076 - val_loss: 1486.4740\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2197.7166 - val_loss: 1465.9982\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2157.2065 - val_loss: 1445.9955\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2117.2710 - val_loss: 1426.4606\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2077.9041 - val_loss: 1407.3878\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2039.1002 - val_loss: 1388.7716\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2000.8541 - val_loss: 1370.6074\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1963.1597 - val_loss: 1352.8893\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1926.0109 - val_loss: 1335.6128\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1889.4034 - val_loss: 1318.7731\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1853.3314 - val_loss: 1302.3635\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1817.7894 - val_loss: 1286.3807\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1782.7727 - val_loss: 1270.8191\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 1748.2755 - val_loss: 1255.6737\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1714.2933 - val_loss: 1240.9399\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1680.8207 - val_loss: 1226.6128\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1647.8525 - val_loss: 1212.6875\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1615.3843 - val_loss: 1199.1594\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1583.4109 - val_loss: 1186.0236\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1551.9274 - val_loss: 1173.2753\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1520.9287 - val_loss: 1160.9100\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1490.4105 - val_loss: 1148.9231\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1460.3680 - val_loss: 1137.3101\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1430.7966 - val_loss: 1126.0663\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1401.6910 - val_loss: 1115.1868\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1373.0477 - val_loss: 1104.6678\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1344.8610 - val_loss: 1094.5040\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1317.1268 - val_loss: 1084.6914\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1289.8407 - val_loss: 1075.2256\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1262.9977 - val_loss: 1066.1016\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1236.5942 - val_loss: 1057.3153\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1210.6252 - val_loss: 1048.8625\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1185.0864 - val_loss: 1040.7385\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1159.9733 - val_loss: 1032.9392\n",
      "Epoch 84/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1135.2819 - val_loss: 1025.4600\n",
      "Epoch 85/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1111.0076 - val_loss: 1018.2966\n",
      "Epoch 86/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1087.1458 - val_loss: 1011.4446\n",
      "Epoch 87/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1063.6927 - val_loss: 1004.9000\n",
      "Epoch 88/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1040.6439 - val_loss: 998.6581\n",
      "Epoch 89/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1017.9955 - val_loss: 992.7150\n",
      "Epoch 90/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 995.7426 - val_loss: 987.0662\n",
      "Epoch 91/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 973.8816 - val_loss: 981.7073\n",
      "Epoch 92/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 952.4081 - val_loss: 976.6343\n",
      "Epoch 93/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 931.3179 - val_loss: 971.8431\n",
      "Epoch 94/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 910.6072 - val_loss: 967.3290\n",
      "Epoch 95/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 890.2715 - val_loss: 963.0882\n",
      "Epoch 96/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 870.3069 - val_loss: 959.1165\n",
      "Epoch 97/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 850.7094 - val_loss: 955.4093\n",
      "Epoch 98/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 831.4745 - val_loss: 951.9627\n",
      "Epoch 99/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 812.5986 - val_loss: 948.7727\n",
      "Epoch 100/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 794.0776 - val_loss: 945.8347\n",
      "Epoch 101/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 775.9073 - val_loss: 943.1448\n",
      "Epoch 102/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 758.0837 - val_loss: 940.6989\n",
      "Epoch 103/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 740.6031 - val_loss: 938.4928\n",
      "Epoch 104/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 723.4615 - val_loss: 936.5224\n",
      "Epoch 105/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 706.6547 - val_loss: 934.7834\n",
      "Epoch 106/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 690.1790 - val_loss: 933.2720\n",
      "Epoch 107/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 674.0303 - val_loss: 931.9838\n",
      "Epoch 108/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 658.2050 - val_loss: 930.9150\n",
      "Epoch 109/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 642.6987 - val_loss: 930.0613\n",
      "Epoch 110/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 627.5078 - val_loss: 929.4184\n",
      "Epoch 111/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 612.6288 - val_loss: 928.9828\n",
      "Epoch 112/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 598.0573 - val_loss: 928.7500\n",
      "Epoch 113/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 583.7897 - val_loss: 928.7162\n",
      "Epoch 114/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 569.8220 - val_loss: 928.8771\n",
      "Epoch 115/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 556.1505 - val_loss: 929.2288\n",
      "Epoch 116/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 542.7712 - val_loss: 929.7672\n",
      "Epoch 117/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 529.6805 - val_loss: 930.4882\n",
      "Epoch 118/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 516.8746 - val_loss: 931.3881\n",
      "Epoch 119/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 504.3499 - val_loss: 932.4626\n",
      "Epoch 120/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 492.1024 - val_loss: 933.7077\n",
      "Epoch 121/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 480.1284 - val_loss: 935.1196\n",
      "Epoch 122/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 468.4244 - val_loss: 936.6942\n",
      "Epoch 123/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 456.9862 - val_loss: 938.4275\n",
      "Epoch 124/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 445.8103 - val_loss: 940.3156\n",
      "Epoch 125/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 434.8931 - val_loss: 942.3545\n",
      "Epoch 126/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 424.2309 - val_loss: 944.5403\n",
      "Epoch 127/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 413.8198 - val_loss: 946.8691\n",
      "Epoch 128/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 403.6566 - val_loss: 949.3368\n",
      "Epoch 129/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 393.7372 - val_loss: 951.9395\n",
      "Epoch 130/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 384.0581 - val_loss: 954.6735\n",
      "Epoch 131/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 374.6158 - val_loss: 957.5350\n",
      "Epoch 132/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 365.4063 - val_loss: 960.5197\n",
      "Epoch 133/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 356.4265 - val_loss: 963.6239\n",
      "Epoch 134/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 347.6725 - val_loss: 966.8440\n",
      "Epoch 135/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 339.1405 - val_loss: 970.1759\n",
      "Epoch 136/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 330.8276 - val_loss: 973.6161\n",
      "Epoch 137/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 322.7296 - val_loss: 977.1603\n",
      "Epoch 138/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 314.8433 - val_loss: 980.8052\n",
      "Epoch 139/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 307.1651 - val_loss: 984.5466\n",
      "Epoch 140/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 299.6916 - val_loss: 988.3810\n",
      "Epoch 141/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 292.4193 - val_loss: 992.3044\n",
      "Epoch 142/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 285.3445 - val_loss: 996.3137\n",
      "Epoch 143/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 278.4637 - val_loss: 1000.4049\n",
      "Epoch 144/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 271.7736 - val_loss: 1004.5742\n",
      "Epoch 145/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 265.2707 - val_loss: 1008.8179\n",
      "Epoch 146/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 258.9518 - val_loss: 1013.1325\n",
      "Epoch 147/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 252.8134 - val_loss: 1017.5146\n",
      "Epoch 148/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 246.8519 - val_loss: 1021.9601\n",
      "Epoch 149/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 241.0643 - val_loss: 1026.4659\n",
      "Epoch 150/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 235.4468 - val_loss: 1031.0286\n",
      "Epoch 151/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 229.9964 - val_loss: 1035.6442\n",
      "Epoch 152/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 224.7096 - val_loss: 1040.3093\n",
      "Epoch 153/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 219.5835 - val_loss: 1045.0211\n",
      "Epoch 154/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 214.6144 - val_loss: 1049.7755\n",
      "Epoch 155/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 209.7994 - val_loss: 1054.5691\n",
      "Epoch 156/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 205.1351 - val_loss: 1059.3990\n",
      "Epoch 157/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 200.6184 - val_loss: 1064.2620\n",
      "Epoch 158/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 196.2460 - val_loss: 1069.1542\n",
      "Epoch 159/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 192.0148 - val_loss: 1074.0734\n",
      "Epoch 160/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 187.9215 - val_loss: 1079.0151\n",
      "Epoch 161/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 183.9634 - val_loss: 1083.9769\n",
      "Epoch 162/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 180.1371 - val_loss: 1088.9556\n",
      "Epoch 163/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 176.4398 - val_loss: 1093.9480\n",
      "Epoch 164/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 172.8682 - val_loss: 1098.9514\n",
      "Epoch 165/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 169.4196 - val_loss: 1103.9622\n",
      "Epoch 166/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 166.0908 - val_loss: 1108.9781\n",
      "Epoch 167/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 162.8790 - val_loss: 1113.9957\n",
      "Epoch 168/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 159.7814 - val_loss: 1119.0127\n",
      "Epoch 169/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 156.7947 - val_loss: 1124.0255\n",
      "Epoch 170/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 153.9165 - val_loss: 1129.0327\n",
      "Epoch 171/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 151.1437 - val_loss: 1134.0302\n",
      "Epoch 172/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 148.4738 - val_loss: 1139.0161\n",
      "Epoch 173/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 145.9039 - val_loss: 1143.9874\n",
      "Epoch 174/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 143.4313 - val_loss: 1148.9420\n",
      "Epoch 175/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 141.0533 - val_loss: 1153.8766\n",
      "Epoch 176/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 138.7674 - val_loss: 1158.7903\n",
      "Epoch 177/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 136.5708 - val_loss: 1163.6793\n",
      "Epoch 178/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 134.4610 - val_loss: 1168.5417\n",
      "Epoch 179/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 132.4355 - val_loss: 1173.3759\n",
      "Epoch 180/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 130.4917 - val_loss: 1178.1787\n",
      "Epoch 181/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 128.6274 - val_loss: 1182.9484\n",
      "Epoch 182/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 126.8400 - val_loss: 1187.6829\n",
      "Epoch 183/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 125.1271 - val_loss: 1192.3805\n",
      "Epoch 184/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 123.4865 - val_loss: 1197.0391\n",
      "Epoch 185/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 121.9157 - val_loss: 1201.6570\n",
      "Epoch 186/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 120.4125 - val_loss: 1206.2317\n",
      "Epoch 187/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 118.9748 - val_loss: 1210.7625\n",
      "Epoch 188/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 117.6003 - val_loss: 1215.2468\n",
      "Epoch 189/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 116.2871 - val_loss: 1219.6832\n",
      "Epoch 190/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 115.0328 - val_loss: 1224.0707\n",
      "Epoch 191/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 113.8354 - val_loss: 1228.4073\n",
      "Epoch 192/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 112.6931 - val_loss: 1232.6920\n",
      "Epoch 193/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 111.6037 - val_loss: 1236.9237\n",
      "Epoch 194/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 110.5654 - val_loss: 1241.1000\n",
      "Epoch 195/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 109.5764 - val_loss: 1245.2202\n",
      "Epoch 196/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 108.6348 - val_loss: 1249.2848\n",
      "Epoch 197/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 107.7385 - val_loss: 1253.2908\n",
      "Epoch 198/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 106.8861 - val_loss: 1257.2380\n",
      "Epoch 199/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 106.0759 - val_loss: 1261.1251\n",
      "Epoch 200/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 105.3061 - val_loss: 1264.9517\n",
      "Epoch 201/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 104.5751 - val_loss: 1268.7166\n",
      "Epoch 202/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 103.8813 - val_loss: 1272.4193\n",
      "Epoch 203/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 103.2234 - val_loss: 1276.0590\n",
      "Epoch 204/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 102.5997 - val_loss: 1279.6353\n",
      "Epoch 205/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 102.0087 - val_loss: 1283.1479\n",
      "Epoch 206/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 101.4491 - val_loss: 1286.5966\n",
      "Epoch 207/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 100.9195 - val_loss: 1289.9806\n",
      "Epoch 208/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 100.4186 - val_loss: 1293.2991\n",
      "Epoch 209/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 99.9451 - val_loss: 1296.5530\n",
      "Epoch 210/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 99.4977 - val_loss: 1299.7412\n",
      "Epoch 211/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 99.0753 - val_loss: 1302.8638\n",
      "Epoch 212/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 98.6767 - val_loss: 1305.9210\n",
      "Epoch 213/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 98.3007 - val_loss: 1308.9135\n",
      "Epoch 214/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 97.9464 - val_loss: 1311.8396\n",
      "Epoch 215/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 97.6126 - val_loss: 1314.7003\n",
      "Epoch 216/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 97.2983 - val_loss: 1317.4963\n",
      "Epoch 217/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 97.0027 - val_loss: 1320.2269\n",
      "Epoch 218/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 96.7246 - val_loss: 1322.8932\n",
      "Epoch 219/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 96.4633 - val_loss: 1325.4955\n",
      "Epoch 220/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 96.2178 - val_loss: 1328.0337\n",
      "Epoch 221/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 95.9874 - val_loss: 1330.5087\n",
      "Epoch 222/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 95.7712 - val_loss: 1332.9203\n",
      "Epoch 223/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 95.5686 - val_loss: 1335.2697\n",
      "Epoch 224/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 95.3786 - val_loss: 1337.5571\n",
      "Epoch 225/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 95.2008 - val_loss: 1339.7832\n",
      "Epoch 226/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 95.0344 - val_loss: 1341.9489\n",
      "Epoch 227/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 94.8786 - val_loss: 1344.0543\n",
      "Epoch 228/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 94.7331 - val_loss: 1346.1007\n",
      "Epoch 229/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 94.5971 - val_loss: 1348.0886\n",
      "Epoch 230/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 94.4701 - val_loss: 1350.0186\n",
      "Epoch 231/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 94.3516 - val_loss: 1351.8922\n",
      "Epoch 232/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 94.2410 - val_loss: 1353.7100\n",
      "Epoch 233/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 94.1379 - val_loss: 1355.4722\n",
      "Epoch 234/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 94.0419 - val_loss: 1357.1797\n",
      "Epoch 235/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 93.9525 - val_loss: 1358.8335\n",
      "Epoch 236/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 93.8694 - val_loss: 1360.4353\n",
      "Epoch 237/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 93.7919 - val_loss: 1361.9854\n",
      "Epoch 238/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 93.7200 - val_loss: 1363.4858\n",
      "Epoch 239/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 93.6531 - val_loss: 1364.9366\n",
      "Epoch 240/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 93.5911 - val_loss: 1366.3389\n",
      "Epoch 241/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 93.5334 - val_loss: 1367.6931\n",
      "Epoch 242/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 93.4799 - val_loss: 1369.0006\n",
      "Epoch 243/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 93.4304 - val_loss: 1370.2627\n",
      "Epoch 244/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 93.3845 - val_loss: 1371.4808\n",
      "Epoch 245/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 93.3419 - val_loss: 1372.6558\n",
      "Epoch 246/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 93.3025 - val_loss: 1373.7874\n",
      "Epoch 247/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 93.2660 - val_loss: 1374.8778\n",
      "Epoch 248/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 93.2322 - val_loss: 1375.9288\n",
      "Epoch 249/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 93.2010 - val_loss: 1376.9387\n",
      "Epoch 250/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 93.1723 - val_loss: 1377.9117\n",
      "Epoch 251/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 93.1456 - val_loss: 1378.8473\n",
      "Epoch 252/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 93.1210 - val_loss: 1379.7458\n",
      "Epoch 253/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 93.0983 - val_loss: 1380.6096\n",
      "Epoch 254/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 93.0773 - val_loss: 1381.4395\n",
      "Epoch 255/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 93.0580 - val_loss: 1382.2355\n",
      "Epoch 256/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 93.0402 - val_loss: 1382.9993\n",
      "Epoch 257/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 93.0238 - val_loss: 1383.7319\n",
      "Epoch 258/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 93.0087 - val_loss: 1384.4338\n",
      "Epoch 259/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.9947 - val_loss: 1385.1069\n",
      "Epoch 260/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.9819 - val_loss: 1385.7511\n",
      "Epoch 261/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.9701 - val_loss: 1386.3688\n",
      "Epoch 262/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.9592 - val_loss: 1386.9586\n",
      "Epoch 263/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.9492 - val_loss: 1387.5231\n",
      "Epoch 264/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.9401 - val_loss: 1388.0636\n",
      "Epoch 265/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.9317 - val_loss: 1388.5806\n",
      "Epoch 266/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.9239 - val_loss: 1389.0726\n",
      "Epoch 267/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.9168 - val_loss: 1389.5438\n",
      "Epoch 268/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.9103 - val_loss: 1389.9938\n",
      "Epoch 269/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 92.9043 - val_loss: 1390.4218\n",
      "Epoch 270/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8988 - val_loss: 1390.8307\n",
      "Epoch 271/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8938 - val_loss: 1391.2208\n",
      "Epoch 272/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8892 - val_loss: 1391.5925\n",
      "Epoch 273/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8849 - val_loss: 1391.9458\n",
      "Epoch 274/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8811 - val_loss: 1392.2823\n",
      "Epoch 275/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8775 - val_loss: 1392.6025\n",
      "Epoch 276/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8743 - val_loss: 1392.9067\n",
      "Epoch 277/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8714 - val_loss: 1393.1963\n",
      "Epoch 278/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8686 - val_loss: 1393.4716\n",
      "Epoch 279/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 92.8662 - val_loss: 1393.7325\n",
      "Epoch 280/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8640 - val_loss: 1393.9807\n",
      "Epoch 281/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8620 - val_loss: 1394.2158\n",
      "Epoch 282/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8602 - val_loss: 1394.4397\n",
      "Epoch 283/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8585 - val_loss: 1394.6515\n",
      "Epoch 284/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8569 - val_loss: 1394.8518\n",
      "Epoch 285/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8556 - val_loss: 1395.0427\n",
      "Epoch 286/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8543 - val_loss: 1395.2223\n",
      "Epoch 287/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8532 - val_loss: 1395.3928\n",
      "Epoch 288/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8523 - val_loss: 1395.5543\n",
      "Epoch 289/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8514 - val_loss: 1395.7076\n",
      "Epoch 290/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8506 - val_loss: 1395.8523\n",
      "Epoch 291/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.8499 - val_loss: 1395.9884\n",
      "Epoch 292/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.8493 - val_loss: 1396.1174\n",
      "Epoch 293/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8487 - val_loss: 1396.2390\n",
      "Epoch 294/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8483 - val_loss: 1396.3542\n",
      "Epoch 295/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8478 - val_loss: 1396.4626\n",
      "Epoch 296/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8475 - val_loss: 1396.5649\n",
      "Epoch 297/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8472 - val_loss: 1396.6613\n",
      "Epoch 298/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8470 - val_loss: 1396.7524\n",
      "Epoch 299/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8468 - val_loss: 1396.8376\n",
      "Epoch 300/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8467 - val_loss: 1396.9186\n",
      "Epoch 301/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8465 - val_loss: 1396.9948\n",
      "Epoch 302/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8465 - val_loss: 1397.0664\n",
      "Epoch 303/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8464 - val_loss: 1397.1331\n",
      "Epoch 304/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8465 - val_loss: 1397.1967\n",
      "Epoch 305/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8465 - val_loss: 1397.2560\n",
      "Epoch 306/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8465 - val_loss: 1397.3113\n",
      "Epoch 307/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8466 - val_loss: 1397.3639\n",
      "Epoch 308/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8468 - val_loss: 1397.4135\n",
      "Epoch 309/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8468 - val_loss: 1397.4589\n",
      "Epoch 310/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8471 - val_loss: 1397.5027\n",
      "Epoch 311/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8472 - val_loss: 1397.5431\n",
      "Epoch 312/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8474 - val_loss: 1397.5813\n",
      "Epoch 313/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8476 - val_loss: 1397.6169\n",
      "Epoch 314/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8478 - val_loss: 1397.6506\n",
      "Epoch 315/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8480 - val_loss: 1397.6820\n",
      "Epoch 316/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8482 - val_loss: 1397.7107\n",
      "Epoch 317/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8484 - val_loss: 1397.7378\n",
      "Epoch 318/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8487 - val_loss: 1397.7635\n",
      "Epoch 319/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8490 - val_loss: 1397.7878\n",
      "Epoch 320/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8493 - val_loss: 1397.8102\n",
      "Epoch 321/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8495 - val_loss: 1397.8311\n",
      "Epoch 322/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8497 - val_loss: 1397.8502\n",
      "Epoch 323/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 92.8501 - val_loss: 1397.8684\n",
      "Epoch 324/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8503 - val_loss: 1397.8848\n",
      "Epoch 325/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8507 - val_loss: 1397.9005\n",
      "Epoch 326/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 92.8510 - val_loss: 1397.9159\n",
      "Epoch 327/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8512 - val_loss: 1397.9293\n",
      "Epoch 328/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8515 - val_loss: 1397.9419\n",
      "Epoch 329/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8519 - val_loss: 1397.9540\n",
      "Epoch 330/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8521 - val_loss: 1397.9652\n",
      "Epoch 331/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8524 - val_loss: 1397.9750\n",
      "Epoch 332/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8527 - val_loss: 1397.9849\n",
      "Epoch 333/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8531 - val_loss: 1397.9935\n",
      "Epoch 334/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8534 - val_loss: 1398.0022\n",
      "Epoch 335/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 92.8536 - val_loss: 1398.0094\n",
      "Epoch 336/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8540 - val_loss: 1398.0166\n",
      "Epoch 337/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8542 - val_loss: 1398.0229\n",
      "Epoch 338/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8546 - val_loss: 1398.0298\n",
      "Epoch 339/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8548 - val_loss: 1398.0354\n",
      "Epoch 340/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8551 - val_loss: 1398.0400\n",
      "Epoch 341/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8554 - val_loss: 1398.0450\n",
      "Epoch 342/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8558 - val_loss: 1398.0492\n",
      "Epoch 343/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8560 - val_loss: 1398.0536\n",
      "Epoch 344/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8563 - val_loss: 1398.0574\n",
      "Epoch 345/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 92.8566 - val_loss: 1398.0608\n",
      "Epoch 346/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8569 - val_loss: 1398.0640\n",
      "Epoch 347/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 92.8572 - val_loss: 1398.0669\n",
      "Epoch 348/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 92.8574 - val_loss: 1398.0693\n",
      "Epoch 349/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8577 - val_loss: 1398.0717\n",
      "Epoch 350/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8580 - val_loss: 1398.0740\n",
      "Epoch 351/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 92.8582 - val_loss: 1398.0759\n",
      "Epoch 352/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8585 - val_loss: 1398.0769\n",
      "Epoch 353/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8588 - val_loss: 1398.0786\n",
      "Epoch 354/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8590 - val_loss: 1398.0802\n",
      "Epoch 355/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8593 - val_loss: 1398.0815\n",
      "Epoch 356/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8596 - val_loss: 1398.0833\n",
      "Epoch 357/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8598 - val_loss: 1398.0846\n",
      "Epoch 358/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8600 - val_loss: 1398.0852\n",
      "Epoch 359/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8603 - val_loss: 1398.0859\n",
      "Epoch 360/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 92.8605 - val_loss: 1398.0868\n",
      "Epoch 361/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8607 - val_loss: 1398.0875\n",
      "Epoch 362/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8609 - val_loss: 1398.0883\n",
      "Epoch 363/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 92.8612 - val_loss: 1398.0890\n",
      "Epoch 364/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8614 - val_loss: 1398.0896\n",
      "Epoch 365/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8616 - val_loss: 1398.0898\n",
      "Epoch 366/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8619 - val_loss: 1398.0906\n",
      "Epoch 367/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8620 - val_loss: 1398.0912\n",
      "Epoch 368/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8623 - val_loss: 1398.0913\n",
      "Epoch 369/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8624 - val_loss: 1398.0914\n",
      "Epoch 370/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8627 - val_loss: 1398.0917\n",
      "Epoch 371/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8629 - val_loss: 1398.0917\n",
      "Epoch 372/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8631 - val_loss: 1398.0914\n",
      "Epoch 373/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8633 - val_loss: 1398.0914\n",
      "Epoch 374/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8635 - val_loss: 1398.0919\n",
      "Epoch 375/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8637 - val_loss: 1398.0919\n",
      "Epoch 376/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8639 - val_loss: 1398.0924\n",
      "Epoch 377/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 92.8640 - val_loss: 1398.0917\n",
      "Epoch 378/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8642 - val_loss: 1398.0917\n",
      "Epoch 379/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 92.8644 - val_loss: 1398.0917\n",
      "Epoch 380/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8645 - val_loss: 1398.0914\n",
      "Epoch 381/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8647 - val_loss: 1398.0914\n",
      "Epoch 382/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8649 - val_loss: 1398.0913\n",
      "Epoch 383/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8650 - val_loss: 1398.0912\n",
      "Epoch 384/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8652 - val_loss: 1398.0917\n",
      "Epoch 385/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8654 - val_loss: 1398.0912\n",
      "Epoch 386/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8655 - val_loss: 1398.0911\n",
      "Epoch 387/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8657 - val_loss: 1398.0909\n",
      "Epoch 388/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8658 - val_loss: 1398.0909\n",
      "Epoch 389/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8659 - val_loss: 1398.0909\n",
      "Epoch 390/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8661 - val_loss: 1398.0907\n",
      "Epoch 391/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8662 - val_loss: 1398.0902\n",
      "Epoch 392/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8664 - val_loss: 1398.0901\n",
      "Epoch 393/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8665 - val_loss: 1398.0900\n",
      "Epoch 394/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8666 - val_loss: 1398.0889\n",
      "Epoch 395/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8668 - val_loss: 1398.0879\n",
      "Epoch 396/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8669 - val_loss: 1398.0876\n",
      "Epoch 397/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8670 - val_loss: 1398.0875\n",
      "Epoch 398/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8671 - val_loss: 1398.0872\n",
      "Epoch 399/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8673 - val_loss: 1398.0873\n",
      "Epoch 400/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8674 - val_loss: 1398.0873\n",
      "Epoch 401/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8676 - val_loss: 1398.0875\n",
      "Epoch 402/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8677 - val_loss: 1398.0875\n",
      "Epoch 403/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8678 - val_loss: 1398.0878\n",
      "Epoch 404/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8679 - val_loss: 1398.0876\n",
      "Epoch 405/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.8680 - val_loss: 1398.0875\n",
      "Epoch 406/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.8680 - val_loss: 1398.0873\n",
      "Epoch 407/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8681 - val_loss: 1398.0870\n",
      "Epoch 408/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8682 - val_loss: 1398.0868\n",
      "Epoch 409/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8684 - val_loss: 1398.0868\n",
      "Epoch 410/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8684 - val_loss: 1398.0863\n",
      "Epoch 411/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8685 - val_loss: 1398.0863\n",
      "Epoch 412/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8687 - val_loss: 1398.0863\n",
      "Epoch 413/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8687 - val_loss: 1398.0863\n",
      "Epoch 414/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8688 - val_loss: 1398.0856\n",
      "Epoch 415/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8689 - val_loss: 1398.0850\n",
      "Epoch 416/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8690 - val_loss: 1398.0848\n",
      "Epoch 417/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8691 - val_loss: 1398.0847\n",
      "Epoch 418/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8691 - val_loss: 1398.0846\n",
      "Epoch 419/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8692 - val_loss: 1398.0842\n",
      "Epoch 420/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8693 - val_loss: 1398.0841\n",
      "Epoch 421/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8694 - val_loss: 1398.0837\n",
      "Epoch 422/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8695 - val_loss: 1398.0837\n",
      "Epoch 423/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8695 - val_loss: 1398.0826\n",
      "Epoch 424/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8696 - val_loss: 1398.0822\n",
      "Epoch 425/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8697 - val_loss: 1398.0817\n",
      "Epoch 426/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 92.8697 - val_loss: 1398.0809\n",
      "Epoch 427/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8700 - val_loss: 1398.0817\n",
      "Epoch 428/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8700 - val_loss: 1398.0819\n",
      "Epoch 429/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8700 - val_loss: 1398.0818\n",
      "Epoch 430/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 92.8701 - val_loss: 1398.0818\n",
      "Epoch 431/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8701 - val_loss: 1398.0817\n",
      "Epoch 432/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8702 - val_loss: 1398.0818\n",
      "Epoch 433/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8702 - val_loss: 1398.0817\n",
      "Epoch 434/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8703 - val_loss: 1398.0817\n",
      "Epoch 435/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8703 - val_loss: 1398.0818\n",
      "Epoch 436/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8704 - val_loss: 1398.0818\n",
      "Epoch 437/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8704 - val_loss: 1398.0817\n",
      "Epoch 438/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8705 - val_loss: 1398.0818\n",
      "Epoch 439/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8706 - val_loss: 1398.0818\n",
      "Epoch 440/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8706 - val_loss: 1398.0818\n",
      "Epoch 441/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8707 - val_loss: 1398.0818\n",
      "Epoch 442/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8707 - val_loss: 1398.0818\n",
      "Epoch 443/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8708 - val_loss: 1398.0818\n",
      "Epoch 444/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8708 - val_loss: 1398.0818\n",
      "Epoch 445/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8709 - val_loss: 1398.0819\n",
      "Epoch 446/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8709 - val_loss: 1398.0819\n",
      "Epoch 447/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8709 - val_loss: 1398.0819\n",
      "Epoch 448/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8710 - val_loss: 1398.0817\n",
      "Epoch 449/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8710 - val_loss: 1398.0815\n",
      "Epoch 450/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8710 - val_loss: 1398.0813\n",
      "Epoch 451/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 92.8711 - val_loss: 1398.0811\n",
      "Epoch 452/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8711 - val_loss: 1398.0811\n",
      "Epoch 453/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8712 - val_loss: 1398.0811\n",
      "Epoch 454/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8712 - val_loss: 1398.0809\n",
      "Epoch 455/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8713 - val_loss: 1398.0808\n",
      "Epoch 456/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8713 - val_loss: 1398.0806\n",
      "Epoch 457/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8713 - val_loss: 1398.0800\n",
      "Epoch 458/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8714 - val_loss: 1398.0795\n",
      "Epoch 459/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8714 - val_loss: 1398.0793\n",
      "Epoch 460/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8714 - val_loss: 1398.0793\n",
      "Epoch 461/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8715 - val_loss: 1398.0795\n",
      "Epoch 462/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8716 - val_loss: 1398.0803\n",
      "Epoch 463/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8715 - val_loss: 1398.0800\n",
      "Epoch 464/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8715 - val_loss: 1398.0793\n",
      "Epoch 465/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8716 - val_loss: 1398.0792\n",
      "Epoch 466/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8716 - val_loss: 1398.0790\n",
      "Epoch 467/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.8717 - val_loss: 1398.0793\n",
      "Epoch 468/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8717 - val_loss: 1398.0796\n",
      "Epoch 469/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8717 - val_loss: 1398.0796\n",
      "Epoch 470/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8717 - val_loss: 1398.0795\n",
      "Epoch 471/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8718 - val_loss: 1398.0790\n",
      "Epoch 472/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 92.8718 - val_loss: 1398.0787\n",
      "Epoch 473/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8718 - val_loss: 1398.0790\n",
      "Epoch 474/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8719 - val_loss: 1398.0790\n",
      "Epoch 475/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8719 - val_loss: 1398.0791\n",
      "Epoch 476/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8719 - val_loss: 1398.0791\n",
      "Epoch 477/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8719 - val_loss: 1398.0790\n",
      "Epoch 478/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8719 - val_loss: 1398.0780\n",
      "Epoch 479/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8720 - val_loss: 1398.0775\n",
      "Epoch 480/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8720 - val_loss: 1398.0775\n",
      "Epoch 481/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8721 - val_loss: 1398.0780\n",
      "Epoch 482/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8721 - val_loss: 1398.0782\n",
      "Epoch 483/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8720 - val_loss: 1398.0780\n",
      "Epoch 484/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8720 - val_loss: 1398.0775\n",
      "Epoch 485/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8721 - val_loss: 1398.0767\n",
      "Epoch 486/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8721 - val_loss: 1398.0764\n",
      "Epoch 487/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 92.8722 - val_loss: 1398.0763\n",
      "Epoch 488/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8722 - val_loss: 1398.0763\n",
      "Epoch 489/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8722 - val_loss: 1398.0763\n",
      "Epoch 490/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8723 - val_loss: 1398.0764\n",
      "Epoch 491/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8722 - val_loss: 1398.0764\n",
      "Epoch 492/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8722 - val_loss: 1398.0764\n",
      "Epoch 493/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 92.8722 - val_loss: 1398.0764\n",
      "Epoch 494/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8723 - val_loss: 1398.0767\n",
      "Epoch 495/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8722 - val_loss: 1398.0759\n",
      "Epoch 496/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8723 - val_loss: 1398.0753\n",
      "Epoch 497/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8723 - val_loss: 1398.0753\n",
      "Epoch 498/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8723 - val_loss: 1398.0753\n",
      "Epoch 499/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.8724 - val_loss: 1398.0753\n",
      "Epoch 500/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8724 - val_loss: 1398.0753\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.1552, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0026, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0004, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0102, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.8734, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 375ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[68.99430439, 68.85985061, 68.72539683, 68.59094304, 68.45648926,\n",
       "        68.32203548, 68.26486928,  0.07724502,  0.23195714,  0.        ,\n",
       "         0.        ,  0.29906166,  0.        , 70.04624183, 69.9874183 ,\n",
       "        69.92859477, 69.8309057 , 69.69645191, 69.56199813, 69.42754435,\n",
       "        69.29309057, 69.15863679, 69.02418301, 68.88972923, 68.75527544,\n",
       "        68.62082166, 68.48636788, 68.3519141 , 68.27420635, 63.90898693,\n",
       "        63.73251634, 63.55604575, 63.37957516, 63.17542017, 62.94852941,\n",
       "        62.72163866, 62.4947479 , 62.38531746,  0.45828229,  0.        ,\n",
       "         0.        ,  0.18153058,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.15015754, 68.6507003 , 68.5162465 , 68.3817927 ,\n",
       "        68.2835434 , 63.9482026 , 63.771732  , 63.5952614 , 63.4187909 ,\n",
       "        63.2258403 , 62.9989496 , 62.7720588 , 62.5451681 , 62.3909197 ,\n",
       "         0.20421855,  0.27068555, 70.9064986 , 70.8304622 , 70.7548319 ,\n",
       "        70.6792017 , 70.6035714 , 70.4318627 , 70.2553922 , 70.0789216 ,\n",
       "        69.902451  ,  0.11838111,  0.        , 53.3801384 ,  0.21667178,\n",
       "         0.        ,  0.        ,  0.55969226,  0.        ,  0.        ,\n",
       "        70.15721893,  0.        ,  0.79900664,  0.        ,  0.        ,\n",
       "         0.        ,  0.20451184,  0.37114802,  0.84229404,  0.62269884,\n",
       "         0.11075502,  0.        ,  0.        ,  0.53167498,  0.4674772 ,\n",
       "         0.27306396,  0.        ,  0.13948822,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([56.75751179, 56.74193482, 56.72635786, 56.7107809 , 56.69520394,\n",
       "       56.67962697, 56.66405001, 56.64847305, 56.63289609, 56.61731912,\n",
       "       56.60174216, 56.5861652 , 56.57058824, 56.55501127, 56.53943431,\n",
       "       56.52385735, 56.50828039, 56.49270342, 56.47712646, 56.4615495 ,\n",
       "       56.44597254, 56.43039557, 56.41481861, 56.39924165, 56.38366469,\n",
       "       56.36808772, 56.35251076, 56.3369338 , 56.32135684, 56.30577987,\n",
       "       56.29020291, 56.27462595, 56.25904899, 56.24347202, 56.22789506,\n",
       "       56.2123181 , 56.19674114, 56.18116417, 56.16558721, 56.15001025,\n",
       "       56.13443329, 56.11885632, 56.10327936, 56.0877024 , 56.07212544,\n",
       "       56.05654847, 56.04097151, 56.02539455, 56.00981759, 55.99424062,\n",
       "       55.97866366, 55.9630867 , 55.94750974, 55.93193277, 55.91635581,\n",
       "       55.90077885, 55.88520189, 55.86962492, 55.85404796, 55.838471  ,\n",
       "       55.82289404, 55.80731707, 55.79174011, 55.77616315, 55.76058619,\n",
       "       55.74500922, 55.72943226, 55.7138553 , 55.69827834, 55.68270137,\n",
       "       55.66712441, 55.65154745, 55.63597049, 55.62039352, 55.60481656,\n",
       "       55.5892396 , 55.57366264, 55.55808567, 55.54250871, 55.52693175,\n",
       "       55.51135479, 55.49577782, 55.48020086, 55.4646239 , 55.44904694,\n",
       "       55.43346997, 55.41789301, 55.40231605, 55.38673909, 55.37116212,\n",
       "       55.35558516, 55.3400082 , 55.32443124, 55.30885427, 55.29327731,\n",
       "       55.27770035, 55.26212339, 55.24654642, 55.23096946, 55.2153925 ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.84867680263452\n",
      "30.60346677667844\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
