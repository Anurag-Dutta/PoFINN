{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       92.600000\n",
       "1       92.387115\n",
       "2       92.174230\n",
       "3       91.961345\n",
       "4       91.748459\n",
       "          ...    \n",
       "1795    68.531326\n",
       "1796    68.529458\n",
       "1797    68.527591\n",
       "1798    68.525724\n",
       "1799    68.523856\n",
       "Name: C7, Length: 1800, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c7_interpolated_1700_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       92.600000\n",
       "1       92.387115\n",
       "2       92.174230\n",
       "3       91.961345\n",
       "4       91.748459\n",
       "          ...    \n",
       "1695     0.000000\n",
       "1696     0.000000\n",
       "1697     0.000000\n",
       "1698     0.000000\n",
       "1699     0.000000\n",
       "Name: C7, Length: 1700, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1700)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92.387115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92.174230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.961345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91.748459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     92.600000  0.000298\n",
       "1     92.387115  0.000298\n",
       "2     92.174230  0.000297\n",
       "3     91.961345  0.000297\n",
       "4     91.748459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 89.3180  # Value for C0\n",
    "K0 = -0.0010  # Value for K0\n",
    "K1 = 0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = -0.0204    # Value for b\n",
    "c = 2.2194    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    92.600000    92.387115    92.174230    91.961345    91.748459   \n",
      "351    92.387115    92.174230    91.961345    91.748459    91.535574   \n",
      "352    92.174230    91.961345    91.748459    91.535574    91.322689   \n",
      "353    91.961345    91.748459    91.535574    91.322689    91.109804   \n",
      "354    91.748459    91.535574    91.322689    91.109804    90.896919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    91.535574    91.322689    91.109804    90.896919    90.691597  ...   \n",
      "351    91.322689    91.109804    90.896919    90.691597    90.579552  ...   \n",
      "352    91.109804    90.896919    90.691597    90.579552    90.467507  ...   \n",
      "353    90.896919    90.691597    90.579552    90.467507    90.355462  ...   \n",
      "354    90.691597    90.579552    90.467507    90.355462    90.243417  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   80.261671    0.000263   80.252334    0.000263   80.242997    0.000263   \n",
      "351   80.252334    0.000263   80.242997    0.000263   80.233660    0.000262   \n",
      "352   80.242997    0.000263   80.233660    0.000262   80.224323    0.000262   \n",
      "353   80.233660    0.000262   80.224323    0.000262   80.214986    0.000262   \n",
      "354   80.224323    0.000262   80.214986    0.000262   80.205649    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   80.233660    0.000262   80.224323    0.000262  \n",
      "351   80.224323    0.000262   80.214986    0.000262  \n",
      "352   80.214986    0.000262   80.205649    0.000262  \n",
      "353   80.205649    0.000262   80.196312    0.000262  \n",
      "354   80.196312    0.000262   80.186975    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1250, 1, 251) (1250, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "16/16 [==============================] - 3s 37ms/step - loss: 5950.2671 - val_loss: 4919.8472\n",
      "Epoch 2/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5884.7905 - val_loss: 4878.4683\n",
      "Epoch 3/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5827.7007 - val_loss: 4820.1621\n",
      "Epoch 4/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5763.8032 - val_loss: 4764.9038\n",
      "Epoch 5/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5712.9893 - val_loss: 4719.4331\n",
      "Epoch 6/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5663.4663 - val_loss: 4674.6426\n",
      "Epoch 7/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5614.6318 - val_loss: 4630.4380\n",
      "Epoch 8/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5566.3696 - val_loss: 4586.7231\n",
      "Epoch 9/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5518.5908 - val_loss: 4543.4355\n",
      "Epoch 10/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5471.2402 - val_loss: 4500.5371\n",
      "Epoch 11/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5424.2832 - val_loss: 4458.0000\n",
      "Epoch 12/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5377.6943 - val_loss: 4415.8081\n",
      "Epoch 13/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5331.4580 - val_loss: 4373.9468\n",
      "Epoch 14/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5285.5615 - val_loss: 4332.4082\n",
      "Epoch 15/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5239.9937 - val_loss: 4291.1816\n",
      "Epoch 16/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5194.7476 - val_loss: 4250.2617\n",
      "Epoch 17/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5149.8159 - val_loss: 4209.6426\n",
      "Epoch 18/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5105.1938 - val_loss: 4169.3208\n",
      "Epoch 19/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5060.8784 - val_loss: 4129.2905\n",
      "Epoch 20/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5016.8618 - val_loss: 4089.5505\n",
      "Epoch 21/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4973.1440 - val_loss: 4050.0955\n",
      "Epoch 22/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4929.7197 - val_loss: 4010.9231\n",
      "Epoch 23/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4886.5859 - val_loss: 3972.0305\n",
      "Epoch 24/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4843.7402 - val_loss: 3933.4155\n",
      "Epoch 25/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4801.1812 - val_loss: 3895.0752\n",
      "Epoch 26/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4758.9033 - val_loss: 3857.0085\n",
      "Epoch 27/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4716.9087 - val_loss: 3819.2117\n",
      "Epoch 28/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4675.1919 - val_loss: 3781.6841\n",
      "Epoch 29/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4633.7515 - val_loss: 3744.4236\n",
      "Epoch 30/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4592.5869 - val_loss: 3707.4275\n",
      "Epoch 31/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4551.6943 - val_loss: 3670.6946\n",
      "Epoch 32/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4511.0723 - val_loss: 3634.2234\n",
      "Epoch 33/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4470.7202 - val_loss: 3598.0117\n",
      "Epoch 34/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4430.6367 - val_loss: 3562.0588\n",
      "Epoch 35/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4390.8179 - val_loss: 3526.3621\n",
      "Epoch 36/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4351.2637 - val_loss: 3490.9194\n",
      "Epoch 37/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4311.9717 - val_loss: 3455.7314\n",
      "Epoch 38/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4272.9409 - val_loss: 3420.7949\n",
      "Epoch 39/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4234.1699 - val_loss: 3386.1089\n",
      "Epoch 40/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4195.6577 - val_loss: 3351.6724\n",
      "Epoch 41/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4157.4014 - val_loss: 3317.4822\n",
      "Epoch 42/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4119.4004 - val_loss: 3283.5400\n",
      "Epoch 43/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4081.6538 - val_loss: 3249.8411\n",
      "Epoch 44/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4044.1589 - val_loss: 3216.3862\n",
      "Epoch 45/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4006.9150 - val_loss: 3183.1741\n",
      "Epoch 46/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3969.9211 - val_loss: 3150.2014\n",
      "Epoch 47/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3933.1753 - val_loss: 3117.4692\n",
      "Epoch 48/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3896.6765 - val_loss: 3084.9751\n",
      "Epoch 49/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3860.4233 - val_loss: 3052.7185\n",
      "Epoch 50/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3824.4146 - val_loss: 3020.6970\n",
      "Epoch 51/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3788.6492 - val_loss: 2988.9102\n",
      "Epoch 52/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3753.1257 - val_loss: 2957.3564\n",
      "Epoch 53/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3717.8418 - val_loss: 2926.0344\n",
      "Epoch 54/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3682.7981 - val_loss: 2894.9434\n",
      "Epoch 55/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3647.9924 - val_loss: 2864.0815\n",
      "Epoch 56/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3613.4231 - val_loss: 2833.4482\n",
      "Epoch 57/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3579.0898 - val_loss: 2803.0422\n",
      "Epoch 58/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3544.9910 - val_loss: 2772.8625\n",
      "Epoch 59/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3511.1252 - val_loss: 2742.9070\n",
      "Epoch 60/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3477.4917 - val_loss: 2713.1763\n",
      "Epoch 61/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3444.0896 - val_loss: 2683.6670\n",
      "Epoch 62/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3410.9170 - val_loss: 2654.3799\n",
      "Epoch 63/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3377.9727 - val_loss: 2625.3135\n",
      "Epoch 64/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3345.2566 - val_loss: 2596.4661\n",
      "Epoch 65/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3312.7666 - val_loss: 2567.8357\n",
      "Epoch 66/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3280.5020 - val_loss: 2539.4236\n",
      "Epoch 67/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3248.4609 - val_loss: 2511.2266\n",
      "Epoch 68/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3216.6436 - val_loss: 2483.2444\n",
      "Epoch 69/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3185.0479 - val_loss: 2455.4766\n",
      "Epoch 70/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3153.6741 - val_loss: 2427.9209\n",
      "Epoch 71/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3122.5188 - val_loss: 2400.5771\n",
      "Epoch 72/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3091.5828 - val_loss: 2373.4441\n",
      "Epoch 73/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3060.8643 - val_loss: 2346.5203\n",
      "Epoch 74/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3030.3628 - val_loss: 2319.8044\n",
      "Epoch 75/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3000.0759 - val_loss: 2293.2974\n",
      "Epoch 76/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2970.0046 - val_loss: 2266.9961\n",
      "Epoch 77/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2940.1467 - val_loss: 2240.9009\n",
      "Epoch 78/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2910.5012 - val_loss: 2215.0095\n",
      "Epoch 79/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2881.0671 - val_loss: 2189.3210\n",
      "Epoch 80/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2851.8430 - val_loss: 2163.8352\n",
      "Epoch 81/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2822.8289 - val_loss: 2138.5513\n",
      "Epoch 82/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2794.0222 - val_loss: 2113.4678\n",
      "Epoch 83/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2765.4248 - val_loss: 2088.5830\n",
      "Epoch 84/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2737.0325 - val_loss: 2063.8975\n",
      "Epoch 85/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2708.8462 - val_loss: 2039.4093\n",
      "Epoch 86/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2680.8647 - val_loss: 2015.1178\n",
      "Epoch 87/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2653.0867 - val_loss: 1991.0219\n",
      "Epoch 88/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2625.5112 - val_loss: 1967.1204\n",
      "Epoch 89/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2598.1370 - val_loss: 1943.4130\n",
      "Epoch 90/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2570.9641 - val_loss: 1919.8981\n",
      "Epoch 91/500\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 2543.9907 - val_loss: 1896.5751\n",
      "Epoch 92/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2517.2158 - val_loss: 1873.4425\n",
      "Epoch 93/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2490.6392 - val_loss: 1850.5006\n",
      "Epoch 94/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2464.2590 - val_loss: 1827.7472\n",
      "Epoch 95/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2438.0754 - val_loss: 1805.1825\n",
      "Epoch 96/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2412.0869 - val_loss: 1782.8051\n",
      "Epoch 97/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2386.2925 - val_loss: 1760.6138\n",
      "Epoch 98/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2360.6912 - val_loss: 1738.6073\n",
      "Epoch 99/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2335.2822 - val_loss: 1716.7858\n",
      "Epoch 100/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2310.0649 - val_loss: 1695.1481\n",
      "Epoch 101/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2285.0386 - val_loss: 1673.6926\n",
      "Epoch 102/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2260.2009 - val_loss: 1652.4192\n",
      "Epoch 103/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2235.5530 - val_loss: 1631.3273\n",
      "Epoch 104/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2211.0928 - val_loss: 1610.4146\n",
      "Epoch 105/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2186.8193 - val_loss: 1589.6812\n",
      "Epoch 106/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2162.7322 - val_loss: 1569.1272\n",
      "Epoch 107/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2138.8306 - val_loss: 1548.7501\n",
      "Epoch 108/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2115.1135 - val_loss: 1528.5493\n",
      "Epoch 109/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2091.5801 - val_loss: 1508.5248\n",
      "Epoch 110/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2068.2295 - val_loss: 1488.6750\n",
      "Epoch 111/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2045.0601 - val_loss: 1468.9996\n",
      "Epoch 112/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2022.0720 - val_loss: 1449.4962\n",
      "Epoch 113/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1999.2639 - val_loss: 1430.1660\n",
      "Epoch 114/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1976.6354 - val_loss: 1411.0074\n",
      "Epoch 115/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1954.1858 - val_loss: 1392.0190\n",
      "Epoch 116/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1931.9133 - val_loss: 1373.2010\n",
      "Epoch 117/500\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1909.8179 - val_loss: 1354.5521\n",
      "Epoch 118/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1887.8990 - val_loss: 1336.0717\n",
      "Epoch 119/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1866.1553 - val_loss: 1317.7585\n",
      "Epoch 120/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1844.5853 - val_loss: 1299.6112\n",
      "Epoch 121/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1823.1896 - val_loss: 1281.6304\n",
      "Epoch 122/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1801.9664 - val_loss: 1263.8140\n",
      "Epoch 123/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1780.9155 - val_loss: 1246.1622\n",
      "Epoch 124/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1760.0353 - val_loss: 1228.6742\n",
      "Epoch 125/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1739.3260 - val_loss: 1211.3483\n",
      "Epoch 126/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1718.7859 - val_loss: 1194.1841\n",
      "Epoch 127/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1698.4148 - val_loss: 1177.1810\n",
      "Epoch 128/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1678.2122 - val_loss: 1160.3379\n",
      "Epoch 129/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1658.1763 - val_loss: 1143.6542\n",
      "Epoch 130/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1638.3065 - val_loss: 1127.1285\n",
      "Epoch 131/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1618.6027 - val_loss: 1110.7615\n",
      "Epoch 132/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1599.0638 - val_loss: 1094.5510\n",
      "Epoch 133/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1579.6890 - val_loss: 1078.4971\n",
      "Epoch 134/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1560.4780 - val_loss: 1062.5983\n",
      "Epoch 135/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1541.4290 - val_loss: 1046.8539\n",
      "Epoch 136/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1522.5419 - val_loss: 1031.2642\n",
      "Epoch 137/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1503.8159 - val_loss: 1015.8271\n",
      "Epoch 138/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1485.2500 - val_loss: 1000.5422\n",
      "Epoch 139/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1466.8441 - val_loss: 985.4091\n",
      "Epoch 140/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1448.5966 - val_loss: 970.4266\n",
      "Epoch 141/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1430.5068 - val_loss: 955.5941\n",
      "Epoch 142/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1412.5746 - val_loss: 940.9112\n",
      "Epoch 143/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1394.7987 - val_loss: 926.3768\n",
      "Epoch 144/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1377.1792 - val_loss: 911.9899\n",
      "Epoch 145/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1359.7141 - val_loss: 897.7505\n",
      "Epoch 146/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1342.4038 - val_loss: 883.6571\n",
      "Epoch 147/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1325.2467 - val_loss: 869.7093\n",
      "Epoch 148/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1308.2427 - val_loss: 855.9058\n",
      "Epoch 149/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1291.3900 - val_loss: 842.2468\n",
      "Epoch 150/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1274.6892 - val_loss: 828.7309\n",
      "Epoch 151/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1258.1389 - val_loss: 815.3571\n",
      "Epoch 152/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1241.7379 - val_loss: 802.1251\n",
      "Epoch 153/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1225.4867 - val_loss: 789.0349\n",
      "Epoch 154/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1209.3839 - val_loss: 776.0837\n",
      "Epoch 155/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1193.4283 - val_loss: 763.2730\n",
      "Epoch 156/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1177.6198 - val_loss: 750.6008\n",
      "Epoch 157/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1161.9576 - val_loss: 738.0668\n",
      "Epoch 158/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1146.4409 - val_loss: 725.6694\n",
      "Epoch 159/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1131.0688 - val_loss: 713.4091\n",
      "Epoch 160/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1115.8406 - val_loss: 701.2843\n",
      "Epoch 161/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1100.7560 - val_loss: 689.2954\n",
      "Epoch 162/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1085.8142 - val_loss: 677.4404\n",
      "Epoch 163/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1071.0146 - val_loss: 665.7194\n",
      "Epoch 164/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1056.3558 - val_loss: 654.1306\n",
      "Epoch 165/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1041.8376 - val_loss: 642.6743\n",
      "Epoch 166/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1027.4589 - val_loss: 631.3492\n",
      "Epoch 167/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1013.2193 - val_loss: 620.1552\n",
      "Epoch 168/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 999.1182 - val_loss: 609.0906\n",
      "Epoch 169/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 985.1544 - val_loss: 598.1559\n",
      "Epoch 170/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 971.3281 - val_loss: 587.3491\n",
      "Epoch 171/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 957.6375 - val_loss: 576.6702\n",
      "Epoch 172/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 944.0826 - val_loss: 566.1185\n",
      "Epoch 173/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 930.6630 - val_loss: 555.6930\n",
      "Epoch 174/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 917.3769 - val_loss: 545.3939\n",
      "Epoch 175/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 904.2247 - val_loss: 535.2190\n",
      "Epoch 176/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 891.2050 - val_loss: 525.1684\n",
      "Epoch 177/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 878.3171 - val_loss: 515.2410\n",
      "Epoch 178/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 865.5608 - val_loss: 505.4364\n",
      "Epoch 179/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 852.9350 - val_loss: 495.7539\n",
      "Epoch 180/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 840.4393 - val_loss: 486.1929\n",
      "Epoch 181/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 828.0722 - val_loss: 476.7517\n",
      "Epoch 182/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 815.8342 - val_loss: 467.4309\n",
      "Epoch 183/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 803.7240 - val_loss: 458.2291\n",
      "Epoch 184/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 791.7411 - val_loss: 449.1456\n",
      "Epoch 185/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 779.8843 - val_loss: 440.1803\n",
      "Epoch 186/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 768.1538 - val_loss: 431.3313\n",
      "Epoch 187/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 756.5477 - val_loss: 422.5984\n",
      "Epoch 188/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 745.0662 - val_loss: 413.9812\n",
      "Epoch 189/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 733.7081 - val_loss: 405.4788\n",
      "Epoch 190/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 722.4733 - val_loss: 397.0905\n",
      "Epoch 191/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 711.3606 - val_loss: 388.8151\n",
      "Epoch 192/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 700.3690 - val_loss: 380.6523\n",
      "Epoch 193/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 689.4985 - val_loss: 372.6014\n",
      "Epoch 194/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 678.7484 - val_loss: 364.6618\n",
      "Epoch 195/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 668.1178 - val_loss: 356.8324\n",
      "Epoch 196/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 657.6060 - val_loss: 349.1133\n",
      "Epoch 197/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 647.2123 - val_loss: 341.5026\n",
      "Epoch 198/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 636.9362 - val_loss: 334.0004\n",
      "Epoch 199/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 626.7766 - val_loss: 326.6054\n",
      "Epoch 200/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 616.7332 - val_loss: 319.3175\n",
      "Epoch 201/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 606.8050 - val_loss: 312.1358\n",
      "Epoch 202/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 596.9915 - val_loss: 305.0593\n",
      "Epoch 203/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 587.2921 - val_loss: 298.0871\n",
      "Epoch 204/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 577.7061 - val_loss: 291.2194\n",
      "Epoch 205/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 568.2326 - val_loss: 284.4548\n",
      "Epoch 206/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 558.8708 - val_loss: 277.7921\n",
      "Epoch 207/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 549.6204 - val_loss: 271.2314\n",
      "Epoch 208/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 540.4802 - val_loss: 264.7718\n",
      "Epoch 209/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 531.4498 - val_loss: 258.4122\n",
      "Epoch 210/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 522.5287 - val_loss: 252.1522\n",
      "Epoch 211/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 513.7160 - val_loss: 245.9909\n",
      "Epoch 212/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 505.0110 - val_loss: 239.9277\n",
      "Epoch 213/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 496.4131 - val_loss: 233.9622\n",
      "Epoch 214/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 487.9214 - val_loss: 228.0925\n",
      "Epoch 215/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 479.5351 - val_loss: 222.3191\n",
      "Epoch 216/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 471.2539 - val_loss: 216.6407\n",
      "Epoch 217/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 463.0772 - val_loss: 211.0568\n",
      "Epoch 218/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 455.0037 - val_loss: 205.5667\n",
      "Epoch 219/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 447.0331 - val_loss: 200.1688\n",
      "Epoch 220/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 439.1645 - val_loss: 194.8633\n",
      "Epoch 221/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 431.3973 - val_loss: 189.6496\n",
      "Epoch 222/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 423.7314 - val_loss: 184.5264\n",
      "Epoch 223/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 416.1653 - val_loss: 179.4935\n",
      "Epoch 224/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 408.6983 - val_loss: 174.5496\n",
      "Epoch 225/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 401.3304 - val_loss: 169.6937\n",
      "Epoch 226/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 394.0600 - val_loss: 164.9259\n",
      "Epoch 227/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 386.8868 - val_loss: 160.2453\n",
      "Epoch 228/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 379.8106 - val_loss: 155.6504\n",
      "Epoch 229/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 372.8299 - val_loss: 151.1413\n",
      "Epoch 230/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 365.9446 - val_loss: 146.7174\n",
      "Epoch 231/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 359.1538 - val_loss: 142.3771\n",
      "Epoch 232/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 352.4563 - val_loss: 138.1201\n",
      "Epoch 233/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 345.8521 - val_loss: 133.9454\n",
      "Epoch 234/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 339.3404 - val_loss: 129.8529\n",
      "Epoch 235/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 332.9203 - val_loss: 125.8410\n",
      "Epoch 236/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 326.5909 - val_loss: 121.9097\n",
      "Epoch 237/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 320.3517 - val_loss: 118.0575\n",
      "Epoch 238/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 314.2023 - val_loss: 114.2844\n",
      "Epoch 239/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 308.1413 - val_loss: 110.5893\n",
      "Epoch 240/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 302.1686 - val_loss: 106.9711\n",
      "Epoch 241/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 296.2831 - val_loss: 103.4295\n",
      "Epoch 242/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 290.4842 - val_loss: 99.9639\n",
      "Epoch 243/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 284.7713 - val_loss: 96.5732\n",
      "Epoch 244/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 279.1438 - val_loss: 93.2566\n",
      "Epoch 245/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 273.6009 - val_loss: 90.0136\n",
      "Epoch 246/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 268.1419 - val_loss: 86.8436\n",
      "Epoch 247/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 262.7660 - val_loss: 83.7450\n",
      "Epoch 248/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 257.4722 - val_loss: 80.7179\n",
      "Epoch 249/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 252.2602 - val_loss: 77.7611\n",
      "Epoch 250/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 247.1291 - val_loss: 74.8743\n",
      "Epoch 251/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 242.0786 - val_loss: 72.0564\n",
      "Epoch 252/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 237.1076 - val_loss: 69.3064\n",
      "Epoch 253/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 232.2153 - val_loss: 66.6241\n",
      "Epoch 254/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 227.4011 - val_loss: 64.0082\n",
      "Epoch 255/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 222.6642 - val_loss: 61.4583\n",
      "Epoch 256/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 218.0043 - val_loss: 58.9737\n",
      "Epoch 257/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 213.4202 - val_loss: 56.5533\n",
      "Epoch 258/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 208.9115 - val_loss: 54.1966\n",
      "Epoch 259/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 204.4774 - val_loss: 51.9027\n",
      "Epoch 260/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 200.1170 - val_loss: 49.6708\n",
      "Epoch 261/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 195.8300 - val_loss: 47.5005\n",
      "Epoch 262/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 191.6152 - val_loss: 45.3906\n",
      "Epoch 263/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 187.4722 - val_loss: 43.3407\n",
      "Epoch 264/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 183.4003 - val_loss: 41.3496\n",
      "Epoch 265/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 179.3986 - val_loss: 39.4170\n",
      "Epoch 266/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 175.4664 - val_loss: 37.5418\n",
      "Epoch 267/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 171.6031 - val_loss: 35.7233\n",
      "Epoch 268/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 167.8080 - val_loss: 33.9610\n",
      "Epoch 269/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 164.0803 - val_loss: 32.2539\n",
      "Epoch 270/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 160.4195 - val_loss: 30.6013\n",
      "Epoch 271/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 156.8246 - val_loss: 29.0024\n",
      "Epoch 272/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 153.2949 - val_loss: 27.4565\n",
      "Epoch 273/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 149.8298 - val_loss: 25.9628\n",
      "Epoch 274/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 146.4285 - val_loss: 24.5204\n",
      "Epoch 275/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 143.0903 - val_loss: 23.1288\n",
      "Epoch 276/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 139.8145 - val_loss: 21.7871\n",
      "Epoch 277/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 136.6004 - val_loss: 20.4947\n",
      "Epoch 278/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 133.4475 - val_loss: 19.2505\n",
      "Epoch 279/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 130.3547 - val_loss: 18.0542\n",
      "Epoch 280/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 127.3216 - val_loss: 16.9047\n",
      "Epoch 281/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 124.3474 - val_loss: 15.8015\n",
      "Epoch 282/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 121.4314 - val_loss: 14.7437\n",
      "Epoch 283/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 118.5729 - val_loss: 13.7305\n",
      "Epoch 284/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 115.7710 - val_loss: 12.7613\n",
      "Epoch 285/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 113.0253 - val_loss: 11.8353\n",
      "Epoch 286/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 110.3350 - val_loss: 10.9518\n",
      "Epoch 287/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 107.6995 - val_loss: 10.1100\n",
      "Epoch 288/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 105.1177 - val_loss: 9.3091\n",
      "Epoch 289/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 102.5893 - val_loss: 8.5485\n",
      "Epoch 290/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 100.1135 - val_loss: 7.8273\n",
      "Epoch 291/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 97.6896 - val_loss: 7.1451\n",
      "Epoch 292/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 95.3170 - val_loss: 6.5008\n",
      "Epoch 293/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 92.9949 - val_loss: 5.8938\n",
      "Epoch 294/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 90.7225 - val_loss: 5.3234\n",
      "Epoch 295/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 88.4993 - val_loss: 4.7889\n",
      "Epoch 296/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 86.3246 - val_loss: 4.2895\n",
      "Epoch 297/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 84.1974 - val_loss: 3.8246\n",
      "Epoch 298/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 82.1175 - val_loss: 3.3934\n",
      "Epoch 299/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 80.0839 - val_loss: 2.9952\n",
      "Epoch 300/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 78.0960 - val_loss: 2.6292\n",
      "Epoch 301/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 76.1532 - val_loss: 2.2949\n",
      "Epoch 302/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 74.2547 - val_loss: 1.9915\n",
      "Epoch 303/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 72.4000 - val_loss: 1.7182\n",
      "Epoch 304/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 70.5883 - val_loss: 1.4745\n",
      "Epoch 305/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 68.8191 - val_loss: 1.2595\n",
      "Epoch 306/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 67.0915 - val_loss: 1.0728\n",
      "Epoch 307/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 65.4050 - val_loss: 0.9134\n",
      "Epoch 308/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 63.7588 - val_loss: 0.7808\n",
      "Epoch 309/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 62.1524 - val_loss: 0.6743\n",
      "Epoch 310/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 60.5850 - val_loss: 0.5932\n",
      "Epoch 311/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 59.0561 - val_loss: 0.5369\n",
      "Epoch 312/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 57.5651 - val_loss: 0.5047\n",
      "Epoch 313/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 56.1113 - val_loss: 0.4959\n",
      "Epoch 314/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 54.6939 - val_loss: 0.5099\n",
      "Epoch 315/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 53.3126 - val_loss: 0.5460\n",
      "Epoch 316/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 51.9665 - val_loss: 0.6037\n",
      "Epoch 317/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 50.6551 - val_loss: 0.6822\n",
      "Epoch 318/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 49.3777 - val_loss: 0.7810\n",
      "Epoch 319/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 48.1337 - val_loss: 0.8994\n",
      "Epoch 320/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 46.9226 - val_loss: 1.0368\n",
      "Epoch 321/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 45.7437 - val_loss: 1.1925\n",
      "Epoch 322/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 44.5963 - val_loss: 1.3661\n",
      "Epoch 323/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 43.4801 - val_loss: 1.5568\n",
      "Epoch 324/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 42.3942 - val_loss: 1.7641\n",
      "Epoch 325/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 41.3382 - val_loss: 1.9874\n",
      "Epoch 326/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 40.3115 - val_loss: 2.2261\n",
      "Epoch 327/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 39.3134 - val_loss: 2.4797\n",
      "Epoch 328/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 38.3434 - val_loss: 2.7475\n",
      "Epoch 329/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 37.4010 - val_loss: 3.0290\n",
      "Epoch 330/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 36.4855 - val_loss: 3.3236\n",
      "Epoch 331/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 35.5964 - val_loss: 3.6309\n",
      "Epoch 332/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 34.7332 - val_loss: 3.9502\n",
      "Epoch 333/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 33.8953 - val_loss: 4.2810\n",
      "Epoch 334/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 33.0822 - val_loss: 4.6228\n",
      "Epoch 335/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 32.2934 - val_loss: 4.9751\n",
      "Epoch 336/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 31.5282 - val_loss: 5.3374\n",
      "Epoch 337/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 30.7863 - val_loss: 5.7091\n",
      "Epoch 338/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 30.0670 - val_loss: 6.0897\n",
      "Epoch 339/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 29.3699 - val_loss: 6.4789\n",
      "Epoch 340/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 28.6943 - val_loss: 6.8759\n",
      "Epoch 341/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 28.0400 - val_loss: 7.2806\n",
      "Epoch 342/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 27.4063 - val_loss: 7.6922\n",
      "Epoch 343/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.7928 - val_loss: 8.1105\n",
      "Epoch 344/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.1990 - val_loss: 8.5347\n",
      "Epoch 345/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 25.6244 - val_loss: 8.9648\n",
      "Epoch 346/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 25.0685 - val_loss: 9.4000\n",
      "Epoch 347/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 24.5309 - val_loss: 9.8401\n",
      "Epoch 348/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 24.0111 - val_loss: 10.2847\n",
      "Epoch 349/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 23.5086 - val_loss: 10.7332\n",
      "Epoch 350/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 23.0231 - val_loss: 11.1854\n",
      "Epoch 351/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 22.5540 - val_loss: 11.6407\n",
      "Epoch 352/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 22.1011 - val_loss: 12.0988\n",
      "Epoch 353/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 21.6638 - val_loss: 12.5593\n",
      "Epoch 354/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 21.2416 - val_loss: 13.0222\n",
      "Epoch 355/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.8342 - val_loss: 13.4863\n",
      "Epoch 356/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.4413 - val_loss: 13.9522\n",
      "Epoch 357/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.0624 - val_loss: 14.4191\n",
      "Epoch 358/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 19.6970 - val_loss: 14.8867\n",
      "Epoch 359/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 19.3448 - val_loss: 15.3545\n",
      "Epoch 360/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 19.0055 - val_loss: 15.8225\n",
      "Epoch 361/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 18.6787 - val_loss: 16.2902\n",
      "Epoch 362/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 18.3640 - val_loss: 16.7576\n",
      "Epoch 363/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 18.0609 - val_loss: 17.2240\n",
      "Epoch 364/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.7693 - val_loss: 17.6893\n",
      "Epoch 365/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.4888 - val_loss: 18.1532\n",
      "Epoch 366/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 17.2190 - val_loss: 18.6156\n",
      "Epoch 367/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.9596 - val_loss: 19.0761\n",
      "Epoch 368/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.7102 - val_loss: 19.5343\n",
      "Epoch 369/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 16.4707 - val_loss: 19.9903\n",
      "Epoch 370/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.2406 - val_loss: 20.4436\n",
      "Epoch 371/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.0195 - val_loss: 20.8942\n",
      "Epoch 372/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.8073 - val_loss: 21.3418\n",
      "Epoch 373/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 15.6037 - val_loss: 21.7862\n",
      "Epoch 374/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.4083 - val_loss: 22.2270\n",
      "Epoch 375/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.2210 - val_loss: 22.6643\n",
      "Epoch 376/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0414 - val_loss: 23.0979\n",
      "Epoch 377/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 14.8692 - val_loss: 23.5275\n",
      "Epoch 378/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 14.7043 - val_loss: 23.9530\n",
      "Epoch 379/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 14.5462 - val_loss: 24.3744\n",
      "Epoch 380/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 14.3949 - val_loss: 24.7912\n",
      "Epoch 381/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 14.2501 - val_loss: 25.2036\n",
      "Epoch 382/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 14.1114 - val_loss: 25.6113\n",
      "Epoch 383/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.9789 - val_loss: 26.0143\n",
      "Epoch 384/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.8521 - val_loss: 26.4121\n",
      "Epoch 385/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.7308 - val_loss: 26.8054\n",
      "Epoch 386/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.6149 - val_loss: 27.1930\n",
      "Epoch 387/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.5042 - val_loss: 27.5757\n",
      "Epoch 388/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.3985 - val_loss: 27.9531\n",
      "Epoch 389/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.2976 - val_loss: 28.3251\n",
      "Epoch 390/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.2012 - val_loss: 28.6918\n",
      "Epoch 391/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.1093 - val_loss: 29.0529\n",
      "Epoch 392/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.0216 - val_loss: 29.4083\n",
      "Epoch 393/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.9379 - val_loss: 29.7584\n",
      "Epoch 394/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 12.8582 - val_loss: 30.1026\n",
      "Epoch 395/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.7822 - val_loss: 30.4411\n",
      "Epoch 396/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.7098 - val_loss: 30.7740\n",
      "Epoch 397/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.6409 - val_loss: 31.1011\n",
      "Epoch 398/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.5752 - val_loss: 31.4221\n",
      "Epoch 399/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.5128 - val_loss: 31.7375\n",
      "Epoch 400/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.4534 - val_loss: 32.0472\n",
      "Epoch 401/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3968 - val_loss: 32.3512\n",
      "Epoch 402/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3430 - val_loss: 32.6492\n",
      "Epoch 403/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2919 - val_loss: 32.9415\n",
      "Epoch 404/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2433 - val_loss: 33.2278\n",
      "Epoch 405/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.1971 - val_loss: 33.5084\n",
      "Epoch 406/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.1533 - val_loss: 33.7834\n",
      "Epoch 407/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.1116 - val_loss: 34.0527\n",
      "Epoch 408/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.0721 - val_loss: 34.3160\n",
      "Epoch 409/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.0345 - val_loss: 34.5735\n",
      "Epoch 410/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.9989 - val_loss: 34.8256\n",
      "Epoch 411/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.9651 - val_loss: 35.0718\n",
      "Epoch 412/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.9331 - val_loss: 35.3122\n",
      "Epoch 413/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.9028 - val_loss: 35.5473\n",
      "Epoch 414/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.8740 - val_loss: 35.7771\n",
      "Epoch 415/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 11.8467 - val_loss: 36.0015\n",
      "Epoch 416/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.8209 - val_loss: 36.2201\n",
      "Epoch 417/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.7964 - val_loss: 36.4336\n",
      "Epoch 418/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.7732 - val_loss: 36.6420\n",
      "Epoch 419/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.7513 - val_loss: 36.8449\n",
      "Epoch 420/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.7305 - val_loss: 37.0428\n",
      "Epoch 421/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.7108 - val_loss: 37.2353\n",
      "Epoch 422/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.6923 - val_loss: 37.4228\n",
      "Epoch 423/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.6747 - val_loss: 37.6056\n",
      "Epoch 424/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 11.6580 - val_loss: 37.7832\n",
      "Epoch 425/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.6423 - val_loss: 37.9562\n",
      "Epoch 426/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.6274 - val_loss: 38.1241\n",
      "Epoch 427/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.6134 - val_loss: 38.2877\n",
      "Epoch 428/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.6001 - val_loss: 38.4467\n",
      "Epoch 429/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.5875 - val_loss: 38.6010\n",
      "Epoch 430/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.5757 - val_loss: 38.7510\n",
      "Epoch 431/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.5645 - val_loss: 38.8964\n",
      "Epoch 432/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 11.5539 - val_loss: 39.0375\n",
      "Epoch 433/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.5439 - val_loss: 39.1745\n",
      "Epoch 434/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.5345 - val_loss: 39.3075\n",
      "Epoch 435/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.5255 - val_loss: 39.4362\n",
      "Epoch 436/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.5171 - val_loss: 39.5610\n",
      "Epoch 437/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.5092 - val_loss: 39.6818\n",
      "Epoch 438/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.5017 - val_loss: 39.7989\n",
      "Epoch 439/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.4946 - val_loss: 39.9121\n",
      "Epoch 440/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.4880 - val_loss: 40.0218\n",
      "Epoch 441/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.4817 - val_loss: 40.1280\n",
      "Epoch 442/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.4757 - val_loss: 40.2306\n",
      "Epoch 443/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.4701 - val_loss: 40.3300\n",
      "Epoch 444/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.4648 - val_loss: 40.4257\n",
      "Epoch 445/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.4599 - val_loss: 40.5184\n",
      "Epoch 446/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.4551 - val_loss: 40.6079\n",
      "Epoch 447/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.4507 - val_loss: 40.6941\n",
      "Epoch 448/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.4466 - val_loss: 40.7774\n",
      "Epoch 449/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.4426 - val_loss: 40.8577\n",
      "Epoch 450/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 11.4389 - val_loss: 40.9354\n",
      "Epoch 451/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.4354 - val_loss: 41.0100\n",
      "Epoch 452/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.4321 - val_loss: 41.0820\n",
      "Epoch 453/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.4290 - val_loss: 41.1515\n",
      "Epoch 454/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 11.4260 - val_loss: 41.2183\n",
      "Epoch 455/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.4232 - val_loss: 41.2825\n",
      "Epoch 456/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.4206 - val_loss: 41.3442\n",
      "Epoch 457/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.4182 - val_loss: 41.4038\n",
      "Epoch 458/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 11.4159 - val_loss: 41.4610\n",
      "Epoch 459/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.4137 - val_loss: 41.5162\n",
      "Epoch 460/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.4116 - val_loss: 41.5689\n",
      "Epoch 461/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.4097 - val_loss: 41.6197\n",
      "Epoch 462/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.4080 - val_loss: 41.6687\n",
      "Epoch 463/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.4062 - val_loss: 41.7157\n",
      "Epoch 464/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.4046 - val_loss: 41.7607\n",
      "Epoch 465/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.4031 - val_loss: 41.8036\n",
      "Epoch 466/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.4016 - val_loss: 41.8450\n",
      "Epoch 467/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 11.4003 - val_loss: 41.8848\n",
      "Epoch 468/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.3990 - val_loss: 41.9226\n",
      "Epoch 469/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.3978 - val_loss: 41.9590\n",
      "Epoch 470/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.3967 - val_loss: 41.9937\n",
      "Epoch 471/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.3957 - val_loss: 42.0272\n",
      "Epoch 472/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.3947 - val_loss: 42.0593\n",
      "Epoch 473/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.3938 - val_loss: 42.0896\n",
      "Epoch 474/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.3930 - val_loss: 42.1189\n",
      "Epoch 475/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.3921 - val_loss: 42.1468\n",
      "Epoch 476/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.3914 - val_loss: 42.1735\n",
      "Epoch 477/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.3906 - val_loss: 42.1992\n",
      "Epoch 478/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.3900 - val_loss: 42.2234\n",
      "Epoch 479/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.3894 - val_loss: 42.2465\n",
      "Epoch 480/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.3888 - val_loss: 42.2686\n",
      "Epoch 481/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.3883 - val_loss: 42.2899\n",
      "Epoch 482/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.3878 - val_loss: 42.3099\n",
      "Epoch 483/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 11.3873 - val_loss: 42.3290\n",
      "Epoch 484/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.3869 - val_loss: 42.3473\n",
      "Epoch 485/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.3865 - val_loss: 42.3648\n",
      "Epoch 486/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.3861 - val_loss: 42.3812\n",
      "Epoch 487/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.3858 - val_loss: 42.3969\n",
      "Epoch 488/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.3855 - val_loss: 42.4120\n",
      "Epoch 489/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.3852 - val_loss: 42.4263\n",
      "Epoch 490/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.3849 - val_loss: 42.4397\n",
      "Epoch 491/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.3847 - val_loss: 42.4526\n",
      "Epoch 492/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.3844 - val_loss: 42.4648\n",
      "Epoch 493/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.3842 - val_loss: 42.4764\n",
      "Epoch 494/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.3840 - val_loss: 42.4873\n",
      "Epoch 495/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.3839 - val_loss: 42.4979\n",
      "Epoch 496/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.3838 - val_loss: 42.5075\n",
      "Epoch 497/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.3837 - val_loss: 42.5170\n",
      "Epoch 498/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.3835 - val_loss: 42.5258\n",
      "Epoch 499/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.3834 - val_loss: 42.5343\n",
      "Epoch 500/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.3834 - val_loss: 42.5421\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(89.3180, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0010, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(-0.0204, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.2194, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 382ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.16556233e+01, 7.16446989e+01, 7.16337745e+01, 7.16228501e+01,\n",
       "        7.16119258e+01, 7.16010014e+01, 7.15821895e+01, 7.15625817e+01,\n",
       "        7.15429739e+01, 7.15233660e+01, 7.15037582e+01, 7.14841503e+01,\n",
       "        7.14645425e+01, 7.14449346e+01, 7.14253268e+01, 7.14057189e+01,\n",
       "        7.13861111e+01, 7.13665033e+01, 7.13468954e+01, 7.13272876e+01,\n",
       "        7.13076797e+01, 7.12880719e+01, 7.12684641e+01, 7.12488562e+01,\n",
       "        7.12292484e+01, 7.12096405e+01, 7.11900327e+01, 7.11704248e+01,\n",
       "        7.11508170e+01, 7.11312092e+01, 7.11116013e+01, 7.10919935e+01,\n",
       "        7.10723856e+01, 7.10527778e+01, 7.10331699e+01, 7.10135621e+01,\n",
       "        7.09939543e+01, 7.09743464e+01, 7.09547386e+01, 7.09351307e+01,\n",
       "        7.09155229e+01, 7.08918301e+01, 7.08526144e+01, 7.08133987e+01,\n",
       "        7.07741830e+01, 7.07349673e+01, 7.06957516e+01, 7.06565360e+01,\n",
       "        7.06173203e+01, 7.05781046e+01, 7.05388889e+01, 7.04996732e+01,\n",
       "        7.04604575e+01, 7.04212418e+01, 7.03820261e+01, 7.03428105e+01,\n",
       "        7.03035948e+01, 7.02643791e+01, 7.02251634e+01, 7.01859477e+01,\n",
       "        7.01467320e+01, 7.01075163e+01, 7.00683006e+01, 7.00290850e+01,\n",
       "        6.99898693e+01, 6.99506536e+01, 6.99114379e+01, 6.98722222e+01,\n",
       "        6.98330065e+01, 6.97937908e+01, 6.97545752e+01, 6.97153595e+01,\n",
       "        6.96761438e+01, 6.96369281e+01, 6.95977124e+01, 6.95584967e+01,\n",
       "        6.95192811e+01, 6.94900327e+01, 6.94704248e+01, 6.94508170e+01,\n",
       "        7.72702789e+01, 2.11114660e-02, 8.97632062e-01, 4.95039791e-01,\n",
       "        0.00000000e+00, 1.45282522e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.84672904e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.99221283e-01, 5.72564155e-02, 8.78671482e-02, 3.76206368e-01,\n",
       "        0.00000000e+00, 6.94524884e-01, 1.64610699e-01, 7.19315648e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([68.70436508, 68.70343137, 68.70249767, 68.70156396, 68.70063025,\n",
       "       68.69939309, 68.69752568, 68.69565826, 68.69379085, 68.69192344,\n",
       "       68.69005602, 68.68818861, 68.6863212 , 68.68445378, 68.68258637,\n",
       "       68.68071895, 68.67885154, 68.67698413, 68.67511671, 68.6732493 ,\n",
       "       68.67138189, 68.66951447, 68.66764706, 68.66577965, 68.66391223,\n",
       "       68.66204482, 68.6601774 , 68.65830999, 68.65644258, 68.65457516,\n",
       "       68.65270775, 68.65084034, 68.64897292, 68.64710551, 68.6452381 ,\n",
       "       68.64337068, 68.64150327, 68.63963585, 68.63776844, 68.63590103,\n",
       "       68.63403361, 68.6321662 , 68.63029879, 68.62843137, 68.62656396,\n",
       "       68.62469655, 68.62282913, 68.62096172, 68.6190943 , 68.61722689,\n",
       "       68.61535948, 68.61349206, 68.61162465, 68.60975724, 68.60788982,\n",
       "       68.60602241, 68.604155  , 68.60228758, 68.60042017, 68.59855275,\n",
       "       68.59668534, 68.59481793, 68.59295051, 68.5910831 , 68.58921569,\n",
       "       68.58734827, 68.58548086, 68.58361345, 68.58174603, 68.57987862,\n",
       "       68.5780112 , 68.57614379, 68.57427638, 68.57240896, 68.57054155,\n",
       "       68.56867414, 68.56680672, 68.56493931, 68.5630719 , 68.56120448,\n",
       "       68.55933707, 68.55746965, 68.55560224, 68.55373483, 68.55186741,\n",
       "       68.55      , 68.54813259, 68.54626517, 68.54439776, 68.54253035,\n",
       "       68.54066293, 68.53879552, 68.5369281 , 68.53506069, 68.53319328,\n",
       "       68.53132586, 68.52945845, 68.52759104, 68.52572362, 68.52385621])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.854400983040147\n",
      "14.751470148392771\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
