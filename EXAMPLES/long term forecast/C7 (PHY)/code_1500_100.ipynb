{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       92.600000\n",
       "1       92.387115\n",
       "2       92.174230\n",
       "3       91.961345\n",
       "4       91.748459\n",
       "          ...    \n",
       "1595    68.816830\n",
       "1596    68.810294\n",
       "1597    68.803758\n",
       "1598    68.799603\n",
       "1599    68.798669\n",
       "Name: C7, Length: 1600, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c7_interpolated_1500_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       92.600000\n",
       "1       92.387115\n",
       "2       92.174230\n",
       "3       91.961345\n",
       "4       91.748459\n",
       "          ...    \n",
       "1495    69.470425\n",
       "1496    69.463889\n",
       "1497    69.457353\n",
       "1498    69.450817\n",
       "1499    69.444281\n",
       "Name: C7, Length: 1500, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1500)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92.387115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92.174230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.961345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91.748459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     92.600000  0.000298\n",
       "1     92.387115  0.000298\n",
       "2     92.174230  0.000297\n",
       "3     91.961345  0.000297\n",
       "4     91.748459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 89.3180  # Value for C0\n",
    "K0 = -0.0010  # Value for K0\n",
    "K1 = 0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = -0.0204    # Value for b\n",
    "c = 2.2194    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    92.600000    92.387115    92.174230    91.961345    91.748459   \n",
      "351    92.387115    92.174230    91.961345    91.748459    91.535574   \n",
      "352    92.174230    91.961345    91.748459    91.535574    91.322689   \n",
      "353    91.961345    91.748459    91.535574    91.322689    91.109804   \n",
      "354    91.748459    91.535574    91.322689    91.109804    90.896919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    91.535574    91.322689    91.109804    90.896919    90.691597  ...   \n",
      "351    91.322689    91.109804    90.896919    90.691597    90.579552  ...   \n",
      "352    91.109804    90.896919    90.691597    90.579552    90.467507  ...   \n",
      "353    90.896919    90.691597    90.579552    90.467507    90.355462  ...   \n",
      "354    90.691597    90.579552    90.467507    90.355462    90.243417  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   80.261671    0.000263   80.252334    0.000263   80.242997    0.000263   \n",
      "351   80.252334    0.000263   80.242997    0.000263   80.233660    0.000262   \n",
      "352   80.242997    0.000263   80.233660    0.000262   80.224323    0.000262   \n",
      "353   80.233660    0.000262   80.224323    0.000262   80.214986    0.000262   \n",
      "354   80.224323    0.000262   80.214986    0.000262   80.205649    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   80.233660    0.000262   80.224323    0.000262  \n",
      "351   80.224323    0.000262   80.214986    0.000262  \n",
      "352   80.214986    0.000262   80.205649    0.000262  \n",
      "353   80.205649    0.000262   80.196312    0.000262  \n",
      "354   80.196312    0.000262   80.186975    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050, 1, 251) (1050, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "14/14 [==============================] - 3s 47ms/step - loss: 6044.0962 - val_loss: 5042.3579\n",
      "Epoch 2/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 5973.4048 - val_loss: 4988.8843\n",
      "Epoch 3/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5915.2773 - val_loss: 4943.2544\n",
      "Epoch 4/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 5865.4873 - val_loss: 4897.7114\n",
      "Epoch 5/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 5815.9707 - val_loss: 4852.5415\n",
      "Epoch 6/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5758.9482 - val_loss: 4787.9321\n",
      "Epoch 7/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5695.2471 - val_loss: 4740.8389\n",
      "Epoch 8/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 5644.0010 - val_loss: 4694.2583\n",
      "Epoch 9/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5593.3813 - val_loss: 4648.2847\n",
      "Epoch 10/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5543.3677 - val_loss: 4602.8389\n",
      "Epoch 11/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 5493.8779 - val_loss: 4557.8564\n",
      "Epoch 12/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5444.8535 - val_loss: 4513.2939\n",
      "Epoch 13/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5396.2529 - val_loss: 4469.1230\n",
      "Epoch 14/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 5348.0518 - val_loss: 4425.3242\n",
      "Epoch 15/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5300.2280 - val_loss: 4381.8818\n",
      "Epoch 16/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5252.7700 - val_loss: 4338.7856\n",
      "Epoch 17/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5205.6675 - val_loss: 4296.0269\n",
      "Epoch 18/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5158.9102 - val_loss: 4253.5977\n",
      "Epoch 19/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 5112.4902 - val_loss: 4211.4922\n",
      "Epoch 20/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5066.4053 - val_loss: 4169.7061\n",
      "Epoch 21/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 5020.6484 - val_loss: 4128.2358\n",
      "Epoch 22/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 4975.2153 - val_loss: 4087.0762\n",
      "Epoch 23/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4930.1011 - val_loss: 4046.2246\n",
      "Epoch 24/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4885.3037 - val_loss: 4005.6770\n",
      "Epoch 25/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4840.8198 - val_loss: 3965.4316\n",
      "Epoch 26/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4796.6455 - val_loss: 3925.4846\n",
      "Epoch 27/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4752.7788 - val_loss: 3885.8340\n",
      "Epoch 28/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4709.2173 - val_loss: 3846.4773\n",
      "Epoch 29/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 4665.9580 - val_loss: 3807.4126\n",
      "Epoch 30/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4622.9995 - val_loss: 3768.6367\n",
      "Epoch 31/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4580.3384 - val_loss: 3730.1492\n",
      "Epoch 32/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4537.9731 - val_loss: 3691.9468\n",
      "Epoch 33/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4495.9023 - val_loss: 3654.0281\n",
      "Epoch 34/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4454.1230 - val_loss: 3616.3914\n",
      "Epoch 35/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4412.6343 - val_loss: 3579.0339\n",
      "Epoch 36/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4371.4326 - val_loss: 3541.9551\n",
      "Epoch 37/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4330.5186 - val_loss: 3505.1523\n",
      "Epoch 38/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4289.8877 - val_loss: 3468.6248\n",
      "Epoch 39/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4249.5410 - val_loss: 3432.3701\n",
      "Epoch 40/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4209.4751 - val_loss: 3396.3867\n",
      "Epoch 41/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4169.6890 - val_loss: 3360.6726\n",
      "Epoch 42/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4130.1802 - val_loss: 3325.2278\n",
      "Epoch 43/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4090.9485 - val_loss: 3290.0488\n",
      "Epoch 44/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4051.9910 - val_loss: 3255.1367\n",
      "Epoch 45/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 4013.3074 - val_loss: 3220.4866\n",
      "Epoch 46/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3974.8953 - val_loss: 3186.0991\n",
      "Epoch 47/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3936.7529 - val_loss: 3151.9724\n",
      "Epoch 48/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3898.8801 - val_loss: 3118.1055\n",
      "Epoch 49/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3861.2742 - val_loss: 3084.4961\n",
      "Epoch 50/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3823.9346 - val_loss: 3051.1436\n",
      "Epoch 51/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3786.8591 - val_loss: 3018.0452\n",
      "Epoch 52/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3750.0466 - val_loss: 2985.2019\n",
      "Epoch 53/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 3713.4958 - val_loss: 2952.6099\n",
      "Epoch 54/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3677.2053 - val_loss: 2920.2688\n",
      "Epoch 55/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3641.1736 - val_loss: 2888.1780\n",
      "Epoch 56/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3605.3999 - val_loss: 2856.3347\n",
      "Epoch 57/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3569.8818 - val_loss: 2824.7393\n",
      "Epoch 58/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3534.6184 - val_loss: 2793.3887\n",
      "Epoch 59/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3499.6086 - val_loss: 2762.2827\n",
      "Epoch 60/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3464.8516 - val_loss: 2731.4202\n",
      "Epoch 61/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3430.3450 - val_loss: 2700.7991\n",
      "Epoch 62/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3396.0884 - val_loss: 2670.4187\n",
      "Epoch 63/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3362.0803 - val_loss: 2640.2776\n",
      "Epoch 64/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3328.3181 - val_loss: 2610.3748\n",
      "Epoch 65/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3294.8030 - val_loss: 2580.7080\n",
      "Epoch 66/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3261.5315 - val_loss: 2551.2769\n",
      "Epoch 67/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3228.5039 - val_loss: 2522.0803\n",
      "Epoch 68/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3195.7178 - val_loss: 2493.1167\n",
      "Epoch 69/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3163.1731 - val_loss: 2464.3848\n",
      "Epoch 70/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3130.8677 - val_loss: 2435.8833\n",
      "Epoch 71/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3098.8003 - val_loss: 2407.6116\n",
      "Epoch 72/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3066.9702 - val_loss: 2379.5674\n",
      "Epoch 73/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3035.3762 - val_loss: 2351.7512\n",
      "Epoch 74/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3004.0166 - val_loss: 2324.1602\n",
      "Epoch 75/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2972.8916 - val_loss: 2296.7952\n",
      "Epoch 76/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2941.9985 - val_loss: 2269.6531\n",
      "Epoch 77/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 2911.3372 - val_loss: 2242.7327\n",
      "Epoch 78/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2880.9053 - val_loss: 2216.0349\n",
      "Epoch 79/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2850.7034 - val_loss: 2189.5562\n",
      "Epoch 80/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2820.7285 - val_loss: 2163.2964\n",
      "Epoch 81/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2790.9800 - val_loss: 2137.2551\n",
      "Epoch 82/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2761.4575 - val_loss: 2111.4297\n",
      "Epoch 83/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2732.1592 - val_loss: 2085.8206\n",
      "Epoch 84/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2703.0840 - val_loss: 2060.4255\n",
      "Epoch 85/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2674.2312 - val_loss: 2035.2434\n",
      "Epoch 86/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2645.5991 - val_loss: 2010.2744\n",
      "Epoch 87/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2617.1873 - val_loss: 1985.5165\n",
      "Epoch 88/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2588.9949 - val_loss: 1960.9681\n",
      "Epoch 89/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2561.0193 - val_loss: 1936.6292\n",
      "Epoch 90/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2533.2607 - val_loss: 1912.4978\n",
      "Epoch 91/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2505.7175 - val_loss: 1888.5736\n",
      "Epoch 92/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2478.3889 - val_loss: 1864.8547\n",
      "Epoch 93/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2451.2744 - val_loss: 1841.3418\n",
      "Epoch 94/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2424.3721 - val_loss: 1818.0312\n",
      "Epoch 95/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2397.6804 - val_loss: 1794.9232\n",
      "Epoch 96/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2371.1995 - val_loss: 1772.0171\n",
      "Epoch 97/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2344.9275 - val_loss: 1749.3107\n",
      "Epoch 98/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2318.8633 - val_loss: 1726.8051\n",
      "Epoch 99/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2293.0068 - val_loss: 1704.4974\n",
      "Epoch 100/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2267.3567 - val_loss: 1682.3875\n",
      "Epoch 101/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2241.9111 - val_loss: 1660.4729\n",
      "Epoch 102/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2216.6694 - val_loss: 1638.7542\n",
      "Epoch 103/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2191.6309 - val_loss: 1617.2295\n",
      "Epoch 104/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2166.7944 - val_loss: 1595.8979\n",
      "Epoch 105/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2142.1592 - val_loss: 1574.7598\n",
      "Epoch 106/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2117.7236 - val_loss: 1553.8119\n",
      "Epoch 107/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2093.4871 - val_loss: 1533.0551\n",
      "Epoch 108/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2069.4487 - val_loss: 1512.4863\n",
      "Epoch 109/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2045.6072 - val_loss: 1492.1073\n",
      "Epoch 110/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2021.9618 - val_loss: 1471.9149\n",
      "Epoch 111/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1998.5114 - val_loss: 1451.9083\n",
      "Epoch 112/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1975.2549 - val_loss: 1432.0874\n",
      "Epoch 113/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1952.1915 - val_loss: 1412.4507\n",
      "Epoch 114/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1929.3202 - val_loss: 1392.9980\n",
      "Epoch 115/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1906.6398 - val_loss: 1373.7268\n",
      "Epoch 116/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1884.1494 - val_loss: 1354.6377\n",
      "Epoch 117/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1861.8483 - val_loss: 1335.7280\n",
      "Epoch 118/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1839.7349 - val_loss: 1316.9984\n",
      "Epoch 119/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1817.8090 - val_loss: 1298.4471\n",
      "Epoch 120/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1796.0699 - val_loss: 1280.0745\n",
      "Epoch 121/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1774.5156 - val_loss: 1261.8771\n",
      "Epoch 122/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1753.1459 - val_loss: 1243.8553\n",
      "Epoch 123/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1731.9592 - val_loss: 1226.0090\n",
      "Epoch 124/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1710.9553 - val_loss: 1208.3363\n",
      "Epoch 125/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1690.1331 - val_loss: 1190.8367\n",
      "Epoch 126/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 1669.4918 - val_loss: 1173.5087\n",
      "Epoch 127/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1649.0305 - val_loss: 1156.3517\n",
      "Epoch 128/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1628.7472 - val_loss: 1139.3644\n",
      "Epoch 129/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1608.6418 - val_loss: 1122.5466\n",
      "Epoch 130/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1588.7134 - val_loss: 1105.8972\n",
      "Epoch 131/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1568.9613 - val_loss: 1089.4141\n",
      "Epoch 132/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1549.3843 - val_loss: 1073.0984\n",
      "Epoch 133/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1529.9816 - val_loss: 1056.9478\n",
      "Epoch 134/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1510.7520 - val_loss: 1040.9618\n",
      "Epoch 135/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1491.6947 - val_loss: 1025.1383\n",
      "Epoch 136/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1472.8086 - val_loss: 1009.4787\n",
      "Epoch 137/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1454.0935 - val_loss: 993.9802\n",
      "Epoch 138/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1435.5483 - val_loss: 978.6428\n",
      "Epoch 139/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1417.1720 - val_loss: 963.4656\n",
      "Epoch 140/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1398.9634 - val_loss: 948.4468\n",
      "Epoch 141/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1380.9216 - val_loss: 933.5860\n",
      "Epoch 142/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1363.0455 - val_loss: 918.8832\n",
      "Epoch 143/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1345.3354 - val_loss: 904.3367\n",
      "Epoch 144/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1327.7894 - val_loss: 889.9453\n",
      "Epoch 145/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1310.4070 - val_loss: 875.7085\n",
      "Epoch 146/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1293.1870 - val_loss: 861.6257\n",
      "Epoch 147/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1276.1290 - val_loss: 847.6954\n",
      "Epoch 148/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1259.2318 - val_loss: 833.9179\n",
      "Epoch 149/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1242.4948 - val_loss: 820.2899\n",
      "Epoch 150/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 1225.9165 - val_loss: 806.8130\n",
      "Epoch 151/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1209.4968 - val_loss: 793.4853\n",
      "Epoch 152/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 1193.2345 - val_loss: 780.3061\n",
      "Epoch 153/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 1177.1285 - val_loss: 767.2747\n",
      "Epoch 154/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1161.1786 - val_loss: 754.3890\n",
      "Epoch 155/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1145.3831 - val_loss: 741.6492\n",
      "Epoch 156/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1129.7411 - val_loss: 729.0546\n",
      "Epoch 157/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1114.2528 - val_loss: 716.6038\n",
      "Epoch 158/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 1098.9166 - val_loss: 704.2962\n",
      "Epoch 159/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1083.7318 - val_loss: 692.1310\n",
      "Epoch 160/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1068.6976 - val_loss: 680.1071\n",
      "Epoch 161/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1053.8130 - val_loss: 668.2238\n",
      "Epoch 162/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1039.0774 - val_loss: 656.4802\n",
      "Epoch 163/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1024.4897 - val_loss: 644.8754\n",
      "Epoch 164/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1010.0493 - val_loss: 633.4091\n",
      "Epoch 165/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 995.7551 - val_loss: 622.0793\n",
      "Epoch 166/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 981.6064 - val_loss: 610.8859\n",
      "Epoch 167/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 967.6023 - val_loss: 599.8280\n",
      "Epoch 168/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 953.7421 - val_loss: 588.9042\n",
      "Epoch 169/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 940.0249 - val_loss: 578.1149\n",
      "Epoch 170/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 926.4501 - val_loss: 567.4578\n",
      "Epoch 171/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 913.0163 - val_loss: 556.9330\n",
      "Epoch 172/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 899.7230 - val_loss: 546.5391\n",
      "Epoch 173/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 886.5693 - val_loss: 536.2759\n",
      "Epoch 174/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 873.5544 - val_loss: 526.1418\n",
      "Epoch 175/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 860.6779 - val_loss: 516.1360\n",
      "Epoch 176/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 847.9380 - val_loss: 506.2585\n",
      "Epoch 177/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 835.3347 - val_loss: 496.5075\n",
      "Epoch 178/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 822.8669 - val_loss: 486.8831\n",
      "Epoch 179/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 810.5342 - val_loss: 477.3835\n",
      "Epoch 180/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 798.3349 - val_loss: 468.0085\n",
      "Epoch 181/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 786.2689 - val_loss: 458.7567\n",
      "Epoch 182/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 774.3353 - val_loss: 449.6281\n",
      "Epoch 183/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 762.5331 - val_loss: 440.6208\n",
      "Epoch 184/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 750.8613 - val_loss: 431.7346\n",
      "Epoch 185/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 739.3193 - val_loss: 422.9684\n",
      "Epoch 186/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 727.9060 - val_loss: 414.3222\n",
      "Epoch 187/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 716.6214 - val_loss: 405.7937\n",
      "Epoch 188/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 705.4640 - val_loss: 397.3837\n",
      "Epoch 189/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 694.4337 - val_loss: 389.0905\n",
      "Epoch 190/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 683.5286 - val_loss: 380.9130\n",
      "Epoch 191/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 672.7485 - val_loss: 372.8509\n",
      "Epoch 192/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 662.0924 - val_loss: 364.9023\n",
      "Epoch 193/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 651.5598 - val_loss: 357.0676\n",
      "Epoch 194/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 641.1497 - val_loss: 349.3458\n",
      "Epoch 195/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 630.8610 - val_loss: 341.7356\n",
      "Epoch 196/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 620.6934 - val_loss: 334.2362\n",
      "Epoch 197/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 610.6459 - val_loss: 326.8474\n",
      "Epoch 198/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 600.7175 - val_loss: 319.5674\n",
      "Epoch 199/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 590.9079 - val_loss: 312.3966\n",
      "Epoch 200/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 581.2158 - val_loss: 305.3329\n",
      "Epoch 201/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 571.6409 - val_loss: 298.3768\n",
      "Epoch 202/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 562.1818 - val_loss: 291.5255\n",
      "Epoch 203/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 552.8381 - val_loss: 284.7807\n",
      "Epoch 204/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 543.6089 - val_loss: 278.1393\n",
      "Epoch 205/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 534.4937 - val_loss: 271.6022\n",
      "Epoch 206/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 525.4912 - val_loss: 265.1670\n",
      "Epoch 207/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 516.6005 - val_loss: 258.8341\n",
      "Epoch 208/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 507.8213 - val_loss: 252.6024\n",
      "Epoch 209/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 499.1525 - val_loss: 246.4705\n",
      "Epoch 210/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 490.5933 - val_loss: 240.4377\n",
      "Epoch 211/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 482.1430 - val_loss: 234.5038\n",
      "Epoch 212/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 473.8006 - val_loss: 228.6676\n",
      "Epoch 213/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 465.5659 - val_loss: 222.9286\n",
      "Epoch 214/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 457.4376 - val_loss: 217.2855\n",
      "Epoch 215/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 449.4150 - val_loss: 211.7376\n",
      "Epoch 216/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 441.4972 - val_loss: 206.2843\n",
      "Epoch 217/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 433.6837 - val_loss: 200.9244\n",
      "Epoch 218/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 425.9735 - val_loss: 195.6575\n",
      "Epoch 219/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 418.3663 - val_loss: 190.4828\n",
      "Epoch 220/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 410.8605 - val_loss: 185.3987\n",
      "Epoch 221/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 403.4554 - val_loss: 180.4056\n",
      "Epoch 222/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 396.1508 - val_loss: 175.5013\n",
      "Epoch 223/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 388.9458 - val_loss: 170.6862\n",
      "Epoch 224/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 381.8393 - val_loss: 165.9590\n",
      "Epoch 225/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 374.8309 - val_loss: 161.3188\n",
      "Epoch 226/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 367.9192 - val_loss: 156.7646\n",
      "Epoch 227/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 361.1037 - val_loss: 152.2958\n",
      "Epoch 228/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 354.3835 - val_loss: 147.9119\n",
      "Epoch 229/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 347.7580 - val_loss: 143.6112\n",
      "Epoch 230/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 341.2262 - val_loss: 139.3938\n",
      "Epoch 231/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 334.7878 - val_loss: 135.2585\n",
      "Epoch 232/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 328.4418 - val_loss: 131.2044\n",
      "Epoch 233/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 322.1872 - val_loss: 127.2314\n",
      "Epoch 234/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 316.0236 - val_loss: 123.3375\n",
      "Epoch 235/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 309.9499 - val_loss: 119.5228\n",
      "Epoch 236/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 303.9653 - val_loss: 115.7859\n",
      "Epoch 237/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 298.0693 - val_loss: 112.1267\n",
      "Epoch 238/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 292.2609 - val_loss: 108.5434\n",
      "Epoch 239/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 286.5391 - val_loss: 105.0360\n",
      "Epoch 240/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 280.9036 - val_loss: 101.6034\n",
      "Epoch 241/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 275.3534 - val_loss: 98.2450\n",
      "Epoch 242/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 269.8877 - val_loss: 94.9595\n",
      "Epoch 243/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 264.5056 - val_loss: 91.7465\n",
      "Epoch 244/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 259.2067 - val_loss: 88.6051\n",
      "Epoch 245/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 253.9897 - val_loss: 85.5344\n",
      "Epoch 246/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 248.8544 - val_loss: 82.5339\n",
      "Epoch 247/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 243.7999 - val_loss: 79.6024\n",
      "Epoch 248/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 238.8251 - val_loss: 76.7394\n",
      "Epoch 249/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 233.9294 - val_loss: 73.9437\n",
      "Epoch 250/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 229.1121 - val_loss: 71.2150\n",
      "Epoch 251/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 224.3724 - val_loss: 68.5521\n",
      "Epoch 252/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 219.7094 - val_loss: 65.9547\n",
      "Epoch 253/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 215.1225 - val_loss: 63.4212\n",
      "Epoch 254/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 210.6109 - val_loss: 60.9515\n",
      "Epoch 255/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 206.1739 - val_loss: 58.5446\n",
      "Epoch 256/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 201.8106 - val_loss: 56.1998\n",
      "Epoch 257/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 197.5205 - val_loss: 53.9161\n",
      "Epoch 258/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 193.3024 - val_loss: 51.6927\n",
      "Epoch 259/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 189.1559 - val_loss: 49.5291\n",
      "Epoch 260/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 185.0802 - val_loss: 47.4241\n",
      "Epoch 261/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 181.0741 - val_loss: 45.3772\n",
      "Epoch 262/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 177.1375 - val_loss: 43.3875\n",
      "Epoch 263/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 173.2693 - val_loss: 41.4543\n",
      "Epoch 264/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 169.4688 - val_loss: 39.5770\n",
      "Epoch 265/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 165.7353 - val_loss: 37.7543\n",
      "Epoch 266/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 162.0682 - val_loss: 35.9861\n",
      "Epoch 267/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 158.4666 - val_loss: 34.2710\n",
      "Epoch 268/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 154.9295 - val_loss: 32.6087\n",
      "Epoch 269/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 151.4566 - val_loss: 30.9980\n",
      "Epoch 270/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 148.0467 - val_loss: 29.4385\n",
      "Epoch 271/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 144.6993 - val_loss: 27.9291\n",
      "Epoch 272/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 141.4137 - val_loss: 26.4693\n",
      "Epoch 273/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 138.1893 - val_loss: 25.0583\n",
      "Epoch 274/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 135.0248 - val_loss: 23.6951\n",
      "Epoch 275/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 131.9202 - val_loss: 22.3794\n",
      "Epoch 276/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 128.8744 - val_loss: 21.1102\n",
      "Epoch 277/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 125.8867 - val_loss: 19.8865\n",
      "Epoch 278/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 122.9564 - val_loss: 18.7080\n",
      "Epoch 279/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 120.0828 - val_loss: 17.5737\n",
      "Epoch 280/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 117.2651 - val_loss: 16.4828\n",
      "Epoch 281/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 114.5026 - val_loss: 15.4348\n",
      "Epoch 282/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 111.7947 - val_loss: 14.4287\n",
      "Epoch 283/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 109.1406 - val_loss: 13.4639\n",
      "Epoch 284/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 106.5395 - val_loss: 12.5398\n",
      "Epoch 285/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 103.9911 - val_loss: 11.6556\n",
      "Epoch 286/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 101.4943 - val_loss: 10.8103\n",
      "Epoch 287/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 99.0483 - val_loss: 10.0035\n",
      "Epoch 288/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 96.6527 - val_loss: 9.2343\n",
      "Epoch 289/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 94.3066 - val_loss: 8.5021\n",
      "Epoch 290/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 92.0097 - val_loss: 7.8062\n",
      "Epoch 291/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 89.7606 - val_loss: 7.1458\n",
      "Epoch 292/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 87.5594 - val_loss: 6.5203\n",
      "Epoch 293/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 85.4049 - val_loss: 5.9289\n",
      "Epoch 294/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 83.2964 - val_loss: 5.3709\n",
      "Epoch 295/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 81.2336 - val_loss: 4.8457\n",
      "Epoch 296/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 79.2154 - val_loss: 4.3526\n",
      "Epoch 297/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 77.2416 - val_loss: 3.8910\n",
      "Epoch 298/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 75.3112 - val_loss: 3.4600\n",
      "Epoch 299/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 73.4236 - val_loss: 3.0590\n",
      "Epoch 300/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 71.5782 - val_loss: 2.6875\n",
      "Epoch 301/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 69.7744 - val_loss: 2.3447\n",
      "Epoch 302/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 68.0114 - val_loss: 2.0300\n",
      "Epoch 303/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 66.2886 - val_loss: 1.7426\n",
      "Epoch 304/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 64.6054 - val_loss: 1.4821\n",
      "Epoch 305/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 62.9612 - val_loss: 1.2477\n",
      "Epoch 306/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 61.3553 - val_loss: 1.0387\n",
      "Epoch 307/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 59.7870 - val_loss: 0.8546\n",
      "Epoch 308/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 58.2559 - val_loss: 0.6947\n",
      "Epoch 309/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 56.7610 - val_loss: 0.5585\n",
      "Epoch 310/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 55.3022 - val_loss: 0.4452\n",
      "Epoch 311/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 53.8785 - val_loss: 0.3543\n",
      "Epoch 312/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 52.4893 - val_loss: 0.2851\n",
      "Epoch 313/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 51.1342 - val_loss: 0.2372\n",
      "Epoch 314/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 49.8124 - val_loss: 0.2098\n",
      "Epoch 315/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 48.5235 - val_loss: 0.2023\n",
      "Epoch 316/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 47.2667 - val_loss: 0.2143\n",
      "Epoch 317/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 46.0415 - val_loss: 0.2452\n",
      "Epoch 318/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 44.8475 - val_loss: 0.2943\n",
      "Epoch 319/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 43.6839 - val_loss: 0.3610\n",
      "Epoch 320/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 42.5502 - val_loss: 0.4450\n",
      "Epoch 321/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 41.4459 - val_loss: 0.5455\n",
      "Epoch 322/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 40.3704 - val_loss: 0.6620\n",
      "Epoch 323/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 39.3231 - val_loss: 0.7940\n",
      "Epoch 324/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 38.3035 - val_loss: 0.9411\n",
      "Epoch 325/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 37.3110 - val_loss: 1.1025\n",
      "Epoch 326/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 36.3452 - val_loss: 1.2779\n",
      "Epoch 327/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 35.4054 - val_loss: 1.4667\n",
      "Epoch 328/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 34.4912 - val_loss: 1.6684\n",
      "Epoch 329/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 33.6020 - val_loss: 1.8826\n",
      "Epoch 330/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 32.7372 - val_loss: 2.1086\n",
      "Epoch 331/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 31.8965 - val_loss: 2.3462\n",
      "Epoch 332/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 31.0792 - val_loss: 2.5947\n",
      "Epoch 333/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 30.2850 - val_loss: 2.8536\n",
      "Epoch 334/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 29.5133 - val_loss: 3.1226\n",
      "Epoch 335/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 28.7636 - val_loss: 3.4012\n",
      "Epoch 336/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 28.0355 - val_loss: 3.6889\n",
      "Epoch 337/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 27.3284 - val_loss: 3.9853\n",
      "Epoch 338/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 26.6420 - val_loss: 4.2899\n",
      "Epoch 339/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 25.9757 - val_loss: 4.6023\n",
      "Epoch 340/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 25.3290 - val_loss: 4.9223\n",
      "Epoch 341/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 24.7016 - val_loss: 5.2491\n",
      "Epoch 342/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 24.0929 - val_loss: 5.5827\n",
      "Epoch 343/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 23.5026 - val_loss: 5.9224\n",
      "Epoch 344/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 22.9302 - val_loss: 6.2678\n",
      "Epoch 345/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 22.3754 - val_loss: 6.6187\n",
      "Epoch 346/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 21.8376 - val_loss: 6.9747\n",
      "Epoch 347/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.3164 - val_loss: 7.3353\n",
      "Epoch 348/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 20.8116 - val_loss: 7.7003\n",
      "Epoch 349/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 20.3225 - val_loss: 8.0692\n",
      "Epoch 350/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 19.8490 - val_loss: 8.4418\n",
      "Epoch 351/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 19.3905 - val_loss: 8.8177\n",
      "Epoch 352/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 18.9467 - val_loss: 9.1965\n",
      "Epoch 353/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 18.5173 - val_loss: 9.5779\n",
      "Epoch 354/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 18.1017 - val_loss: 9.9618\n",
      "Epoch 355/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 17.6997 - val_loss: 10.3477\n",
      "Epoch 356/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.3110 - val_loss: 10.7352\n",
      "Epoch 357/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.9351 - val_loss: 11.1243\n",
      "Epoch 358/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.5717 - val_loss: 11.5145\n",
      "Epoch 359/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.2206 - val_loss: 11.9056\n",
      "Epoch 360/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 15.8812 - val_loss: 12.2972\n",
      "Epoch 361/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 15.5534 - val_loss: 12.6894\n",
      "Epoch 362/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 15.2367 - val_loss: 13.0816\n",
      "Epoch 363/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.9310 - val_loss: 13.4734\n",
      "Epoch 364/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 14.6360 - val_loss: 13.8651\n",
      "Epoch 365/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 14.3511 - val_loss: 14.2561\n",
      "Epoch 366/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 14.0762 - val_loss: 14.6463\n",
      "Epoch 367/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 13.8111 - val_loss: 15.0354\n",
      "Epoch 368/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 13.5553 - val_loss: 15.4233\n",
      "Epoch 369/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 13.3087 - val_loss: 15.8097\n",
      "Epoch 370/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.0709 - val_loss: 16.1945\n",
      "Epoch 371/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 12.8416 - val_loss: 16.5774\n",
      "Epoch 372/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 12.6208 - val_loss: 16.9583\n",
      "Epoch 373/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 12.4080 - val_loss: 17.3370\n",
      "Epoch 374/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 12.2030 - val_loss: 17.7134\n",
      "Epoch 375/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 12.0056 - val_loss: 18.0871\n",
      "Epoch 376/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 11.8156 - val_loss: 18.4582\n",
      "Epoch 377/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 11.6327 - val_loss: 18.8265\n",
      "Epoch 378/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 11.4566 - val_loss: 19.1919\n",
      "Epoch 379/500\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 11.2872 - val_loss: 19.5542\n",
      "Epoch 380/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 11.1242 - val_loss: 19.9133\n",
      "Epoch 381/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.9674 - val_loss: 20.2690\n",
      "Epoch 382/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 10.8167 - val_loss: 20.6213\n",
      "Epoch 383/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 10.6718 - val_loss: 20.9699\n",
      "Epoch 384/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 10.5326 - val_loss: 21.3149\n",
      "Epoch 385/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 10.3988 - val_loss: 21.6561\n",
      "Epoch 386/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 10.2702 - val_loss: 21.9934\n",
      "Epoch 387/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 10.1467 - val_loss: 22.3268\n",
      "Epoch 388/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 10.0281 - val_loss: 22.6563\n",
      "Epoch 389/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.9142 - val_loss: 22.9814\n",
      "Epoch 390/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.8049 - val_loss: 23.3026\n",
      "Epoch 391/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7000 - val_loss: 23.6194\n",
      "Epoch 392/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.5993 - val_loss: 23.9319\n",
      "Epoch 393/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 9.5028 - val_loss: 24.2401\n",
      "Epoch 394/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.4101 - val_loss: 24.5437\n",
      "Epoch 395/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.3213 - val_loss: 24.8431\n",
      "Epoch 396/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.2361 - val_loss: 25.1380\n",
      "Epoch 397/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.1544 - val_loss: 25.4283\n",
      "Epoch 398/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.0762 - val_loss: 25.7141\n",
      "Epoch 399/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 9.0012 - val_loss: 25.9952\n",
      "Epoch 400/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 8.9293 - val_loss: 26.2719\n",
      "Epoch 401/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 8.8605 - val_loss: 26.5439\n",
      "Epoch 402/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 8.7945 - val_loss: 26.8112\n",
      "Epoch 403/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 8.7314 - val_loss: 27.0739\n",
      "Epoch 404/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 8.6709 - val_loss: 27.3319\n",
      "Epoch 405/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 8.6131 - val_loss: 27.5855\n",
      "Epoch 406/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 8.5576 - val_loss: 27.8341\n",
      "Epoch 407/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 8.5047 - val_loss: 28.0783\n",
      "Epoch 408/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 8.4539 - val_loss: 28.3179\n",
      "Epoch 409/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 8.4053 - val_loss: 28.5526\n",
      "Epoch 410/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 8.3589 - val_loss: 28.7829\n",
      "Epoch 411/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 8.3145 - val_loss: 29.0085\n",
      "Epoch 412/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 8.2721 - val_loss: 29.2297\n",
      "Epoch 413/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 8.2314 - val_loss: 29.4462\n",
      "Epoch 414/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 8.1926 - val_loss: 29.6583\n",
      "Epoch 415/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 8.1555 - val_loss: 29.8658\n",
      "Epoch 416/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 8.1200 - val_loss: 30.0689\n",
      "Epoch 417/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 8.0860 - val_loss: 30.2674\n",
      "Epoch 418/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 8.0537 - val_loss: 30.4616\n",
      "Epoch 419/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 8.0227 - val_loss: 30.6517\n",
      "Epoch 420/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.9931 - val_loss: 30.8373\n",
      "Epoch 421/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.9648 - val_loss: 31.0187\n",
      "Epoch 422/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.9378 - val_loss: 31.1958\n",
      "Epoch 423/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.9119 - val_loss: 31.3685\n",
      "Epoch 424/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.8873 - val_loss: 31.5373\n",
      "Epoch 425/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.8638 - val_loss: 31.7020\n",
      "Epoch 426/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 7.8413 - val_loss: 31.8628\n",
      "Epoch 427/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 7.8198 - val_loss: 32.0193\n",
      "Epoch 428/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 7.7993 - val_loss: 32.1722\n",
      "Epoch 429/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.7797 - val_loss: 32.3210\n",
      "Epoch 430/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.7611 - val_loss: 32.4660\n",
      "Epoch 431/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.7432 - val_loss: 32.6071\n",
      "Epoch 432/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.7262 - val_loss: 32.7447\n",
      "Epoch 433/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.7099 - val_loss: 32.8783\n",
      "Epoch 434/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.6944 - val_loss: 33.0084\n",
      "Epoch 435/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 7.6796 - val_loss: 33.1351\n",
      "Epoch 436/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 7.6655 - val_loss: 33.2583\n",
      "Epoch 437/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.6519 - val_loss: 33.3781\n",
      "Epoch 438/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.6391 - val_loss: 33.4945\n",
      "Epoch 439/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.6268 - val_loss: 33.6075\n",
      "Epoch 440/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.6150 - val_loss: 33.7176\n",
      "Epoch 441/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.6038 - val_loss: 33.8239\n",
      "Epoch 442/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.5932 - val_loss: 33.9274\n",
      "Epoch 443/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.5830 - val_loss: 34.0281\n",
      "Epoch 444/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.5732 - val_loss: 34.1255\n",
      "Epoch 445/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.5639 - val_loss: 34.2200\n",
      "Epoch 446/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.5551 - val_loss: 34.3113\n",
      "Epoch 447/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.5466 - val_loss: 34.4002\n",
      "Epoch 448/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.5385 - val_loss: 34.4862\n",
      "Epoch 449/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.5308 - val_loss: 34.5696\n",
      "Epoch 450/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.5234 - val_loss: 34.6504\n",
      "Epoch 451/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.5164 - val_loss: 34.7286\n",
      "Epoch 452/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.5097 - val_loss: 34.8041\n",
      "Epoch 453/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.5034 - val_loss: 34.8772\n",
      "Epoch 454/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 7.4973 - val_loss: 34.9480\n",
      "Epoch 455/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4914 - val_loss: 35.0165\n",
      "Epoch 456/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4859 - val_loss: 35.0826\n",
      "Epoch 457/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4805 - val_loss: 35.1464\n",
      "Epoch 458/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4755 - val_loss: 35.2081\n",
      "Epoch 459/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4707 - val_loss: 35.2678\n",
      "Epoch 460/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4661 - val_loss: 35.3250\n",
      "Epoch 461/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 7.4617 - val_loss: 35.3807\n",
      "Epoch 462/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 7.4575 - val_loss: 35.4341\n",
      "Epoch 463/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4535 - val_loss: 35.4860\n",
      "Epoch 464/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4497 - val_loss: 35.5355\n",
      "Epoch 465/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4461 - val_loss: 35.5833\n",
      "Epoch 466/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4426 - val_loss: 35.6295\n",
      "Epoch 467/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4393 - val_loss: 35.6741\n",
      "Epoch 468/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4362 - val_loss: 35.7169\n",
      "Epoch 469/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4332 - val_loss: 35.7583\n",
      "Epoch 470/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 7.4303 - val_loss: 35.7980\n",
      "Epoch 471/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4276 - val_loss: 35.8363\n",
      "Epoch 472/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4250 - val_loss: 35.8732\n",
      "Epoch 473/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4225 - val_loss: 35.9083\n",
      "Epoch 474/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4201 - val_loss: 35.9421\n",
      "Epoch 475/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4179 - val_loss: 35.9745\n",
      "Epoch 476/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 7.4158 - val_loss: 36.0060\n",
      "Epoch 477/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4137 - val_loss: 36.0361\n",
      "Epoch 478/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4118 - val_loss: 36.0649\n",
      "Epoch 479/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4099 - val_loss: 36.0924\n",
      "Epoch 480/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4081 - val_loss: 36.1192\n",
      "Epoch 481/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4065 - val_loss: 36.1448\n",
      "Epoch 482/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4049 - val_loss: 36.1687\n",
      "Epoch 483/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4034 - val_loss: 36.1922\n",
      "Epoch 484/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4020 - val_loss: 36.2145\n",
      "Epoch 485/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.4006 - val_loss: 36.2358\n",
      "Epoch 486/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 7.3993 - val_loss: 36.2564\n",
      "Epoch 487/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.3981 - val_loss: 36.2760\n",
      "Epoch 488/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 7.3969 - val_loss: 36.2948\n",
      "Epoch 489/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 7.3958 - val_loss: 36.3127\n",
      "Epoch 490/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.3947 - val_loss: 36.3299\n",
      "Epoch 491/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.3937 - val_loss: 36.3463\n",
      "Epoch 492/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.3928 - val_loss: 36.3622\n",
      "Epoch 493/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.3919 - val_loss: 36.3773\n",
      "Epoch 494/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.3910 - val_loss: 36.3913\n",
      "Epoch 495/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.3902 - val_loss: 36.4048\n",
      "Epoch 496/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.3895 - val_loss: 36.4178\n",
      "Epoch 497/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.3888 - val_loss: 36.4300\n",
      "Epoch 498/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.3881 - val_loss: 36.4416\n",
      "Epoch 499/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.3875 - val_loss: 36.4526\n",
      "Epoch 500/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.3869 - val_loss: 36.4634\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(89.3180, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0010, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(-0.0204, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.2194, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 423ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[73.52208217, 73.47285948, 73.43056256, 73.38826564, 73.34596872,\n",
       "        73.3036718 , 73.26137488, 73.21907796, 73.17678105, 73.13448413,\n",
       "        73.09218721, 73.04989029, 73.00759337, 72.96529645, 72.92299953,\n",
       "        72.88070261, 72.8384057 , 72.79610878, 72.75381186, 72.71151494,\n",
       "        72.66921802, 72.6269211 , 72.58462418, 72.54232726, 72.50003035,\n",
       "        72.45773343, 72.41543651, 72.37313959, 72.33084267, 72.28854575,\n",
       "        72.24624883, 72.20395191, 72.161655  , 72.11935808, 72.07706116,\n",
       "        72.03476424, 71.99246732, 71.97971289, 71.96878852, 71.95786415,\n",
       "        71.94693978, 71.93601541, 71.92509104, 71.91416667, 71.9032423 ,\n",
       "        71.89231793, 71.88139356, 71.87046919, 71.85954482, 71.84862045,\n",
       "        71.83769608, 71.82677171, 71.81584734, 71.80492297, 71.7939986 ,\n",
       "        71.78307423, 71.77214986, 71.76122549, 71.75030112, 71.73937675,\n",
       "        71.72845238, 71.71752801, 71.70660364, 71.69567927, 71.6847549 ,\n",
       "        71.67383053, 71.66290616, 71.65198179, 71.64105742, 71.63013305,\n",
       "        71.61920868, 71.60828431, 71.59526144, 71.57565359, 71.55604575,\n",
       "        71.53643791, 71.51683007, 71.49722222, 71.47761438, 71.45800654,\n",
       "        78.06593323,  0.        ,  0.23394889,  0.        ,  0.16414489,\n",
       "         0.63008541,  0.4376162 ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.15371898,  0.508596  ,\n",
       "         0.58503479,  0.        ,  0.        ,  0.        ,  0.35463122]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([69.4377451 , 69.43120915, 69.4246732 , 69.41813725, 69.41160131,\n",
       "       69.40506536, 69.39852941, 69.39199346, 69.38545752, 69.37892157,\n",
       "       69.37238562, 69.36584967, 69.35931373, 69.35277778, 69.34624183,\n",
       "       69.33970588, 69.33316993, 69.32663399, 69.32009804, 69.31356209,\n",
       "       69.30702614, 69.3004902 , 69.29395425, 69.2874183 , 69.28088235,\n",
       "       69.27434641, 69.26781046, 69.26127451, 69.25473856, 69.24820261,\n",
       "       69.24166667, 69.23513072, 69.22859477, 69.22205882, 69.21552288,\n",
       "       69.20898693, 69.20245098, 69.19591503, 69.18937908, 69.18284314,\n",
       "       69.17630719, 69.16977124, 69.16323529, 69.15669935, 69.1501634 ,\n",
       "       69.14362745, 69.1370915 , 69.13055556, 69.12401961, 69.11748366,\n",
       "       69.11094771, 69.10441176, 69.09787582, 69.09133987, 69.08480392,\n",
       "       69.07826797, 69.07173203, 69.06519608, 69.05866013, 69.05212418,\n",
       "       69.04558824, 69.03905229, 69.03251634, 69.02598039, 69.01944444,\n",
       "       69.0129085 , 69.00637255, 68.9998366 , 68.99330065, 68.98676471,\n",
       "       68.98022876, 68.97369281, 68.96715686, 68.96062092, 68.95408497,\n",
       "       68.94754902, 68.94101307, 68.93447712, 68.92794118, 68.92140523,\n",
       "       68.91486928, 68.90833333, 68.90179739, 68.89526144, 68.88872549,\n",
       "       68.88218954, 68.87565359, 68.86911765, 68.8625817 , 68.85604575,\n",
       "       68.8495098 , 68.84297386, 68.83643791, 68.82990196, 68.82336601,\n",
       "       68.81683007, 68.81029412, 68.80375817, 68.79960317, 68.79866947])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.080067118701237\n",
      "15.567693123604705\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
