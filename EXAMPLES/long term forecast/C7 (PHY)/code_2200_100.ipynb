{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       92.600000\n",
       "1       92.387115\n",
       "2       92.174230\n",
       "3       91.961345\n",
       "4       91.748459\n",
       "          ...    \n",
       "2295    63.960976\n",
       "2296    63.953324\n",
       "2297    63.945672\n",
       "2298    63.938020\n",
       "2299    63.930368\n",
       "Name: C7, Length: 2300, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c7_interpolated_2200_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       92.600000\n",
       "1       92.387115\n",
       "2       92.174230\n",
       "3       91.961345\n",
       "4       91.748459\n",
       "          ...    \n",
       "2195     0.279349\n",
       "2196     0.000000\n",
       "2197     0.000000\n",
       "2198     0.481617\n",
       "2199     0.000000\n",
       "Name: C7, Length: 2200, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2200)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92.387115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92.174230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.961345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91.748459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     92.600000  0.000298\n",
       "1     92.387115  0.000298\n",
       "2     92.174230  0.000297\n",
       "3     91.961345  0.000297\n",
       "4     91.748459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAAsTAAALEwEAmpwYAAAoiUlEQVR4nO3deXxc5X0u8Oc3o31fLGvHsvG+YhBgwhYWG0NIICnNpc1CE3q5/TRpSclSktx+CA1NbpPebLcpKQkk5F4IJYR9jSGY3TYy3iUb2/ImS7Jla7Vk7e/9Y86MZjQzmnPOnDmL9HzzIdKMzvLOsfTMO+95F1FKgYiIvMfndAGIiMgcBjgRkUcxwImIPIoBTkTkUQxwIiKPSrPzZLNmzVJ1dXV2npKIyPO2bt16SilVNvl5WwO8rq4ODQ0Ndp6SiMjzRORIrOfZhEJE5FEMcCIij2KAExF5FAOciMijGOBERB7FACci8igGOBGRR3kiwJ/b0YpHNsfsBklENGN5IsBf3t2OH2/Yj7Fxzl1ORBTkiQBfv7wCp84MYdvRLqeLQkTkGp4I8KsWz0aG34eXd7c7XRQiItfwRIDnZabh8gWz8NLudnAJOCKiAE8EOBBoRjnefRZv7j/ldFGIiFzBMwH+8VVVmDsrF995dg+GRsecLg4RkeM8E+BZ6X7c+4llOHSqHw+80ex0cYiIHOeZAAeAKxaW4WMrKvHvrx/Asc4Bp4tDROQoTwU4APzPG5fA7xPc+sAm/G7LUQyPjjtdJCIiR3guwCsLs/GbL1yEsvxMfPPJXbjq3zbi0c0MciKaecTObnn19fXKqiXVlFJ448MO/OTV/dh+rBvVRdn4zJpzcPHcUiyvLkBmmt+S8xAROU1Etiql6ic/b+uamFYSEXx00WxcubAMb+4/hZ+9th8/eHkfACAjzYfzaopQX1eMG1dWYWlVgcOlJSKynmdr4LF09A1h65EuNBzuxPtHurDneA9GxxU+uboad61diNqSnJSdm4goVeLVwKdVgE/WMzCCX7x5EA+9fQhKAZ+/ZA6+dNV8FOdm2FYGIqJkzcgAD2rtPosfb/gQT3zQgrzMNPzZ+TVYM68UF88tYZgTkevN6AAP2tfehx9t2Ic3PuzA4Eig18riinysmVeKS84NBHpRDgOdiNyFAR5meHQcO1u6san5NN5rPo2tR7owODIOEWBxRQHWzCvBJfNKcREDnYhcgAE+haHRMexs6cGmg6ex6dBpNBzuwtBoINCXVBRgzbxSrJlXggvr2ORCRPZjgBswNDqGHcd6sKn5NDZpNfQhbaDQwvI8XFhXgovmBv6rLMx2uLRENN0xwJMQDPT3D3di86FOfHCkC2eGRgEANcXZgTCvK8GFc0swb1YuRMThEhPRdMIAt9Do2Dj2tvdhy6FObDnUifcPd+J0/zAAYFZeBi6sK8FHzi3FdcsrMDs/y+HSEpHXMcBTSCmF5lP9gTA/1IkthzvR0nUWIsDFc0tw48oqXL+8AqV5mU4XlYg8iAFus/0n+vD8zjY8v7MVBzv64fcJLplXihtXVuK6ZRW8GUpEujHAHaKUwt72Pryghfnh0wNI8wkunT8LN66sxLplFSjMTne6mETkYkkFuIj8A4C/BqAA7ALwBQCVAB4DUApgK4DPKaWGpzrOTAzwcEop7GntxXM7W/HCzja0dJ1Ful9wxYIy3LiqEtcuKUd+FsOciCKZDnARqQbwNoClSqmzIvI4gBcB3ADgSaXUYyLyCwA7lFL3T3WsmR7g4ZRS2NHSg+d3tOKFXW1o6xlERpoPVy4sw/KqQswpzdH+y0VxTjp7thDNYMlOJ5sGIFtERgDkAGgDcDWAv9R+/jCA7wCYMsBpgojgvNoinFdbhG/dsATbjnXhuR1teLXpBDY0nojYNj8zDedogX5OSS7qSnO0x7moLMiCz8dwJ5qJ9Dah3AngXwCcBfBHAHcC2KSUmq/9vBbAS0qp5TH2vQPAHQBwzjnnXHDkyBHrSj9NDY6M4VjnAI6cHsCRzgEcPd2Pw6cHcLRzAC1dAxgZm/g3y/D7UFOSjbrSXJxTkoPakhzMystAcU4GSnIzUJybgZKcDGRncIELIq8yXQMXkWIANwGYC6AbwO8BrNd7YqXUAwAeAAJNKHr3m8my0v1YUJ6PBeX5UT8bG1do7T6Lo8GAP90fCvrNzafRPzwW55g+lOZmojg3fSLcw0K+LC8DiyoKMKckhzV6Io/Q04RyLYBDSqkOABCRJwFcCqBIRNKUUqMAagAcT10xKcjvE9RqNe1L50f+TCmF7oERdA4Mo6t/GJ39w+gaGMbp/uDjEXQNBJ4/2jmAzv5h9A2ORhwjLzMNS6sKsLyqEMurC7CsqhDnluUize+55VOJpj09AX4UwBoRyUGgCeUaAA0AXgdwCwI9UW4D8EyqCkn6iAiKtRo1yvTtMzw6ju6BYZzoHUJTWy92t/Zg9/EePLrlSGjK3cw0H5ZUFmB5dTDYC7GgPI/rjk5Dr+89iRU1hZjl0kFnnf3D2H6sC1cvLne6KK6gtw38XgD/DcAogG0IdCmsRiC8S7TnPquUGprqOOyF4h1j4wrNHWewu7UHe473hr72aXPApPsFC2bnh2rptSXZqCjIRkVhFnvNeNTgyBgW/9PLWFpZgBfvvFz3fv1Do/jNu4fxN1eeC3+Km99u+ve3saOlB7vvvQ55mfYv6Ts6No7/fLMZt182F1np9lVgkuqFopS6B8A9k55uBnCRBWUjF/L7JNQO/8nVgefGxxWOdQ1g9/GJmvqrTSfxeENLxL4ZaT5UFmahvCALlYVZqCjMQkXo+2xUFGShLD8z5X/sZMzIWOAT15HT/Yb2+/5LTfh/m46irjQXH1tZqXu/lq4B3Pd8E35y63m6w/BgR6Bs4zYNQPzVW83Iy0zDrRedAwB4vKEFP3xlHwaGR/H16xbbUoapeHZVerKfzyeYU5qLOWF/qEopnOwbQmv3WbT3DKKtZxAnegNf23sGse1oN9p7BjGshUOQ3yeYnZ+J8oIszJ2Vi8UV+VhcWYAlFfkoy89kDd4B41omGr2JHbyPMjLp3ziR7z7fiFf2nMDGfSexfrm+4A8Gt9+m34/7XmgCgFCADwwHXuvZYWOvNVUY4JQUEUF5QaC2HY9SCp39w2jvHYwZ8pubT+OpbRP3wEtyMwKBXlGAxZX5WFJRgAXlebZ+ZJ2JxrUE9xkMx2DwG83Uif307zhmsoxWCVb83fLhkQFOKSciKM3LRGleJpZVFcbcpmdgBE3tvdjb1ou97X1oau/D77YcxdmRQLdInyBQU9dq6YsrCnD+nGKUcFIwywRrt0bDKXgfzeinJqWMh7Ey+WZhldA1ckmCM8DJFQpz0rWl60pDz42NKxztHMDetl40tfdhb1svdrZ044WdbQACQVM/pwTrlpVj7dJyzCnNdar400KoCcVwEAe+Go20cRO12WCA2jgHX4Tgad0R3wxwcjG/TzB3Vi7mzsrF9Ssm2kj7Bkewt70Pb+0/hT/uacd9LzThvheasLA8D2uXlmPt0gqsrC50TS3JK5TJ2qWC2aaXYM3d+D5OUS5LcAY4eU5+VjourAssMn3X2oU41jmADY0n8MfGdvzijWb8/PWDKC/IxLVLAjXzS84tZZ91HcZMNqGMa/fzjDZrKBNt4OPO5ndYM5M7EpwBTp5XW5KDL142F1+8bC66B4bxp70nsaHxBJ7adhyPbD6KvMw0XLmoDOuWluOji2Zz/vU4guFotIfHRA3c6PnMh+GLu9rwZxfUGN4vWUrnm9yrjSewoDwv5c16DHCaVopyMvCp82vwqfNrMDgyhncPnsKGxhPY0HgSL+xsQ5pPcPG8Ely1aDbOqy3CsqrCaT3R19i4wref2oUL5hTjlgtqpqztBnuhGL0ZaaY3SWA/fV0CH3r7ENYtK0dNcU7oua/+fgc+ubo61NzzvRebUD+nGOuWVRgqg1GJ7hO0dA3gjQ878O2ndiPNJzjwvRtSWh4GOE1bWel+XL24HFcvLse/3Kyw7Vh3qKkl2L/X7xMsKs/HqtpCrKopwqraIiyYnTdt5n45dWYIj71/DI+9fwzP7WzDfTctxzmlOTG3DXWRM/jSQ71QDJYt2PQyVW22Z2AE//x8I97c34HffCFy3GB4hv5u81E8ve04rlhYltLupuMJXutdj+/AlkOdAIBRG9p7GOA0I/h8ggvmFOOCOcW4+/rFONk7iB0tPdhxrBs7tJ4tv9tyDACQne7HiupCrKwpxCptzvaa4mxPDi4KhsjlC2Zh6+FOrP3xG/j7axbgv18+DxlpkUk9ZrJJw0xbNhB2Q3KK3Ua0lN+4rwObmk/H3W50PDCg7JHNR3H7ZXMNlcOIRK81zeYb5wxwmpFmF2Rh7dIsrF0amBRJKYXDpwew41g3th/rxs6Wbvx20xEMv30IQGBw0cqaQqyoLkRZfiYKstJRkJ2Ggqx0FGanoyA7HQVZ6chK97kq6Me0ueNvPq8aP7xlFe59bg9++Mo+PLXtOO64Yh6WVxVi/uw8ZKT5Ipo0egdH8E9P78bq2iKsqCnC4op85MaZeyRYzzTcfzy0X/wdx8Jqsd9/aW/C7b77fCPu33gQFYWZqCvNxdDoOIqy0zG7IBP/48pzURC2ZOHYuMI3ntiJm1dX4fIF+mZ/Uwl6zswry8W7B+O/0ViNAU6EQI0q2GXx5tXVAAIzNX54og/bj3Vjx7Fu7GzpwRsfdkzZBzndL1q4p6MgKy0U7MGwL8hOR1FOemBemIJsVBZmoSiFk3+NajXYNL+gojAL93/2Avxp7wnc8+wefOOJnaEyz5+djwy/aNcC2H28B89sb8Uz21tDz80tzcWSqgIsrSzAZfNnYYXWVTP8ZmRTWy++92ITllQW4IoFZbh4XgnS4zRH6RnIE/wEccm8Urw3qQau1ESQjo6PY+3ScvQPjWJ0TGHL4U7sPt4Lv09QkpuBjr4hLK8qDHVH/cYTO9Dc0Y+GI134wwct+NjKStxz41L0DY3ip6/ux9evW4TakuimpuA//U9e3Y+L6krwkfmzIn6em2FvpDLAieLISPNheXVg+tzPrpkDABgaHUPv2VH0Do6g9+wIegdHta8jcZ9v7T4bejw0Gj2HRqY2+Vdw0q+KwuzQ40rtuVl5mab6tQdrpuETh129uBxXLpyNQ6f60djWi6a2XjS29obabsOnRfjRp1ehICsdjdo2u1p68MLONvzwlX2YnZ+Ja5bMxo5j3QACYdpwuBNv7T+Fdw+exgNvNqMoJx1rl5Rj/fIKXLZgVkR3zvCBPMc6B/Be82lcu6Q8YnTtqDa/yi0X1OCudQvx5794L+o1jo8rjCtgWVUBvnLtQry4qw1bDgdey6//6kLUluTgqn/biMHRicVOwidgm5WXgdeaTmD70W584rwqPLujFc/uaMXGr3004jxKKfyfPx0IPf7LX23G/Z85H9evqMSJ3kE8uvkoHt1yNPE/ioUY4EQGZKb5UZbvR1m+ufmyB0fGouaFae85G5oXpuFIF070tkUsmwdETt+7vLoQy6oKsaQyHzkJanzBGuzktlm/TzB/dh7mz87DJ1ZVAQgE4bxvvYiL506Mhq0qysaaeaW4dunE/Nud/cPYuO8kXms6iae3tYamOwj/FLHxax/FntZevLKnHS/vacfvt7agOCcdd61diL+46Byk+X0T85r4BL9+5zAeeucQMvw+fP6SOfj7axegICt9ovx+wYV1JfjWDYvxvRcjm1KCbffx2p8ztbb+4bA3z7zMNJzRpka+89qFWF1bhFt+8S4efOtQaJu/+OWmiOPEWu3qS49+gCf/9lL8cU87/mPjwZjnTyUGOJGNstL9qCrKRlVRdtxtxscVTvcPawF/Fu29g2jpOoumtl5saDwRqj36BJhXloflVROhvrSqIKKf++hYMNys61VTkjvRVfMf/mt7aCKy8PjMzvBj/fIKrF9egeHRcbxz8BQeeKMZ//TMHjyy+Sju+fiyiN4rY+PjyMnw48aVlXjwnUN4evtxfP26RVhVW5Sw/BOfMmJvE7xZG+vTT9Dy6kJUFWWjWZuu9v7PnI+/ffSDKa/DLz9fj28+uQv3Pd+I1ecUISvdh7zMNJw6MzzlflZigBO5jM8nKMvPRFl+JlbURE7+pZRCW88gdh/vwZ7WXuxp7cGm5k48rbVVA8Cc0hwsqwostBEMcL/f5B3GBPKzJiIkXlt2RpoPVy2ajY8uLMPLuwNdOMNrt8H9MtJ8+MEtq/C5NXX4znN78I9/2IVq7Y0u3tzx9z3fiNf3nQQQvwYeCvCRqaeAzQ+7SbuqtgifXF2NJz+Iv1JkXmYavrZuIe5+chfaewchELxz99V4dPNR3Ptc45TnsgoDnMhDRCRUgw8ftHLqzBD2tPZqwR4I9xd3tYd+nmVgKgEVlt6JYn/9sgr89r0jAAKfCKbKfRHB9SsqcdXi2fjPN5rx41c/BBAdzitqCvHE31yCZ3e04vtac0lOjMFWCsDe9r7QIg85mf6oMosAeRlpyE73o61nMHa5tK95WZFx+LV1i6YMcAD48/paPPTOIXx44gyy0/3ITPOjOMe+GTIZ4ETTwKy8TFy5sAxXLpzoDtdzdgSNrb1o7z2L+rrihMcw0xEmoheGzv2z0v2489oFyM30474XmpDml6jgFxHcdF41rl1SjrcPnMKlk3p7hFtVW4Svrl0Y9zX6fILygkycOjPlio/Iz5xoehIBKguzUZSTjlptBOjk5SdFAm8+iysK8OGJMxHP24UBTjRNFWan45JzSxNvaBEJS3A9GRZsHpmqW2ZuZhquC/ukITGOnO4TXLFw6n7cBdnp6Dk7MuU2RTnRc+ScW5aHrHR99w+c6P4/PcYLE5GljAwCX6f1UDE6p8zkwDOTf0pnSTPTfAmXfKsojF5Vyj1DsmJjgBNRXHoGGH26vhZAYASnndN1T27SiCVWjX3yvsGX6JYpYo1ggBNRBCfWTLDynFbkcKzgj2qnt/B8ZjHAiQiA8cmoJovovWJjquk9VfibRCrK50SOM8CJKIqRGnGyWWhkNsPJm9jxacHhVdymxAAnorhSW5G2sZY+6VwRbeDaz2KVxu3N4gxwIrKMnhuLlp0r7k/CujOmMIDdMG0wA5yIIlgRwUajTW93wNjnMh6kicI31o/jvTdN3MyUiK92YIATkSXc3FYMJPcm4VYMcCKKYiTskr6JCXNragIG3jR0HDze63DzGxMDnIjiMt4UYuDYFvVemfIcSZZjquYZFzSBM8CJaBIHqpxJnXLykHxLBvJMONk3iK7+4bjvTpPbvO3MdQY4EYUkE37h+eaG2ulkZt8kDnb0Y/V3NyTcjgN5iMgVDA3kSTK6Jgby6DjXpI30ttXra0px4btOAgxwIorLaKYZC37nRA7kmYLLM11XgItIkYg8ISJ7RaRJRC4RkRIR2SAi+7WviWeMJyKKwUzrRrzeKxL3gQGx+oFPKqXE39Q2emvgPwXwslJqMYBVAJoA3A3gNaXUAgCvaY+JyOOsGcjjvqprZBt9CsonwWNbf+h4Ega4iBQCuALAgwCglBpWSnUDuAnAw9pmDwO4OTVFJCK7BLPHVI046d4rxpNP7yk92Lyti54a+FwAHQB+LSLbRORXIpILoFwp1aZt0w6gPNbOInKHiDSISENHR4c1pSYim+i5s5j6Ulh9qlgLOniRngBPA3A+gPuVUqsB9GNSc4kKXI2Y74VKqQeUUvVKqfqysqnXrSMibzM2kGciOc1U3uP1Xgk/brJNOVPtLS5oBNcT4C0AWpRSm7XHTyAQ6CdEpBIAtK8nU1NEIrKTJeN43FirNbCgQ8wVeRJcl4k8d9FkVkqpdgDHRGSR9tQ1ABoBPAvgNu252wA8k5ISEpFtgsFm57SwE+c2vo/+qVCMLxbhBWk6t/s7AI+ISAaAZgBfQCD8HxeR2wEcAfDp1BSRiJxiJNRcPOfTlNzYY0YvXQGulNoOoD7Gj66xtDRE5Dnh8Wem5p7K5dvsWFHIyTcAjsQkIkdMHhRv/jj6JpMyM0Vu+BtAor0nFnTQfZqkMcCJKIIVCx/YFmLsB05EFJDcQB5rzu0UL7aFM8CJKC63zeKXioE8Zg8aq5nFbgxwInKUgrJ4IE/Y9zG21yNWn+5EN2idCHIGOBFZLtmlzPTSPR/4FKM1rcYVeYjIMfEnxkgtp280On1+MxjgRBSSXE3VO0N5dC/ooIMX5gMnIorJbDtz+D76p4WNjsspV5UPnzDLaMESlSXBYzswwInIcnpq7sk2WcS7+RnruFGDfRJNZhVjUI7ueVc4kIeInBJoAndgMiuH+2Gn4uynzgyl4KgTGOBEFKJ3WLoXbT3Sha88ti3q+WR7pEy1+6f+492kjp0IA5yILKGU+2vuT29vte68OoL/aOeAZeeLhQFORElJvi3bXH/uwBtGjG3CAt3saMlYmydc0MGBJXoY4ERkOV0DeaJuLKamLHrFvPnp8jYkBjgRRTDSrY84HzgRucUU84ok4lTmm1qKzYKBPG6onDPAiSgpkRM+mTuG+f1idQSP+W3SErXTcyAPEU0LhmruBtI7YtSn/lNEHyfWiM5YNz9NxDIH8hCRo2xpA5/cXGPDKacbBjgRRQhvKnB6dKQdzNaYuaADEblKMlkUr1+2rn2TOO9UJkI29SnLBR2IyHOsGMhj5mRKxb6taLpXScTNzxhH0Ttjosnzm8EAJ6IoydaI9TS9RE3H6sJRMy4sUgQGOBFFCl/v1+UBZoVkXyPbwInIFZIJI6XMLU4c2Nf8eZ2quUd/yrC/HAxwIvIkhdjBHxnoEvb/BphvArcVA5yIopgdXBN6ztBAHv3beqFFx85PBAxwInKEW25aJt/XnZNZEZFLuLGpYCpm4tPIJ4y4540z8ZcVx9aLAU5EIcnURpNZS9P8ftEiWsANjJZM9InAjVPsMsCJKDkWtSAYbVFJZc+VZFp32AZORI6yt7Jp4Iapzc3NesLYE/3ARcQvIttE5Hnt8VwR2SwiB0Tkv0QkI3XFJCK7RCx2kMJwsurQbhmI4/b5wO8E0BT2+F8B/FgpNR9AF4DbrSwYEdkv6XlN7J7NSucJJ78sO280ppKuABeRGgAfA/Ar7bEAuBrAE9omDwO4OQXlIyKPsD0T45wvclIq/aKH/0w+nftCX28N/CcAvgFgXHtcCqBbKTWqPW4BUG1t0YjIKcYG18RY3cZETd4l3cINc7LYCQNcRG4EcFIptdXMCUTkDhFpEJGGjo4OM4cgIofYsaBD0m8WFpbFyLHd8IajpwZ+KYBPiMhhAI8h0HTyUwBFIpKmbVMD4HisnZVSDyil6pVS9WVlZRYUmYhSya6mkPAATHXT+eTeJKl4ia4cyKOU+qZSqkYpVQfgVgB/Ukp9BsDrAG7RNrsNwDMpKyUR2SKpFXlc0kYcuTix/lcU0XYeYz833vdMph/4PwK4S0QOINAm/qA1RSIi5yXXN9vUau4G93HNG0bUkHr72lbSEm8yQSm1EcBG7ftmABdZXyQicgs7sshIDKeiPG6ZVMsMjsQkIkdYcYNUKZ2jJWPsp8dUh55cfjtu+E7GACeiCMk0TJi9gZeqG39m+4HH4o4Gm0gMcCIKCdZmTeVpkmtpWjWZlRUtIsaCn/OBE5EL6ZqGNfXFSCkvl58BTkSOSmZJNQWlK4DN1sqNDORx4l4oA5yIIiTTHG0ojC0YyGNWrC6IXuyNwgAnohAnI8yy6WXDvzdb8461Kn2cdydPzAdORDNHkvcwbXkjcGOvELsxwIkoLj09LJxsegj0A9ezZfyNrKqlu31BByKiKZmquStlycAaY+fUd2wzb06umsyKiGYWu+YYSVmNNWJBB+vOErffuWVnMI4BTkQTQlOiGt9VRQzkMTGZlcF9UlnTTSb4uSo9EbmCroE8DlZBAxme3MrxZsM6ai4UBy4EA5yIHOVEb5JY5/ReL3AGOBFZyFCrhomBPHbPO+L2UGeAE1EEpZK/kWkm+KwbyBO+Io/JYxjYz8lulAxwIgqZHEVGosnuFXKCZ0t2wi0PjqAPYYATUVKczD9L3zIMJrkbgp8BTkSWMVMLV8rewS+Bk+rbzA0hPRUGOBE5IuqGpK55YXVskqLQnW6r0hPRNJPUijwRx7GgMAmYnbp2OmGAE1FcRoLPiRqqUvoWdJiKTPpqdL/QYy7oQEReY0VwuWFBB8uOzcmsiMiLTM2hYiBMjTaTJ//mMnGAeOXkgg5E5BpKJV8/1TO4xc75tI2O4EwmlDmZFRE5Ijp79IeRU80gyealZQs6sA2ciLzHe43gehd0cDsGOBFZxoVdpS0Tf0EHzoVCRC6hYPPISO1U+trNjW2T7Fzfbq+VM8CJKMTOvs1JH1ppixonuaCD+ZJMWtDBgZo4A5yILGH7fCYWc7IpxCwGOBElJaJ2azLEnZqK1grsB05EM1ZoXm+LjhceqFYu6ODGzxcMcCKKELG6vIn99YamqZXrJz1WUEkvvDz1gsf6ubIfuIjUisjrItIoIntE5E7t+RIR2SAi+7WvxakvLhGlkpPLgxlldY3Y6Et3w6XSUwMfBfBVpdRSAGsAfElElgK4G8BrSqkFAF7THhPRDJVMoNq+noOFJ3QyxxMGuFKqTSn1gfZ9H4AmANUAbgLwsLbZwwBuTlEZicjFIu5hJnOcFCShlcd0Yy8bQ23gIlIHYDWAzQDKlVJt2o/aAZTH2ecOEWkQkYaOjo5kykpENkkmq4xmZjLnUkrfpFiGJ7MKfjWyOr2hM1hDd4CLSB6APwD4ilKqN/xnKvDWFPOfQSn1gFKqXilVX1ZWllRhiSj1wrv0pbJN3Myhw/exqkJs1YIOTtAV4CKSjkB4P6KUelJ7+oSIVGo/rwRwMjVFJCK7JBVKSdak7TRj+oFL4C34QQBNSqkfhf3oWQC3ad/fBuAZ64tHRG4XXkt3YTOxZdz40tJ0bHMpgM8B2CUi27XnvgXgfwF4XERuB3AEwKdTUkIisl0yIyPNNruYGcoeq5QRA3mCxzTcRdBEH3UHquIJA1wp9Tbiv/xrrC0OETkt2YE8hs9nZEm1GAVKdg6TYPAaPbYb+sxzJCYRhdiZScmeyu75U+JxMsgZ4ERkCaX9z+y+dkplWz1XpSciRxnJoFRMQqVX7LAMW9DBXBN4zO3deIOWAU5EcZnqq21w+6SDMdlFjUNfzQ32iToOV6UnIqfYVdE09+Zgb5dFXTMdpr4YcTHAiSiMM3HkxuYJL2CAE1EUM4GqlHtHVFrTrOG+dxkGOBHFZWbBYLt71U21ELOeoshEI3jMY5guiA0Y4ETkKPfVayeYXcnHLgxwIopgXzNI2A1JvXuEz0Zo8qyW99N282RWRDRzJFOrVCrZBR1SOHWtBcdw441WBjgRRTGSVZPbyZOdm8QQFR384Y/Mr0qf/ILLdmCAE1FcbmjnTTXzozWdvzgMcCKaxO55SZTJboupL6euXjgOBjkDnIhC7IwiW2c+TMFSbG7AACeiKGZqtwr2LoYcPOfU+1n3LpHopTkxrSwDnIiSEpVbDjcNGw3SYBOI4f2cbwJngBMRJcPVixoT0czizHwm+k5qdgHl8Iw19PJcUMueCgOciEKC+Wh6lKOR9S3jnNvQ+Wx8t0l0KvYDJyJXMRKqwTC1O8iiJtOa4mexDxC9n1cwwImIksAFHYho5rJhHnGzXfzcXitngBNRhKTC1MiNxclzmJhYvsxMUc22mydq33eiNwoDnIhCQsPCzQxtDx7DYws6xNrPKxjgRBSXnqYHLwZfOLPFD02CxX7gRDSTpbozoMffY+JigBNRBCN9ua08n9FZ/cwuomxornOXf7xggBNRyMRAHrM3+gycy8TxY+WpVSGb6DCJB/JwMisichEjkRQMOKcXOgg/v742fJNdDLX9OB84EXmW04E9kzHAichx5uYfV/qberigAxHNBGZvDgb2NRHEwaYXswsQm9stxnGijxQxi2GiNnAO5CEiJ1kxQ6DefUwdW88alQYH8pjuBx7jfHZLKsBFZL2I7BORAyJyt1WFIiJnKABdA8MYN1CRzsnwAwDePXgKgyPjuvcLnmNvex+aT/Wjf2jUQEmBe59rxLHOs3jrwKmE26b7JqLu1+8cMnSeoBt+9pap/VIpzeyOIuIH8HMAawG0AHhfRJ5VSjVaVTgisldbzyDaegbxatNJ3ftUFmUBAH773hFD52rvGQQA/PCVfbr3CQ/5P+0NlLGjbyju9uNau8eJvsHQc99/aW/MbWPVpBvbenWXze+LXRWvu/sF1BRn482vXwVfnG3MSqYGfhGAA0qpZqXUMIDHANxkTbGIyCsy0/ym9hsd119bD3rnYOLa9vDYxHF7BwOBv3FfR9ztewdHAABLKguifnbg5JmE5wtG8pj2kcIfI1Vbus5GvIlYJZkArwZwLOxxi/ZcBBG5Q0QaRKShoyP+RSQi531iVRUAoCgnHTeurER5fpau/b581fzQ91+/bpGufdYvq4h4/NKdlyfc556PL4t67uWvRO63vKow9P2K6sD3z335sohtFpbnhcp55cIy7bl83HReFf7qI3Vxy5SfmYYllQU4r7YIFQWBa/OxlVVYM68Ef3f1AgDAmnmlMcs+OmZ99xcxO7WiiNwCYL1S6q+1x58DcLFS6svx9qmvr1cNDQ2mzkdENFOJyFalVP3k55OpgR8HUBv2uEZ7joiIbJBMgL8PYIGIzBWRDAC3AnjWmmIREVEipnuhKKVGReTLAF4B4AfwkFJqj2UlIyKiKZkOcABQSr0I4EWLykJERAZwJCYRkUcxwImIPIoBTkTkUQxwIiKPMj2Qx9TJRDoAGJswYcIsAInH0c4svCbReE1i43WJ5qVrMkcpVTb5SVsDPBki0hBrJNJMxmsSjdckNl6XaNPhmrAJhYjIoxjgREQe5aUAf8DpArgQr0k0XpPYeF2ief6aeKYNnIiIInmpBk5ERGEY4EREHuWJAJ/JiyeLyGER2SUi20WkQXuuREQ2iMh+7Wux9ryIyM+067RTRM53tvTWEJGHROSkiOwOe87wNRCR27Tt94vIbU68FqvEuSbfEZHj2u/KdhG5Iexn39SuyT4RuS7s+WnztyUitSLyuog0isgeEblTe376/q4opVz9HwJT1R4EMA9ABoAdAJY6XS4bX/9hALMmPfcDAHdr398N4F+1728A8BICy/StAbDZ6fJbdA2uAHA+gN1mrwGAEgDN2tdi7ftip1+bxdfkOwC+FmPbpdrfTSaAudrfk3+6/W0BqARwvvZ9PoAPtdc+bX9XvFAD5+LJ0W4C8LD2/cMAbg57/rcqYBOAIhGpdKB8llJKvQmgc9LTRq/BdQA2KKU6lVJdADYAWJ/ywqdInGsSz00AHlNKDSmlDgE4gMDf1bT621JKtSmlPtC+7wPQhMA6vdP2d8ULAa5r8eRpTAH4o4hsFZE7tOfKlVJt2vftAMq172fStTJ6DWbKtfmy1hzwULCpADPwmohIHYDVADZjGv+ueCHAZ7rLlFLnA7gewJdE5IrwH6rAZ74Z3ReU1yDkfgDnAjgPQBuA/+1oaRwiInkA/gDgK0qp3vCfTbffFS8E+IxePFkpdVz7ehLAUwh87D0RbBrRvp7UNp9J18roNZj210YpdUIpNaaUGgfwSwR+V4AZdE1EJB2B8H5EKfWk9vS0/V3xQoDP2MWTRSRXRPKD3wNYB2A3Aq8/eGf8NgDPaN8/C+Dz2t31NQB6wj46TjdGr8ErANaJSLHWtLBOe27amHS/45MI/K4AgWtyq4hkishcAAsAbME0+9sSEQHwIIAmpdSPwn40fX9XnL6Lquc/BO4Wf4jAHfNvO10eG1/3PAR6BuwAsCf42gGUAngNwH4ArwIo0Z4XAD/XrtMuAPVOvwaLrsPvEGgSGEGgPfJ2M9cAwBcRuIF3AMAXnH5dKbgm/1d7zTsRCKfKsO2/rV2TfQCuD3t+2vxtAbgMgeaRnQC2a//dMJ1/VziUnojIo7zQhEJERDEwwImIPIoBTkTkUQxwIiKPYoATEXkUA5yIyKMY4EREHvX/AcLMKcD1nvkDAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 89.3180  # Value for C0\n",
    "K0 = -0.0010  # Value for K0\n",
    "K1 = 0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = -0.0204    # Value for b\n",
    "c = 2.2194    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    92.600000    92.387115    92.174230    91.961345    91.748459   \n",
      "351    92.387115    92.174230    91.961345    91.748459    91.535574   \n",
      "352    92.174230    91.961345    91.748459    91.535574    91.322689   \n",
      "353    91.961345    91.748459    91.535574    91.322689    91.109804   \n",
      "354    91.748459    91.535574    91.322689    91.109804    90.896919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    91.535574    91.322689    91.109804    90.896919    90.691597  ...   \n",
      "351    91.322689    91.109804    90.896919    90.691597    90.579552  ...   \n",
      "352    91.109804    90.896919    90.691597    90.579552    90.467507  ...   \n",
      "353    90.896919    90.691597    90.579552    90.467507    90.355462  ...   \n",
      "354    90.691597    90.579552    90.467507    90.355462    90.243417  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   80.261671    0.000263   80.252334    0.000263   80.242997    0.000263   \n",
      "351   80.252334    0.000263   80.242997    0.000263   80.233660    0.000262   \n",
      "352   80.242997    0.000263   80.233660    0.000262   80.224323    0.000262   \n",
      "353   80.233660    0.000262   80.224323    0.000262   80.214986    0.000262   \n",
      "354   80.224323    0.000262   80.214986    0.000262   80.205649    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   80.233660    0.000262   80.224323    0.000262  \n",
      "351   80.224323    0.000262   80.214986    0.000262  \n",
      "352   80.214986    0.000262   80.205649    0.000262  \n",
      "353   80.205649    0.000262   80.196312    0.000262  \n",
      "354   80.196312    0.000262   80.186975    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1750, 1, 251) (1750, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 2s 23ms/step - loss: 5428.7852 - val_loss: 3470.8542\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5178.3149 - val_loss: 3272.0889\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5019.0259 - val_loss: 3192.0967\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4897.5503 - val_loss: 3116.6228\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4785.6509 - val_loss: 3048.5454\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4678.6128 - val_loss: 2982.9856\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4574.8076 - val_loss: 2919.4717\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4473.6426 - val_loss: 2857.7332\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4374.7754 - val_loss: 2797.6101\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4278.0010 - val_loss: 2738.9971\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4183.1777 - val_loss: 2681.8184\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4090.2043 - val_loss: 2626.0168\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3999.0061 - val_loss: 2571.5461\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3909.5195 - val_loss: 2518.3687\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3821.6943 - val_loss: 2466.4519\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3735.4871 - val_loss: 2415.7661\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3650.8611 - val_loss: 2366.2854\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 3567.7822 - val_loss: 2317.9863\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3486.2197 - val_loss: 2270.8464\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3406.1458 - val_loss: 2224.8447\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3327.5337 - val_loss: 2179.9619\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3250.3604 - val_loss: 2136.1790\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3174.6018 - val_loss: 2093.4778\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3100.2363 - val_loss: 2051.8406\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3027.2417 - val_loss: 2011.2511\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2955.5986 - val_loss: 1971.6923\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2885.2876 - val_loss: 1933.1489\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2816.2891 - val_loss: 1895.6049\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2748.5847 - val_loss: 1859.0441\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2682.1548 - val_loss: 1823.4526\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2616.9844 - val_loss: 1788.8156\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2553.0544 - val_loss: 1755.1182\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2490.3481 - val_loss: 1722.3453\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2428.8494 - val_loss: 1690.4839\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2368.5408 - val_loss: 1659.5197\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2309.4072 - val_loss: 1629.4390\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2251.4321 - val_loss: 1600.2279\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2194.6003 - val_loss: 1571.8727\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2138.8958 - val_loss: 1544.3607\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2084.3032 - val_loss: 1517.6790\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2030.8090 - val_loss: 1491.8132\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1978.3961 - val_loss: 1466.7516\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1927.0513 - val_loss: 1442.4808\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1876.7595 - val_loss: 1418.9889\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1827.5057 - val_loss: 1396.2621\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1779.2770 - val_loss: 1374.2886\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1732.0582 - val_loss: 1353.0559\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1685.8353 - val_loss: 1332.5519\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1640.5952 - val_loss: 1312.7639\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1596.3234 - val_loss: 1293.6803\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1553.0068 - val_loss: 1275.2888\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1510.6311 - val_loss: 1257.5776\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1469.1837 - val_loss: 1240.5344\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1428.6510 - val_loss: 1224.1484\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1389.0198 - val_loss: 1208.4067\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1350.2771 - val_loss: 1193.2983\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1312.4103 - val_loss: 1178.8120\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1275.4058 - val_loss: 1164.9359\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1239.2510 - val_loss: 1151.6584\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1203.9335 - val_loss: 1138.9689\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1169.4408 - val_loss: 1126.8552\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1135.7601 - val_loss: 1115.3068\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1102.8789 - val_loss: 1104.3124\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1070.7854 - val_loss: 1093.8607\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1039.4666 - val_loss: 1083.9412\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1008.9111 - val_loss: 1074.5425\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 979.1061 - val_loss: 1065.6543\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 950.0405 - val_loss: 1057.2653\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 921.7018 - val_loss: 1049.3650\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 894.0782 - val_loss: 1041.9426\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 867.1582 - val_loss: 1034.9880\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 840.9299 - val_loss: 1028.4899\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 815.3815 - val_loss: 1022.4384\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 790.5019 - val_loss: 1016.8229\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 766.2799 - val_loss: 1011.6333\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 742.7037 - val_loss: 1006.8589\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 719.7620 - val_loss: 1002.4900\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 697.4437 - val_loss: 998.5161\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 675.7377 - val_loss: 994.9269\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 654.6328 - val_loss: 991.7135\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 634.1185 - val_loss: 988.8648\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 614.1834 - val_loss: 986.3713\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 594.8170 - val_loss: 984.2233\n",
      "Epoch 84/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 576.0084 - val_loss: 982.4111\n",
      "Epoch 85/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 557.7469 - val_loss: 980.9251\n",
      "Epoch 86/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 540.0220 - val_loss: 979.7554\n",
      "Epoch 87/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 522.8232 - val_loss: 978.8929\n",
      "Epoch 88/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 506.1400 - val_loss: 978.3279\n",
      "Epoch 89/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 489.9622 - val_loss: 978.0511\n",
      "Epoch 90/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 474.2794 - val_loss: 978.0533\n",
      "Epoch 91/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 459.0815 - val_loss: 978.3251\n",
      "Epoch 92/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 444.3582 - val_loss: 978.8577\n",
      "Epoch 93/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 430.0997 - val_loss: 979.6418\n",
      "Epoch 94/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 416.2959 - val_loss: 980.6686\n",
      "Epoch 95/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 402.9374 - val_loss: 981.9290\n",
      "Epoch 96/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 390.0138 - val_loss: 983.4146\n",
      "Epoch 97/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 377.5155 - val_loss: 985.1163\n",
      "Epoch 98/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 365.4333 - val_loss: 987.0257\n",
      "Epoch 99/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 353.7574 - val_loss: 989.1343\n",
      "Epoch 100/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 342.4784 - val_loss: 991.4338\n",
      "Epoch 101/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 331.5870 - val_loss: 993.9155\n",
      "Epoch 102/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 321.0740 - val_loss: 996.5716\n",
      "Epoch 103/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 310.9303 - val_loss: 999.3937\n",
      "Epoch 104/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 301.1466 - val_loss: 1002.3738\n",
      "Epoch 105/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 291.7143 - val_loss: 1005.5043\n",
      "Epoch 106/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 282.6243 - val_loss: 1008.7768\n",
      "Epoch 107/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 273.8680 - val_loss: 1012.1842\n",
      "Epoch 108/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 265.4366 - val_loss: 1015.7187\n",
      "Epoch 109/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 257.3216 - val_loss: 1019.3726\n",
      "Epoch 110/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 249.5144 - val_loss: 1023.1389\n",
      "Epoch 111/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 242.0070 - val_loss: 1027.0101\n",
      "Epoch 112/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 234.7908 - val_loss: 1030.9791\n",
      "Epoch 113/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 227.8579 - val_loss: 1035.0389\n",
      "Epoch 114/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 221.2003 - val_loss: 1039.1829\n",
      "Epoch 115/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 214.8098 - val_loss: 1043.4039\n",
      "Epoch 116/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 208.6791 - val_loss: 1047.6956\n",
      "Epoch 117/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 202.8000 - val_loss: 1052.0516\n",
      "Epoch 118/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 197.1652 - val_loss: 1056.4650\n",
      "Epoch 119/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 191.7671 - val_loss: 1060.9302\n",
      "Epoch 120/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 186.5985 - val_loss: 1065.4407\n",
      "Epoch 121/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 181.6521 - val_loss: 1069.9911\n",
      "Epoch 122/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 176.9207 - val_loss: 1074.5750\n",
      "Epoch 123/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 172.3976 - val_loss: 1079.1870\n",
      "Epoch 124/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 168.0758 - val_loss: 1083.8213\n",
      "Epoch 125/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 163.9486 - val_loss: 1088.4728\n",
      "Epoch 126/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 160.0092 - val_loss: 1093.1365\n",
      "Epoch 127/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 156.2514 - val_loss: 1097.8069\n",
      "Epoch 128/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 152.6687 - val_loss: 1102.4791\n",
      "Epoch 129/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 149.2549 - val_loss: 1107.1487\n",
      "Epoch 130/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 146.0041 - val_loss: 1111.8109\n",
      "Epoch 131/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 142.9100 - val_loss: 1116.4611\n",
      "Epoch 132/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 139.9670 - val_loss: 1121.0946\n",
      "Epoch 133/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 137.1693 - val_loss: 1125.7086\n",
      "Epoch 134/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 134.5113 - val_loss: 1130.2979\n",
      "Epoch 135/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 131.9877 - val_loss: 1134.8585\n",
      "Epoch 136/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 129.5930 - val_loss: 1139.3878\n",
      "Epoch 137/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 127.3222 - val_loss: 1143.8815\n",
      "Epoch 138/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 125.1702 - val_loss: 1148.3363\n",
      "Epoch 139/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 123.1321 - val_loss: 1152.7493\n",
      "Epoch 140/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 121.2031 - val_loss: 1157.1172\n",
      "Epoch 141/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 119.3784 - val_loss: 1161.4370\n",
      "Epoch 142/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 117.6538 - val_loss: 1165.7061\n",
      "Epoch 143/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 116.0247 - val_loss: 1169.9216\n",
      "Epoch 144/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 114.4869 - val_loss: 1174.0819\n",
      "Epoch 145/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 113.0361 - val_loss: 1178.1838\n",
      "Epoch 146/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 111.6686 - val_loss: 1182.2256\n",
      "Epoch 147/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 110.3803 - val_loss: 1186.2050\n",
      "Epoch 148/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 109.1676 - val_loss: 1190.1205\n",
      "Epoch 149/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 108.0267 - val_loss: 1193.9706\n",
      "Epoch 150/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 106.9543 - val_loss: 1197.7534\n",
      "Epoch 151/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 105.9468 - val_loss: 1201.4674\n",
      "Epoch 152/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 105.0010 - val_loss: 1205.1112\n",
      "Epoch 153/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 104.1140 - val_loss: 1208.6842\n",
      "Epoch 154/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 103.2824 - val_loss: 1212.1855\n",
      "Epoch 155/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 102.5035 - val_loss: 1215.6140\n",
      "Epoch 156/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 101.7745 - val_loss: 1218.9684\n",
      "Epoch 157/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 101.0926 - val_loss: 1222.2488\n",
      "Epoch 158/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 100.4554 - val_loss: 1225.4543\n",
      "Epoch 159/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 99.8603 - val_loss: 1228.5850\n",
      "Epoch 160/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 99.3050 - val_loss: 1231.6400\n",
      "Epoch 161/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 98.7872 - val_loss: 1234.6199\n",
      "Epoch 162/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 98.3047 - val_loss: 1237.5233\n",
      "Epoch 163/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 97.8555 - val_loss: 1240.3523\n",
      "Epoch 164/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 97.4375 - val_loss: 1243.1053\n",
      "Epoch 165/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 97.0491 - val_loss: 1245.7836\n",
      "Epoch 166/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 96.6882 - val_loss: 1248.3871\n",
      "Epoch 167/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 96.3532 - val_loss: 1250.9164\n",
      "Epoch 168/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 96.0425 - val_loss: 1253.3723\n",
      "Epoch 169/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 95.7546 - val_loss: 1255.7551\n",
      "Epoch 170/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 95.4879 - val_loss: 1258.0649\n",
      "Epoch 171/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 95.2411 - val_loss: 1260.3033\n",
      "Epoch 172/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 95.0130 - val_loss: 1262.4716\n",
      "Epoch 173/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 94.8022 - val_loss: 1264.5697\n",
      "Epoch 174/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 94.6076 - val_loss: 1266.5983\n",
      "Epoch 175/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 94.4281 - val_loss: 1268.5597\n",
      "Epoch 176/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 94.2626 - val_loss: 1270.4537\n",
      "Epoch 177/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 94.1101 - val_loss: 1272.2819\n",
      "Epoch 178/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 93.9698 - val_loss: 1274.0464\n",
      "Epoch 179/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 93.8407 - val_loss: 1275.7460\n",
      "Epoch 180/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 93.7221 - val_loss: 1277.3845\n",
      "Epoch 181/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 93.6132 - val_loss: 1278.9615\n",
      "Epoch 182/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 93.5132 - val_loss: 1280.4794\n",
      "Epoch 183/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 93.4214 - val_loss: 1281.9393\n",
      "Epoch 184/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 93.3374 - val_loss: 1283.3414\n",
      "Epoch 185/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 93.2604 - val_loss: 1284.6882\n",
      "Epoch 186/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 93.1900 - val_loss: 1285.9818\n",
      "Epoch 187/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 93.1255 - val_loss: 1287.2223\n",
      "Epoch 188/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 93.0666 - val_loss: 1288.4102\n",
      "Epoch 189/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 93.0129 - val_loss: 1289.5497\n",
      "Epoch 190/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.9637 - val_loss: 1290.6388\n",
      "Epoch 191/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.9190 - val_loss: 1291.6823\n",
      "Epoch 192/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.8781 - val_loss: 1292.6791\n",
      "Epoch 193/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 92.8409 - val_loss: 1293.6316\n",
      "Epoch 194/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.8070 - val_loss: 1294.5408\n",
      "Epoch 195/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.7762 - val_loss: 1295.4081\n",
      "Epoch 196/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.7482 - val_loss: 1296.2358\n",
      "Epoch 197/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.7227 - val_loss: 1297.0249\n",
      "Epoch 198/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.6995 - val_loss: 1297.7759\n",
      "Epoch 199/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.6785 - val_loss: 1298.4910\n",
      "Epoch 200/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.6595 - val_loss: 1299.1710\n",
      "Epoch 201/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.6422 - val_loss: 1299.8173\n",
      "Epoch 202/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.6264 - val_loss: 1300.4314\n",
      "Epoch 203/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.6122 - val_loss: 1301.0142\n",
      "Epoch 204/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5994 - val_loss: 1301.5673\n",
      "Epoch 205/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5878 - val_loss: 1302.0918\n",
      "Epoch 206/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5773 - val_loss: 1302.5883\n",
      "Epoch 207/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 92.5678 - val_loss: 1303.0582\n",
      "Epoch 208/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5592 - val_loss: 1303.5032\n",
      "Epoch 209/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5514 - val_loss: 1303.9220\n",
      "Epoch 210/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5445 - val_loss: 1304.3033\n",
      "Epoch 211/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.4991 - val_loss: 1302.5645\n",
      "Epoch 212/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.6982 - val_loss: 1304.2050\n",
      "Epoch 213/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5812 - val_loss: 1304.7548\n",
      "Epoch 214/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5606 - val_loss: 1305.1869\n",
      "Epoch 215/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5496 - val_loss: 1305.5724\n",
      "Epoch 216/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5414 - val_loss: 1305.9238\n",
      "Epoch 217/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5348 - val_loss: 1306.2452\n",
      "Epoch 218/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5295 - val_loss: 1306.5414\n",
      "Epoch 219/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5249 - val_loss: 1306.8156\n",
      "Epoch 220/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5210 - val_loss: 1307.0685\n",
      "Epoch 221/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5176 - val_loss: 1307.3024\n",
      "Epoch 222/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5147 - val_loss: 1307.5198\n",
      "Epoch 223/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5122 - val_loss: 1307.7211\n",
      "Epoch 224/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5101 - val_loss: 1307.9077\n",
      "Epoch 225/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5082 - val_loss: 1308.0793\n",
      "Epoch 226/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5067 - val_loss: 1308.2389\n",
      "Epoch 227/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5053 - val_loss: 1308.3862\n",
      "Epoch 228/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5043 - val_loss: 1308.5231\n",
      "Epoch 229/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5033 - val_loss: 1308.6497\n",
      "Epoch 230/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5026 - val_loss: 1308.7666\n",
      "Epoch 231/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 92.5020 - val_loss: 1308.8749\n",
      "Epoch 232/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5015 - val_loss: 1308.9750\n",
      "Epoch 233/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 92.5012 - val_loss: 1309.0671\n",
      "Epoch 234/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 92.5010 - val_loss: 1309.1530\n",
      "Epoch 235/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5007 - val_loss: 1309.2312\n",
      "Epoch 236/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 92.5008 - val_loss: 1309.3040\n",
      "Epoch 237/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5008 - val_loss: 1309.3710\n",
      "Epoch 238/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5009 - val_loss: 1309.4320\n",
      "Epoch 239/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5011 - val_loss: 1309.4886\n",
      "Epoch 240/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5013 - val_loss: 1309.5400\n",
      "Epoch 241/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5017 - val_loss: 1309.5878\n",
      "Epoch 242/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5020 - val_loss: 1309.6315\n",
      "Epoch 243/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5024 - val_loss: 1309.6710\n",
      "Epoch 244/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5028 - val_loss: 1309.7078\n",
      "Epoch 245/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5033 - val_loss: 1309.7412\n",
      "Epoch 246/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5039 - val_loss: 1309.7720\n",
      "Epoch 247/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5043 - val_loss: 1309.7998\n",
      "Epoch 248/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5049 - val_loss: 1309.8253\n",
      "Epoch 249/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5056 - val_loss: 1309.8484\n",
      "Epoch 250/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5062 - val_loss: 1309.8696\n",
      "Epoch 251/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5069 - val_loss: 1309.8890\n",
      "Epoch 252/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5076 - val_loss: 1309.9061\n",
      "Epoch 253/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5082 - val_loss: 1309.9222\n",
      "Epoch 254/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5089 - val_loss: 1309.9362\n",
      "Epoch 255/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5097 - val_loss: 1309.9492\n",
      "Epoch 256/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5104 - val_loss: 1309.9609\n",
      "Epoch 257/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5111 - val_loss: 1309.9714\n",
      "Epoch 258/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 92.5119 - val_loss: 1309.9805\n",
      "Epoch 259/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5127 - val_loss: 1309.9884\n",
      "Epoch 260/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5134 - val_loss: 1309.9961\n",
      "Epoch 261/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5142 - val_loss: 1310.0026\n",
      "Epoch 262/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5149 - val_loss: 1310.0090\n",
      "Epoch 263/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5158 - val_loss: 1310.0138\n",
      "Epoch 264/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5165 - val_loss: 1310.0184\n",
      "Epoch 265/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5173 - val_loss: 1310.0225\n",
      "Epoch 266/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 92.5181 - val_loss: 1310.0261\n",
      "Epoch 267/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5189 - val_loss: 1310.0291\n",
      "Epoch 268/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5197 - val_loss: 1310.0311\n",
      "Epoch 269/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5205 - val_loss: 1310.0343\n",
      "Epoch 270/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5212 - val_loss: 1310.0359\n",
      "Epoch 271/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5220 - val_loss: 1310.0369\n",
      "Epoch 272/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5228 - val_loss: 1310.0381\n",
      "Epoch 273/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5236 - val_loss: 1310.0387\n",
      "Epoch 274/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5244 - val_loss: 1310.0391\n",
      "Epoch 275/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5251 - val_loss: 1310.0394\n",
      "Epoch 276/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5259 - val_loss: 1310.0392\n",
      "Epoch 277/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5267 - val_loss: 1310.0391\n",
      "Epoch 278/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5275 - val_loss: 1310.0391\n",
      "Epoch 279/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5282 - val_loss: 1310.0389\n",
      "Epoch 280/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5290 - val_loss: 1310.0381\n",
      "Epoch 281/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5297 - val_loss: 1310.0374\n",
      "Epoch 282/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5305 - val_loss: 1310.0371\n",
      "Epoch 283/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 92.5312 - val_loss: 1310.0367\n",
      "Epoch 284/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5320 - val_loss: 1310.0360\n",
      "Epoch 285/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5327 - val_loss: 1310.0355\n",
      "Epoch 286/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5334 - val_loss: 1310.0343\n",
      "Epoch 287/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5341 - val_loss: 1310.0327\n",
      "Epoch 288/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5348 - val_loss: 1310.0315\n",
      "Epoch 289/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5356 - val_loss: 1310.0309\n",
      "Epoch 290/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5362 - val_loss: 1310.0298\n",
      "Epoch 291/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 92.5369 - val_loss: 1310.0286\n",
      "Epoch 292/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5376 - val_loss: 1310.0273\n",
      "Epoch 293/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5383 - val_loss: 1310.0262\n",
      "Epoch 294/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5389 - val_loss: 1310.0243\n",
      "Epoch 295/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5396 - val_loss: 1310.0228\n",
      "Epoch 296/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5403 - val_loss: 1310.0217\n",
      "Epoch 297/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5409 - val_loss: 1310.0209\n",
      "Epoch 298/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5415 - val_loss: 1310.0197\n",
      "Epoch 299/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5422 - val_loss: 1310.0187\n",
      "Epoch 300/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5428 - val_loss: 1310.0172\n",
      "Epoch 301/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5434 - val_loss: 1310.0160\n",
      "Epoch 302/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5440 - val_loss: 1310.0149\n",
      "Epoch 303/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5447 - val_loss: 1310.0139\n",
      "Epoch 304/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5452 - val_loss: 1310.0121\n",
      "Epoch 305/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 92.5458 - val_loss: 1310.0109\n",
      "Epoch 306/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5464 - val_loss: 1310.0095\n",
      "Epoch 307/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5470 - val_loss: 1310.0085\n",
      "Epoch 308/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5475 - val_loss: 1310.0070\n",
      "Epoch 309/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5480 - val_loss: 1310.0057\n",
      "Epoch 310/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5486 - val_loss: 1310.0045\n",
      "Epoch 311/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5492 - val_loss: 1310.0032\n",
      "Epoch 312/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5497 - val_loss: 1310.0020\n",
      "Epoch 313/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5502 - val_loss: 1310.0012\n",
      "Epoch 314/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5507 - val_loss: 1310.0000\n",
      "Epoch 315/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5512 - val_loss: 1309.9988\n",
      "Epoch 316/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5517 - val_loss: 1309.9976\n",
      "Epoch 317/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5523 - val_loss: 1309.9968\n",
      "Epoch 318/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5528 - val_loss: 1309.9958\n",
      "Epoch 319/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5532 - val_loss: 1309.9949\n",
      "Epoch 320/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5537 - val_loss: 1309.9935\n",
      "Epoch 321/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5541 - val_loss: 1309.9922\n",
      "Epoch 322/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5546 - val_loss: 1309.9915\n",
      "Epoch 323/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5551 - val_loss: 1309.9905\n",
      "Epoch 324/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5555 - val_loss: 1309.9894\n",
      "Epoch 325/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5560 - val_loss: 1309.9888\n",
      "Epoch 326/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5564 - val_loss: 1309.9873\n",
      "Epoch 327/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5568 - val_loss: 1309.9861\n",
      "Epoch 328/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 92.5572 - val_loss: 1309.9852\n",
      "Epoch 329/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5577 - val_loss: 1309.9843\n",
      "Epoch 330/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5581 - val_loss: 1309.9832\n",
      "Epoch 331/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5584 - val_loss: 1309.9824\n",
      "Epoch 332/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5588 - val_loss: 1309.9813\n",
      "Epoch 333/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5592 - val_loss: 1309.9806\n",
      "Epoch 334/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5596 - val_loss: 1309.9794\n",
      "Epoch 335/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5600 - val_loss: 1309.9783\n",
      "Epoch 336/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5604 - val_loss: 1309.9772\n",
      "Epoch 337/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5608 - val_loss: 1309.9767\n",
      "Epoch 338/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5611 - val_loss: 1309.9762\n",
      "Epoch 339/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5614 - val_loss: 1309.9756\n",
      "Epoch 340/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5618 - val_loss: 1309.9750\n",
      "Epoch 341/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5621 - val_loss: 1309.9746\n",
      "Epoch 342/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5624 - val_loss: 1309.9739\n",
      "Epoch 343/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5628 - val_loss: 1309.9733\n",
      "Epoch 344/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5631 - val_loss: 1309.9722\n",
      "Epoch 345/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5634 - val_loss: 1309.9711\n",
      "Epoch 346/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5637 - val_loss: 1309.9709\n",
      "Epoch 347/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5641 - val_loss: 1309.9706\n",
      "Epoch 348/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5644 - val_loss: 1309.9701\n",
      "Epoch 349/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5646 - val_loss: 1309.9698\n",
      "Epoch 350/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 92.5649 - val_loss: 1309.9692\n",
      "Epoch 351/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5652 - val_loss: 1309.9688\n",
      "Epoch 352/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5654 - val_loss: 1309.9683\n",
      "Epoch 353/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5657 - val_loss: 1309.9673\n",
      "Epoch 354/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5659 - val_loss: 1309.9664\n",
      "Epoch 355/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5662 - val_loss: 1309.9652\n",
      "Epoch 356/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5665 - val_loss: 1309.9645\n",
      "Epoch 357/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5668 - val_loss: 1309.9637\n",
      "Epoch 358/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5671 - val_loss: 1309.9630\n",
      "Epoch 359/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5673 - val_loss: 1309.9629\n",
      "Epoch 360/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5675 - val_loss: 1309.9620\n",
      "Epoch 361/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5678 - val_loss: 1309.9615\n",
      "Epoch 362/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5680 - val_loss: 1309.9611\n",
      "Epoch 363/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5682 - val_loss: 1309.9602\n",
      "Epoch 364/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5684 - val_loss: 1309.9595\n",
      "Epoch 365/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5687 - val_loss: 1309.9592\n",
      "Epoch 366/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5688 - val_loss: 1309.9591\n",
      "Epoch 367/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5691 - val_loss: 1309.9587\n",
      "Epoch 368/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5693 - val_loss: 1309.9583\n",
      "Epoch 369/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5695 - val_loss: 1309.9578\n",
      "Epoch 370/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5696 - val_loss: 1309.9573\n",
      "Epoch 371/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5699 - val_loss: 1309.9573\n",
      "Epoch 372/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5700 - val_loss: 1309.9569\n",
      "Epoch 373/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5702 - val_loss: 1309.9563\n",
      "Epoch 374/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 92.5704 - val_loss: 1309.9558\n",
      "Epoch 375/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5706 - val_loss: 1309.9556\n",
      "Epoch 376/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5708 - val_loss: 1309.9553\n",
      "Epoch 377/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5709 - val_loss: 1309.9548\n",
      "Epoch 378/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5711 - val_loss: 1309.9540\n",
      "Epoch 379/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5713 - val_loss: 1309.9539\n",
      "Epoch 380/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5715 - val_loss: 1309.9532\n",
      "Epoch 381/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5717 - val_loss: 1309.9525\n",
      "Epoch 382/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5718 - val_loss: 1309.9525\n",
      "Epoch 383/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5720 - val_loss: 1309.9525\n",
      "Epoch 384/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5721 - val_loss: 1309.9525\n",
      "Epoch 385/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5722 - val_loss: 1309.9521\n",
      "Epoch 386/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5724 - val_loss: 1309.9519\n",
      "Epoch 387/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5725 - val_loss: 1309.9518\n",
      "Epoch 388/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5726 - val_loss: 1309.9515\n",
      "Epoch 389/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5727 - val_loss: 1309.9509\n",
      "Epoch 390/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5729 - val_loss: 1309.9502\n",
      "Epoch 391/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5730 - val_loss: 1309.9502\n",
      "Epoch 392/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5732 - val_loss: 1309.9500\n",
      "Epoch 393/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5733 - val_loss: 1309.9493\n",
      "Epoch 394/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5734 - val_loss: 1309.9490\n",
      "Epoch 395/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 92.5735 - val_loss: 1309.9480\n",
      "Epoch 396/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5737 - val_loss: 1309.9481\n",
      "Epoch 397/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5738 - val_loss: 1309.9485\n",
      "Epoch 398/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5739 - val_loss: 1309.9480\n",
      "Epoch 399/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5740 - val_loss: 1309.9479\n",
      "Epoch 400/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5741 - val_loss: 1309.9476\n",
      "Epoch 401/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5742 - val_loss: 1309.9476\n",
      "Epoch 402/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5743 - val_loss: 1309.9476\n",
      "Epoch 403/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5744 - val_loss: 1309.9475\n",
      "Epoch 404/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5745 - val_loss: 1309.9475\n",
      "Epoch 405/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5746 - val_loss: 1309.9471\n",
      "Epoch 406/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5746 - val_loss: 1309.9467\n",
      "Epoch 407/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5747 - val_loss: 1309.9460\n",
      "Epoch 408/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5749 - val_loss: 1309.9457\n",
      "Epoch 409/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5749 - val_loss: 1309.9454\n",
      "Epoch 410/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5750 - val_loss: 1309.9449\n",
      "Epoch 411/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5751 - val_loss: 1309.9445\n",
      "Epoch 412/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5752 - val_loss: 1309.9443\n",
      "Epoch 413/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5753 - val_loss: 1309.9445\n",
      "Epoch 414/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5754 - val_loss: 1309.9443\n",
      "Epoch 415/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5755 - val_loss: 1309.9446\n",
      "Epoch 416/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 92.5756 - val_loss: 1309.9441\n",
      "Epoch 417/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5756 - val_loss: 1309.9440\n",
      "Epoch 418/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5757 - val_loss: 1309.9437\n",
      "Epoch 419/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5758 - val_loss: 1309.9436\n",
      "Epoch 420/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5759 - val_loss: 1309.9436\n",
      "Epoch 421/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5760 - val_loss: 1309.9436\n",
      "Epoch 422/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5760 - val_loss: 1309.9437\n",
      "Epoch 423/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5761 - val_loss: 1309.9436\n",
      "Epoch 424/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5761 - val_loss: 1309.9430\n",
      "Epoch 425/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5762 - val_loss: 1309.9425\n",
      "Epoch 426/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5762 - val_loss: 1309.9417\n",
      "Epoch 427/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5763 - val_loss: 1309.9418\n",
      "Epoch 428/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5764 - val_loss: 1309.9417\n",
      "Epoch 429/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5764 - val_loss: 1309.9417\n",
      "Epoch 430/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5765 - val_loss: 1309.9414\n",
      "Epoch 431/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5766 - val_loss: 1309.9417\n",
      "Epoch 432/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5767 - val_loss: 1309.9424\n",
      "Epoch 433/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5766 - val_loss: 1309.9423\n",
      "Epoch 434/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5767 - val_loss: 1309.9420\n",
      "Epoch 435/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5767 - val_loss: 1309.9413\n",
      "Epoch 436/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 92.5768 - val_loss: 1309.9410\n",
      "Epoch 437/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5768 - val_loss: 1309.9406\n",
      "Epoch 438/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5769 - val_loss: 1309.9406\n",
      "Epoch 439/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5770 - val_loss: 1309.9410\n",
      "Epoch 440/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5770 - val_loss: 1309.9410\n",
      "Epoch 441/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5771 - val_loss: 1309.9410\n",
      "Epoch 442/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5771 - val_loss: 1309.9407\n",
      "Epoch 443/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5771 - val_loss: 1309.9406\n",
      "Epoch 444/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5772 - val_loss: 1309.9402\n",
      "Epoch 445/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5773 - val_loss: 1309.9407\n",
      "Epoch 446/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5773 - val_loss: 1309.9410\n",
      "Epoch 447/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5773 - val_loss: 1309.9410\n",
      "Epoch 448/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 92.5773 - val_loss: 1309.9407\n",
      "Epoch 449/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5773 - val_loss: 1309.9406\n",
      "Epoch 450/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5774 - val_loss: 1309.9397\n",
      "Epoch 451/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5775 - val_loss: 1309.9395\n",
      "Epoch 452/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5775 - val_loss: 1309.9392\n",
      "Epoch 453/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5775 - val_loss: 1309.9390\n",
      "Epoch 454/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5775 - val_loss: 1309.9384\n",
      "Epoch 455/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5776 - val_loss: 1309.9377\n",
      "Epoch 456/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5776 - val_loss: 1309.9380\n",
      "Epoch 457/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5777 - val_loss: 1309.9387\n",
      "Epoch 458/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5778 - val_loss: 1309.9391\n",
      "Epoch 459/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5777 - val_loss: 1309.9391\n",
      "Epoch 460/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5777 - val_loss: 1309.9392\n",
      "Epoch 461/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5778 - val_loss: 1309.9396\n",
      "Epoch 462/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5778 - val_loss: 1309.9396\n",
      "Epoch 463/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5778 - val_loss: 1309.9393\n",
      "Epoch 464/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5779 - val_loss: 1309.9392\n",
      "Epoch 465/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5779 - val_loss: 1309.9390\n",
      "Epoch 466/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5779 - val_loss: 1309.9387\n",
      "Epoch 467/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5779 - val_loss: 1309.9384\n",
      "Epoch 468/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 92.5779 - val_loss: 1309.9375\n",
      "Epoch 469/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5780 - val_loss: 1309.9373\n",
      "Epoch 470/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5780 - val_loss: 1309.9371\n",
      "Epoch 471/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5781 - val_loss: 1309.9375\n",
      "Epoch 472/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5781 - val_loss: 1309.9376\n",
      "Epoch 473/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5781 - val_loss: 1309.9376\n",
      "Epoch 474/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5780 - val_loss: 1309.9371\n",
      "Epoch 475/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5781 - val_loss: 1309.9363\n",
      "Epoch 476/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5781 - val_loss: 1309.9355\n",
      "Epoch 477/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5781 - val_loss: 1309.9355\n",
      "Epoch 478/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5782 - val_loss: 1309.9355\n",
      "Epoch 479/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5782 - val_loss: 1309.9355\n",
      "Epoch 480/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5782 - val_loss: 1309.9357\n",
      "Epoch 481/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5782 - val_loss: 1309.9354\n",
      "Epoch 482/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5783 - val_loss: 1309.9354\n",
      "Epoch 483/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5783 - val_loss: 1309.9353\n",
      "Epoch 484/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5783 - val_loss: 1309.9353\n",
      "Epoch 485/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5783 - val_loss: 1309.9355\n",
      "Epoch 486/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5783 - val_loss: 1309.9358\n",
      "Epoch 487/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5784 - val_loss: 1309.9364\n",
      "Epoch 488/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 92.5784 - val_loss: 1309.9371\n",
      "Epoch 489/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 92.5785 - val_loss: 1309.9380\n",
      "Epoch 490/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5784 - val_loss: 1309.9379\n",
      "Epoch 491/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5784 - val_loss: 1309.9379\n",
      "Epoch 492/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5784 - val_loss: 1309.9379\n",
      "Epoch 493/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5784 - val_loss: 1309.9381\n",
      "Epoch 494/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5784 - val_loss: 1309.9374\n",
      "Epoch 495/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.5785 - val_loss: 1309.9374\n",
      "Epoch 496/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5785 - val_loss: 1309.9376\n",
      "Epoch 497/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5785 - val_loss: 1309.9380\n",
      "Epoch 498/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5785 - val_loss: 1309.9381\n",
      "Epoch 499/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5785 - val_loss: 1309.9380\n",
      "Epoch 500/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5785 - val_loss: 1309.9374\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(89.3180, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0010, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(-0.0204, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.2194, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 380ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.01075163e+01, 6.99898693e+01, 6.98722222e+01, 6.97545752e+01,\n",
       "        6.96379281e+01, 6.95232881e+01, 6.94508170e+01, 8.97632062e-01,\n",
       "        1.45282522e-01, 0.00000000e+00, 0.00000000e+00, 8.78671480e-02,\n",
       "        6.94524884e-01, 7.10462418e+01, 7.09874183e+01, 7.09285948e+01,\n",
       "        7.08395425e+01, 7.07218954e+01, 7.06042484e+01, 7.04866013e+01,\n",
       "        7.03689543e+01, 7.02513072e+01, 7.01336601e+01, 7.00160131e+01,\n",
       "        6.98983660e+01, 6.97807189e+01, 6.96630719e+01, 6.95454248e+01,\n",
       "        6.94638889e+01, 7.33036718e+01, 7.29229995e+01, 7.25423273e+01,\n",
       "        7.21616550e+01, 7.19360154e+01, 7.18376961e+01, 7.17393768e+01,\n",
       "        7.16410574e+01, 7.14972222e+01, 4.37616200e-01, 5.08596000e-01,\n",
       "        3.45702950e-01, 7.47229160e-01, 0.00000000e+00, 4.47287800e-01,\n",
       "        1.57974410e-01, 5.82629080e-01, 5.34027160e-01, 6.98068627e+01,\n",
       "        6.96892157e+01, 6.95715686e+01, 6.94769608e+01, 7.33882656e+01,\n",
       "        7.30075934e+01, 7.26269211e+01, 7.22462488e+01, 7.21957864e+01,\n",
       "        7.21859545e+01, 7.21761225e+01, 7.21662906e+01, 7.21536438e+01,\n",
       "        1.64144900e-01, 0.00000000e+00, 7.20206653e+01, 7.20899601e+01,\n",
       "        7.20801282e+01, 7.20702962e+01, 7.20604643e+01, 7.20431863e+01,\n",
       "        7.20255392e+01, 7.20078922e+01, 6.94004510e+01, 0.00000000e+00,\n",
       "        4.19966800e-01, 5.15501900e+01, 0.00000000e+00, 4.64938100e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        6.79171295e+01, 4.94966596e-01, 4.37364936e-01, 3.13672334e-01,\n",
       "        7.44943559e-01, 6.66617870e-01, 1.61012948e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 5.37882745e-03, 0.00000000e+00, 6.31546617e-01,\n",
       "        0.00000000e+00, 2.10530251e-01, 0.00000000e+00, 2.82563329e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([64.68790053, 64.68024868, 64.67259684, 64.664945  , 64.65729316,\n",
       "       64.64964132, 64.64198948, 64.63433764, 64.6266858 , 64.61903396,\n",
       "       64.61138211, 64.60373027, 64.59607843, 64.58842659, 64.58077475,\n",
       "       64.57312291, 64.56547107, 64.55781923, 64.55016738, 64.54251554,\n",
       "       64.5348637 , 64.52721186, 64.51956002, 64.51190818, 64.50425634,\n",
       "       64.4966045 , 64.48895265, 64.48130081, 64.47364897, 64.46599713,\n",
       "       64.45834529, 64.45069345, 64.44304161, 64.43538977, 64.42773792,\n",
       "       64.42008608, 64.41243424, 64.4047824 , 64.39713056, 64.38947872,\n",
       "       64.38182688, 64.37417504, 64.36652319, 64.35887135, 64.35121951,\n",
       "       64.34356767, 64.33591583, 64.32826399, 64.32061215, 64.31296031,\n",
       "       64.30530846, 64.29765662, 64.29000478, 64.28235294, 64.2747011 ,\n",
       "       64.26704926, 64.25939742, 64.25174558, 64.24409374, 64.23644189,\n",
       "       64.22879005, 64.22113821, 64.21348637, 64.20583453, 64.19818269,\n",
       "       64.19053085, 64.18287901, 64.17522716, 64.16757532, 64.15992348,\n",
       "       64.15227164, 64.1446198 , 64.13696796, 64.12931612, 64.12166428,\n",
       "       64.11401243, 64.10636059, 64.09870875, 64.09105691, 64.08340507,\n",
       "       64.07575323, 64.06810139, 64.06044955, 64.0527977 , 64.04514586,\n",
       "       64.03749402, 64.02984218, 64.02219034, 64.0145385 , 64.00688666,\n",
       "       63.99923482, 63.99158297, 63.98393113, 63.97627929, 63.96862745,\n",
       "       63.96097561, 63.95332377, 63.94567193, 63.93802009, 63.93036824])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.73317128745277\n",
      "31.909166044633317\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
