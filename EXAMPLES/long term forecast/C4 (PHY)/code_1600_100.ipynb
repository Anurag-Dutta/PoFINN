{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "1695    65.702080\n",
       "1696    65.691529\n",
       "1697    65.680978\n",
       "1698    65.670427\n",
       "1699    65.659876\n",
       "Name: C4, Length: 1700, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c4_interpolated_1600_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "1595     0.759919\n",
       "1596     0.668464\n",
       "1597     0.000000\n",
       "1598     0.097889\n",
       "1599     0.000000\n",
       "Name: C4, Length: 1600, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1600)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.409524</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.219048</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.028571</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.838095</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.600000  0.000298\n",
       "1     88.409524  0.000298\n",
       "2     88.219048  0.000297\n",
       "3     88.028571  0.000297\n",
       "4     87.838095  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6856  # Value for C0\n",
    "K0 = -0.0008  # Value for K0\n",
    "K1 = -0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0128    # Value for b\n",
    "c = -2.3003    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.600000    88.409524    88.219048    88.028571    87.838095   \n",
      "351    88.409524    88.219048    88.028571    87.838095    87.647619   \n",
      "352    88.219048    88.028571    87.838095    87.647619    87.457143   \n",
      "353    88.028571    87.838095    87.647619    87.457143    87.266667   \n",
      "354    87.838095    87.647619    87.457143    87.266667    87.076190   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.647619    87.457143    87.266667    87.076190    86.896218  ...   \n",
      "351    87.457143    87.266667    87.076190    86.896218    86.845798  ...   \n",
      "352    87.266667    87.076190    86.896218    86.845798    86.795378  ...   \n",
      "353    87.076190    86.896218    86.845798    86.795378    86.744958  ...   \n",
      "354    86.896218    86.845798    86.795378    86.744958    86.694538  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   78.880345    0.000263   78.856069    0.000263   78.831793    0.000263   \n",
      "351   78.856069    0.000263   78.831793    0.000263   78.807516    0.000262   \n",
      "352   78.831793    0.000263   78.807516    0.000262   78.783240    0.000262   \n",
      "353   78.807516    0.000262   78.783240    0.000262   78.758964    0.000262   \n",
      "354   78.783240    0.000262   78.758964    0.000262   78.734687    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   78.807516    0.000262   78.783240    0.000262  \n",
      "351   78.783240    0.000262   78.758964    0.000262  \n",
      "352   78.758964    0.000262   78.734687    0.000262  \n",
      "353   78.734687    0.000262   78.710411    0.000262  \n",
      "354   78.710411    0.000262   78.686134    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1150, 1, 251) (1150, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "15/15 [==============================] - 2s 34ms/step - loss: 5480.9570 - val_loss: 4658.1016\n",
      "Epoch 2/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5420.7148 - val_loss: 4620.3511\n",
      "Epoch 3/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5379.9546 - val_loss: 4582.6714\n",
      "Epoch 4/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5339.3638 - val_loss: 4545.2207\n",
      "Epoch 5/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5299.0210 - val_loss: 4508.0171\n",
      "Epoch 6/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5258.9277 - val_loss: 4471.0527\n",
      "Epoch 7/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5219.0796 - val_loss: 4431.8472\n",
      "Epoch 8/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5163.4229 - val_loss: 4380.2593\n",
      "Epoch 9/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 5119.6299 - val_loss: 4340.9443\n",
      "Epoch 10/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5077.3706 - val_loss: 4302.2373\n",
      "Epoch 11/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5035.7246 - val_loss: 4264.0571\n",
      "Epoch 12/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4994.5898 - val_loss: 4226.3120\n",
      "Epoch 13/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4953.8809 - val_loss: 4188.9414\n",
      "Epoch 14/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4913.5439 - val_loss: 4151.9058\n",
      "Epoch 15/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4873.5444 - val_loss: 4115.1812\n",
      "Epoch 16/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4833.8574 - val_loss: 4078.7473\n",
      "Epoch 17/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 4794.4673 - val_loss: 4042.5928\n",
      "Epoch 18/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4755.3604 - val_loss: 4006.7068\n",
      "Epoch 19/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4716.5283 - val_loss: 3962.7212\n",
      "Epoch 20/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4667.1162 - val_loss: 3923.2107\n",
      "Epoch 21/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4623.9907 - val_loss: 3883.8066\n",
      "Epoch 22/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4581.5972 - val_loss: 3845.3074\n",
      "Epoch 23/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4540.1040 - val_loss: 3807.5522\n",
      "Epoch 24/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4499.3267 - val_loss: 3770.3901\n",
      "Epoch 25/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4459.1284 - val_loss: 3733.7241\n",
      "Epoch 26/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4419.4219 - val_loss: 3697.4900\n",
      "Epoch 27/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4380.1494 - val_loss: 3661.6467\n",
      "Epoch 28/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4341.2705 - val_loss: 3626.1628\n",
      "Epoch 29/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4302.7568 - val_loss: 3591.0164\n",
      "Epoch 30/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4264.5874 - val_loss: 3556.1904\n",
      "Epoch 31/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4226.7446 - val_loss: 3521.6702\n",
      "Epoch 32/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4189.2163 - val_loss: 3487.4453\n",
      "Epoch 33/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4151.9888 - val_loss: 3453.5068\n",
      "Epoch 34/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4115.0547 - val_loss: 3419.8462\n",
      "Epoch 35/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4078.4070 - val_loss: 3386.4565\n",
      "Epoch 36/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4042.0364 - val_loss: 3353.3325\n",
      "Epoch 37/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4005.9390 - val_loss: 3320.4685\n",
      "Epoch 38/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3970.1084 - val_loss: 3287.8606\n",
      "Epoch 39/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3934.5405 - val_loss: 3255.5051\n",
      "Epoch 40/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3899.2314 - val_loss: 3223.3965\n",
      "Epoch 41/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3864.1772 - val_loss: 3191.5322\n",
      "Epoch 42/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3829.3733 - val_loss: 3159.9089\n",
      "Epoch 43/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3794.8176 - val_loss: 3128.5249\n",
      "Epoch 44/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3760.5076 - val_loss: 3097.3760\n",
      "Epoch 45/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3726.4390 - val_loss: 3066.4609\n",
      "Epoch 46/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3692.6101 - val_loss: 3035.7759\n",
      "Epoch 47/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3659.0188 - val_loss: 3005.3201\n",
      "Epoch 48/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3625.6616 - val_loss: 2975.0903\n",
      "Epoch 49/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3592.5386 - val_loss: 2945.0845\n",
      "Epoch 50/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3559.6448 - val_loss: 2915.3015\n",
      "Epoch 51/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3526.9810 - val_loss: 2885.7400\n",
      "Epoch 52/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3494.5432 - val_loss: 2856.3967\n",
      "Epoch 53/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3462.3303 - val_loss: 2827.2703\n",
      "Epoch 54/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3430.3411 - val_loss: 2798.3591\n",
      "Epoch 55/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3398.5735 - val_loss: 2769.6633\n",
      "Epoch 56/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3367.0249 - val_loss: 2741.1782\n",
      "Epoch 57/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3335.6953 - val_loss: 2712.9055\n",
      "Epoch 58/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3304.5828 - val_loss: 2684.8411\n",
      "Epoch 59/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3273.6855 - val_loss: 2656.9851\n",
      "Epoch 60/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3243.0024 - val_loss: 2629.3359\n",
      "Epoch 61/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3212.5315 - val_loss: 2601.8916\n",
      "Epoch 62/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3182.2715 - val_loss: 2574.6516\n",
      "Epoch 63/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3152.2219 - val_loss: 2547.6140\n",
      "Epoch 64/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3122.3804 - val_loss: 2520.7783\n",
      "Epoch 65/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3092.7461 - val_loss: 2494.1428\n",
      "Epoch 66/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3063.3181 - val_loss: 2467.7068\n",
      "Epoch 67/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3034.0952 - val_loss: 2441.4673\n",
      "Epoch 68/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3005.0752 - val_loss: 2415.4255\n",
      "Epoch 69/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2976.2576 - val_loss: 2389.5786\n",
      "Epoch 70/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2947.6414 - val_loss: 2363.9265\n",
      "Epoch 71/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2919.2253 - val_loss: 2338.4678\n",
      "Epoch 72/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2891.0076 - val_loss: 2313.2012\n",
      "Epoch 73/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2862.9883 - val_loss: 2288.1252\n",
      "Epoch 74/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2835.1655 - val_loss: 2263.2390\n",
      "Epoch 75/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2807.5386 - val_loss: 2238.5425\n",
      "Epoch 76/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2780.1052 - val_loss: 2214.0339\n",
      "Epoch 77/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2752.8660 - val_loss: 2189.7119\n",
      "Epoch 78/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2725.8196 - val_loss: 2165.5759\n",
      "Epoch 79/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2698.9644 - val_loss: 2141.6245\n",
      "Epoch 80/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2672.2996 - val_loss: 2117.8582\n",
      "Epoch 81/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2645.8242 - val_loss: 2094.2734\n",
      "Epoch 82/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2619.5376 - val_loss: 2070.8706\n",
      "Epoch 83/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 2593.4380 - val_loss: 2047.6492\n",
      "Epoch 84/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2567.5249 - val_loss: 2024.6079\n",
      "Epoch 85/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2541.7974 - val_loss: 2001.7465\n",
      "Epoch 86/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2516.2546 - val_loss: 1979.0614\n",
      "Epoch 87/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2490.8953 - val_loss: 1956.5548\n",
      "Epoch 88/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2465.7188 - val_loss: 1934.2247\n",
      "Epoch 89/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2440.7241 - val_loss: 1912.0695\n",
      "Epoch 90/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2415.9097 - val_loss: 1890.0880\n",
      "Epoch 91/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2391.2756 - val_loss: 1868.2814\n",
      "Epoch 92/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2366.8203 - val_loss: 1846.6470\n",
      "Epoch 93/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2342.5437 - val_loss: 1825.1848\n",
      "Epoch 94/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2318.4446 - val_loss: 1803.8932\n",
      "Epoch 95/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 2294.5212 - val_loss: 1782.7720\n",
      "Epoch 96/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2270.7737 - val_loss: 1761.8195\n",
      "Epoch 97/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2247.2009 - val_loss: 1741.0358\n",
      "Epoch 98/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2223.8015 - val_loss: 1720.4198\n",
      "Epoch 99/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2200.5754 - val_loss: 1699.9703\n",
      "Epoch 100/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2177.5212 - val_loss: 1679.6865\n",
      "Epoch 101/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2154.6387 - val_loss: 1659.5680\n",
      "Epoch 102/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 2131.9260 - val_loss: 1639.6134\n",
      "Epoch 103/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2109.3831 - val_loss: 1619.8225\n",
      "Epoch 104/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2087.0090 - val_loss: 1600.1938\n",
      "Epoch 105/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2064.8032 - val_loss: 1580.7273\n",
      "Epoch 106/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2042.7639 - val_loss: 1561.4216\n",
      "Epoch 107/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2020.8916 - val_loss: 1542.2764\n",
      "Epoch 108/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1999.1842 - val_loss: 1523.2898\n",
      "Epoch 109/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1977.6421 - val_loss: 1504.4629\n",
      "Epoch 110/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1956.2639 - val_loss: 1485.7928\n",
      "Epoch 111/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1935.0486 - val_loss: 1467.2804\n",
      "Epoch 112/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1913.9960 - val_loss: 1448.9235\n",
      "Epoch 113/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1893.1051 - val_loss: 1430.7228\n",
      "Epoch 114/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1872.3750 - val_loss: 1412.6769\n",
      "Epoch 115/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1851.8051 - val_loss: 1394.7842\n",
      "Epoch 116/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1831.3940 - val_loss: 1377.0454\n",
      "Epoch 117/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1811.1417 - val_loss: 1359.4581\n",
      "Epoch 118/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1791.0471 - val_loss: 1342.0233\n",
      "Epoch 119/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 1771.1097 - val_loss: 1324.7390\n",
      "Epoch 120/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1751.3289 - val_loss: 1307.6051\n",
      "Epoch 121/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1731.7029 - val_loss: 1290.6206\n",
      "Epoch 122/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1712.2321 - val_loss: 1273.7849\n",
      "Epoch 123/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1692.9159 - val_loss: 1257.0972\n",
      "Epoch 124/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1673.7524 - val_loss: 1240.5565\n",
      "Epoch 125/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1654.7423 - val_loss: 1224.1622\n",
      "Epoch 126/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1635.8832 - val_loss: 1207.9142\n",
      "Epoch 127/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1617.1757 - val_loss: 1191.8105\n",
      "Epoch 128/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1598.6183 - val_loss: 1175.8517\n",
      "Epoch 129/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1580.2113 - val_loss: 1160.0363\n",
      "Epoch 130/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1561.9528 - val_loss: 1144.3639\n",
      "Epoch 131/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1543.8427 - val_loss: 1128.8334\n",
      "Epoch 132/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1525.8805 - val_loss: 1113.4447\n",
      "Epoch 133/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1508.0651 - val_loss: 1098.1971\n",
      "Epoch 134/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1490.3959 - val_loss: 1083.0887\n",
      "Epoch 135/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1472.8723 - val_loss: 1068.1206\n",
      "Epoch 136/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1455.4935 - val_loss: 1053.2911\n",
      "Epoch 137/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1438.2585 - val_loss: 1038.5990\n",
      "Epoch 138/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1421.1674 - val_loss: 1024.0439\n",
      "Epoch 139/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1404.2186 - val_loss: 1009.6258\n",
      "Epoch 140/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1387.4125 - val_loss: 995.3443\n",
      "Epoch 141/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1370.7473 - val_loss: 981.1973\n",
      "Epoch 142/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1354.2231 - val_loss: 967.1850\n",
      "Epoch 143/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1337.8387 - val_loss: 953.3066\n",
      "Epoch 144/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1321.5939 - val_loss: 939.5612\n",
      "Epoch 145/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1305.4875 - val_loss: 925.9484\n",
      "Epoch 146/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1289.5192 - val_loss: 912.4678\n",
      "Epoch 147/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1273.6884 - val_loss: 899.1178\n",
      "Epoch 148/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1257.9941 - val_loss: 885.8983\n",
      "Epoch 149/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1242.4358 - val_loss: 872.8091\n",
      "Epoch 150/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1227.0132 - val_loss: 859.8483\n",
      "Epoch 151/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1211.7249 - val_loss: 847.0161\n",
      "Epoch 152/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1196.5706 - val_loss: 834.3120\n",
      "Epoch 153/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 1181.5500 - val_loss: 821.7350\n",
      "Epoch 154/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1166.6620 - val_loss: 809.2847\n",
      "Epoch 155/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1151.9060 - val_loss: 796.9599\n",
      "Epoch 156/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1137.2814 - val_loss: 784.7603\n",
      "Epoch 157/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1122.7880 - val_loss: 772.6852\n",
      "Epoch 158/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1108.4246 - val_loss: 760.7343\n",
      "Epoch 159/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1094.1907 - val_loss: 748.9064\n",
      "Epoch 160/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1080.0854 - val_loss: 737.2007\n",
      "Epoch 161/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1066.1084 - val_loss: 725.6178\n",
      "Epoch 162/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1052.2593 - val_loss: 714.1551\n",
      "Epoch 163/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1038.5370 - val_loss: 702.8134\n",
      "Epoch 164/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1024.9409 - val_loss: 691.5916\n",
      "Epoch 165/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1011.4705 - val_loss: 680.4892\n",
      "Epoch 166/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 998.1251 - val_loss: 669.5054\n",
      "Epoch 167/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 984.9042 - val_loss: 658.6396\n",
      "Epoch 168/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 971.8073 - val_loss: 647.8911\n",
      "Epoch 169/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 958.8333 - val_loss: 637.2601\n",
      "Epoch 170/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 945.9821 - val_loss: 626.7440\n",
      "Epoch 171/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 933.2522 - val_loss: 616.3436\n",
      "Epoch 172/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 920.6437 - val_loss: 606.0590\n",
      "Epoch 173/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 908.1563 - val_loss: 595.8872\n",
      "Epoch 174/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 895.7886 - val_loss: 585.8297\n",
      "Epoch 175/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 883.5402 - val_loss: 575.8854\n",
      "Epoch 176/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 871.4106 - val_loss: 566.0526\n",
      "Epoch 177/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 859.3993 - val_loss: 556.3318\n",
      "Epoch 178/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 847.5059 - val_loss: 546.7222\n",
      "Epoch 179/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 835.7287 - val_loss: 537.2223\n",
      "Epoch 180/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 824.0683 - val_loss: 527.8324\n",
      "Epoch 181/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 812.5232 - val_loss: 518.5516\n",
      "Epoch 182/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 801.0933 - val_loss: 509.3795\n",
      "Epoch 183/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 789.7778 - val_loss: 500.3147\n",
      "Epoch 184/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 778.5762 - val_loss: 491.3576\n",
      "Epoch 185/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 767.4877 - val_loss: 482.5067\n",
      "Epoch 186/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 756.5120 - val_loss: 473.7622\n",
      "Epoch 187/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 745.6483 - val_loss: 465.1223\n",
      "Epoch 188/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 734.8956 - val_loss: 456.5876\n",
      "Epoch 189/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 724.2541 - val_loss: 448.1564\n",
      "Epoch 190/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 713.7225 - val_loss: 439.8290\n",
      "Epoch 191/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 703.3006 - val_loss: 431.6045\n",
      "Epoch 192/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 692.9877 - val_loss: 423.4819\n",
      "Epoch 193/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 682.7831 - val_loss: 415.4612\n",
      "Epoch 194/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 672.6862 - val_loss: 407.5408\n",
      "Epoch 195/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 662.6960 - val_loss: 399.7204\n",
      "Epoch 196/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 652.8126 - val_loss: 392.0004\n",
      "Epoch 197/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 643.0352 - val_loss: 384.3789\n",
      "Epoch 198/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 633.3630 - val_loss: 376.8564\n",
      "Epoch 199/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 623.7955 - val_loss: 369.4310\n",
      "Epoch 200/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 614.3318 - val_loss: 362.1029\n",
      "Epoch 201/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 604.9717 - val_loss: 354.8713\n",
      "Epoch 202/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 595.7145 - val_loss: 347.7357\n",
      "Epoch 203/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 586.5597 - val_loss: 340.6955\n",
      "Epoch 204/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 577.5065 - val_loss: 333.7498\n",
      "Epoch 205/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 568.5543 - val_loss: 326.8979\n",
      "Epoch 206/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 559.7028 - val_loss: 320.1395\n",
      "Epoch 207/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 550.9509 - val_loss: 313.4736\n",
      "Epoch 208/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 542.2983 - val_loss: 306.9003\n",
      "Epoch 209/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 533.7444 - val_loss: 300.4180\n",
      "Epoch 210/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 525.2884 - val_loss: 294.0268\n",
      "Epoch 211/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 516.9298 - val_loss: 287.7256\n",
      "Epoch 212/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 508.6679 - val_loss: 281.5141\n",
      "Epoch 213/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 500.5023 - val_loss: 275.3916\n",
      "Epoch 214/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 492.4324 - val_loss: 269.3575\n",
      "Epoch 215/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 484.4575 - val_loss: 263.4108\n",
      "Epoch 216/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 476.5768 - val_loss: 257.5515\n",
      "Epoch 217/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 468.7900 - val_loss: 251.7785\n",
      "Epoch 218/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 461.0963 - val_loss: 246.0911\n",
      "Epoch 219/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 453.4952 - val_loss: 240.4893\n",
      "Epoch 220/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 445.9860 - val_loss: 234.9715\n",
      "Epoch 221/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 438.5682 - val_loss: 229.5379\n",
      "Epoch 222/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 431.2413 - val_loss: 224.1879\n",
      "Epoch 223/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 424.0045 - val_loss: 218.9204\n",
      "Epoch 224/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 416.8573 - val_loss: 213.7350\n",
      "Epoch 225/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 409.7992 - val_loss: 208.6309\n",
      "Epoch 226/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 402.8294 - val_loss: 203.6077\n",
      "Epoch 227/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 395.9473 - val_loss: 198.6645\n",
      "Epoch 228/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 389.1521 - val_loss: 193.8004\n",
      "Epoch 229/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 382.4436 - val_loss: 189.0158\n",
      "Epoch 230/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 375.8211 - val_loss: 184.3091\n",
      "Epoch 231/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 369.2839 - val_loss: 179.6802\n",
      "Epoch 232/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 362.8316 - val_loss: 175.1284\n",
      "Epoch 233/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 356.4633 - val_loss: 170.6529\n",
      "Epoch 234/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 350.1785 - val_loss: 166.2531\n",
      "Epoch 235/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 343.9767 - val_loss: 161.9283\n",
      "Epoch 236/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 337.8572 - val_loss: 157.6780\n",
      "Epoch 237/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 331.8193 - val_loss: 153.5016\n",
      "Epoch 238/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 325.8627 - val_loss: 149.3982\n",
      "Epoch 239/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 319.9863 - val_loss: 145.3674\n",
      "Epoch 240/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 314.1898 - val_loss: 141.4085\n",
      "Epoch 241/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 308.4727 - val_loss: 137.5206\n",
      "Epoch 242/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 302.8341 - val_loss: 133.7037\n",
      "Epoch 243/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 297.2738 - val_loss: 129.9566\n",
      "Epoch 244/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 291.7909 - val_loss: 126.2791\n",
      "Epoch 245/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 286.3849 - val_loss: 122.6702\n",
      "Epoch 246/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 281.0551 - val_loss: 119.1295\n",
      "Epoch 247/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 275.8008 - val_loss: 115.6560\n",
      "Epoch 248/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 270.6217 - val_loss: 112.2494\n",
      "Epoch 249/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 265.5169 - val_loss: 108.9091\n",
      "Epoch 250/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 260.4859 - val_loss: 105.6340\n",
      "Epoch 251/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 255.5279 - val_loss: 102.4238\n",
      "Epoch 252/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 250.6424 - val_loss: 99.2781\n",
      "Epoch 253/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 245.8292 - val_loss: 96.1959\n",
      "Epoch 254/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 241.0873 - val_loss: 93.1768\n",
      "Epoch 255/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 236.4162 - val_loss: 90.2202\n",
      "Epoch 256/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 231.8155 - val_loss: 87.3250\n",
      "Epoch 257/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 227.2841 - val_loss: 84.4909\n",
      "Epoch 258/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 222.8217 - val_loss: 81.7174\n",
      "Epoch 259/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 218.4276 - val_loss: 79.0035\n",
      "Epoch 260/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 214.1011 - val_loss: 76.3487\n",
      "Epoch 261/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 209.8416 - val_loss: 73.7524\n",
      "Epoch 262/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 205.6488 - val_loss: 71.2141\n",
      "Epoch 263/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 201.5219 - val_loss: 68.7329\n",
      "Epoch 264/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 197.4602 - val_loss: 66.3084\n",
      "Epoch 265/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 193.4635 - val_loss: 63.9399\n",
      "Epoch 266/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 189.5305 - val_loss: 61.6265\n",
      "Epoch 267/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 185.6611 - val_loss: 59.3678\n",
      "Epoch 268/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 181.8545 - val_loss: 57.1633\n",
      "Epoch 269/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 178.1102 - val_loss: 55.0123\n",
      "Epoch 270/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 174.4277 - val_loss: 52.9137\n",
      "Epoch 271/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 170.8061 - val_loss: 50.8675\n",
      "Epoch 272/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 167.2448 - val_loss: 48.8727\n",
      "Epoch 273/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 163.7436 - val_loss: 46.9286\n",
      "Epoch 274/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 160.3014 - val_loss: 45.0352\n",
      "Epoch 275/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 156.9181 - val_loss: 43.1911\n",
      "Epoch 276/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 153.5926 - val_loss: 41.3957\n",
      "Epoch 277/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 150.3245 - val_loss: 39.6489\n",
      "Epoch 278/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 147.1133 - val_loss: 37.9497\n",
      "Epoch 279/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 143.9582 - val_loss: 36.2975\n",
      "Epoch 280/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 140.8586 - val_loss: 34.6917\n",
      "Epoch 281/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 137.8142 - val_loss: 33.1319\n",
      "Epoch 282/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 134.8243 - val_loss: 31.6172\n",
      "Epoch 283/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 131.8882 - val_loss: 30.1471\n",
      "Epoch 284/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 129.0052 - val_loss: 28.7207\n",
      "Epoch 285/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 126.1748 - val_loss: 27.3377\n",
      "Epoch 286/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 123.3965 - val_loss: 25.9974\n",
      "Epoch 287/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 120.6697 - val_loss: 24.6992\n",
      "Epoch 288/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 117.9936 - val_loss: 23.4422\n",
      "Epoch 289/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 115.3677 - val_loss: 22.2262\n",
      "Epoch 290/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 112.7914 - val_loss: 21.0503\n",
      "Epoch 291/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 110.2643 - val_loss: 19.9138\n",
      "Epoch 292/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 107.7854 - val_loss: 18.8165\n",
      "Epoch 293/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 105.3546 - val_loss: 17.7574\n",
      "Epoch 294/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 102.9708 - val_loss: 16.7359\n",
      "Epoch 295/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 100.6339 - val_loss: 15.7517\n",
      "Epoch 296/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 98.3433 - val_loss: 14.8040\n",
      "Epoch 297/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 96.0981 - val_loss: 13.8921\n",
      "Epoch 298/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 93.8978 - val_loss: 13.0154\n",
      "Epoch 299/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 91.7417 - val_loss: 12.1735\n",
      "Epoch 300/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 89.6296 - val_loss: 11.3656\n",
      "Epoch 301/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 87.5606 - val_loss: 10.5913\n",
      "Epoch 302/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 85.5343 - val_loss: 9.8498\n",
      "Epoch 303/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 83.5500 - val_loss: 9.1405\n",
      "Epoch 304/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 81.6072 - val_loss: 8.4630\n",
      "Epoch 305/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 79.7053 - val_loss: 7.8165\n",
      "Epoch 306/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 77.8439 - val_loss: 7.2007\n",
      "Epoch 307/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 76.0222 - val_loss: 6.6147\n",
      "Epoch 308/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 74.2397 - val_loss: 6.0581\n",
      "Epoch 309/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 72.4958 - val_loss: 5.5302\n",
      "Epoch 310/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 70.7900 - val_loss: 5.0305\n",
      "Epoch 311/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 69.1218 - val_loss: 4.5584\n",
      "Epoch 312/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 67.4906 - val_loss: 4.1134\n",
      "Epoch 313/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 65.8959 - val_loss: 3.6949\n",
      "Epoch 314/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 64.3369 - val_loss: 3.3022\n",
      "Epoch 315/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 62.8133 - val_loss: 2.9350\n",
      "Epoch 316/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 61.3245 - val_loss: 2.5925\n",
      "Epoch 317/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 59.8700 - val_loss: 2.2743\n",
      "Epoch 318/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 58.4492 - val_loss: 1.9798\n",
      "Epoch 319/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 57.0616 - val_loss: 1.7084\n",
      "Epoch 320/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 55.7065 - val_loss: 1.4597\n",
      "Epoch 321/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 54.3837 - val_loss: 1.2329\n",
      "Epoch 322/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 53.0924 - val_loss: 1.0278\n",
      "Epoch 323/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 51.8323 - val_loss: 0.8437\n",
      "Epoch 324/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 50.6027 - val_loss: 0.6800\n",
      "Epoch 325/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 49.4031 - val_loss: 0.5362\n",
      "Epoch 326/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 48.2331 - val_loss: 0.4120\n",
      "Epoch 327/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 47.0920 - val_loss: 0.3066\n",
      "Epoch 328/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 45.9794 - val_loss: 0.2196\n",
      "Epoch 329/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 44.8950 - val_loss: 0.1506\n",
      "Epoch 330/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 43.8379 - val_loss: 0.0989\n",
      "Epoch 331/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 42.8078 - val_loss: 0.0642\n",
      "Epoch 332/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 41.8042 - val_loss: 0.0458\n",
      "Epoch 333/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 40.8267 - val_loss: 0.0435\n",
      "Epoch 334/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 39.8747 - val_loss: 0.0565\n",
      "Epoch 335/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 38.9478 - val_loss: 0.0846\n",
      "Epoch 336/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 38.0455 - val_loss: 0.1271\n",
      "Epoch 337/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 37.1673 - val_loss: 0.1837\n",
      "Epoch 338/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 36.3127 - val_loss: 0.2538\n",
      "Epoch 339/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 35.4814 - val_loss: 0.3371\n",
      "Epoch 340/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 34.6727 - val_loss: 0.4330\n",
      "Epoch 341/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 33.8863 - val_loss: 0.5412\n",
      "Epoch 342/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 33.1218 - val_loss: 0.6612\n",
      "Epoch 343/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 32.3786 - val_loss: 0.7925\n",
      "Epoch 344/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 31.6564 - val_loss: 0.9347\n",
      "Epoch 345/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 30.9546 - val_loss: 1.0875\n",
      "Epoch 346/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 30.2730 - val_loss: 1.2503\n",
      "Epoch 347/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 29.6108 - val_loss: 1.4228\n",
      "Epoch 348/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 28.9681 - val_loss: 1.6046\n",
      "Epoch 349/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 28.3441 - val_loss: 1.7952\n",
      "Epoch 350/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 27.7383 - val_loss: 1.9943\n",
      "Epoch 351/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 27.1507 - val_loss: 2.2016\n",
      "Epoch 352/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 26.5806 - val_loss: 2.4165\n",
      "Epoch 353/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 26.0276 - val_loss: 2.6388\n",
      "Epoch 354/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.4915 - val_loss: 2.8680\n",
      "Epoch 355/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.9717 - val_loss: 3.1039\n",
      "Epoch 356/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.4680 - val_loss: 3.3460\n",
      "Epoch 357/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 23.9798 - val_loss: 3.5941\n",
      "Epoch 358/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 23.5069 - val_loss: 3.8476\n",
      "Epoch 359/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 23.0490 - val_loss: 4.1065\n",
      "Epoch 360/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 22.6055 - val_loss: 4.3702\n",
      "Epoch 361/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 22.1762 - val_loss: 4.6384\n",
      "Epoch 362/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 21.7607 - val_loss: 4.9110\n",
      "Epoch 363/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 21.3587 - val_loss: 5.1875\n",
      "Epoch 364/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 20.9698 - val_loss: 5.4676\n",
      "Epoch 365/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 20.5937 - val_loss: 5.7510\n",
      "Epoch 366/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 20.2301 - val_loss: 6.0375\n",
      "Epoch 367/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 19.8787 - val_loss: 6.3268\n",
      "Epoch 368/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 19.5390 - val_loss: 6.6185\n",
      "Epoch 369/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 19.2108 - val_loss: 6.9126\n",
      "Epoch 370/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.8938 - val_loss: 7.2086\n",
      "Epoch 371/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 18.5877 - val_loss: 7.5062\n",
      "Epoch 372/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.2922 - val_loss: 7.8053\n",
      "Epoch 373/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.0069 - val_loss: 8.1056\n",
      "Epoch 374/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 17.7317 - val_loss: 8.4068\n",
      "Epoch 375/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 17.4662 - val_loss: 8.7088\n",
      "Epoch 376/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 17.2101 - val_loss: 9.0113\n",
      "Epoch 377/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.9633 - val_loss: 9.3142\n",
      "Epoch 378/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 16.7253 - val_loss: 9.6170\n",
      "Epoch 379/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 16.4959 - val_loss: 9.9198\n",
      "Epoch 380/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 16.2749 - val_loss: 10.2222\n",
      "Epoch 381/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 16.0621 - val_loss: 10.5241\n",
      "Epoch 382/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.8571 - val_loss: 10.8253\n",
      "Epoch 383/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 15.6597 - val_loss: 11.1256\n",
      "Epoch 384/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.4698 - val_loss: 11.4249\n",
      "Epoch 385/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 15.2871 - val_loss: 11.7228\n",
      "Epoch 386/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 15.1113 - val_loss: 12.0194\n",
      "Epoch 387/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 14.9423 - val_loss: 12.3145\n",
      "Epoch 388/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 14.7798 - val_loss: 12.6079\n",
      "Epoch 389/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.6235 - val_loss: 12.8995\n",
      "Epoch 390/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4734 - val_loss: 13.1892\n",
      "Epoch 391/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.3291 - val_loss: 13.4767\n",
      "Epoch 392/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.1905 - val_loss: 13.7619\n",
      "Epoch 393/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 14.0575 - val_loss: 14.0449\n",
      "Epoch 394/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.9298 - val_loss: 14.3253\n",
      "Epoch 395/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 13.8072 - val_loss: 14.6033\n",
      "Epoch 396/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.6896 - val_loss: 14.8784\n",
      "Epoch 397/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 13.5767 - val_loss: 15.1509\n",
      "Epoch 398/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 13.4685 - val_loss: 15.4205\n",
      "Epoch 399/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.3648 - val_loss: 15.6870\n",
      "Epoch 400/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.2654 - val_loss: 15.9507\n",
      "Epoch 401/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.1700 - val_loss: 16.2112\n",
      "Epoch 402/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.0788 - val_loss: 16.4685\n",
      "Epoch 403/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 12.9913 - val_loss: 16.7227\n",
      "Epoch 404/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 12.9075 - val_loss: 16.9735\n",
      "Epoch 405/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 12.8274 - val_loss: 17.2207\n",
      "Epoch 406/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 12.7506 - val_loss: 17.4647\n",
      "Epoch 407/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.6772 - val_loss: 17.7052\n",
      "Epoch 408/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.6070 - val_loss: 17.9422\n",
      "Epoch 409/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.5398 - val_loss: 18.1756\n",
      "Epoch 410/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.4755 - val_loss: 18.4055\n",
      "Epoch 411/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.4141 - val_loss: 18.6318\n",
      "Epoch 412/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 12.3554 - val_loss: 18.8543\n",
      "Epoch 413/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 12.2993 - val_loss: 19.0732\n",
      "Epoch 414/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 12.2457 - val_loss: 19.2883\n",
      "Epoch 415/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 12.1945 - val_loss: 19.4999\n",
      "Epoch 416/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 12.1456 - val_loss: 19.7077\n",
      "Epoch 417/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.0989 - val_loss: 19.9118\n",
      "Epoch 418/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.0543 - val_loss: 20.1121\n",
      "Epoch 419/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.0118 - val_loss: 20.3087\n",
      "Epoch 420/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.9712 - val_loss: 20.5015\n",
      "Epoch 421/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.9325 - val_loss: 20.6905\n",
      "Epoch 422/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8956 - val_loss: 20.8759\n",
      "Epoch 423/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.8604 - val_loss: 21.0576\n",
      "Epoch 424/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.8268 - val_loss: 21.2356\n",
      "Epoch 425/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.7948 - val_loss: 21.4099\n",
      "Epoch 426/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7643 - val_loss: 21.5805\n",
      "Epoch 427/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7352 - val_loss: 21.7474\n",
      "Epoch 428/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7075 - val_loss: 21.9107\n",
      "Epoch 429/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.6811 - val_loss: 22.0706\n",
      "Epoch 430/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.6560 - val_loss: 22.2268\n",
      "Epoch 431/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.6320 - val_loss: 22.3793\n",
      "Epoch 432/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.6092 - val_loss: 22.5284\n",
      "Epoch 433/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.5875 - val_loss: 22.6741\n",
      "Epoch 434/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.5668 - val_loss: 22.8161\n",
      "Epoch 435/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.5472 - val_loss: 22.9550\n",
      "Epoch 436/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.5285 - val_loss: 23.0903\n",
      "Epoch 437/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.5106 - val_loss: 23.2224\n",
      "Epoch 438/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.4937 - val_loss: 23.3511\n",
      "Epoch 439/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.4775 - val_loss: 23.4766\n",
      "Epoch 440/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.4622 - val_loss: 23.5990\n",
      "Epoch 441/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.4476 - val_loss: 23.7181\n",
      "Epoch 442/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.4337 - val_loss: 23.8340\n",
      "Epoch 443/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.4205 - val_loss: 23.9471\n",
      "Epoch 444/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.4079 - val_loss: 24.0569\n",
      "Epoch 445/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 11.3960 - val_loss: 24.1636\n",
      "Epoch 446/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.3846 - val_loss: 24.2676\n",
      "Epoch 447/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.3738 - val_loss: 24.3687\n",
      "Epoch 448/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.3636 - val_loss: 24.4669\n",
      "Epoch 449/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.3538 - val_loss: 24.5623\n",
      "Epoch 450/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.3445 - val_loss: 24.6551\n",
      "Epoch 451/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3357 - val_loss: 24.7450\n",
      "Epoch 452/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3273 - val_loss: 24.8322\n",
      "Epoch 453/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3193 - val_loss: 24.9170\n",
      "Epoch 454/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3117 - val_loss: 24.9993\n",
      "Epoch 455/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.3045 - val_loss: 25.0788\n",
      "Epoch 456/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.2977 - val_loss: 25.1562\n",
      "Epoch 457/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.2912 - val_loss: 25.2310\n",
      "Epoch 458/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.2850 - val_loss: 25.3035\n",
      "Epoch 459/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.2791 - val_loss: 25.3738\n",
      "Epoch 460/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.2736 - val_loss: 25.4417\n",
      "Epoch 461/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.2683 - val_loss: 25.5075\n",
      "Epoch 462/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.2632 - val_loss: 25.5713\n",
      "Epoch 463/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.2584 - val_loss: 25.6328\n",
      "Epoch 464/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 11.2539 - val_loss: 25.6924\n",
      "Epoch 465/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.2496 - val_loss: 25.7500\n",
      "Epoch 466/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.2455 - val_loss: 25.8057\n",
      "Epoch 467/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.2416 - val_loss: 25.8596\n",
      "Epoch 468/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.2378 - val_loss: 25.9115\n",
      "Epoch 469/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.2343 - val_loss: 25.9616\n",
      "Epoch 470/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.2310 - val_loss: 26.0100\n",
      "Epoch 471/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.2278 - val_loss: 26.0567\n",
      "Epoch 472/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.2248 - val_loss: 26.1018\n",
      "Epoch 473/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.2219 - val_loss: 26.1452\n",
      "Epoch 474/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.2192 - val_loss: 26.1871\n",
      "Epoch 475/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.2166 - val_loss: 26.2273\n",
      "Epoch 476/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.2142 - val_loss: 26.2662\n",
      "Epoch 477/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.2118 - val_loss: 26.3035\n",
      "Epoch 478/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.2096 - val_loss: 26.3396\n",
      "Epoch 479/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.2075 - val_loss: 26.3742\n",
      "Epoch 480/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.2055 - val_loss: 26.4074\n",
      "Epoch 481/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.2037 - val_loss: 26.4395\n",
      "Epoch 482/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.2019 - val_loss: 26.4703\n",
      "Epoch 483/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.2002 - val_loss: 26.4998\n",
      "Epoch 484/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.1986 - val_loss: 26.5283\n",
      "Epoch 485/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.1970 - val_loss: 26.5556\n",
      "Epoch 486/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.1956 - val_loss: 26.5819\n",
      "Epoch 487/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.1942 - val_loss: 26.6069\n",
      "Epoch 488/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.1930 - val_loss: 26.6312\n",
      "Epoch 489/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.1917 - val_loss: 26.6544\n",
      "Epoch 490/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.1905 - val_loss: 26.6765\n",
      "Epoch 491/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.1895 - val_loss: 26.6979\n",
      "Epoch 492/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.1884 - val_loss: 26.7182\n",
      "Epoch 493/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.1874 - val_loss: 26.7377\n",
      "Epoch 494/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.1865 - val_loss: 26.7564\n",
      "Epoch 495/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.1856 - val_loss: 26.7745\n",
      "Epoch 496/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.1848 - val_loss: 26.7916\n",
      "Epoch 497/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.1840 - val_loss: 26.8079\n",
      "Epoch 498/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.1833 - val_loss: 26.8236\n",
      "Epoch 499/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.1826 - val_loss: 26.8387\n",
      "Epoch 500/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.1819 - val_loss: 26.8530\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6856, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0008, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0128, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(-2.3003, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 317ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.04228992e+01, 7.04144958e+01, 7.04060924e+01, 7.03976891e+01,\n",
       "        7.03892857e+01, 7.03808823e+01, 7.03724790e+01, 7.03640756e+01,\n",
       "        7.03556723e+01, 7.03472689e+01, 7.03388655e+01, 7.03304622e+01,\n",
       "        7.03220588e+01, 7.03136555e+01, 7.03052521e+01, 7.02968487e+01,\n",
       "        7.02884454e+01, 7.02800420e+01, 7.02716387e+01, 7.02632353e+01,\n",
       "        7.02548319e+01, 7.02464286e+01, 7.02380252e+01, 7.02296219e+01,\n",
       "        7.02212185e+01, 7.02128151e+01, 7.02044118e+01, 7.01960084e+01,\n",
       "        7.01876050e+01, 7.01792017e+01, 7.01707983e+01, 7.01623950e+01,\n",
       "        7.01539916e+01, 7.01455882e+01, 7.01371849e+01, 7.01287815e+01,\n",
       "        7.01203781e+01, 7.01119748e+01, 7.01035714e+01, 7.00951681e+01,\n",
       "        7.00867647e+01, 7.00783613e+01, 7.00699580e+01, 7.00615546e+01,\n",
       "        7.00531513e+01, 7.00447479e+01, 7.00363445e+01, 7.00279412e+01,\n",
       "        7.00195378e+01, 7.00111345e+01, 7.00027311e+01, 6.99943277e+01,\n",
       "        6.99859244e+01, 6.99775210e+01, 6.99691177e+01, 6.99607143e+01,\n",
       "        6.99523109e+01, 6.99439076e+01, 6.99355042e+01, 6.99271008e+01,\n",
       "        6.99186975e+01, 6.99102941e+01, 6.99018908e+01, 6.98934874e+01,\n",
       "        6.98850840e+01, 6.98766807e+01, 6.98682773e+01, 6.98598739e+01,\n",
       "        6.98514706e+01, 6.98430672e+01, 6.98346639e+01, 6.98262605e+01,\n",
       "        6.98178571e+01, 6.98094538e+01, 6.98010504e+01, 6.97509804e+01,\n",
       "        6.96949580e+01, 6.96389356e+01, 6.95829132e+01, 6.95268908e+01,\n",
       "        7.52419281e+01, 3.72043848e-01, 2.08795503e-01, 1.14993483e-01,\n",
       "        1.93131804e-01, 2.78723180e-01, 2.48600349e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.49832010e-01,\n",
       "        0.00000000e+00, 3.28298062e-02, 3.13602656e-01, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([66.7044141 , 66.69386321, 66.68331232, 66.67276144, 66.66221055,\n",
       "       66.65165966, 66.64110878, 66.63055789, 66.620007  , 66.60945612,\n",
       "       66.59890523, 66.58835434, 66.57780345, 66.56725257, 66.55670168,\n",
       "       66.54615079, 66.53559991, 66.52504902, 66.51449813, 66.50394725,\n",
       "       66.49339636, 66.48284547, 66.47229458, 66.4617437 , 66.45119281,\n",
       "       66.44064192, 66.43009104, 66.41954015, 66.40898926, 66.39843838,\n",
       "       66.38788749, 66.3773366 , 66.36678571, 66.35623483, 66.34568394,\n",
       "       66.33513305, 66.32458217, 66.31403128, 66.30348039, 66.29292951,\n",
       "       66.28237862, 66.27182773, 66.26127684, 66.25072596, 66.24017507,\n",
       "       66.22962418, 66.2190733 , 66.20852241, 66.19797152, 66.18742063,\n",
       "       66.17686975, 66.16631886, 66.15576797, 66.14521709, 66.1346662 ,\n",
       "       66.12411531, 66.11356443, 66.10301354, 66.09246265, 66.08191176,\n",
       "       66.07136088, 66.06080999, 66.0502591 , 66.03970822, 66.02915733,\n",
       "       66.01860644, 66.00805556, 65.99750467, 65.98695378, 65.97640289,\n",
       "       65.96585201, 65.95530112, 65.94475023, 65.93419935, 65.92364846,\n",
       "       65.91309757, 65.90254669, 65.8919958 , 65.88144491, 65.87089402,\n",
       "       65.86034314, 65.84979225, 65.83924136, 65.82869048, 65.81813959,\n",
       "       65.8075887 , 65.79703782, 65.78648693, 65.77593604, 65.76538515,\n",
       "       65.75483427, 65.74428338, 65.73373249, 65.72318161, 65.71263072,\n",
       "       65.70207983, 65.69152894, 65.68097806, 65.67042717, 65.65987628])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.833904216783637\n",
      "15.603913455807437\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
