{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "2295    57.508711\n",
       "2296    57.496413\n",
       "2297    57.484116\n",
       "2298    57.471818\n",
       "2299    57.459520\n",
       "Name: C4, Length: 2300, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c4_interpolated_2200_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "2195     0.000000\n",
       "2196     0.026960\n",
       "2197     0.000000\n",
       "2198     0.000000\n",
       "2199     0.257641\n",
       "Name: C4, Length: 2200, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2200)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.409524</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.219048</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.028571</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.838095</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.600000  0.000298\n",
       "1     88.409524  0.000298\n",
       "2     88.219048  0.000297\n",
       "3     88.028571  0.000297\n",
       "4     87.838095  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6856  # Value for C0\n",
    "K0 = -0.0008  # Value for K0\n",
    "K1 = -0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0128    # Value for b\n",
    "c = -2.3003    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.600000    88.409524    88.219048    88.028571    87.838095   \n",
      "351    88.409524    88.219048    88.028571    87.838095    87.647619   \n",
      "352    88.219048    88.028571    87.838095    87.647619    87.457143   \n",
      "353    88.028571    87.838095    87.647619    87.457143    87.266667   \n",
      "354    87.838095    87.647619    87.457143    87.266667    87.076190   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.647619    87.457143    87.266667    87.076190    86.896218  ...   \n",
      "351    87.457143    87.266667    87.076190    86.896218    86.845798  ...   \n",
      "352    87.266667    87.076190    86.896218    86.845798    86.795378  ...   \n",
      "353    87.076190    86.896218    86.845798    86.795378    86.744958  ...   \n",
      "354    86.896218    86.845798    86.795378    86.744958    86.694538  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   78.880345    0.000263   78.856069    0.000263   78.831793    0.000263   \n",
      "351   78.856069    0.000263   78.831793    0.000263   78.807516    0.000262   \n",
      "352   78.831793    0.000263   78.807516    0.000262   78.783240    0.000262   \n",
      "353   78.807516    0.000262   78.783240    0.000262   78.758964    0.000262   \n",
      "354   78.783240    0.000262   78.758964    0.000262   78.734687    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   78.807516    0.000262   78.783240    0.000262  \n",
      "351   78.783240    0.000262   78.758964    0.000262  \n",
      "352   78.758964    0.000262   78.734687    0.000262  \n",
      "353   78.734687    0.000262   78.710411    0.000262  \n",
      "354   78.710411    0.000262   78.686134    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1750, 1, 251) (1750, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 2s 22ms/step - loss: 5172.1870 - val_loss: 3362.5496\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4975.4551 - val_loss: 3224.5239\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4851.4468 - val_loss: 3156.5007\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4768.4463 - val_loss: 3103.2532\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4689.5449 - val_loss: 3052.0627\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4612.6909 - val_loss: 3001.9099\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4529.4541 - val_loss: 2949.2815\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4450.0327 - val_loss: 2894.2336\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4373.1978 - val_loss: 2844.8667\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4298.1587 - val_loss: 2793.2869\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4208.1841 - val_loss: 2736.6921\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4130.3657 - val_loss: 2687.3789\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4054.7947 - val_loss: 2639.6694\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3981.1443 - val_loss: 2593.2690\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3909.1113 - val_loss: 2547.9854\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3838.3567 - val_loss: 2503.7417\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3768.9006 - val_loss: 2460.4604\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3700.6172 - val_loss: 2418.0957\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3633.4436 - val_loss: 2376.6101\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3567.3362 - val_loss: 2335.9744\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3502.2590 - val_loss: 2296.1638\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3438.1792 - val_loss: 2257.1582\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3375.0740 - val_loss: 2218.9387\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3312.9197 - val_loss: 2181.4900\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3251.6960 - val_loss: 2144.7961\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3191.3857 - val_loss: 2108.8438\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3131.9724 - val_loss: 2073.6211\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3073.4412 - val_loss: 2039.1154\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3015.7783 - val_loss: 2005.3160\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2958.9697 - val_loss: 1972.2119\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2903.0046 - val_loss: 1939.7931\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2847.8699 - val_loss: 1908.0493\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2793.5549 - val_loss: 1876.9712\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2740.0500 - val_loss: 1846.5494\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2687.3435 - val_loss: 1816.7736\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2635.4253 - val_loss: 1787.6343\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2584.2864 - val_loss: 1759.1185\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2533.9177 - val_loss: 1731.2084\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 2484.3096 - val_loss: 1703.7870\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2435.4431 - val_loss: 1679.7661\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2387.3413 - val_loss: 1652.4735\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2339.9631 - val_loss: 1627.0132\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2293.3115 - val_loss: 1602.1384\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2247.3784 - val_loss: 1577.8419\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2202.1553 - val_loss: 1554.1165\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2157.6345 - val_loss: 1530.9557\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2113.8096 - val_loss: 1508.3514\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2070.6711 - val_loss: 1486.2975\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2028.2119 - val_loss: 1464.7867\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1986.4257 - val_loss: 1443.8126\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1945.3041 - val_loss: 1423.3685\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1904.8403 - val_loss: 1403.4473\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1865.0271 - val_loss: 1384.0431\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1825.8569 - val_loss: 1365.1487\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1787.3241 - val_loss: 1346.7581\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1749.4209 - val_loss: 1328.8645\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1712.1404 - val_loss: 1311.4619\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1675.4762 - val_loss: 1294.5442\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1639.4213 - val_loss: 1278.1045\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1603.9695 - val_loss: 1262.1371\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1569.1138 - val_loss: 1246.6357\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1534.8479 - val_loss: 1231.5939\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1501.1655 - val_loss: 1217.0065\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1468.0603 - val_loss: 1202.8662\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1435.5259 - val_loss: 1189.1680\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1403.5557 - val_loss: 1175.9054\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1372.1440 - val_loss: 1163.0726\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1341.2836 - val_loss: 1150.6638\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1310.9696 - val_loss: 1138.6729\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1281.1954 - val_loss: 1127.0946\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1251.9550 - val_loss: 1115.9224\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1223.2419 - val_loss: 1105.1509\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1195.0508 - val_loss: 1094.7743\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1167.3755 - val_loss: 1084.7871\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 1140.2097 - val_loss: 1075.1827\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1113.5482 - val_loss: 1065.9568\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1087.3848 - val_loss: 1057.1028\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1061.7140 - val_loss: 1048.6152\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1036.5300 - val_loss: 1040.4886\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1011.8268 - val_loss: 1032.7173\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 987.5986 - val_loss: 1025.2960\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 963.8399 - val_loss: 1018.2189\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 940.5455 - val_loss: 1011.4803\n",
      "Epoch 84/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 917.7094 - val_loss: 1005.0753\n",
      "Epoch 85/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 895.3262 - val_loss: 998.9979\n",
      "Epoch 86/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 873.3897 - val_loss: 993.2430\n",
      "Epoch 87/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 851.8953 - val_loss: 987.8048\n",
      "Epoch 88/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 830.8372 - val_loss: 982.6781\n",
      "Epoch 89/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 810.2097 - val_loss: 977.8577\n",
      "Epoch 90/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 790.0073 - val_loss: 973.3378\n",
      "Epoch 91/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 770.2252 - val_loss: 969.1135\n",
      "Epoch 92/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 750.8576 - val_loss: 965.1791\n",
      "Epoch 93/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 731.8992 - val_loss: 961.5297\n",
      "Epoch 94/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 713.3446 - val_loss: 958.1593\n",
      "Epoch 95/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 695.1884 - val_loss: 955.0630\n",
      "Epoch 96/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 677.4256 - val_loss: 952.2359\n",
      "Epoch 97/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 660.0505 - val_loss: 949.6721\n",
      "Epoch 98/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 643.0584 - val_loss: 947.3668\n",
      "Epoch 99/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 626.4436 - val_loss: 945.3146\n",
      "Epoch 100/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 610.2015 - val_loss: 943.5104\n",
      "Epoch 101/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 594.3262 - val_loss: 941.9489\n",
      "Epoch 102/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 578.8129 - val_loss: 940.6248\n",
      "Epoch 103/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 563.6565 - val_loss: 939.5335\n",
      "Epoch 104/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 548.8517 - val_loss: 938.6691\n",
      "Epoch 105/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 534.3936 - val_loss: 938.0270\n",
      "Epoch 106/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 520.2773 - val_loss: 937.6021\n",
      "Epoch 107/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 506.4972 - val_loss: 937.3892\n",
      "Epoch 108/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 493.0488 - val_loss: 937.3833\n",
      "Epoch 109/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 479.9269 - val_loss: 937.5791\n",
      "Epoch 110/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 467.1263 - val_loss: 937.9719\n",
      "Epoch 111/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 454.6423 - val_loss: 938.5566\n",
      "Epoch 112/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 442.4701 - val_loss: 939.3281\n",
      "Epoch 113/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 430.6045 - val_loss: 940.2815\n",
      "Epoch 114/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 419.0408 - val_loss: 941.4120\n",
      "Epoch 115/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 407.7738 - val_loss: 942.7145\n",
      "Epoch 116/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 396.7990 - val_loss: 944.1843\n",
      "Epoch 117/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 386.1115 - val_loss: 945.8162\n",
      "Epoch 118/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 375.7062 - val_loss: 947.6057\n",
      "Epoch 119/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 365.5788 - val_loss: 949.5477\n",
      "Epoch 120/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 355.7242 - val_loss: 951.6376\n",
      "Epoch 121/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 346.1376 - val_loss: 953.8704\n",
      "Epoch 122/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 336.8146 - val_loss: 956.2416\n",
      "Epoch 123/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 327.7503 - val_loss: 958.7464\n",
      "Epoch 124/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 318.9401 - val_loss: 961.3801\n",
      "Epoch 125/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 310.3794 - val_loss: 964.1377\n",
      "Epoch 126/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 302.0634 - val_loss: 967.0154\n",
      "Epoch 127/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 293.9878 - val_loss: 970.0079\n",
      "Epoch 128/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 286.1479 - val_loss: 973.1109\n",
      "Epoch 129/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 278.5391 - val_loss: 976.3199\n",
      "Epoch 130/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 271.1570 - val_loss: 979.6304\n",
      "Epoch 131/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 263.9970 - val_loss: 983.0381\n",
      "Epoch 132/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 257.0547 - val_loss: 986.5410\n",
      "Epoch 133/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 250.3259 - val_loss: 990.2953\n",
      "Epoch 134/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 243.8058 - val_loss: 993.7016\n",
      "Epoch 135/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 237.4902 - val_loss: 997.4493\n",
      "Epoch 136/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 231.3750 - val_loss: 1001.2720\n",
      "Epoch 137/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 225.4558 - val_loss: 1005.1650\n",
      "Epoch 138/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 219.7281 - val_loss: 1009.1246\n",
      "Epoch 139/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 214.1880 - val_loss: 1013.1465\n",
      "Epoch 140/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 208.8312 - val_loss: 1017.2266\n",
      "Epoch 141/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 203.6533 - val_loss: 1021.3609\n",
      "Epoch 142/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 198.6504 - val_loss: 1025.5452\n",
      "Epoch 143/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 193.8184 - val_loss: 1029.7760\n",
      "Epoch 144/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 189.1532 - val_loss: 1034.0483\n",
      "Epoch 145/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 184.6510 - val_loss: 1038.3593\n",
      "Epoch 146/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 180.3075 - val_loss: 1042.7047\n",
      "Epoch 147/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 176.1190 - val_loss: 1047.0812\n",
      "Epoch 148/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 172.0815 - val_loss: 1051.4845\n",
      "Epoch 149/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 168.1913 - val_loss: 1055.9114\n",
      "Epoch 150/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 164.4444 - val_loss: 1060.3579\n",
      "Epoch 151/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 160.8373 - val_loss: 1064.8209\n",
      "Epoch 152/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 157.3660 - val_loss: 1069.2966\n",
      "Epoch 153/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 154.0268 - val_loss: 1073.7819\n",
      "Epoch 154/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 150.8163 - val_loss: 1078.2731\n",
      "Epoch 155/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 147.7310 - val_loss: 1082.7676\n",
      "Epoch 156/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 144.7672 - val_loss: 1087.2615\n",
      "Epoch 157/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 141.9213 - val_loss: 1091.7520\n",
      "Epoch 158/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 139.1901 - val_loss: 1096.2360\n",
      "Epoch 159/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 136.5699 - val_loss: 1100.7106\n",
      "Epoch 160/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 134.0577 - val_loss: 1105.1725\n",
      "Epoch 161/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 131.6501 - val_loss: 1109.6199\n",
      "Epoch 162/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 129.3438 - val_loss: 1114.0486\n",
      "Epoch 163/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 127.1357 - val_loss: 1118.4567\n",
      "Epoch 164/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 125.0227 - val_loss: 1122.8418\n",
      "Epoch 165/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 123.0016 - val_loss: 1127.2007\n",
      "Epoch 166/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 121.0695 - val_loss: 1131.5316\n",
      "Epoch 167/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 119.2235 - val_loss: 1135.8313\n",
      "Epoch 168/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 117.4605 - val_loss: 1140.0989\n",
      "Epoch 169/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 115.7777 - val_loss: 1144.3306\n",
      "Epoch 170/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 114.1724 - val_loss: 1148.5251\n",
      "Epoch 171/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 112.6418 - val_loss: 1152.6802\n",
      "Epoch 172/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 111.1833 - val_loss: 1156.7943\n",
      "Epoch 173/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 109.7942 - val_loss: 1160.8649\n",
      "Epoch 174/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 108.4719 - val_loss: 1164.8905\n",
      "Epoch 175/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 107.2140 - val_loss: 1168.8695\n",
      "Epoch 176/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 106.0179 - val_loss: 1172.8002\n",
      "Epoch 177/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 104.8814 - val_loss: 1176.6809\n",
      "Epoch 178/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 103.8019 - val_loss: 1180.5100\n",
      "Epoch 179/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 102.7774 - val_loss: 1184.2870\n",
      "Epoch 180/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 101.8055 - val_loss: 1188.0098\n",
      "Epoch 181/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 100.8841 - val_loss: 1191.6772\n",
      "Epoch 182/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 100.0112 - val_loss: 1195.2882\n",
      "Epoch 183/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 99.1847 - val_loss: 1198.8416\n",
      "Epoch 184/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 98.4025 - val_loss: 1202.3365\n",
      "Epoch 185/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 97.6628 - val_loss: 1205.7733\n",
      "Epoch 186/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 96.9635 - val_loss: 1209.1494\n",
      "Epoch 187/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 96.3031 - val_loss: 1212.4648\n",
      "Epoch 188/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 95.6798 - val_loss: 1215.7186\n",
      "Epoch 189/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 95.0918 - val_loss: 1218.9106\n",
      "Epoch 190/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 94.5374 - val_loss: 1222.0400\n",
      "Epoch 191/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 94.0151 - val_loss: 1225.1069\n",
      "Epoch 192/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 93.5234 - val_loss: 1228.1107\n",
      "Epoch 193/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 93.0606 - val_loss: 1231.0505\n",
      "Epoch 194/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.6256 - val_loss: 1233.9274\n",
      "Epoch 195/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 92.2168 - val_loss: 1236.7402\n",
      "Epoch 196/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 91.8328 - val_loss: 1239.4889\n",
      "Epoch 197/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 91.4726 - val_loss: 1242.1740\n",
      "Epoch 198/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 91.1347 - val_loss: 1244.7955\n",
      "Epoch 199/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 90.8181 - val_loss: 1247.3535\n",
      "Epoch 200/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 90.5216 - val_loss: 1249.8485\n",
      "Epoch 201/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 90.2440 - val_loss: 1252.2800\n",
      "Epoch 202/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 89.9844 - val_loss: 1254.6500\n",
      "Epoch 203/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 89.7418 - val_loss: 1256.9572\n",
      "Epoch 204/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 89.5151 - val_loss: 1259.2018\n",
      "Epoch 205/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 89.3036 - val_loss: 1261.3856\n",
      "Epoch 206/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 89.1063 - val_loss: 1263.5093\n",
      "Epoch 207/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.9223 - val_loss: 1265.5721\n",
      "Epoch 208/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.7509 - val_loss: 1267.5752\n",
      "Epoch 209/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.5914 - val_loss: 1269.5198\n",
      "Epoch 210/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 88.4430 - val_loss: 1271.4054\n",
      "Epoch 211/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.3052 - val_loss: 1273.2345\n",
      "Epoch 212/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.1770 - val_loss: 1275.0070\n",
      "Epoch 213/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.0580 - val_loss: 1276.7235\n",
      "Epoch 214/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 87.9476 - val_loss: 1278.3855\n",
      "Epoch 215/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 87.8452 - val_loss: 1279.9924\n",
      "Epoch 216/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 87.7504 - val_loss: 1281.5471\n",
      "Epoch 217/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 87.6626 - val_loss: 1283.0493\n",
      "Epoch 218/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 87.5813 - val_loss: 1284.5001\n",
      "Epoch 219/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 87.5062 - val_loss: 1285.9012\n",
      "Epoch 220/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 87.4367 - val_loss: 1287.2535\n",
      "Epoch 221/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 87.3725 - val_loss: 1288.5571\n",
      "Epoch 222/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 87.3132 - val_loss: 1289.8140\n",
      "Epoch 223/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 87.2585 - val_loss: 1291.0243\n",
      "Epoch 224/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 87.2082 - val_loss: 1292.1893\n",
      "Epoch 225/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 87.1618 - val_loss: 1293.3103\n",
      "Epoch 226/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 87.1191 - val_loss: 1294.3889\n",
      "Epoch 227/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 87.0797 - val_loss: 1295.4257\n",
      "Epoch 228/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 87.0434 - val_loss: 1296.4216\n",
      "Epoch 229/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 87.0102 - val_loss: 1297.3772\n",
      "Epoch 230/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 86.9796 - val_loss: 1298.2950\n",
      "Epoch 231/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.9515 - val_loss: 1299.1749\n",
      "Epoch 232/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.9258 - val_loss: 1300.0184\n",
      "Epoch 233/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.9021 - val_loss: 1300.8263\n",
      "Epoch 234/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.8804 - val_loss: 1301.6002\n",
      "Epoch 235/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.8605 - val_loss: 1302.3409\n",
      "Epoch 236/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.8423 - val_loss: 1303.0497\n",
      "Epoch 237/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.8256 - val_loss: 1303.7266\n",
      "Epoch 238/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 86.8104 - val_loss: 1304.3739\n",
      "Epoch 239/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 86.7964 - val_loss: 1304.9912\n",
      "Epoch 240/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.7836 - val_loss: 1305.5803\n",
      "Epoch 241/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7720 - val_loss: 1306.1425\n",
      "Epoch 242/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.7613 - val_loss: 1306.6785\n",
      "Epoch 243/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7516 - val_loss: 1307.1887\n",
      "Epoch 244/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 86.7428 - val_loss: 1307.6750\n",
      "Epoch 245/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.7347 - val_loss: 1308.1381\n",
      "Epoch 246/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7273 - val_loss: 1308.5782\n",
      "Epoch 247/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7205 - val_loss: 1308.9966\n",
      "Epoch 248/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7144 - val_loss: 1309.3931\n",
      "Epoch 249/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7088 - val_loss: 1309.7704\n",
      "Epoch 250/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7038 - val_loss: 1310.1282\n",
      "Epoch 251/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6992 - val_loss: 1310.4683\n",
      "Epoch 252/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6950 - val_loss: 1310.7902\n",
      "Epoch 253/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6913 - val_loss: 1311.0956\n",
      "Epoch 254/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6878 - val_loss: 1311.3845\n",
      "Epoch 255/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6847 - val_loss: 1311.6581\n",
      "Epoch 256/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6819 - val_loss: 1311.9163\n",
      "Epoch 257/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6794 - val_loss: 1312.1605\n",
      "Epoch 258/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6770 - val_loss: 1312.3911\n",
      "Epoch 259/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6750 - val_loss: 1312.6085\n",
      "Epoch 260/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6732 - val_loss: 1312.8143\n",
      "Epoch 261/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 86.6716 - val_loss: 1313.0081\n",
      "Epoch 262/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6701 - val_loss: 1313.1903\n",
      "Epoch 263/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6688 - val_loss: 1313.3625\n",
      "Epoch 264/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6676 - val_loss: 1313.5244\n",
      "Epoch 265/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6666 - val_loss: 1313.6770\n",
      "Epoch 266/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 86.6657 - val_loss: 1313.8201\n",
      "Epoch 267/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6649 - val_loss: 1313.9543\n",
      "Epoch 268/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6643 - val_loss: 1314.0809\n",
      "Epoch 269/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6637 - val_loss: 1314.1996\n",
      "Epoch 270/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6632 - val_loss: 1314.3114\n",
      "Epoch 271/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6628 - val_loss: 1314.4156\n",
      "Epoch 272/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6625 - val_loss: 1314.5127\n",
      "Epoch 273/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6622 - val_loss: 1314.6034\n",
      "Epoch 274/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6620 - val_loss: 1314.6895\n",
      "Epoch 275/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6619 - val_loss: 1314.7694\n",
      "Epoch 276/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6618 - val_loss: 1314.8439\n",
      "Epoch 277/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6618 - val_loss: 1314.9142\n",
      "Epoch 278/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6618 - val_loss: 1314.9792\n",
      "Epoch 279/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 86.6618 - val_loss: 1315.0398\n",
      "Epoch 280/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6620 - val_loss: 1315.0968\n",
      "Epoch 281/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 86.6621 - val_loss: 1315.1489\n",
      "Epoch 282/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6623 - val_loss: 1315.1980\n",
      "Epoch 283/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6625 - val_loss: 1315.2441\n",
      "Epoch 284/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6627 - val_loss: 1315.2858\n",
      "Epoch 285/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6630 - val_loss: 1315.3252\n",
      "Epoch 286/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6632 - val_loss: 1315.3613\n",
      "Epoch 287/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6635 - val_loss: 1315.3950\n",
      "Epoch 288/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6639 - val_loss: 1315.4270\n",
      "Epoch 289/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6642 - val_loss: 1315.4556\n",
      "Epoch 290/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6645 - val_loss: 1315.4824\n",
      "Epoch 291/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6649 - val_loss: 1315.5070\n",
      "Epoch 292/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6653 - val_loss: 1315.5297\n",
      "Epoch 293/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6657 - val_loss: 1315.5510\n",
      "Epoch 294/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6661 - val_loss: 1315.5704\n",
      "Epoch 295/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 86.6665 - val_loss: 1315.5884\n",
      "Epoch 296/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6669 - val_loss: 1315.6045\n",
      "Epoch 297/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6673 - val_loss: 1315.6193\n",
      "Epoch 298/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6678 - val_loss: 1315.6332\n",
      "Epoch 299/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6682 - val_loss: 1315.6455\n",
      "Epoch 300/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6687 - val_loss: 1315.6578\n",
      "Epoch 301/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6691 - val_loss: 1315.6680\n",
      "Epoch 302/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6696 - val_loss: 1315.6780\n",
      "Epoch 303/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6700 - val_loss: 1315.6863\n",
      "Epoch 304/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 86.6705 - val_loss: 1315.6945\n",
      "Epoch 305/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 86.6710 - val_loss: 1315.7010\n",
      "Epoch 306/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 86.6714 - val_loss: 1315.7078\n",
      "Epoch 307/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 86.6719 - val_loss: 1315.7129\n",
      "Epoch 308/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 86.6724 - val_loss: 1315.7179\n",
      "Epoch 309/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 86.6728 - val_loss: 1315.7225\n",
      "Epoch 310/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6733 - val_loss: 1315.7263\n",
      "Epoch 311/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6738 - val_loss: 1315.7299\n",
      "Epoch 312/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6743 - val_loss: 1315.7334\n",
      "Epoch 313/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6747 - val_loss: 1315.7363\n",
      "Epoch 314/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6752 - val_loss: 1315.7391\n",
      "Epoch 315/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6757 - val_loss: 1315.7415\n",
      "Epoch 316/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6761 - val_loss: 1315.7430\n",
      "Epoch 317/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6765 - val_loss: 1315.7449\n",
      "Epoch 318/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6769 - val_loss: 1315.7462\n",
      "Epoch 319/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6774 - val_loss: 1315.7472\n",
      "Epoch 320/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6779 - val_loss: 1315.7477\n",
      "Epoch 321/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 86.6783 - val_loss: 1315.7480\n",
      "Epoch 322/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6788 - val_loss: 1315.7483\n",
      "Epoch 323/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6792 - val_loss: 1315.7493\n",
      "Epoch 324/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6796 - val_loss: 1315.7491\n",
      "Epoch 325/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6801 - val_loss: 1315.7489\n",
      "Epoch 326/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6805 - val_loss: 1315.7495\n",
      "Epoch 327/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 86.6809 - val_loss: 1315.7488\n",
      "Epoch 328/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 86.6813 - val_loss: 1315.7480\n",
      "Epoch 329/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 86.6817 - val_loss: 1315.7476\n",
      "Epoch 330/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 86.6821 - val_loss: 1315.7471\n",
      "Epoch 331/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6825 - val_loss: 1315.7462\n",
      "Epoch 332/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6829 - val_loss: 1315.7455\n",
      "Epoch 333/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6833 - val_loss: 1315.7455\n",
      "Epoch 334/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 86.6837 - val_loss: 1315.7451\n",
      "Epoch 335/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6841 - val_loss: 1315.7445\n",
      "Epoch 336/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6845 - val_loss: 1315.7439\n",
      "Epoch 337/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6848 - val_loss: 1315.7428\n",
      "Epoch 338/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6852 - val_loss: 1315.7423\n",
      "Epoch 339/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6856 - val_loss: 1315.7408\n",
      "Epoch 340/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6859 - val_loss: 1315.7399\n",
      "Epoch 341/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6862 - val_loss: 1315.7390\n",
      "Epoch 342/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6866 - val_loss: 1315.7380\n",
      "Epoch 343/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6869 - val_loss: 1315.7372\n",
      "Epoch 344/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6872 - val_loss: 1315.7354\n",
      "Epoch 345/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6876 - val_loss: 1315.7335\n",
      "Epoch 346/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 86.6880 - val_loss: 1315.7332\n",
      "Epoch 347/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6882 - val_loss: 1315.7319\n",
      "Epoch 348/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 86.6886 - val_loss: 1315.7302\n",
      "Epoch 349/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6889 - val_loss: 1315.7285\n",
      "Epoch 350/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6892 - val_loss: 1315.7283\n",
      "Epoch 351/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6895 - val_loss: 1315.7273\n",
      "Epoch 352/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6899 - val_loss: 1315.7268\n",
      "Epoch 353/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6901 - val_loss: 1315.7260\n",
      "Epoch 354/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6904 - val_loss: 1315.7245\n",
      "Epoch 355/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6907 - val_loss: 1315.7239\n",
      "Epoch 356/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6910 - val_loss: 1315.7229\n",
      "Epoch 357/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6912 - val_loss: 1315.7216\n",
      "Epoch 358/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6915 - val_loss: 1315.7206\n",
      "Epoch 359/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6918 - val_loss: 1315.7198\n",
      "Epoch 360/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 86.6920 - val_loss: 1315.7181\n",
      "Epoch 361/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6923 - val_loss: 1315.7173\n",
      "Epoch 362/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6925 - val_loss: 1315.7159\n",
      "Epoch 363/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6928 - val_loss: 1315.7152\n",
      "Epoch 364/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6931 - val_loss: 1315.7142\n",
      "Epoch 365/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6933 - val_loss: 1315.7129\n",
      "Epoch 366/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6935 - val_loss: 1315.7115\n",
      "Epoch 367/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6937 - val_loss: 1315.7103\n",
      "Epoch 368/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6940 - val_loss: 1315.7100\n",
      "Epoch 369/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6942 - val_loss: 1315.7092\n",
      "Epoch 370/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 86.6945 - val_loss: 1315.7083\n",
      "Epoch 371/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 86.6947 - val_loss: 1315.7076\n",
      "Epoch 372/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6949 - val_loss: 1315.7073\n",
      "Epoch 373/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6951 - val_loss: 1315.7061\n",
      "Epoch 374/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6953 - val_loss: 1315.7052\n",
      "Epoch 375/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6955 - val_loss: 1315.7040\n",
      "Epoch 376/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6957 - val_loss: 1315.7036\n",
      "Epoch 377/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6959 - val_loss: 1315.7031\n",
      "Epoch 378/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6961 - val_loss: 1315.7014\n",
      "Epoch 379/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6963 - val_loss: 1315.7013\n",
      "Epoch 380/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6965 - val_loss: 1315.7004\n",
      "Epoch 381/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6966 - val_loss: 1315.6995\n",
      "Epoch 382/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6968 - val_loss: 1315.6981\n",
      "Epoch 383/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6970 - val_loss: 1315.6975\n",
      "Epoch 384/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 86.6972 - val_loss: 1315.6973\n",
      "Epoch 385/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6973 - val_loss: 1315.6967\n",
      "Epoch 386/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6975 - val_loss: 1315.6959\n",
      "Epoch 387/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6976 - val_loss: 1315.6948\n",
      "Epoch 388/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6979 - val_loss: 1315.6938\n",
      "Epoch 389/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6980 - val_loss: 1315.6935\n",
      "Epoch 390/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6982 - val_loss: 1315.6927\n",
      "Epoch 391/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6983 - val_loss: 1315.6912\n",
      "Epoch 392/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 86.6984 - val_loss: 1315.6907\n",
      "Epoch 393/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6986 - val_loss: 1315.6906\n",
      "Epoch 394/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6987 - val_loss: 1315.6899\n",
      "Epoch 395/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 86.6988 - val_loss: 1315.6895\n",
      "Epoch 396/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6990 - val_loss: 1315.6891\n",
      "Epoch 397/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6991 - val_loss: 1315.6879\n",
      "Epoch 398/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6993 - val_loss: 1315.6874\n",
      "Epoch 399/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6994 - val_loss: 1315.6864\n",
      "Epoch 400/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6995 - val_loss: 1315.6859\n",
      "Epoch 401/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 86.6997 - val_loss: 1315.6851\n",
      "Epoch 402/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6998 - val_loss: 1315.6847\n",
      "Epoch 403/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.6999 - val_loss: 1315.6843\n",
      "Epoch 404/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7000 - val_loss: 1315.6840\n",
      "Epoch 405/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7002 - val_loss: 1315.6838\n",
      "Epoch 406/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7002 - val_loss: 1315.6837\n",
      "Epoch 407/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 86.7003 - val_loss: 1315.6832\n",
      "Epoch 408/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 86.7004 - val_loss: 1315.6829\n",
      "Epoch 409/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7005 - val_loss: 1315.6818\n",
      "Epoch 410/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7006 - val_loss: 1315.6814\n",
      "Epoch 411/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7007 - val_loss: 1315.6803\n",
      "Epoch 412/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7009 - val_loss: 1315.6802\n",
      "Epoch 413/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7010 - val_loss: 1315.6798\n",
      "Epoch 414/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7010 - val_loss: 1315.6793\n",
      "Epoch 415/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7012 - val_loss: 1315.6792\n",
      "Epoch 416/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7012 - val_loss: 1315.6783\n",
      "Epoch 417/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7013 - val_loss: 1315.6775\n",
      "Epoch 418/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7014 - val_loss: 1315.6770\n",
      "Epoch 419/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7015 - val_loss: 1315.6766\n",
      "Epoch 420/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 86.7016 - val_loss: 1315.6763\n",
      "Epoch 421/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7017 - val_loss: 1315.6760\n",
      "Epoch 422/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 86.7018 - val_loss: 1315.6760\n",
      "Epoch 423/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 86.7018 - val_loss: 1315.6755\n",
      "Epoch 424/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7019 - val_loss: 1315.6750\n",
      "Epoch 425/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7020 - val_loss: 1315.6744\n",
      "Epoch 426/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7021 - val_loss: 1315.6742\n",
      "Epoch 427/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7021 - val_loss: 1315.6733\n",
      "Epoch 428/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 86.7022 - val_loss: 1315.6737\n",
      "Epoch 429/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7022 - val_loss: 1315.6731\n",
      "Epoch 430/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7023 - val_loss: 1315.6729\n",
      "Epoch 431/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.7024 - val_loss: 1315.6726\n",
      "Epoch 432/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7025 - val_loss: 1315.6720\n",
      "Epoch 433/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7025 - val_loss: 1315.6707\n",
      "Epoch 434/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7026 - val_loss: 1315.6703\n",
      "Epoch 435/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7027 - val_loss: 1315.6700\n",
      "Epoch 436/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.7027 - val_loss: 1315.6694\n",
      "Epoch 437/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.7028 - val_loss: 1315.6693\n",
      "Epoch 438/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7029 - val_loss: 1315.6689\n",
      "Epoch 439/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7029 - val_loss: 1315.6687\n",
      "Epoch 440/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7030 - val_loss: 1315.6686\n",
      "Epoch 441/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7030 - val_loss: 1315.6683\n",
      "Epoch 442/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7031 - val_loss: 1315.6682\n",
      "Epoch 443/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 86.7031 - val_loss: 1315.6681\n",
      "Epoch 444/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.7032 - val_loss: 1315.6678\n",
      "Epoch 445/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.7032 - val_loss: 1315.6677\n",
      "Epoch 446/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 86.7032 - val_loss: 1315.6676\n",
      "Epoch 447/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7033 - val_loss: 1315.6672\n",
      "Epoch 448/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.7033 - val_loss: 1315.6670\n",
      "Epoch 449/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.7034 - val_loss: 1315.6660\n",
      "Epoch 450/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7035 - val_loss: 1315.6653\n",
      "Epoch 451/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7035 - val_loss: 1315.6649\n",
      "Epoch 452/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7036 - val_loss: 1315.6648\n",
      "Epoch 453/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7036 - val_loss: 1315.6647\n",
      "Epoch 454/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7036 - val_loss: 1315.6639\n",
      "Epoch 455/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7037 - val_loss: 1315.6641\n",
      "Epoch 456/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7037 - val_loss: 1315.6637\n",
      "Epoch 457/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 86.7037 - val_loss: 1315.6635\n",
      "Epoch 458/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.7038 - val_loss: 1315.6631\n",
      "Epoch 459/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 86.7038 - val_loss: 1315.6626\n",
      "Epoch 460/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7038 - val_loss: 1315.6621\n",
      "Epoch 461/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7039 - val_loss: 1315.6614\n",
      "Epoch 462/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7039 - val_loss: 1315.6610\n",
      "Epoch 463/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.7040 - val_loss: 1315.6611\n",
      "Epoch 464/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7040 - val_loss: 1315.6610\n",
      "Epoch 465/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7040 - val_loss: 1315.6605\n",
      "Epoch 466/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7040 - val_loss: 1315.6603\n",
      "Epoch 467/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 86.7041 - val_loss: 1315.6599\n",
      "Epoch 468/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.7041 - val_loss: 1315.6598\n",
      "Epoch 469/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.7042 - val_loss: 1315.6597\n",
      "Epoch 470/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 86.7042 - val_loss: 1315.6594\n",
      "Epoch 471/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.7042 - val_loss: 1315.6594\n",
      "Epoch 472/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.7042 - val_loss: 1315.6593\n",
      "Epoch 473/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7043 - val_loss: 1315.6593\n",
      "Epoch 474/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7044 - val_loss: 1315.6598\n",
      "Epoch 475/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7044 - val_loss: 1315.6604\n",
      "Epoch 476/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7043 - val_loss: 1315.6593\n",
      "Epoch 477/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7044 - val_loss: 1315.6584\n",
      "Epoch 478/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7043 - val_loss: 1315.6581\n",
      "Epoch 479/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7043 - val_loss: 1315.6578\n",
      "Epoch 480/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7044 - val_loss: 1315.6570\n",
      "Epoch 481/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 86.7044 - val_loss: 1315.6565\n",
      "Epoch 482/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7044 - val_loss: 1315.6566\n",
      "Epoch 483/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.7045 - val_loss: 1315.6567\n",
      "Epoch 484/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7045 - val_loss: 1315.6569\n",
      "Epoch 485/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7046 - val_loss: 1315.6571\n",
      "Epoch 486/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7046 - val_loss: 1315.6572\n",
      "Epoch 487/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 86.7046 - val_loss: 1315.6570\n",
      "Epoch 488/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.7046 - val_loss: 1315.6567\n",
      "Epoch 489/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.7046 - val_loss: 1315.6564\n",
      "Epoch 490/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7046 - val_loss: 1315.6550\n",
      "Epoch 491/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7046 - val_loss: 1315.6550\n",
      "Epoch 492/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7047 - val_loss: 1315.6544\n",
      "Epoch 493/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7047 - val_loss: 1315.6547\n",
      "Epoch 494/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7047 - val_loss: 1315.6544\n",
      "Epoch 495/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7048 - val_loss: 1315.6543\n",
      "Epoch 496/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7048 - val_loss: 1315.6538\n",
      "Epoch 497/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7048 - val_loss: 1315.6532\n",
      "Epoch 498/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7048 - val_loss: 1315.6531\n",
      "Epoch 499/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7048 - val_loss: 1315.6523\n",
      "Epoch 500/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 86.7048 - val_loss: 1315.6523\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6856, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0008, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0128, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(-2.3003, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 327ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.86678805e+01, 6.84998133e+01, 6.83317460e+01, 6.81636788e+01,\n",
       "        6.79956116e+01, 6.78275443e+01, 6.77248203e+01, 0.00000000e+00,\n",
       "        2.26760343e-01, 1.96406811e-01, 0.00000000e+00, 3.19723994e-01,\n",
       "        0.00000000e+00, 6.98626751e+01, 6.98374650e+01, 6.98122549e+01,\n",
       "        6.97136321e+01, 6.95455649e+01, 6.93774977e+01, 6.92094304e+01,\n",
       "        6.90413632e+01, 6.88732960e+01, 6.87052288e+01, 6.85371615e+01,\n",
       "        6.83690943e+01, 6.82010271e+01, 6.80329598e+01, 6.78648926e+01,\n",
       "        6.77448016e+01, 7.06609944e+01, 7.05853642e+01, 7.05097339e+01,\n",
       "        7.04341036e+01, 7.03584734e+01, 7.02828431e+01, 7.02072129e+01,\n",
       "        7.01315826e+01, 7.00559524e+01, 5.23406863e-01, 7.59918988e-01,\n",
       "        4.44159660e-02, 7.15077221e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.82383753e+01,\n",
       "        6.80703081e+01, 6.79022409e+01, 6.77647829e+01, 7.06778011e+01,\n",
       "        7.06031709e+01, 7.05256541e+01, 7.04509104e+01, 7.03752801e+01,\n",
       "        7.02996499e+01, 7.02240196e+01, 7.01483894e+01, 7.00727591e+01,\n",
       "        2.70426870e-01, 3.46078570e-01, 7.04060924e+01, 7.03304622e+01,\n",
       "        7.02548319e+01, 7.01792017e+01, 7.01035714e+01, 7.00279412e+01,\n",
       "        6.99523109e+01, 6.98766807e+01, 6.98010504e+01, 1.14993480e-01,\n",
       "        0.00000000e+00, 5.52103882e+01, 4.52467410e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.31345890e-01,\n",
       "        7.04412918e+01, 4.59578156e-01, 0.00000000e+00, 4.21105236e-01,\n",
       "        0.00000000e+00, 1.35093439e+00, 0.00000000e+00, 3.38270009e-01,\n",
       "        4.05110896e-01, 5.16006649e-01, 5.79337291e-02, 1.03447664e+00,\n",
       "        0.00000000e+00, 2.72663683e-01, 4.82198000e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.79676020e-01, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58.67698299, 58.66468539, 58.65238778, 58.64009018, 58.62779258,\n",
       "       58.61549498, 58.60319738, 58.59089977, 58.57860217, 58.56630457,\n",
       "       58.55400697, 58.54170937, 58.52941176, 58.51711416, 58.50481656,\n",
       "       58.49251896, 58.48022136, 58.46792375, 58.45562615, 58.44332855,\n",
       "       58.43103095, 58.41873335, 58.40643575, 58.39413814, 58.38184054,\n",
       "       58.36954294, 58.35724534, 58.34494774, 58.33265013, 58.32035253,\n",
       "       58.30805493, 58.29575733, 58.28345973, 58.27116212, 58.25886452,\n",
       "       58.24656692, 58.23426932, 58.22197172, 58.20967411, 58.19737651,\n",
       "       58.18507891, 58.17278131, 58.16048371, 58.1481861 , 58.1358885 ,\n",
       "       58.1235909 , 58.1112933 , 58.0989957 , 58.08669809, 58.07440049,\n",
       "       58.06210289, 58.04980529, 58.03750769, 58.02521008, 58.01291248,\n",
       "       58.00061488, 57.98831728, 57.97601968, 57.96372207, 57.95142447,\n",
       "       57.93912687, 57.92682927, 57.91453167, 57.90223406, 57.88993646,\n",
       "       57.87763886, 57.86534126, 57.85304366, 57.84074605, 57.82844845,\n",
       "       57.81615085, 57.80385325, 57.79155565, 57.77925804, 57.76696044,\n",
       "       57.75466284, 57.74236524, 57.73006764, 57.71777003, 57.70547243,\n",
       "       57.69317483, 57.68087723, 57.66857963, 57.65628203, 57.64398442,\n",
       "       57.63168682, 57.61938922, 57.60709162, 57.59479402, 57.58249641,\n",
       "       57.57019881, 57.55790121, 57.54560361, 57.53330601, 57.5210084 ,\n",
       "       57.5087108 , 57.4964132 , 57.4841156 , 57.471818  , 57.45952039])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.14936920051042\n",
      "31.56631917882023\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
