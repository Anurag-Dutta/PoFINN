{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "1595    66.755726\n",
       "1596    66.745735\n",
       "1597    66.735745\n",
       "1598    66.725516\n",
       "1599    66.714965\n",
       "Name: C4, Length: 1600, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c4_interpolated_1500_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "1495    67.754792\n",
       "1496    67.744802\n",
       "1497    67.734811\n",
       "1498    67.724820\n",
       "1499    67.714830\n",
       "Name: C4, Length: 1500, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1500)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.409524</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.219048</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.028571</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.838095</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.600000  0.000298\n",
       "1     88.409524  0.000298\n",
       "2     88.219048  0.000297\n",
       "3     88.028571  0.000297\n",
       "4     87.838095  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6856  # Value for C0\n",
    "K0 = -0.0008  # Value for K0\n",
    "K1 = -0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0128    # Value for b\n",
    "c = -2.3003    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.600000    88.409524    88.219048    88.028571    87.838095   \n",
      "351    88.409524    88.219048    88.028571    87.838095    87.647619   \n",
      "352    88.219048    88.028571    87.838095    87.647619    87.457143   \n",
      "353    88.028571    87.838095    87.647619    87.457143    87.266667   \n",
      "354    87.838095    87.647619    87.457143    87.266667    87.076190   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.647619    87.457143    87.266667    87.076190    86.896218  ...   \n",
      "351    87.457143    87.266667    87.076190    86.896218    86.845798  ...   \n",
      "352    87.266667    87.076190    86.896218    86.845798    86.795378  ...   \n",
      "353    87.076190    86.896218    86.845798    86.795378    86.744958  ...   \n",
      "354    86.896218    86.845798    86.795378    86.744958    86.694538  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   78.880345    0.000263   78.856069    0.000263   78.831793    0.000263   \n",
      "351   78.856069    0.000263   78.831793    0.000263   78.807516    0.000262   \n",
      "352   78.831793    0.000263   78.807516    0.000262   78.783240    0.000262   \n",
      "353   78.807516    0.000262   78.783240    0.000262   78.758964    0.000262   \n",
      "354   78.783240    0.000262   78.758964    0.000262   78.734687    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   78.807516    0.000262   78.783240    0.000262  \n",
      "351   78.783240    0.000262   78.758964    0.000262  \n",
      "352   78.758964    0.000262   78.734687    0.000262  \n",
      "353   78.734687    0.000262   78.710411    0.000262  \n",
      "354   78.710411    0.000262   78.686134    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050, 1, 251) (1050, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "14/14 [==============================] - 2s 37ms/step - loss: 5583.8511 - val_loss: 4715.6646\n",
      "Epoch 2/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5502.4097 - val_loss: 4658.0723\n",
      "Epoch 3/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5445.2095 - val_loss: 4608.0996\n",
      "Epoch 4/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5391.2534 - val_loss: 4558.5117\n",
      "Epoch 5/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5337.7402 - val_loss: 4509.3794\n",
      "Epoch 6/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5284.6958 - val_loss: 4460.6899\n",
      "Epoch 7/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5232.1025 - val_loss: 4412.4248\n",
      "Epoch 8/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5179.9375 - val_loss: 4364.5703\n",
      "Epoch 9/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5128.1929 - val_loss: 4317.1162\n",
      "Epoch 10/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5076.8555 - val_loss: 4270.0552\n",
      "Epoch 11/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5025.9204 - val_loss: 4223.3804\n",
      "Epoch 12/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4975.3799 - val_loss: 4177.0864\n",
      "Epoch 13/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4925.2300 - val_loss: 4131.1704\n",
      "Epoch 14/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4875.4658 - val_loss: 4085.6277\n",
      "Epoch 15/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4826.0859 - val_loss: 4040.4553\n",
      "Epoch 16/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4777.0850 - val_loss: 3995.6506\n",
      "Epoch 17/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4728.4600 - val_loss: 3951.2102\n",
      "Epoch 18/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4680.2090 - val_loss: 3907.1321\n",
      "Epoch 19/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4632.3291 - val_loss: 3863.4128\n",
      "Epoch 20/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4584.8184 - val_loss: 3820.0503\n",
      "Epoch 21/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4537.6709 - val_loss: 3777.0422\n",
      "Epoch 22/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4490.8882 - val_loss: 3734.3862\n",
      "Epoch 23/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4444.4668 - val_loss: 3692.0798\n",
      "Epoch 24/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4398.4043 - val_loss: 3650.1211\n",
      "Epoch 25/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4352.6982 - val_loss: 3608.5081\n",
      "Epoch 26/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4307.3462 - val_loss: 3567.2378\n",
      "Epoch 27/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4262.3462 - val_loss: 3526.3088\n",
      "Epoch 28/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4217.6963 - val_loss: 3485.7188\n",
      "Epoch 29/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4173.3945 - val_loss: 3445.4656\n",
      "Epoch 30/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 4129.4385 - val_loss: 3405.5476\n",
      "Epoch 31/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4085.8262 - val_loss: 3365.9629\n",
      "Epoch 32/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4042.5569 - val_loss: 3326.7087\n",
      "Epoch 33/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3999.6262 - val_loss: 3287.7837\n",
      "Epoch 34/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3957.0334 - val_loss: 3249.1851\n",
      "Epoch 35/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3914.7769 - val_loss: 3210.9126\n",
      "Epoch 36/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3872.8538 - val_loss: 3172.9629\n",
      "Epoch 37/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3831.2634 - val_loss: 3135.3342\n",
      "Epoch 38/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3790.0029 - val_loss: 3098.0256\n",
      "Epoch 39/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3749.0706 - val_loss: 3061.0339\n",
      "Epoch 40/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3708.4648 - val_loss: 3024.3584\n",
      "Epoch 41/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3668.1833 - val_loss: 2987.9961\n",
      "Epoch 42/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3628.2244 - val_loss: 2951.9458\n",
      "Epoch 43/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3588.5864 - val_loss: 2916.2065\n",
      "Epoch 44/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3549.2668 - val_loss: 2880.7749\n",
      "Epoch 45/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3510.2649 - val_loss: 2845.6504\n",
      "Epoch 46/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3471.5784 - val_loss: 2810.8306\n",
      "Epoch 47/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3433.2051 - val_loss: 2776.3137\n",
      "Epoch 48/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3395.1438 - val_loss: 2742.0989\n",
      "Epoch 49/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3357.3926 - val_loss: 2708.1826\n",
      "Epoch 50/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3319.9492 - val_loss: 2674.5647\n",
      "Epoch 51/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3282.8127 - val_loss: 2641.2429\n",
      "Epoch 52/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3245.9810 - val_loss: 2608.2156\n",
      "Epoch 53/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3209.4521 - val_loss: 2575.4805\n",
      "Epoch 54/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3173.2251 - val_loss: 2543.0364\n",
      "Epoch 55/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3137.2976 - val_loss: 2510.8816\n",
      "Epoch 56/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3101.6680 - val_loss: 2479.0151\n",
      "Epoch 57/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3066.3345 - val_loss: 2447.4341\n",
      "Epoch 58/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3031.2957 - val_loss: 2416.1370\n",
      "Epoch 59/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2996.5500 - val_loss: 2385.1233\n",
      "Epoch 60/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2962.0952 - val_loss: 2354.3904\n",
      "Epoch 61/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2927.9304 - val_loss: 2323.9363\n",
      "Epoch 62/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2894.0535 - val_loss: 2293.7605\n",
      "Epoch 63/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2860.4629 - val_loss: 2263.8604\n",
      "Epoch 64/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2827.1575 - val_loss: 2234.2346\n",
      "Epoch 65/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 2794.1348 - val_loss: 2204.8823\n",
      "Epoch 66/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2761.3940 - val_loss: 2175.8013\n",
      "Epoch 67/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2728.9321 - val_loss: 2146.9895\n",
      "Epoch 68/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2696.7495 - val_loss: 2118.4463\n",
      "Epoch 69/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2664.8438 - val_loss: 2090.1694\n",
      "Epoch 70/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2633.2134 - val_loss: 2062.1580\n",
      "Epoch 71/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2601.8560 - val_loss: 2034.4099\n",
      "Epoch 72/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2570.7715 - val_loss: 2006.9240\n",
      "Epoch 73/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2539.9570 - val_loss: 1979.6979\n",
      "Epoch 74/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2509.4116 - val_loss: 1952.7311\n",
      "Epoch 75/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2479.1340 - val_loss: 1926.0219\n",
      "Epoch 76/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2449.1221 - val_loss: 1899.5684\n",
      "Epoch 77/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2419.3752 - val_loss: 1873.3684\n",
      "Epoch 78/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2389.8911 - val_loss: 1847.4220\n",
      "Epoch 79/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2360.6680 - val_loss: 1821.7271\n",
      "Epoch 80/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2331.7053 - val_loss: 1796.2819\n",
      "Epoch 81/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2303.0010 - val_loss: 1771.0851\n",
      "Epoch 82/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2274.5535 - val_loss: 1746.1346\n",
      "Epoch 83/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2246.3616 - val_loss: 1721.4301\n",
      "Epoch 84/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2218.4241 - val_loss: 1696.9696\n",
      "Epoch 85/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2190.7393 - val_loss: 1672.7513\n",
      "Epoch 86/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2163.3052 - val_loss: 1648.7740\n",
      "Epoch 87/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2136.1213 - val_loss: 1625.0363\n",
      "Epoch 88/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2109.1851 - val_loss: 1601.5375\n",
      "Epoch 89/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2082.4961 - val_loss: 1578.2745\n",
      "Epoch 90/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2056.0525 - val_loss: 1555.2468\n",
      "Epoch 91/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2029.8533 - val_loss: 1532.4540\n",
      "Epoch 92/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2003.8959 - val_loss: 1509.8928\n",
      "Epoch 93/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1978.1796 - val_loss: 1487.5627\n",
      "Epoch 94/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1952.7036 - val_loss: 1465.4628\n",
      "Epoch 95/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1927.4658 - val_loss: 1443.5909\n",
      "Epoch 96/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1902.4652 - val_loss: 1421.9462\n",
      "Epoch 97/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1877.6998 - val_loss: 1400.5264\n",
      "Epoch 98/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1853.1687 - val_loss: 1379.3306\n",
      "Epoch 99/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 1828.8707 - val_loss: 1358.3575\n",
      "Epoch 100/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1804.8037 - val_loss: 1337.6061\n",
      "Epoch 101/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1780.9674 - val_loss: 1317.0747\n",
      "Epoch 102/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1757.3593 - val_loss: 1296.7616\n",
      "Epoch 103/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1733.9789 - val_loss: 1276.6663\n",
      "Epoch 104/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1710.8245 - val_loss: 1256.7866\n",
      "Epoch 105/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1687.8947 - val_loss: 1237.1211\n",
      "Epoch 106/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1665.1881 - val_loss: 1217.6694\n",
      "Epoch 107/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1642.7037 - val_loss: 1198.4292\n",
      "Epoch 108/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1620.4404 - val_loss: 1179.4003\n",
      "Epoch 109/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1598.3960 - val_loss: 1160.5793\n",
      "Epoch 110/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1576.5698 - val_loss: 1141.9674\n",
      "Epoch 111/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1554.9607 - val_loss: 1123.5616\n",
      "Epoch 112/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1533.5665 - val_loss: 1105.3608\n",
      "Epoch 113/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1512.3866 - val_loss: 1087.3649\n",
      "Epoch 114/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1491.4197 - val_loss: 1069.5701\n",
      "Epoch 115/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1470.6637 - val_loss: 1051.9783\n",
      "Epoch 116/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1450.1187 - val_loss: 1034.5854\n",
      "Epoch 117/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1429.7825 - val_loss: 1017.3912\n",
      "Epoch 118/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1409.6537 - val_loss: 1000.3948\n",
      "Epoch 119/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1389.7310 - val_loss: 983.5945\n",
      "Epoch 120/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1370.0134 - val_loss: 966.9885\n",
      "Epoch 121/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1350.5001 - val_loss: 950.5765\n",
      "Epoch 122/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1331.1888 - val_loss: 934.3567\n",
      "Epoch 123/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1312.0790 - val_loss: 918.3284\n",
      "Epoch 124/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1293.1698 - val_loss: 902.4893\n",
      "Epoch 125/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1274.4587 - val_loss: 886.8386\n",
      "Epoch 126/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1255.9452 - val_loss: 871.3758\n",
      "Epoch 127/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1237.6283 - val_loss: 856.0982\n",
      "Epoch 128/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1219.5065 - val_loss: 841.0061\n",
      "Epoch 129/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1201.5786 - val_loss: 826.0971\n",
      "Epoch 130/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 1183.8428 - val_loss: 811.3702\n",
      "Epoch 131/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1166.2983 - val_loss: 796.8245\n",
      "Epoch 132/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1148.9446 - val_loss: 782.4580\n",
      "Epoch 133/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1131.7793 - val_loss: 768.2698\n",
      "Epoch 134/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1114.8011 - val_loss: 754.2595\n",
      "Epoch 135/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1098.0098 - val_loss: 740.4249\n",
      "Epoch 136/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1081.4037 - val_loss: 726.7653\n",
      "Epoch 137/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1064.9817 - val_loss: 713.2798\n",
      "Epoch 138/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1048.7427 - val_loss: 699.9654\n",
      "Epoch 139/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1032.6853 - val_loss: 686.8227\n",
      "Epoch 140/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1016.8076 - val_loss: 673.8494\n",
      "Epoch 141/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1001.1095 - val_loss: 661.0451\n",
      "Epoch 142/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 985.5894 - val_loss: 648.4084\n",
      "Epoch 143/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 970.2460 - val_loss: 635.9379\n",
      "Epoch 144/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 955.0781 - val_loss: 623.6324\n",
      "Epoch 145/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 940.0847 - val_loss: 611.4902\n",
      "Epoch 146/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 925.2645 - val_loss: 599.5115\n",
      "Epoch 147/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 910.6163 - val_loss: 587.6929\n",
      "Epoch 148/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 896.1388 - val_loss: 576.0353\n",
      "Epoch 149/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 881.8312 - val_loss: 564.5365\n",
      "Epoch 150/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 867.6917 - val_loss: 553.1957\n",
      "Epoch 151/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 853.7201 - val_loss: 542.0115\n",
      "Epoch 152/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 839.9144 - val_loss: 530.9825\n",
      "Epoch 153/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 826.2736 - val_loss: 520.1083\n",
      "Epoch 154/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 812.7968 - val_loss: 509.3867\n",
      "Epoch 155/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 799.4824 - val_loss: 498.8170\n",
      "Epoch 156/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 786.3297 - val_loss: 488.3983\n",
      "Epoch 157/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 773.3373 - val_loss: 478.1281\n",
      "Epoch 158/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 760.5038 - val_loss: 468.0070\n",
      "Epoch 159/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 747.8287 - val_loss: 458.0330\n",
      "Epoch 160/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 735.3102 - val_loss: 448.2050\n",
      "Epoch 161/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 722.9476 - val_loss: 438.5212\n",
      "Epoch 162/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 710.7394 - val_loss: 428.9821\n",
      "Epoch 163/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 698.6848 - val_loss: 419.5845\n",
      "Epoch 164/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 686.7822 - val_loss: 410.3289\n",
      "Epoch 165/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 675.0311 - val_loss: 401.2128\n",
      "Epoch 166/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 663.4298 - val_loss: 392.2359\n",
      "Epoch 167/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 651.9775 - val_loss: 383.3972\n",
      "Epoch 168/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 640.6729 - val_loss: 374.6948\n",
      "Epoch 169/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 629.5149 - val_loss: 366.1277\n",
      "Epoch 170/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 618.5023 - val_loss: 357.6950\n",
      "Epoch 171/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 607.6340 - val_loss: 349.3957\n",
      "Epoch 172/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 596.8710 - val_loss: 337.6689\n",
      "Epoch 173/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 580.6249 - val_loss: 327.5019\n",
      "Epoch 174/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 567.3068 - val_loss: 317.5559\n",
      "Epoch 175/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 554.5073 - val_loss: 308.1297\n",
      "Epoch 176/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 542.3043 - val_loss: 299.1350\n",
      "Epoch 177/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 530.5822 - val_loss: 290.4903\n",
      "Epoch 178/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 519.2505 - val_loss: 282.1402\n",
      "Epoch 179/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 508.2496 - val_loss: 274.0479\n",
      "Epoch 180/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 497.5391 - val_loss: 266.1885\n",
      "Epoch 181/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 487.0907 - val_loss: 258.5427\n",
      "Epoch 182/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 476.8830 - val_loss: 251.0962\n",
      "Epoch 183/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 466.9001 - val_loss: 243.8374\n",
      "Epoch 184/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 457.1290 - val_loss: 236.7583\n",
      "Epoch 185/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 447.5595 - val_loss: 229.8496\n",
      "Epoch 186/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 438.1822 - val_loss: 223.1056\n",
      "Epoch 187/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 428.9897 - val_loss: 216.5202\n",
      "Epoch 188/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 419.9761 - val_loss: 210.0885\n",
      "Epoch 189/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 411.1349 - val_loss: 203.8056\n",
      "Epoch 190/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 402.4612 - val_loss: 197.6678\n",
      "Epoch 191/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 393.9506 - val_loss: 191.6712\n",
      "Epoch 192/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 385.5984 - val_loss: 185.8121\n",
      "Epoch 193/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 377.4013 - val_loss: 180.0877\n",
      "Epoch 194/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 369.3556 - val_loss: 174.4947\n",
      "Epoch 195/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 361.4580 - val_loss: 169.0305\n",
      "Epoch 196/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 353.7053 - val_loss: 163.6923\n",
      "Epoch 197/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 346.0946 - val_loss: 158.4777\n",
      "Epoch 198/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 338.6233 - val_loss: 153.3840\n",
      "Epoch 199/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 331.2888 - val_loss: 148.4090\n",
      "Epoch 200/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 324.0883 - val_loss: 143.5510\n",
      "Epoch 201/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 317.0199 - val_loss: 138.8072\n",
      "Epoch 202/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 310.0809 - val_loss: 134.1762\n",
      "Epoch 203/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 303.2696 - val_loss: 129.6554\n",
      "Epoch 204/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 296.5833 - val_loss: 125.2429\n",
      "Epoch 205/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 290.0204 - val_loss: 120.9371\n",
      "Epoch 206/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 283.5786 - val_loss: 116.7362\n",
      "Epoch 207/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 277.2564 - val_loss: 112.6383\n",
      "Epoch 208/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 271.0517 - val_loss: 108.6417\n",
      "Epoch 209/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 264.9626 - val_loss: 104.7449\n",
      "Epoch 210/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 258.9875 - val_loss: 100.9457\n",
      "Epoch 211/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 253.1248 - val_loss: 97.2430\n",
      "Epoch 212/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 247.3723 - val_loss: 93.6350\n",
      "Epoch 213/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 241.7287 - val_loss: 90.1203\n",
      "Epoch 214/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 236.1927 - val_loss: 86.6970\n",
      "Epoch 215/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 230.7620 - val_loss: 83.3638\n",
      "Epoch 216/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 225.4355 - val_loss: 80.1194\n",
      "Epoch 217/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 220.2116 - val_loss: 76.9620\n",
      "Epoch 218/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 215.0887 - val_loss: 73.8902\n",
      "Epoch 219/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 210.0654 - val_loss: 70.9027\n",
      "Epoch 220/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 205.1405 - val_loss: 67.9981\n",
      "Epoch 221/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 200.3119 - val_loss: 65.1749\n",
      "Epoch 222/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 195.5789 - val_loss: 62.4321\n",
      "Epoch 223/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 190.9400 - val_loss: 59.7677\n",
      "Epoch 224/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 186.3935 - val_loss: 57.1809\n",
      "Epoch 225/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 181.9382 - val_loss: 54.6702\n",
      "Epoch 226/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 177.5727 - val_loss: 52.2339\n",
      "Epoch 227/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 173.2954 - val_loss: 49.8714\n",
      "Epoch 228/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 169.1057 - val_loss: 47.5810\n",
      "Epoch 229/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 165.0016 - val_loss: 45.3614\n",
      "Epoch 230/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 160.9822 - val_loss: 43.2114\n",
      "Epoch 231/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 157.0460 - val_loss: 41.1299\n",
      "Epoch 232/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 153.1920 - val_loss: 39.1157\n",
      "Epoch 233/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 149.4191 - val_loss: 37.1674\n",
      "Epoch 234/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 145.7256 - val_loss: 35.2838\n",
      "Epoch 235/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 142.1105 - val_loss: 33.4639\n",
      "Epoch 236/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 138.5727 - val_loss: 31.7062\n",
      "Epoch 237/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 135.1109 - val_loss: 30.0098\n",
      "Epoch 238/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 131.7239 - val_loss: 28.3734\n",
      "Epoch 239/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 128.4106 - val_loss: 26.7960\n",
      "Epoch 240/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 125.1698 - val_loss: 25.2762\n",
      "Epoch 241/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 122.0002 - val_loss: 23.8130\n",
      "Epoch 242/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 118.9011 - val_loss: 22.4055\n",
      "Epoch 243/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 115.8710 - val_loss: 21.0523\n",
      "Epoch 244/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 112.9090 - val_loss: 19.7524\n",
      "Epoch 245/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 110.0138 - val_loss: 18.5045\n",
      "Epoch 246/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 107.1844 - val_loss: 17.3080\n",
      "Epoch 247/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 104.4197 - val_loss: 16.1615\n",
      "Epoch 248/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 101.7188 - val_loss: 15.0640\n",
      "Epoch 249/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 99.0803 - val_loss: 14.0146\n",
      "Epoch 250/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 96.5036 - val_loss: 13.0120\n",
      "Epoch 251/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 93.9871 - val_loss: 12.0552\n",
      "Epoch 252/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 91.5301 - val_loss: 11.1433\n",
      "Epoch 253/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 89.1316 - val_loss: 10.2755\n",
      "Epoch 254/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 86.7906 - val_loss: 9.4503\n",
      "Epoch 255/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 84.5060 - val_loss: 8.6673\n",
      "Epoch 256/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 82.2769 - val_loss: 7.9252\n",
      "Epoch 257/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 80.1022 - val_loss: 7.2229\n",
      "Epoch 258/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 77.9809 - val_loss: 6.5598\n",
      "Epoch 259/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 75.9124 - val_loss: 5.9347\n",
      "Epoch 260/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 73.8952 - val_loss: 5.3466\n",
      "Epoch 261/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 71.9286 - val_loss: 4.7948\n",
      "Epoch 262/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 70.0117 - val_loss: 4.2783\n",
      "Epoch 263/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 68.1437 - val_loss: 3.7961\n",
      "Epoch 264/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 66.3234 - val_loss: 3.3476\n",
      "Epoch 265/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 64.5502 - val_loss: 2.9316\n",
      "Epoch 266/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 62.8231 - val_loss: 2.5472\n",
      "Epoch 267/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 61.1409 - val_loss: 2.1938\n",
      "Epoch 268/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 59.5030 - val_loss: 1.8704\n",
      "Epoch 269/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 57.9084 - val_loss: 1.5762\n",
      "Epoch 270/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 56.3564 - val_loss: 1.3103\n",
      "Epoch 271/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 54.8463 - val_loss: 1.0719\n",
      "Epoch 272/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 53.3767 - val_loss: 0.8603\n",
      "Epoch 273/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 51.9473 - val_loss: 0.6746\n",
      "Epoch 274/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 50.5570 - val_loss: 0.5140\n",
      "Epoch 275/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 49.2053 - val_loss: 0.3777\n",
      "Epoch 276/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 47.8909 - val_loss: 0.2650\n",
      "Epoch 277/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 46.6133 - val_loss: 0.1752\n",
      "Epoch 278/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 45.3719 - val_loss: 0.1074\n",
      "Epoch 279/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 44.1656 - val_loss: 0.0609\n",
      "Epoch 280/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 42.9936 - val_loss: 0.0351\n",
      "Epoch 281/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 41.8555 - val_loss: 0.0292\n",
      "Epoch 282/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 40.7503 - val_loss: 0.0425\n",
      "Epoch 283/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 39.6773 - val_loss: 0.0743\n",
      "Epoch 284/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 38.6359 - val_loss: 0.1239\n",
      "Epoch 285/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 37.6253 - val_loss: 0.1907\n",
      "Epoch 286/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 36.6447 - val_loss: 0.2740\n",
      "Epoch 287/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 35.6936 - val_loss: 0.3732\n",
      "Epoch 288/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 34.7710 - val_loss: 0.4875\n",
      "Epoch 289/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 33.8766 - val_loss: 0.6165\n",
      "Epoch 290/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 33.0095 - val_loss: 0.7595\n",
      "Epoch 291/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 32.1691 - val_loss: 0.9158\n",
      "Epoch 292/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 31.3547 - val_loss: 1.0849\n",
      "Epoch 293/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 30.5659 - val_loss: 1.2662\n",
      "Epoch 294/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 29.8018 - val_loss: 1.4592\n",
      "Epoch 295/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 29.0619 - val_loss: 1.6632\n",
      "Epoch 296/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 28.3456 - val_loss: 1.8778\n",
      "Epoch 297/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 27.6522 - val_loss: 2.1023\n",
      "Epoch 298/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 26.9812 - val_loss: 2.3365\n",
      "Epoch 299/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 26.3320 - val_loss: 2.5794\n",
      "Epoch 300/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 25.7041 - val_loss: 2.8310\n",
      "Epoch 301/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 25.0968 - val_loss: 3.0904\n",
      "Epoch 302/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 24.5096 - val_loss: 3.3573\n",
      "Epoch 303/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 23.9423 - val_loss: 3.6312\n",
      "Epoch 304/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 23.3938 - val_loss: 3.9117\n",
      "Epoch 305/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 22.8640 - val_loss: 4.1984\n",
      "Epoch 306/500\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 22.3521 - val_loss: 4.4907\n",
      "Epoch 307/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 21.8578 - val_loss: 4.7882\n",
      "Epoch 308/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 21.3805 - val_loss: 5.0906\n",
      "Epoch 309/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 20.9199 - val_loss: 5.3976\n",
      "Epoch 310/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 20.4751 - val_loss: 5.7085\n",
      "Epoch 311/500\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 20.0461 - val_loss: 6.0231\n",
      "Epoch 312/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 19.6323 - val_loss: 6.3410\n",
      "Epoch 313/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 19.2332 - val_loss: 6.6617\n",
      "Epoch 314/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 18.8485 - val_loss: 6.9851\n",
      "Epoch 315/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 18.4776 - val_loss: 7.3107\n",
      "Epoch 316/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 18.1202 - val_loss: 7.6382\n",
      "Epoch 317/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.7759 - val_loss: 7.9673\n",
      "Epoch 318/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 17.4441 - val_loss: 8.2976\n",
      "Epoch 319/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 17.1247 - val_loss: 8.6290\n",
      "Epoch 320/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.8171 - val_loss: 8.9610\n",
      "Epoch 321/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 16.5210 - val_loss: 9.2934\n",
      "Epoch 322/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.2361 - val_loss: 9.6260\n",
      "Epoch 323/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 15.9620 - val_loss: 9.9585\n",
      "Epoch 324/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 15.6983 - val_loss: 10.2906\n",
      "Epoch 325/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 15.4446 - val_loss: 10.6222\n",
      "Epoch 326/500\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 15.2008 - val_loss: 10.9528\n",
      "Epoch 327/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.9664 - val_loss: 11.2824\n",
      "Epoch 328/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 14.7412 - val_loss: 11.6106\n",
      "Epoch 329/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.5248 - val_loss: 11.9374\n",
      "Epoch 330/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.3169 - val_loss: 12.2625\n",
      "Epoch 331/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1173 - val_loss: 12.5857\n",
      "Epoch 332/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.9256 - val_loss: 12.9069\n",
      "Epoch 333/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 13.7416 - val_loss: 13.2256\n",
      "Epoch 334/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 13.5650 - val_loss: 13.5425\n",
      "Epoch 335/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 13.3955 - val_loss: 13.8563\n",
      "Epoch 336/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 13.2330 - val_loss: 14.1676\n",
      "Epoch 337/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.0770 - val_loss: 14.4760\n",
      "Epoch 338/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 12.9276 - val_loss: 14.7816\n",
      "Epoch 339/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 12.7843 - val_loss: 15.0838\n",
      "Epoch 340/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 12.6469 - val_loss: 15.3830\n",
      "Epoch 341/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 12.5153 - val_loss: 15.6788\n",
      "Epoch 342/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 12.3893 - val_loss: 15.9711\n",
      "Epoch 343/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 12.2685 - val_loss: 16.2600\n",
      "Epoch 344/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 12.1529 - val_loss: 16.5452\n",
      "Epoch 345/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 12.0421 - val_loss: 16.8268\n",
      "Epoch 346/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 11.9361 - val_loss: 17.1046\n",
      "Epoch 347/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 11.8346 - val_loss: 17.3787\n",
      "Epoch 348/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 11.7375 - val_loss: 17.6487\n",
      "Epoch 349/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 11.6446 - val_loss: 17.9149\n",
      "Epoch 350/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 11.5557 - val_loss: 18.1770\n",
      "Epoch 351/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 11.4708 - val_loss: 18.4351\n",
      "Epoch 352/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 11.3895 - val_loss: 18.6889\n",
      "Epoch 353/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 11.3118 - val_loss: 18.9389\n",
      "Epoch 354/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 11.2376 - val_loss: 19.1845\n",
      "Epoch 355/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 11.1665 - val_loss: 19.4258\n",
      "Epoch 356/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 11.0987 - val_loss: 19.6630\n",
      "Epoch 357/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 11.0339 - val_loss: 19.8962\n",
      "Epoch 358/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 10.9720 - val_loss: 20.1246\n",
      "Epoch 359/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.9129 - val_loss: 20.3490\n",
      "Epoch 360/500\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 10.8565 - val_loss: 20.5692\n",
      "Epoch 361/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.8026 - val_loss: 20.7854\n",
      "Epoch 362/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.7511 - val_loss: 20.9967\n",
      "Epoch 363/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.7020 - val_loss: 21.2040\n",
      "Epoch 364/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.6552 - val_loss: 21.4071\n",
      "Epoch 365/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.6105 - val_loss: 21.6059\n",
      "Epoch 366/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 10.5677 - val_loss: 21.8007\n",
      "Epoch 367/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 10.5271 - val_loss: 21.9912\n",
      "Epoch 368/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.4882 - val_loss: 22.1777\n",
      "Epoch 369/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.4511 - val_loss: 22.3598\n",
      "Epoch 370/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 10.4158 - val_loss: 22.5379\n",
      "Epoch 371/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 10.3821 - val_loss: 22.7120\n",
      "Epoch 372/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 10.3499 - val_loss: 22.8821\n",
      "Epoch 373/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 10.3192 - val_loss: 23.0479\n",
      "Epoch 374/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 10.2900 - val_loss: 23.2100\n",
      "Epoch 375/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 10.2622 - val_loss: 23.3680\n",
      "Epoch 376/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 10.2356 - val_loss: 23.5222\n",
      "Epoch 377/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 10.2103 - val_loss: 23.6725\n",
      "Epoch 378/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 10.1862 - val_loss: 23.8190\n",
      "Epoch 379/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 10.1632 - val_loss: 23.9620\n",
      "Epoch 380/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 10.1412 - val_loss: 24.1011\n",
      "Epoch 381/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 10.1204 - val_loss: 24.2364\n",
      "Epoch 382/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 10.1005 - val_loss: 24.3682\n",
      "Epoch 383/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 10.0815 - val_loss: 24.4964\n",
      "Epoch 384/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 10.0634 - val_loss: 24.6213\n",
      "Epoch 385/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 10.0462 - val_loss: 24.7427\n",
      "Epoch 386/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 10.0298 - val_loss: 24.8606\n",
      "Epoch 387/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 10.0141 - val_loss: 24.9753\n",
      "Epoch 388/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.9992 - val_loss: 25.0867\n",
      "Epoch 389/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.9850 - val_loss: 25.1947\n",
      "Epoch 390/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.9716 - val_loss: 25.2998\n",
      "Epoch 391/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.9587 - val_loss: 25.4017\n",
      "Epoch 392/500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 9.9464 - val_loss: 25.5007\n",
      "Epoch 393/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.9347 - val_loss: 25.5968\n",
      "Epoch 394/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.9235 - val_loss: 25.6898\n",
      "Epoch 395/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.9129 - val_loss: 25.7798\n",
      "Epoch 396/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.9028 - val_loss: 25.8674\n",
      "Epoch 397/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.8932 - val_loss: 25.9519\n",
      "Epoch 398/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 9.8840 - val_loss: 26.0337\n",
      "Epoch 399/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.8753 - val_loss: 26.1132\n",
      "Epoch 400/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.8669 - val_loss: 26.1900\n",
      "Epoch 401/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.8590 - val_loss: 26.2642\n",
      "Epoch 402/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.8514 - val_loss: 26.3360\n",
      "Epoch 403/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.8443 - val_loss: 26.4056\n",
      "Epoch 404/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.8373 - val_loss: 26.4724\n",
      "Epoch 405/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.8309 - val_loss: 26.5371\n",
      "Epoch 406/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.8247 - val_loss: 26.5996\n",
      "Epoch 407/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.8188 - val_loss: 26.6601\n",
      "Epoch 408/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.8131 - val_loss: 26.7182\n",
      "Epoch 409/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.8078 - val_loss: 26.7746\n",
      "Epoch 410/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.8026 - val_loss: 26.8287\n",
      "Epoch 411/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7978 - val_loss: 26.8812\n",
      "Epoch 412/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7931 - val_loss: 26.9317\n",
      "Epoch 413/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7887 - val_loss: 26.9806\n",
      "Epoch 414/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7844 - val_loss: 27.0272\n",
      "Epoch 415/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7805 - val_loss: 27.0722\n",
      "Epoch 416/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7767 - val_loss: 27.1157\n",
      "Epoch 417/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7730 - val_loss: 27.1575\n",
      "Epoch 418/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 9.7696 - val_loss: 27.1976\n",
      "Epoch 419/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7663 - val_loss: 27.2362\n",
      "Epoch 420/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7632 - val_loss: 27.2734\n",
      "Epoch 421/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7602 - val_loss: 27.3091\n",
      "Epoch 422/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7573 - val_loss: 27.3435\n",
      "Epoch 423/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7547 - val_loss: 27.3764\n",
      "Epoch 424/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7521 - val_loss: 27.4079\n",
      "Epoch 425/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7497 - val_loss: 27.4383\n",
      "Epoch 426/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7473 - val_loss: 27.4674\n",
      "Epoch 427/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7452 - val_loss: 27.4954\n",
      "Epoch 428/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7431 - val_loss: 27.5221\n",
      "Epoch 429/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7411 - val_loss: 27.5477\n",
      "Epoch 430/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7392 - val_loss: 27.5723\n",
      "Epoch 431/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7374 - val_loss: 27.5960\n",
      "Epoch 432/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7357 - val_loss: 27.6185\n",
      "Epoch 433/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7341 - val_loss: 27.6401\n",
      "Epoch 434/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7326 - val_loss: 27.6608\n",
      "Epoch 435/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7312 - val_loss: 27.6806\n",
      "Epoch 436/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7298 - val_loss: 27.6995\n",
      "Epoch 437/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7285 - val_loss: 27.7174\n",
      "Epoch 438/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 9.7273 - val_loss: 27.7345\n",
      "Epoch 439/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7261 - val_loss: 27.7509\n",
      "Epoch 440/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7250 - val_loss: 27.7668\n",
      "Epoch 441/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7240 - val_loss: 27.7817\n",
      "Epoch 442/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7230 - val_loss: 27.7962\n",
      "Epoch 443/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7220 - val_loss: 27.8098\n",
      "Epoch 444/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7212 - val_loss: 27.8226\n",
      "Epoch 445/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7204 - val_loss: 27.8352\n",
      "Epoch 446/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7196 - val_loss: 27.8469\n",
      "Epoch 447/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7189 - val_loss: 27.8581\n",
      "Epoch 448/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7182 - val_loss: 27.8684\n",
      "Epoch 449/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7176 - val_loss: 27.8786\n",
      "Epoch 450/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7169 - val_loss: 27.8882\n",
      "Epoch 451/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7164 - val_loss: 27.8974\n",
      "Epoch 452/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7159 - val_loss: 27.9061\n",
      "Epoch 453/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7154 - val_loss: 27.9145\n",
      "Epoch 454/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7149 - val_loss: 27.9223\n",
      "Epoch 455/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7145 - val_loss: 27.9297\n",
      "Epoch 456/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7141 - val_loss: 27.9370\n",
      "Epoch 457/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7137 - val_loss: 27.9436\n",
      "Epoch 458/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 9.7134 - val_loss: 27.9500\n",
      "Epoch 459/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7131 - val_loss: 27.9559\n",
      "Epoch 460/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7128 - val_loss: 27.9617\n",
      "Epoch 461/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7125 - val_loss: 27.9671\n",
      "Epoch 462/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7122 - val_loss: 27.9724\n",
      "Epoch 463/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7120 - val_loss: 27.9772\n",
      "Epoch 464/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7118 - val_loss: 27.9817\n",
      "Epoch 465/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7116 - val_loss: 27.9859\n",
      "Epoch 466/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7115 - val_loss: 27.9900\n",
      "Epoch 467/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7113 - val_loss: 27.9938\n",
      "Epoch 468/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7112 - val_loss: 27.9974\n",
      "Epoch 469/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7111 - val_loss: 28.0010\n",
      "Epoch 470/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7110 - val_loss: 28.0041\n",
      "Epoch 471/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7109 - val_loss: 28.0070\n",
      "Epoch 472/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7108 - val_loss: 28.0099\n",
      "Epoch 473/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7108 - val_loss: 28.0125\n",
      "Epoch 474/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7107 - val_loss: 28.0149\n",
      "Epoch 475/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7107 - val_loss: 28.0173\n",
      "Epoch 476/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7107 - val_loss: 28.0194\n",
      "Epoch 477/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 9.7107 - val_loss: 28.0215\n",
      "Epoch 478/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7107 - val_loss: 28.0236\n",
      "Epoch 479/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7107 - val_loss: 28.0253\n",
      "Epoch 480/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7107 - val_loss: 28.0270\n",
      "Epoch 481/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7107 - val_loss: 28.0287\n",
      "Epoch 482/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7107 - val_loss: 28.0301\n",
      "Epoch 483/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7108 - val_loss: 28.0316\n",
      "Epoch 484/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7108 - val_loss: 28.0327\n",
      "Epoch 485/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7109 - val_loss: 28.0337\n",
      "Epoch 486/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7109 - val_loss: 28.0349\n",
      "Epoch 487/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7110 - val_loss: 28.0358\n",
      "Epoch 488/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.7111 - val_loss: 28.0366\n",
      "Epoch 489/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7112 - val_loss: 28.0376\n",
      "Epoch 490/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7112 - val_loss: 28.0384\n",
      "Epoch 491/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7113 - val_loss: 28.0388\n",
      "Epoch 492/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7114 - val_loss: 28.0395\n",
      "Epoch 493/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7116 - val_loss: 28.0403\n",
      "Epoch 494/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7116 - val_loss: 28.0407\n",
      "Epoch 495/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 9.7118 - val_loss: 28.0414\n",
      "Epoch 496/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7118 - val_loss: 28.0417\n",
      "Epoch 497/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7119 - val_loss: 28.0420\n",
      "Epoch 498/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7121 - val_loss: 28.0423\n",
      "Epoch 499/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.7122 - val_loss: 28.0426\n",
      "Epoch 500/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7123 - val_loss: 28.0430\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6856, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0008, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0128, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(-2.3003, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 364ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.07180672e+01, 7.06946078e+01, 7.06862045e+01, 7.06778011e+01,\n",
       "        7.06693978e+01, 7.06609944e+01, 7.06525910e+01, 7.06441877e+01,\n",
       "        7.06357843e+01, 7.06273810e+01, 7.06189776e+01, 7.06105742e+01,\n",
       "        7.06021709e+01, 7.05937675e+01, 7.05853642e+01, 7.05769608e+01,\n",
       "        7.05685574e+01, 7.05601541e+01, 7.05517507e+01, 7.05433473e+01,\n",
       "        7.05349440e+01, 7.05265406e+01, 7.05181372e+01, 7.05097339e+01,\n",
       "        7.05013305e+01, 7.04929272e+01, 7.04845238e+01, 7.04761204e+01,\n",
       "        7.04677171e+01, 7.04593137e+01, 7.04509104e+01, 7.04425070e+01,\n",
       "        7.04341036e+01, 7.04257003e+01, 7.04172969e+01, 7.04088936e+01,\n",
       "        7.04004902e+01, 7.03920868e+01, 7.03836835e+01, 7.03752801e+01,\n",
       "        7.03668768e+01, 7.03584734e+01, 7.03500700e+01, 7.03416667e+01,\n",
       "        7.03332633e+01, 7.03248599e+01, 7.03164566e+01, 7.03080532e+01,\n",
       "        7.02996499e+01, 7.02912465e+01, 7.02828431e+01, 7.02744398e+01,\n",
       "        7.02660364e+01, 7.02576330e+01, 7.02492297e+01, 7.02408263e+01,\n",
       "        7.02324230e+01, 7.02240196e+01, 7.02156162e+01, 7.02072129e+01,\n",
       "        7.01988095e+01, 7.01904062e+01, 7.01820028e+01, 7.01735994e+01,\n",
       "        7.01651961e+01, 7.01567927e+01, 7.01483894e+01, 7.01399860e+01,\n",
       "        7.01315826e+01, 7.01231793e+01, 7.01147759e+01, 7.01063726e+01,\n",
       "        7.00979692e+01, 7.00895658e+01, 7.00811625e+01, 7.00727591e+01,\n",
       "        7.00643557e+01, 7.00559524e+01, 7.00475490e+01, 7.00391457e+01,\n",
       "        7.56191101e+01, 0.00000000e+00, 6.44442618e-01, 0.00000000e+00,\n",
       "        2.70426869e-01, 6.12748042e-03, 5.23406863e-01, 1.77199483e-01,\n",
       "        1.63515449e-01, 0.00000000e+00, 0.00000000e+00, 7.30717063e-01,\n",
       "        0.00000000e+00, 3.46078575e-01, 3.29938740e-01, 7.59918988e-01,\n",
       "        6.68463528e-01, 0.00000000e+00, 9.78886187e-02, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([67.70483894, 67.69484827, 67.68485761, 67.67486695, 67.66487628,\n",
       "       67.65488562, 67.64489496, 67.6349043 , 67.62491363, 67.61492297,\n",
       "       67.60493231, 67.59494164, 67.58495098, 67.57496032, 67.56496965,\n",
       "       67.55497899, 67.54498833, 67.53499767, 67.525007  , 67.51501634,\n",
       "       67.50502568, 67.49503501, 67.48504435, 67.47505369, 67.46506303,\n",
       "       67.45507236, 67.4450817 , 67.43509104, 67.42510037, 67.41510971,\n",
       "       67.40511905, 67.39512838, 67.38513772, 67.37514706, 67.3651564 ,\n",
       "       67.35516573, 67.34517507, 67.33518441, 67.32519374, 67.31520308,\n",
       "       67.30521242, 67.29522176, 67.28523109, 67.27524043, 67.26524977,\n",
       "       67.2552591 , 67.24526844, 67.23527778, 67.22528711, 67.21529645,\n",
       "       67.20530579, 67.19531513, 67.18532446, 67.1753338 , 67.16534314,\n",
       "       67.15535247, 67.14536181, 67.13537115, 67.12538049, 67.11538982,\n",
       "       67.10539916, 67.0954085 , 67.08541783, 67.07542717, 67.06543651,\n",
       "       67.05544585, 67.04545518, 67.03546452, 67.02547386, 67.01548319,\n",
       "       67.00549253, 66.99550187, 66.9855112 , 66.97552054, 66.96552988,\n",
       "       66.95553922, 66.94554855, 66.93555789, 66.92556723, 66.91557656,\n",
       "       66.9055859 , 66.89559524, 66.88560458, 66.87561391, 66.86562325,\n",
       "       66.85563259, 66.84564192, 66.83565126, 66.8256606 , 66.81566993,\n",
       "       66.80567927, 66.79568861, 66.78569795, 66.77570728, 66.76571662,\n",
       "       66.75572596, 66.74573529, 66.73574463, 66.72551587, 66.71496499])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.153866605224838\n",
      "15.181869802232576\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
