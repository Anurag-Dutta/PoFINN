{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "2495    55.049190\n",
       "2496    55.036893\n",
       "2497    55.024595\n",
       "2498    55.012298\n",
       "2499    55.000000\n",
       "Name: C4, Length: 2500, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c4_interpolated_2400_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "2395     0.367244\n",
       "2396     0.000000\n",
       "2397     0.000000\n",
       "2398     0.595918\n",
       "2399     0.310173\n",
       "Name: C4, Length: 2400, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2400)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.409524</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.219048</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.028571</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.838095</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.600000  0.000298\n",
       "1     88.409524  0.000298\n",
       "2     88.219048  0.000297\n",
       "3     88.028571  0.000297\n",
       "4     87.838095  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6856  # Value for C0\n",
    "K0 = -0.0008  # Value for K0\n",
    "K1 = -0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0128    # Value for b\n",
    "c = -2.3003    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.600000    88.409524    88.219048    88.028571    87.838095   \n",
      "351    88.409524    88.219048    88.028571    87.838095    87.647619   \n",
      "352    88.219048    88.028571    87.838095    87.647619    87.457143   \n",
      "353    88.028571    87.838095    87.647619    87.457143    87.266667   \n",
      "354    87.838095    87.647619    87.457143    87.266667    87.076190   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.647619    87.457143    87.266667    87.076190    86.896218  ...   \n",
      "351    87.457143    87.266667    87.076190    86.896218    86.845798  ...   \n",
      "352    87.266667    87.076190    86.896218    86.845798    86.795378  ...   \n",
      "353    87.076190    86.896218    86.845798    86.795378    86.744958  ...   \n",
      "354    86.896218    86.845798    86.795378    86.744958    86.694538  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   78.880345    0.000263   78.856069    0.000263   78.831793    0.000263   \n",
      "351   78.856069    0.000263   78.831793    0.000263   78.807516    0.000262   \n",
      "352   78.831793    0.000263   78.807516    0.000262   78.783240    0.000262   \n",
      "353   78.807516    0.000262   78.783240    0.000262   78.758964    0.000262   \n",
      "354   78.783240    0.000262   78.758964    0.000262   78.734687    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   78.807516    0.000262   78.783240    0.000262  \n",
      "351   78.783240    0.000262   78.758964    0.000262  \n",
      "352   78.758964    0.000262   78.734687    0.000262  \n",
      "353   78.734687    0.000262   78.710411    0.000262  \n",
      "354   78.710411    0.000262   78.686134    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1950, 1, 251) (1950, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "25/25 [==============================] - 3s 22ms/step - loss: 5040.9810 - val_loss: 3018.4211\n",
      "Epoch 2/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 4809.4775 - val_loss: 2857.6191\n",
      "Epoch 3/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4638.7271 - val_loss: 2773.8474\n",
      "Epoch 4/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4511.9453 - val_loss: 2700.4961\n",
      "Epoch 5/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4397.7832 - val_loss: 2633.0603\n",
      "Epoch 6/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4288.6235 - val_loss: 2568.7273\n",
      "Epoch 7/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 4183.1982 - val_loss: 2506.5361\n",
      "Epoch 8/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4080.8828 - val_loss: 2447.2124\n",
      "Epoch 9/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3973.2273 - val_loss: 2381.7671\n",
      "Epoch 10/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3868.2090 - val_loss: 2322.9426\n",
      "Epoch 11/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3768.5029 - val_loss: 2266.6277\n",
      "Epoch 12/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 3671.9036 - val_loss: 2212.4543\n",
      "Epoch 13/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3577.9419 - val_loss: 2160.2102\n",
      "Epoch 14/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3486.3538 - val_loss: 2109.7659\n",
      "Epoch 15/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3396.9746 - val_loss: 2061.0295\n",
      "Epoch 16/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 3309.6895 - val_loss: 2013.9338\n",
      "Epoch 17/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3224.4109 - val_loss: 1968.4204\n",
      "Epoch 18/500\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 3141.0674 - val_loss: 1924.4417\n",
      "Epoch 19/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 3059.6006 - val_loss: 1881.9550\n",
      "Epoch 20/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2979.9580 - val_loss: 1840.9222\n",
      "Epoch 21/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2902.0950 - val_loss: 1801.3080\n",
      "Epoch 22/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2825.9695 - val_loss: 1763.0790\n",
      "Epoch 23/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2751.5439 - val_loss: 1726.2043\n",
      "Epoch 24/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2678.7820 - val_loss: 1690.6547\n",
      "Epoch 25/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2607.6511 - val_loss: 1656.4017\n",
      "Epoch 26/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2538.1194 - val_loss: 1623.4182\n",
      "Epoch 27/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2470.1570 - val_loss: 1591.6776\n",
      "Epoch 28/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2403.7351 - val_loss: 1561.1547\n",
      "Epoch 29/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2338.8262 - val_loss: 1531.8248\n",
      "Epoch 30/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2275.4036 - val_loss: 1503.6631\n",
      "Epoch 31/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2213.4402 - val_loss: 1476.6465\n",
      "Epoch 32/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2152.9116 - val_loss: 1450.7509\n",
      "Epoch 33/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2093.7937 - val_loss: 1425.9539\n",
      "Epoch 34/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2036.0615 - val_loss: 1402.2332\n",
      "Epoch 35/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1979.6914 - val_loss: 1379.5667\n",
      "Epoch 36/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1924.6609 - val_loss: 1357.9315\n",
      "Epoch 37/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1870.9465 - val_loss: 1337.3076\n",
      "Epoch 38/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1818.5262 - val_loss: 1317.6727\n",
      "Epoch 39/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1767.3789 - val_loss: 1299.0065\n",
      "Epoch 40/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1717.4821 - val_loss: 1281.2878\n",
      "Epoch 41/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1668.8145 - val_loss: 1264.4966\n",
      "Epoch 42/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1621.3556 - val_loss: 1248.6121\n",
      "Epoch 43/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1575.0846 - val_loss: 1233.6146\n",
      "Epoch 44/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1529.9810 - val_loss: 1219.4840\n",
      "Epoch 45/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1486.0247 - val_loss: 1206.2009\n",
      "Epoch 46/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1443.1953 - val_loss: 1193.7461\n",
      "Epoch 47/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1401.4740 - val_loss: 1182.0995\n",
      "Epoch 48/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1360.8406 - val_loss: 1171.2427\n",
      "Epoch 49/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1321.2760 - val_loss: 1161.1564\n",
      "Epoch 50/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1282.7616 - val_loss: 1151.8220\n",
      "Epoch 51/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1245.2778 - val_loss: 1143.2209\n",
      "Epoch 52/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1208.8065 - val_loss: 1135.3347\n",
      "Epoch 53/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1173.3293 - val_loss: 1128.1450\n",
      "Epoch 54/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 1138.8275 - val_loss: 1121.6339\n",
      "Epoch 55/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 1105.2833 - val_loss: 1115.7828\n",
      "Epoch 56/500\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 1072.6783 - val_loss: 1110.5745\n",
      "Epoch 57/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1040.9952 - val_loss: 1105.9910\n",
      "Epoch 58/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 1010.2164 - val_loss: 1102.0150\n",
      "Epoch 59/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 980.3241 - val_loss: 1098.6289\n",
      "Epoch 60/500\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 951.3014 - val_loss: 1095.8154\n",
      "Epoch 61/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 923.1309 - val_loss: 1093.5577\n",
      "Epoch 62/500\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 895.7959 - val_loss: 1091.8386\n",
      "Epoch 63/500\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 869.2795 - val_loss: 1090.6415\n",
      "Epoch 64/500\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 843.5649 - val_loss: 1089.9495\n",
      "Epoch 65/500\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 818.6358 - val_loss: 1089.7461\n",
      "Epoch 66/500\n",
      "25/25 [==============================] - 0s 17ms/step - loss: 794.4758 - val_loss: 1090.0154\n",
      "Epoch 67/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 771.0685 - val_loss: 1090.7406\n",
      "Epoch 68/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 748.3981 - val_loss: 1091.9059\n",
      "Epoch 69/500\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 726.4487 - val_loss: 1093.4954\n",
      "Epoch 70/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 705.2043 - val_loss: 1095.4932\n",
      "Epoch 71/500\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 684.6497 - val_loss: 1097.8840\n",
      "Epoch 72/500\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 664.7692 - val_loss: 1100.6522\n",
      "Epoch 73/500\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 645.5475 - val_loss: 1103.7826\n",
      "Epoch 74/500\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 626.9695 - val_loss: 1107.2601\n",
      "Epoch 75/500\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 609.0202 - val_loss: 1111.0696\n",
      "Epoch 76/500\n",
      "25/25 [==============================] - 1s 26ms/step - loss: 591.6849 - val_loss: 1115.1965\n",
      "Epoch 77/500\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 574.9489 - val_loss: 1119.6260\n",
      "Epoch 78/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 558.7971 - val_loss: 1124.3440\n",
      "Epoch 79/500\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 543.2156 - val_loss: 1129.3361\n",
      "Epoch 80/500\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 528.1903 - val_loss: 1134.5883\n",
      "Epoch 81/500\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 513.7071 - val_loss: 1140.0868\n",
      "Epoch 82/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 499.7520 - val_loss: 1145.8177\n",
      "Epoch 83/500\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 486.3111 - val_loss: 1151.7678\n",
      "Epoch 84/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 473.3709 - val_loss: 1157.9238\n",
      "Epoch 85/500\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 460.9182 - val_loss: 1164.2727\n",
      "Epoch 86/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 448.9395 - val_loss: 1170.8011\n",
      "Epoch 87/500\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 437.4221 - val_loss: 1177.4971\n",
      "Epoch 88/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 426.3528 - val_loss: 1184.3480\n",
      "Epoch 89/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 415.7188 - val_loss: 1191.3417\n",
      "Epoch 90/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 405.5078 - val_loss: 1198.4659\n",
      "Epoch 91/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 395.7074 - val_loss: 1205.7087\n",
      "Epoch 92/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 386.3053 - val_loss: 1213.0596\n",
      "Epoch 93/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 377.2895 - val_loss: 1220.5063\n",
      "Epoch 94/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 368.6482 - val_loss: 1228.0381\n",
      "Epoch 95/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 360.3698 - val_loss: 1235.6448\n",
      "Epoch 96/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 352.4427 - val_loss: 1243.3149\n",
      "Epoch 97/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 344.8559 - val_loss: 1251.0386\n",
      "Epoch 98/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 337.5983 - val_loss: 1258.8058\n",
      "Epoch 99/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 330.6588 - val_loss: 1266.6071\n",
      "Epoch 100/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 324.0271 - val_loss: 1274.4330\n",
      "Epoch 101/500\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 317.6923 - val_loss: 1282.2734\n",
      "Epoch 102/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 311.6444 - val_loss: 1290.1201\n",
      "Epoch 103/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 305.8734 - val_loss: 1297.9647\n",
      "Epoch 104/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 300.3693 - val_loss: 1305.7985\n",
      "Epoch 105/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 295.1224 - val_loss: 1313.6129\n",
      "Epoch 106/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 290.1235 - val_loss: 1321.4008\n",
      "Epoch 107/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 285.3631 - val_loss: 1329.1541\n",
      "Epoch 108/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 280.8322 - val_loss: 1336.8662\n",
      "Epoch 109/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 276.5223 - val_loss: 1344.5298\n",
      "Epoch 110/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 272.4243 - val_loss: 1352.1387\n",
      "Epoch 111/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 268.5301 - val_loss: 1359.6854\n",
      "Epoch 112/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 264.8315 - val_loss: 1367.1650\n",
      "Epoch 113/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 261.3205 - val_loss: 1374.5715\n",
      "Epoch 114/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 257.9893 - val_loss: 1381.8986\n",
      "Epoch 115/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 254.8304 - val_loss: 1389.1423\n",
      "Epoch 116/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 251.8365 - val_loss: 1396.2971\n",
      "Epoch 117/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 249.0003 - val_loss: 1403.3586\n",
      "Epoch 118/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 246.3151 - val_loss: 1410.3217\n",
      "Epoch 119/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 243.7741 - val_loss: 1417.1832\n",
      "Epoch 120/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 241.3708 - val_loss: 1423.9397\n",
      "Epoch 121/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 239.0989 - val_loss: 1430.5865\n",
      "Epoch 122/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 236.9523 - val_loss: 1437.1216\n",
      "Epoch 123/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 234.9252 - val_loss: 1443.5409\n",
      "Epoch 124/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 233.0118 - val_loss: 1449.8431\n",
      "Epoch 125/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 231.2066 - val_loss: 1456.0248\n",
      "Epoch 126/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 229.5046 - val_loss: 1462.0840\n",
      "Epoch 127/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 227.9004 - val_loss: 1468.0193\n",
      "Epoch 128/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 226.3892 - val_loss: 1473.8289\n",
      "Epoch 129/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 224.9662 - val_loss: 1479.5122\n",
      "Epoch 130/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 223.6269 - val_loss: 1485.0659\n",
      "Epoch 131/500\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 222.3670 - val_loss: 1490.4912\n",
      "Epoch 132/500\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 221.1823 - val_loss: 1495.7869\n",
      "Epoch 133/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 220.0688 - val_loss: 1500.9518\n",
      "Epoch 134/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 219.0227 - val_loss: 1505.9863\n",
      "Epoch 135/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 218.0402 - val_loss: 1510.8904\n",
      "Epoch 136/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 217.1179 - val_loss: 1515.6641\n",
      "Epoch 137/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 216.2524 - val_loss: 1520.3080\n",
      "Epoch 138/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 215.4406 - val_loss: 1524.8223\n",
      "Epoch 139/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 214.6793 - val_loss: 1529.2076\n",
      "Epoch 140/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 213.9657 - val_loss: 1533.4653\n",
      "Epoch 141/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 213.2970 - val_loss: 1537.5969\n",
      "Epoch 142/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 212.6706 - val_loss: 1541.6025\n",
      "Epoch 143/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 212.0839 - val_loss: 1545.4833\n",
      "Epoch 144/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 211.5347 - val_loss: 1549.2415\n",
      "Epoch 145/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 211.0206 - val_loss: 1552.8789\n",
      "Epoch 146/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 210.5396 - val_loss: 1556.3955\n",
      "Epoch 147/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 210.0896 - val_loss: 1559.7960\n",
      "Epoch 148/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 209.6687 - val_loss: 1563.0795\n",
      "Epoch 149/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 209.2750 - val_loss: 1566.2491\n",
      "Epoch 150/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 208.9071 - val_loss: 1569.3068\n",
      "Epoch 151/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 208.5631 - val_loss: 1572.2552\n",
      "Epoch 152/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 208.2415 - val_loss: 1575.0953\n",
      "Epoch 153/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 207.9411 - val_loss: 1577.8311\n",
      "Epoch 154/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 207.6602 - val_loss: 1580.4633\n",
      "Epoch 155/500\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 207.3979 - val_loss: 1582.9952\n",
      "Epoch 156/500\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 207.1527 - val_loss: 1585.4290\n",
      "Epoch 157/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 206.9237 - val_loss: 1587.7657\n",
      "Epoch 158/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 206.7099 - val_loss: 1590.0111\n",
      "Epoch 159/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 206.5101 - val_loss: 1592.1644\n",
      "Epoch 160/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 206.3234 - val_loss: 1594.2294\n",
      "Epoch 161/500\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 206.1491 - val_loss: 1596.2091\n",
      "Epoch 162/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 205.9862 - val_loss: 1598.1046\n",
      "Epoch 163/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 205.8341 - val_loss: 1599.9205\n",
      "Epoch 164/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 205.6921 - val_loss: 1601.6566\n",
      "Epoch 165/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 205.5593 - val_loss: 1603.3181\n",
      "Epoch 166/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 205.4353 - val_loss: 1604.9049\n",
      "Epoch 167/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 205.3195 - val_loss: 1606.4216\n",
      "Epoch 168/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 205.2112 - val_loss: 1607.8685\n",
      "Epoch 169/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 205.1102 - val_loss: 1609.2501\n",
      "Epoch 170/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 205.0157 - val_loss: 1610.5665\n",
      "Epoch 171/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 204.9274 - val_loss: 1611.8223\n",
      "Epoch 172/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 204.8449 - val_loss: 1613.0183\n",
      "Epoch 173/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 204.7678 - val_loss: 1614.1578\n",
      "Epoch 174/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 204.6958 - val_loss: 1615.2417\n",
      "Epoch 175/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 204.6284 - val_loss: 1616.2720\n",
      "Epoch 176/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 204.5656 - val_loss: 1617.2522\n",
      "Epoch 177/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 204.5067 - val_loss: 1618.1835\n",
      "Epoch 178/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 204.4518 - val_loss: 1619.0676\n",
      "Epoch 179/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 204.4004 - val_loss: 1619.9082\n",
      "Epoch 180/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 204.3523 - val_loss: 1620.7040\n",
      "Epoch 181/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 204.3075 - val_loss: 1621.4601\n",
      "Epoch 182/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 204.2655 - val_loss: 1622.1766\n",
      "Epoch 183/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 204.2263 - val_loss: 1622.8545\n",
      "Epoch 184/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 204.1896 - val_loss: 1623.4968\n",
      "Epoch 185/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 204.1553 - val_loss: 1624.1050\n",
      "Epoch 186/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 204.1233 - val_loss: 1624.6799\n",
      "Epoch 187/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 204.0934 - val_loss: 1625.2239\n",
      "Epoch 188/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 204.0656 - val_loss: 1625.7397\n",
      "Epoch 189/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 204.0394 - val_loss: 1626.2247\n",
      "Epoch 190/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 204.0150 - val_loss: 1626.6841\n",
      "Epoch 191/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.9922 - val_loss: 1627.1168\n",
      "Epoch 192/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 203.9710 - val_loss: 1627.5260\n",
      "Epoch 193/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.9512 - val_loss: 1627.9115\n",
      "Epoch 194/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.9326 - val_loss: 1628.2748\n",
      "Epoch 195/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.9153 - val_loss: 1628.6178\n",
      "Epoch 196/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.8992 - val_loss: 1628.9399\n",
      "Epoch 197/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.8842 - val_loss: 1629.2435\n",
      "Epoch 198/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.8701 - val_loss: 1629.5298\n",
      "Epoch 199/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.8571 - val_loss: 1629.7993\n",
      "Epoch 200/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 203.8450 - val_loss: 1630.0518\n",
      "Epoch 201/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.8336 - val_loss: 1630.2904\n",
      "Epoch 202/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.8231 - val_loss: 1630.5133\n",
      "Epoch 203/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.8133 - val_loss: 1630.7228\n",
      "Epoch 204/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.8042 - val_loss: 1630.9207\n",
      "Epoch 205/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7957 - val_loss: 1631.1056\n",
      "Epoch 206/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7878 - val_loss: 1631.2799\n",
      "Epoch 207/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7805 - val_loss: 1631.4425\n",
      "Epoch 208/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7738 - val_loss: 1631.5948\n",
      "Epoch 209/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7675 - val_loss: 1631.7380\n",
      "Epoch 210/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7617 - val_loss: 1631.8733\n",
      "Epoch 211/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7563 - val_loss: 1631.9985\n",
      "Epoch 212/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7513 - val_loss: 1632.1165\n",
      "Epoch 213/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7467 - val_loss: 1632.2266\n",
      "Epoch 214/500\n",
      "25/25 [==============================] - 0s 20ms/step - loss: 203.7425 - val_loss: 1632.3301\n",
      "Epoch 215/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 203.7386 - val_loss: 1632.4268\n",
      "Epoch 216/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7349 - val_loss: 1632.5165\n",
      "Epoch 217/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7316 - val_loss: 1632.6014\n",
      "Epoch 218/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7285 - val_loss: 1632.6803\n",
      "Epoch 219/500\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 203.7256 - val_loss: 1632.7540\n",
      "Epoch 220/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 203.7232 - val_loss: 1632.8229\n",
      "Epoch 221/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7208 - val_loss: 1632.8872\n",
      "Epoch 222/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7186 - val_loss: 1632.9473\n",
      "Epoch 223/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7167 - val_loss: 1633.0037\n",
      "Epoch 224/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7148 - val_loss: 1633.0554\n",
      "Epoch 225/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7133 - val_loss: 1633.1045\n",
      "Epoch 226/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 203.7118 - val_loss: 1633.1503\n",
      "Epoch 227/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 203.7104 - val_loss: 1633.1920\n",
      "Epoch 228/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 203.7093 - val_loss: 1633.2314\n",
      "Epoch 229/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 203.7083 - val_loss: 1633.2678\n",
      "Epoch 230/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 203.7074 - val_loss: 1633.3029\n",
      "Epoch 231/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 203.7065 - val_loss: 1633.3352\n",
      "Epoch 232/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7057 - val_loss: 1633.3647\n",
      "Epoch 233/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7050 - val_loss: 1633.3917\n",
      "Epoch 234/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7046 - val_loss: 1633.4182\n",
      "Epoch 235/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7040 - val_loss: 1633.4430\n",
      "Epoch 236/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 203.7035 - val_loss: 1633.4647\n",
      "Epoch 237/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 203.7031 - val_loss: 1633.4850\n",
      "Epoch 238/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7029 - val_loss: 1633.5045\n",
      "Epoch 239/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7026 - val_loss: 1633.5225\n",
      "Epoch 240/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 203.7024 - val_loss: 1633.5386\n",
      "Epoch 241/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7023 - val_loss: 1633.5542\n",
      "Epoch 242/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 203.7022 - val_loss: 1633.5690\n",
      "Epoch 243/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7022 - val_loss: 1633.5824\n",
      "Epoch 244/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7021 - val_loss: 1633.5955\n",
      "Epoch 245/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 203.7021 - val_loss: 1633.6067\n",
      "Epoch 246/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7021 - val_loss: 1633.6177\n",
      "Epoch 247/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7021 - val_loss: 1633.6277\n",
      "Epoch 248/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7023 - val_loss: 1633.6372\n",
      "Epoch 249/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7023 - val_loss: 1633.6461\n",
      "Epoch 250/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7025 - val_loss: 1633.6541\n",
      "Epoch 251/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7026 - val_loss: 1633.6615\n",
      "Epoch 252/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 203.7028 - val_loss: 1633.6682\n",
      "Epoch 253/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 203.7029 - val_loss: 1633.6746\n",
      "Epoch 254/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7031 - val_loss: 1633.6813\n",
      "Epoch 255/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7033 - val_loss: 1633.6862\n",
      "Epoch 256/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7035 - val_loss: 1633.6917\n",
      "Epoch 257/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7038 - val_loss: 1633.6963\n",
      "Epoch 258/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7040 - val_loss: 1633.7003\n",
      "Epoch 259/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7042 - val_loss: 1633.7046\n",
      "Epoch 260/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7045 - val_loss: 1633.7084\n",
      "Epoch 261/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7047 - val_loss: 1633.7120\n",
      "Epoch 262/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 203.7049 - val_loss: 1633.7148\n",
      "Epoch 263/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7052 - val_loss: 1633.7173\n",
      "Epoch 264/500\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 203.7055 - val_loss: 1633.7212\n",
      "Epoch 265/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7057 - val_loss: 1633.7235\n",
      "Epoch 266/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7060 - val_loss: 1633.7252\n",
      "Epoch 267/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7063 - val_loss: 1633.7281\n",
      "Epoch 268/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7065 - val_loss: 1633.7300\n",
      "Epoch 269/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7068 - val_loss: 1633.7314\n",
      "Epoch 270/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7070 - val_loss: 1633.7329\n",
      "Epoch 271/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7073 - val_loss: 1633.7350\n",
      "Epoch 272/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 203.7075 - val_loss: 1633.7363\n",
      "Epoch 273/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 203.7078 - val_loss: 1633.7377\n",
      "Epoch 274/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 203.7081 - val_loss: 1633.7389\n",
      "Epoch 275/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 203.7084 - val_loss: 1633.7404\n",
      "Epoch 276/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7086 - val_loss: 1633.7410\n",
      "Epoch 277/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7088 - val_loss: 1633.7423\n",
      "Epoch 278/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7091 - val_loss: 1633.7429\n",
      "Epoch 279/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7094 - val_loss: 1633.7444\n",
      "Epoch 280/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7095 - val_loss: 1633.7458\n",
      "Epoch 281/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7098 - val_loss: 1633.7463\n",
      "Epoch 282/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7100 - val_loss: 1633.7466\n",
      "Epoch 283/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7103 - val_loss: 1633.7473\n",
      "Epoch 284/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7105 - val_loss: 1633.7476\n",
      "Epoch 285/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7108 - val_loss: 1633.7484\n",
      "Epoch 286/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7109 - val_loss: 1633.7489\n",
      "Epoch 287/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 203.7112 - val_loss: 1633.7494\n",
      "Epoch 288/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7113 - val_loss: 1633.7496\n",
      "Epoch 289/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7115 - val_loss: 1633.7496\n",
      "Epoch 290/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7117 - val_loss: 1633.7500\n",
      "Epoch 291/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7119 - val_loss: 1633.7500\n",
      "Epoch 292/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 203.7122 - val_loss: 1633.7509\n",
      "Epoch 293/500\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 203.7123 - val_loss: 1633.7515\n",
      "Epoch 294/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7125 - val_loss: 1633.7521\n",
      "Epoch 295/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7128 - val_loss: 1633.7526\n",
      "Epoch 296/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7128 - val_loss: 1633.7526\n",
      "Epoch 297/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7131 - val_loss: 1633.7526\n",
      "Epoch 298/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7132 - val_loss: 1633.7526\n",
      "Epoch 299/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7134 - val_loss: 1633.7527\n",
      "Epoch 300/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7136 - val_loss: 1633.7537\n",
      "Epoch 301/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7137 - val_loss: 1633.7539\n",
      "Epoch 302/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7139 - val_loss: 1633.7542\n",
      "Epoch 303/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7141 - val_loss: 1633.7543\n",
      "Epoch 304/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7142 - val_loss: 1633.7545\n",
      "Epoch 305/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7144 - val_loss: 1633.7548\n",
      "Epoch 306/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7145 - val_loss: 1633.7546\n",
      "Epoch 307/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 203.7146 - val_loss: 1633.7546\n",
      "Epoch 308/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7148 - val_loss: 1633.7543\n",
      "Epoch 309/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 203.7150 - val_loss: 1633.7546\n",
      "Epoch 310/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 203.7151 - val_loss: 1633.7552\n",
      "Epoch 311/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7152 - val_loss: 1633.7551\n",
      "Epoch 312/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7153 - val_loss: 1633.7552\n",
      "Epoch 313/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7155 - val_loss: 1633.7557\n",
      "Epoch 314/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7156 - val_loss: 1633.7562\n",
      "Epoch 315/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7156 - val_loss: 1633.7557\n",
      "Epoch 316/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7158 - val_loss: 1633.7560\n",
      "Epoch 317/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7159 - val_loss: 1633.7562\n",
      "Epoch 318/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7160 - val_loss: 1633.7566\n",
      "Epoch 319/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7161 - val_loss: 1633.7565\n",
      "Epoch 320/500\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 203.7162 - val_loss: 1633.7562\n",
      "Epoch 321/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7162 - val_loss: 1633.7562\n",
      "Epoch 322/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7164 - val_loss: 1633.7561\n",
      "Epoch 323/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7165 - val_loss: 1633.7557\n",
      "Epoch 324/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7167 - val_loss: 1633.7557\n",
      "Epoch 325/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7167 - val_loss: 1633.7557\n",
      "Epoch 326/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7168 - val_loss: 1633.7556\n",
      "Epoch 327/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7169 - val_loss: 1633.7556\n",
      "Epoch 328/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7170 - val_loss: 1633.7561\n",
      "Epoch 329/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7171 - val_loss: 1633.7561\n",
      "Epoch 330/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 203.7172 - val_loss: 1633.7561\n",
      "Epoch 331/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7173 - val_loss: 1633.7557\n",
      "Epoch 332/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7173 - val_loss: 1633.7556\n",
      "Epoch 333/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7175 - val_loss: 1633.7550\n",
      "Epoch 334/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7176 - val_loss: 1633.7552\n",
      "Epoch 335/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7176 - val_loss: 1633.7565\n",
      "Epoch 336/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7176 - val_loss: 1633.7567\n",
      "Epoch 337/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7177 - val_loss: 1633.7562\n",
      "Epoch 338/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7177 - val_loss: 1633.7560\n",
      "Epoch 339/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7178 - val_loss: 1633.7555\n",
      "Epoch 340/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7179 - val_loss: 1633.7551\n",
      "Epoch 341/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7180 - val_loss: 1633.7550\n",
      "Epoch 342/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7181 - val_loss: 1633.7552\n",
      "Epoch 343/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7182 - val_loss: 1633.7556\n",
      "Epoch 344/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7183 - val_loss: 1633.7562\n",
      "Epoch 345/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7183 - val_loss: 1633.7562\n",
      "Epoch 346/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7183 - val_loss: 1633.7562\n",
      "Epoch 347/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7184 - val_loss: 1633.7561\n",
      "Epoch 348/500\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 203.7184 - val_loss: 1633.7560\n",
      "Epoch 349/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7185 - val_loss: 1633.7556\n",
      "Epoch 350/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7186 - val_loss: 1633.7551\n",
      "Epoch 351/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7186 - val_loss: 1633.7545\n",
      "Epoch 352/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7188 - val_loss: 1633.7543\n",
      "Epoch 353/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7189 - val_loss: 1633.7545\n",
      "Epoch 354/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7189 - val_loss: 1633.7556\n",
      "Epoch 355/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7189 - val_loss: 1633.7567\n",
      "Epoch 356/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7188 - val_loss: 1633.7571\n",
      "Epoch 357/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7189 - val_loss: 1633.7573\n",
      "Epoch 358/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7189 - val_loss: 1633.7567\n",
      "Epoch 359/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7189 - val_loss: 1633.7566\n",
      "Epoch 360/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7190 - val_loss: 1633.7557\n",
      "Epoch 361/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 203.7191 - val_loss: 1633.7561\n",
      "Epoch 362/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 203.7191 - val_loss: 1633.7572\n",
      "Epoch 363/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 203.7191 - val_loss: 1633.7581\n",
      "Epoch 364/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 203.7191 - val_loss: 1633.7582\n",
      "Epoch 365/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7191 - val_loss: 1633.7578\n",
      "Epoch 366/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7192 - val_loss: 1633.7583\n",
      "Epoch 367/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7192 - val_loss: 1633.7590\n",
      "Epoch 368/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7192 - val_loss: 1633.7590\n",
      "Epoch 369/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7192 - val_loss: 1633.7590\n",
      "Epoch 370/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7192 - val_loss: 1633.7585\n",
      "Epoch 371/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7193 - val_loss: 1633.7582\n",
      "Epoch 372/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 203.7193 - val_loss: 1633.7577\n",
      "Epoch 373/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 203.7194 - val_loss: 1633.7567\n",
      "Epoch 374/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7194 - val_loss: 1633.7557\n",
      "Epoch 375/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7195 - val_loss: 1633.7561\n",
      "Epoch 376/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7196 - val_loss: 1633.7557\n",
      "Epoch 377/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7196 - val_loss: 1633.7561\n",
      "Epoch 378/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7197 - val_loss: 1633.7572\n",
      "Epoch 379/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7197 - val_loss: 1633.7583\n",
      "Epoch 380/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7196 - val_loss: 1633.7590\n",
      "Epoch 381/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7196 - val_loss: 1633.7593\n",
      "Epoch 382/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7196 - val_loss: 1633.7598\n",
      "Epoch 383/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 203.7196 - val_loss: 1633.7603\n",
      "Epoch 384/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7196 - val_loss: 1633.7603\n",
      "Epoch 385/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7196 - val_loss: 1633.7603\n",
      "Epoch 386/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7196 - val_loss: 1633.7598\n",
      "Epoch 387/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 203.7196 - val_loss: 1633.7593\n",
      "Epoch 388/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7197 - val_loss: 1633.7590\n",
      "Epoch 389/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7197 - val_loss: 1633.7590\n",
      "Epoch 390/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7198 - val_loss: 1633.7585\n",
      "Epoch 391/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7198 - val_loss: 1633.7572\n",
      "Epoch 392/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7199 - val_loss: 1633.7567\n",
      "Epoch 393/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7199 - val_loss: 1633.7565\n",
      "Epoch 394/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7200 - val_loss: 1633.7565\n",
      "Epoch 395/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7200 - val_loss: 1633.7560\n",
      "Epoch 396/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7200 - val_loss: 1633.7560\n",
      "Epoch 397/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7200 - val_loss: 1633.7562\n",
      "Epoch 398/500\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 203.7200 - val_loss: 1633.7566\n",
      "Epoch 399/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 203.7200 - val_loss: 1633.7567\n",
      "Epoch 400/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7200 - val_loss: 1633.7568\n",
      "Epoch 401/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7200 - val_loss: 1633.7573\n",
      "Epoch 402/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 203.7200 - val_loss: 1633.7573\n",
      "Epoch 403/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7201 - val_loss: 1633.7568\n",
      "Epoch 404/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7201 - val_loss: 1633.7567\n",
      "Epoch 405/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7201 - val_loss: 1633.7568\n",
      "Epoch 406/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7201 - val_loss: 1633.7576\n",
      "Epoch 407/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7201 - val_loss: 1633.7587\n",
      "Epoch 408/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7201 - val_loss: 1633.7587\n",
      "Epoch 409/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7201 - val_loss: 1633.7590\n",
      "Epoch 410/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7201 - val_loss: 1633.7587\n",
      "Epoch 411/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7201 - val_loss: 1633.7587\n",
      "Epoch 412/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7201 - val_loss: 1633.7587\n",
      "Epoch 413/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7201 - val_loss: 1633.7582\n",
      "Epoch 414/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 203.7202 - val_loss: 1633.7573\n",
      "Epoch 415/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7202 - val_loss: 1633.7568\n",
      "Epoch 416/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7202 - val_loss: 1633.7562\n",
      "Epoch 417/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7202 - val_loss: 1633.7562\n",
      "Epoch 418/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7557\n",
      "Epoch 419/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7552\n",
      "Epoch 420/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7203 - val_loss: 1633.7556\n",
      "Epoch 421/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 203.7203 - val_loss: 1633.7556\n",
      "Epoch 422/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7204 - val_loss: 1633.7557\n",
      "Epoch 423/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7204 - val_loss: 1633.7561\n",
      "Epoch 424/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7204 - val_loss: 1633.7562\n",
      "Epoch 425/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7204 - val_loss: 1633.7562\n",
      "Epoch 426/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7204 - val_loss: 1633.7565\n",
      "Epoch 427/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7204 - val_loss: 1633.7565\n",
      "Epoch 428/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7204 - val_loss: 1633.7566\n",
      "Epoch 429/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7204 - val_loss: 1633.7565\n",
      "Epoch 430/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7205 - val_loss: 1633.7566\n",
      "Epoch 431/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7204 - val_loss: 1633.7567\n",
      "Epoch 432/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7205 - val_loss: 1633.7572\n",
      "Epoch 433/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7204 - val_loss: 1633.7578\n",
      "Epoch 434/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7204 - val_loss: 1633.7587\n",
      "Epoch 435/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7204 - val_loss: 1633.7590\n",
      "Epoch 436/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7204 - val_loss: 1633.7596\n",
      "Epoch 437/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7204 - val_loss: 1633.7599\n",
      "Epoch 438/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7203 - val_loss: 1633.7604\n",
      "Epoch 439/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 203.7203 - val_loss: 1633.7611\n",
      "Epoch 440/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7203 - val_loss: 1633.7612\n",
      "Epoch 441/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7203 - val_loss: 1633.7614\n",
      "Epoch 442/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7616\n",
      "Epoch 443/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7620\n",
      "Epoch 444/500\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 203.7203 - val_loss: 1633.7620\n",
      "Epoch 445/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7622\n",
      "Epoch 446/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7620\n",
      "Epoch 447/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7622\n",
      "Epoch 448/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7623\n",
      "Epoch 449/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7623\n",
      "Epoch 450/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7623\n",
      "Epoch 451/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7625\n",
      "Epoch 452/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7625\n",
      "Epoch 453/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7625\n",
      "Epoch 454/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 203.7203 - val_loss: 1633.7627\n",
      "Epoch 455/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 203.7203 - val_loss: 1633.7627\n",
      "Epoch 456/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7625\n",
      "Epoch 457/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 203.7203 - val_loss: 1633.7625\n",
      "Epoch 458/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7627\n",
      "Epoch 459/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7628\n",
      "Epoch 460/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7629\n",
      "Epoch 461/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7633\n",
      "Epoch 462/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7633\n",
      "Epoch 463/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7638\n",
      "Epoch 464/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7204 - val_loss: 1633.7642\n",
      "Epoch 465/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7204 - val_loss: 1633.7644\n",
      "Epoch 466/500\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 203.7203 - val_loss: 1633.7653\n",
      "Epoch 467/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 203.7203 - val_loss: 1633.7662\n",
      "Epoch 468/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 203.7203 - val_loss: 1633.7664\n",
      "Epoch 469/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 203.7203 - val_loss: 1633.7667\n",
      "Epoch 470/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7675\n",
      "Epoch 471/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 203.7203 - val_loss: 1633.7675\n",
      "Epoch 472/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7202 - val_loss: 1633.7678\n",
      "Epoch 473/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7202 - val_loss: 1633.7678\n",
      "Epoch 474/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 203.7202 - val_loss: 1633.7679\n",
      "Epoch 475/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7202 - val_loss: 1633.7676\n",
      "Epoch 476/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7202 - val_loss: 1633.7673\n",
      "Epoch 477/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7202 - val_loss: 1633.7675\n",
      "Epoch 478/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 203.7202 - val_loss: 1633.7671\n",
      "Epoch 479/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7203 - val_loss: 1633.7671\n",
      "Epoch 480/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7203 - val_loss: 1633.7667\n",
      "Epoch 481/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7668\n",
      "Epoch 482/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7668\n",
      "Epoch 483/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7668\n",
      "Epoch 484/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7203 - val_loss: 1633.7668\n",
      "Epoch 485/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7670\n",
      "Epoch 486/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7670\n",
      "Epoch 487/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7203 - val_loss: 1633.7670\n",
      "Epoch 488/500\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 203.7204 - val_loss: 1633.7671\n",
      "Epoch 489/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 203.7204 - val_loss: 1633.7671\n",
      "Epoch 490/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 203.7204 - val_loss: 1633.7670\n",
      "Epoch 491/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7204 - val_loss: 1633.7671\n",
      "Epoch 492/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7204 - val_loss: 1633.7671\n",
      "Epoch 493/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7204 - val_loss: 1633.7671\n",
      "Epoch 494/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7204 - val_loss: 1633.7670\n",
      "Epoch 495/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7204 - val_loss: 1633.7670\n",
      "Epoch 496/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.7204 - val_loss: 1633.7665\n",
      "Epoch 497/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7204 - val_loss: 1633.7659\n",
      "Epoch 498/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 203.7203 - val_loss: 1633.7655\n",
      "Epoch 499/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 203.7204 - val_loss: 1633.7654\n",
      "Epoch 500/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 203.7204 - val_loss: 1633.7650\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6856, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0008, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0128, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(-2.3003, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 560ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.00531513e+01, 6.99775210e+01, 6.99018908e+01, 6.98262605e+01,\n",
       "        7.52419281e+01, 0.00000000e+00, 3.13602660e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.96247760e-01,\n",
       "        0.00000000e+00, 7.01904062e+01, 7.01147759e+01, 7.00391457e+01,\n",
       "        1.63515449e-01, 0.00000000e+00, 7.03724790e+01, 7.02978487e+01,\n",
       "        7.02212185e+01, 7.01455882e+01, 7.00699580e+01, 6.99943277e+01,\n",
       "        6.99186975e+01, 6.98430672e+01, 6.95829132e+01, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.01259804e+01, 7.00503501e+01, 6.99747199e+01,\n",
       "        6.98990896e+01, 6.98234594e+01, 6.94521942e+01, 6.89479925e+01,\n",
       "        6.84437908e+01, 6.79395892e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.22684860e+01, 0.00000000e+00, 1.68435514e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.98598739e+01,\n",
       "        6.96949580e+01, 2.78723180e-01, 0.00000000e+00, 7.01175770e+01,\n",
       "        7.00419468e+01, 6.99663165e+01, 6.98906863e+01, 6.98150560e+01,\n",
       "        6.93961718e+01, 6.88919701e+01, 6.83877684e+01, 6.78835668e+01,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.98458684e+01, 6.96015873e+01,\n",
       "        6.90973856e+01, 6.85931839e+01, 6.80889823e+01, 7.06862045e+01,\n",
       "        7.04593137e+01, 7.02324230e+01, 0.00000000e+00, 3.87018085e-01,\n",
       "        0.00000000e+00, 6.37546730e+01, 0.00000000e+00, 7.67748475e-01,\n",
       "        9.00180340e-01, 1.61020607e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        6.72492981e+01, 0.00000000e+00, 1.79905474e-01, 1.86276942e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.02461034e-01, 3.36194858e-02,\n",
       "        5.90033650e-01, 8.22303474e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.05914399e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([56.21746259, 56.20516499, 56.19286739, 56.18056979, 56.16827219,\n",
       "       56.15597458, 56.14367698, 56.13137938, 56.11908178, 56.10678418,\n",
       "       56.09448658, 56.08218897, 56.06989137, 56.05759377, 56.04529617,\n",
       "       56.03299857, 56.02070096, 56.00840336, 55.99610576, 55.98380816,\n",
       "       55.97151056, 55.95921295, 55.94691535, 55.93461775, 55.92232015,\n",
       "       55.91002255, 55.89772494, 55.88542734, 55.87312974, 55.86083214,\n",
       "       55.84853454, 55.83623693, 55.82393933, 55.81164173, 55.79934413,\n",
       "       55.78704653, 55.77474892, 55.76245132, 55.75015372, 55.73785612,\n",
       "       55.72555852, 55.71326091, 55.70096331, 55.68866571, 55.67636811,\n",
       "       55.66407051, 55.6517729 , 55.6394753 , 55.6271777 , 55.6148801 ,\n",
       "       55.6025825 , 55.59028489, 55.57798729, 55.56568969, 55.55339209,\n",
       "       55.54109449, 55.52879688, 55.51649928, 55.50420168, 55.49190408,\n",
       "       55.47960648, 55.46730887, 55.45501127, 55.44271367, 55.43041607,\n",
       "       55.41811847, 55.40582086, 55.39352326, 55.38122566, 55.36892806,\n",
       "       55.35663046, 55.34433286, 55.33203525, 55.31973765, 55.30744005,\n",
       "       55.29514245, 55.28284485, 55.27054724, 55.25824964, 55.24595204,\n",
       "       55.23365444, 55.22135684, 55.20905923, 55.19676163, 55.18446403,\n",
       "       55.17216643, 55.15986883, 55.14757122, 55.13527362, 55.12297602,\n",
       "       55.11067842, 55.09838082, 55.08608321, 55.07378561, 55.06148801,\n",
       "       55.04919041, 55.03689281, 55.0245952 , 55.0122976 , 55.        ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.064788727431065\n",
      "35.35325008245795\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
