{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "1795    64.840966\n",
       "1796    64.832563\n",
       "1797    64.824160\n",
       "1798    64.815756\n",
       "1799    64.807353\n",
       "Name: C4, Length: 1800, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c4_interpolated_1700_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "1695     0.149832\n",
       "1696     0.000000\n",
       "1697     0.032830\n",
       "1698     0.313603\n",
       "1699     0.000000\n",
       "Name: C4, Length: 1700, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1700)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.409524</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.219048</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.028571</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.838095</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.600000  0.000298\n",
       "1     88.409524  0.000298\n",
       "2     88.219048  0.000297\n",
       "3     88.028571  0.000297\n",
       "4     87.838095  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6856  # Value for C0\n",
    "K0 = -0.0008  # Value for K0\n",
    "K1 = -0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0128    # Value for b\n",
    "c = -2.3003    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.600000    88.409524    88.219048    88.028571    87.838095   \n",
      "351    88.409524    88.219048    88.028571    87.838095    87.647619   \n",
      "352    88.219048    88.028571    87.838095    87.647619    87.457143   \n",
      "353    88.028571    87.838095    87.647619    87.457143    87.266667   \n",
      "354    87.838095    87.647619    87.457143    87.266667    87.076190   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.647619    87.457143    87.266667    87.076190    86.896218  ...   \n",
      "351    87.457143    87.266667    87.076190    86.896218    86.845798  ...   \n",
      "352    87.266667    87.076190    86.896218    86.845798    86.795378  ...   \n",
      "353    87.076190    86.896218    86.845798    86.795378    86.744958  ...   \n",
      "354    86.896218    86.845798    86.795378    86.744958    86.694538  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   78.880345    0.000263   78.856069    0.000263   78.831793    0.000263   \n",
      "351   78.856069    0.000263   78.831793    0.000263   78.807516    0.000262   \n",
      "352   78.831793    0.000263   78.807516    0.000262   78.783240    0.000262   \n",
      "353   78.807516    0.000262   78.783240    0.000262   78.758964    0.000262   \n",
      "354   78.783240    0.000262   78.758964    0.000262   78.734687    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   78.807516    0.000262   78.783240    0.000262  \n",
      "351   78.783240    0.000262   78.758964    0.000262  \n",
      "352   78.758964    0.000262   78.734687    0.000262  \n",
      "353   78.734687    0.000262   78.710411    0.000262  \n",
      "354   78.710411    0.000262   78.686134    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1250, 1, 251) (1250, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "16/16 [==============================] - 2s 29ms/step - loss: 5424.5366 - val_loss: 4580.8262\n",
      "Epoch 2/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5362.1714 - val_loss: 4527.9771\n",
      "Epoch 3/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5314.3623 - val_loss: 4486.3662\n",
      "Epoch 4/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5269.3745 - val_loss: 4445.0703\n",
      "Epoch 5/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5224.7412 - val_loss: 4404.1240\n",
      "Epoch 6/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5180.4580 - val_loss: 4363.5044\n",
      "Epoch 7/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5136.5054 - val_loss: 4323.1914\n",
      "Epoch 8/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5092.8643 - val_loss: 4283.1743\n",
      "Epoch 9/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5049.5239 - val_loss: 4243.4448\n",
      "Epoch 10/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5006.4761 - val_loss: 4203.9961\n",
      "Epoch 11/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4963.7158 - val_loss: 4164.8232\n",
      "Epoch 12/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4921.2368 - val_loss: 4125.9233\n",
      "Epoch 13/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4879.0391 - val_loss: 4087.2942\n",
      "Epoch 14/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4837.1172 - val_loss: 4048.9314\n",
      "Epoch 15/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4795.4683 - val_loss: 4010.8337\n",
      "Epoch 16/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4754.0913 - val_loss: 3972.9990\n",
      "Epoch 17/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4712.9829 - val_loss: 3935.4246\n",
      "Epoch 18/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4672.1431 - val_loss: 3898.1089\n",
      "Epoch 19/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4631.5674 - val_loss: 3861.0508\n",
      "Epoch 20/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4591.2554 - val_loss: 3824.2478\n",
      "Epoch 21/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4551.2051 - val_loss: 3787.6987\n",
      "Epoch 22/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4511.4160 - val_loss: 3751.4016\n",
      "Epoch 23/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4471.8838 - val_loss: 3715.3555\n",
      "Epoch 24/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4432.6099 - val_loss: 3679.5586\n",
      "Epoch 25/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4393.5913 - val_loss: 3644.0090\n",
      "Epoch 26/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4354.8271 - val_loss: 3608.7063\n",
      "Epoch 27/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4316.3149 - val_loss: 3573.6479\n",
      "Epoch 28/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4278.0552 - val_loss: 3538.8342\n",
      "Epoch 29/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4240.0439 - val_loss: 3504.2625\n",
      "Epoch 30/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4202.2822 - val_loss: 3469.9307\n",
      "Epoch 31/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4164.7676 - val_loss: 3435.8394\n",
      "Epoch 32/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4127.4985 - val_loss: 3401.9868\n",
      "Epoch 33/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4090.4749 - val_loss: 3368.3711\n",
      "Epoch 34/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4053.6936 - val_loss: 3334.9910\n",
      "Epoch 35/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4017.1543 - val_loss: 3301.8452\n",
      "Epoch 36/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3980.8557 - val_loss: 3268.9331\n",
      "Epoch 37/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3944.7974 - val_loss: 3236.2529\n",
      "Epoch 38/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 3908.9768 - val_loss: 3203.8042\n",
      "Epoch 39/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3873.3933 - val_loss: 3171.5845\n",
      "Epoch 40/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3838.0452 - val_loss: 3139.5935\n",
      "Epoch 41/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3802.9326 - val_loss: 3107.8296\n",
      "Epoch 42/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3768.0527 - val_loss: 3076.2925\n",
      "Epoch 43/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3733.4055 - val_loss: 3044.9800\n",
      "Epoch 44/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3698.9888 - val_loss: 3013.8916\n",
      "Epoch 45/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3664.8022 - val_loss: 2983.0254\n",
      "Epoch 46/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3630.8452 - val_loss: 2952.3816\n",
      "Epoch 47/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3597.1150 - val_loss: 2921.9573\n",
      "Epoch 48/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3563.6113 - val_loss: 2891.7532\n",
      "Epoch 49/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3530.3330 - val_loss: 2861.7666\n",
      "Epoch 50/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3497.2788 - val_loss: 2831.9980\n",
      "Epoch 51/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3464.4480 - val_loss: 2802.4451\n",
      "Epoch 52/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3431.8391 - val_loss: 2773.1069\n",
      "Epoch 53/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3399.4509 - val_loss: 2743.9824\n",
      "Epoch 54/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3367.2825 - val_loss: 2715.0710\n",
      "Epoch 55/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3335.3328 - val_loss: 2686.3716\n",
      "Epoch 56/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 3303.6006 - val_loss: 2657.8821\n",
      "Epoch 57/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3272.0854 - val_loss: 2629.6025\n",
      "Epoch 58/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3240.7852 - val_loss: 2601.5315\n",
      "Epoch 59/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3209.7002 - val_loss: 2573.6682\n",
      "Epoch 60/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3178.8276 - val_loss: 2546.0117\n",
      "Epoch 61/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3148.1685 - val_loss: 2518.5603\n",
      "Epoch 62/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3117.7205 - val_loss: 2491.3130\n",
      "Epoch 63/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3087.4824 - val_loss: 2464.2698\n",
      "Epoch 64/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3057.4543 - val_loss: 2437.4287\n",
      "Epoch 65/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3027.6338 - val_loss: 2410.7903\n",
      "Epoch 66/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2998.0212 - val_loss: 2384.3511\n",
      "Epoch 67/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2968.6147 - val_loss: 2358.1116\n",
      "Epoch 68/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2939.4136 - val_loss: 2332.0708\n",
      "Epoch 69/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2910.4172 - val_loss: 2306.2280\n",
      "Epoch 70/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2881.6235 - val_loss: 2280.5813\n",
      "Epoch 71/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2853.0322 - val_loss: 2255.1299\n",
      "Epoch 72/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2824.6431 - val_loss: 2229.8740\n",
      "Epoch 73/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2796.4539 - val_loss: 2204.8115\n",
      "Epoch 74/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2768.4644 - val_loss: 2179.9414\n",
      "Epoch 75/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2740.6729 - val_loss: 2155.2629\n",
      "Epoch 76/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2713.0793 - val_loss: 2130.7756\n",
      "Epoch 77/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2685.6826 - val_loss: 2106.4792\n",
      "Epoch 78/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2658.4812 - val_loss: 2082.3716\n",
      "Epoch 79/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2631.4756 - val_loss: 2058.4521\n",
      "Epoch 80/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2604.6633 - val_loss: 2034.7190\n",
      "Epoch 81/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2578.0437 - val_loss: 2011.1726\n",
      "Epoch 82/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2551.6167 - val_loss: 1987.8124\n",
      "Epoch 83/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2525.3806 - val_loss: 1964.6355\n",
      "Epoch 84/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2499.3345 - val_loss: 1941.6429\n",
      "Epoch 85/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2473.4780 - val_loss: 1918.8329\n",
      "Epoch 86/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2447.8101 - val_loss: 1896.2046\n",
      "Epoch 87/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2422.3296 - val_loss: 1873.7581\n",
      "Epoch 88/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2397.0359 - val_loss: 1851.4907\n",
      "Epoch 89/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2371.9280 - val_loss: 1829.4028\n",
      "Epoch 90/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2347.0049 - val_loss: 1807.4929\n",
      "Epoch 91/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2322.2659 - val_loss: 1785.7612\n",
      "Epoch 92/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2297.7097 - val_loss: 1764.2065\n",
      "Epoch 93/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2273.3364 - val_loss: 1742.8268\n",
      "Epoch 94/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2249.1445 - val_loss: 1721.6223\n",
      "Epoch 95/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2225.1335 - val_loss: 1700.5924\n",
      "Epoch 96/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2201.3025 - val_loss: 1679.7355\n",
      "Epoch 97/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2177.6501 - val_loss: 1659.0508\n",
      "Epoch 98/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2154.1760 - val_loss: 1638.5376\n",
      "Epoch 99/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2130.8794 - val_loss: 1618.1958\n",
      "Epoch 100/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2107.7593 - val_loss: 1598.0238\n",
      "Epoch 101/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2084.8147 - val_loss: 1578.0208\n",
      "Epoch 102/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2062.0449 - val_loss: 1558.1863\n",
      "Epoch 103/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2039.4495 - val_loss: 1538.5194\n",
      "Epoch 104/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2017.0273 - val_loss: 1519.0193\n",
      "Epoch 105/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1994.7772 - val_loss: 1499.6844\n",
      "Epoch 106/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1972.6992 - val_loss: 1480.5153\n",
      "Epoch 107/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1950.7919 - val_loss: 1461.5109\n",
      "Epoch 108/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1929.0546 - val_loss: 1442.6699\n",
      "Epoch 109/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1907.4865 - val_loss: 1423.9907\n",
      "Epoch 110/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1886.0872 - val_loss: 1405.4746\n",
      "Epoch 111/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1864.8555 - val_loss: 1387.1190\n",
      "Epoch 112/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1843.7903 - val_loss: 1368.9241\n",
      "Epoch 113/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1822.8912 - val_loss: 1350.8887\n",
      "Epoch 114/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1802.1576 - val_loss: 1333.0123\n",
      "Epoch 115/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1781.5890 - val_loss: 1315.2938\n",
      "Epoch 116/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1761.1840 - val_loss: 1297.7330\n",
      "Epoch 117/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1740.9420 - val_loss: 1280.3282\n",
      "Epoch 118/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1720.8623 - val_loss: 1263.0800\n",
      "Epoch 119/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1700.9441 - val_loss: 1245.9857\n",
      "Epoch 120/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1681.1868 - val_loss: 1229.0464\n",
      "Epoch 121/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1661.5894 - val_loss: 1212.2603\n",
      "Epoch 122/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1642.1514 - val_loss: 1195.6267\n",
      "Epoch 123/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1622.8716 - val_loss: 1179.1460\n",
      "Epoch 124/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1603.7500 - val_loss: 1162.8159\n",
      "Epoch 125/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1584.7852 - val_loss: 1146.6365\n",
      "Epoch 126/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1565.9769 - val_loss: 1130.6073\n",
      "Epoch 127/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1547.3241 - val_loss: 1114.7267\n",
      "Epoch 128/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1528.8267 - val_loss: 1098.9946\n",
      "Epoch 129/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1510.4827 - val_loss: 1083.4100\n",
      "Epoch 130/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1492.2925 - val_loss: 1067.9722\n",
      "Epoch 131/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1474.2545 - val_loss: 1052.6802\n",
      "Epoch 132/500\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1456.3689 - val_loss: 1037.5344\n",
      "Epoch 133/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1438.6344 - val_loss: 1022.5327\n",
      "Epoch 134/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1421.0508 - val_loss: 1007.6750\n",
      "Epoch 135/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1403.6163 - val_loss: 992.9608\n",
      "Epoch 136/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1386.3317 - val_loss: 978.3891\n",
      "Epoch 137/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1369.1948 - val_loss: 963.9589\n",
      "Epoch 138/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1352.2062 - val_loss: 949.6699\n",
      "Epoch 139/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1335.3641 - val_loss: 935.5212\n",
      "Epoch 140/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1318.6687 - val_loss: 921.5126\n",
      "Epoch 141/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1302.1191 - val_loss: 907.6430\n",
      "Epoch 142/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1285.7141 - val_loss: 893.9116\n",
      "Epoch 143/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1269.4529 - val_loss: 880.3175\n",
      "Epoch 144/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1253.3361 - val_loss: 866.8606\n",
      "Epoch 145/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1237.3615 - val_loss: 853.5395\n",
      "Epoch 146/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1221.5293 - val_loss: 840.3539\n",
      "Epoch 147/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1205.8384 - val_loss: 827.3032\n",
      "Epoch 148/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1190.2881 - val_loss: 814.3870\n",
      "Epoch 149/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1174.8782 - val_loss: 801.6031\n",
      "Epoch 150/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1159.6075 - val_loss: 788.9525\n",
      "Epoch 151/500\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1144.4756 - val_loss: 776.4339\n",
      "Epoch 152/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1129.4813 - val_loss: 764.0462\n",
      "Epoch 153/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1114.6249 - val_loss: 751.7896\n",
      "Epoch 154/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1099.9052 - val_loss: 739.6624\n",
      "Epoch 155/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1085.3213 - val_loss: 727.6652\n",
      "Epoch 156/500\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1070.8729 - val_loss: 715.7952\n",
      "Epoch 157/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1056.5588 - val_loss: 704.0547\n",
      "Epoch 158/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1042.3790 - val_loss: 692.4399\n",
      "Epoch 159/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1028.3323 - val_loss: 680.9528\n",
      "Epoch 160/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1014.4186 - val_loss: 669.5908\n",
      "Epoch 161/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1000.6364 - val_loss: 658.3539\n",
      "Epoch 162/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 986.9859 - val_loss: 647.2422\n",
      "Epoch 163/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 973.4658 - val_loss: 636.2532\n",
      "Epoch 164/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 960.0759 - val_loss: 625.3879\n",
      "Epoch 165/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 946.8152 - val_loss: 614.6448\n",
      "Epoch 166/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 933.6829 - val_loss: 604.0226\n",
      "Epoch 167/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 920.6790 - val_loss: 593.5224\n",
      "Epoch 168/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 907.8025 - val_loss: 583.1432\n",
      "Epoch 169/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 895.0526 - val_loss: 572.8827\n",
      "Epoch 170/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 882.4286 - val_loss: 562.7413\n",
      "Epoch 171/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 869.9302 - val_loss: 552.7189\n",
      "Epoch 172/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 857.5566 - val_loss: 542.8140\n",
      "Epoch 173/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 845.3072 - val_loss: 533.0257\n",
      "Epoch 174/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 833.1811 - val_loss: 523.3541\n",
      "Epoch 175/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 821.1779 - val_loss: 513.7988\n",
      "Epoch 176/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 809.2969 - val_loss: 504.3573\n",
      "Epoch 177/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 797.5373 - val_loss: 495.0304\n",
      "Epoch 178/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 785.8983 - val_loss: 485.8173\n",
      "Epoch 179/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 774.3798 - val_loss: 476.7175\n",
      "Epoch 180/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 762.9808 - val_loss: 467.7301\n",
      "Epoch 181/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 751.7006 - val_loss: 458.8538\n",
      "Epoch 182/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 740.5384 - val_loss: 450.0889\n",
      "Epoch 183/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 729.4940 - val_loss: 441.4341\n",
      "Epoch 184/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 718.5665 - val_loss: 432.8893\n",
      "Epoch 185/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 707.7555 - val_loss: 424.4534\n",
      "Epoch 186/500\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 697.0601 - val_loss: 416.1252\n",
      "Epoch 187/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 686.4798 - val_loss: 407.9054\n",
      "Epoch 188/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 676.0134 - val_loss: 399.7916\n",
      "Epoch 189/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 665.6607 - val_loss: 391.7846\n",
      "Epoch 190/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 655.4211 - val_loss: 383.8831\n",
      "Epoch 191/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 645.2941 - val_loss: 376.0864\n",
      "Epoch 192/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 635.2788 - val_loss: 368.3947\n",
      "Epoch 193/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 625.3746 - val_loss: 360.8059\n",
      "Epoch 194/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 615.5810 - val_loss: 353.3202\n",
      "Epoch 195/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 605.8971 - val_loss: 345.9373\n",
      "Epoch 196/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 596.3226 - val_loss: 338.6559\n",
      "Epoch 197/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 586.8569 - val_loss: 331.4751\n",
      "Epoch 198/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 577.4989 - val_loss: 324.3949\n",
      "Epoch 199/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 568.2481 - val_loss: 317.4137\n",
      "Epoch 200/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 559.1039 - val_loss: 310.5315\n",
      "Epoch 201/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 550.0656 - val_loss: 303.7478\n",
      "Epoch 202/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 541.1326 - val_loss: 297.0620\n",
      "Epoch 203/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 532.3046 - val_loss: 290.4723\n",
      "Epoch 204/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 523.5806 - val_loss: 283.9800\n",
      "Epoch 205/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 514.9601 - val_loss: 277.5826\n",
      "Epoch 206/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 506.4420 - val_loss: 271.2799\n",
      "Epoch 207/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 498.0262 - val_loss: 265.0717\n",
      "Epoch 208/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 489.7120 - val_loss: 258.9569\n",
      "Epoch 209/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 481.4982 - val_loss: 252.9351\n",
      "Epoch 210/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 473.3850 - val_loss: 247.0050\n",
      "Epoch 211/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 465.3712 - val_loss: 241.1667\n",
      "Epoch 212/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 457.4560 - val_loss: 235.4194\n",
      "Epoch 213/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 449.6392 - val_loss: 229.7621\n",
      "Epoch 214/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 441.9202 - val_loss: 224.1945\n",
      "Epoch 215/500\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 434.2981 - val_loss: 218.7154\n",
      "Epoch 216/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 426.7721 - val_loss: 213.3247\n",
      "Epoch 217/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 419.3419 - val_loss: 208.0213\n",
      "Epoch 218/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 412.0068 - val_loss: 202.8049\n",
      "Epoch 219/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 404.7661 - val_loss: 197.6747\n",
      "Epoch 220/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 397.6192 - val_loss: 192.6290\n",
      "Epoch 221/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 390.5648 - val_loss: 187.6687\n",
      "Epoch 222/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 383.6031 - val_loss: 182.7921\n",
      "Epoch 223/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 376.7331 - val_loss: 177.9990\n",
      "Epoch 224/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 369.9543 - val_loss: 173.2884\n",
      "Epoch 225/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 363.2656 - val_loss: 168.6599\n",
      "Epoch 226/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 356.6668 - val_loss: 164.1126\n",
      "Epoch 227/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 350.1573 - val_loss: 159.6461\n",
      "Epoch 228/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 343.7365 - val_loss: 155.2593\n",
      "Epoch 229/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 337.4033 - val_loss: 150.9519\n",
      "Epoch 230/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 331.1573 - val_loss: 146.7230\n",
      "Epoch 231/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 324.9979 - val_loss: 142.5719\n",
      "Epoch 232/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 318.9244 - val_loss: 138.4980\n",
      "Epoch 233/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 312.9361 - val_loss: 134.5006\n",
      "Epoch 234/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 307.0323 - val_loss: 130.5788\n",
      "Epoch 235/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 301.2124 - val_loss: 126.7324\n",
      "Epoch 236/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 295.4760 - val_loss: 122.9601\n",
      "Epoch 237/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 289.8219 - val_loss: 119.2614\n",
      "Epoch 238/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 284.2499 - val_loss: 115.6362\n",
      "Epoch 239/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 278.7593 - val_loss: 112.0829\n",
      "Epoch 240/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 273.3492 - val_loss: 108.6015\n",
      "Epoch 241/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 268.0192 - val_loss: 105.1909\n",
      "Epoch 242/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 262.7686 - val_loss: 101.8505\n",
      "Epoch 243/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 257.5965 - val_loss: 98.5797\n",
      "Epoch 244/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 252.5024 - val_loss: 95.3777\n",
      "Epoch 245/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 247.4857 - val_loss: 92.2439\n",
      "Epoch 246/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 242.5458 - val_loss: 89.1774\n",
      "Epoch 247/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 237.6818 - val_loss: 86.1777\n",
      "Epoch 248/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 232.8932 - val_loss: 83.2442\n",
      "Epoch 249/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 228.1793 - val_loss: 80.3759\n",
      "Epoch 250/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 223.5394 - val_loss: 77.5723\n",
      "Epoch 251/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 218.9728 - val_loss: 74.8325\n",
      "Epoch 252/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 214.4789 - val_loss: 72.1562\n",
      "Epoch 253/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 210.0573 - val_loss: 69.5423\n",
      "Epoch 254/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 205.7070 - val_loss: 66.9902\n",
      "Epoch 255/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 201.4272 - val_loss: 64.4992\n",
      "Epoch 256/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 197.2178 - val_loss: 62.0690\n",
      "Epoch 257/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 193.0776 - val_loss: 59.6982\n",
      "Epoch 258/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 189.0063 - val_loss: 57.3866\n",
      "Epoch 259/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 185.0030 - val_loss: 55.1332\n",
      "Epoch 260/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 181.0670 - val_loss: 52.9374\n",
      "Epoch 261/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 177.1978 - val_loss: 50.7986\n",
      "Epoch 262/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 173.3947 - val_loss: 48.7160\n",
      "Epoch 263/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 169.6572 - val_loss: 46.6891\n",
      "Epoch 264/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 165.9843 - val_loss: 44.7170\n",
      "Epoch 265/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 162.3756 - val_loss: 42.7989\n",
      "Epoch 266/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 158.8302 - val_loss: 40.9342\n",
      "Epoch 267/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 155.3476 - val_loss: 39.1223\n",
      "Epoch 268/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 151.9270 - val_loss: 37.3625\n",
      "Epoch 269/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 148.5681 - val_loss: 35.6540\n",
      "Epoch 270/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 145.2699 - val_loss: 33.9961\n",
      "Epoch 271/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 142.0317 - val_loss: 32.3883\n",
      "Epoch 272/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 138.8532 - val_loss: 30.8296\n",
      "Epoch 273/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 135.7334 - val_loss: 29.3194\n",
      "Epoch 274/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 132.6717 - val_loss: 27.8572\n",
      "Epoch 275/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 129.6674 - val_loss: 26.4420\n",
      "Epoch 276/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 126.7201 - val_loss: 25.0733\n",
      "Epoch 277/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 123.8291 - val_loss: 23.7502\n",
      "Epoch 278/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 120.9934 - val_loss: 22.4724\n",
      "Epoch 279/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 118.2127 - val_loss: 21.2390\n",
      "Epoch 280/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 115.4861 - val_loss: 20.0491\n",
      "Epoch 281/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 112.8128 - val_loss: 18.9022\n",
      "Epoch 282/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 110.1926 - val_loss: 17.7978\n",
      "Epoch 283/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 107.6248 - val_loss: 16.7350\n",
      "Epoch 284/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 105.1085 - val_loss: 15.7130\n",
      "Epoch 285/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 102.6431 - val_loss: 14.7313\n",
      "Epoch 286/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 100.2279 - val_loss: 13.7894\n",
      "Epoch 287/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 97.8626 - val_loss: 12.8862\n",
      "Epoch 288/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 95.5462 - val_loss: 12.0213\n",
      "Epoch 289/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 93.2782 - val_loss: 11.1939\n",
      "Epoch 290/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 91.0578 - val_loss: 10.4034\n",
      "Epoch 291/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 88.8846 - val_loss: 9.6492\n",
      "Epoch 292/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 86.7580 - val_loss: 8.9305\n",
      "Epoch 293/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 84.6771 - val_loss: 8.2465\n",
      "Epoch 294/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 82.6412 - val_loss: 7.5969\n",
      "Epoch 295/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 80.6500 - val_loss: 6.9808\n",
      "Epoch 296/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 78.7025 - val_loss: 6.3976\n",
      "Epoch 297/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 76.7985 - val_loss: 5.8466\n",
      "Epoch 298/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 74.9369 - val_loss: 5.3272\n",
      "Epoch 299/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 73.1175 - val_loss: 4.8386\n",
      "Epoch 300/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 71.3394 - val_loss: 4.3804\n",
      "Epoch 301/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 69.6020 - val_loss: 3.9518\n",
      "Epoch 302/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 67.9048 - val_loss: 3.5523\n",
      "Epoch 303/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 66.2473 - val_loss: 3.1810\n",
      "Epoch 304/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 64.6286 - val_loss: 2.8374\n",
      "Epoch 305/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 63.0480 - val_loss: 2.5211\n",
      "Epoch 306/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 61.5054 - val_loss: 2.2311\n",
      "Epoch 307/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 59.9999 - val_loss: 1.9670\n",
      "Epoch 308/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 58.5309 - val_loss: 1.7281\n",
      "Epoch 309/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 57.0977 - val_loss: 1.5138\n",
      "Epoch 310/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 55.7000 - val_loss: 1.3236\n",
      "Epoch 311/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 54.3368 - val_loss: 1.1567\n",
      "Epoch 312/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 53.0079 - val_loss: 1.0126\n",
      "Epoch 313/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 51.7123 - val_loss: 0.8906\n",
      "Epoch 314/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 50.4497 - val_loss: 0.7903\n",
      "Epoch 315/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 49.2195 - val_loss: 0.7110\n",
      "Epoch 316/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 48.0212 - val_loss: 0.6521\n",
      "Epoch 317/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 46.8540 - val_loss: 0.6131\n",
      "Epoch 318/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 45.7176 - val_loss: 0.5933\n",
      "Epoch 319/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 44.6113 - val_loss: 0.5922\n",
      "Epoch 320/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 43.5345 - val_loss: 0.6092\n",
      "Epoch 321/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 42.4868 - val_loss: 0.6439\n",
      "Epoch 322/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 41.4674 - val_loss: 0.6955\n",
      "Epoch 323/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 40.4759 - val_loss: 0.7637\n",
      "Epoch 324/500\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 39.5118 - val_loss: 0.8477\n",
      "Epoch 325/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 38.5745 - val_loss: 0.9472\n",
      "Epoch 326/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 37.6635 - val_loss: 1.0615\n",
      "Epoch 327/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 36.7781 - val_loss: 1.1902\n",
      "Epoch 328/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 35.9180 - val_loss: 1.3327\n",
      "Epoch 329/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 35.0827 - val_loss: 1.4886\n",
      "Epoch 330/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 34.2715 - val_loss: 1.6572\n",
      "Epoch 331/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 33.4840 - val_loss: 1.8382\n",
      "Epoch 332/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 32.7198 - val_loss: 2.0310\n",
      "Epoch 333/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 31.9782 - val_loss: 2.2350\n",
      "Epoch 334/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 31.2589 - val_loss: 2.4500\n",
      "Epoch 335/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 30.5612 - val_loss: 2.6753\n",
      "Epoch 336/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 29.8846 - val_loss: 2.9106\n",
      "Epoch 337/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 29.2288 - val_loss: 3.1554\n",
      "Epoch 338/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 28.5934 - val_loss: 3.4091\n",
      "Epoch 339/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 27.9777 - val_loss: 3.6713\n",
      "Epoch 340/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 27.3814 - val_loss: 3.9417\n",
      "Epoch 341/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.8041 - val_loss: 4.2198\n",
      "Epoch 342/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.2450 - val_loss: 4.5052\n",
      "Epoch 343/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 25.7041 - val_loss: 4.7974\n",
      "Epoch 344/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 25.1806 - val_loss: 5.0961\n",
      "Epoch 345/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 24.6743 - val_loss: 5.4008\n",
      "Epoch 346/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 24.1847 - val_loss: 5.7111\n",
      "Epoch 347/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 23.7113 - val_loss: 6.0267\n",
      "Epoch 348/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 23.2539 - val_loss: 6.3472\n",
      "Epoch 349/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 22.8119 - val_loss: 6.6722\n",
      "Epoch 350/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 22.3849 - val_loss: 7.0013\n",
      "Epoch 351/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 21.9727 - val_loss: 7.3343\n",
      "Epoch 352/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 21.5747 - val_loss: 7.6707\n",
      "Epoch 353/500\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 21.1905 - val_loss: 8.0101\n",
      "Epoch 354/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 20.8199 - val_loss: 8.3525\n",
      "Epoch 355/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.4624 - val_loss: 8.6972\n",
      "Epoch 356/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 20.1177 - val_loss: 9.0441\n",
      "Epoch 357/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 19.7854 - val_loss: 9.3927\n",
      "Epoch 358/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 19.4652 - val_loss: 9.7429\n",
      "Epoch 359/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 19.1567 - val_loss: 10.0945\n",
      "Epoch 360/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 18.8595 - val_loss: 10.4469\n",
      "Epoch 361/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 18.5735 - val_loss: 10.8000\n",
      "Epoch 362/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 18.2981 - val_loss: 11.1535\n",
      "Epoch 363/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 18.0331 - val_loss: 11.5071\n",
      "Epoch 364/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.7782 - val_loss: 11.8606\n",
      "Epoch 365/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.5331 - val_loss: 12.2139\n",
      "Epoch 366/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.2975 - val_loss: 12.5664\n",
      "Epoch 367/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.0710 - val_loss: 12.9182\n",
      "Epoch 368/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 16.8534 - val_loss: 13.2688\n",
      "Epoch 369/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.6445 - val_loss: 13.6184\n",
      "Epoch 370/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.4438 - val_loss: 13.9664\n",
      "Epoch 371/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.2512 - val_loss: 14.3128\n",
      "Epoch 372/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.0664 - val_loss: 14.6573\n",
      "Epoch 373/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.8891 - val_loss: 14.9998\n",
      "Epoch 374/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.7191 - val_loss: 15.3401\n",
      "Epoch 375/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.5561 - val_loss: 15.6780\n",
      "Epoch 376/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.3999 - val_loss: 16.0133\n",
      "Epoch 377/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.2503 - val_loss: 16.3459\n",
      "Epoch 378/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.1070 - val_loss: 16.6757\n",
      "Epoch 379/500\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 14.9698 - val_loss: 17.0025\n",
      "Epoch 380/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 14.8385 - val_loss: 17.3262\n",
      "Epoch 381/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 14.7129 - val_loss: 17.6466\n",
      "Epoch 382/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 14.5927 - val_loss: 17.9637\n",
      "Epoch 383/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 14.4778 - val_loss: 18.2773\n",
      "Epoch 384/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 14.3680 - val_loss: 18.5872\n",
      "Epoch 385/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 14.2630 - val_loss: 18.8935\n",
      "Epoch 386/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 14.1627 - val_loss: 19.1961\n",
      "Epoch 387/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 14.0670 - val_loss: 19.4947\n",
      "Epoch 388/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 13.9756 - val_loss: 19.7896\n",
      "Epoch 389/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 13.8883 - val_loss: 20.0804\n",
      "Epoch 390/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 13.8051 - val_loss: 20.3672\n",
      "Epoch 391/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 13.7257 - val_loss: 20.6496\n",
      "Epoch 392/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 13.6500 - val_loss: 20.9279\n",
      "Epoch 393/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.5778 - val_loss: 21.2020\n",
      "Epoch 394/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.5091 - val_loss: 21.4715\n",
      "Epoch 395/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 13.4436 - val_loss: 21.7369\n",
      "Epoch 396/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 13.3813 - val_loss: 21.9978\n",
      "Epoch 397/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 13.3220 - val_loss: 22.2542\n",
      "Epoch 398/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.2655 - val_loss: 22.5063\n",
      "Epoch 399/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.2118 - val_loss: 22.7539\n",
      "Epoch 400/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.1607 - val_loss: 22.9969\n",
      "Epoch 401/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 13.1121 - val_loss: 23.2357\n",
      "Epoch 402/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.0659 - val_loss: 23.4697\n",
      "Epoch 403/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.0221 - val_loss: 23.6993\n",
      "Epoch 404/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 12.9804 - val_loss: 23.9244\n",
      "Epoch 405/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.9408 - val_loss: 24.1449\n",
      "Epoch 406/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.9032 - val_loss: 24.3610\n",
      "Epoch 407/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.8676 - val_loss: 24.5726\n",
      "Epoch 408/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.8337 - val_loss: 24.7797\n",
      "Epoch 409/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.8016 - val_loss: 24.9824\n",
      "Epoch 410/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.7711 - val_loss: 25.1807\n",
      "Epoch 411/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.7423 - val_loss: 25.3744\n",
      "Epoch 412/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.7150 - val_loss: 25.5640\n",
      "Epoch 413/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.6890 - val_loss: 25.7491\n",
      "Epoch 414/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.6645 - val_loss: 25.9299\n",
      "Epoch 415/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.6412 - val_loss: 26.1064\n",
      "Epoch 416/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.6192 - val_loss: 26.2785\n",
      "Epoch 417/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.5983 - val_loss: 26.4467\n",
      "Epoch 418/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.5786 - val_loss: 26.6107\n",
      "Epoch 419/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.5599 - val_loss: 26.7704\n",
      "Epoch 420/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.5423 - val_loss: 26.9263\n",
      "Epoch 421/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.5256 - val_loss: 27.0781\n",
      "Epoch 422/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.5098 - val_loss: 27.2258\n",
      "Epoch 423/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.4948 - val_loss: 27.3696\n",
      "Epoch 424/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.4807 - val_loss: 27.5097\n",
      "Epoch 425/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.4674 - val_loss: 27.6459\n",
      "Epoch 426/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.4548 - val_loss: 27.7783\n",
      "Epoch 427/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.4429 - val_loss: 27.9072\n",
      "Epoch 428/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.4316 - val_loss: 28.0323\n",
      "Epoch 429/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.4210 - val_loss: 28.1540\n",
      "Epoch 430/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.4110 - val_loss: 28.2719\n",
      "Epoch 431/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.4016 - val_loss: 28.3865\n",
      "Epoch 432/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3926 - val_loss: 28.4978\n",
      "Epoch 433/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3842 - val_loss: 28.6056\n",
      "Epoch 434/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3763 - val_loss: 28.7104\n",
      "Epoch 435/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3687 - val_loss: 28.8119\n",
      "Epoch 436/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 12.3617 - val_loss: 28.9103\n",
      "Epoch 437/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3550 - val_loss: 29.0057\n",
      "Epoch 438/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3487 - val_loss: 29.0979\n",
      "Epoch 439/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3427 - val_loss: 29.1872\n",
      "Epoch 440/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3372 - val_loss: 29.2738\n",
      "Epoch 441/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3319 - val_loss: 29.3573\n",
      "Epoch 442/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3269 - val_loss: 29.4381\n",
      "Epoch 443/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3222 - val_loss: 29.5163\n",
      "Epoch 444/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3178 - val_loss: 29.5917\n",
      "Epoch 445/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3137 - val_loss: 29.6649\n",
      "Epoch 446/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3097 - val_loss: 29.7354\n",
      "Epoch 447/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3060 - val_loss: 29.8034\n",
      "Epoch 448/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.3025 - val_loss: 29.8691\n",
      "Epoch 449/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.2992 - val_loss: 29.9324\n",
      "Epoch 450/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.2962 - val_loss: 29.9934\n",
      "Epoch 451/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.2932 - val_loss: 30.0524\n",
      "Epoch 452/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2905 - val_loss: 30.1091\n",
      "Epoch 453/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2880 - val_loss: 30.1639\n",
      "Epoch 454/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.2855 - val_loss: 30.2166\n",
      "Epoch 455/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2832 - val_loss: 30.2674\n",
      "Epoch 456/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2811 - val_loss: 30.3161\n",
      "Epoch 457/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2791 - val_loss: 30.3630\n",
      "Epoch 458/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.2772 - val_loss: 30.4083\n",
      "Epoch 459/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2754 - val_loss: 30.4516\n",
      "Epoch 460/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.2737 - val_loss: 30.4934\n",
      "Epoch 461/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.2721 - val_loss: 30.5334\n",
      "Epoch 462/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2707 - val_loss: 30.5720\n",
      "Epoch 463/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2692 - val_loss: 30.6091\n",
      "Epoch 464/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2679 - val_loss: 30.6445\n",
      "Epoch 465/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.2667 - val_loss: 30.6785\n",
      "Epoch 466/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 12.2655 - val_loss: 30.7113\n",
      "Epoch 467/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2645 - val_loss: 30.7426\n",
      "Epoch 468/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2635 - val_loss: 30.7727\n",
      "Epoch 469/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2625 - val_loss: 30.8014\n",
      "Epoch 470/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2616 - val_loss: 30.8288\n",
      "Epoch 471/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2608 - val_loss: 30.8553\n",
      "Epoch 472/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2600 - val_loss: 30.8806\n",
      "Epoch 473/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2592 - val_loss: 30.9045\n",
      "Epoch 474/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2586 - val_loss: 30.9277\n",
      "Epoch 475/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2580 - val_loss: 30.9497\n",
      "Epoch 476/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2574 - val_loss: 30.9708\n",
      "Epoch 477/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2568 - val_loss: 30.9911\n",
      "Epoch 478/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2563 - val_loss: 31.0104\n",
      "Epoch 479/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2558 - val_loss: 31.0286\n",
      "Epoch 480/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.2554 - val_loss: 31.0463\n",
      "Epoch 481/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2550 - val_loss: 31.0629\n",
      "Epoch 482/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.2546 - val_loss: 31.0790\n",
      "Epoch 483/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2543 - val_loss: 31.0943\n",
      "Epoch 484/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2539 - val_loss: 31.1087\n",
      "Epoch 485/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2536 - val_loss: 31.1224\n",
      "Epoch 486/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2534 - val_loss: 31.1354\n",
      "Epoch 487/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2532 - val_loss: 31.1479\n",
      "Epoch 488/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.2529 - val_loss: 31.1598\n",
      "Epoch 489/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2527 - val_loss: 31.1711\n",
      "Epoch 490/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2525 - val_loss: 31.1818\n",
      "Epoch 491/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.2524 - val_loss: 31.1920\n",
      "Epoch 492/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.2522 - val_loss: 31.2015\n",
      "Epoch 493/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.2521 - val_loss: 31.2109\n",
      "Epoch 494/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.2520 - val_loss: 31.2197\n",
      "Epoch 495/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2519 - val_loss: 31.2280\n",
      "Epoch 496/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 12.2518 - val_loss: 31.2356\n",
      "Epoch 497/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2518 - val_loss: 31.2432\n",
      "Epoch 498/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 12.2517 - val_loss: 31.2501\n",
      "Epoch 499/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.2517 - val_loss: 31.2568\n",
      "Epoch 500/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.2517 - val_loss: 31.2632\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6856, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0008, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0128, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(-2.3003, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 343ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.01427871e+01, 7.01343837e+01, 7.01259804e+01, 7.01175770e+01,\n",
       "        7.01091737e+01, 7.01007703e+01, 7.00923669e+01, 7.00839636e+01,\n",
       "        7.00755602e+01, 7.00671569e+01, 7.00587535e+01, 7.00503501e+01,\n",
       "        7.00419468e+01, 7.00335434e+01, 7.00251401e+01, 7.00167367e+01,\n",
       "        7.00083333e+01, 6.99999300e+01, 6.99915266e+01, 6.99831233e+01,\n",
       "        6.99747199e+01, 6.99663165e+01, 6.99579132e+01, 6.99495098e+01,\n",
       "        6.99411064e+01, 6.99327031e+01, 6.99242997e+01, 6.99158964e+01,\n",
       "        6.99074930e+01, 6.98990896e+01, 6.98906863e+01, 6.98822829e+01,\n",
       "        6.98738795e+01, 6.98654762e+01, 6.98570728e+01, 6.98486695e+01,\n",
       "        6.98402661e+01, 6.98318627e+01, 6.98234594e+01, 6.98150560e+01,\n",
       "        6.98066527e+01, 6.97883287e+01, 6.97323063e+01, 6.96762839e+01,\n",
       "        6.96202614e+01, 6.95642390e+01, 6.95082166e+01, 6.94521942e+01,\n",
       "        6.93961718e+01, 6.93401494e+01, 6.92841270e+01, 6.92281046e+01,\n",
       "        6.91720822e+01, 6.91160598e+01, 6.90600374e+01, 6.90040149e+01,\n",
       "        6.89479925e+01, 6.88919701e+01, 6.88359477e+01, 6.87799253e+01,\n",
       "        6.87239029e+01, 6.86678805e+01, 6.86118581e+01, 6.85558357e+01,\n",
       "        6.84998133e+01, 6.84437908e+01, 6.83877684e+01, 6.83317460e+01,\n",
       "        6.82757236e+01, 6.82197012e+01, 6.81636788e+01, 6.81076564e+01,\n",
       "        6.80516340e+01, 6.79956116e+01, 6.79395892e+01, 6.78835668e+01,\n",
       "        6.78275443e+01, 6.77847642e+01, 6.77547922e+01, 6.77248203e+01,\n",
       "        7.49386597e+01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.26760343e-01, 4.88929823e-02, 5.72302759e-01,\n",
       "        1.96406811e-01, 3.88379961e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.19723994e-01, 0.00000000e+00,\n",
       "        3.58016789e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([65.6493254 , 65.63877451, 65.62822362, 65.61767274, 65.60712185,\n",
       "       65.59726891, 65.58886555, 65.58046218, 65.57205882, 65.56365546,\n",
       "       65.5552521 , 65.54684874, 65.53844538, 65.53004202, 65.52163866,\n",
       "       65.51323529, 65.50483193, 65.49642857, 65.48802521, 65.47962185,\n",
       "       65.47121849, 65.46281513, 65.45441176, 65.4460084 , 65.43760504,\n",
       "       65.42920168, 65.42079832, 65.41239496, 65.4039916 , 65.39558824,\n",
       "       65.38718487, 65.37878151, 65.37037815, 65.36197479, 65.35357143,\n",
       "       65.34516807, 65.33676471, 65.32836134, 65.31995798, 65.31155462,\n",
       "       65.30315126, 65.2947479 , 65.28634454, 65.27794118, 65.26953782,\n",
       "       65.26113445, 65.25273109, 65.24432773, 65.23592437, 65.22752101,\n",
       "       65.21911765, 65.21071429, 65.20231092, 65.19390756, 65.1855042 ,\n",
       "       65.17710084, 65.16869748, 65.16029412, 65.15189076, 65.14348739,\n",
       "       65.13508403, 65.12668067, 65.11827731, 65.10987395, 65.10147059,\n",
       "       65.09306723, 65.08466387, 65.0762605 , 65.06785714, 65.05945378,\n",
       "       65.05105042, 65.04264706, 65.0342437 , 65.02584034, 65.01743697,\n",
       "       65.00903361, 65.00063025, 64.99222689, 64.98382353, 64.97542017,\n",
       "       64.96701681, 64.95861345, 64.95021008, 64.94180672, 64.93340336,\n",
       "       64.925     , 64.91659664, 64.90819328, 64.89978992, 64.89138655,\n",
       "       64.88298319, 64.87457983, 64.86617647, 64.85777311, 64.84936975,\n",
       "       64.84096639, 64.83256303, 64.82415966, 64.8157563 , 64.80735294])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.488254443693464\n",
      "15.653344878790268\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
