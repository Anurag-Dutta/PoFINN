{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "2395    56.278951\n",
       "2396    56.266653\n",
       "2397    56.254355\n",
       "2398    56.242058\n",
       "2399    56.229760\n",
       "Name: C4, Length: 2400, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c4_interpolated_2300_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "2295     0.482198\n",
       "2296     0.000000\n",
       "2297     0.000000\n",
       "2298     0.000000\n",
       "2299     0.000000\n",
       "Name: C4, Length: 2300, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2300)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.409524</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.219048</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.028571</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.838095</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.600000  0.000298\n",
       "1     88.409524  0.000298\n",
       "2     88.219048  0.000297\n",
       "3     88.028571  0.000297\n",
       "4     87.838095  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6856  # Value for C0\n",
    "K0 = -0.0008  # Value for K0\n",
    "K1 = -0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0128    # Value for b\n",
    "c = -2.3003    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.600000    88.409524    88.219048    88.028571    87.838095   \n",
      "351    88.409524    88.219048    88.028571    87.838095    87.647619   \n",
      "352    88.219048    88.028571    87.838095    87.647619    87.457143   \n",
      "353    88.028571    87.838095    87.647619    87.457143    87.266667   \n",
      "354    87.838095    87.647619    87.457143    87.266667    87.076190   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.647619    87.457143    87.266667    87.076190    86.896218  ...   \n",
      "351    87.457143    87.266667    87.076190    86.896218    86.845798  ...   \n",
      "352    87.266667    87.076190    86.896218    86.845798    86.795378  ...   \n",
      "353    87.076190    86.896218    86.845798    86.795378    86.744958  ...   \n",
      "354    86.896218    86.845798    86.795378    86.744958    86.694538  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   78.880345    0.000263   78.856069    0.000263   78.831793    0.000263   \n",
      "351   78.856069    0.000263   78.831793    0.000263   78.807516    0.000262   \n",
      "352   78.831793    0.000263   78.807516    0.000262   78.783240    0.000262   \n",
      "353   78.807516    0.000262   78.783240    0.000262   78.758964    0.000262   \n",
      "354   78.783240    0.000262   78.758964    0.000262   78.734687    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   78.807516    0.000262   78.783240    0.000262  \n",
      "351   78.783240    0.000262   78.758964    0.000262  \n",
      "352   78.758964    0.000262   78.734687    0.000262  \n",
      "353   78.734687    0.000262   78.710411    0.000262  \n",
      "354   78.710411    0.000262   78.686134    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1850, 1, 251) (1850, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "24/24 [==============================] - 3s 37ms/step - loss: 5130.6177 - val_loss: 3194.6213\n",
      "Epoch 2/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 4847.2461 - val_loss: 3021.7754\n",
      "Epoch 3/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 4723.2461 - val_loss: 2956.0310\n",
      "Epoch 4/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 4625.7114 - val_loss: 2893.5710\n",
      "Epoch 5/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 4531.7710 - val_loss: 2832.7031\n",
      "Epoch 6/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 4440.5195 - val_loss: 2775.8035\n",
      "Epoch 7/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 4351.4961 - val_loss: 2720.4587\n",
      "Epoch 8/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 4264.3545 - val_loss: 2666.5088\n",
      "Epoch 9/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 4178.9121 - val_loss: 2613.8562\n",
      "Epoch 10/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 4095.0522 - val_loss: 2562.4346\n",
      "Epoch 11/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 4012.6899 - val_loss: 2512.1929\n",
      "Epoch 12/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3931.7639 - val_loss: 2463.0933\n",
      "Epoch 13/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3852.2256 - val_loss: 2415.1033\n",
      "Epoch 14/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3774.0349 - val_loss: 2368.1951\n",
      "Epoch 15/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 3697.1582 - val_loss: 2322.3445\n",
      "Epoch 16/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 3621.5662 - val_loss: 2277.5308\n",
      "Epoch 17/500\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 3547.2324 - val_loss: 2233.7341\n",
      "Epoch 18/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3474.1340 - val_loss: 2190.9360\n",
      "Epoch 19/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3402.2510 - val_loss: 2149.1201\n",
      "Epoch 20/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3331.5610 - val_loss: 2108.2705\n",
      "Epoch 21/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3262.0479 - val_loss: 2068.3713\n",
      "Epoch 22/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3193.6919 - val_loss: 2029.4077\n",
      "Epoch 23/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3126.4771 - val_loss: 1991.3663\n",
      "Epoch 24/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3060.3884 - val_loss: 1954.2334\n",
      "Epoch 25/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2995.4087 - val_loss: 1917.9955\n",
      "Epoch 26/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2931.5244 - val_loss: 1882.6399\n",
      "Epoch 27/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2868.7202 - val_loss: 1848.1536\n",
      "Epoch 28/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2806.9824 - val_loss: 1814.5247\n",
      "Epoch 29/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2746.2969 - val_loss: 1781.7408\n",
      "Epoch 30/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2686.6516 - val_loss: 1749.7902\n",
      "Epoch 31/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2628.0317 - val_loss: 1718.6611\n",
      "Epoch 32/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2570.4246 - val_loss: 1688.3425\n",
      "Epoch 33/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 2513.8193 - val_loss: 1658.8226\n",
      "Epoch 34/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 2458.2017 - val_loss: 1630.0898\n",
      "Epoch 35/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 2403.5605 - val_loss: 1602.1337\n",
      "Epoch 36/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2349.8831 - val_loss: 1574.9436\n",
      "Epoch 37/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2297.1584 - val_loss: 1548.5078\n",
      "Epoch 38/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2245.3740 - val_loss: 1522.8169\n",
      "Epoch 39/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 2194.5193 - val_loss: 1497.8593\n",
      "Epoch 40/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 2144.5815 - val_loss: 1473.6250\n",
      "Epoch 41/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 2095.5513 - val_loss: 1450.1038\n",
      "Epoch 42/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2047.4163 - val_loss: 1427.2852\n",
      "Epoch 43/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2000.1660 - val_loss: 1405.1591\n",
      "Epoch 44/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1953.7892 - val_loss: 1383.7157\n",
      "Epoch 45/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1908.2754 - val_loss: 1362.9447\n",
      "Epoch 46/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1863.6145 - val_loss: 1342.8364\n",
      "Epoch 47/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1819.7949 - val_loss: 1323.3810\n",
      "Epoch 48/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1776.8073 - val_loss: 1304.5685\n",
      "Epoch 49/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1734.6404 - val_loss: 1286.3896\n",
      "Epoch 50/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1693.2847 - val_loss: 1268.8348\n",
      "Epoch 51/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1652.7299 - val_loss: 1251.8939\n",
      "Epoch 52/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1612.9662 - val_loss: 1235.5581\n",
      "Epoch 53/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1573.9834 - val_loss: 1219.8177\n",
      "Epoch 54/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1535.7711 - val_loss: 1204.6635\n",
      "Epoch 55/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1498.3204 - val_loss: 1190.0858\n",
      "Epoch 56/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1461.6207 - val_loss: 1176.0760\n",
      "Epoch 57/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1425.6636 - val_loss: 1162.6246\n",
      "Epoch 58/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1390.4386 - val_loss: 1149.7224\n",
      "Epoch 59/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1355.9360 - val_loss: 1137.3606\n",
      "Epoch 60/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1322.1471 - val_loss: 1125.5300\n",
      "Epoch 61/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 1289.0623 - val_loss: 1114.2217\n",
      "Epoch 62/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 1256.6720 - val_loss: 1103.4265\n",
      "Epoch 63/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 1224.9675 - val_loss: 1093.1359\n",
      "Epoch 64/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1193.9388 - val_loss: 1083.3409\n",
      "Epoch 65/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1163.5780 - val_loss: 1074.0327\n",
      "Epoch 66/500\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 1133.8750 - val_loss: 1065.2029\n",
      "Epoch 67/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1104.8209 - val_loss: 1056.8423\n",
      "Epoch 68/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1076.4077 - val_loss: 1048.9424\n",
      "Epoch 69/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1048.6255 - val_loss: 1041.4946\n",
      "Epoch 70/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1021.4659 - val_loss: 1034.4906\n",
      "Epoch 71/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 994.9201 - val_loss: 1027.9213\n",
      "Epoch 72/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 968.9791 - val_loss: 1021.7786\n",
      "Epoch 73/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 943.6349 - val_loss: 1016.0541\n",
      "Epoch 74/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 918.8782 - val_loss: 1010.7392\n",
      "Epoch 75/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 894.7007 - val_loss: 1005.8255\n",
      "Epoch 76/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 871.0939 - val_loss: 1001.3047\n",
      "Epoch 77/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 848.0494 - val_loss: 997.1686\n",
      "Epoch 78/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 825.5585 - val_loss: 993.4086\n",
      "Epoch 79/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 803.6131 - val_loss: 990.0169\n",
      "Epoch 80/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 782.2047 - val_loss: 986.9848\n",
      "Epoch 81/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 761.3253 - val_loss: 984.3045\n",
      "Epoch 82/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 740.9661 - val_loss: 981.9677\n",
      "Epoch 83/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 721.1193 - val_loss: 979.9665\n",
      "Epoch 84/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 701.7766 - val_loss: 978.2925\n",
      "Epoch 85/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 682.9301 - val_loss: 976.9381\n",
      "Epoch 86/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 664.5714 - val_loss: 975.8948\n",
      "Epoch 87/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 646.6928 - val_loss: 975.1551\n",
      "Epoch 88/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 629.2860 - val_loss: 974.7110\n",
      "Epoch 89/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 612.3433 - val_loss: 974.5544\n",
      "Epoch 90/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 595.8568 - val_loss: 974.6777\n",
      "Epoch 91/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 579.8184 - val_loss: 975.0729\n",
      "Epoch 92/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 564.2205 - val_loss: 975.7326\n",
      "Epoch 93/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 549.0554 - val_loss: 976.6486\n",
      "Epoch 94/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 534.3153 - val_loss: 977.8135\n",
      "Epoch 95/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 519.9925 - val_loss: 979.2198\n",
      "Epoch 96/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 506.0792 - val_loss: 980.8596\n",
      "Epoch 97/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 492.5681 - val_loss: 982.7255\n",
      "Epoch 98/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 479.4515 - val_loss: 984.8099\n",
      "Epoch 99/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 466.7220 - val_loss: 987.1056\n",
      "Epoch 100/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 454.3719 - val_loss: 989.6050\n",
      "Epoch 101/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 442.3941 - val_loss: 992.3007\n",
      "Epoch 102/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 430.7812 - val_loss: 995.1857\n",
      "Epoch 103/500\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 419.5257 - val_loss: 998.2523\n",
      "Epoch 104/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 408.6205 - val_loss: 1001.4937\n",
      "Epoch 105/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 398.0581 - val_loss: 1004.9025\n",
      "Epoch 106/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 387.8317 - val_loss: 1008.4717\n",
      "Epoch 107/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 377.9341 - val_loss: 1012.1943\n",
      "Epoch 108/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 368.3581 - val_loss: 1016.0636\n",
      "Epoch 109/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 359.0966 - val_loss: 1020.0721\n",
      "Epoch 110/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 350.1431 - val_loss: 1024.2133\n",
      "Epoch 111/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 341.4904 - val_loss: 1028.4803\n",
      "Epoch 112/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 333.1316 - val_loss: 1032.8668\n",
      "Epoch 113/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 325.0600 - val_loss: 1037.3656\n",
      "Epoch 114/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 317.2689 - val_loss: 1041.9706\n",
      "Epoch 115/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 309.7515 - val_loss: 1046.6749\n",
      "Epoch 116/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 302.5013 - val_loss: 1051.4725\n",
      "Epoch 117/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 295.5117 - val_loss: 1056.3568\n",
      "Epoch 118/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 288.7763 - val_loss: 1061.3217\n",
      "Epoch 119/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 282.2886 - val_loss: 1066.3608\n",
      "Epoch 120/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 276.0423 - val_loss: 1071.4686\n",
      "Epoch 121/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 270.0311 - val_loss: 1076.6387\n",
      "Epoch 122/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 264.2489 - val_loss: 1081.8649\n",
      "Epoch 123/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 258.6892 - val_loss: 1087.1417\n",
      "Epoch 124/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 253.3463 - val_loss: 1092.4636\n",
      "Epoch 125/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 248.2141 - val_loss: 1097.8247\n",
      "Epoch 126/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 243.2865 - val_loss: 1103.2196\n",
      "Epoch 127/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 238.5579 - val_loss: 1108.6431\n",
      "Epoch 128/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 234.0222 - val_loss: 1114.0895\n",
      "Epoch 129/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 229.6740 - val_loss: 1119.5538\n",
      "Epoch 130/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 225.5076 - val_loss: 1125.0311\n",
      "Epoch 131/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 221.5174 - val_loss: 1130.5161\n",
      "Epoch 132/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 217.6979 - val_loss: 1136.0040\n",
      "Epoch 133/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 214.0439 - val_loss: 1141.4906\n",
      "Epoch 134/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 210.5502 - val_loss: 1146.9701\n",
      "Epoch 135/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 207.2114 - val_loss: 1152.4396\n",
      "Epoch 136/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 204.0224 - val_loss: 1157.8932\n",
      "Epoch 137/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 200.9782 - val_loss: 1163.3279\n",
      "Epoch 138/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 198.0739 - val_loss: 1168.7385\n",
      "Epoch 139/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 195.3046 - val_loss: 1174.1222\n",
      "Epoch 140/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 192.6656 - val_loss: 1179.4736\n",
      "Epoch 141/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 190.1525 - val_loss: 1184.7903\n",
      "Epoch 142/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 187.7603 - val_loss: 1190.0679\n",
      "Epoch 143/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 185.4848 - val_loss: 1195.3036\n",
      "Epoch 144/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 183.3213 - val_loss: 1200.4938\n",
      "Epoch 145/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 181.2660 - val_loss: 1205.6349\n",
      "Epoch 146/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 179.3145 - val_loss: 1210.7242\n",
      "Epoch 147/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 177.4628 - val_loss: 1215.7587\n",
      "Epoch 148/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 175.7068 - val_loss: 1220.7358\n",
      "Epoch 149/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 174.0427 - val_loss: 1225.6526\n",
      "Epoch 150/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 172.4667 - val_loss: 1230.5067\n",
      "Epoch 151/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 170.9751 - val_loss: 1235.2961\n",
      "Epoch 152/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 169.5643 - val_loss: 1240.0178\n",
      "Epoch 153/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 168.2308 - val_loss: 1244.6707\n",
      "Epoch 154/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 166.9713 - val_loss: 1249.2517\n",
      "Epoch 155/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 165.7824 - val_loss: 1253.7600\n",
      "Epoch 156/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 164.6609 - val_loss: 1258.1931\n",
      "Epoch 157/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 163.6039 - val_loss: 1262.5498\n",
      "Epoch 158/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 162.6082 - val_loss: 1266.8291\n",
      "Epoch 159/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 161.6709 - val_loss: 1271.0292\n",
      "Epoch 160/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 160.7893 - val_loss: 1275.1495\n",
      "Epoch 161/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 159.9606 - val_loss: 1279.1888\n",
      "Epoch 162/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 159.1822 - val_loss: 1283.1455\n",
      "Epoch 163/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 158.4516 - val_loss: 1287.0199\n",
      "Epoch 164/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 157.7662 - val_loss: 1290.8107\n",
      "Epoch 165/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 157.1239 - val_loss: 1294.5177\n",
      "Epoch 166/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 156.5223 - val_loss: 1298.1407\n",
      "Epoch 167/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 155.9593 - val_loss: 1301.6793\n",
      "Epoch 168/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 155.4326 - val_loss: 1305.1328\n",
      "Epoch 169/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 154.9406 - val_loss: 1308.5015\n",
      "Epoch 170/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 154.4810 - val_loss: 1311.7853\n",
      "Epoch 171/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 154.0523 - val_loss: 1314.9845\n",
      "Epoch 172/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 153.6525 - val_loss: 1318.0999\n",
      "Epoch 173/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 153.2799 - val_loss: 1321.1313\n",
      "Epoch 174/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 152.9330 - val_loss: 1324.0793\n",
      "Epoch 175/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 152.6103 - val_loss: 1326.9442\n",
      "Epoch 176/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 152.3103 - val_loss: 1329.7268\n",
      "Epoch 177/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 152.0315 - val_loss: 1332.4281\n",
      "Epoch 178/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 151.7727 - val_loss: 1335.0485\n",
      "Epoch 179/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 151.5327 - val_loss: 1337.5886\n",
      "Epoch 180/500\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 151.3102 - val_loss: 1340.0500\n",
      "Epoch 181/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 151.1040 - val_loss: 1342.4337\n",
      "Epoch 182/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 150.9133 - val_loss: 1344.7406\n",
      "Epoch 183/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 150.7368 - val_loss: 1346.9711\n",
      "Epoch 184/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 150.5736 - val_loss: 1349.1273\n",
      "Epoch 185/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 150.4231 - val_loss: 1351.2103\n",
      "Epoch 186/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 150.2840 - val_loss: 1353.2214\n",
      "Epoch 187/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 150.1557 - val_loss: 1355.1615\n",
      "Epoch 188/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 150.0375 - val_loss: 1357.0321\n",
      "Epoch 189/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 149.9286 - val_loss: 1358.8347\n",
      "Epoch 190/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 149.8284 - val_loss: 1360.5713\n",
      "Epoch 191/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 149.7362 - val_loss: 1362.2415\n",
      "Epoch 192/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 149.6515 - val_loss: 1363.8486\n",
      "Epoch 193/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 149.5737 - val_loss: 1365.3927\n",
      "Epoch 194/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 149.5023 - val_loss: 1366.8776\n",
      "Epoch 195/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 149.4367 - val_loss: 1368.3029\n",
      "Epoch 196/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 149.3765 - val_loss: 1369.6705\n",
      "Epoch 197/500\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 149.3214 - val_loss: 1370.9818\n",
      "Epoch 198/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 149.2710 - val_loss: 1372.2389\n",
      "Epoch 199/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 149.2248 - val_loss: 1373.4432\n",
      "Epoch 200/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 149.1825 - val_loss: 1374.5955\n",
      "Epoch 201/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 149.1439 - val_loss: 1375.6978\n",
      "Epoch 202/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 149.1087 - val_loss: 1376.7521\n",
      "Epoch 203/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 149.0764 - val_loss: 1377.7598\n",
      "Epoch 204/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 149.0470 - val_loss: 1378.7223\n",
      "Epoch 205/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 149.0201 - val_loss: 1379.6406\n",
      "Epoch 206/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.9957 - val_loss: 1380.5160\n",
      "Epoch 207/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.9733 - val_loss: 1381.3511\n",
      "Epoch 208/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.9530 - val_loss: 1382.1469\n",
      "Epoch 209/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.9346 - val_loss: 1382.9044\n",
      "Epoch 210/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.9178 - val_loss: 1383.6257\n",
      "Epoch 211/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.9024 - val_loss: 1384.3115\n",
      "Epoch 212/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8885 - val_loss: 1384.9631\n",
      "Epoch 213/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8759 - val_loss: 1385.5826\n",
      "Epoch 214/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8644 - val_loss: 1386.1700\n",
      "Epoch 215/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8540 - val_loss: 1386.7277\n",
      "Epoch 216/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8446 - val_loss: 1387.2565\n",
      "Epoch 217/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8361 - val_loss: 1387.7577\n",
      "Epoch 218/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8284 - val_loss: 1388.2332\n",
      "Epoch 219/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8214 - val_loss: 1388.6827\n",
      "Epoch 220/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8150 - val_loss: 1389.1079\n",
      "Epoch 221/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8093 - val_loss: 1389.5094\n",
      "Epoch 222/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8041 - val_loss: 1389.8887\n",
      "Epoch 223/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7995 - val_loss: 1390.2474\n",
      "Epoch 224/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7954 - val_loss: 1390.5864\n",
      "Epoch 225/500\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 148.7917 - val_loss: 1390.9056\n",
      "Epoch 226/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 148.7883 - val_loss: 1391.2067\n",
      "Epoch 227/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7853 - val_loss: 1391.4904\n",
      "Epoch 228/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7827 - val_loss: 1391.7576\n",
      "Epoch 229/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7803 - val_loss: 1392.0089\n",
      "Epoch 230/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7781 - val_loss: 1392.2452\n",
      "Epoch 231/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7764 - val_loss: 1392.4678\n",
      "Epoch 232/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7747 - val_loss: 1392.6760\n",
      "Epoch 233/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7733 - val_loss: 1392.8728\n",
      "Epoch 234/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7720 - val_loss: 1393.0565\n",
      "Epoch 235/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 148.7710 - val_loss: 1393.2295\n",
      "Epoch 236/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7701 - val_loss: 1393.3910\n",
      "Epoch 237/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7694 - val_loss: 1393.5428\n",
      "Epoch 238/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7687 - val_loss: 1393.6849\n",
      "Epoch 239/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7682 - val_loss: 1393.8180\n",
      "Epoch 240/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7678 - val_loss: 1393.9426\n",
      "Epoch 241/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7675 - val_loss: 1394.0586\n",
      "Epoch 242/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7673 - val_loss: 1394.1671\n",
      "Epoch 243/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7672 - val_loss: 1394.2684\n",
      "Epoch 244/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7672 - val_loss: 1394.3627\n",
      "Epoch 245/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7672 - val_loss: 1394.4510\n",
      "Epoch 246/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7672 - val_loss: 1394.5327\n",
      "Epoch 247/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7674 - val_loss: 1394.6099\n",
      "Epoch 248/500\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 148.7676 - val_loss: 1394.6797\n",
      "Epoch 249/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7679 - val_loss: 1394.7463\n",
      "Epoch 250/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7682 - val_loss: 1394.8080\n",
      "Epoch 251/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7686 - val_loss: 1394.8652\n",
      "Epoch 252/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7689 - val_loss: 1394.9177\n",
      "Epoch 253/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7693 - val_loss: 1394.9673\n",
      "Epoch 254/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7698 - val_loss: 1395.0131\n",
      "Epoch 255/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7703 - val_loss: 1395.0553\n",
      "Epoch 256/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7707 - val_loss: 1395.0939\n",
      "Epoch 257/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7713 - val_loss: 1395.1305\n",
      "Epoch 258/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7718 - val_loss: 1395.1641\n",
      "Epoch 259/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7723 - val_loss: 1395.1949\n",
      "Epoch 260/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7729 - val_loss: 1395.2239\n",
      "Epoch 261/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7735 - val_loss: 1395.2498\n",
      "Epoch 262/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7741 - val_loss: 1395.2740\n",
      "Epoch 263/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7747 - val_loss: 1395.2966\n",
      "Epoch 264/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7753 - val_loss: 1395.3179\n",
      "Epoch 265/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7759 - val_loss: 1395.3362\n",
      "Epoch 266/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7765 - val_loss: 1395.3540\n",
      "Epoch 267/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7771 - val_loss: 1395.3699\n",
      "Epoch 268/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7778 - val_loss: 1395.3850\n",
      "Epoch 269/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7784 - val_loss: 1395.3979\n",
      "Epoch 270/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7791 - val_loss: 1395.4106\n",
      "Epoch 271/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7797 - val_loss: 1395.4219\n",
      "Epoch 272/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7803 - val_loss: 1395.4316\n",
      "Epoch 273/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7810 - val_loss: 1395.4414\n",
      "Epoch 274/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7816 - val_loss: 1395.4500\n",
      "Epoch 275/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 148.7822 - val_loss: 1395.4576\n",
      "Epoch 276/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7828 - val_loss: 1395.4642\n",
      "Epoch 277/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7835 - val_loss: 1395.4701\n",
      "Epoch 278/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7841 - val_loss: 1395.4761\n",
      "Epoch 279/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7847 - val_loss: 1395.4810\n",
      "Epoch 280/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7853 - val_loss: 1395.4858\n",
      "Epoch 281/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7859 - val_loss: 1395.4893\n",
      "Epoch 282/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7866 - val_loss: 1395.4930\n",
      "Epoch 283/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7872 - val_loss: 1395.4969\n",
      "Epoch 284/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7878 - val_loss: 1395.5001\n",
      "Epoch 285/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7883 - val_loss: 1395.5026\n",
      "Epoch 286/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7889 - val_loss: 1395.5048\n",
      "Epoch 287/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7895 - val_loss: 1395.5068\n",
      "Epoch 288/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7901 - val_loss: 1395.5087\n",
      "Epoch 289/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7906 - val_loss: 1395.5098\n",
      "Epoch 290/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7912 - val_loss: 1395.5109\n",
      "Epoch 291/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7917 - val_loss: 1395.5125\n",
      "Epoch 292/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7922 - val_loss: 1395.5131\n",
      "Epoch 293/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7928 - val_loss: 1395.5133\n",
      "Epoch 294/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7934 - val_loss: 1395.5142\n",
      "Epoch 295/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7939 - val_loss: 1395.5153\n",
      "Epoch 296/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7943 - val_loss: 1395.5154\n",
      "Epoch 297/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7948 - val_loss: 1395.5154\n",
      "Epoch 298/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7953 - val_loss: 1395.5154\n",
      "Epoch 299/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7958 - val_loss: 1395.5151\n",
      "Epoch 300/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7963 - val_loss: 1395.5151\n",
      "Epoch 301/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7968 - val_loss: 1395.5148\n",
      "Epoch 302/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7972 - val_loss: 1395.5140\n",
      "Epoch 303/500\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 148.7977 - val_loss: 1395.5138\n",
      "Epoch 304/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7982 - val_loss: 1395.5138\n",
      "Epoch 305/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7986 - val_loss: 1395.5135\n",
      "Epoch 306/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7990 - val_loss: 1395.5129\n",
      "Epoch 307/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.7994 - val_loss: 1395.5123\n",
      "Epoch 308/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.7998 - val_loss: 1395.5120\n",
      "Epoch 309/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8003 - val_loss: 1395.5117\n",
      "Epoch 310/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8006 - val_loss: 1395.5105\n",
      "Epoch 311/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8011 - val_loss: 1395.5104\n",
      "Epoch 312/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8015 - val_loss: 1395.5098\n",
      "Epoch 313/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8018 - val_loss: 1395.5096\n",
      "Epoch 314/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8021 - val_loss: 1395.5083\n",
      "Epoch 315/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8026 - val_loss: 1395.5078\n",
      "Epoch 316/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8029 - val_loss: 1395.5062\n",
      "Epoch 317/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8033 - val_loss: 1395.5062\n",
      "Epoch 318/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8037 - val_loss: 1395.5061\n",
      "Epoch 319/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8039 - val_loss: 1395.5052\n",
      "Epoch 320/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8043 - val_loss: 1395.5051\n",
      "Epoch 321/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8046 - val_loss: 1395.5040\n",
      "Epoch 322/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8050 - val_loss: 1395.5034\n",
      "Epoch 323/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8053 - val_loss: 1395.5027\n",
      "Epoch 324/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8055 - val_loss: 1395.5020\n",
      "Epoch 325/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8059 - val_loss: 1395.5015\n",
      "Epoch 326/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8061 - val_loss: 1395.5007\n",
      "Epoch 327/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8064 - val_loss: 1395.5000\n",
      "Epoch 328/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8067 - val_loss: 1395.5000\n",
      "Epoch 329/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8070 - val_loss: 1395.4991\n",
      "Epoch 330/500\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 148.8073 - val_loss: 1395.4980\n",
      "Epoch 331/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 148.8076 - val_loss: 1395.4978\n",
      "Epoch 332/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8078 - val_loss: 1395.4968\n",
      "Epoch 333/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8081 - val_loss: 1395.4957\n",
      "Epoch 334/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8083 - val_loss: 1395.4956\n",
      "Epoch 335/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8086 - val_loss: 1395.4949\n",
      "Epoch 336/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8088 - val_loss: 1395.4941\n",
      "Epoch 337/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8091 - val_loss: 1395.4935\n",
      "Epoch 338/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8093 - val_loss: 1395.4927\n",
      "Epoch 339/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8095 - val_loss: 1395.4922\n",
      "Epoch 340/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8097 - val_loss: 1395.4917\n",
      "Epoch 341/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8100 - val_loss: 1395.4908\n",
      "Epoch 342/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8102 - val_loss: 1395.4907\n",
      "Epoch 343/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8103 - val_loss: 1395.4901\n",
      "Epoch 344/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8105 - val_loss: 1395.4897\n",
      "Epoch 345/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8108 - val_loss: 1395.4897\n",
      "Epoch 346/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8109 - val_loss: 1395.4894\n",
      "Epoch 347/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8111 - val_loss: 1395.4884\n",
      "Epoch 348/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8113 - val_loss: 1395.4877\n",
      "Epoch 349/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8116 - val_loss: 1395.4873\n",
      "Epoch 350/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8118 - val_loss: 1395.4867\n",
      "Epoch 351/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8120 - val_loss: 1395.4867\n",
      "Epoch 352/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8121 - val_loss: 1395.4865\n",
      "Epoch 353/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8122 - val_loss: 1395.4856\n",
      "Epoch 354/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 148.8123 - val_loss: 1395.4851\n",
      "Epoch 355/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8126 - val_loss: 1395.4847\n",
      "Epoch 356/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8127 - val_loss: 1395.4843\n",
      "Epoch 357/500\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 148.8129 - val_loss: 1395.4841\n",
      "Epoch 358/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8130 - val_loss: 1395.4841\n",
      "Epoch 359/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8132 - val_loss: 1395.4836\n",
      "Epoch 360/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8133 - val_loss: 1395.4830\n",
      "Epoch 361/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8135 - val_loss: 1395.4832\n",
      "Epoch 362/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8136 - val_loss: 1395.4829\n",
      "Epoch 363/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8137 - val_loss: 1395.4825\n",
      "Epoch 364/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8138 - val_loss: 1395.4823\n",
      "Epoch 365/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8139 - val_loss: 1395.4818\n",
      "Epoch 366/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8140 - val_loss: 1395.4812\n",
      "Epoch 367/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8142 - val_loss: 1395.4811\n",
      "Epoch 368/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8144 - val_loss: 1395.4813\n",
      "Epoch 369/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8145 - val_loss: 1395.4812\n",
      "Epoch 370/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8145 - val_loss: 1395.4806\n",
      "Epoch 371/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8147 - val_loss: 1395.4802\n",
      "Epoch 372/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8148 - val_loss: 1395.4796\n",
      "Epoch 373/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8148 - val_loss: 1395.4792\n",
      "Epoch 374/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8150 - val_loss: 1395.4796\n",
      "Epoch 375/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8150 - val_loss: 1395.4788\n",
      "Epoch 376/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8152 - val_loss: 1395.4785\n",
      "Epoch 377/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8152 - val_loss: 1395.4779\n",
      "Epoch 378/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8153 - val_loss: 1395.4769\n",
      "Epoch 379/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8155 - val_loss: 1395.4771\n",
      "Epoch 380/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8155 - val_loss: 1395.4766\n",
      "Epoch 381/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8156 - val_loss: 1395.4761\n",
      "Epoch 382/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 148.8157 - val_loss: 1395.4753\n",
      "Epoch 383/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8158 - val_loss: 1395.4753\n",
      "Epoch 384/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8159 - val_loss: 1395.4753\n",
      "Epoch 385/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8160 - val_loss: 1395.4752\n",
      "Epoch 386/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8161 - val_loss: 1395.4746\n",
      "Epoch 387/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8163 - val_loss: 1395.4752\n",
      "Epoch 388/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8163 - val_loss: 1395.4756\n",
      "Epoch 389/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8163 - val_loss: 1395.4755\n",
      "Epoch 390/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8164 - val_loss: 1395.4753\n",
      "Epoch 391/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8165 - val_loss: 1395.4753\n",
      "Epoch 392/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8165 - val_loss: 1395.4751\n",
      "Epoch 393/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8166 - val_loss: 1395.4751\n",
      "Epoch 394/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8167 - val_loss: 1395.4752\n",
      "Epoch 395/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8167 - val_loss: 1395.4751\n",
      "Epoch 396/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8168 - val_loss: 1395.4751\n",
      "Epoch 397/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8168 - val_loss: 1395.4750\n",
      "Epoch 398/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8169 - val_loss: 1395.4744\n",
      "Epoch 399/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8169 - val_loss: 1395.4736\n",
      "Epoch 400/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8170 - val_loss: 1395.4735\n",
      "Epoch 401/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8171 - val_loss: 1395.4735\n",
      "Epoch 402/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8171 - val_loss: 1395.4729\n",
      "Epoch 403/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8171 - val_loss: 1395.4728\n",
      "Epoch 404/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8171 - val_loss: 1395.4720\n",
      "Epoch 405/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8172 - val_loss: 1395.4718\n",
      "Epoch 406/500\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 148.8173 - val_loss: 1395.4718\n",
      "Epoch 407/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8174 - val_loss: 1395.4719\n",
      "Epoch 408/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8174 - val_loss: 1395.4722\n",
      "Epoch 409/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8174 - val_loss: 1395.4719\n",
      "Epoch 410/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8174 - val_loss: 1395.4719\n",
      "Epoch 411/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8175 - val_loss: 1395.4718\n",
      "Epoch 412/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8176 - val_loss: 1395.4722\n",
      "Epoch 413/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8177 - val_loss: 1395.4723\n",
      "Epoch 414/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8177 - val_loss: 1395.4722\n",
      "Epoch 415/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8177 - val_loss: 1395.4718\n",
      "Epoch 416/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8177 - val_loss: 1395.4718\n",
      "Epoch 417/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8178 - val_loss: 1395.4719\n",
      "Epoch 418/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8178 - val_loss: 1395.4722\n",
      "Epoch 419/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8179 - val_loss: 1395.4718\n",
      "Epoch 420/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8179 - val_loss: 1395.4718\n",
      "Epoch 421/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8180 - val_loss: 1395.4717\n",
      "Epoch 422/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8180 - val_loss: 1395.4717\n",
      "Epoch 423/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8180 - val_loss: 1395.4717\n",
      "Epoch 424/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8180 - val_loss: 1395.4708\n",
      "Epoch 425/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8181 - val_loss: 1395.4708\n",
      "Epoch 426/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8181 - val_loss: 1395.4709\n",
      "Epoch 427/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8182 - val_loss: 1395.4717\n",
      "Epoch 428/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 148.8181 - val_loss: 1395.4712\n",
      "Epoch 429/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 148.8181 - val_loss: 1395.4708\n",
      "Epoch 430/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8182 - val_loss: 1395.4702\n",
      "Epoch 431/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8182 - val_loss: 1395.4698\n",
      "Epoch 432/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8183 - val_loss: 1395.4695\n",
      "Epoch 433/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8183 - val_loss: 1395.4691\n",
      "Epoch 434/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8184 - val_loss: 1395.4697\n",
      "Epoch 435/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8185 - val_loss: 1395.4702\n",
      "Epoch 436/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8184 - val_loss: 1395.4706\n",
      "Epoch 437/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8184 - val_loss: 1395.4701\n",
      "Epoch 438/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8184 - val_loss: 1395.4697\n",
      "Epoch 439/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8184 - val_loss: 1395.4694\n",
      "Epoch 440/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8185 - val_loss: 1395.4688\n",
      "Epoch 441/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8185 - val_loss: 1395.4684\n",
      "Epoch 442/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8185 - val_loss: 1395.4683\n",
      "Epoch 443/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8185 - val_loss: 1395.4684\n",
      "Epoch 444/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8186 - val_loss: 1395.4686\n",
      "Epoch 445/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8186 - val_loss: 1395.4688\n",
      "Epoch 446/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8186 - val_loss: 1395.4689\n",
      "Epoch 447/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8186 - val_loss: 1395.4692\n",
      "Epoch 448/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8186 - val_loss: 1395.4692\n",
      "Epoch 449/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8186 - val_loss: 1395.4689\n",
      "Epoch 450/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8186 - val_loss: 1395.4685\n",
      "Epoch 451/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8186 - val_loss: 1395.4679\n",
      "Epoch 452/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8187 - val_loss: 1395.4673\n",
      "Epoch 453/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8187 - val_loss: 1395.4664\n",
      "Epoch 454/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8187 - val_loss: 1395.4658\n",
      "Epoch 455/500\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 148.8187 - val_loss: 1395.4653\n",
      "Epoch 456/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8187 - val_loss: 1395.4647\n",
      "Epoch 457/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8188 - val_loss: 1395.4652\n",
      "Epoch 458/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8188 - val_loss: 1395.4647\n",
      "Epoch 459/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8188 - val_loss: 1395.4650\n",
      "Epoch 460/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8189 - val_loss: 1395.4653\n",
      "Epoch 461/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8189 - val_loss: 1395.4656\n",
      "Epoch 462/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8189 - val_loss: 1395.4664\n",
      "Epoch 463/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8189 - val_loss: 1395.4670\n",
      "Epoch 464/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8189 - val_loss: 1395.4674\n",
      "Epoch 465/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8189 - val_loss: 1395.4675\n",
      "Epoch 466/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 148.8189 - val_loss: 1395.4673\n",
      "Epoch 467/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8189 - val_loss: 1395.4673\n",
      "Epoch 468/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8189 - val_loss: 1395.4675\n",
      "Epoch 469/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8190 - val_loss: 1395.4679\n",
      "Epoch 470/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8190 - val_loss: 1395.4684\n",
      "Epoch 471/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8190 - val_loss: 1395.4689\n",
      "Epoch 472/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8189 - val_loss: 1395.4694\n",
      "Epoch 473/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8189 - val_loss: 1395.4697\n",
      "Epoch 474/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8189 - val_loss: 1395.4697\n",
      "Epoch 475/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8189 - val_loss: 1395.4697\n",
      "Epoch 476/500\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 148.8189 - val_loss: 1395.4689\n",
      "Epoch 477/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8189 - val_loss: 1395.4684\n",
      "Epoch 478/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8189 - val_loss: 1395.4673\n",
      "Epoch 479/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8190 - val_loss: 1395.4669\n",
      "Epoch 480/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8190 - val_loss: 1395.4661\n",
      "Epoch 481/500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 148.8190 - val_loss: 1395.4658\n",
      "Epoch 482/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8190 - val_loss: 1395.4653\n",
      "Epoch 483/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8191 - val_loss: 1395.4652\n",
      "Epoch 484/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8190 - val_loss: 1395.4652\n",
      "Epoch 485/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8190 - val_loss: 1395.4650\n",
      "Epoch 486/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8190 - val_loss: 1395.4647\n",
      "Epoch 487/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8191 - val_loss: 1395.4647\n",
      "Epoch 488/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8192 - val_loss: 1395.4650\n",
      "Epoch 489/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8192 - val_loss: 1395.4650\n",
      "Epoch 490/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8191 - val_loss: 1395.4655\n",
      "Epoch 491/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8191 - val_loss: 1395.4655\n",
      "Epoch 492/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8191 - val_loss: 1395.4655\n",
      "Epoch 493/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8191 - val_loss: 1395.4653\n",
      "Epoch 494/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8191 - val_loss: 1395.4653\n",
      "Epoch 495/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8191 - val_loss: 1395.4653\n",
      "Epoch 496/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8191 - val_loss: 1395.4653\n",
      "Epoch 497/500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 148.8191 - val_loss: 1395.4653\n",
      "Epoch 498/500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 148.8191 - val_loss: 1395.4652\n",
      "Epoch 499/500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 148.8191 - val_loss: 1395.4650\n",
      "Epoch 500/500\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 148.8191 - val_loss: 1395.4650\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6856, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0008, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0128, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(-2.3003, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 385ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.03326333e+01, 7.02576330e+01, 7.01820028e+01, 7.01063726e+01,\n",
       "        7.56191101e+01, 0.00000000e+00, 9.78886190e-02, 2.32655033e-01,\n",
       "        1.62875891e-01, 3.43400270e-01, 5.80880420e-02, 7.16635060e-02,\n",
       "        5.68655014e-01, 6.83504202e+01, 6.81823529e+01, 6.80142857e+01,\n",
       "        6.78462185e+01, 6.77348109e+01, 7.06525910e+01, 7.05769608e+01,\n",
       "        7.05013305e+01, 7.04257003e+01, 7.03500700e+01, 7.02744398e+01,\n",
       "        7.01988095e+01, 7.01231793e+01, 7.00475490e+01, 1.77199480e-01,\n",
       "        6.68463530e-01, 7.03808823e+01, 7.03052521e+01, 7.02296219e+01,\n",
       "        7.01539916e+01, 7.00783613e+01, 7.00027311e+01, 6.99271008e+01,\n",
       "        6.98514706e+01, 6.96389356e+01, 2.48600350e-01, 1.49832010e-01,\n",
       "        8.27042500e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.87215070e-01, 0.00000000e+00, 0.00000000e+00, 7.01399860e+01,\n",
       "        7.00643557e+01, 6.12748000e-03, 3.29938740e-01, 7.03976891e+01,\n",
       "        7.03220588e+01, 7.02464286e+01, 7.01707983e+01, 7.00951681e+01,\n",
       "        7.00195378e+01, 6.99439076e+01, 6.98682773e+01, 6.97509804e+01,\n",
       "        1.93131804e-01, 0.00000000e+00, 0.00000000e+00, 7.00755602e+01,\n",
       "        6.99999300e+01, 6.99242997e+01, 6.98486695e+01, 6.96202614e+01,\n",
       "        6.91160598e+01, 6.86118581e+01, 6.81076564e+01, 7.49386597e+01,\n",
       "        3.88379961e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.05797285e-01, 0.00000000e+00,\n",
       "        5.98060760e+01, 3.41597885e-01, 0.00000000e+00, 5.01324773e-01,\n",
       "        8.31944168e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.51398289e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.67244035e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.95918179e-01, 3.10172886e-01, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([57.44722279, 57.43492519, 57.42262759, 57.41032999, 57.39803238,\n",
       "       57.38573478, 57.37343718, 57.36113958, 57.34884198, 57.33654437,\n",
       "       57.32424677, 57.31194917, 57.29965157, 57.28735397, 57.27505636,\n",
       "       57.26275876, 57.25046116, 57.23816356, 57.22586596, 57.21356835,\n",
       "       57.20127075, 57.18897315, 57.17667555, 57.16437795, 57.15208034,\n",
       "       57.13978274, 57.12748514, 57.11518754, 57.10288994, 57.09059233,\n",
       "       57.07829473, 57.06599713, 57.05369953, 57.04140193, 57.02910432,\n",
       "       57.01680672, 57.00450912, 56.99221152, 56.97991392, 56.96761631,\n",
       "       56.95531871, 56.94302111, 56.93072351, 56.91842591, 56.9061283 ,\n",
       "       56.8938307 , 56.8815331 , 56.8692355 , 56.8569379 , 56.8446403 ,\n",
       "       56.83234269, 56.82004509, 56.80774749, 56.79544989, 56.78315229,\n",
       "       56.77085468, 56.75855708, 56.74625948, 56.73396188, 56.72166428,\n",
       "       56.70936667, 56.69706907, 56.68477147, 56.67247387, 56.66017627,\n",
       "       56.64787866, 56.63558106, 56.62328346, 56.61098586, 56.59868826,\n",
       "       56.58639065, 56.57409305, 56.56179545, 56.54949785, 56.53720025,\n",
       "       56.52490264, 56.51260504, 56.50030744, 56.48800984, 56.47571224,\n",
       "       56.46341463, 56.45111703, 56.43881943, 56.42652183, 56.41422423,\n",
       "       56.40192662, 56.38962902, 56.37733142, 56.36503382, 56.35273622,\n",
       "       56.34043861, 56.32814101, 56.31584341, 56.30354581, 56.29124821,\n",
       "       56.2789506 , 56.266653  , 56.2543554 , 56.2420578 , 56.2297602 ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.39325655976586\n",
      "35.13137048261038\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
