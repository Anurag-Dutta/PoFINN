{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "1695    62.245168\n",
       "1696    62.240500\n",
       "1697    62.235831\n",
       "1698    62.231162\n",
       "1699    62.226494\n",
       "Name: C8, Length: 1700, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c8_interpolated_1600_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "1595     0.000000\n",
       "1596     0.000000\n",
       "1597     0.000000\n",
       "1598     0.000000\n",
       "1599     0.000000\n",
       "Name: C8, Length: 1600, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1600)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.987115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.774230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.561345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.348459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.200000  0.000298\n",
       "1     87.987115  0.000298\n",
       "2     87.774230  0.000297\n",
       "3     87.561345  0.000297\n",
       "4     87.348459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 85.7336  # Value for C0\n",
    "K0 = -0.0020  # Value for K0\n",
    "K1 = 0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0125    # Value for b\n",
    "c = 2.3008    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.200000    87.987115    87.774230    87.561345    87.348459   \n",
      "351    87.987115    87.774230    87.561345    87.348459    87.135574   \n",
      "352    87.774230    87.561345    87.348459    87.135574    86.922689   \n",
      "353    87.561345    87.348459    87.135574    86.922689    86.709804   \n",
      "354    87.348459    87.135574    86.922689    86.709804    86.496919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.135574    86.922689    86.709804    86.496919    86.294538  ...   \n",
      "351    86.922689    86.709804    86.496919    86.294538    86.221709  ...   \n",
      "352    86.709804    86.496919    86.294538    86.221709    86.148880  ...   \n",
      "353    86.496919    86.294538    86.221709    86.148880    86.076050  ...   \n",
      "354    86.294538    86.221709    86.148880    86.076050    86.003221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   76.094351    0.000263   76.058870    0.000263   76.023389    0.000263   \n",
      "351   76.058870    0.000263   76.023389    0.000263   75.987909    0.000262   \n",
      "352   76.023389    0.000263   75.987909    0.000262   75.952428    0.000262   \n",
      "353   75.987909    0.000262   75.952428    0.000262   75.916947    0.000262   \n",
      "354   75.952428    0.000262   75.916947    0.000262   75.881466    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   75.987909    0.000262   75.952428    0.000262  \n",
      "351   75.952428    0.000262   75.916947    0.000262  \n",
      "352   75.916947    0.000262   75.881466    0.000262  \n",
      "353   75.881466    0.000262   75.845985    0.000262  \n",
      "354   75.845985    0.000262   75.810504    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1150, 1, 251) (1150, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "15/15 [==============================] - 3s 36ms/step - loss: 5055.3877 - val_loss: 4074.5796\n",
      "Epoch 2/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4995.1714 - val_loss: 4032.2019\n",
      "Epoch 3/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4948.2778 - val_loss: 3990.0178\n",
      "Epoch 4/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 4901.6353 - val_loss: 3948.1250\n",
      "Epoch 5/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4855.3066 - val_loss: 3906.5464\n",
      "Epoch 6/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4809.3042 - val_loss: 3865.2832\n",
      "Epoch 7/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4763.6284 - val_loss: 3824.3357\n",
      "Epoch 8/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4718.2778 - val_loss: 3783.7021\n",
      "Epoch 9/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4673.2515 - val_loss: 3743.3806\n",
      "Epoch 10/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4624.6230 - val_loss: 3690.2441\n",
      "Epoch 11/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4563.4487 - val_loss: 3626.3911\n",
      "Epoch 12/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4494.0449 - val_loss: 3577.7083\n",
      "Epoch 13/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4440.3638 - val_loss: 3530.4048\n",
      "Epoch 14/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4386.4019 - val_loss: 3479.9553\n",
      "Epoch 15/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4330.6670 - val_loss: 3431.3657\n",
      "Epoch 16/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4277.0244 - val_loss: 3384.1436\n",
      "Epoch 17/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4212.8115 - val_loss: 3322.5332\n",
      "Epoch 18/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4154.5332 - val_loss: 3273.8015\n",
      "Epoch 19/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4100.6455 - val_loss: 3226.4929\n",
      "Epoch 20/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4048.2163 - val_loss: 3180.3074\n",
      "Epoch 21/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3982.5969 - val_loss: 3119.2869\n",
      "Epoch 22/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3926.8840 - val_loss: 3071.1953\n",
      "Epoch 23/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3873.5125 - val_loss: 3024.4895\n",
      "Epoch 24/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3821.5583 - val_loss: 2978.9539\n",
      "Epoch 25/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3770.7769 - val_loss: 2934.3960\n",
      "Epoch 26/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3720.9856 - val_loss: 2890.6829\n",
      "Epoch 27/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3672.0586 - val_loss: 2847.7266\n",
      "Epoch 28/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3623.9104 - val_loss: 2805.4617\n",
      "Epoch 29/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3576.4768 - val_loss: 2763.8396\n",
      "Epoch 30/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3529.7112 - val_loss: 2722.8235\n",
      "Epoch 31/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3483.5742 - val_loss: 2682.3828\n",
      "Epoch 32/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3438.0371 - val_loss: 2642.4927\n",
      "Epoch 33/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3393.0730 - val_loss: 2603.1321\n",
      "Epoch 34/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3348.6606 - val_loss: 2564.2832\n",
      "Epoch 35/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3304.7817 - val_loss: 2525.9307\n",
      "Epoch 36/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3261.4207 - val_loss: 2488.0598\n",
      "Epoch 37/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3218.5627 - val_loss: 2450.6602\n",
      "Epoch 38/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3176.1953 - val_loss: 2413.7195\n",
      "Epoch 39/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3134.3079 - val_loss: 2377.2292\n",
      "Epoch 40/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3092.8901 - val_loss: 2341.1787\n",
      "Epoch 41/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3051.9316 - val_loss: 2305.5620\n",
      "Epoch 42/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3011.4248 - val_loss: 2270.3699\n",
      "Epoch 43/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2971.3621 - val_loss: 2235.5959\n",
      "Epoch 44/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 2931.7354 - val_loss: 2201.2336\n",
      "Epoch 45/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2892.5393 - val_loss: 2167.2769\n",
      "Epoch 46/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2853.7664 - val_loss: 2133.7200\n",
      "Epoch 47/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2815.4104 - val_loss: 2100.5576\n",
      "Epoch 48/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2777.4666 - val_loss: 2067.7837\n",
      "Epoch 49/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2739.8965 - val_loss: 2031.9286\n",
      "Epoch 50/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2696.8022 - val_loss: 1995.5586\n",
      "Epoch 51/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2654.8301 - val_loss: 1959.6467\n",
      "Epoch 52/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2613.9165 - val_loss: 1924.8678\n",
      "Epoch 53/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2574.1741 - val_loss: 1891.0292\n",
      "Epoch 54/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2535.3843 - val_loss: 1857.9645\n",
      "Epoch 55/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2497.3855 - val_loss: 1825.5645\n",
      "Epoch 56/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2460.0747 - val_loss: 1793.7592\n",
      "Epoch 57/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2423.3838 - val_loss: 1762.4998\n",
      "Epoch 58/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2387.2637 - val_loss: 1731.7502\n",
      "Epoch 59/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2351.6790 - val_loss: 1701.4841\n",
      "Epoch 60/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2316.6018 - val_loss: 1671.6793\n",
      "Epoch 61/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2282.0100 - val_loss: 1642.3182\n",
      "Epoch 62/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2247.8862 - val_loss: 1613.3872\n",
      "Epoch 63/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2214.2141 - val_loss: 1584.8726\n",
      "Epoch 64/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2180.9817 - val_loss: 1556.7644\n",
      "Epoch 65/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2148.1770 - val_loss: 1529.0527\n",
      "Epoch 66/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2115.7898 - val_loss: 1501.7285\n",
      "Epoch 67/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2083.8118 - val_loss: 1474.7847\n",
      "Epoch 68/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2052.2341 - val_loss: 1448.2140\n",
      "Epoch 69/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 2021.0503 - val_loss: 1422.0095\n",
      "Epoch 70/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1990.2526 - val_loss: 1396.1659\n",
      "Epoch 71/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1959.8351 - val_loss: 1370.6776\n",
      "Epoch 72/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1929.7916 - val_loss: 1345.5381\n",
      "Epoch 73/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1900.1168 - val_loss: 1320.7424\n",
      "Epoch 74/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 1870.8058 - val_loss: 1296.2876\n",
      "Epoch 75/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1841.8533 - val_loss: 1272.1680\n",
      "Epoch 76/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1813.2549 - val_loss: 1248.3789\n",
      "Epoch 77/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1785.0055 - val_loss: 1224.9175\n",
      "Epoch 78/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1757.1011 - val_loss: 1201.7777\n",
      "Epoch 79/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1729.5377 - val_loss: 1178.9579\n",
      "Epoch 80/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1702.3116 - val_loss: 1156.4534\n",
      "Epoch 81/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1675.4187 - val_loss: 1134.2605\n",
      "Epoch 82/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1648.8553 - val_loss: 1112.3757\n",
      "Epoch 83/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1622.6178 - val_loss: 1090.7958\n",
      "Epoch 84/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1596.7023 - val_loss: 1069.5177\n",
      "Epoch 85/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1571.1064 - val_loss: 1048.5370\n",
      "Epoch 86/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1545.8258 - val_loss: 1027.8525\n",
      "Epoch 87/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 1520.8583 - val_loss: 1007.4601\n",
      "Epoch 88/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1496.2002 - val_loss: 987.3560\n",
      "Epoch 89/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1471.8483 - val_loss: 967.5390\n",
      "Epoch 90/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1447.7997 - val_loss: 948.0053\n",
      "Epoch 91/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1424.0520 - val_loss: 928.7522\n",
      "Epoch 92/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1400.6017 - val_loss: 909.7763\n",
      "Epoch 93/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1377.4456 - val_loss: 891.0753\n",
      "Epoch 94/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1354.5814 - val_loss: 872.6466\n",
      "Epoch 95/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1332.0068 - val_loss: 854.4875\n",
      "Epoch 96/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1309.7185 - val_loss: 836.5950\n",
      "Epoch 97/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1287.7139 - val_loss: 818.9669\n",
      "Epoch 98/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1265.9901 - val_loss: 801.6006\n",
      "Epoch 99/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1244.5455 - val_loss: 784.4938\n",
      "Epoch 100/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1223.3766 - val_loss: 767.6429\n",
      "Epoch 101/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1202.4811 - val_loss: 751.0468\n",
      "Epoch 102/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1181.8568 - val_loss: 734.7025\n",
      "Epoch 103/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1161.5012 - val_loss: 718.6079\n",
      "Epoch 104/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1141.4117 - val_loss: 702.7594\n",
      "Epoch 105/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 1121.5857 - val_loss: 687.1561\n",
      "Epoch 106/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1102.0212 - val_loss: 671.7947\n",
      "Epoch 107/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1082.7153 - val_loss: 656.6735\n",
      "Epoch 108/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1063.6667 - val_loss: 641.7896\n",
      "Epoch 109/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1044.8721 - val_loss: 627.1408\n",
      "Epoch 110/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1026.3289 - val_loss: 612.7255\n",
      "Epoch 111/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1008.0362 - val_loss: 598.5405\n",
      "Epoch 112/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 989.9909 - val_loss: 584.5841\n",
      "Epoch 113/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 972.1909 - val_loss: 570.8546\n",
      "Epoch 114/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 954.6339 - val_loss: 557.3484\n",
      "Epoch 115/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 937.3177 - val_loss: 544.0644\n",
      "Epoch 116/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 920.2401 - val_loss: 530.9999\n",
      "Epoch 117/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 903.3989 - val_loss: 518.1531\n",
      "Epoch 118/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 886.7925 - val_loss: 505.5220\n",
      "Epoch 119/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 870.4179 - val_loss: 493.1039\n",
      "Epoch 120/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 854.2736 - val_loss: 480.8970\n",
      "Epoch 121/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 838.3572 - val_loss: 468.8986\n",
      "Epoch 122/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 822.6665 - val_loss: 457.1081\n",
      "Epoch 123/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 807.2001 - val_loss: 445.5217\n",
      "Epoch 124/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 791.9556 - val_loss: 434.1388\n",
      "Epoch 125/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 776.9304 - val_loss: 422.9561\n",
      "Epoch 126/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 762.1232 - val_loss: 411.9725\n",
      "Epoch 127/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 747.5313 - val_loss: 401.1854\n",
      "Epoch 128/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 733.1533 - val_loss: 390.5927\n",
      "Epoch 129/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 718.9869 - val_loss: 380.1927\n",
      "Epoch 130/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 705.0301 - val_loss: 369.9832\n",
      "Epoch 131/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 691.2808 - val_loss: 359.9627\n",
      "Epoch 132/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 677.7375 - val_loss: 350.1280\n",
      "Epoch 133/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 664.3977 - val_loss: 340.4786\n",
      "Epoch 134/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 651.2595 - val_loss: 331.0114\n",
      "Epoch 135/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 638.3214 - val_loss: 321.7253\n",
      "Epoch 136/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 625.5810 - val_loss: 312.6172\n",
      "Epoch 137/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 613.0365 - val_loss: 303.6860\n",
      "Epoch 138/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 600.6859 - val_loss: 294.9300\n",
      "Epoch 139/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 588.5275 - val_loss: 286.3466\n",
      "Epoch 140/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 576.5593 - val_loss: 277.9341\n",
      "Epoch 141/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 564.7793 - val_loss: 269.6905\n",
      "Epoch 142/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 553.1859 - val_loss: 261.6144\n",
      "Epoch 143/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 541.7768 - val_loss: 253.7034\n",
      "Epoch 144/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 530.5505 - val_loss: 245.9555\n",
      "Epoch 145/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 519.5051 - val_loss: 238.3688\n",
      "Epoch 146/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 508.6384 - val_loss: 230.9421\n",
      "Epoch 147/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 497.9490 - val_loss: 223.6727\n",
      "Epoch 148/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 487.4347 - val_loss: 216.5595\n",
      "Epoch 149/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 477.0941 - val_loss: 209.6003\n",
      "Epoch 150/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 466.9250 - val_loss: 202.7927\n",
      "Epoch 151/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 456.9258 - val_loss: 196.1354\n",
      "Epoch 152/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 447.0945 - val_loss: 189.6266\n",
      "Epoch 153/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 437.4294 - val_loss: 183.2647\n",
      "Epoch 154/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 427.9288 - val_loss: 177.0473\n",
      "Epoch 155/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 418.5906 - val_loss: 170.9731\n",
      "Epoch 156/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 409.4135 - val_loss: 165.0396\n",
      "Epoch 157/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 400.3953 - val_loss: 159.2458\n",
      "Epoch 158/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 391.5346 - val_loss: 153.5892\n",
      "Epoch 159/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 382.8294 - val_loss: 148.0686\n",
      "Epoch 160/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 374.2780 - val_loss: 142.6816\n",
      "Epoch 161/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 365.8786 - val_loss: 137.4272\n",
      "Epoch 162/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 357.6296 - val_loss: 132.3027\n",
      "Epoch 163/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 349.5289 - val_loss: 127.3072\n",
      "Epoch 164/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 341.5753 - val_loss: 122.4384\n",
      "Epoch 165/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 333.7668 - val_loss: 117.6951\n",
      "Epoch 166/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 326.1018 - val_loss: 113.0745\n",
      "Epoch 167/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 318.5782 - val_loss: 108.5760\n",
      "Epoch 168/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 311.1949 - val_loss: 104.1972\n",
      "Epoch 169/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 303.9500 - val_loss: 99.9366\n",
      "Epoch 170/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 296.8416 - val_loss: 95.7925\n",
      "Epoch 171/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 289.8681 - val_loss: 91.7631\n",
      "Epoch 172/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 283.0279 - val_loss: 87.8470\n",
      "Epoch 173/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 276.3195 - val_loss: 84.0419\n",
      "Epoch 174/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 269.7409 - val_loss: 80.3466\n",
      "Epoch 175/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 263.2908 - val_loss: 76.7593\n",
      "Epoch 176/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 256.9674 - val_loss: 73.2785\n",
      "Epoch 177/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 250.7690 - val_loss: 69.9021\n",
      "Epoch 178/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 244.6941 - val_loss: 66.6288\n",
      "Epoch 179/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 238.7410 - val_loss: 63.4569\n",
      "Epoch 180/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 232.9080 - val_loss: 60.3849\n",
      "Epoch 181/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 227.1936 - val_loss: 57.4105\n",
      "Epoch 182/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 221.5963 - val_loss: 54.5329\n",
      "Epoch 183/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 216.1144 - val_loss: 51.7497\n",
      "Epoch 184/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 210.7462 - val_loss: 49.0603\n",
      "Epoch 185/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 205.4904 - val_loss: 46.4620\n",
      "Epoch 186/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 200.3452 - val_loss: 43.9541\n",
      "Epoch 187/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 195.3092 - val_loss: 41.5345\n",
      "Epoch 188/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 190.3808 - val_loss: 39.2017\n",
      "Epoch 189/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 185.5583 - val_loss: 36.9542\n",
      "Epoch 190/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 180.8404 - val_loss: 34.7904\n",
      "Epoch 191/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 176.2255 - val_loss: 32.7087\n",
      "Epoch 192/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 171.7119 - val_loss: 30.7080\n",
      "Epoch 193/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 167.2984 - val_loss: 28.7860\n",
      "Epoch 194/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 162.9833 - val_loss: 26.9416\n",
      "Epoch 195/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 158.7649 - val_loss: 25.1734\n",
      "Epoch 196/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 154.6422 - val_loss: 23.4798\n",
      "Epoch 197/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 150.6134 - val_loss: 21.8591\n",
      "Epoch 198/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 146.6770 - val_loss: 20.3101\n",
      "Epoch 199/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 142.8317 - val_loss: 18.8312\n",
      "Epoch 200/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 139.0761 - val_loss: 17.4210\n",
      "Epoch 201/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 135.4086 - val_loss: 16.0779\n",
      "Epoch 202/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 131.8278 - val_loss: 14.8005\n",
      "Epoch 203/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 128.3323 - val_loss: 13.5875\n",
      "Epoch 204/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 124.9207 - val_loss: 12.4373\n",
      "Epoch 205/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 121.5916 - val_loss: 11.3487\n",
      "Epoch 206/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 118.3436 - val_loss: 10.3201\n",
      "Epoch 207/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 115.1752 - val_loss: 9.3502\n",
      "Epoch 208/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 112.0852 - val_loss: 8.4376\n",
      "Epoch 209/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 109.0721 - val_loss: 7.5810\n",
      "Epoch 210/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 106.1347 - val_loss: 6.7790\n",
      "Epoch 211/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 103.2715 - val_loss: 6.0302\n",
      "Epoch 212/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 100.4812 - val_loss: 5.3334\n",
      "Epoch 213/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 97.7625 - val_loss: 4.6872\n",
      "Epoch 214/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 95.1143 - val_loss: 4.0903\n",
      "Epoch 215/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 92.5350 - val_loss: 3.5415\n",
      "Epoch 216/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 90.0235 - val_loss: 3.0394\n",
      "Epoch 217/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 87.5785 - val_loss: 2.5827\n",
      "Epoch 218/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 85.1986 - val_loss: 2.1703\n",
      "Epoch 219/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 82.8825 - val_loss: 1.8009\n",
      "Epoch 220/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 80.6291 - val_loss: 1.4732\n",
      "Epoch 221/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 78.4373 - val_loss: 1.1861\n",
      "Epoch 222/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 76.3056 - val_loss: 0.9384\n",
      "Epoch 223/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 74.2329 - val_loss: 0.7288\n",
      "Epoch 224/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 72.2182 - val_loss: 0.5562\n",
      "Epoch 225/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 70.2600 - val_loss: 0.4195\n",
      "Epoch 226/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 68.3574 - val_loss: 0.3174\n",
      "Epoch 227/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 66.5090 - val_loss: 0.2490\n",
      "Epoch 228/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 64.7138 - val_loss: 0.2130\n",
      "Epoch 229/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 62.9706 - val_loss: 0.2083\n",
      "Epoch 230/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 61.2784 - val_loss: 0.2340\n",
      "Epoch 231/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 59.6359 - val_loss: 0.2888\n",
      "Epoch 232/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 58.0422 - val_loss: 0.3719\n",
      "Epoch 233/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 56.4961 - val_loss: 0.4820\n",
      "Epoch 234/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 54.9967 - val_loss: 0.6182\n",
      "Epoch 235/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 53.5426 - val_loss: 0.7796\n",
      "Epoch 236/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 52.1330 - val_loss: 0.9651\n",
      "Epoch 237/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 50.7668 - val_loss: 1.1737\n",
      "Epoch 238/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 49.4431 - val_loss: 1.4044\n",
      "Epoch 239/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 48.1607 - val_loss: 1.6564\n",
      "Epoch 240/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 46.9188 - val_loss: 1.9287\n",
      "Epoch 241/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 45.7164 - val_loss: 2.2204\n",
      "Epoch 242/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 44.5524 - val_loss: 2.5306\n",
      "Epoch 243/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 43.4260 - val_loss: 2.8584\n",
      "Epoch 244/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 42.3361 - val_loss: 3.2030\n",
      "Epoch 245/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 41.2821 - val_loss: 3.5635\n",
      "Epoch 246/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 40.2626 - val_loss: 3.9390\n",
      "Epoch 247/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 39.2772 - val_loss: 4.3288\n",
      "Epoch 248/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 38.3247 - val_loss: 4.7320\n",
      "Epoch 249/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 37.4044 - val_loss: 5.1480\n",
      "Epoch 250/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 36.5154 - val_loss: 5.5758\n",
      "Epoch 251/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 35.6569 - val_loss: 6.0149\n",
      "Epoch 252/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 34.8279 - val_loss: 6.4644\n",
      "Epoch 253/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 34.0278 - val_loss: 6.9237\n",
      "Epoch 254/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 33.2556 - val_loss: 7.3920\n",
      "Epoch 255/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 32.5108 - val_loss: 7.8686\n",
      "Epoch 256/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 31.7924 - val_loss: 8.3530\n",
      "Epoch 257/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 31.0998 - val_loss: 8.8444\n",
      "Epoch 258/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 30.4321 - val_loss: 9.3422\n",
      "Epoch 259/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 29.7887 - val_loss: 9.8458\n",
      "Epoch 260/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 29.1688 - val_loss: 10.3547\n",
      "Epoch 261/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 28.5718 - val_loss: 10.8681\n",
      "Epoch 262/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 27.9970 - val_loss: 11.3856\n",
      "Epoch 263/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 27.4437 - val_loss: 11.9064\n",
      "Epoch 264/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 26.9113 - val_loss: 12.4305\n",
      "Epoch 265/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 26.3991 - val_loss: 12.9569\n",
      "Epoch 266/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.9065 - val_loss: 13.4852\n",
      "Epoch 267/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 25.4328 - val_loss: 14.0149\n",
      "Epoch 268/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 24.9776 - val_loss: 14.5456\n",
      "Epoch 269/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 24.5401 - val_loss: 15.0769\n",
      "Epoch 270/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 24.1198 - val_loss: 15.6083\n",
      "Epoch 271/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 23.7161 - val_loss: 16.1395\n",
      "Epoch 272/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 23.3286 - val_loss: 16.6698\n",
      "Epoch 273/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 22.9565 - val_loss: 17.1991\n",
      "Epoch 274/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 22.5996 - val_loss: 17.7269\n",
      "Epoch 275/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 22.2572 - val_loss: 18.2525\n",
      "Epoch 276/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 21.9288 - val_loss: 18.7762\n",
      "Epoch 277/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 21.6140 - val_loss: 19.2972\n",
      "Epoch 278/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 21.3123 - val_loss: 19.8154\n",
      "Epoch 279/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 21.0231 - val_loss: 20.3303\n",
      "Epoch 280/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 20.7462 - val_loss: 20.8419\n",
      "Epoch 281/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 20.4809 - val_loss: 21.3497\n",
      "Epoch 282/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 20.2270 - val_loss: 21.8535\n",
      "Epoch 283/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 19.9839 - val_loss: 22.3528\n",
      "Epoch 284/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 19.7514 - val_loss: 22.8478\n",
      "Epoch 285/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 19.5290 - val_loss: 23.3381\n",
      "Epoch 286/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 19.3163 - val_loss: 23.8232\n",
      "Epoch 287/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 19.1130 - val_loss: 24.3034\n",
      "Epoch 288/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.9186 - val_loss: 24.7783\n",
      "Epoch 289/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.7329 - val_loss: 25.2474\n",
      "Epoch 290/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.5556 - val_loss: 25.7109\n",
      "Epoch 291/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.3862 - val_loss: 26.1686\n",
      "Epoch 292/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.2245 - val_loss: 26.6202\n",
      "Epoch 293/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.0702 - val_loss: 27.0657\n",
      "Epoch 294/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 17.9230 - val_loss: 27.5049\n",
      "Epoch 295/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 17.7826 - val_loss: 27.9378\n",
      "Epoch 296/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 17.6487 - val_loss: 28.3642\n",
      "Epoch 297/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 17.5210 - val_loss: 28.7842\n",
      "Epoch 298/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 17.3993 - val_loss: 29.1973\n",
      "Epoch 299/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 17.2833 - val_loss: 29.6039\n",
      "Epoch 300/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 17.1728 - val_loss: 30.0035\n",
      "Epoch 301/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 17.0676 - val_loss: 30.3966\n",
      "Epoch 302/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.9674 - val_loss: 30.7826\n",
      "Epoch 303/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.8721 - val_loss: 31.1616\n",
      "Epoch 304/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.7813 - val_loss: 31.5337\n",
      "Epoch 305/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.6950 - val_loss: 31.8990\n",
      "Epoch 306/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 16.6128 - val_loss: 32.2570\n",
      "Epoch 307/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.5347 - val_loss: 32.6083\n",
      "Epoch 308/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.4605 - val_loss: 32.9524\n",
      "Epoch 309/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.3899 - val_loss: 33.2894\n",
      "Epoch 310/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.3228 - val_loss: 33.6197\n",
      "Epoch 311/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.2590 - val_loss: 33.9428\n",
      "Epoch 312/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.1985 - val_loss: 34.2593\n",
      "Epoch 313/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.1410 - val_loss: 34.5686\n",
      "Epoch 314/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.0863 - val_loss: 34.8712\n",
      "Epoch 315/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.0344 - val_loss: 35.1669\n",
      "Epoch 316/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.9852 - val_loss: 35.4557\n",
      "Epoch 317/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.9385 - val_loss: 35.7378\n",
      "Epoch 318/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.8942 - val_loss: 36.0134\n",
      "Epoch 319/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.8521 - val_loss: 36.2819\n",
      "Epoch 320/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.8122 - val_loss: 36.5438\n",
      "Epoch 321/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.7744 - val_loss: 36.7997\n",
      "Epoch 322/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 15.7385 - val_loss: 37.0489\n",
      "Epoch 323/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.7045 - val_loss: 37.2916\n",
      "Epoch 324/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 15.6723 - val_loss: 37.5284\n",
      "Epoch 325/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.6417 - val_loss: 37.7586\n",
      "Epoch 326/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.6127 - val_loss: 37.9828\n",
      "Epoch 327/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.5852 - val_loss: 38.2010\n",
      "Epoch 328/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.5592 - val_loss: 38.4130\n",
      "Epoch 329/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.5346 - val_loss: 38.6195\n",
      "Epoch 330/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.5113 - val_loss: 38.8200\n",
      "Epoch 331/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.4891 - val_loss: 39.0147\n",
      "Epoch 332/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.4682 - val_loss: 39.2039\n",
      "Epoch 333/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.4484 - val_loss: 39.3877\n",
      "Epoch 334/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.4296 - val_loss: 39.5660\n",
      "Epoch 335/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.4118 - val_loss: 39.7391\n",
      "Epoch 336/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.3949 - val_loss: 39.9070\n",
      "Epoch 337/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.3790 - val_loss: 40.0697\n",
      "Epoch 338/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 15.3639 - val_loss: 40.2274\n",
      "Epoch 339/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.3496 - val_loss: 40.3802\n",
      "Epoch 340/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.3360 - val_loss: 40.5281\n",
      "Epoch 341/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.3232 - val_loss: 40.6712\n",
      "Epoch 342/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.3111 - val_loss: 40.8100\n",
      "Epoch 343/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 15.2996 - val_loss: 40.9442\n",
      "Epoch 344/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.2888 - val_loss: 41.0740\n",
      "Epoch 345/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.2785 - val_loss: 41.1996\n",
      "Epoch 346/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.2687 - val_loss: 41.3207\n",
      "Epoch 347/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.2595 - val_loss: 41.4377\n",
      "Epoch 348/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.2508 - val_loss: 41.5508\n",
      "Epoch 349/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.2426 - val_loss: 41.6598\n",
      "Epoch 350/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.2347 - val_loss: 41.7653\n",
      "Epoch 351/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 15.2273 - val_loss: 41.8668\n",
      "Epoch 352/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.2204 - val_loss: 41.9649\n",
      "Epoch 353/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.2138 - val_loss: 42.0594\n",
      "Epoch 354/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.2075 - val_loss: 42.1506\n",
      "Epoch 355/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.2015 - val_loss: 42.2380\n",
      "Epoch 356/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1960 - val_loss: 42.3229\n",
      "Epoch 357/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1906 - val_loss: 42.4043\n",
      "Epoch 358/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 15.1856 - val_loss: 42.4825\n",
      "Epoch 359/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1809 - val_loss: 42.5580\n",
      "Epoch 360/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1764 - val_loss: 42.6306\n",
      "Epoch 361/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1721 - val_loss: 42.7005\n",
      "Epoch 362/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1681 - val_loss: 42.7673\n",
      "Epoch 363/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1643 - val_loss: 42.8318\n",
      "Epoch 364/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1607 - val_loss: 42.8935\n",
      "Epoch 365/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1573 - val_loss: 42.9529\n",
      "Epoch 366/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1541 - val_loss: 43.0097\n",
      "Epoch 367/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1511 - val_loss: 43.0644\n",
      "Epoch 368/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1482 - val_loss: 43.1167\n",
      "Epoch 369/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1454 - val_loss: 43.1667\n",
      "Epoch 370/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 15.1429 - val_loss: 43.2147\n",
      "Epoch 371/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1405 - val_loss: 43.2607\n",
      "Epoch 372/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1382 - val_loss: 43.3049\n",
      "Epoch 373/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1361 - val_loss: 43.3471\n",
      "Epoch 374/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 15.1340 - val_loss: 43.3876\n",
      "Epoch 375/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1321 - val_loss: 43.4263\n",
      "Epoch 376/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1303 - val_loss: 43.4632\n",
      "Epoch 377/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.1286 - val_loss: 43.4986\n",
      "Epoch 378/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1270 - val_loss: 43.5321\n",
      "Epoch 379/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1255 - val_loss: 43.5643\n",
      "Epoch 380/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1241 - val_loss: 43.5952\n",
      "Epoch 381/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1228 - val_loss: 43.6245\n",
      "Epoch 382/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1215 - val_loss: 43.6525\n",
      "Epoch 383/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1204 - val_loss: 43.6794\n",
      "Epoch 384/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1192 - val_loss: 43.7048\n",
      "Epoch 385/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1182 - val_loss: 43.7290\n",
      "Epoch 386/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1172 - val_loss: 43.7520\n",
      "Epoch 387/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1163 - val_loss: 43.7740\n",
      "Epoch 388/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1155 - val_loss: 43.7948\n",
      "Epoch 389/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1147 - val_loss: 43.8147\n",
      "Epoch 390/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1140 - val_loss: 43.8335\n",
      "Epoch 391/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1133 - val_loss: 43.8515\n",
      "Epoch 392/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1126 - val_loss: 43.8686\n",
      "Epoch 393/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1121 - val_loss: 43.8848\n",
      "Epoch 394/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1115 - val_loss: 43.9001\n",
      "Epoch 395/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1110 - val_loss: 43.9147\n",
      "Epoch 396/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1105 - val_loss: 43.9284\n",
      "Epoch 397/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1102 - val_loss: 43.9417\n",
      "Epoch 398/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 15.1097 - val_loss: 43.9539\n",
      "Epoch 399/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1094 - val_loss: 43.9658\n",
      "Epoch 400/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1090 - val_loss: 43.9768\n",
      "Epoch 401/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1088 - val_loss: 43.9873\n",
      "Epoch 402/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1085 - val_loss: 43.9973\n",
      "Epoch 403/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1083 - val_loss: 44.0065\n",
      "Epoch 404/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1081 - val_loss: 44.0154\n",
      "Epoch 405/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1079 - val_loss: 44.0238\n",
      "Epoch 406/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1078 - val_loss: 44.0318\n",
      "Epoch 407/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1076 - val_loss: 44.0392\n",
      "Epoch 408/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1075 - val_loss: 44.0459\n",
      "Epoch 409/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1074 - val_loss: 44.0529\n",
      "Epoch 410/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1073 - val_loss: 44.0591\n",
      "Epoch 411/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1073 - val_loss: 44.0650\n",
      "Epoch 412/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 15.1072 - val_loss: 44.0707\n",
      "Epoch 413/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1072 - val_loss: 44.0757\n",
      "Epoch 414/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1072 - val_loss: 44.0804\n",
      "Epoch 415/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1072 - val_loss: 44.0849\n",
      "Epoch 416/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1073 - val_loss: 44.0894\n",
      "Epoch 417/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1073 - val_loss: 44.0932\n",
      "Epoch 418/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1073 - val_loss: 44.0970\n",
      "Epoch 419/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1074 - val_loss: 44.1006\n",
      "Epoch 420/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1074 - val_loss: 44.1037\n",
      "Epoch 421/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1075 - val_loss: 44.1068\n",
      "Epoch 422/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1076 - val_loss: 44.1093\n",
      "Epoch 423/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1078 - val_loss: 44.1123\n",
      "Epoch 424/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1078 - val_loss: 44.1146\n",
      "Epoch 425/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1080 - val_loss: 44.1167\n",
      "Epoch 426/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1081 - val_loss: 44.1191\n",
      "Epoch 427/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1082 - val_loss: 44.1209\n",
      "Epoch 428/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 15.1084 - val_loss: 44.1230\n",
      "Epoch 429/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 15.1085 - val_loss: 44.1246\n",
      "Epoch 430/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.1087 - val_loss: 44.1264\n",
      "Epoch 431/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1088 - val_loss: 44.1277\n",
      "Epoch 432/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1090 - val_loss: 44.1290\n",
      "Epoch 433/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1091 - val_loss: 44.1300\n",
      "Epoch 434/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1093 - val_loss: 44.1311\n",
      "Epoch 435/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1095 - val_loss: 44.1322\n",
      "Epoch 436/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1097 - val_loss: 44.1330\n",
      "Epoch 437/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1099 - val_loss: 44.1338\n",
      "Epoch 438/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1100 - val_loss: 44.1344\n",
      "Epoch 439/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1103 - val_loss: 44.1350\n",
      "Epoch 440/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1105 - val_loss: 44.1357\n",
      "Epoch 441/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1107 - val_loss: 44.1362\n",
      "Epoch 442/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1108 - val_loss: 44.1365\n",
      "Epoch 443/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1111 - val_loss: 44.1368\n",
      "Epoch 444/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1113 - val_loss: 44.1371\n",
      "Epoch 445/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1115 - val_loss: 44.1376\n",
      "Epoch 446/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 15.1117 - val_loss: 44.1377\n",
      "Epoch 447/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1119 - val_loss: 44.1381\n",
      "Epoch 448/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1121 - val_loss: 44.1381\n",
      "Epoch 449/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1123 - val_loss: 44.1382\n",
      "Epoch 450/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1125 - val_loss: 44.1381\n",
      "Epoch 451/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1127 - val_loss: 44.1381\n",
      "Epoch 452/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1130 - val_loss: 44.1379\n",
      "Epoch 453/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1132 - val_loss: 44.1379\n",
      "Epoch 454/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1134 - val_loss: 44.1379\n",
      "Epoch 455/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1136 - val_loss: 44.1379\n",
      "Epoch 456/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1139 - val_loss: 44.1378\n",
      "Epoch 457/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1141 - val_loss: 44.1377\n",
      "Epoch 458/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1143 - val_loss: 44.1373\n",
      "Epoch 459/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1145 - val_loss: 44.1371\n",
      "Epoch 460/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1148 - val_loss: 44.1368\n",
      "Epoch 461/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 15.1150 - val_loss: 44.1367\n",
      "Epoch 462/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 15.1152 - val_loss: 44.1365\n",
      "Epoch 463/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 15.1154 - val_loss: 44.1363\n",
      "Epoch 464/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.1156 - val_loss: 44.1362\n",
      "Epoch 465/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1159 - val_loss: 44.1360\n",
      "Epoch 466/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1161 - val_loss: 44.1357\n",
      "Epoch 467/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1163 - val_loss: 44.1352\n",
      "Epoch 468/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1165 - val_loss: 44.1350\n",
      "Epoch 469/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1167 - val_loss: 44.1348\n",
      "Epoch 470/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1169 - val_loss: 44.1346\n",
      "Epoch 471/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1171 - val_loss: 44.1342\n",
      "Epoch 472/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1174 - val_loss: 44.1339\n",
      "Epoch 473/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1176 - val_loss: 44.1336\n",
      "Epoch 474/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1178 - val_loss: 44.1331\n",
      "Epoch 475/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1180 - val_loss: 44.1327\n",
      "Epoch 476/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1182 - val_loss: 44.1324\n",
      "Epoch 477/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1185 - val_loss: 44.1319\n",
      "Epoch 478/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1187 - val_loss: 44.1312\n",
      "Epoch 479/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1189 - val_loss: 44.1310\n",
      "Epoch 480/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1191 - val_loss: 44.1306\n",
      "Epoch 481/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 15.1193 - val_loss: 44.1304\n",
      "Epoch 482/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1195 - val_loss: 44.1301\n",
      "Epoch 483/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1197 - val_loss: 44.1295\n",
      "Epoch 484/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1199 - val_loss: 44.1292\n",
      "Epoch 485/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1201 - val_loss: 44.1287\n",
      "Epoch 486/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1203 - val_loss: 44.1283\n",
      "Epoch 487/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1206 - val_loss: 44.1280\n",
      "Epoch 488/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1208 - val_loss: 44.1277\n",
      "Epoch 489/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1210 - val_loss: 44.1275\n",
      "Epoch 490/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1212 - val_loss: 44.1273\n",
      "Epoch 491/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1214 - val_loss: 44.1270\n",
      "Epoch 492/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1216 - val_loss: 44.1268\n",
      "Epoch 493/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1217 - val_loss: 44.1264\n",
      "Epoch 494/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 15.1219 - val_loss: 44.1259\n",
      "Epoch 495/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1221 - val_loss: 44.1255\n",
      "Epoch 496/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1223 - val_loss: 44.1252\n",
      "Epoch 497/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1225 - val_loss: 44.1249\n",
      "Epoch 498/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1227 - val_loss: 44.1244\n",
      "Epoch 499/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1229 - val_loss: 44.1239\n",
      "Epoch 500/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1231 - val_loss: 44.1236\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(85.7336, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0020, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0125, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3008, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 358ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.58725140e+01, 6.58459034e+01, 6.58192927e+01, 6.57930672e+01,\n",
       "        6.57678571e+01, 6.57426471e+01, 6.57174370e+01, 6.56922269e+01,\n",
       "        6.56670168e+01, 6.56418067e+01, 6.56165966e+01, 6.55913865e+01,\n",
       "        6.55661765e+01, 6.55409664e+01, 6.55157563e+01, 6.54905462e+01,\n",
       "        6.54653361e+01, 6.54401261e+01, 6.54149160e+01, 6.53897059e+01,\n",
       "        6.53644958e+01, 6.53392857e+01, 6.53140756e+01, 6.52888655e+01,\n",
       "        6.52636555e+01, 6.52384454e+01, 6.52132353e+01, 6.51880252e+01,\n",
       "        6.51628151e+01, 6.51376050e+01, 6.51123950e+01, 6.50871849e+01,\n",
       "        6.50619748e+01, 6.50367647e+01, 6.50115546e+01, 6.49863445e+01,\n",
       "        6.49611345e+01, 6.49359244e+01, 6.49107143e+01, 6.48903361e+01,\n",
       "        6.48735294e+01, 6.48567227e+01, 6.48399160e+01, 6.48231092e+01,\n",
       "        6.48063025e+01, 6.47894958e+01, 6.47726891e+01, 6.47558823e+01,\n",
       "        6.47390756e+01, 6.47222689e+01, 6.47054622e+01, 6.46886555e+01,\n",
       "        6.46718487e+01, 6.46550420e+01, 6.46382353e+01, 6.46214286e+01,\n",
       "        6.46046219e+01, 6.45878151e+01, 6.45710084e+01, 6.45542017e+01,\n",
       "        6.45373950e+01, 6.45205882e+01, 6.45037815e+01, 6.44869748e+01,\n",
       "        6.44701681e+01, 6.44533613e+01, 6.44365546e+01, 6.44197479e+01,\n",
       "        6.44029412e+01, 6.43861345e+01, 6.43693277e+01, 6.43525210e+01,\n",
       "        6.43357143e+01, 6.43189076e+01, 6.43021008e+01, 6.42901961e+01,\n",
       "        6.42789916e+01, 6.42677871e+01, 6.42565826e+01, 6.42453781e+01,\n",
       "        7.15446396e+01, 0.00000000e+00, 0.00000000e+00, 5.04386365e-01,\n",
       "        3.56669366e-01, 2.35125870e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.63190544e-01, 9.73631144e-01, 0.00000000e+00,\n",
       "        4.41821933e-01, 0.00000000e+00, 5.59358597e-01, 0.00000000e+00,\n",
       "        3.91087681e-02, 5.72994351e-01, 0.00000000e+00, 3.62969339e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([62.6886788 , 62.68401027, 62.67934174, 62.6746732 , 62.67000467,\n",
       "       62.66533613, 62.6606676 , 62.65599907, 62.65133053, 62.646662  ,\n",
       "       62.64199346, 62.63732493, 62.6326564 , 62.62798786, 62.62331933,\n",
       "       62.61865079, 62.61398226, 62.60931373, 62.60464519, 62.59997666,\n",
       "       62.59530812, 62.59063959, 62.58597106, 62.58130252, 62.57663399,\n",
       "       62.57196545, 62.56729692, 62.56262838, 62.55795985, 62.55329132,\n",
       "       62.54862278, 62.54395425, 62.53928571, 62.53461718, 62.52994865,\n",
       "       62.52528011, 62.52061158, 62.51594304, 62.51127451, 62.50660598,\n",
       "       62.50193744, 62.49726891, 62.49260037, 62.48793184, 62.48326331,\n",
       "       62.47859477, 62.47392624, 62.4692577 , 62.46458917, 62.45992063,\n",
       "       62.4552521 , 62.45058357, 62.44591503, 62.4412465 , 62.43657796,\n",
       "       62.43190943, 62.4272409 , 62.42257236, 62.41790383, 62.41323529,\n",
       "       62.40856676, 62.40389823, 62.39922969, 62.39456116, 62.38989262,\n",
       "       62.38522409, 62.38055556, 62.37588702, 62.37121849, 62.36654995,\n",
       "       62.36188142, 62.35721289, 62.35254435, 62.34787582, 62.34320728,\n",
       "       62.33853875, 62.33387021, 62.32920168, 62.32453315, 62.31986461,\n",
       "       62.31519608, 62.31052754, 62.30585901, 62.30119048, 62.29652194,\n",
       "       62.29185341, 62.28718487, 62.28251634, 62.27784781, 62.27317927,\n",
       "       62.26851074, 62.2638422 , 62.25917367, 62.25450514, 62.2498366 ,\n",
       "       62.24516807, 62.24049953, 62.235831  , 62.23116246, 62.22649393])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.141529652779248\n",
      "13.84288732970914\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
