{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "2295    57.139721\n",
       "2296    57.130703\n",
       "2297    57.121685\n",
       "2298    57.112667\n",
       "2299    57.103648\n",
       "Name: C8, Length: 2300, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c8_interpolated_2200_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "2195     0.000000\n",
       "2196     0.000000\n",
       "2197     0.851952\n",
       "2198     0.000000\n",
       "2199     0.336337\n",
       "Name: C8, Length: 2200, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2200)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.987115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.774230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.561345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.348459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.200000  0.000298\n",
       "1     87.987115  0.000298\n",
       "2     87.774230  0.000297\n",
       "3     87.561345  0.000297\n",
       "4     87.348459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 85.7336  # Value for C0\n",
    "K0 = -0.0020  # Value for K0\n",
    "K1 = 0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0125    # Value for b\n",
    "c = 2.3008    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.200000    87.987115    87.774230    87.561345    87.348459   \n",
      "351    87.987115    87.774230    87.561345    87.348459    87.135574   \n",
      "352    87.774230    87.561345    87.348459    87.135574    86.922689   \n",
      "353    87.561345    87.348459    87.135574    86.922689    86.709804   \n",
      "354    87.348459    87.135574    86.922689    86.709804    86.496919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.135574    86.922689    86.709804    86.496919    86.294538  ...   \n",
      "351    86.922689    86.709804    86.496919    86.294538    86.221709  ...   \n",
      "352    86.709804    86.496919    86.294538    86.221709    86.148880  ...   \n",
      "353    86.496919    86.294538    86.221709    86.148880    86.076050  ...   \n",
      "354    86.294538    86.221709    86.148880    86.076050    86.003221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   76.094351    0.000263   76.058870    0.000263   76.023389    0.000263   \n",
      "351   76.058870    0.000263   76.023389    0.000263   75.987909    0.000262   \n",
      "352   76.023389    0.000263   75.987909    0.000262   75.952428    0.000262   \n",
      "353   75.987909    0.000262   75.952428    0.000262   75.916947    0.000262   \n",
      "354   75.952428    0.000262   75.916947    0.000262   75.881466    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   75.987909    0.000262   75.952428    0.000262  \n",
      "351   75.952428    0.000262   75.916947    0.000262  \n",
      "352   75.916947    0.000262   75.881466    0.000262  \n",
      "353   75.881466    0.000262   75.845985    0.000262  \n",
      "354   75.845985    0.000262   75.810504    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1750, 1, 251) (1750, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 3s 34ms/step - loss: 4606.4365 - val_loss: 2890.0249\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 4408.3750 - val_loss: 2756.6274\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 4291.0947 - val_loss: 2694.8516\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 4199.1621 - val_loss: 2636.9678\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 4109.8125 - val_loss: 2580.9624\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 4022.6558 - val_loss: 2526.4551\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 3937.3557 - val_loss: 2473.1194\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 3853.7251 - val_loss: 2421.5972\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 3771.6514 - val_loss: 2370.9775\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 3691.0601 - val_loss: 2321.5305\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 3611.8965 - val_loss: 2273.2219\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 3534.1196 - val_loss: 2226.0232\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 3457.6943 - val_loss: 2179.9102\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 3382.5925 - val_loss: 2134.8608\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 3308.7878 - val_loss: 2090.8560\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 3236.2578 - val_loss: 2047.8778\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 3164.9817 - val_loss: 2005.9087\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 3094.9392 - val_loss: 1964.9333\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 3026.1125 - val_loss: 1924.9357\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 2958.4832 - val_loss: 1885.9009\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 2892.0354 - val_loss: 1847.8147\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 2826.7522 - val_loss: 1810.6636\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2762.6184 - val_loss: 1774.4327\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 2699.6184 - val_loss: 1739.1099\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 2637.7380 - val_loss: 1704.6809\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 2576.9617 - val_loss: 1671.1338\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2517.2759 - val_loss: 1638.4556\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2458.6658 - val_loss: 1606.6335\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 2401.1194 - val_loss: 1575.6558\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2344.6221 - val_loss: 1545.5096\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 2289.1599 - val_loss: 1516.1830\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2234.7205 - val_loss: 1487.6644\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2181.2913 - val_loss: 1459.9421\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2128.8582 - val_loss: 1433.1761\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2075.2078 - val_loss: 1403.6891\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2018.5645 - val_loss: 1375.4222\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1964.2227 - val_loss: 1348.6307\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1911.8641 - val_loss: 1323.0414\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1861.1150 - val_loss: 1298.5011\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1811.7665 - val_loss: 1274.9188\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1763.6929 - val_loss: 1252.2330\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1716.8093 - val_loss: 1230.3998\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1671.0537 - val_loss: 1209.3834\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1626.3785 - val_loss: 1189.1550\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1582.7443 - val_loss: 1169.6899\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1540.1189 - val_loss: 1150.9662\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1498.4729 - val_loss: 1132.9633\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1457.7809 - val_loss: 1115.6632\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1418.0203 - val_loss: 1099.0493\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1379.1702 - val_loss: 1083.1052\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1341.2108 - val_loss: 1067.8156\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1304.1239 - val_loss: 1053.1656\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 1267.8920 - val_loss: 1039.1418\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1232.4990 - val_loss: 1025.7303\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1197.9291 - val_loss: 1012.9180\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1164.1667 - val_loss: 1000.6916\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1131.1973 - val_loss: 989.0392\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 1099.0068 - val_loss: 977.9481\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1067.5819 - val_loss: 967.4066\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1036.9087 - val_loss: 957.4026\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1006.9742 - val_loss: 947.9249\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 977.7656 - val_loss: 938.9616\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 949.2702 - val_loss: 930.5016\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 921.4753 - val_loss: 922.5338\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 894.3696 - val_loss: 915.0474\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 867.9406 - val_loss: 908.0312\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 842.1766 - val_loss: 901.4747\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 817.0664 - val_loss: 895.3675\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 792.5986 - val_loss: 889.6987\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 768.7614 - val_loss: 884.4581\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 745.5441 - val_loss: 879.6354\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 722.9356 - val_loss: 875.2205\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 700.9252 - val_loss: 871.2031\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 679.5018 - val_loss: 867.5735\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 658.6551 - val_loss: 864.3214\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 638.3744 - val_loss: 861.4374\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 618.6497 - val_loss: 858.9116\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 599.4701 - val_loss: 856.7342\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 580.8259 - val_loss: 854.8958\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 562.7067 - val_loss: 853.3868\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 545.1027 - val_loss: 852.1981\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 528.0038 - val_loss: 851.3198\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 511.4005 - val_loss: 850.7433\n",
      "Epoch 84/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 495.2830 - val_loss: 850.4590\n",
      "Epoch 85/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 479.6416 - val_loss: 850.4579\n",
      "Epoch 86/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 464.4671 - val_loss: 850.7313\n",
      "Epoch 87/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 449.7495 - val_loss: 851.2700\n",
      "Epoch 88/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 435.4797 - val_loss: 852.0650\n",
      "Epoch 89/500\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 421.6483 - val_loss: 853.1080\n",
      "Epoch 90/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 408.2463 - val_loss: 854.3900\n",
      "Epoch 91/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 395.2644 - val_loss: 855.9025\n",
      "Epoch 92/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 382.6938 - val_loss: 857.6370\n",
      "Epoch 93/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 370.5255 - val_loss: 859.5851\n",
      "Epoch 94/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 358.7507 - val_loss: 861.7383\n",
      "Epoch 95/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 347.3607 - val_loss: 864.0889\n",
      "Epoch 96/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 336.3465 - val_loss: 866.6281\n",
      "Epoch 97/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 325.6998 - val_loss: 869.3481\n",
      "Epoch 98/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 315.4121 - val_loss: 872.2409\n",
      "Epoch 99/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 305.4749 - val_loss: 875.2986\n",
      "Epoch 100/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 295.8798 - val_loss: 878.5137\n",
      "Epoch 101/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 286.6187 - val_loss: 881.8784\n",
      "Epoch 102/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 277.6833 - val_loss: 885.3848\n",
      "Epoch 103/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 269.0657 - val_loss: 889.0258\n",
      "Epoch 104/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 260.7578 - val_loss: 892.7936\n",
      "Epoch 105/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 252.7520 - val_loss: 896.6815\n",
      "Epoch 106/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 245.0402 - val_loss: 900.6819\n",
      "Epoch 107/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 237.6148 - val_loss: 904.7878\n",
      "Epoch 108/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 230.4684 - val_loss: 908.9924\n",
      "Epoch 109/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 223.5934 - val_loss: 913.2888\n",
      "Epoch 110/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 216.9821 - val_loss: 917.6705\n",
      "Epoch 111/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 210.6274 - val_loss: 922.1304\n",
      "Epoch 112/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 204.5223 - val_loss: 926.6623\n",
      "Epoch 113/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 198.6595 - val_loss: 931.2597\n",
      "Epoch 114/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 193.0322 - val_loss: 935.9167\n",
      "Epoch 115/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 187.6332 - val_loss: 940.6268\n",
      "Epoch 116/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 182.4559 - val_loss: 945.3841\n",
      "Epoch 117/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 177.4936 - val_loss: 950.1829\n",
      "Epoch 118/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 172.7396 - val_loss: 955.0173\n",
      "Epoch 119/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 168.1875 - val_loss: 959.8818\n",
      "Epoch 120/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 163.8309 - val_loss: 964.7705\n",
      "Epoch 121/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 159.6636 - val_loss: 969.6790\n",
      "Epoch 122/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 155.6794 - val_loss: 974.6013\n",
      "Epoch 123/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 151.8723 - val_loss: 979.5325\n",
      "Epoch 124/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 148.2363 - val_loss: 984.4678\n",
      "Epoch 125/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 144.7655 - val_loss: 989.4022\n",
      "Epoch 126/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 141.4545 - val_loss: 994.3314\n",
      "Epoch 127/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 138.2974 - val_loss: 999.2507\n",
      "Epoch 128/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 135.2888 - val_loss: 1004.1559\n",
      "Epoch 129/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 132.4235 - val_loss: 1009.0422\n",
      "Epoch 130/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 129.6960 - val_loss: 1013.9062\n",
      "Epoch 131/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 127.1014 - val_loss: 1018.7441\n",
      "Epoch 132/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 124.6344 - val_loss: 1023.5518\n",
      "Epoch 133/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 122.2903 - val_loss: 1028.3253\n",
      "Epoch 134/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 120.0644 - val_loss: 1033.0619\n",
      "Epoch 135/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 117.9518 - val_loss: 1037.7581\n",
      "Epoch 136/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 115.9481 - val_loss: 1042.4100\n",
      "Epoch 137/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 114.0488 - val_loss: 1047.0154\n",
      "Epoch 138/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 112.2497 - val_loss: 1051.5713\n",
      "Epoch 139/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 110.5465 - val_loss: 1056.0745\n",
      "Epoch 140/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 108.9351 - val_loss: 1060.5231\n",
      "Epoch 141/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 107.4116 - val_loss: 1064.9138\n",
      "Epoch 142/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 105.9721 - val_loss: 1069.2458\n",
      "Epoch 143/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 104.6129 - val_loss: 1073.5150\n",
      "Epoch 144/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 103.3303 - val_loss: 1077.7209\n",
      "Epoch 145/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 102.1208 - val_loss: 1081.8608\n",
      "Epoch 146/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 100.9811 - val_loss: 1085.9338\n",
      "Epoch 147/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 99.9078 - val_loss: 1089.9376\n",
      "Epoch 148/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 98.8977 - val_loss: 1093.8712\n",
      "Epoch 149/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 97.9478 - val_loss: 1097.7329\n",
      "Epoch 150/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 97.0552 - val_loss: 1101.5216\n",
      "Epoch 151/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 96.2170 - val_loss: 1105.2365\n",
      "Epoch 152/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 95.4303 - val_loss: 1108.8770\n",
      "Epoch 153/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 94.6926 - val_loss: 1112.4413\n",
      "Epoch 154/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 94.0012 - val_loss: 1115.9297\n",
      "Epoch 155/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 93.3539 - val_loss: 1119.3419\n",
      "Epoch 156/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.7480 - val_loss: 1122.6760\n",
      "Epoch 157/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.1815 - val_loss: 1125.9332\n",
      "Epoch 158/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 91.6522 - val_loss: 1129.1123\n",
      "Epoch 159/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 91.1579 - val_loss: 1132.2134\n",
      "Epoch 160/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 90.6967 - val_loss: 1135.2378\n",
      "Epoch 161/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 90.2668 - val_loss: 1138.1835\n",
      "Epoch 162/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 89.8662 - val_loss: 1141.0515\n",
      "Epoch 163/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 89.4932 - val_loss: 1143.8428\n",
      "Epoch 164/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 89.1463 - val_loss: 1146.5571\n",
      "Epoch 165/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8237 - val_loss: 1149.1957\n",
      "Epoch 166/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.5241 - val_loss: 1151.7585\n",
      "Epoch 167/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.2459 - val_loss: 1154.2458\n",
      "Epoch 168/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 87.9880 - val_loss: 1156.6588\n",
      "Epoch 169/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 87.7489 - val_loss: 1158.9982\n",
      "Epoch 170/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 87.5274 - val_loss: 1161.2648\n",
      "Epoch 171/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 87.3224 - val_loss: 1163.4602\n",
      "Epoch 172/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 87.1328 - val_loss: 1165.5845\n",
      "Epoch 173/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.9575 - val_loss: 1167.6390\n",
      "Epoch 174/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.7957 - val_loss: 1169.6246\n",
      "Epoch 175/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.6463 - val_loss: 1171.5425\n",
      "Epoch 176/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.5087 - val_loss: 1173.3940\n",
      "Epoch 177/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.3817 - val_loss: 1175.1803\n",
      "Epoch 178/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.2648 - val_loss: 1176.9027\n",
      "Epoch 179/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.1573 - val_loss: 1178.5626\n",
      "Epoch 180/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 86.0584 - val_loss: 1180.1606\n",
      "Epoch 181/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.9675 - val_loss: 1181.6993\n",
      "Epoch 182/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.8839 - val_loss: 1183.1788\n",
      "Epoch 183/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.8073 - val_loss: 1184.6011\n",
      "Epoch 184/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.7370 - val_loss: 1185.9675\n",
      "Epoch 185/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.6726 - val_loss: 1187.2791\n",
      "Epoch 186/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.6135 - val_loss: 1188.5378\n",
      "Epoch 187/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.5595 - val_loss: 1189.7449\n",
      "Epoch 188/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 85.5100 - val_loss: 1190.9017\n",
      "Epoch 189/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.4648 - val_loss: 1192.0095\n",
      "Epoch 190/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.4235 - val_loss: 1193.0699\n",
      "Epoch 191/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.3857 - val_loss: 1194.0837\n",
      "Epoch 192/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.3513 - val_loss: 1195.0529\n",
      "Epoch 193/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.3199 - val_loss: 1195.9789\n",
      "Epoch 194/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.2913 - val_loss: 1196.8641\n",
      "Epoch 195/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.2651 - val_loss: 1197.7079\n",
      "Epoch 196/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.2413 - val_loss: 1198.5129\n",
      "Epoch 197/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 85.2196 - val_loss: 1199.2804\n",
      "Epoch 198/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.2000 - val_loss: 1200.0105\n",
      "Epoch 199/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.1821 - val_loss: 1200.7063\n",
      "Epoch 200/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.1658 - val_loss: 1201.3676\n",
      "Epoch 201/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.1510 - val_loss: 1201.9965\n",
      "Epoch 202/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.1375 - val_loss: 1202.5941\n",
      "Epoch 203/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.1254 - val_loss: 1203.1609\n",
      "Epoch 204/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.1143 - val_loss: 1203.6993\n",
      "Epoch 205/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.1043 - val_loss: 1204.2098\n",
      "Epoch 206/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0952 - val_loss: 1204.6934\n",
      "Epoch 207/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0870 - val_loss: 1205.1517\n",
      "Epoch 208/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0795 - val_loss: 1205.5852\n",
      "Epoch 209/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0729 - val_loss: 1205.9955\n",
      "Epoch 210/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0668 - val_loss: 1206.3834\n",
      "Epoch 211/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0613 - val_loss: 1206.7496\n",
      "Epoch 212/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0564 - val_loss: 1207.0951\n",
      "Epoch 213/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 85.0520 - val_loss: 1207.4218\n",
      "Epoch 214/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 85.0480 - val_loss: 1207.7292\n",
      "Epoch 215/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0444 - val_loss: 1208.0192\n",
      "Epoch 216/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 85.0412 - val_loss: 1208.2910\n",
      "Epoch 217/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0384 - val_loss: 1208.5487\n",
      "Epoch 218/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0359 - val_loss: 1208.7904\n",
      "Epoch 219/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0336 - val_loss: 1209.0173\n",
      "Epoch 220/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0316 - val_loss: 1209.2303\n",
      "Epoch 221/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0299 - val_loss: 1209.4313\n",
      "Epoch 222/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 85.0283 - val_loss: 1209.6195\n",
      "Epoch 223/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0270 - val_loss: 1209.7953\n",
      "Epoch 224/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 85.0258 - val_loss: 1209.9598\n",
      "Epoch 225/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0248 - val_loss: 1210.1140\n",
      "Epoch 226/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 85.0239 - val_loss: 1210.2578\n",
      "Epoch 227/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0233 - val_loss: 1210.3932\n",
      "Epoch 228/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0227 - val_loss: 1210.5194\n",
      "Epoch 229/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 85.0223 - val_loss: 1210.6371\n",
      "Epoch 230/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0219 - val_loss: 1210.7465\n",
      "Epoch 231/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0216 - val_loss: 1210.8485\n",
      "Epoch 232/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0215 - val_loss: 1210.9441\n",
      "Epoch 233/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0214 - val_loss: 1211.0327\n",
      "Epoch 234/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0214 - val_loss: 1211.1149\n",
      "Epoch 235/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0215 - val_loss: 1211.1917\n",
      "Epoch 236/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0216 - val_loss: 1211.2628\n",
      "Epoch 237/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0217 - val_loss: 1211.3290\n",
      "Epoch 238/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0220 - val_loss: 1211.3898\n",
      "Epoch 239/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0223 - val_loss: 1211.4464\n",
      "Epoch 240/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0226 - val_loss: 1211.4988\n",
      "Epoch 241/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0230 - val_loss: 1211.5476\n",
      "Epoch 242/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0234 - val_loss: 1211.5922\n",
      "Epoch 243/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0238 - val_loss: 1211.6328\n",
      "Epoch 244/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0244 - val_loss: 1211.6715\n",
      "Epoch 245/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0248 - val_loss: 1211.7057\n",
      "Epoch 246/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0253 - val_loss: 1211.7379\n",
      "Epoch 247/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0259 - val_loss: 1211.7675\n",
      "Epoch 248/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 85.0265 - val_loss: 1211.7939\n",
      "Epoch 249/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 85.0271 - val_loss: 1211.8186\n",
      "Epoch 250/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 85.0277 - val_loss: 1211.8411\n",
      "Epoch 251/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0283 - val_loss: 1211.8625\n",
      "Epoch 252/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0290 - val_loss: 1211.8812\n",
      "Epoch 253/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0296 - val_loss: 1211.8984\n",
      "Epoch 254/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0304 - val_loss: 1211.9137\n",
      "Epoch 255/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 85.0310 - val_loss: 1211.9286\n",
      "Epoch 256/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0317 - val_loss: 1211.9410\n",
      "Epoch 257/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 85.0324 - val_loss: 1211.9526\n",
      "Epoch 258/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0331 - val_loss: 1211.9633\n",
      "Epoch 259/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0338 - val_loss: 1211.9723\n",
      "Epoch 260/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0345 - val_loss: 1211.9806\n",
      "Epoch 261/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0352 - val_loss: 1211.9883\n",
      "Epoch 262/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0360 - val_loss: 1211.9952\n",
      "Epoch 263/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0367 - val_loss: 1212.0009\n",
      "Epoch 264/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0374 - val_loss: 1212.0061\n",
      "Epoch 265/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0382 - val_loss: 1212.0110\n",
      "Epoch 266/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0388 - val_loss: 1212.0144\n",
      "Epoch 267/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0396 - val_loss: 1212.0182\n",
      "Epoch 268/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0403 - val_loss: 1212.0205\n",
      "Epoch 269/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0410 - val_loss: 1212.0237\n",
      "Epoch 270/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0417 - val_loss: 1212.0254\n",
      "Epoch 271/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0424 - val_loss: 1212.0265\n",
      "Epoch 272/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0432 - val_loss: 1212.0278\n",
      "Epoch 273/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 85.0439 - val_loss: 1212.0293\n",
      "Epoch 274/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 85.0445 - val_loss: 1212.0299\n",
      "Epoch 275/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0452 - val_loss: 1212.0300\n",
      "Epoch 276/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0459 - val_loss: 1212.0302\n",
      "Epoch 277/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0466 - val_loss: 1212.0298\n",
      "Epoch 278/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 85.0473 - val_loss: 1212.0297\n",
      "Epoch 279/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0480 - val_loss: 1212.0294\n",
      "Epoch 280/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0487 - val_loss: 1212.0281\n",
      "Epoch 281/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0494 - val_loss: 1212.0277\n",
      "Epoch 282/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0500 - val_loss: 1212.0271\n",
      "Epoch 283/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0507 - val_loss: 1212.0259\n",
      "Epoch 284/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0513 - val_loss: 1212.0251\n",
      "Epoch 285/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0519 - val_loss: 1212.0239\n",
      "Epoch 286/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0526 - val_loss: 1212.0220\n",
      "Epoch 287/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0533 - val_loss: 1212.0209\n",
      "Epoch 288/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0539 - val_loss: 1212.0201\n",
      "Epoch 289/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 85.0545 - val_loss: 1212.0187\n",
      "Epoch 290/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0551 - val_loss: 1212.0170\n",
      "Epoch 291/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0557 - val_loss: 1212.0157\n",
      "Epoch 292/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0563 - val_loss: 1212.0138\n",
      "Epoch 293/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0569 - val_loss: 1212.0121\n",
      "Epoch 294/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0575 - val_loss: 1212.0107\n",
      "Epoch 295/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0580 - val_loss: 1212.0089\n",
      "Epoch 296/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0586 - val_loss: 1212.0074\n",
      "Epoch 297/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 85.0592 - val_loss: 1212.0059\n",
      "Epoch 298/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 85.0597 - val_loss: 1212.0037\n",
      "Epoch 299/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0603 - val_loss: 1212.0023\n",
      "Epoch 300/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0608 - val_loss: 1212.0009\n",
      "Epoch 301/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0613 - val_loss: 1211.9983\n",
      "Epoch 302/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0619 - val_loss: 1211.9972\n",
      "Epoch 303/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0624 - val_loss: 1211.9954\n",
      "Epoch 304/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0629 - val_loss: 1211.9941\n",
      "Epoch 305/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0634 - val_loss: 1211.9919\n",
      "Epoch 306/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0639 - val_loss: 1211.9907\n",
      "Epoch 307/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0644 - val_loss: 1211.9891\n",
      "Epoch 308/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0649 - val_loss: 1211.9875\n",
      "Epoch 309/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0654 - val_loss: 1211.9856\n",
      "Epoch 310/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0659 - val_loss: 1211.9845\n",
      "Epoch 311/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0663 - val_loss: 1211.9829\n",
      "Epoch 312/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0667 - val_loss: 1211.9812\n",
      "Epoch 313/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 85.0672 - val_loss: 1211.9795\n",
      "Epoch 314/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0676 - val_loss: 1211.9779\n",
      "Epoch 315/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0681 - val_loss: 1211.9760\n",
      "Epoch 316/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0685 - val_loss: 1211.9746\n",
      "Epoch 317/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0689 - val_loss: 1211.9733\n",
      "Epoch 318/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0694 - val_loss: 1211.9725\n",
      "Epoch 319/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0697 - val_loss: 1211.9711\n",
      "Epoch 320/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0701 - val_loss: 1211.9691\n",
      "Epoch 321/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0705 - val_loss: 1211.9679\n",
      "Epoch 322/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0709 - val_loss: 1211.9662\n",
      "Epoch 323/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0714 - val_loss: 1211.9656\n",
      "Epoch 324/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0717 - val_loss: 1211.9634\n",
      "Epoch 325/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0720 - val_loss: 1211.9619\n",
      "Epoch 326/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 85.0725 - val_loss: 1211.9609\n",
      "Epoch 327/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0728 - val_loss: 1211.9598\n",
      "Epoch 328/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0731 - val_loss: 1211.9586\n",
      "Epoch 329/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0735 - val_loss: 1211.9570\n",
      "Epoch 330/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0738 - val_loss: 1211.9557\n",
      "Epoch 331/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0741 - val_loss: 1211.9543\n",
      "Epoch 332/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0745 - val_loss: 1211.9537\n",
      "Epoch 333/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0748 - val_loss: 1211.9525\n",
      "Epoch 334/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0751 - val_loss: 1211.9513\n",
      "Epoch 335/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0755 - val_loss: 1211.9506\n",
      "Epoch 336/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 85.0758 - val_loss: 1211.9496\n",
      "Epoch 337/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 85.0761 - val_loss: 1211.9490\n",
      "Epoch 338/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 85.0763 - val_loss: 1211.9480\n",
      "Epoch 339/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 85.0766 - val_loss: 1211.9468\n",
      "Epoch 340/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 85.0770 - val_loss: 1211.9458\n",
      "Epoch 341/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 85.0772 - val_loss: 1211.9449\n",
      "Epoch 342/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 85.0774 - val_loss: 1211.9436\n",
      "Epoch 343/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0777 - val_loss: 1211.9420\n",
      "Epoch 344/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0780 - val_loss: 1211.9413\n",
      "Epoch 345/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0782 - val_loss: 1211.9402\n",
      "Epoch 346/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0785 - val_loss: 1211.9392\n",
      "Epoch 347/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0787 - val_loss: 1211.9380\n",
      "Epoch 348/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0790 - val_loss: 1211.9371\n",
      "Epoch 349/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0792 - val_loss: 1211.9364\n",
      "Epoch 350/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0795 - val_loss: 1211.9359\n",
      "Epoch 351/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0797 - val_loss: 1211.9344\n",
      "Epoch 352/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0800 - val_loss: 1211.9342\n",
      "Epoch 353/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0802 - val_loss: 1211.9333\n",
      "Epoch 354/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0804 - val_loss: 1211.9326\n",
      "Epoch 355/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0805 - val_loss: 1211.9318\n",
      "Epoch 356/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0808 - val_loss: 1211.9310\n",
      "Epoch 357/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0810 - val_loss: 1211.9307\n",
      "Epoch 358/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 85.0812 - val_loss: 1211.9293\n",
      "Epoch 359/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 85.0814 - val_loss: 1211.9287\n",
      "Epoch 360/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0816 - val_loss: 1211.9280\n",
      "Epoch 361/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0818 - val_loss: 1211.9275\n",
      "Epoch 362/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0820 - val_loss: 1211.9268\n",
      "Epoch 363/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0822 - val_loss: 1211.9264\n",
      "Epoch 364/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0824 - val_loss: 1211.9260\n",
      "Epoch 365/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0825 - val_loss: 1211.9253\n",
      "Epoch 366/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0826 - val_loss: 1211.9243\n",
      "Epoch 367/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0829 - val_loss: 1211.9240\n",
      "Epoch 368/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0830 - val_loss: 1211.9233\n",
      "Epoch 369/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0832 - val_loss: 1211.9227\n",
      "Epoch 370/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0834 - val_loss: 1211.9222\n",
      "Epoch 371/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0835 - val_loss: 1211.9218\n",
      "Epoch 372/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0837 - val_loss: 1211.9210\n",
      "Epoch 373/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0838 - val_loss: 1211.9207\n",
      "Epoch 374/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0840 - val_loss: 1211.9198\n",
      "Epoch 375/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0841 - val_loss: 1211.9197\n",
      "Epoch 376/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0842 - val_loss: 1211.9188\n",
      "Epoch 377/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0844 - val_loss: 1211.9187\n",
      "Epoch 378/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0845 - val_loss: 1211.9181\n",
      "Epoch 379/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0846 - val_loss: 1211.9171\n",
      "Epoch 380/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 85.0848 - val_loss: 1211.9166\n",
      "Epoch 381/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0849 - val_loss: 1211.9156\n",
      "Epoch 382/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0850 - val_loss: 1211.9152\n",
      "Epoch 383/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0851 - val_loss: 1211.9149\n",
      "Epoch 384/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0853 - val_loss: 1211.9144\n",
      "Epoch 385/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0854 - val_loss: 1211.9142\n",
      "Epoch 386/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0855 - val_loss: 1211.9138\n",
      "Epoch 387/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0856 - val_loss: 1211.9127\n",
      "Epoch 388/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0857 - val_loss: 1211.9124\n",
      "Epoch 389/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0859 - val_loss: 1211.9117\n",
      "Epoch 390/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 85.0860 - val_loss: 1211.9114\n",
      "Epoch 391/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0861 - val_loss: 1211.9113\n",
      "Epoch 392/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0862 - val_loss: 1211.9109\n",
      "Epoch 393/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0863 - val_loss: 1211.9104\n",
      "Epoch 394/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0864 - val_loss: 1211.9102\n",
      "Epoch 395/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0865 - val_loss: 1211.9097\n",
      "Epoch 396/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0866 - val_loss: 1211.9097\n",
      "Epoch 397/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0867 - val_loss: 1211.9097\n",
      "Epoch 398/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0867 - val_loss: 1211.9093\n",
      "Epoch 399/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0869 - val_loss: 1211.9094\n",
      "Epoch 400/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0869 - val_loss: 1211.9094\n",
      "Epoch 401/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 85.0870 - val_loss: 1211.9094\n",
      "Epoch 402/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 85.0871 - val_loss: 1211.9086\n",
      "Epoch 403/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0871 - val_loss: 1211.9081\n",
      "Epoch 404/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0872 - val_loss: 1211.9080\n",
      "Epoch 405/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0873 - val_loss: 1211.9077\n",
      "Epoch 406/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0874 - val_loss: 1211.9075\n",
      "Epoch 407/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0875 - val_loss: 1211.9075\n",
      "Epoch 408/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0875 - val_loss: 1211.9071\n",
      "Epoch 409/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0876 - val_loss: 1211.9062\n",
      "Epoch 410/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 85.0877 - val_loss: 1211.9061\n",
      "Epoch 411/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0877 - val_loss: 1211.9058\n",
      "Epoch 412/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0878 - val_loss: 1211.9055\n",
      "Epoch 413/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0879 - val_loss: 1211.9050\n",
      "Epoch 414/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0880 - val_loss: 1211.9054\n",
      "Epoch 415/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 85.0880 - val_loss: 1211.9048\n",
      "Epoch 416/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0881 - val_loss: 1211.9043\n",
      "Epoch 417/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0882 - val_loss: 1211.9044\n",
      "Epoch 418/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0882 - val_loss: 1211.9048\n",
      "Epoch 419/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0882 - val_loss: 1211.9048\n",
      "Epoch 420/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0883 - val_loss: 1211.9045\n",
      "Epoch 421/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0884 - val_loss: 1211.9045\n",
      "Epoch 422/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0884 - val_loss: 1211.9043\n",
      "Epoch 423/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 85.0885 - val_loss: 1211.9044\n",
      "Epoch 424/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 85.0886 - val_loss: 1211.9048\n",
      "Epoch 425/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0886 - val_loss: 1211.9048\n",
      "Epoch 426/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0886 - val_loss: 1211.9043\n",
      "Epoch 427/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0886 - val_loss: 1211.9041\n",
      "Epoch 428/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0887 - val_loss: 1211.9041\n",
      "Epoch 429/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0888 - val_loss: 1211.9038\n",
      "Epoch 430/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0888 - val_loss: 1211.9034\n",
      "Epoch 431/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0889 - val_loss: 1211.9032\n",
      "Epoch 432/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0889 - val_loss: 1211.9031\n",
      "Epoch 433/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0890 - val_loss: 1211.9032\n",
      "Epoch 434/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0890 - val_loss: 1211.9030\n",
      "Epoch 435/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0890 - val_loss: 1211.9027\n",
      "Epoch 436/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0890 - val_loss: 1211.9017\n",
      "Epoch 437/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0891 - val_loss: 1211.9017\n",
      "Epoch 438/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0892 - val_loss: 1211.9022\n",
      "Epoch 439/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0892 - val_loss: 1211.9027\n",
      "Epoch 440/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0892 - val_loss: 1211.9025\n",
      "Epoch 441/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0892 - val_loss: 1211.9021\n",
      "Epoch 442/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0893 - val_loss: 1211.9017\n",
      "Epoch 443/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0893 - val_loss: 1211.9021\n",
      "Epoch 444/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 85.0894 - val_loss: 1211.9020\n",
      "Epoch 445/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 85.0894 - val_loss: 1211.9021\n",
      "Epoch 446/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 85.0894 - val_loss: 1211.9017\n",
      "Epoch 447/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0894 - val_loss: 1211.9012\n",
      "Epoch 448/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0894 - val_loss: 1211.9009\n",
      "Epoch 449/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0895 - val_loss: 1211.9008\n",
      "Epoch 450/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0896 - val_loss: 1211.9008\n",
      "Epoch 451/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0896 - val_loss: 1211.9008\n",
      "Epoch 452/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0896 - val_loss: 1211.9008\n",
      "Epoch 453/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 85.0896 - val_loss: 1211.9005\n",
      "Epoch 454/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 85.0896 - val_loss: 1211.8994\n",
      "Epoch 455/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 85.0896 - val_loss: 1211.8990\n",
      "Epoch 456/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0897 - val_loss: 1211.8988\n",
      "Epoch 457/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0898 - val_loss: 1211.8984\n",
      "Epoch 458/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0898 - val_loss: 1211.8988\n",
      "Epoch 459/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0898 - val_loss: 1211.8988\n",
      "Epoch 460/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0898 - val_loss: 1211.8984\n",
      "Epoch 461/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0899 - val_loss: 1211.8984\n",
      "Epoch 462/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0899 - val_loss: 1211.8988\n",
      "Epoch 463/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0899 - val_loss: 1211.8987\n",
      "Epoch 464/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 85.0899 - val_loss: 1211.8986\n",
      "Epoch 465/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 85.0899 - val_loss: 1211.8983\n",
      "Epoch 466/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0900 - val_loss: 1211.8988\n",
      "Epoch 467/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0900 - val_loss: 1211.8990\n",
      "Epoch 468/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0900 - val_loss: 1211.8990\n",
      "Epoch 469/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 85.0900 - val_loss: 1211.8988\n",
      "Epoch 470/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0901 - val_loss: 1211.8988\n",
      "Epoch 471/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0901 - val_loss: 1211.8988\n",
      "Epoch 472/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0901 - val_loss: 1211.8984\n",
      "Epoch 473/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0900 - val_loss: 1211.8978\n",
      "Epoch 474/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0900 - val_loss: 1211.8973\n",
      "Epoch 475/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 85.0900 - val_loss: 1211.8965\n",
      "Epoch 476/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0901 - val_loss: 1211.8955\n",
      "Epoch 477/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0902 - val_loss: 1211.8955\n",
      "Epoch 478/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0902 - val_loss: 1211.8962\n",
      "Epoch 479/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0902 - val_loss: 1211.8964\n",
      "Epoch 480/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0902 - val_loss: 1211.8965\n",
      "Epoch 481/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0902 - val_loss: 1211.8967\n",
      "Epoch 482/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0902 - val_loss: 1211.8964\n",
      "Epoch 483/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 85.0902 - val_loss: 1211.8962\n",
      "Epoch 484/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0903 - val_loss: 1211.8962\n",
      "Epoch 485/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0903 - val_loss: 1211.8962\n",
      "Epoch 486/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0903 - val_loss: 1211.8962\n",
      "Epoch 487/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0903 - val_loss: 1211.8959\n",
      "Epoch 488/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0903 - val_loss: 1211.8959\n",
      "Epoch 489/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0903 - val_loss: 1211.8960\n",
      "Epoch 490/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0904 - val_loss: 1211.8960\n",
      "Epoch 491/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0904 - val_loss: 1211.8965\n",
      "Epoch 492/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0904 - val_loss: 1211.8966\n",
      "Epoch 493/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0904 - val_loss: 1211.8969\n",
      "Epoch 494/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0904 - val_loss: 1211.8970\n",
      "Epoch 495/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0904 - val_loss: 1211.8972\n",
      "Epoch 496/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0904 - val_loss: 1211.8972\n",
      "Epoch 497/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0904 - val_loss: 1211.8972\n",
      "Epoch 498/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0904 - val_loss: 1211.8971\n",
      "Epoch 499/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0904 - val_loss: 1211.8967\n",
      "Epoch 500/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 85.0904 - val_loss: 1211.8962\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(85.7336, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0020, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0125, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3008, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 427ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.40735761e+01, 6.40399627e+01, 6.40063492e+01, 6.39727358e+01,\n",
       "        6.39391223e+01, 6.39055089e+01, 6.38156863e+01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.55659880e-01,\n",
       "        0.00000000e+00, 6.44253501e+01, 6.43749300e+01, 6.43245098e+01,\n",
       "        6.42827264e+01, 6.42491130e+01, 6.42154995e+01, 6.41818861e+01,\n",
       "        6.41482726e+01, 6.41146592e+01, 6.40808458e+01, 6.40474323e+01,\n",
       "        6.40138189e+01, 6.39802054e+01, 6.39465920e+01, 6.39129785e+01,\n",
       "        6.38380952e+01, 6.66264823e+01, 6.63869865e+01, 6.61474907e+01,\n",
       "        6.59079949e+01, 6.56754202e+01, 6.54485294e+01, 6.52216387e+01,\n",
       "        6.49947479e+01, 6.48119048e+01, 6.18574840e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.03691721e-01, 0.00000000e+00, 6.39876751e+01,\n",
       "        6.39540616e+01, 6.39204482e+01, 6.38605042e+01, 6.66797035e+01,\n",
       "        6.64402077e+01, 6.62007119e+01, 6.59612161e+01, 6.57258403e+01,\n",
       "        6.54998496e+01, 6.52720588e+01, 6.50451681e+01, 6.48455182e+01,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.58192927e+01, 6.55913865e+01,\n",
       "        6.53644958e+01, 6.51376050e+01, 6.49107143e+01, 6.47558823e+01,\n",
       "        6.46036219e+01, 6.44533613e+01, 6.43021008e+01, 5.04386365e-01,\n",
       "        4.41821933e-01, 4.89203835e+01, 8.30763102e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.29907712e-01, 1.36335313e-01,\n",
       "        6.35700455e+01, 0.00000000e+00, 4.65473950e-01, 1.63792036e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.55069226e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.39390180e-01, 4.81500268e-01, 3.36926967e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.77997667e-01,\n",
       "        5.07665277e-01, 5.06889880e-01, 6.42051637e-01, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([57.99645419, 57.98743595, 57.97841771, 57.96939947, 57.96038123,\n",
       "       57.95136298, 57.94234474, 57.9333265 , 57.92430826, 57.91529002,\n",
       "       57.90627178, 57.89725354, 57.88823529, 57.87921705, 57.87019881,\n",
       "       57.86118057, 57.85216233, 57.84314409, 57.83412585, 57.8251076 ,\n",
       "       57.81608936, 57.80707112, 57.79805288, 57.78903464, 57.7800164 ,\n",
       "       57.77099816, 57.76197991, 57.75296167, 57.74394343, 57.73492519,\n",
       "       57.72590695, 57.71688871, 57.70787047, 57.69885222, 57.68983398,\n",
       "       57.68081574, 57.6717975 , 57.66277926, 57.65376102, 57.64474278,\n",
       "       57.63572453, 57.62670629, 57.61768805, 57.60866981, 57.59965157,\n",
       "       57.59063333, 57.58161509, 57.57259684, 57.5635786 , 57.55456036,\n",
       "       57.54554212, 57.53652388, 57.52750564, 57.51848739, 57.50946915,\n",
       "       57.50045091, 57.49143267, 57.48241443, 57.47339619, 57.46437795,\n",
       "       57.4553597 , 57.44634146, 57.43732322, 57.42830498, 57.41928674,\n",
       "       57.4102685 , 57.40125026, 57.39223201, 57.38321377, 57.37419553,\n",
       "       57.36517729, 57.35615905, 57.34714081, 57.33812257, 57.32910432,\n",
       "       57.32008608, 57.31106784, 57.3020496 , 57.29303136, 57.28401312,\n",
       "       57.27499488, 57.26597663, 57.25695839, 57.24794015, 57.23892191,\n",
       "       57.22990367, 57.22088543, 57.21186719, 57.20284894, 57.1938307 ,\n",
       "       57.18481246, 57.17579422, 57.16677598, 57.15775774, 57.1487395 ,\n",
       "       57.13972125, 57.13070301, 57.12168477, 57.11266653, 57.10364829])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.36191687490438\n",
      "29.173288265472046\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
