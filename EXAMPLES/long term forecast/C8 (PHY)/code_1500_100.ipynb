{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "1595    62.728852\n",
       "1596    62.717647\n",
       "1597    62.706443\n",
       "1598    62.698016\n",
       "1599    62.693347\n",
       "Name: C8, Length: 1600, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c8_interpolated_1500_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "1495    63.849300\n",
       "1496    63.838095\n",
       "1497    63.826891\n",
       "1498    63.815686\n",
       "1499    63.804482\n",
       "Name: C8, Length: 1500, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1500)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.987115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.774230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.561345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.348459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.200000  0.000298\n",
       "1     87.987115  0.000298\n",
       "2     87.774230  0.000297\n",
       "3     87.561345  0.000297\n",
       "4     87.348459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 85.7336  # Value for C0\n",
    "K0 = -0.0020  # Value for K0\n",
    "K1 = 0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0125    # Value for b\n",
    "c = 2.3008    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.200000    87.987115    87.774230    87.561345    87.348459   \n",
      "351    87.987115    87.774230    87.561345    87.348459    87.135574   \n",
      "352    87.774230    87.561345    87.348459    87.135574    86.922689   \n",
      "353    87.561345    87.348459    87.135574    86.922689    86.709804   \n",
      "354    87.348459    87.135574    86.922689    86.709804    86.496919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.135574    86.922689    86.709804    86.496919    86.294538  ...   \n",
      "351    86.922689    86.709804    86.496919    86.294538    86.221709  ...   \n",
      "352    86.709804    86.496919    86.294538    86.221709    86.148880  ...   \n",
      "353    86.496919    86.294538    86.221709    86.148880    86.076050  ...   \n",
      "354    86.294538    86.221709    86.148880    86.076050    86.003221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   76.094351    0.000263   76.058870    0.000263   76.023389    0.000263   \n",
      "351   76.058870    0.000263   76.023389    0.000263   75.987909    0.000262   \n",
      "352   76.023389    0.000263   75.987909    0.000262   75.952428    0.000262   \n",
      "353   75.987909    0.000262   75.952428    0.000262   75.916947    0.000262   \n",
      "354   75.952428    0.000262   75.916947    0.000262   75.881466    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   75.987909    0.000262   75.952428    0.000262  \n",
      "351   75.952428    0.000262   75.916947    0.000262  \n",
      "352   75.916947    0.000262   75.881466    0.000262  \n",
      "353   75.881466    0.000262   75.845985    0.000262  \n",
      "354   75.845985    0.000262   75.810504    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050, 1, 251) (1050, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "14/14 [==============================] - 2s 36ms/step - loss: 5149.5117 - val_loss: 4178.7725\n",
      "Epoch 2/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 5092.4297 - val_loss: 4141.6187\n",
      "Epoch 3/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5052.5703 - val_loss: 4106.7100\n",
      "Epoch 4/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5014.0591 - val_loss: 4072.0000\n",
      "Epoch 5/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4975.8105 - val_loss: 4037.5745\n",
      "Epoch 6/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4937.8589 - val_loss: 4003.4199\n",
      "Epoch 7/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 4900.1821 - val_loss: 3969.5178\n",
      "Epoch 8/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4862.7632 - val_loss: 3935.8555\n",
      "Epoch 9/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4825.5884 - val_loss: 3902.4226\n",
      "Epoch 10/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4788.6499 - val_loss: 3869.2119\n",
      "Epoch 11/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4751.9399 - val_loss: 3836.2197\n",
      "Epoch 12/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4715.4546 - val_loss: 3803.4412\n",
      "Epoch 13/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4679.1895 - val_loss: 3770.8733\n",
      "Epoch 14/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4643.1421 - val_loss: 3738.5139\n",
      "Epoch 15/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4597.8276 - val_loss: 3694.4375\n",
      "Epoch 16/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4556.7871 - val_loss: 3659.0295\n",
      "Epoch 17/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4517.7388 - val_loss: 3624.2886\n",
      "Epoch 18/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4479.3960 - val_loss: 3590.1453\n",
      "Epoch 19/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4441.6460 - val_loss: 3556.4875\n",
      "Epoch 20/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4404.3784 - val_loss: 3523.2375\n",
      "Epoch 21/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4367.5220 - val_loss: 3490.3452\n",
      "Epoch 22/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4331.0288 - val_loss: 3457.6858\n",
      "Epoch 23/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4281.7637 - val_loss: 3410.5667\n",
      "Epoch 24/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4240.4595 - val_loss: 3374.6670\n",
      "Epoch 25/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4200.7886 - val_loss: 3339.6106\n",
      "Epoch 26/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4161.9980 - val_loss: 3305.2839\n",
      "Epoch 27/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4123.9321 - val_loss: 3271.5464\n",
      "Epoch 28/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4086.4561 - val_loss: 3238.3025\n",
      "Epoch 29/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 4049.4783 - val_loss: 3205.4880\n",
      "Epoch 30/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4008.3606 - val_loss: 3165.1960\n",
      "Epoch 31/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3965.8782 - val_loss: 3128.7251\n",
      "Epoch 32/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3925.4487 - val_loss: 3093.2156\n",
      "Epoch 33/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3886.0647 - val_loss: 3058.5837\n",
      "Epoch 34/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3847.5571 - val_loss: 3024.6602\n",
      "Epoch 35/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3809.7598 - val_loss: 2991.3237\n",
      "Epoch 36/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3772.5574 - val_loss: 2958.4929\n",
      "Epoch 37/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3735.8728 - val_loss: 2926.1130\n",
      "Epoch 38/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3699.6526 - val_loss: 2894.1428\n",
      "Epoch 39/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3663.8577 - val_loss: 2862.5530\n",
      "Epoch 40/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3628.4568 - val_loss: 2831.3203\n",
      "Epoch 41/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3593.4277 - val_loss: 2800.4268\n",
      "Epoch 42/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3558.7510 - val_loss: 2769.8557\n",
      "Epoch 43/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3524.4106 - val_loss: 2739.5955\n",
      "Epoch 44/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3490.3940 - val_loss: 2709.6343\n",
      "Epoch 45/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3456.6902 - val_loss: 2679.9636\n",
      "Epoch 46/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3423.2881 - val_loss: 2650.5750\n",
      "Epoch 47/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3390.1809 - val_loss: 2621.4617\n",
      "Epoch 48/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3357.3601 - val_loss: 2592.6160\n",
      "Epoch 49/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3324.8203 - val_loss: 2564.0337\n",
      "Epoch 50/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3292.5535 - val_loss: 2534.9746\n",
      "Epoch 51/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3257.8669 - val_loss: 2502.5403\n",
      "Epoch 52/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3220.8101 - val_loss: 2470.1653\n",
      "Epoch 53/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3184.5032 - val_loss: 2438.7505\n",
      "Epoch 54/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 3149.2029 - val_loss: 2408.1472\n",
      "Epoch 55/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3114.7175 - val_loss: 2378.1968\n",
      "Epoch 56/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3080.8931 - val_loss: 2348.7905\n",
      "Epoch 57/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3047.6265 - val_loss: 2319.8569\n",
      "Epoch 58/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3014.8477 - val_loss: 2291.3462\n",
      "Epoch 59/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2982.5085 - val_loss: 2263.2234\n",
      "Epoch 60/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2950.5745 - val_loss: 2235.4614\n",
      "Epoch 61/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2919.0176 - val_loss: 2208.0398\n",
      "Epoch 62/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2887.8169 - val_loss: 2180.9419\n",
      "Epoch 63/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2856.9551 - val_loss: 2154.1531\n",
      "Epoch 64/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2826.4175 - val_loss: 2127.6619\n",
      "Epoch 65/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2796.1931 - val_loss: 2101.4595\n",
      "Epoch 66/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2766.2705 - val_loss: 2075.5356\n",
      "Epoch 67/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2736.6416 - val_loss: 2049.8845\n",
      "Epoch 68/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2707.2979 - val_loss: 2024.4989\n",
      "Epoch 69/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2678.2332 - val_loss: 1999.3729\n",
      "Epoch 70/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2649.4404 - val_loss: 1974.5011\n",
      "Epoch 71/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2620.9148 - val_loss: 1949.8777\n",
      "Epoch 72/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2592.6499 - val_loss: 1925.5001\n",
      "Epoch 73/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2564.6426 - val_loss: 1901.3633\n",
      "Epoch 74/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2536.8875 - val_loss: 1877.4631\n",
      "Epoch 75/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2509.3806 - val_loss: 1853.7964\n",
      "Epoch 76/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2482.1187 - val_loss: 1830.3594\n",
      "Epoch 77/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 2455.0977 - val_loss: 1807.1495\n",
      "Epoch 78/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2428.3149 - val_loss: 1784.1638\n",
      "Epoch 79/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2401.7668 - val_loss: 1761.3988\n",
      "Epoch 80/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2375.4509 - val_loss: 1738.8533\n",
      "Epoch 81/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2349.3643 - val_loss: 1716.5236\n",
      "Epoch 82/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2323.5044 - val_loss: 1694.4076\n",
      "Epoch 83/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2297.8682 - val_loss: 1672.5033\n",
      "Epoch 84/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2272.4541 - val_loss: 1650.8080\n",
      "Epoch 85/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2247.2590 - val_loss: 1629.3195\n",
      "Epoch 86/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2222.2812 - val_loss: 1608.0367\n",
      "Epoch 87/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2197.5186 - val_loss: 1586.9573\n",
      "Epoch 88/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2172.9690 - val_loss: 1566.0786\n",
      "Epoch 89/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2148.6306 - val_loss: 1545.3990\n",
      "Epoch 90/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2124.5010 - val_loss: 1524.9178\n",
      "Epoch 91/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2100.5786 - val_loss: 1504.6326\n",
      "Epoch 92/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2076.8623 - val_loss: 1484.5410\n",
      "Epoch 93/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2053.3494 - val_loss: 1464.6431\n",
      "Epoch 94/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2030.0387 - val_loss: 1444.9360\n",
      "Epoch 95/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2006.9287 - val_loss: 1425.4182\n",
      "Epoch 96/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1984.0172 - val_loss: 1406.0881\n",
      "Epoch 97/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1961.3036 - val_loss: 1386.9457\n",
      "Epoch 98/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1938.7854 - val_loss: 1367.9873\n",
      "Epoch 99/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1916.4620 - val_loss: 1349.2133\n",
      "Epoch 100/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1894.3309 - val_loss: 1330.6205\n",
      "Epoch 101/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1872.3914 - val_loss: 1312.2094\n",
      "Epoch 102/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1850.6421 - val_loss: 1293.9783\n",
      "Epoch 103/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1829.0813 - val_loss: 1275.9250\n",
      "Epoch 104/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1807.7078 - val_loss: 1258.0494\n",
      "Epoch 105/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1786.5204 - val_loss: 1240.3486\n",
      "Epoch 106/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1765.5178 - val_loss: 1222.8229\n",
      "Epoch 107/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1744.6979 - val_loss: 1205.4701\n",
      "Epoch 108/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1724.0610 - val_loss: 1188.2889\n",
      "Epoch 109/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1703.6044 - val_loss: 1171.2788\n",
      "Epoch 110/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1683.3276 - val_loss: 1154.4393\n",
      "Epoch 111/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1663.2297 - val_loss: 1137.7682\n",
      "Epoch 112/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1643.3091 - val_loss: 1121.2634\n",
      "Epoch 113/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1623.5645 - val_loss: 1104.9255\n",
      "Epoch 114/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1603.9948 - val_loss: 1088.7532\n",
      "Epoch 115/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1584.5996 - val_loss: 1072.7440\n",
      "Epoch 116/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1565.3765 - val_loss: 1056.8978\n",
      "Epoch 117/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1546.3246 - val_loss: 1041.2134\n",
      "Epoch 118/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1527.4437 - val_loss: 1025.6896\n",
      "Epoch 119/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1508.7319 - val_loss: 1010.3260\n",
      "Epoch 120/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1490.1891 - val_loss: 995.1209\n",
      "Epoch 121/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1471.8131 - val_loss: 980.0734\n",
      "Epoch 122/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1453.6034 - val_loss: 965.1822\n",
      "Epoch 123/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1435.5592 - val_loss: 950.4466\n",
      "Epoch 124/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1417.6787 - val_loss: 935.8654\n",
      "Epoch 125/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1399.9619 - val_loss: 921.4381\n",
      "Epoch 126/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1382.4073 - val_loss: 907.1633\n",
      "Epoch 127/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1365.0134 - val_loss: 893.0397\n",
      "Epoch 128/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1347.7799 - val_loss: 879.0669\n",
      "Epoch 129/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1330.7053 - val_loss: 865.2437\n",
      "Epoch 130/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1313.7893 - val_loss: 851.5690\n",
      "Epoch 131/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1297.0304 - val_loss: 838.0421\n",
      "Epoch 132/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1280.4277 - val_loss: 824.6617\n",
      "Epoch 133/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1263.9805 - val_loss: 811.4272\n",
      "Epoch 134/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1247.6874 - val_loss: 798.3378\n",
      "Epoch 135/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1231.5483 - val_loss: 785.3925\n",
      "Epoch 136/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1215.5615 - val_loss: 772.5894\n",
      "Epoch 137/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1199.7261 - val_loss: 759.9290\n",
      "Epoch 138/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1184.0415 - val_loss: 747.4091\n",
      "Epoch 139/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1168.5066 - val_loss: 735.0300\n",
      "Epoch 140/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1153.1207 - val_loss: 722.7901\n",
      "Epoch 141/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1137.8827 - val_loss: 710.6881\n",
      "Epoch 142/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1122.7916 - val_loss: 698.7242\n",
      "Epoch 143/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1107.8469 - val_loss: 686.8970\n",
      "Epoch 144/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1093.0471 - val_loss: 675.2051\n",
      "Epoch 145/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1078.3921 - val_loss: 663.6487\n",
      "Epoch 146/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1063.8804 - val_loss: 652.2259\n",
      "Epoch 147/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1049.5117 - val_loss: 640.9364\n",
      "Epoch 148/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1035.2844 - val_loss: 629.7787\n",
      "Epoch 149/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1021.1979 - val_loss: 618.7527\n",
      "Epoch 150/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1007.2517 - val_loss: 607.8569\n",
      "Epoch 151/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 993.4446 - val_loss: 597.0914\n",
      "Epoch 152/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 979.7762 - val_loss: 586.4544\n",
      "Epoch 153/500\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 966.2454 - val_loss: 575.9457\n",
      "Epoch 154/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 952.8510 - val_loss: 565.5634\n",
      "Epoch 155/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 939.5923 - val_loss: 555.3077\n",
      "Epoch 156/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 926.4689 - val_loss: 545.1776\n",
      "Epoch 157/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 913.4796 - val_loss: 535.1721\n",
      "Epoch 158/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 900.6234 - val_loss: 525.2897\n",
      "Epoch 159/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 887.9001 - val_loss: 515.5311\n",
      "Epoch 160/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 875.3088 - val_loss: 505.8946\n",
      "Epoch 161/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 862.8481 - val_loss: 496.3786\n",
      "Epoch 162/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 850.5173 - val_loss: 486.9837\n",
      "Epoch 163/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 838.3159 - val_loss: 477.7080\n",
      "Epoch 164/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 826.2430 - val_loss: 468.5511\n",
      "Epoch 165/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 814.2976 - val_loss: 459.5119\n",
      "Epoch 166/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 802.4790 - val_loss: 450.5905\n",
      "Epoch 167/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 790.7867 - val_loss: 441.7852\n",
      "Epoch 168/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 779.2195 - val_loss: 433.0957\n",
      "Epoch 169/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 767.7772 - val_loss: 424.5208\n",
      "Epoch 170/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 756.4583 - val_loss: 416.0596\n",
      "Epoch 171/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 745.2620 - val_loss: 407.7116\n",
      "Epoch 172/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 734.1880 - val_loss: 399.4753\n",
      "Epoch 173/500\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 723.2355 - val_loss: 391.3514\n",
      "Epoch 174/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 712.4033 - val_loss: 383.3380\n",
      "Epoch 175/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 701.6912 - val_loss: 375.4344\n",
      "Epoch 176/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 691.0978 - val_loss: 367.6401\n",
      "Epoch 177/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 680.6227 - val_loss: 359.9542\n",
      "Epoch 178/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 670.2651 - val_loss: 352.3757\n",
      "Epoch 179/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 660.0244 - val_loss: 344.9041\n",
      "Epoch 180/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 649.8990 - val_loss: 337.5384\n",
      "Epoch 181/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 639.8889 - val_loss: 330.2772\n",
      "Epoch 182/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 629.9934 - val_loss: 323.1216\n",
      "Epoch 183/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 620.2114 - val_loss: 316.0686\n",
      "Epoch 184/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 610.5421 - val_loss: 309.1189\n",
      "Epoch 185/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 600.9852 - val_loss: 302.2713\n",
      "Epoch 186/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 591.5393 - val_loss: 295.5251\n",
      "Epoch 187/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 582.2041 - val_loss: 288.8789\n",
      "Epoch 188/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 572.9786 - val_loss: 282.3329\n",
      "Epoch 189/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 563.8623 - val_loss: 275.8855\n",
      "Epoch 190/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 554.8541 - val_loss: 269.5361\n",
      "Epoch 191/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 545.9535 - val_loss: 263.2840\n",
      "Epoch 192/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 537.1595 - val_loss: 257.1289\n",
      "Epoch 193/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 528.4717 - val_loss: 251.0693\n",
      "Epoch 194/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 519.8892 - val_loss: 245.1052\n",
      "Epoch 195/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 511.4114 - val_loss: 239.2350\n",
      "Epoch 196/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 503.0372 - val_loss: 233.4589\n",
      "Epoch 197/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 494.7663 - val_loss: 227.7748\n",
      "Epoch 198/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 486.5975 - val_loss: 222.1829\n",
      "Epoch 199/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 478.5303 - val_loss: 216.6826\n",
      "Epoch 200/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 470.5641 - val_loss: 211.2727\n",
      "Epoch 201/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 462.6980 - val_loss: 205.9523\n",
      "Epoch 202/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 454.9313 - val_loss: 200.7208\n",
      "Epoch 203/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 447.2629 - val_loss: 195.5776\n",
      "Epoch 204/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 439.6927 - val_loss: 190.5217\n",
      "Epoch 205/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 432.2193 - val_loss: 185.5523\n",
      "Epoch 206/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 424.8426 - val_loss: 180.6688\n",
      "Epoch 207/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 417.5614 - val_loss: 175.8710\n",
      "Epoch 208/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 410.3755 - val_loss: 171.1570\n",
      "Epoch 209/500\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 403.2834 - val_loss: 166.5270\n",
      "Epoch 210/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 396.2852 - val_loss: 161.9798\n",
      "Epoch 211/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 389.3797 - val_loss: 157.5146\n",
      "Epoch 212/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 382.5663 - val_loss: 153.1309\n",
      "Epoch 213/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 375.8443 - val_loss: 148.8277\n",
      "Epoch 214/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 369.2127 - val_loss: 144.6042\n",
      "Epoch 215/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 362.6712 - val_loss: 140.4600\n",
      "Epoch 216/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 356.2187 - val_loss: 136.3940\n",
      "Epoch 217/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 349.8547 - val_loss: 132.4059\n",
      "Epoch 218/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 343.5786 - val_loss: 128.4947\n",
      "Epoch 219/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 337.3895 - val_loss: 124.6594\n",
      "Epoch 220/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 331.2866 - val_loss: 120.8996\n",
      "Epoch 221/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 325.2693 - val_loss: 117.2141\n",
      "Epoch 222/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 319.3367 - val_loss: 113.6029\n",
      "Epoch 223/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 313.4885 - val_loss: 110.0644\n",
      "Epoch 224/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 307.7234 - val_loss: 106.5986\n",
      "Epoch 225/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 302.0414 - val_loss: 103.2042\n",
      "Epoch 226/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 296.4413 - val_loss: 99.8809\n",
      "Epoch 227/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 290.9227 - val_loss: 96.6277\n",
      "Epoch 228/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 285.4845 - val_loss: 93.4441\n",
      "Epoch 229/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 280.1262 - val_loss: 90.3289\n",
      "Epoch 230/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 274.8471 - val_loss: 87.2818\n",
      "Epoch 231/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 269.6462 - val_loss: 84.3018\n",
      "Epoch 232/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 264.5232 - val_loss: 81.3882\n",
      "Epoch 233/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 259.4773 - val_loss: 78.5403\n",
      "Epoch 234/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 254.5075 - val_loss: 75.7574\n",
      "Epoch 235/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 249.6135 - val_loss: 73.0388\n",
      "Epoch 236/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 244.7945 - val_loss: 70.3838\n",
      "Epoch 237/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 240.0497 - val_loss: 67.7913\n",
      "Epoch 238/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 235.3781 - val_loss: 65.2610\n",
      "Epoch 239/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 230.7796 - val_loss: 62.7922\n",
      "Epoch 240/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 226.2532 - val_loss: 60.3839\n",
      "Epoch 241/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 221.7982 - val_loss: 58.0353\n",
      "Epoch 242/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 217.4139 - val_loss: 55.7460\n",
      "Epoch 243/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 213.0997 - val_loss: 53.5151\n",
      "Epoch 244/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 208.8551 - val_loss: 51.3419\n",
      "Epoch 245/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 204.6791 - val_loss: 49.2259\n",
      "Epoch 246/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 200.5710 - val_loss: 47.1661\n",
      "Epoch 247/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 196.5303 - val_loss: 45.1617\n",
      "Epoch 248/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 192.5560 - val_loss: 43.2120\n",
      "Epoch 249/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 188.6474 - val_loss: 41.3165\n",
      "Epoch 250/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 184.8043 - val_loss: 39.4745\n",
      "Epoch 251/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 181.0258 - val_loss: 37.6851\n",
      "Epoch 252/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 177.3113 - val_loss: 35.9477\n",
      "Epoch 253/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 173.6595 - val_loss: 34.2615\n",
      "Epoch 254/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 170.0706 - val_loss: 32.6260\n",
      "Epoch 255/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 166.5435 - val_loss: 31.0401\n",
      "Epoch 256/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 163.0774 - val_loss: 29.5036\n",
      "Epoch 257/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 159.6717 - val_loss: 28.0154\n",
      "Epoch 258/500\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 156.3260 - val_loss: 26.5751\n",
      "Epoch 259/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 153.0393 - val_loss: 25.1816\n",
      "Epoch 260/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 149.8111 - val_loss: 23.8346\n",
      "Epoch 261/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 146.6408 - val_loss: 22.5333\n",
      "Epoch 262/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 143.5275 - val_loss: 21.2768\n",
      "Epoch 263/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 140.4707 - val_loss: 20.0646\n",
      "Epoch 264/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 137.4698 - val_loss: 18.8961\n",
      "Epoch 265/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 134.5240 - val_loss: 17.7705\n",
      "Epoch 266/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 131.6327 - val_loss: 16.6870\n",
      "Epoch 267/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 128.7950 - val_loss: 15.6451\n",
      "Epoch 268/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 126.0107 - val_loss: 14.6440\n",
      "Epoch 269/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 123.2788 - val_loss: 13.6831\n",
      "Epoch 270/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 120.5988 - val_loss: 12.7617\n",
      "Epoch 271/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 117.9701 - val_loss: 11.8791\n",
      "Epoch 272/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 115.3920 - val_loss: 11.0347\n",
      "Epoch 273/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 112.8638 - val_loss: 10.2279\n",
      "Epoch 274/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 110.3849 - val_loss: 9.4579\n",
      "Epoch 275/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 107.9548 - val_loss: 8.7240\n",
      "Epoch 276/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 105.5725 - val_loss: 8.0257\n",
      "Epoch 277/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 103.2377 - val_loss: 7.3622\n",
      "Epoch 278/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 100.9495 - val_loss: 6.7330\n",
      "Epoch 279/500\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 98.7074 - val_loss: 6.1373\n",
      "Epoch 280/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 96.5108 - val_loss: 5.5746\n",
      "Epoch 281/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 94.3593 - val_loss: 5.0441\n",
      "Epoch 282/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 92.2519 - val_loss: 4.5452\n",
      "Epoch 283/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 90.1880 - val_loss: 4.0774\n",
      "Epoch 284/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 88.1673 - val_loss: 3.6400\n",
      "Epoch 285/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 86.1890 - val_loss: 3.2324\n",
      "Epoch 286/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 84.2524 - val_loss: 2.8539\n",
      "Epoch 287/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 82.3570 - val_loss: 2.5039\n",
      "Epoch 288/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 80.5024 - val_loss: 2.1819\n",
      "Epoch 289/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 78.6877 - val_loss: 1.8871\n",
      "Epoch 290/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 76.9124 - val_loss: 1.6191\n",
      "Epoch 291/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 75.1759 - val_loss: 1.3771\n",
      "Epoch 292/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 73.4776 - val_loss: 1.1606\n",
      "Epoch 293/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 71.8169 - val_loss: 0.9690\n",
      "Epoch 294/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 70.1932 - val_loss: 0.8017\n",
      "Epoch 295/500\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 68.6061 - val_loss: 0.6581\n",
      "Epoch 296/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 67.0548 - val_loss: 0.5377\n",
      "Epoch 297/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 65.5389 - val_loss: 0.4399\n",
      "Epoch 298/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 64.0578 - val_loss: 0.3640\n",
      "Epoch 299/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 62.6106 - val_loss: 0.3096\n",
      "Epoch 300/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 61.1973 - val_loss: 0.2760\n",
      "Epoch 301/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 59.8169 - val_loss: 0.2627\n",
      "Epoch 302/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 58.4692 - val_loss: 0.2692\n",
      "Epoch 303/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 57.1533 - val_loss: 0.2950\n",
      "Epoch 304/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 55.8689 - val_loss: 0.3393\n",
      "Epoch 305/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 54.6154 - val_loss: 0.4018\n",
      "Epoch 306/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 53.3922 - val_loss: 0.4819\n",
      "Epoch 307/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 52.1989 - val_loss: 0.5791\n",
      "Epoch 308/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 51.0349 - val_loss: 0.6929\n",
      "Epoch 309/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 49.8996 - val_loss: 0.8226\n",
      "Epoch 310/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 48.7926 - val_loss: 0.9679\n",
      "Epoch 311/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 47.7133 - val_loss: 1.1283\n",
      "Epoch 312/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 46.6612 - val_loss: 1.3031\n",
      "Epoch 313/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 45.6359 - val_loss: 1.4920\n",
      "Epoch 314/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 44.6368 - val_loss: 1.6944\n",
      "Epoch 315/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 43.6634 - val_loss: 1.9099\n",
      "Epoch 316/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 42.7153 - val_loss: 2.1380\n",
      "Epoch 317/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 41.7919 - val_loss: 2.3782\n",
      "Epoch 318/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 40.8927 - val_loss: 2.6300\n",
      "Epoch 319/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 40.0173 - val_loss: 2.8930\n",
      "Epoch 320/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 39.1653 - val_loss: 3.1668\n",
      "Epoch 321/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 38.3360 - val_loss: 3.4509\n",
      "Epoch 322/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 37.5293 - val_loss: 3.7449\n",
      "Epoch 323/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 36.7443 - val_loss: 4.0484\n",
      "Epoch 324/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 35.9809 - val_loss: 4.3607\n",
      "Epoch 325/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 35.2385 - val_loss: 4.6817\n",
      "Epoch 326/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 34.5167 - val_loss: 5.0108\n",
      "Epoch 327/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 33.8152 - val_loss: 5.3477\n",
      "Epoch 328/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 33.1333 - val_loss: 5.6920\n",
      "Epoch 329/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 32.4707 - val_loss: 6.0432\n",
      "Epoch 330/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 31.8270 - val_loss: 6.4011\n",
      "Epoch 331/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 31.2017 - val_loss: 6.7652\n",
      "Epoch 332/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 30.5945 - val_loss: 7.1350\n",
      "Epoch 333/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 30.0049 - val_loss: 7.5104\n",
      "Epoch 334/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 29.4325 - val_loss: 7.8908\n",
      "Epoch 335/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 28.8771 - val_loss: 8.2760\n",
      "Epoch 336/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 28.3381 - val_loss: 8.6656\n",
      "Epoch 337/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 27.8152 - val_loss: 9.0593\n",
      "Epoch 338/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 27.3079 - val_loss: 9.4567\n",
      "Epoch 339/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 26.8160 - val_loss: 9.8575\n",
      "Epoch 340/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 26.3391 - val_loss: 10.2614\n",
      "Epoch 341/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 25.8768 - val_loss: 10.6680\n",
      "Epoch 342/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 25.4287 - val_loss: 11.0771\n",
      "Epoch 343/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 24.9945 - val_loss: 11.4885\n",
      "Epoch 344/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 24.5738 - val_loss: 11.9016\n",
      "Epoch 345/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 24.1664 - val_loss: 12.3163\n",
      "Epoch 346/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 23.7718 - val_loss: 12.7325\n",
      "Epoch 347/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 23.3898 - val_loss: 13.1497\n",
      "Epoch 348/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 23.0200 - val_loss: 13.5675\n",
      "Epoch 349/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 22.6621 - val_loss: 13.9860\n",
      "Epoch 350/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 22.3158 - val_loss: 14.4048\n",
      "Epoch 351/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 21.9807 - val_loss: 14.8234\n",
      "Epoch 352/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 21.6568 - val_loss: 15.2420\n",
      "Epoch 353/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 21.3434 - val_loss: 15.6600\n",
      "Epoch 354/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 21.0405 - val_loss: 16.0774\n",
      "Epoch 355/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 20.7477 - val_loss: 16.4940\n",
      "Epoch 356/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 20.4647 - val_loss: 16.9094\n",
      "Epoch 357/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 20.1913 - val_loss: 17.3235\n",
      "Epoch 358/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 19.9272 - val_loss: 17.7362\n",
      "Epoch 359/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 19.6721 - val_loss: 18.1472\n",
      "Epoch 360/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 19.4258 - val_loss: 18.5563\n",
      "Epoch 361/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 19.1880 - val_loss: 18.9634\n",
      "Epoch 362/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 18.9584 - val_loss: 19.3681\n",
      "Epoch 363/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 18.7369 - val_loss: 19.7706\n",
      "Epoch 364/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 18.5232 - val_loss: 20.1706\n",
      "Epoch 365/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 18.3171 - val_loss: 20.5679\n",
      "Epoch 366/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 18.1182 - val_loss: 20.9622\n",
      "Epoch 367/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 17.9265 - val_loss: 21.3536\n",
      "Epoch 368/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 17.7417 - val_loss: 21.7419\n",
      "Epoch 369/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.5636 - val_loss: 22.1269\n",
      "Epoch 370/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 17.3919 - val_loss: 22.5088\n",
      "Epoch 371/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.2265 - val_loss: 22.8869\n",
      "Epoch 372/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 17.0672 - val_loss: 23.2614\n",
      "Epoch 373/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9137 - val_loss: 23.6323\n",
      "Epoch 374/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.7660 - val_loss: 23.9994\n",
      "Epoch 375/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 16.6238 - val_loss: 24.3628\n",
      "Epoch 376/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.4868 - val_loss: 24.7219\n",
      "Epoch 377/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.3551 - val_loss: 25.0771\n",
      "Epoch 378/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 16.2283 - val_loss: 25.4281\n",
      "Epoch 379/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.1063 - val_loss: 25.7750\n",
      "Epoch 380/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 15.9890 - val_loss: 26.1175\n",
      "Epoch 381/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 15.8762 - val_loss: 26.4557\n",
      "Epoch 382/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 15.7677 - val_loss: 26.7896\n",
      "Epoch 383/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 15.6635 - val_loss: 27.1189\n",
      "Epoch 384/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 15.5632 - val_loss: 27.4437\n",
      "Epoch 385/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 15.4669 - val_loss: 27.7640\n",
      "Epoch 386/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 15.3744 - val_loss: 28.0796\n",
      "Epoch 387/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 15.2855 - val_loss: 28.3908\n",
      "Epoch 388/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 15.2001 - val_loss: 28.6971\n",
      "Epoch 389/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 15.1181 - val_loss: 28.9988\n",
      "Epoch 390/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 15.0394 - val_loss: 29.2958\n",
      "Epoch 391/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.9638 - val_loss: 29.5879\n",
      "Epoch 392/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.8913 - val_loss: 29.8755\n",
      "Epoch 393/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.8217 - val_loss: 30.1581\n",
      "Epoch 394/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.7549 - val_loss: 30.4363\n",
      "Epoch 395/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.6908 - val_loss: 30.7096\n",
      "Epoch 396/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.6292 - val_loss: 30.9781\n",
      "Epoch 397/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 14.5702 - val_loss: 31.2418\n",
      "Epoch 398/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.5137 - val_loss: 31.5008\n",
      "Epoch 399/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.4594 - val_loss: 31.7550\n",
      "Epoch 400/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 14.4074 - val_loss: 32.0045\n",
      "Epoch 401/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 14.3576 - val_loss: 32.2492\n",
      "Epoch 402/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.3097 - val_loss: 32.4892\n",
      "Epoch 403/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 14.2639 - val_loss: 32.7246\n",
      "Epoch 404/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.2200 - val_loss: 32.9553\n",
      "Epoch 405/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 14.1779 - val_loss: 33.1815\n",
      "Epoch 406/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1376 - val_loss: 33.4029\n",
      "Epoch 407/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.0990 - val_loss: 33.6201\n",
      "Epoch 408/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.0620 - val_loss: 33.8322\n",
      "Epoch 409/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.0266 - val_loss: 34.0400\n",
      "Epoch 410/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 13.9927 - val_loss: 34.2437\n",
      "Epoch 411/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 13.9601 - val_loss: 34.4424\n",
      "Epoch 412/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.9290 - val_loss: 34.6371\n",
      "Epoch 413/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.8992 - val_loss: 34.8273\n",
      "Epoch 414/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 13.8707 - val_loss: 35.0133\n",
      "Epoch 415/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 13.8433 - val_loss: 35.1949\n",
      "Epoch 416/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 13.8172 - val_loss: 35.3724\n",
      "Epoch 417/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 13.7921 - val_loss: 35.5457\n",
      "Epoch 418/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 13.7682 - val_loss: 35.7149\n",
      "Epoch 419/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 13.7452 - val_loss: 35.8801\n",
      "Epoch 420/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.7233 - val_loss: 36.0413\n",
      "Epoch 421/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.7023 - val_loss: 36.1983\n",
      "Epoch 422/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 13.6821 - val_loss: 36.3516\n",
      "Epoch 423/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 13.6629 - val_loss: 36.5011\n",
      "Epoch 424/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.6445 - val_loss: 36.6466\n",
      "Epoch 425/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.6269 - val_loss: 36.7886\n",
      "Epoch 426/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.6100 - val_loss: 36.9268\n",
      "Epoch 427/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.5939 - val_loss: 37.0613\n",
      "Epoch 428/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 13.5784 - val_loss: 37.1922\n",
      "Epoch 429/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.5637 - val_loss: 37.3199\n",
      "Epoch 430/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.5495 - val_loss: 37.4438\n",
      "Epoch 431/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.5360 - val_loss: 37.5646\n",
      "Epoch 432/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.5231 - val_loss: 37.6820\n",
      "Epoch 433/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.5107 - val_loss: 37.7962\n",
      "Epoch 434/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.4989 - val_loss: 37.9070\n",
      "Epoch 435/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.4875 - val_loss: 38.0149\n",
      "Epoch 436/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.4767 - val_loss: 38.1195\n",
      "Epoch 437/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.4664 - val_loss: 38.2214\n",
      "Epoch 438/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.4564 - val_loss: 38.3202\n",
      "Epoch 439/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.4469 - val_loss: 38.4160\n",
      "Epoch 440/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.4379 - val_loss: 38.5089\n",
      "Epoch 441/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 13.4292 - val_loss: 38.5990\n",
      "Epoch 442/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.4209 - val_loss: 38.6864\n",
      "Epoch 443/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.4130 - val_loss: 38.7715\n",
      "Epoch 444/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 13.4054 - val_loss: 38.8533\n",
      "Epoch 445/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 13.3981 - val_loss: 38.9331\n",
      "Epoch 446/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.3911 - val_loss: 39.0102\n",
      "Epoch 447/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.3845 - val_loss: 39.0850\n",
      "Epoch 448/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.3782 - val_loss: 39.1574\n",
      "Epoch 449/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.3721 - val_loss: 39.2272\n",
      "Epoch 450/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 13.3663 - val_loss: 39.2952\n",
      "Epoch 451/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.3607 - val_loss: 39.3606\n",
      "Epoch 452/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.3554 - val_loss: 39.4240\n",
      "Epoch 453/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.3504 - val_loss: 39.4853\n",
      "Epoch 454/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.3455 - val_loss: 39.5446\n",
      "Epoch 455/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.3408 - val_loss: 39.6019\n",
      "Epoch 456/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.3364 - val_loss: 39.6573\n",
      "Epoch 457/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.3321 - val_loss: 39.7106\n",
      "Epoch 458/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.3281 - val_loss: 39.7625\n",
      "Epoch 459/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.3242 - val_loss: 39.8122\n",
      "Epoch 460/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 13.3205 - val_loss: 39.8604\n",
      "Epoch 461/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 13.3170 - val_loss: 39.9068\n",
      "Epoch 462/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.3135 - val_loss: 39.9516\n",
      "Epoch 463/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 13.3103 - val_loss: 39.9946\n",
      "Epoch 464/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.3073 - val_loss: 40.0361\n",
      "Epoch 465/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.3043 - val_loss: 40.0763\n",
      "Epoch 466/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 13.3015 - val_loss: 40.1148\n",
      "Epoch 467/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 13.2988 - val_loss: 40.1520\n",
      "Epoch 468/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 13.2962 - val_loss: 40.1875\n",
      "Epoch 469/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2938 - val_loss: 40.2219\n",
      "Epoch 470/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2915 - val_loss: 40.2552\n",
      "Epoch 471/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2892 - val_loss: 40.2870\n",
      "Epoch 472/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2871 - val_loss: 40.3180\n",
      "Epoch 473/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2851 - val_loss: 40.3472\n",
      "Epoch 474/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2832 - val_loss: 40.3756\n",
      "Epoch 475/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 13.2813 - val_loss: 40.4029\n",
      "Epoch 476/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 13.2796 - val_loss: 40.4291\n",
      "Epoch 477/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2779 - val_loss: 40.4542\n",
      "Epoch 478/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2763 - val_loss: 40.4783\n",
      "Epoch 479/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2748 - val_loss: 40.5013\n",
      "Epoch 480/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2733 - val_loss: 40.5236\n",
      "Epoch 481/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2719 - val_loss: 40.5449\n",
      "Epoch 482/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2706 - val_loss: 40.5653\n",
      "Epoch 483/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2694 - val_loss: 40.5850\n",
      "Epoch 484/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2682 - val_loss: 40.6034\n",
      "Epoch 485/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2671 - val_loss: 40.6215\n",
      "Epoch 486/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2660 - val_loss: 40.6387\n",
      "Epoch 487/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2650 - val_loss: 40.6553\n",
      "Epoch 488/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 13.2641 - val_loss: 40.6710\n",
      "Epoch 489/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 13.2632 - val_loss: 40.6860\n",
      "Epoch 490/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2623 - val_loss: 40.7004\n",
      "Epoch 491/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2615 - val_loss: 40.7141\n",
      "Epoch 492/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2607 - val_loss: 40.7273\n",
      "Epoch 493/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2600 - val_loss: 40.7400\n",
      "Epoch 494/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2594 - val_loss: 40.7521\n",
      "Epoch 495/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2587 - val_loss: 40.7635\n",
      "Epoch 496/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2581 - val_loss: 40.7744\n",
      "Epoch 497/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2575 - val_loss: 40.7850\n",
      "Epoch 498/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2569 - val_loss: 40.7948\n",
      "Epoch 499/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2565 - val_loss: 40.8047\n",
      "Epoch 500/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.2559 - val_loss: 40.8138\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(85.7336, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0020, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0125, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3008, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 369ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.67665616e+01, 6.67329248e+01, 6.67063142e+01, 6.66797035e+01,\n",
       "        6.66530929e+01, 6.66264823e+01, 6.65998716e+01, 6.65732610e+01,\n",
       "        6.65466503e+01, 6.65200397e+01, 6.64934290e+01, 6.64668184e+01,\n",
       "        6.64402077e+01, 6.64135971e+01, 6.63869865e+01, 6.63603758e+01,\n",
       "        6.63337652e+01, 6.63071545e+01, 6.62805439e+01, 6.62539332e+01,\n",
       "        6.62273226e+01, 6.62007119e+01, 6.61741013e+01, 6.61474907e+01,\n",
       "        6.61208800e+01, 6.60942694e+01, 6.60676587e+01, 6.60410481e+01,\n",
       "        6.60144374e+01, 6.59878268e+01, 6.59612161e+01, 6.59346055e+01,\n",
       "        6.59079949e+01, 6.58813842e+01, 6.58547736e+01, 6.58281629e+01,\n",
       "        6.58015523e+01, 6.57762605e+01, 6.57510504e+01, 6.57258403e+01,\n",
       "        6.57006303e+01, 6.56754202e+01, 6.56502101e+01, 6.56250000e+01,\n",
       "        6.55997899e+01, 6.55745798e+01, 6.55493697e+01, 6.55241597e+01,\n",
       "        6.54989496e+01, 6.54737395e+01, 6.54485294e+01, 6.54233193e+01,\n",
       "        6.53981092e+01, 6.53728992e+01, 6.53476891e+01, 6.53224790e+01,\n",
       "        6.52972689e+01, 6.52720588e+01, 6.52468487e+01, 6.52216387e+01,\n",
       "        6.51964286e+01, 6.51712185e+01, 6.51460084e+01, 6.51207983e+01,\n",
       "        6.50955882e+01, 6.50703781e+01, 6.50451681e+01, 6.50199580e+01,\n",
       "        6.49947479e+01, 6.49695378e+01, 6.49443277e+01, 6.49191177e+01,\n",
       "        6.48959384e+01, 6.48791317e+01, 6.48623249e+01, 6.48455182e+01,\n",
       "        6.48287115e+01, 6.48119048e+01, 6.47950980e+01, 6.47782913e+01,\n",
       "        7.19575653e+01, 0.00000000e+00, 6.06023788e-01, 7.51617134e-01,\n",
       "        0.00000000e+00, 1.78536296e-01, 6.18574843e-02, 1.53829455e-01,\n",
       "        1.06690124e-01, 0.00000000e+00, 4.76361141e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.73489523e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.14818931e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([63.79327731, 63.78207283, 63.77086835, 63.75966387, 63.74845938,\n",
       "       63.7372549 , 63.72605042, 63.71484594, 63.70364146, 63.69243697,\n",
       "       63.68123249, 63.67002801, 63.65882353, 63.64761905, 63.63641457,\n",
       "       63.62521008, 63.6140056 , 63.60280112, 63.59159664, 63.58039216,\n",
       "       63.56918768, 63.55798319, 63.54677871, 63.53557423, 63.52436975,\n",
       "       63.51316527, 63.50196078, 63.4907563 , 63.47955182, 63.46834734,\n",
       "       63.45714286, 63.44593838, 63.43473389, 63.42352941, 63.41232493,\n",
       "       63.40112045, 63.38991597, 63.37871148, 63.367507  , 63.35630252,\n",
       "       63.34509804, 63.33389356, 63.32268908, 63.31148459, 63.30028011,\n",
       "       63.28907563, 63.27787115, 63.26666667, 63.25546218, 63.2442577 ,\n",
       "       63.23305322, 63.22184874, 63.21064426, 63.19943978, 63.18823529,\n",
       "       63.17703081, 63.16582633, 63.15462185, 63.14341737, 63.13221289,\n",
       "       63.1210084 , 63.10980392, 63.09859944, 63.08739496, 63.07619048,\n",
       "       63.06498599, 63.05378151, 63.04257703, 63.03137255, 63.02016807,\n",
       "       63.00896359, 62.9977591 , 62.98655462, 62.97535014, 62.96414566,\n",
       "       62.95294118, 62.94173669, 62.93053221, 62.91932773, 62.90812325,\n",
       "       62.89691877, 62.88571429, 62.8745098 , 62.86330532, 62.85210084,\n",
       "       62.84089636, 62.82969188, 62.81848739, 62.80728291, 62.79607843,\n",
       "       62.78487395, 62.77366947, 62.76246499, 62.7512605 , 62.74005602,\n",
       "       62.72885154, 62.71764706, 62.70644258, 62.69801587, 62.69334734])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.39837050923397\n",
      "13.89188008108702\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
