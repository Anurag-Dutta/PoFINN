{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "2195    58.041545\n",
       "2196    58.032527\n",
       "2197    58.023509\n",
       "2198    58.014491\n",
       "2199    58.005472\n",
       "Name: C8, Length: 2200, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c8_interpolated_2100_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "2095     0.000000\n",
       "2096     0.197040\n",
       "2097     0.000000\n",
       "2098     0.000000\n",
       "2099     0.039498\n",
       "Name: C8, Length: 2100, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2100)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.987115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.774230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.561345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.348459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.200000  0.000298\n",
       "1     87.987115  0.000298\n",
       "2     87.774230  0.000297\n",
       "3     87.561345  0.000297\n",
       "4     87.348459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 85.7336  # Value for C0\n",
    "K0 = -0.0020  # Value for K0\n",
    "K1 = 0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0125    # Value for b\n",
    "c = 2.3008    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.200000    87.987115    87.774230    87.561345    87.348459   \n",
      "351    87.987115    87.774230    87.561345    87.348459    87.135574   \n",
      "352    87.774230    87.561345    87.348459    87.135574    86.922689   \n",
      "353    87.561345    87.348459    87.135574    86.922689    86.709804   \n",
      "354    87.348459    87.135574    86.922689    86.709804    86.496919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.135574    86.922689    86.709804    86.496919    86.294538  ...   \n",
      "351    86.922689    86.709804    86.496919    86.294538    86.221709  ...   \n",
      "352    86.709804    86.496919    86.294538    86.221709    86.148880  ...   \n",
      "353    86.496919    86.294538    86.221709    86.148880    86.076050  ...   \n",
      "354    86.294538    86.221709    86.148880    86.076050    86.003221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   76.094351    0.000263   76.058870    0.000263   76.023389    0.000263   \n",
      "351   76.058870    0.000263   76.023389    0.000263   75.987909    0.000262   \n",
      "352   76.023389    0.000263   75.987909    0.000262   75.952428    0.000262   \n",
      "353   75.987909    0.000262   75.952428    0.000262   75.916947    0.000262   \n",
      "354   75.952428    0.000262   75.916947    0.000262   75.881466    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   75.987909    0.000262   75.952428    0.000262  \n",
      "351   75.952428    0.000262   75.916947    0.000262  \n",
      "352   75.916947    0.000262   75.881466    0.000262  \n",
      "353   75.881466    0.000262   75.845985    0.000262  \n",
      "354   75.845985    0.000262   75.810504    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1650, 1, 251) (1650, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "21/21 [==============================] - 2s 25ms/step - loss: 4696.3716 - val_loss: 3029.8699\n",
      "Epoch 2/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 4605.2041 - val_loss: 2987.9707\n",
      "Epoch 3/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 4525.5830 - val_loss: 2942.3789\n",
      "Epoch 4/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 4450.2480 - val_loss: 2897.7791\n",
      "Epoch 5/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 4371.8555 - val_loss: 2857.4646\n",
      "Epoch 6/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 4300.6733 - val_loss: 2818.1040\n",
      "Epoch 7/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 4231.0127 - val_loss: 2779.4666\n",
      "Epoch 8/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 4159.2021 - val_loss: 2735.8313\n",
      "Epoch 9/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 4083.1016 - val_loss: 2695.3257\n",
      "Epoch 10/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 4012.2651 - val_loss: 2656.0608\n",
      "Epoch 11/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 3943.2109 - val_loss: 2617.8140\n",
      "Epoch 12/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 3875.5969 - val_loss: 2580.3855\n",
      "Epoch 13/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 3809.2019 - val_loss: 2543.6523\n",
      "Epoch 14/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 3743.8999 - val_loss: 2507.4822\n",
      "Epoch 15/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 3677.4353 - val_loss: 2466.2412\n",
      "Epoch 16/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 3599.2437 - val_loss: 2427.8425\n",
      "Epoch 17/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 3531.1946 - val_loss: 2390.6897\n",
      "Epoch 18/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 3465.2280 - val_loss: 2354.6877\n",
      "Epoch 19/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 3400.9504 - val_loss: 2319.5986\n",
      "Epoch 20/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 3338.0503 - val_loss: 2285.2891\n",
      "Epoch 21/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 3276.3518 - val_loss: 2251.6794\n",
      "Epoch 22/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 3215.7500 - val_loss: 2218.7197\n",
      "Epoch 23/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 3156.1726 - val_loss: 2186.3728\n",
      "Epoch 24/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 3097.5679 - val_loss: 2154.6123\n",
      "Epoch 25/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 3039.8958 - val_loss: 2123.4165\n",
      "Epoch 26/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 2983.1245 - val_loss: 2092.7664\n",
      "Epoch 27/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 2927.2280 - val_loss: 2062.6489\n",
      "Epoch 28/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 2872.1841 - val_loss: 2033.0491\n",
      "Epoch 29/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 2817.9724 - val_loss: 2003.9572\n",
      "Epoch 30/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 2764.5767 - val_loss: 1975.3623\n",
      "Epoch 31/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 2711.9810 - val_loss: 1947.2549\n",
      "Epoch 32/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 2660.1716 - val_loss: 1919.6263\n",
      "Epoch 33/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 2609.1348 - val_loss: 1892.4694\n",
      "Epoch 34/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 2558.8586 - val_loss: 1865.7758\n",
      "Epoch 35/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 2509.3320 - val_loss: 1839.5391\n",
      "Epoch 36/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 2460.5442 - val_loss: 1813.7520\n",
      "Epoch 37/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 2412.4846 - val_loss: 1788.4083\n",
      "Epoch 38/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 2365.1448 - val_loss: 1763.5024\n",
      "Epoch 39/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 2318.5134 - val_loss: 1739.0281\n",
      "Epoch 40/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 2272.5828 - val_loss: 1714.9794\n",
      "Epoch 41/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 2227.3442 - val_loss: 1691.3512\n",
      "Epoch 42/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 2182.7886 - val_loss: 1668.1383\n",
      "Epoch 43/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 2138.9092 - val_loss: 1645.3351\n",
      "Epoch 44/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 2095.6963 - val_loss: 1622.9364\n",
      "Epoch 45/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 2053.1431 - val_loss: 1600.9379\n",
      "Epoch 46/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 2011.2421 - val_loss: 1579.3342\n",
      "Epoch 47/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 1969.9852 - val_loss: 1558.1208\n",
      "Epoch 48/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 1929.3663 - val_loss: 1537.2927\n",
      "Epoch 49/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 1889.3771 - val_loss: 1516.8461\n",
      "Epoch 50/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 1850.0112 - val_loss: 1496.7755\n",
      "Epoch 51/500\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 1811.2616 - val_loss: 1477.0774\n",
      "Epoch 52/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 1773.1215 - val_loss: 1457.7467\n",
      "Epoch 53/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 1735.5845 - val_loss: 1438.7792\n",
      "Epoch 54/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 1698.6436 - val_loss: 1420.1708\n",
      "Epoch 55/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 1662.2922 - val_loss: 1401.9175\n",
      "Epoch 56/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1626.5244 - val_loss: 1384.0149\n",
      "Epoch 57/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 1591.3342 - val_loss: 1366.4590\n",
      "Epoch 58/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 1556.7144 - val_loss: 1349.2451\n",
      "Epoch 59/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 1522.6587 - val_loss: 1332.3702\n",
      "Epoch 60/500\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 1489.1624 - val_loss: 1315.8301\n",
      "Epoch 61/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1456.2177 - val_loss: 1299.6201\n",
      "Epoch 62/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1423.8195 - val_loss: 1283.7367\n",
      "Epoch 63/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1391.9619 - val_loss: 1268.1763\n",
      "Epoch 64/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1360.6389 - val_loss: 1252.9352\n",
      "Epoch 65/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1329.8448 - val_loss: 1238.0090\n",
      "Epoch 66/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1299.5737 - val_loss: 1223.3942\n",
      "Epoch 67/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1269.8201 - val_loss: 1209.0872\n",
      "Epoch 68/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1240.5771 - val_loss: 1195.0837\n",
      "Epoch 69/500\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 1211.8405 - val_loss: 1181.3807\n",
      "Epoch 70/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1183.6041 - val_loss: 1167.9744\n",
      "Epoch 71/500\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 1155.8632 - val_loss: 1154.8610\n",
      "Epoch 72/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 1128.6108 - val_loss: 1142.0366\n",
      "Epoch 73/500\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 1101.8429 - val_loss: 1129.4985\n",
      "Epoch 74/500\n",
      "21/21 [==============================] - 0s 18ms/step - loss: 1075.5529 - val_loss: 1117.2421\n",
      "Epoch 75/500\n",
      "21/21 [==============================] - 0s 16ms/step - loss: 1049.7358 - val_loss: 1105.2643\n",
      "Epoch 76/500\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 1024.3865 - val_loss: 1093.5619\n",
      "Epoch 77/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 999.4993 - val_loss: 1082.1306\n",
      "Epoch 78/500\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 975.0697 - val_loss: 1070.9677\n",
      "Epoch 79/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 951.0913 - val_loss: 1060.0691\n",
      "Epoch 80/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 927.5590 - val_loss: 1049.4318\n",
      "Epoch 81/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 904.4684 - val_loss: 1039.0522\n",
      "Epoch 82/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 881.8137 - val_loss: 1028.9269\n",
      "Epoch 83/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 859.5900 - val_loss: 1019.0525\n",
      "Epoch 84/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 837.7919 - val_loss: 1009.4255\n",
      "Epoch 85/500\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 816.4150 - val_loss: 1000.0428\n",
      "Epoch 86/500\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 795.4537 - val_loss: 990.9006\n",
      "Epoch 87/500\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 774.9030 - val_loss: 981.9961\n",
      "Epoch 88/500\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 754.7581 - val_loss: 973.3257\n",
      "Epoch 89/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 735.0138 - val_loss: 964.8860\n",
      "Epoch 90/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 715.6650 - val_loss: 956.6740\n",
      "Epoch 91/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 696.7074 - val_loss: 948.6863\n",
      "Epoch 92/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 678.1357 - val_loss: 940.9195\n",
      "Epoch 93/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 659.9450 - val_loss: 933.3705\n",
      "Epoch 94/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 642.1302 - val_loss: 926.0358\n",
      "Epoch 95/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 624.6871 - val_loss: 918.9124\n",
      "Epoch 96/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 607.6101 - val_loss: 911.9973\n",
      "Epoch 97/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 590.8951 - val_loss: 905.2869\n",
      "Epoch 98/500\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 574.5372 - val_loss: 898.7781\n",
      "Epoch 99/500\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 558.5311 - val_loss: 892.4679\n",
      "Epoch 100/500\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 542.8729 - val_loss: 886.3528\n",
      "Epoch 101/500\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 527.5573 - val_loss: 880.4301\n",
      "Epoch 102/500\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 512.5799 - val_loss: 874.6963\n",
      "Epoch 103/500\n",
      "21/21 [==============================] - 0s 18ms/step - loss: 497.9360 - val_loss: 869.1478\n",
      "Epoch 104/500\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 483.6205 - val_loss: 863.7821\n",
      "Epoch 105/500\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 469.6293 - val_loss: 858.5958\n",
      "Epoch 106/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 455.9579 - val_loss: 853.5848\n",
      "Epoch 107/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 442.6013 - val_loss: 848.7461\n",
      "Epoch 108/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 429.5552 - val_loss: 844.0755\n",
      "Epoch 109/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 416.8147 - val_loss: 839.5659\n",
      "Epoch 110/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 404.3756 - val_loss: 835.2000\n",
      "Epoch 111/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 392.0566 - val_loss: 830.1772\n",
      "Epoch 112/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 376.8832 - val_loss: 824.5385\n",
      "Epoch 113/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 362.2431 - val_loss: 819.4339\n",
      "Epoch 114/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 348.6558 - val_loss: 814.8203\n",
      "Epoch 115/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 335.9025 - val_loss: 810.5747\n",
      "Epoch 116/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 323.7917 - val_loss: 806.6289\n",
      "Epoch 117/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 312.2156 - val_loss: 802.9416\n",
      "Epoch 118/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 301.1077 - val_loss: 799.4858\n",
      "Epoch 119/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 290.4235 - val_loss: 796.2418\n",
      "Epoch 120/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 280.1301 - val_loss: 793.1946\n",
      "Epoch 121/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 270.2025 - val_loss: 790.3320\n",
      "Epoch 122/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 260.6204 - val_loss: 787.6437\n",
      "Epoch 123/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 251.3665 - val_loss: 785.1212\n",
      "Epoch 124/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 242.4265 - val_loss: 782.7565\n",
      "Epoch 125/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 233.7873 - val_loss: 780.5428\n",
      "Epoch 126/500\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 225.4378 - val_loss: 778.4737\n",
      "Epoch 127/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 217.3672 - val_loss: 776.5432\n",
      "Epoch 128/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 209.5660 - val_loss: 774.7462\n",
      "Epoch 129/500\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 202.0257 - val_loss: 773.0772\n",
      "Epoch 130/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 194.7378 - val_loss: 771.5316\n",
      "Epoch 131/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 187.6948 - val_loss: 770.1047\n",
      "Epoch 132/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 180.8892 - val_loss: 768.7924\n",
      "Epoch 133/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 174.3139 - val_loss: 767.5898\n",
      "Epoch 134/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 167.9624 - val_loss: 766.4936\n",
      "Epoch 135/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 161.8282 - val_loss: 765.4997\n",
      "Epoch 136/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 155.9053 - val_loss: 764.6040\n",
      "Epoch 137/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 150.1876 - val_loss: 763.8032\n",
      "Epoch 138/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 144.6695 - val_loss: 763.0935\n",
      "Epoch 139/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 139.3454 - val_loss: 762.4716\n",
      "Epoch 140/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 134.2099 - val_loss: 761.9342\n",
      "Epoch 141/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 129.2577 - val_loss: 761.4777\n",
      "Epoch 142/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 124.4835 - val_loss: 761.0992\n",
      "Epoch 143/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 119.8826 - val_loss: 760.7955\n",
      "Epoch 144/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 115.4500 - val_loss: 760.5634\n",
      "Epoch 145/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 111.1809 - val_loss: 760.4002\n",
      "Epoch 146/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 107.0707 - val_loss: 760.3027\n",
      "Epoch 147/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 103.1147 - val_loss: 760.2682\n",
      "Epoch 148/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 99.3088 - val_loss: 760.2939\n",
      "Epoch 149/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 95.6485 - val_loss: 760.3771\n",
      "Epoch 150/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 92.1295 - val_loss: 760.5151\n",
      "Epoch 151/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 88.7477 - val_loss: 760.7053\n",
      "Epoch 152/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 85.4990 - val_loss: 760.9452\n",
      "Epoch 153/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 82.3794 - val_loss: 761.2321\n",
      "Epoch 154/500\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 79.3848 - val_loss: 761.5638\n",
      "Epoch 155/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 76.5114 - val_loss: 761.9378\n",
      "Epoch 156/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 73.7557 - val_loss: 762.3519\n",
      "Epoch 157/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 71.1139 - val_loss: 762.8037\n",
      "Epoch 158/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 68.5825 - val_loss: 763.2911\n",
      "Epoch 159/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 66.1578 - val_loss: 763.8116\n",
      "Epoch 160/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 63.8363 - val_loss: 764.3633\n",
      "Epoch 161/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 61.6147 - val_loss: 764.9443\n",
      "Epoch 162/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 59.4896 - val_loss: 765.5526\n",
      "Epoch 163/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 57.4578 - val_loss: 766.1860\n",
      "Epoch 164/500\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 55.5162 - val_loss: 766.8428\n",
      "Epoch 165/500\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 53.6616 - val_loss: 767.5208\n",
      "Epoch 166/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 51.8910 - val_loss: 768.2186\n",
      "Epoch 167/500\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 50.2014 - val_loss: 768.9343\n",
      "Epoch 168/500\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 48.5899 - val_loss: 769.6662\n",
      "Epoch 169/500\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 47.0538 - val_loss: 770.4125\n",
      "Epoch 170/500\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 45.5901 - val_loss: 771.1719\n",
      "Epoch 171/500\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 44.1964 - val_loss: 771.9427\n",
      "Epoch 172/500\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 42.8698 - val_loss: 772.7233\n",
      "Epoch 173/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 41.6079 - val_loss: 773.5125\n",
      "Epoch 174/500\n",
      "21/21 [==============================] - 0s 16ms/step - loss: 40.4080 - val_loss: 774.3087\n",
      "Epoch 175/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 39.2678 - val_loss: 775.1107\n",
      "Epoch 176/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 38.1850 - val_loss: 775.9171\n",
      "Epoch 177/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 37.1572 - val_loss: 776.7266\n",
      "Epoch 178/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 36.1821 - val_loss: 777.5382\n",
      "Epoch 179/500\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 35.2577 - val_loss: 778.3506\n",
      "Epoch 180/500\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 34.3816 - val_loss: 779.1629\n",
      "Epoch 181/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 33.5520 - val_loss: 779.9736\n",
      "Epoch 182/500\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 32.7667 - val_loss: 780.7822\n",
      "Epoch 183/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 32.0240 - val_loss: 781.5875\n",
      "Epoch 184/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 31.3216 - val_loss: 782.3886\n",
      "Epoch 185/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 30.6582 - val_loss: 783.1845\n",
      "Epoch 186/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 30.0318 - val_loss: 783.9749\n",
      "Epoch 187/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 29.4406 - val_loss: 784.7584\n",
      "Epoch 188/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 28.8831 - val_loss: 785.5347\n",
      "Epoch 189/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 28.3575 - val_loss: 786.3029\n",
      "Epoch 190/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 27.8626 - val_loss: 787.0623\n",
      "Epoch 191/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 27.3967 - val_loss: 787.8126\n",
      "Epoch 192/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 26.9583 - val_loss: 788.5529\n",
      "Epoch 193/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 26.5462 - val_loss: 789.2828\n",
      "Epoch 194/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 26.1590 - val_loss: 790.0017\n",
      "Epoch 195/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 25.7955 - val_loss: 790.7094\n",
      "Epoch 196/500\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 25.4544 - val_loss: 791.4055\n",
      "Epoch 197/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 25.1345 - val_loss: 792.0895\n",
      "Epoch 198/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 24.8347 - val_loss: 792.7611\n",
      "Epoch 199/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 24.5539 - val_loss: 793.4198\n",
      "Epoch 200/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 24.2911 - val_loss: 794.0657\n",
      "Epoch 201/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 24.0452 - val_loss: 794.6984\n",
      "Epoch 202/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 23.8154 - val_loss: 795.3175\n",
      "Epoch 203/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 23.6008 - val_loss: 795.9231\n",
      "Epoch 204/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 23.4004 - val_loss: 796.5150\n",
      "Epoch 205/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 23.2135 - val_loss: 797.0930\n",
      "Epoch 206/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 23.0392 - val_loss: 797.6569\n",
      "Epoch 207/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 22.8769 - val_loss: 798.2068\n",
      "Epoch 208/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 22.7257 - val_loss: 798.7429\n",
      "Epoch 209/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 22.5850 - val_loss: 799.2649\n",
      "Epoch 210/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 22.4542 - val_loss: 799.7728\n",
      "Epoch 211/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 22.3327 - val_loss: 800.2668\n",
      "Epoch 212/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 22.2197 - val_loss: 800.7468\n",
      "Epoch 213/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 22.1150 - val_loss: 801.2130\n",
      "Epoch 214/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 22.0178 - val_loss: 801.6652\n",
      "Epoch 215/500\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 21.9277 - val_loss: 802.1040\n",
      "Epoch 216/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 21.8443 - val_loss: 802.5290\n",
      "Epoch 217/500\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 21.7670 - val_loss: 802.9408\n",
      "Epoch 218/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 21.6955 - val_loss: 803.3392\n",
      "Epoch 219/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 21.6294 - val_loss: 803.7248\n",
      "Epoch 220/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 21.5683 - val_loss: 804.0974\n",
      "Epoch 221/500\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 21.5118 - val_loss: 804.4574\n",
      "Epoch 222/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 21.4597 - val_loss: 804.8048\n",
      "Epoch 223/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 21.4117 - val_loss: 805.1399\n",
      "Epoch 224/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 21.3674 - val_loss: 805.4630\n",
      "Epoch 225/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 21.3266 - val_loss: 805.7742\n",
      "Epoch 226/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 21.2890 - val_loss: 806.0740\n",
      "Epoch 227/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 21.2544 - val_loss: 806.3623\n",
      "Epoch 228/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 21.2225 - val_loss: 806.6395\n",
      "Epoch 229/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 21.1933 - val_loss: 806.9061\n",
      "Epoch 230/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 21.1663 - val_loss: 807.1619\n",
      "Epoch 231/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 21.1416 - val_loss: 807.4075\n",
      "Epoch 232/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 21.1190 - val_loss: 807.6432\n",
      "Epoch 233/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 21.0981 - val_loss: 807.8690\n",
      "Epoch 234/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 21.0790 - val_loss: 808.0852\n",
      "Epoch 235/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 21.0615 - val_loss: 808.2925\n",
      "Epoch 236/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 21.0454 - val_loss: 808.4904\n",
      "Epoch 237/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 21.0308 - val_loss: 808.6801\n",
      "Epoch 238/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 21.0173 - val_loss: 808.8611\n",
      "Epoch 239/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 21.0050 - val_loss: 809.0340\n",
      "Epoch 240/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9937 - val_loss: 809.1989\n",
      "Epoch 241/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9835 - val_loss: 809.3560\n",
      "Epoch 242/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9741 - val_loss: 809.5059\n",
      "Epoch 243/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.9655 - val_loss: 809.6484\n",
      "Epoch 244/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9577 - val_loss: 809.7843\n",
      "Epoch 245/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9506 - val_loss: 809.9135\n",
      "Epoch 246/500\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 20.9441 - val_loss: 810.0361\n",
      "Epoch 247/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.9382 - val_loss: 810.1528\n",
      "Epoch 248/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9328 - val_loss: 810.2635\n",
      "Epoch 249/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.9279 - val_loss: 810.3688\n",
      "Epoch 250/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9235 - val_loss: 810.4684\n",
      "Epoch 251/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.9194 - val_loss: 810.5628\n",
      "Epoch 252/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9158 - val_loss: 810.6521\n",
      "Epoch 253/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9125 - val_loss: 810.7366\n",
      "Epoch 254/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9095 - val_loss: 810.8166\n",
      "Epoch 255/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9068 - val_loss: 810.8920\n",
      "Epoch 256/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9044 - val_loss: 810.9633\n",
      "Epoch 257/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9022 - val_loss: 811.0307\n",
      "Epoch 258/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9003 - val_loss: 811.0941\n",
      "Epoch 259/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8986 - val_loss: 811.1540\n",
      "Epoch 260/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8971 - val_loss: 811.2102\n",
      "Epoch 261/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8957 - val_loss: 811.2630\n",
      "Epoch 262/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8945 - val_loss: 811.3129\n",
      "Epoch 263/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8935 - val_loss: 811.3596\n",
      "Epoch 264/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8926 - val_loss: 811.4037\n",
      "Epoch 265/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.8918 - val_loss: 811.4448\n",
      "Epoch 266/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8911 - val_loss: 811.4837\n",
      "Epoch 267/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8906 - val_loss: 811.5197\n",
      "Epoch 268/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.8901 - val_loss: 811.5535\n",
      "Epoch 269/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8898 - val_loss: 811.5851\n",
      "Epoch 270/500\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 20.8895 - val_loss: 811.6149\n",
      "Epoch 271/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.8893 - val_loss: 811.6423\n",
      "Epoch 272/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.8892 - val_loss: 811.6682\n",
      "Epoch 273/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.8891 - val_loss: 811.6923\n",
      "Epoch 274/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8891 - val_loss: 811.7146\n",
      "Epoch 275/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8892 - val_loss: 811.7354\n",
      "Epoch 276/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8892 - val_loss: 811.7545\n",
      "Epoch 277/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8895 - val_loss: 811.7723\n",
      "Epoch 278/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8897 - val_loss: 811.7890\n",
      "Epoch 279/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8899 - val_loss: 811.8044\n",
      "Epoch 280/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8902 - val_loss: 811.8187\n",
      "Epoch 281/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8906 - val_loss: 811.8318\n",
      "Epoch 282/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8909 - val_loss: 811.8441\n",
      "Epoch 283/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8913 - val_loss: 811.8551\n",
      "Epoch 284/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8917 - val_loss: 811.8654\n",
      "Epoch 285/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8922 - val_loss: 811.8749\n",
      "Epoch 286/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8926 - val_loss: 811.8835\n",
      "Epoch 287/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8932 - val_loss: 811.8916\n",
      "Epoch 288/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8936 - val_loss: 811.8987\n",
      "Epoch 289/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8942 - val_loss: 811.9053\n",
      "Epoch 290/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8948 - val_loss: 811.9116\n",
      "Epoch 291/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8954 - val_loss: 811.9171\n",
      "Epoch 292/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8959 - val_loss: 811.9220\n",
      "Epoch 293/500\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 20.8966 - val_loss: 811.9264\n",
      "Epoch 294/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8971 - val_loss: 811.9303\n",
      "Epoch 295/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.8978 - val_loss: 811.9336\n",
      "Epoch 296/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8984 - val_loss: 811.9370\n",
      "Epoch 297/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8991 - val_loss: 811.9398\n",
      "Epoch 298/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.8997 - val_loss: 811.9423\n",
      "Epoch 299/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9004 - val_loss: 811.9445\n",
      "Epoch 300/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9011 - val_loss: 811.9464\n",
      "Epoch 301/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9017 - val_loss: 811.9480\n",
      "Epoch 302/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9024 - val_loss: 811.9496\n",
      "Epoch 303/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9031 - val_loss: 811.9507\n",
      "Epoch 304/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9037 - val_loss: 811.9517\n",
      "Epoch 305/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9045 - val_loss: 811.9526\n",
      "Epoch 306/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9051 - val_loss: 811.9530\n",
      "Epoch 307/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9059 - val_loss: 811.9536\n",
      "Epoch 308/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 20.9065 - val_loss: 811.9540\n",
      "Epoch 309/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9072 - val_loss: 811.9543\n",
      "Epoch 310/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9079 - val_loss: 811.9542\n",
      "Epoch 311/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9086 - val_loss: 811.9540\n",
      "Epoch 312/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9093 - val_loss: 811.9539\n",
      "Epoch 313/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9100 - val_loss: 811.9537\n",
      "Epoch 314/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.9107 - val_loss: 811.9533\n",
      "Epoch 315/500\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 20.9114 - val_loss: 811.9526\n",
      "Epoch 316/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.9121 - val_loss: 811.9524\n",
      "Epoch 317/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9128 - val_loss: 811.9516\n",
      "Epoch 318/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.9134 - val_loss: 811.9509\n",
      "Epoch 319/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9142 - val_loss: 811.9503\n",
      "Epoch 320/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9148 - val_loss: 811.9496\n",
      "Epoch 321/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9155 - val_loss: 811.9490\n",
      "Epoch 322/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9162 - val_loss: 811.9479\n",
      "Epoch 323/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9169 - val_loss: 811.9473\n",
      "Epoch 324/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9176 - val_loss: 811.9465\n",
      "Epoch 325/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9182 - val_loss: 811.9457\n",
      "Epoch 326/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9188 - val_loss: 811.9447\n",
      "Epoch 327/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9195 - val_loss: 811.9440\n",
      "Epoch 328/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9202 - val_loss: 811.9431\n",
      "Epoch 329/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9208 - val_loss: 811.9420\n",
      "Epoch 330/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9214 - val_loss: 811.9412\n",
      "Epoch 331/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9220 - val_loss: 811.9401\n",
      "Epoch 332/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9227 - val_loss: 811.9390\n",
      "Epoch 333/500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 20.9233 - val_loss: 811.9382\n",
      "Epoch 334/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9239 - val_loss: 811.9371\n",
      "Epoch 335/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9246 - val_loss: 811.9361\n",
      "Epoch 336/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9252 - val_loss: 811.9351\n",
      "Epoch 337/500\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 20.9257 - val_loss: 811.9339\n",
      "Epoch 338/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.9264 - val_loss: 811.9330\n",
      "Epoch 339/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9270 - val_loss: 811.9319\n",
      "Epoch 340/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9276 - val_loss: 811.9310\n",
      "Epoch 341/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9282 - val_loss: 811.9300\n",
      "Epoch 342/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9287 - val_loss: 811.9289\n",
      "Epoch 343/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9293 - val_loss: 811.9279\n",
      "Epoch 344/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9299 - val_loss: 811.9270\n",
      "Epoch 345/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9304 - val_loss: 811.9260\n",
      "Epoch 346/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9310 - val_loss: 811.9251\n",
      "Epoch 347/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9315 - val_loss: 811.9238\n",
      "Epoch 348/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9321 - val_loss: 811.9230\n",
      "Epoch 349/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9326 - val_loss: 811.9221\n",
      "Epoch 350/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9331 - val_loss: 811.9211\n",
      "Epoch 351/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9337 - val_loss: 811.9202\n",
      "Epoch 352/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9342 - val_loss: 811.9193\n",
      "Epoch 353/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9347 - val_loss: 811.9182\n",
      "Epoch 354/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9353 - val_loss: 811.9172\n",
      "Epoch 355/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9358 - val_loss: 811.9164\n",
      "Epoch 356/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9362 - val_loss: 811.9154\n",
      "Epoch 357/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9367 - val_loss: 811.9146\n",
      "Epoch 358/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9372 - val_loss: 811.9138\n",
      "Epoch 359/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9377 - val_loss: 811.9128\n",
      "Epoch 360/500\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 20.9381 - val_loss: 811.9120\n",
      "Epoch 361/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.9386 - val_loss: 811.9111\n",
      "Epoch 362/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9391 - val_loss: 811.9102\n",
      "Epoch 363/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9395 - val_loss: 811.9092\n",
      "Epoch 364/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9400 - val_loss: 811.9083\n",
      "Epoch 365/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9405 - val_loss: 811.9076\n",
      "Epoch 366/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9409 - val_loss: 811.9069\n",
      "Epoch 367/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9413 - val_loss: 811.9061\n",
      "Epoch 368/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9418 - val_loss: 811.9056\n",
      "Epoch 369/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9421 - val_loss: 811.9045\n",
      "Epoch 370/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9425 - val_loss: 811.9041\n",
      "Epoch 371/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9429 - val_loss: 811.9030\n",
      "Epoch 372/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9434 - val_loss: 811.9025\n",
      "Epoch 373/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9437 - val_loss: 811.9015\n",
      "Epoch 374/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9442 - val_loss: 811.9011\n",
      "Epoch 375/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9445 - val_loss: 811.9003\n",
      "Epoch 376/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9449 - val_loss: 811.8996\n",
      "Epoch 377/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.9453 - val_loss: 811.8990\n",
      "Epoch 378/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 20.9456 - val_loss: 811.8981\n",
      "Epoch 379/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9460 - val_loss: 811.8975\n",
      "Epoch 380/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9464 - val_loss: 811.8970\n",
      "Epoch 381/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.9467 - val_loss: 811.8964\n",
      "Epoch 382/500\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 20.9471 - val_loss: 811.8956\n",
      "Epoch 383/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.9474 - val_loss: 811.8948\n",
      "Epoch 384/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9477 - val_loss: 811.8943\n",
      "Epoch 385/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.9481 - val_loss: 811.8937\n",
      "Epoch 386/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9484 - val_loss: 811.8930\n",
      "Epoch 387/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9487 - val_loss: 811.8923\n",
      "Epoch 388/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9491 - val_loss: 811.8918\n",
      "Epoch 389/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9494 - val_loss: 811.8914\n",
      "Epoch 390/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9497 - val_loss: 811.8911\n",
      "Epoch 391/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9499 - val_loss: 811.8904\n",
      "Epoch 392/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9502 - val_loss: 811.8898\n",
      "Epoch 393/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9505 - val_loss: 811.8892\n",
      "Epoch 394/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9508 - val_loss: 811.8886\n",
      "Epoch 395/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9511 - val_loss: 811.8881\n",
      "Epoch 396/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9514 - val_loss: 811.8874\n",
      "Epoch 397/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.9516 - val_loss: 811.8867\n",
      "Epoch 398/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9520 - val_loss: 811.8864\n",
      "Epoch 399/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9522 - val_loss: 811.8859\n",
      "Epoch 400/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9525 - val_loss: 811.8854\n",
      "Epoch 401/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9527 - val_loss: 811.8850\n",
      "Epoch 402/500\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 20.9530 - val_loss: 811.8848\n",
      "Epoch 403/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.9532 - val_loss: 811.8843\n",
      "Epoch 404/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9534 - val_loss: 811.8836\n",
      "Epoch 405/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9537 - val_loss: 811.8832\n",
      "Epoch 406/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9539 - val_loss: 811.8829\n",
      "Epoch 407/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9541 - val_loss: 811.8824\n",
      "Epoch 408/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9544 - val_loss: 811.8820\n",
      "Epoch 409/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9546 - val_loss: 811.8817\n",
      "Epoch 410/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 20.9548 - val_loss: 811.8809\n",
      "Epoch 411/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 20.9551 - val_loss: 811.8808\n",
      "Epoch 412/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9553 - val_loss: 811.8804\n",
      "Epoch 413/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9555 - val_loss: 811.8801\n",
      "Epoch 414/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9557 - val_loss: 811.8798\n",
      "Epoch 415/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9558 - val_loss: 811.8793\n",
      "Epoch 416/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9561 - val_loss: 811.8788\n",
      "Epoch 417/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9563 - val_loss: 811.8785\n",
      "Epoch 418/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9565 - val_loss: 811.8784\n",
      "Epoch 419/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9566 - val_loss: 811.8779\n",
      "Epoch 420/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9568 - val_loss: 811.8776\n",
      "Epoch 421/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.9570 - val_loss: 811.8771\n",
      "Epoch 422/500\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 20.9571 - val_loss: 811.8768\n",
      "Epoch 423/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.9574 - val_loss: 811.8765\n",
      "Epoch 424/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.9575 - val_loss: 811.8760\n",
      "Epoch 425/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9577 - val_loss: 811.8759\n",
      "Epoch 426/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.9578 - val_loss: 811.8757\n",
      "Epoch 427/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9580 - val_loss: 811.8754\n",
      "Epoch 428/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9581 - val_loss: 811.8751\n",
      "Epoch 429/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9583 - val_loss: 811.8749\n",
      "Epoch 430/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9585 - val_loss: 811.8745\n",
      "Epoch 431/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9586 - val_loss: 811.8743\n",
      "Epoch 432/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9588 - val_loss: 811.8738\n",
      "Epoch 433/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9589 - val_loss: 811.8738\n",
      "Epoch 434/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9590 - val_loss: 811.8734\n",
      "Epoch 435/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9592 - val_loss: 811.8731\n",
      "Epoch 436/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9593 - val_loss: 811.8728\n",
      "Epoch 437/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9595 - val_loss: 811.8726\n",
      "Epoch 438/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9596 - val_loss: 811.8721\n",
      "Epoch 439/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9597 - val_loss: 811.8721\n",
      "Epoch 440/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9598 - val_loss: 811.8719\n",
      "Epoch 441/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9599 - val_loss: 811.8715\n",
      "Epoch 442/500\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 20.9600 - val_loss: 811.8711\n",
      "Epoch 443/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9602 - val_loss: 811.8708\n",
      "Epoch 444/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9604 - val_loss: 811.8708\n",
      "Epoch 445/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9604 - val_loss: 811.8705\n",
      "Epoch 446/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9606 - val_loss: 811.8704\n",
      "Epoch 447/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9606 - val_loss: 811.8702\n",
      "Epoch 448/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9608 - val_loss: 811.8701\n",
      "Epoch 449/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 20.9609 - val_loss: 811.8700\n",
      "Epoch 450/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9610 - val_loss: 811.8698\n",
      "Epoch 451/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.9611 - val_loss: 811.8696\n",
      "Epoch 452/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9612 - val_loss: 811.8694\n",
      "Epoch 453/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9612 - val_loss: 811.8691\n",
      "Epoch 454/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9614 - val_loss: 811.8690\n",
      "Epoch 455/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 20.9615 - val_loss: 811.8690\n",
      "Epoch 456/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9616 - val_loss: 811.8690\n",
      "Epoch 457/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9616 - val_loss: 811.8687\n",
      "Epoch 458/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9617 - val_loss: 811.8685\n",
      "Epoch 459/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9618 - val_loss: 811.8680\n",
      "Epoch 460/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.9619 - val_loss: 811.8679\n",
      "Epoch 461/500\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 20.9620 - val_loss: 811.8679\n",
      "Epoch 462/500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 20.9621 - val_loss: 811.8677\n",
      "Epoch 463/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9621 - val_loss: 811.8675\n",
      "Epoch 464/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9622 - val_loss: 811.8673\n",
      "Epoch 465/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9623 - val_loss: 811.8673\n",
      "Epoch 466/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9624 - val_loss: 811.8671\n",
      "Epoch 467/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9625 - val_loss: 811.8669\n",
      "Epoch 468/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9625 - val_loss: 811.8669\n",
      "Epoch 469/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9626 - val_loss: 811.8668\n",
      "Epoch 470/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9627 - val_loss: 811.8666\n",
      "Epoch 471/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.9627 - val_loss: 811.8664\n",
      "Epoch 472/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9628 - val_loss: 811.8662\n",
      "Epoch 473/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9629 - val_loss: 811.8663\n",
      "Epoch 474/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9629 - val_loss: 811.8661\n",
      "Epoch 475/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9630 - val_loss: 811.8660\n",
      "Epoch 476/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9631 - val_loss: 811.8658\n",
      "Epoch 477/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9631 - val_loss: 811.8656\n",
      "Epoch 478/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9632 - val_loss: 811.8655\n",
      "Epoch 479/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9632 - val_loss: 811.8654\n",
      "Epoch 480/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9633 - val_loss: 811.8652\n",
      "Epoch 481/500\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 20.9634 - val_loss: 811.8652\n",
      "Epoch 482/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.9635 - val_loss: 811.8652\n",
      "Epoch 483/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9635 - val_loss: 811.8652\n",
      "Epoch 484/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.9635 - val_loss: 811.8651\n",
      "Epoch 485/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9636 - val_loss: 811.8647\n",
      "Epoch 486/500\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20.9637 - val_loss: 811.8647\n",
      "Epoch 487/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9637 - val_loss: 811.8647\n",
      "Epoch 488/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9637 - val_loss: 811.8647\n",
      "Epoch 489/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9638 - val_loss: 811.8647\n",
      "Epoch 490/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9638 - val_loss: 811.8644\n",
      "Epoch 491/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9638 - val_loss: 811.8645\n",
      "Epoch 492/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9639 - val_loss: 811.8644\n",
      "Epoch 493/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9639 - val_loss: 811.8645\n",
      "Epoch 494/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9639 - val_loss: 811.8642\n",
      "Epoch 495/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9640 - val_loss: 811.8642\n",
      "Epoch 496/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9640 - val_loss: 811.8640\n",
      "Epoch 497/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9641 - val_loss: 811.8639\n",
      "Epoch 498/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9641 - val_loss: 811.8639\n",
      "Epoch 499/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9642 - val_loss: 811.8639\n",
      "Epoch 500/500\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20.9642 - val_loss: 811.8636\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(85.7336, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0020, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0125, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3008, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 512ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[64.5205882 , 64.4701681 , 64.4197479 , 64.3693277 , 64.3189076 ,\n",
       "        64.2789916 , 64.2453781 ,  0.        ,  0.23512587,  0.        ,\n",
       "         0.        ,  0.5593586 ,  0.57299435, 65.02836134, 64.95273109,\n",
       "        64.88473389, 64.83431373, 64.78389356, 64.73347339, 64.68305322,\n",
       "        64.63263305, 64.58221289, 64.53179272, 64.48137255, 64.43095238,\n",
       "        64.38053221, 64.33011204, 64.28646125, 64.25284781, 64.21923436,\n",
       "        64.18562092, 64.15200747, 64.11839402, 64.08478058, 64.05116713,\n",
       "        64.01755369, 63.98394024, 63.9503268 , 63.91671335, 63.84929972,\n",
       "         1.11738777,  0.1888071 ,  0.        ,  0.53950113,  0.32698724,\n",
       "         0.        ,  0.43648201, 64.3917367 , 64.3413165 , 64.2939309 ,\n",
       "        64.2603175 , 64.226704  , 64.1930906 , 64.1594771 , 64.1258637 ,\n",
       "        64.0922502 , 64.0586368 , 64.0250233 , 63.9914099 , 63.9577964 ,\n",
       "        63.924183  , 63.8717087 , 66.7063142 , 66.4668184 , 66.2273226 ,\n",
       "        65.9869827 , 65.7510504 , 65.5241597 , 65.2972689 , 65.0703781 ,\n",
       "        64.8623249 ,  0.75161713,  0.        , 68.789444  ,  0.08362568,\n",
       "         0.        ,  0.17432921,  0.22967985,  0.        ,  0.        ,\n",
       "        47.15135574,  0.12042482,  0.        ,  0.24751106,  0.58038557,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.56010783,\n",
       "         0.        ,  0.        ,  0.34930974,  0.        ,  0.39924729,\n",
       "         0.        ,  0.        ,  0.85195249,  0.        ,  0.33633748]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58.76985294, 58.76471755, 58.75958217, 58.75444678, 58.74931139,\n",
       "       58.744176  , 58.73904062, 58.73390523, 58.72876984, 58.72363445,\n",
       "       58.71849907, 58.71336368, 58.70822829, 58.7030929 , 58.69795752,\n",
       "       58.69282213, 58.68768674, 58.68255135, 58.67741597, 58.67228058,\n",
       "       58.66714519, 58.6620098 , 58.65687442, 58.65173903, 58.64660364,\n",
       "       58.64146825, 58.63633287, 58.63119748, 58.62606209, 58.6209267 ,\n",
       "       58.61579132, 58.61065593, 58.60552054, 58.60038515, 58.59165813,\n",
       "       58.58263989, 58.57362164, 58.5646034 , 58.55558516, 58.54656692,\n",
       "       58.53754868, 58.52853044, 58.5195122 , 58.51049395, 58.50147571,\n",
       "       58.49245747, 58.48343923, 58.47442099, 58.46540275, 58.45638451,\n",
       "       58.44736626, 58.43834802, 58.42932978, 58.42031154, 58.4112933 ,\n",
       "       58.40227506, 58.39325681, 58.38423857, 58.37522033, 58.36620209,\n",
       "       58.35718385, 58.34816561, 58.33914737, 58.33012912, 58.32111088,\n",
       "       58.31209264, 58.3030744 , 58.29405616, 58.28503792, 58.27601968,\n",
       "       58.26700143, 58.25798319, 58.24896495, 58.23994671, 58.23092847,\n",
       "       58.22191023, 58.21289199, 58.20387374, 58.1948555 , 58.18583726,\n",
       "       58.17681902, 58.16780078, 58.15878254, 58.1497643 , 58.14074605,\n",
       "       58.13172781, 58.12270957, 58.11369133, 58.10467309, 58.09565485,\n",
       "       58.08663661, 58.07761836, 58.06860012, 58.05958188, 58.05056364,\n",
       "       58.0415454 , 58.03252716, 58.02350892, 58.01449067, 58.00547243])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.04251019129815\n",
      "26.91705285968315\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
