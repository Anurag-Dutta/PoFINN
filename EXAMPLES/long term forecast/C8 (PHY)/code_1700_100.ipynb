{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "1795    61.356629\n",
       "1796    61.347292\n",
       "1797    61.337955\n",
       "1798    61.328618\n",
       "1799    61.319281\n",
       "Name: C8, Length: 1800, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c8_interpolated_1700_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "1695     0.000000\n",
       "1696     0.039109\n",
       "1697     0.572994\n",
       "1698     0.000000\n",
       "1699     0.362969\n",
       "Name: C8, Length: 1700, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1700)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.987115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.774230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.561345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.348459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.200000  0.000298\n",
       "1     87.987115  0.000298\n",
       "2     87.774230  0.000297\n",
       "3     87.561345  0.000297\n",
       "4     87.348459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 85.7336  # Value for C0\n",
    "K0 = -0.0020  # Value for K0\n",
    "K1 = 0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0125    # Value for b\n",
    "c = 2.3008    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.200000    87.987115    87.774230    87.561345    87.348459   \n",
      "351    87.987115    87.774230    87.561345    87.348459    87.135574   \n",
      "352    87.774230    87.561345    87.348459    87.135574    86.922689   \n",
      "353    87.561345    87.348459    87.135574    86.922689    86.709804   \n",
      "354    87.348459    87.135574    86.922689    86.709804    86.496919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.135574    86.922689    86.709804    86.496919    86.294538  ...   \n",
      "351    86.922689    86.709804    86.496919    86.294538    86.221709  ...   \n",
      "352    86.709804    86.496919    86.294538    86.221709    86.148880  ...   \n",
      "353    86.496919    86.294538    86.221709    86.148880    86.076050  ...   \n",
      "354    86.294538    86.221709    86.148880    86.076050    86.003221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   76.094351    0.000263   76.058870    0.000263   76.023389    0.000263   \n",
      "351   76.058870    0.000263   76.023389    0.000263   75.987909    0.000262   \n",
      "352   76.023389    0.000263   75.987909    0.000262   75.952428    0.000262   \n",
      "353   75.987909    0.000262   75.952428    0.000262   75.916947    0.000262   \n",
      "354   75.952428    0.000262   75.916947    0.000262   75.881466    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   75.987909    0.000262   75.952428    0.000262  \n",
      "351   75.952428    0.000262   75.916947    0.000262  \n",
      "352   75.916947    0.000262   75.881466    0.000262  \n",
      "353   75.881466    0.000262   75.845985    0.000262  \n",
      "354   75.845985    0.000262   75.810504    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1250, 1, 251) (1250, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "16/16 [==============================] - 2s 32ms/step - loss: 4873.2710 - val_loss: 3869.7566\n",
      "Epoch 2/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4763.6680 - val_loss: 3815.4690\n",
      "Epoch 3/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4701.4644 - val_loss: 3746.6917\n",
      "Epoch 4/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4620.8208 - val_loss: 3682.9485\n",
      "Epoch 5/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4553.8789 - val_loss: 3625.1436\n",
      "Epoch 6/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4488.4995 - val_loss: 3565.5752\n",
      "Epoch 7/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4415.2261 - val_loss: 3497.3887\n",
      "Epoch 8/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4345.8242 - val_loss: 3437.4089\n",
      "Epoch 9/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4279.5430 - val_loss: 3378.8784\n",
      "Epoch 10/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4214.7231 - val_loss: 3321.5852\n",
      "Epoch 11/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4151.1328 - val_loss: 3265.3479\n",
      "Epoch 12/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4088.6042 - val_loss: 3210.0500\n",
      "Epoch 13/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4027.0320 - val_loss: 3155.6155\n",
      "Epoch 14/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3966.3425 - val_loss: 3101.9902\n",
      "Epoch 15/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3906.4854 - val_loss: 3049.1370\n",
      "Epoch 16/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3847.4238 - val_loss: 2997.0244\n",
      "Epoch 17/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3789.1272 - val_loss: 2945.6299\n",
      "Epoch 18/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3731.5737 - val_loss: 2894.9336\n",
      "Epoch 19/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3674.7417 - val_loss: 2844.9194\n",
      "Epoch 20/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3618.6165 - val_loss: 2795.5723\n",
      "Epoch 21/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3563.1826 - val_loss: 2746.8809\n",
      "Epoch 22/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 3508.4272 - val_loss: 2698.8323\n",
      "Epoch 23/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3454.3406 - val_loss: 2651.4180\n",
      "Epoch 24/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3400.9099 - val_loss: 2604.6274\n",
      "Epoch 25/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3348.1274 - val_loss: 2558.4541\n",
      "Epoch 26/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3295.9849 - val_loss: 2512.8877\n",
      "Epoch 27/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3244.4727 - val_loss: 2467.9214\n",
      "Epoch 28/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3193.5837 - val_loss: 2423.5488\n",
      "Epoch 29/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3143.3103 - val_loss: 2379.7615\n",
      "Epoch 30/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3093.6462 - val_loss: 2336.5540\n",
      "Epoch 31/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3044.5842 - val_loss: 2293.9202\n",
      "Epoch 32/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2996.1184 - val_loss: 2251.8525\n",
      "Epoch 33/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2948.2410 - val_loss: 2210.3462\n",
      "Epoch 34/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2900.9482 - val_loss: 2169.3953\n",
      "Epoch 35/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2854.2332 - val_loss: 2128.9941\n",
      "Epoch 36/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2808.0896 - val_loss: 2089.1377\n",
      "Epoch 37/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2762.5127 - val_loss: 2049.8196\n",
      "Epoch 38/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2717.4971 - val_loss: 2011.0345\n",
      "Epoch 39/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2673.0376 - val_loss: 1972.7788\n",
      "Epoch 40/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2629.1279 - val_loss: 1935.0463\n",
      "Epoch 41/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2585.7637 - val_loss: 1897.8319\n",
      "Epoch 42/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2542.9402 - val_loss: 1861.1310\n",
      "Epoch 43/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2500.6523 - val_loss: 1824.9380\n",
      "Epoch 44/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2458.8948 - val_loss: 1789.2499\n",
      "Epoch 45/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2417.6633 - val_loss: 1754.0605\n",
      "Epoch 46/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2376.9524 - val_loss: 1719.3657\n",
      "Epoch 47/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2336.7581 - val_loss: 1685.1603\n",
      "Epoch 48/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2297.0757 - val_loss: 1651.4404\n",
      "Epoch 49/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2257.8997 - val_loss: 1618.2017\n",
      "Epoch 50/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2219.2273 - val_loss: 1585.4385\n",
      "Epoch 51/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2181.0525 - val_loss: 1553.1471\n",
      "Epoch 52/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2143.3716 - val_loss: 1521.3237\n",
      "Epoch 53/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2106.1802 - val_loss: 1489.9634\n",
      "Epoch 54/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2069.4736 - val_loss: 1459.0620\n",
      "Epoch 55/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2033.2488 - val_loss: 1428.6152\n",
      "Epoch 56/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1997.4993 - val_loss: 1398.6184\n",
      "Epoch 57/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1962.2220 - val_loss: 1369.0685\n",
      "Epoch 58/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1927.4135 - val_loss: 1339.9604\n",
      "Epoch 59/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1893.0682 - val_loss: 1311.2894\n",
      "Epoch 60/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1859.1831 - val_loss: 1283.0531\n",
      "Epoch 61/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1825.7539 - val_loss: 1255.2460\n",
      "Epoch 62/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1792.7756 - val_loss: 1227.8647\n",
      "Epoch 63/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1760.2451 - val_loss: 1200.9052\n",
      "Epoch 64/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1728.1584 - val_loss: 1174.3629\n",
      "Epoch 65/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1696.5115 - val_loss: 1148.2347\n",
      "Epoch 66/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1665.3000 - val_loss: 1122.5160\n",
      "Epoch 67/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1634.5204 - val_loss: 1097.2028\n",
      "Epoch 68/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1604.1678 - val_loss: 1072.2916\n",
      "Epoch 69/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1574.2395 - val_loss: 1047.7791\n",
      "Epoch 70/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1544.7317 - val_loss: 1023.6599\n",
      "Epoch 71/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1515.6396 - val_loss: 999.9319\n",
      "Epoch 72/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1486.9604 - val_loss: 976.5901\n",
      "Epoch 73/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1458.6898 - val_loss: 953.6308\n",
      "Epoch 74/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1430.8245 - val_loss: 931.0510\n",
      "Epoch 75/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1403.3596 - val_loss: 908.8460\n",
      "Epoch 76/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1376.2925 - val_loss: 887.0124\n",
      "Epoch 77/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1349.6193 - val_loss: 865.5471\n",
      "Epoch 78/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1323.3357 - val_loss: 844.4453\n",
      "Epoch 79/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1297.4386 - val_loss: 823.7046\n",
      "Epoch 80/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1271.9248 - val_loss: 803.3210\n",
      "Epoch 81/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1246.7900 - val_loss: 783.2897\n",
      "Epoch 82/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1222.0304 - val_loss: 763.6080\n",
      "Epoch 83/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1197.6426 - val_loss: 744.2723\n",
      "Epoch 84/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1173.6234 - val_loss: 725.2789\n",
      "Epoch 85/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1149.9691 - val_loss: 706.6243\n",
      "Epoch 86/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1126.6761 - val_loss: 688.3057\n",
      "Epoch 87/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1103.7408 - val_loss: 670.3179\n",
      "Epoch 88/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1081.1604 - val_loss: 652.6588\n",
      "Epoch 89/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1058.9307 - val_loss: 635.3248\n",
      "Epoch 90/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1037.0483 - val_loss: 618.3112\n",
      "Epoch 91/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1015.5097 - val_loss: 601.6152\n",
      "Epoch 92/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 994.3114 - val_loss: 585.2341\n",
      "Epoch 93/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 973.4505 - val_loss: 569.1631\n",
      "Epoch 94/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 952.9228 - val_loss: 553.4001\n",
      "Epoch 95/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 932.7262 - val_loss: 537.9411\n",
      "Epoch 96/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 912.8558 - val_loss: 522.7822\n",
      "Epoch 97/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 893.3094 - val_loss: 507.9207\n",
      "Epoch 98/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 874.0829 - val_loss: 493.3529\n",
      "Epoch 99/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 855.1736 - val_loss: 479.0760\n",
      "Epoch 100/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 836.5779 - val_loss: 465.0857\n",
      "Epoch 101/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 818.2925 - val_loss: 451.3794\n",
      "Epoch 102/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 800.3140 - val_loss: 437.9536\n",
      "Epoch 103/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 782.6392 - val_loss: 424.8048\n",
      "Epoch 104/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 765.2652 - val_loss: 411.9296\n",
      "Epoch 105/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 748.1879 - val_loss: 399.3253\n",
      "Epoch 106/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 731.4049 - val_loss: 386.9879\n",
      "Epoch 107/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 714.9125 - val_loss: 374.9150\n",
      "Epoch 108/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 698.7079 - val_loss: 363.1025\n",
      "Epoch 109/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 682.7876 - val_loss: 351.5479\n",
      "Epoch 110/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 667.1486 - val_loss: 340.2473\n",
      "Epoch 111/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 651.7880 - val_loss: 329.1988\n",
      "Epoch 112/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 636.7020 - val_loss: 318.3971\n",
      "Epoch 113/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 621.8880 - val_loss: 307.8409\n",
      "Epoch 114/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 607.3423 - val_loss: 297.5258\n",
      "Epoch 115/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 593.0624 - val_loss: 287.4494\n",
      "Epoch 116/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 579.0450 - val_loss: 277.6082\n",
      "Epoch 117/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 565.2867 - val_loss: 267.9991\n",
      "Epoch 118/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 551.7845 - val_loss: 258.6190\n",
      "Epoch 119/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 538.5358 - val_loss: 249.4650\n",
      "Epoch 120/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 525.5371 - val_loss: 240.5339\n",
      "Epoch 121/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 512.7856 - val_loss: 231.8221\n",
      "Epoch 122/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 500.2784 - val_loss: 223.3277\n",
      "Epoch 123/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 488.0121 - val_loss: 215.0467\n",
      "Epoch 124/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 475.9840 - val_loss: 206.9766\n",
      "Epoch 125/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 464.1914 - val_loss: 199.1137\n",
      "Epoch 126/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 452.6304 - val_loss: 191.4556\n",
      "Epoch 127/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 441.2988 - val_loss: 183.9986\n",
      "Epoch 128/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 430.1932 - val_loss: 176.7407\n",
      "Epoch 129/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 419.3112 - val_loss: 169.6780\n",
      "Epoch 130/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 408.6496 - val_loss: 162.8082\n",
      "Epoch 131/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 398.2055 - val_loss: 156.1282\n",
      "Epoch 132/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 387.9760 - val_loss: 149.6348\n",
      "Epoch 133/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 377.9581 - val_loss: 143.3253\n",
      "Epoch 134/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 368.1490 - val_loss: 137.1966\n",
      "Epoch 135/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 358.5461 - val_loss: 131.2457\n",
      "Epoch 136/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 349.1463 - val_loss: 125.4705\n",
      "Epoch 137/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 339.9470 - val_loss: 119.8675\n",
      "Epoch 138/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 330.9452 - val_loss: 114.4335\n",
      "Epoch 139/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 322.1379 - val_loss: 109.1662\n",
      "Epoch 140/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 313.5226 - val_loss: 104.0630\n",
      "Epoch 141/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 305.0966 - val_loss: 99.1205\n",
      "Epoch 142/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 296.8569 - val_loss: 94.3361\n",
      "Epoch 143/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 288.8009 - val_loss: 89.7070\n",
      "Epoch 144/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 280.9259 - val_loss: 85.2310\n",
      "Epoch 145/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 273.2291 - val_loss: 80.9044\n",
      "Epoch 146/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 265.7079 - val_loss: 76.7252\n",
      "Epoch 147/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 258.3595 - val_loss: 72.6903\n",
      "Epoch 148/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 251.1814 - val_loss: 68.7970\n",
      "Epoch 149/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 244.1707 - val_loss: 65.0433\n",
      "Epoch 150/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 237.3251 - val_loss: 61.4256\n",
      "Epoch 151/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 230.6417 - val_loss: 57.9413\n",
      "Epoch 152/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 224.1179 - val_loss: 54.5885\n",
      "Epoch 153/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 217.7512 - val_loss: 51.3640\n",
      "Epoch 154/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 211.5389 - val_loss: 48.2656\n",
      "Epoch 155/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 205.4787 - val_loss: 45.2903\n",
      "Epoch 156/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 199.5677 - val_loss: 42.4358\n",
      "Epoch 157/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 193.8037 - val_loss: 39.6991\n",
      "Epoch 158/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 188.1840 - val_loss: 37.0786\n",
      "Epoch 159/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 182.7063 - val_loss: 34.5712\n",
      "Epoch 160/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 177.3680 - val_loss: 32.1741\n",
      "Epoch 161/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 172.1666 - val_loss: 29.8855\n",
      "Epoch 162/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 167.0997 - val_loss: 27.7026\n",
      "Epoch 163/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 162.1650 - val_loss: 25.6230\n",
      "Epoch 164/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 157.3599 - val_loss: 23.6444\n",
      "Epoch 165/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 152.6822 - val_loss: 21.7642\n",
      "Epoch 166/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 148.1293 - val_loss: 19.9803\n",
      "Epoch 167/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 143.6991 - val_loss: 18.2902\n",
      "Epoch 168/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 139.3892 - val_loss: 16.6916\n",
      "Epoch 169/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 135.1972 - val_loss: 15.1822\n",
      "Epoch 170/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 131.1210 - val_loss: 13.7597\n",
      "Epoch 171/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 127.1581 - val_loss: 12.4218\n",
      "Epoch 172/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 123.3064 - val_loss: 11.1666\n",
      "Epoch 173/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 119.5637 - val_loss: 9.9913\n",
      "Epoch 174/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 115.9277 - val_loss: 8.8943\n",
      "Epoch 175/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 112.3963 - val_loss: 7.8731\n",
      "Epoch 176/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 108.9673 - val_loss: 6.9256\n",
      "Epoch 177/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 105.6385 - val_loss: 6.0499\n",
      "Epoch 178/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 102.4080 - val_loss: 5.2436\n",
      "Epoch 179/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 99.2735 - val_loss: 4.5048\n",
      "Epoch 180/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 96.2328 - val_loss: 3.8315\n",
      "Epoch 181/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 93.2842 - val_loss: 3.2216\n",
      "Epoch 182/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 90.4253 - val_loss: 2.6732\n",
      "Epoch 183/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 87.6544 - val_loss: 2.1841\n",
      "Epoch 184/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 84.9694 - val_loss: 1.7527\n",
      "Epoch 185/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 82.3682 - val_loss: 1.3769\n",
      "Epoch 186/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 79.8491 - val_loss: 1.0549\n",
      "Epoch 187/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 77.4101 - val_loss: 0.7847\n",
      "Epoch 188/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 75.0492 - val_loss: 0.5647\n",
      "Epoch 189/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 72.7647 - val_loss: 0.3928\n",
      "Epoch 190/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 70.5547 - val_loss: 0.2675\n",
      "Epoch 191/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 68.4173 - val_loss: 0.1870\n",
      "Epoch 192/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 66.3506 - val_loss: 0.1494\n",
      "Epoch 193/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 64.3531 - val_loss: 0.1533\n",
      "Epoch 194/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 62.4230 - val_loss: 0.1968\n",
      "Epoch 195/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 60.5585 - val_loss: 0.2783\n",
      "Epoch 196/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 58.7579 - val_loss: 0.3963\n",
      "Epoch 197/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 57.0195 - val_loss: 0.5492\n",
      "Epoch 198/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 55.3419 - val_loss: 0.7354\n",
      "Epoch 199/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 53.7232 - val_loss: 0.9535\n",
      "Epoch 200/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 52.1619 - val_loss: 1.2018\n",
      "Epoch 201/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 50.6566 - val_loss: 1.4791\n",
      "Epoch 202/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 49.2055 - val_loss: 1.7838\n",
      "Epoch 203/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 47.8071 - val_loss: 2.1146\n",
      "Epoch 204/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 46.4600 - val_loss: 2.4700\n",
      "Epoch 205/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 45.1629 - val_loss: 2.8488\n",
      "Epoch 206/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 43.9141 - val_loss: 3.2496\n",
      "Epoch 207/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 42.7121 - val_loss: 3.6713\n",
      "Epoch 208/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 41.5558 - val_loss: 4.1124\n",
      "Epoch 209/500\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 40.4437 - val_loss: 4.5718\n",
      "Epoch 210/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 39.3746 - val_loss: 5.0484\n",
      "Epoch 211/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 38.3470 - val_loss: 5.5407\n",
      "Epoch 212/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 37.3598 - val_loss: 6.0481\n",
      "Epoch 213/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 36.4115 - val_loss: 6.5691\n",
      "Epoch 214/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 35.5011 - val_loss: 7.1028\n",
      "Epoch 215/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 34.6272 - val_loss: 7.6482\n",
      "Epoch 216/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 33.7888 - val_loss: 8.2040\n",
      "Epoch 217/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 32.9847 - val_loss: 8.7696\n",
      "Epoch 218/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 32.2137 - val_loss: 9.3438\n",
      "Epoch 219/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 31.4748 - val_loss: 9.9256\n",
      "Epoch 220/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 30.7669 - val_loss: 10.5144\n",
      "Epoch 221/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 30.0888 - val_loss: 11.1093\n",
      "Epoch 222/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 29.4395 - val_loss: 11.7092\n",
      "Epoch 223/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 28.8182 - val_loss: 12.3134\n",
      "Epoch 224/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 28.2238 - val_loss: 12.9213\n",
      "Epoch 225/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 27.6552 - val_loss: 13.5320\n",
      "Epoch 226/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 27.1117 - val_loss: 14.1446\n",
      "Epoch 227/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.5923 - val_loss: 14.7587\n",
      "Epoch 228/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 26.0961 - val_loss: 15.3736\n",
      "Epoch 229/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 25.6222 - val_loss: 15.9887\n",
      "Epoch 230/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 25.1698 - val_loss: 16.6028\n",
      "Epoch 231/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 24.7381 - val_loss: 17.2162\n",
      "Epoch 232/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 24.3264 - val_loss: 17.8275\n",
      "Epoch 233/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 23.9337 - val_loss: 18.4368\n",
      "Epoch 234/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 23.5595 - val_loss: 19.0433\n",
      "Epoch 235/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 23.2029 - val_loss: 19.6465\n",
      "Epoch 236/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 22.8633 - val_loss: 20.2459\n",
      "Epoch 237/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 22.5399 - val_loss: 20.8413\n",
      "Epoch 238/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 22.2321 - val_loss: 21.4320\n",
      "Epoch 239/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 21.9393 - val_loss: 22.0176\n",
      "Epoch 240/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 21.6609 - val_loss: 22.5979\n",
      "Epoch 241/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 21.3962 - val_loss: 23.1725\n",
      "Epoch 242/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 21.1447 - val_loss: 23.7409\n",
      "Epoch 243/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.9057 - val_loss: 24.3030\n",
      "Epoch 244/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.6789 - val_loss: 24.8583\n",
      "Epoch 245/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.4636 - val_loss: 25.4068\n",
      "Epoch 246/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 20.2592 - val_loss: 25.9481\n",
      "Epoch 247/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.0654 - val_loss: 26.4817\n",
      "Epoch 248/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 19.8817 - val_loss: 27.0079\n",
      "Epoch 249/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 19.7076 - val_loss: 27.5262\n",
      "Epoch 250/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 19.5426 - val_loss: 28.0362\n",
      "Epoch 251/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 19.3865 - val_loss: 28.5381\n",
      "Epoch 252/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 19.2386 - val_loss: 29.0317\n",
      "Epoch 253/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 19.0987 - val_loss: 29.5168\n",
      "Epoch 254/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 18.9663 - val_loss: 29.9936\n",
      "Epoch 255/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 18.8410 - val_loss: 30.4615\n",
      "Epoch 256/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 18.7227 - val_loss: 30.9205\n",
      "Epoch 257/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 18.6109 - val_loss: 31.3706\n",
      "Epoch 258/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 18.5053 - val_loss: 31.8118\n",
      "Epoch 259/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 18.4055 - val_loss: 32.2442\n",
      "Epoch 260/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 18.3115 - val_loss: 32.6674\n",
      "Epoch 261/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 18.2227 - val_loss: 33.0816\n",
      "Epoch 262/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 18.1389 - val_loss: 33.4868\n",
      "Epoch 263/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 18.0599 - val_loss: 33.8831\n",
      "Epoch 264/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.9854 - val_loss: 34.2704\n",
      "Epoch 265/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.9153 - val_loss: 34.6485\n",
      "Epoch 266/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 17.8492 - val_loss: 35.0179\n",
      "Epoch 267/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.7869 - val_loss: 35.3783\n",
      "Epoch 268/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.7283 - val_loss: 35.7298\n",
      "Epoch 269/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 17.6732 - val_loss: 36.0722\n",
      "Epoch 270/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.6214 - val_loss: 36.4062\n",
      "Epoch 271/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 17.5726 - val_loss: 36.7314\n",
      "Epoch 272/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 17.5267 - val_loss: 37.0480\n",
      "Epoch 273/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.4836 - val_loss: 37.3562\n",
      "Epoch 274/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.4431 - val_loss: 37.6560\n",
      "Epoch 275/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.4050 - val_loss: 37.9476\n",
      "Epoch 276/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.3692 - val_loss: 38.2307\n",
      "Epoch 277/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.3357 - val_loss: 38.5059\n",
      "Epoch 278/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.3042 - val_loss: 38.7731\n",
      "Epoch 279/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.2746 - val_loss: 39.0322\n",
      "Epoch 280/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.2469 - val_loss: 39.2839\n",
      "Epoch 281/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.2208 - val_loss: 39.5278\n",
      "Epoch 282/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.1964 - val_loss: 39.7642\n",
      "Epoch 283/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.1735 - val_loss: 39.9932\n",
      "Epoch 284/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.1520 - val_loss: 40.2150\n",
      "Epoch 285/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.1319 - val_loss: 40.4295\n",
      "Epoch 286/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.1131 - val_loss: 40.6375\n",
      "Epoch 287/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.0955 - val_loss: 40.8383\n",
      "Epoch 288/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.0789 - val_loss: 41.0326\n",
      "Epoch 289/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.0634 - val_loss: 41.2203\n",
      "Epoch 290/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 17.0489 - val_loss: 41.4016\n",
      "Epoch 291/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.0353 - val_loss: 41.5768\n",
      "Epoch 292/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.0226 - val_loss: 41.7460\n",
      "Epoch 293/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.0106 - val_loss: 41.9091\n",
      "Epoch 294/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.9995 - val_loss: 42.0661\n",
      "Epoch 295/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.9890 - val_loss: 42.2177\n",
      "Epoch 296/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.9793 - val_loss: 42.3635\n",
      "Epoch 297/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.9701 - val_loss: 42.5043\n",
      "Epoch 298/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.9616 - val_loss: 42.6396\n",
      "Epoch 299/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.9536 - val_loss: 42.7700\n",
      "Epoch 300/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.9461 - val_loss: 42.8953\n",
      "Epoch 301/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.9391 - val_loss: 43.0157\n",
      "Epoch 302/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.9325 - val_loss: 43.1314\n",
      "Epoch 303/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.9264 - val_loss: 43.2425\n",
      "Epoch 304/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.9207 - val_loss: 43.3494\n",
      "Epoch 305/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.9153 - val_loss: 43.4519\n",
      "Epoch 306/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.9104 - val_loss: 43.5504\n",
      "Epoch 307/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.9057 - val_loss: 43.6449\n",
      "Epoch 308/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.9013 - val_loss: 43.7352\n",
      "Epoch 309/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8972 - val_loss: 43.8219\n",
      "Epoch 310/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 16.8934 - val_loss: 43.9048\n",
      "Epoch 311/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 16.8898 - val_loss: 43.9843\n",
      "Epoch 312/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 16.8865 - val_loss: 44.0602\n",
      "Epoch 313/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8834 - val_loss: 44.1328\n",
      "Epoch 314/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8805 - val_loss: 44.2023\n",
      "Epoch 315/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8778 - val_loss: 44.2689\n",
      "Epoch 316/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8753 - val_loss: 44.3324\n",
      "Epoch 317/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8729 - val_loss: 44.3928\n",
      "Epoch 318/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8708 - val_loss: 44.4509\n",
      "Epoch 319/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8687 - val_loss: 44.5059\n",
      "Epoch 320/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8668 - val_loss: 44.5584\n",
      "Epoch 321/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8651 - val_loss: 44.6087\n",
      "Epoch 322/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8634 - val_loss: 44.6565\n",
      "Epoch 323/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8619 - val_loss: 44.7020\n",
      "Epoch 324/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8605 - val_loss: 44.7452\n",
      "Epoch 325/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8592 - val_loss: 44.7865\n",
      "Epoch 326/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 16.8580 - val_loss: 44.8257\n",
      "Epoch 327/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8568 - val_loss: 44.8630\n",
      "Epoch 328/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8558 - val_loss: 44.8982\n",
      "Epoch 329/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 16.8548 - val_loss: 44.9318\n",
      "Epoch 330/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8540 - val_loss: 44.9636\n",
      "Epoch 331/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8531 - val_loss: 44.9941\n",
      "Epoch 332/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8524 - val_loss: 45.0228\n",
      "Epoch 333/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8517 - val_loss: 45.0500\n",
      "Epoch 334/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8511 - val_loss: 45.0759\n",
      "Epoch 335/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8505 - val_loss: 45.1003\n",
      "Epoch 336/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 16.8500 - val_loss: 45.1234\n",
      "Epoch 337/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8496 - val_loss: 45.1454\n",
      "Epoch 338/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8492 - val_loss: 45.1661\n",
      "Epoch 339/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 16.8488 - val_loss: 45.1857\n",
      "Epoch 340/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 16.8484 - val_loss: 45.2041\n",
      "Epoch 341/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 16.8482 - val_loss: 45.2217\n",
      "Epoch 342/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8479 - val_loss: 45.2381\n",
      "Epoch 343/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 16.8477 - val_loss: 45.2535\n",
      "Epoch 344/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 16.8475 - val_loss: 45.2683\n",
      "Epoch 345/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8473 - val_loss: 45.2820\n",
      "Epoch 346/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8472 - val_loss: 45.2949\n",
      "Epoch 347/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8472 - val_loss: 45.3072\n",
      "Epoch 348/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8471 - val_loss: 45.3186\n",
      "Epoch 349/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8471 - val_loss: 45.3294\n",
      "Epoch 350/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8470 - val_loss: 45.3395\n",
      "Epoch 351/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8470 - val_loss: 45.3488\n",
      "Epoch 352/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8470 - val_loss: 45.3580\n",
      "Epoch 353/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8471 - val_loss: 45.3662\n",
      "Epoch 354/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8471 - val_loss: 45.3736\n",
      "Epoch 355/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8472 - val_loss: 45.3808\n",
      "Epoch 356/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 16.8473 - val_loss: 45.3874\n",
      "Epoch 357/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8475 - val_loss: 45.3939\n",
      "Epoch 358/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8476 - val_loss: 45.3998\n",
      "Epoch 359/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8477 - val_loss: 45.4052\n",
      "Epoch 360/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 16.8479 - val_loss: 45.4103\n",
      "Epoch 361/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8481 - val_loss: 45.4152\n",
      "Epoch 362/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8482 - val_loss: 45.4197\n",
      "Epoch 363/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8484 - val_loss: 45.4238\n",
      "Epoch 364/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8486 - val_loss: 45.4277\n",
      "Epoch 365/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8488 - val_loss: 45.4310\n",
      "Epoch 366/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 16.8491 - val_loss: 45.4342\n",
      "Epoch 367/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 16.8492 - val_loss: 45.4371\n",
      "Epoch 368/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 16.8495 - val_loss: 45.4399\n",
      "Epoch 369/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 16.8497 - val_loss: 45.4423\n",
      "Epoch 370/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 16.8500 - val_loss: 45.4445\n",
      "Epoch 371/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8502 - val_loss: 45.4466\n",
      "Epoch 372/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8505 - val_loss: 45.4483\n",
      "Epoch 373/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 16.8508 - val_loss: 45.4502\n",
      "Epoch 374/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 16.8511 - val_loss: 45.4518\n",
      "Epoch 375/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8513 - val_loss: 45.4536\n",
      "Epoch 376/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 16.8516 - val_loss: 45.4547\n",
      "Epoch 377/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 16.8519 - val_loss: 45.4560\n",
      "Epoch 378/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 16.8521 - val_loss: 45.4571\n",
      "Epoch 379/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8525 - val_loss: 45.4580\n",
      "Epoch 380/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8528 - val_loss: 45.4587\n",
      "Epoch 381/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 16.8531 - val_loss: 45.4595\n",
      "Epoch 382/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8533 - val_loss: 45.4600\n",
      "Epoch 383/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 16.8537 - val_loss: 45.4604\n",
      "Epoch 384/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 16.8540 - val_loss: 45.4611\n",
      "Epoch 385/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8543 - val_loss: 45.4613\n",
      "Epoch 386/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8546 - val_loss: 45.4618\n",
      "Epoch 387/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8549 - val_loss: 45.4619\n",
      "Epoch 388/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8552 - val_loss: 45.4619\n",
      "Epoch 389/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8555 - val_loss: 45.4619\n",
      "Epoch 390/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 16.8558 - val_loss: 45.4620\n",
      "Epoch 391/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8562 - val_loss: 45.4620\n",
      "Epoch 392/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8565 - val_loss: 45.4619\n",
      "Epoch 393/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8568 - val_loss: 45.4619\n",
      "Epoch 394/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 16.8571 - val_loss: 45.4619\n",
      "Epoch 395/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 16.8574 - val_loss: 45.4617\n",
      "Epoch 396/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8578 - val_loss: 45.4613\n",
      "Epoch 397/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8581 - val_loss: 45.4611\n",
      "Epoch 398/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8584 - val_loss: 45.4607\n",
      "Epoch 399/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8587 - val_loss: 45.4603\n",
      "Epoch 400/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8591 - val_loss: 45.4600\n",
      "Epoch 401/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8594 - val_loss: 45.4598\n",
      "Epoch 402/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8597 - val_loss: 45.4592\n",
      "Epoch 403/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8600 - val_loss: 45.4591\n",
      "Epoch 404/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8603 - val_loss: 45.4585\n",
      "Epoch 405/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8607 - val_loss: 45.4582\n",
      "Epoch 406/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8609 - val_loss: 45.4578\n",
      "Epoch 407/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8613 - val_loss: 45.4573\n",
      "Epoch 408/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 16.8616 - val_loss: 45.4571\n",
      "Epoch 409/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8619 - val_loss: 45.4564\n",
      "Epoch 410/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8622 - val_loss: 45.4560\n",
      "Epoch 411/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8625 - val_loss: 45.4554\n",
      "Epoch 412/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8629 - val_loss: 45.4549\n",
      "Epoch 413/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8632 - val_loss: 45.4547\n",
      "Epoch 414/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8635 - val_loss: 45.4540\n",
      "Epoch 415/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8638 - val_loss: 45.4536\n",
      "Epoch 416/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8641 - val_loss: 45.4529\n",
      "Epoch 417/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8644 - val_loss: 45.4522\n",
      "Epoch 418/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8647 - val_loss: 45.4517\n",
      "Epoch 419/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8650 - val_loss: 45.4511\n",
      "Epoch 420/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8653 - val_loss: 45.4507\n",
      "Epoch 421/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8656 - val_loss: 45.4502\n",
      "Epoch 422/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8659 - val_loss: 45.4496\n",
      "Epoch 423/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8662 - val_loss: 45.4491\n",
      "Epoch 424/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8665 - val_loss: 45.4483\n",
      "Epoch 425/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8668 - val_loss: 45.4478\n",
      "Epoch 426/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8671 - val_loss: 45.4473\n",
      "Epoch 427/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 16.8674 - val_loss: 45.4467\n",
      "Epoch 428/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8677 - val_loss: 45.4461\n",
      "Epoch 429/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8680 - val_loss: 45.4456\n",
      "Epoch 430/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8683 - val_loss: 45.4452\n",
      "Epoch 431/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8686 - val_loss: 45.4447\n",
      "Epoch 432/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8688 - val_loss: 45.4441\n",
      "Epoch 433/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8691 - val_loss: 45.4435\n",
      "Epoch 434/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8694 - val_loss: 45.4430\n",
      "Epoch 435/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8697 - val_loss: 45.4425\n",
      "Epoch 436/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8700 - val_loss: 45.4420\n",
      "Epoch 437/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8702 - val_loss: 45.4414\n",
      "Epoch 438/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8705 - val_loss: 45.4411\n",
      "Epoch 439/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8708 - val_loss: 45.4406\n",
      "Epoch 440/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8710 - val_loss: 45.4400\n",
      "Epoch 441/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 16.8713 - val_loss: 45.4394\n",
      "Epoch 442/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8716 - val_loss: 45.4390\n",
      "Epoch 443/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8718 - val_loss: 45.4385\n",
      "Epoch 444/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8721 - val_loss: 45.4381\n",
      "Epoch 445/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8724 - val_loss: 45.4377\n",
      "Epoch 446/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8726 - val_loss: 45.4372\n",
      "Epoch 447/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8729 - val_loss: 45.4367\n",
      "Epoch 448/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8731 - val_loss: 45.4362\n",
      "Epoch 449/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8734 - val_loss: 45.4359\n",
      "Epoch 450/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8736 - val_loss: 45.4354\n",
      "Epoch 451/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8739 - val_loss: 45.4350\n",
      "Epoch 452/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8741 - val_loss: 45.4343\n",
      "Epoch 453/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8743 - val_loss: 45.4338\n",
      "Epoch 454/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8746 - val_loss: 45.4330\n",
      "Epoch 455/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8749 - val_loss: 45.4327\n",
      "Epoch 456/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8751 - val_loss: 45.4322\n",
      "Epoch 457/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 16.8753 - val_loss: 45.4315\n",
      "Epoch 458/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8756 - val_loss: 45.4311\n",
      "Epoch 459/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8758 - val_loss: 45.4308\n",
      "Epoch 460/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8761 - val_loss: 45.4303\n",
      "Epoch 461/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8763 - val_loss: 45.4299\n",
      "Epoch 462/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8765 - val_loss: 45.4294\n",
      "Epoch 463/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8767 - val_loss: 45.4292\n",
      "Epoch 464/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8769 - val_loss: 45.4287\n",
      "Epoch 465/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8771 - val_loss: 45.4282\n",
      "Epoch 466/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8774 - val_loss: 45.4278\n",
      "Epoch 467/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8775 - val_loss: 45.4272\n",
      "Epoch 468/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8778 - val_loss: 45.4266\n",
      "Epoch 469/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8780 - val_loss: 45.4264\n",
      "Epoch 470/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8782 - val_loss: 45.4259\n",
      "Epoch 471/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 16.8784 - val_loss: 45.4256\n",
      "Epoch 472/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8786 - val_loss: 45.4252\n",
      "Epoch 473/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8789 - val_loss: 45.4250\n",
      "Epoch 474/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8791 - val_loss: 45.4246\n",
      "Epoch 475/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8792 - val_loss: 45.4240\n",
      "Epoch 476/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8795 - val_loss: 45.4237\n",
      "Epoch 477/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8796 - val_loss: 45.4233\n",
      "Epoch 478/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8798 - val_loss: 45.4228\n",
      "Epoch 479/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8800 - val_loss: 45.4224\n",
      "Epoch 480/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8802 - val_loss: 45.4220\n",
      "Epoch 481/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8804 - val_loss: 45.4215\n",
      "Epoch 482/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8806 - val_loss: 45.4213\n",
      "Epoch 483/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8808 - val_loss: 45.4207\n",
      "Epoch 484/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8810 - val_loss: 45.4205\n",
      "Epoch 485/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8812 - val_loss: 45.4203\n",
      "Epoch 486/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8813 - val_loss: 45.4199\n",
      "Epoch 487/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8815 - val_loss: 45.4194\n",
      "Epoch 488/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8817 - val_loss: 45.4191\n",
      "Epoch 489/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 16.8819 - val_loss: 45.4188\n",
      "Epoch 490/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8820 - val_loss: 45.4180\n",
      "Epoch 491/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8822 - val_loss: 45.4179\n",
      "Epoch 492/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8824 - val_loss: 45.4177\n",
      "Epoch 493/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8825 - val_loss: 45.4174\n",
      "Epoch 494/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8827 - val_loss: 45.4172\n",
      "Epoch 495/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8828 - val_loss: 45.4169\n",
      "Epoch 496/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8830 - val_loss: 45.4169\n",
      "Epoch 497/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8832 - val_loss: 45.4167\n",
      "Epoch 498/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8833 - val_loss: 45.4162\n",
      "Epoch 499/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 16.8835 - val_loss: 45.4158\n",
      "Epoch 500/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8836 - val_loss: 45.4154\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(85.7336, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0020, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0125, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3008, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 357ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[65.02836134, 65.00315126, 64.97794118, 64.95273109, 64.92752101,\n",
       "        64.90231092, 64.88473389, 64.86792717, 64.85112045, 64.83431373,\n",
       "        64.817507  , 64.80070028, 64.78389356, 64.76708683, 64.75028011,\n",
       "        64.73347339, 64.71666667, 64.69985994, 64.68305322, 64.6662465 ,\n",
       "        64.64943978, 64.63263305, 64.61582633, 64.59901961, 64.58221289,\n",
       "        64.56540616, 64.54859944, 64.53179272, 64.51498599, 64.49817927,\n",
       "        64.48137255, 64.46456583, 64.4477591 , 64.43095238, 64.41414566,\n",
       "        64.39733894, 64.38053221, 64.36372549, 64.34691877, 64.33011204,\n",
       "        64.31330532, 64.29766573, 64.28646125, 64.27525677, 64.26405229,\n",
       "        64.25284781, 64.24164332, 64.23043884, 64.21923436, 64.20802988,\n",
       "        64.1968254 , 64.18562092, 64.17441643, 64.16321195, 64.15200747,\n",
       "        64.14080299, 64.12959851, 64.11839402, 64.10718954, 64.09598506,\n",
       "        64.08478058, 64.0735761 , 64.06237162, 64.05116713, 64.03996265,\n",
       "        64.02875817, 64.01755369, 64.00634921, 63.99514472, 63.98394024,\n",
       "        63.97273576, 63.96153128, 63.9503268 , 63.93912232, 63.92791783,\n",
       "        63.91671335, 63.90550887, 63.88291317, 63.84929972, 63.81568627,\n",
       "        71.13771057,  1.11738777,  0.        ,  0.33966091,  0.1888071 ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.19325858,\n",
       "         0.53950113,  0.        ,  0.29578042,  0.32698724,  0.25565988,\n",
       "         0.        ,  0.        ,  0.        ,  0.35896707,  0.43648201]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([62.2218254 , 62.21715686, 62.21248833, 62.20781979, 62.20315126,\n",
       "       62.19696545, 62.18762838, 62.17829132, 62.16895425, 62.15961718,\n",
       "       62.15028011, 62.14094304, 62.13160598, 62.12226891, 62.11293184,\n",
       "       62.10359477, 62.0942577 , 62.08492063, 62.07558357, 62.0662465 ,\n",
       "       62.05690943, 62.04757236, 62.03823529, 62.02889823, 62.01956116,\n",
       "       62.01022409, 62.00088702, 61.99154995, 61.98221289, 61.97287582,\n",
       "       61.96353875, 61.95420168, 61.94486461, 61.93552754, 61.92619048,\n",
       "       61.91685341, 61.90751634, 61.89817927, 61.8888422 , 61.87950514,\n",
       "       61.87016807, 61.860831  , 61.85149393, 61.84215686, 61.83281979,\n",
       "       61.82348273, 61.81414566, 61.80480859, 61.79547152, 61.78613445,\n",
       "       61.77679739, 61.76746032, 61.75812325, 61.74878618, 61.73944911,\n",
       "       61.73011204, 61.72077498, 61.71143791, 61.70210084, 61.69276377,\n",
       "       61.6834267 , 61.67408964, 61.66475257, 61.6554155 , 61.64607843,\n",
       "       61.63674136, 61.6274043 , 61.61806723, 61.60873016, 61.59939309,\n",
       "       61.59005602, 61.58071895, 61.57138189, 61.56204482, 61.55270775,\n",
       "       61.54337068, 61.53403361, 61.52469655, 61.51535948, 61.50602241,\n",
       "       61.49668534, 61.48734827, 61.4780112 , 61.46867414, 61.45933707,\n",
       "       61.45      , 61.44066293, 61.43132586, 61.4219888 , 61.41265173,\n",
       "       61.40331466, 61.39397759, 61.38464052, 61.37530345, 61.36596639,\n",
       "       61.35662932, 61.34729225, 61.33795518, 61.32861811, 61.31928105])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.783561226999137\n",
      "13.722897702693679\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
