{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "1895    60.034384\n",
       "1896    60.020378\n",
       "1897    60.006373\n",
       "1898    59.992367\n",
       "1899    59.978361\n",
       "Name: C8, Length: 1900, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c8_interpolated_1800_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "1795     0.000000\n",
       "1796     0.000000\n",
       "1797     0.000000\n",
       "1798     0.358967\n",
       "1799     0.436482\n",
       "Name: C8, Length: 1800, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1800)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.987115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.774230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.561345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.348459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.200000  0.000298\n",
       "1     87.987115  0.000298\n",
       "2     87.774230  0.000297\n",
       "3     87.561345  0.000297\n",
       "4     87.348459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 85.7336  # Value for C0\n",
    "K0 = -0.0020  # Value for K0\n",
    "K1 = 0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0125    # Value for b\n",
    "c = 2.3008    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.200000    87.987115    87.774230    87.561345    87.348459   \n",
      "351    87.987115    87.774230    87.561345    87.348459    87.135574   \n",
      "352    87.774230    87.561345    87.348459    87.135574    86.922689   \n",
      "353    87.561345    87.348459    87.135574    86.922689    86.709804   \n",
      "354    87.348459    87.135574    86.922689    86.709804    86.496919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.135574    86.922689    86.709804    86.496919    86.294538  ...   \n",
      "351    86.922689    86.709804    86.496919    86.294538    86.221709  ...   \n",
      "352    86.709804    86.496919    86.294538    86.221709    86.148880  ...   \n",
      "353    86.496919    86.294538    86.221709    86.148880    86.076050  ...   \n",
      "354    86.294538    86.221709    86.148880    86.076050    86.003221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   76.094351    0.000263   76.058870    0.000263   76.023389    0.000263   \n",
      "351   76.058870    0.000263   76.023389    0.000263   75.987909    0.000262   \n",
      "352   76.023389    0.000263   75.987909    0.000262   75.952428    0.000262   \n",
      "353   75.987909    0.000262   75.952428    0.000262   75.916947    0.000262   \n",
      "354   75.952428    0.000262   75.916947    0.000262   75.881466    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   75.987909    0.000262   75.952428    0.000262  \n",
      "351   75.952428    0.000262   75.916947    0.000262  \n",
      "352   75.916947    0.000262   75.881466    0.000262  \n",
      "353   75.881466    0.000262   75.845985    0.000262  \n",
      "354   75.845985    0.000262   75.810504    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1350, 1, 251) (1350, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "17/17 [==============================] - 2s 28ms/step - loss: 4870.4912 - val_loss: 3707.4539\n",
      "Epoch 2/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4776.3555 - val_loss: 3648.2661\n",
      "Epoch 3/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4715.5322 - val_loss: 3598.9351\n",
      "Epoch 4/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4657.5068 - val_loss: 3550.3577\n",
      "Epoch 5/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4598.7822 - val_loss: 3496.1240\n",
      "Epoch 6/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4534.8506 - val_loss: 3446.1858\n",
      "Epoch 7/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4475.8770 - val_loss: 3397.0461\n",
      "Epoch 8/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4417.8975 - val_loss: 3348.7932\n",
      "Epoch 9/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4360.8394 - val_loss: 3301.3042\n",
      "Epoch 10/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4304.5723 - val_loss: 3254.4924\n",
      "Epoch 11/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4249.0151 - val_loss: 3208.3027\n",
      "Epoch 12/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4194.1152 - val_loss: 3162.7014\n",
      "Epoch 13/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4139.8369 - val_loss: 3117.6624\n",
      "Epoch 14/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4086.1555 - val_loss: 3073.1675\n",
      "Epoch 15/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4033.0518 - val_loss: 3029.2021\n",
      "Epoch 16/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 3980.5112 - val_loss: 2985.7537\n",
      "Epoch 17/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3928.5203 - val_loss: 2942.8137\n",
      "Epoch 18/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3877.0693 - val_loss: 2900.3728\n",
      "Epoch 19/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3826.1499 - val_loss: 2858.4231\n",
      "Epoch 20/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3768.9043 - val_loss: 2804.3438\n",
      "Epoch 21/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3708.2158 - val_loss: 2759.0923\n",
      "Epoch 22/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3654.0540 - val_loss: 2715.0828\n",
      "Epoch 23/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3601.2720 - val_loss: 2672.1643\n",
      "Epoch 24/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 3549.6157 - val_loss: 2630.1316\n",
      "Epoch 25/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3498.8843 - val_loss: 2588.8569\n",
      "Epoch 26/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3448.9539 - val_loss: 2548.2629\n",
      "Epoch 27/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3399.7483 - val_loss: 2508.2971\n",
      "Epoch 28/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3351.2141 - val_loss: 2468.9241\n",
      "Epoch 29/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3303.3147 - val_loss: 2430.1162\n",
      "Epoch 30/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3256.0222 - val_loss: 2391.8533\n",
      "Epoch 31/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3209.3127 - val_loss: 2354.1167\n",
      "Epoch 32/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3163.1699 - val_loss: 2316.8945\n",
      "Epoch 33/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3117.5781 - val_loss: 2280.1731\n",
      "Epoch 34/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3072.5247 - val_loss: 2243.9431\n",
      "Epoch 35/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3027.9983 - val_loss: 2208.1941\n",
      "Epoch 36/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2983.9890 - val_loss: 2172.9192\n",
      "Epoch 37/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2940.4883 - val_loss: 2138.1099\n",
      "Epoch 38/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2897.4883 - val_loss: 2103.7603\n",
      "Epoch 39/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2854.9805 - val_loss: 2069.8630\n",
      "Epoch 40/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2812.9590 - val_loss: 2036.4122\n",
      "Epoch 41/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2771.4177 - val_loss: 2003.4023\n",
      "Epoch 42/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2730.3503 - val_loss: 1970.8290\n",
      "Epoch 43/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2689.7510 - val_loss: 1938.6864\n",
      "Epoch 44/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2649.6155 - val_loss: 1906.9688\n",
      "Epoch 45/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2609.9368 - val_loss: 1875.6729\n",
      "Epoch 46/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2570.7114 - val_loss: 1844.7939\n",
      "Epoch 47/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2531.9338 - val_loss: 1814.3269\n",
      "Epoch 48/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2493.6003 - val_loss: 1784.2686\n",
      "Epoch 49/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2455.7065 - val_loss: 1754.6136\n",
      "Epoch 50/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2418.2473 - val_loss: 1725.3597\n",
      "Epoch 51/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2381.2192 - val_loss: 1696.5018\n",
      "Epoch 52/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2344.6184 - val_loss: 1668.0360\n",
      "Epoch 53/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2308.4402 - val_loss: 1639.9594\n",
      "Epoch 54/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2272.6814 - val_loss: 1612.2681\n",
      "Epoch 55/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2237.3381 - val_loss: 1584.9581\n",
      "Epoch 56/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2202.4065 - val_loss: 1558.0264\n",
      "Epoch 57/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2167.8831 - val_loss: 1531.4690\n",
      "Epoch 58/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2133.7644 - val_loss: 1505.2827\n",
      "Epoch 59/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2100.0466 - val_loss: 1479.4650\n",
      "Epoch 60/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2066.7261 - val_loss: 1454.0116\n",
      "Epoch 61/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2033.8007 - val_loss: 1428.9196\n",
      "Epoch 62/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2001.2653 - val_loss: 1404.1860\n",
      "Epoch 63/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1969.1183 - val_loss: 1379.8076\n",
      "Epoch 64/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1937.3550 - val_loss: 1355.7815\n",
      "Epoch 65/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1905.9731 - val_loss: 1332.1044\n",
      "Epoch 66/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1874.9695 - val_loss: 1308.7749\n",
      "Epoch 67/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1844.3408 - val_loss: 1285.7802\n",
      "Epoch 68/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1801.8268 - val_loss: 1251.1083\n",
      "Epoch 69/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1765.9391 - val_loss: 1225.1439\n",
      "Epoch 70/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1731.9561 - val_loss: 1200.2876\n",
      "Epoch 71/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1699.2172 - val_loss: 1176.3114\n",
      "Epoch 72/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1667.4470 - val_loss: 1153.0458\n",
      "Epoch 73/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1636.4675 - val_loss: 1130.3885\n",
      "Epoch 74/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1606.1697 - val_loss: 1108.2747\n",
      "Epoch 75/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1576.4811 - val_loss: 1086.6586\n",
      "Epoch 76/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1547.3529 - val_loss: 1065.5088\n",
      "Epoch 77/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1518.7483 - val_loss: 1044.8002\n",
      "Epoch 78/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1490.6390 - val_loss: 1024.5133\n",
      "Epoch 79/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1463.0031 - val_loss: 1004.6323\n",
      "Epoch 80/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1435.8224 - val_loss: 985.1433\n",
      "Epoch 81/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1409.0809 - val_loss: 966.0352\n",
      "Epoch 82/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1382.7659 - val_loss: 947.2980\n",
      "Epoch 83/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1356.8657 - val_loss: 928.9214\n",
      "Epoch 84/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1331.3704 - val_loss: 910.8987\n",
      "Epoch 85/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1306.2706 - val_loss: 893.2228\n",
      "Epoch 86/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1281.5582 - val_loss: 875.8858\n",
      "Epoch 87/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1257.2256 - val_loss: 858.8818\n",
      "Epoch 88/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1233.2657 - val_loss: 842.2057\n",
      "Epoch 89/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1209.6721 - val_loss: 825.8503\n",
      "Epoch 90/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1186.4386 - val_loss: 809.8120\n",
      "Epoch 91/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1163.5597 - val_loss: 794.0854\n",
      "Epoch 92/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1141.0300 - val_loss: 778.6654\n",
      "Epoch 93/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1118.8444 - val_loss: 763.5475\n",
      "Epoch 94/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 1096.9976 - val_loss: 748.7279\n",
      "Epoch 95/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1075.4854 - val_loss: 734.2017\n",
      "Epoch 96/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1054.3032 - val_loss: 719.9650\n",
      "Epoch 97/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1033.4465 - val_loss: 706.0147\n",
      "Epoch 98/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1012.9114 - val_loss: 692.3456\n",
      "Epoch 99/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 992.6937 - val_loss: 678.9548\n",
      "Epoch 100/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 972.7892 - val_loss: 665.8386\n",
      "Epoch 101/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 953.1943 - val_loss: 652.9927\n",
      "Epoch 102/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 933.9050 - val_loss: 640.4138\n",
      "Epoch 103/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 914.9180 - val_loss: 628.0997\n",
      "Epoch 104/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 896.2296 - val_loss: 616.0457\n",
      "Epoch 105/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 877.8362 - val_loss: 604.2488\n",
      "Epoch 106/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 859.7345 - val_loss: 592.7059\n",
      "Epoch 107/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 841.9211 - val_loss: 581.4137\n",
      "Epoch 108/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 824.3927 - val_loss: 570.3688\n",
      "Epoch 109/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 807.1458 - val_loss: 559.5681\n",
      "Epoch 110/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 790.1775 - val_loss: 549.0092\n",
      "Epoch 111/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 773.4849 - val_loss: 538.6883\n",
      "Epoch 112/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 757.0645 - val_loss: 528.6023\n",
      "Epoch 113/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 740.9131 - val_loss: 518.7486\n",
      "Epoch 114/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 725.0278 - val_loss: 509.1240\n",
      "Epoch 115/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 709.4057 - val_loss: 499.7257\n",
      "Epoch 116/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 694.0441 - val_loss: 490.5507\n",
      "Epoch 117/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 678.9395 - val_loss: 481.5968\n",
      "Epoch 118/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 664.0894 - val_loss: 472.8602\n",
      "Epoch 119/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 649.4909 - val_loss: 464.3383\n",
      "Epoch 120/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 635.1408 - val_loss: 456.0281\n",
      "Epoch 121/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 621.0368 - val_loss: 447.9276\n",
      "Epoch 122/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 607.1757 - val_loss: 440.0334\n",
      "Epoch 123/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 593.5552 - val_loss: 432.3430\n",
      "Epoch 124/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 580.1722 - val_loss: 424.8534\n",
      "Epoch 125/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 567.0240 - val_loss: 417.5621\n",
      "Epoch 126/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 554.1082 - val_loss: 410.4662\n",
      "Epoch 127/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 541.4216 - val_loss: 403.5637\n",
      "Epoch 128/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 528.9623 - val_loss: 396.8507\n",
      "Epoch 129/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 516.7268 - val_loss: 390.3258\n",
      "Epoch 130/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 504.7131 - val_loss: 383.9857\n",
      "Epoch 131/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 492.9184 - val_loss: 377.8279\n",
      "Epoch 132/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 481.3400 - val_loss: 371.8500\n",
      "Epoch 133/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 469.9754 - val_loss: 366.0487\n",
      "Epoch 134/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 458.8222 - val_loss: 360.4225\n",
      "Epoch 135/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 447.8778 - val_loss: 354.9680\n",
      "Epoch 136/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 437.1395 - val_loss: 349.6829\n",
      "Epoch 137/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 426.6048 - val_loss: 344.5649\n",
      "Epoch 138/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 416.2714 - val_loss: 339.6111\n",
      "Epoch 139/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 406.1369 - val_loss: 334.8194\n",
      "Epoch 140/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 396.1985 - val_loss: 330.1870\n",
      "Epoch 141/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 386.4542 - val_loss: 325.7113\n",
      "Epoch 142/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 376.9012 - val_loss: 321.3900\n",
      "Epoch 143/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 367.5370 - val_loss: 317.2209\n",
      "Epoch 144/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 358.3596 - val_loss: 313.2010\n",
      "Epoch 145/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 349.3661 - val_loss: 309.3282\n",
      "Epoch 146/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 340.5547 - val_loss: 305.6003\n",
      "Epoch 147/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 331.9225 - val_loss: 302.0142\n",
      "Epoch 148/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 323.4672 - val_loss: 298.5680\n",
      "Epoch 149/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 315.1866 - val_loss: 295.2591\n",
      "Epoch 150/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 307.0782 - val_loss: 292.0855\n",
      "Epoch 151/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 299.1401 - val_loss: 289.0441\n",
      "Epoch 152/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 291.3695 - val_loss: 286.1331\n",
      "Epoch 153/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 283.7643 - val_loss: 283.3502\n",
      "Epoch 154/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 276.3222 - val_loss: 280.6927\n",
      "Epoch 155/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 269.0408 - val_loss: 278.1585\n",
      "Epoch 156/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 261.9181 - val_loss: 275.7452\n",
      "Epoch 157/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 254.9517 - val_loss: 273.4506\n",
      "Epoch 158/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 248.1393 - val_loss: 271.2722\n",
      "Epoch 159/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 241.4786 - val_loss: 269.2079\n",
      "Epoch 160/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 234.9677 - val_loss: 267.2555\n",
      "Epoch 161/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 228.6039 - val_loss: 265.4125\n",
      "Epoch 162/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 222.3854 - val_loss: 263.6767\n",
      "Epoch 163/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 216.3097 - val_loss: 262.0461\n",
      "Epoch 164/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 210.3750 - val_loss: 260.5183\n",
      "Epoch 165/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 204.5789 - val_loss: 259.0911\n",
      "Epoch 166/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 198.9193 - val_loss: 257.7624\n",
      "Epoch 167/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 193.3939 - val_loss: 256.5298\n",
      "Epoch 168/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 188.0007 - val_loss: 255.3913\n",
      "Epoch 169/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 182.7377 - val_loss: 254.3448\n",
      "Epoch 170/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 177.6026 - val_loss: 253.3881\n",
      "Epoch 171/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 172.5935 - val_loss: 252.5191\n",
      "Epoch 172/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 167.7082 - val_loss: 251.7356\n",
      "Epoch 173/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 162.9445 - val_loss: 251.0355\n",
      "Epoch 174/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 158.3007 - val_loss: 250.4168\n",
      "Epoch 175/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 153.7742 - val_loss: 249.8774\n",
      "Epoch 176/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 149.3636 - val_loss: 249.4153\n",
      "Epoch 177/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 145.0666 - val_loss: 249.0285\n",
      "Epoch 178/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 140.8813 - val_loss: 248.7148\n",
      "Epoch 179/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 136.8057 - val_loss: 248.4723\n",
      "Epoch 180/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 132.8376 - val_loss: 248.2991\n",
      "Epoch 181/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 128.9755 - val_loss: 248.1931\n",
      "Epoch 182/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 125.2171 - val_loss: 248.1523\n",
      "Epoch 183/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 121.5602 - val_loss: 248.1750\n",
      "Epoch 184/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 118.0034 - val_loss: 248.2590\n",
      "Epoch 185/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 114.5444 - val_loss: 248.4026\n",
      "Epoch 186/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 111.1817 - val_loss: 248.6039\n",
      "Epoch 187/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 107.9133 - val_loss: 248.8609\n",
      "Epoch 188/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 104.7373 - val_loss: 249.1719\n",
      "Epoch 189/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 101.6520 - val_loss: 249.5350\n",
      "Epoch 190/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 98.6553 - val_loss: 249.9484\n",
      "Epoch 191/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 95.7456 - val_loss: 250.4103\n",
      "Epoch 192/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 92.9211 - val_loss: 250.9190\n",
      "Epoch 193/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 90.1799 - val_loss: 251.4727\n",
      "Epoch 194/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 87.5203 - val_loss: 252.0697\n",
      "Epoch 195/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 84.9408 - val_loss: 252.7083\n",
      "Epoch 196/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 82.4393 - val_loss: 253.3867\n",
      "Epoch 197/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 80.0144 - val_loss: 254.1035\n",
      "Epoch 198/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 77.6641 - val_loss: 254.8568\n",
      "Epoch 199/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 75.3871 - val_loss: 255.6450\n",
      "Epoch 200/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 73.1815 - val_loss: 256.4667\n",
      "Epoch 201/500\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 71.0459 - val_loss: 257.3201\n",
      "Epoch 202/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 68.9784 - val_loss: 258.2039\n",
      "Epoch 203/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 66.9775 - val_loss: 259.1163\n",
      "Epoch 204/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 65.0417 - val_loss: 260.0560\n",
      "Epoch 205/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 63.1696 - val_loss: 261.0214\n",
      "Epoch 206/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 61.3592 - val_loss: 262.0112\n",
      "Epoch 207/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 59.6094 - val_loss: 263.0237\n",
      "Epoch 208/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 57.9184 - val_loss: 264.0578\n",
      "Epoch 209/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 56.2851 - val_loss: 265.1118\n",
      "Epoch 210/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 54.7077 - val_loss: 266.1846\n",
      "Epoch 211/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 53.1850 - val_loss: 267.2749\n",
      "Epoch 212/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 51.7153 - val_loss: 268.3812\n",
      "Epoch 213/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 50.2975 - val_loss: 269.5023\n",
      "Epoch 214/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 48.9300 - val_loss: 270.6371\n",
      "Epoch 215/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 47.6116 - val_loss: 271.7840\n",
      "Epoch 216/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 46.3409 - val_loss: 272.9421\n",
      "Epoch 217/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 45.1166 - val_loss: 274.1103\n",
      "Epoch 218/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 43.9372 - val_loss: 275.2872\n",
      "Epoch 219/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 42.8018 - val_loss: 276.4718\n",
      "Epoch 220/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 41.7089 - val_loss: 277.6629\n",
      "Epoch 221/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 40.6575 - val_loss: 278.8596\n",
      "Epoch 222/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 39.6461 - val_loss: 280.0608\n",
      "Epoch 223/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 38.6737 - val_loss: 281.2656\n",
      "Epoch 224/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 37.7391 - val_loss: 282.4726\n",
      "Epoch 225/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 36.8411 - val_loss: 283.6813\n",
      "Epoch 226/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 35.9787 - val_loss: 284.8905\n",
      "Epoch 227/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 35.1506 - val_loss: 286.0996\n",
      "Epoch 228/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 34.3558 - val_loss: 287.3072\n",
      "Epoch 229/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 33.5934 - val_loss: 288.5128\n",
      "Epoch 230/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 32.8623 - val_loss: 289.7156\n",
      "Epoch 231/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 32.1615 - val_loss: 290.9148\n",
      "Epoch 232/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 31.4898 - val_loss: 292.1095\n",
      "Epoch 233/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 30.8464 - val_loss: 293.2990\n",
      "Epoch 234/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 30.2303 - val_loss: 294.4826\n",
      "Epoch 235/500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 29.6406 - val_loss: 295.6598\n",
      "Epoch 236/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 29.0764 - val_loss: 296.8297\n",
      "Epoch 237/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 28.5368 - val_loss: 297.9915\n",
      "Epoch 238/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 28.0209 - val_loss: 299.1450\n",
      "Epoch 239/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.5280 - val_loss: 300.2892\n",
      "Epoch 240/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 27.0571 - val_loss: 301.4242\n",
      "Epoch 241/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 26.6074 - val_loss: 302.5488\n",
      "Epoch 242/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 26.1782 - val_loss: 303.6627\n",
      "Epoch 243/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 25.7688 - val_loss: 304.7654\n",
      "Epoch 244/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 25.3783 - val_loss: 305.8567\n",
      "Epoch 245/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 25.0060 - val_loss: 306.9358\n",
      "Epoch 246/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 24.6513 - val_loss: 308.0025\n",
      "Epoch 247/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 24.3135 - val_loss: 309.0565\n",
      "Epoch 248/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 23.9918 - val_loss: 310.0972\n",
      "Epoch 249/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 23.6858 - val_loss: 311.1243\n",
      "Epoch 250/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 23.3946 - val_loss: 312.1378\n",
      "Epoch 251/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 23.1177 - val_loss: 313.1370\n",
      "Epoch 252/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 22.8546 - val_loss: 314.1217\n",
      "Epoch 253/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 22.6047 - val_loss: 315.0919\n",
      "Epoch 254/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 22.3674 - val_loss: 316.0470\n",
      "Epoch 255/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 22.1422 - val_loss: 316.9871\n",
      "Epoch 256/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 21.9285 - val_loss: 317.9119\n",
      "Epoch 257/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 21.7258 - val_loss: 318.8217\n",
      "Epoch 258/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 21.5337 - val_loss: 319.7155\n",
      "Epoch 259/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 21.3517 - val_loss: 320.5937\n",
      "Epoch 260/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 21.1794 - val_loss: 321.4562\n",
      "Epoch 261/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 21.0162 - val_loss: 322.3024\n",
      "Epoch 262/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 20.8618 - val_loss: 323.1329\n",
      "Epoch 263/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 20.7159 - val_loss: 323.9476\n",
      "Epoch 264/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 20.5778 - val_loss: 324.7461\n",
      "Epoch 265/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 20.4474 - val_loss: 325.5284\n",
      "Epoch 266/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 20.3243 - val_loss: 326.2945\n",
      "Epoch 267/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 20.2080 - val_loss: 327.0451\n",
      "Epoch 268/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 20.0983 - val_loss: 327.7793\n",
      "Epoch 269/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 19.9948 - val_loss: 328.4973\n",
      "Epoch 270/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 19.8973 - val_loss: 329.1994\n",
      "Epoch 271/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 19.8054 - val_loss: 329.8856\n",
      "Epoch 272/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 19.7188 - val_loss: 330.5563\n",
      "Epoch 273/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 19.6373 - val_loss: 331.2111\n",
      "Epoch 274/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 19.5606 - val_loss: 331.8502\n",
      "Epoch 275/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 19.4885 - val_loss: 332.4741\n",
      "Epoch 276/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 19.4206 - val_loss: 333.0823\n",
      "Epoch 277/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 19.3569 - val_loss: 333.6751\n",
      "Epoch 278/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 19.2971 - val_loss: 334.2536\n",
      "Epoch 279/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 19.2409 - val_loss: 334.8164\n",
      "Epoch 280/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 19.1881 - val_loss: 335.3647\n",
      "Epoch 281/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 19.1386 - val_loss: 335.8980\n",
      "Epoch 282/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 19.0923 - val_loss: 336.4175\n",
      "Epoch 283/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 19.0487 - val_loss: 336.9225\n",
      "Epoch 284/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 19.0080 - val_loss: 337.4134\n",
      "Epoch 285/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.9699 - val_loss: 337.8904\n",
      "Epoch 286/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.9342 - val_loss: 338.3538\n",
      "Epoch 287/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.9007 - val_loss: 338.8037\n",
      "Epoch 288/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.8695 - val_loss: 339.2404\n",
      "Epoch 289/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.8403 - val_loss: 339.6641\n",
      "Epoch 290/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.8131 - val_loss: 340.0752\n",
      "Epoch 291/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.7875 - val_loss: 340.4733\n",
      "Epoch 292/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.7638 - val_loss: 340.8596\n",
      "Epoch 293/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.7415 - val_loss: 341.2335\n",
      "Epoch 294/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.7208 - val_loss: 341.5954\n",
      "Epoch 295/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 18.7014 - val_loss: 341.9456\n",
      "Epoch 296/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.6834 - val_loss: 342.2846\n",
      "Epoch 297/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.6666 - val_loss: 342.6123\n",
      "Epoch 298/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.6510 - val_loss: 342.9292\n",
      "Epoch 299/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.6365 - val_loss: 343.2355\n",
      "Epoch 300/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.6229 - val_loss: 343.5313\n",
      "Epoch 301/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.6103 - val_loss: 343.8170\n",
      "Epoch 302/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.5985 - val_loss: 344.0926\n",
      "Epoch 303/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.5876 - val_loss: 344.3584\n",
      "Epoch 304/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.5774 - val_loss: 344.6148\n",
      "Epoch 305/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.5680 - val_loss: 344.8620\n",
      "Epoch 306/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.5593 - val_loss: 345.1001\n",
      "Epoch 307/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.5512 - val_loss: 345.3293\n",
      "Epoch 308/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.5436 - val_loss: 345.5501\n",
      "Epoch 309/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.5367 - val_loss: 345.7625\n",
      "Epoch 310/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.5301 - val_loss: 345.9670\n",
      "Epoch 311/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.5241 - val_loss: 346.1633\n",
      "Epoch 312/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.5186 - val_loss: 346.3523\n",
      "Epoch 313/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.5134 - val_loss: 346.5333\n",
      "Epoch 314/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.5086 - val_loss: 346.7078\n",
      "Epoch 315/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 18.5042 - val_loss: 346.8756\n",
      "Epoch 316/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.5001 - val_loss: 347.0360\n",
      "Epoch 317/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4963 - val_loss: 347.1903\n",
      "Epoch 318/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4928 - val_loss: 347.3377\n",
      "Epoch 319/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4896 - val_loss: 347.4793\n",
      "Epoch 320/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4866 - val_loss: 347.6150\n",
      "Epoch 321/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4838 - val_loss: 347.7452\n",
      "Epoch 322/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4812 - val_loss: 347.8693\n",
      "Epoch 323/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4789 - val_loss: 347.9884\n",
      "Epoch 324/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4767 - val_loss: 348.1020\n",
      "Epoch 325/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4747 - val_loss: 348.2104\n",
      "Epoch 326/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4729 - val_loss: 348.3141\n",
      "Epoch 327/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4712 - val_loss: 348.4132\n",
      "Epoch 328/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4697 - val_loss: 348.5080\n",
      "Epoch 329/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4682 - val_loss: 348.5981\n",
      "Epoch 330/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4669 - val_loss: 348.6845\n",
      "Epoch 331/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4658 - val_loss: 348.7668\n",
      "Epoch 332/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4647 - val_loss: 348.8450\n",
      "Epoch 333/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4637 - val_loss: 348.9200\n",
      "Epoch 334/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4628 - val_loss: 348.9910\n",
      "Epoch 335/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4619 - val_loss: 349.0587\n",
      "Epoch 336/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4612 - val_loss: 349.1230\n",
      "Epoch 337/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4605 - val_loss: 349.1840\n",
      "Epoch 338/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 18.4600 - val_loss: 349.2425\n",
      "Epoch 339/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4594 - val_loss: 349.2975\n",
      "Epoch 340/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4589 - val_loss: 349.3498\n",
      "Epoch 341/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4585 - val_loss: 349.3998\n",
      "Epoch 342/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 18.4581 - val_loss: 349.4469\n",
      "Epoch 343/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4578 - val_loss: 349.4917\n",
      "Epoch 344/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4575 - val_loss: 349.5343\n",
      "Epoch 345/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4573 - val_loss: 349.5745\n",
      "Epoch 346/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4570 - val_loss: 349.6127\n",
      "Epoch 347/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4569 - val_loss: 349.6483\n",
      "Epoch 348/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4568 - val_loss: 349.6824\n",
      "Epoch 349/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4567 - val_loss: 349.7144\n",
      "Epoch 350/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4566 - val_loss: 349.7450\n",
      "Epoch 351/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4566 - val_loss: 349.7740\n",
      "Epoch 352/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4565 - val_loss: 349.8011\n",
      "Epoch 353/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4565 - val_loss: 349.8266\n",
      "Epoch 354/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4566 - val_loss: 349.8508\n",
      "Epoch 355/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4566 - val_loss: 349.8736\n",
      "Epoch 356/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4567 - val_loss: 349.8947\n",
      "Epoch 357/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4568 - val_loss: 349.9150\n",
      "Epoch 358/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4569 - val_loss: 349.9341\n",
      "Epoch 359/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4570 - val_loss: 349.9516\n",
      "Epoch 360/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4571 - val_loss: 349.9685\n",
      "Epoch 361/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 18.4573 - val_loss: 349.9840\n",
      "Epoch 362/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4574 - val_loss: 349.9990\n",
      "Epoch 363/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4576 - val_loss: 350.0128\n",
      "Epoch 364/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4578 - val_loss: 350.0259\n",
      "Epoch 365/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4580 - val_loss: 350.0380\n",
      "Epoch 366/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4581 - val_loss: 350.0492\n",
      "Epoch 367/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4584 - val_loss: 350.0594\n",
      "Epoch 368/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4586 - val_loss: 350.0695\n",
      "Epoch 369/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4588 - val_loss: 350.0786\n",
      "Epoch 370/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4591 - val_loss: 350.0871\n",
      "Epoch 371/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4593 - val_loss: 350.0951\n",
      "Epoch 372/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4596 - val_loss: 350.1025\n",
      "Epoch 373/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4599 - val_loss: 350.1092\n",
      "Epoch 374/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4601 - val_loss: 350.1156\n",
      "Epoch 375/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4604 - val_loss: 350.1217\n",
      "Epoch 376/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4607 - val_loss: 350.1273\n",
      "Epoch 377/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4609 - val_loss: 350.1320\n",
      "Epoch 378/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4613 - val_loss: 350.1368\n",
      "Epoch 379/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 18.4615 - val_loss: 350.1413\n",
      "Epoch 380/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4618 - val_loss: 350.1453\n",
      "Epoch 381/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4621 - val_loss: 350.1489\n",
      "Epoch 382/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4624 - val_loss: 350.1519\n",
      "Epoch 383/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4627 - val_loss: 350.1549\n",
      "Epoch 384/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4631 - val_loss: 350.1577\n",
      "Epoch 385/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4634 - val_loss: 350.1604\n",
      "Epoch 386/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4637 - val_loss: 350.1628\n",
      "Epoch 387/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4640 - val_loss: 350.1647\n",
      "Epoch 388/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4643 - val_loss: 350.1666\n",
      "Epoch 389/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4646 - val_loss: 350.1680\n",
      "Epoch 390/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4650 - val_loss: 350.1696\n",
      "Epoch 391/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4653 - val_loss: 350.1711\n",
      "Epoch 392/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4656 - val_loss: 350.1721\n",
      "Epoch 393/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4659 - val_loss: 350.1729\n",
      "Epoch 394/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4662 - val_loss: 350.1740\n",
      "Epoch 395/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4665 - val_loss: 350.1747\n",
      "Epoch 396/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4669 - val_loss: 350.1752\n",
      "Epoch 397/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4672 - val_loss: 350.1760\n",
      "Epoch 398/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4675 - val_loss: 350.1763\n",
      "Epoch 399/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4678 - val_loss: 350.1764\n",
      "Epoch 400/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4681 - val_loss: 350.1765\n",
      "Epoch 401/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 18.4685 - val_loss: 350.1766\n",
      "Epoch 402/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4688 - val_loss: 350.1767\n",
      "Epoch 403/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4691 - val_loss: 350.1767\n",
      "Epoch 404/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4694 - val_loss: 350.1768\n",
      "Epoch 405/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4697 - val_loss: 350.1767\n",
      "Epoch 406/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4701 - val_loss: 350.1764\n",
      "Epoch 407/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4703 - val_loss: 350.1762\n",
      "Epoch 408/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4707 - val_loss: 350.1759\n",
      "Epoch 409/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4710 - val_loss: 350.1756\n",
      "Epoch 410/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4713 - val_loss: 350.1752\n",
      "Epoch 411/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4716 - val_loss: 350.1749\n",
      "Epoch 412/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4719 - val_loss: 350.1741\n",
      "Epoch 413/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4722 - val_loss: 350.1736\n",
      "Epoch 414/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4725 - val_loss: 350.1727\n",
      "Epoch 415/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4729 - val_loss: 350.1723\n",
      "Epoch 416/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4732 - val_loss: 350.1715\n",
      "Epoch 417/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4735 - val_loss: 350.1710\n",
      "Epoch 418/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4738 - val_loss: 350.1704\n",
      "Epoch 419/500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 18.4741 - val_loss: 350.1700\n",
      "Epoch 420/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4744 - val_loss: 350.1694\n",
      "Epoch 421/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4747 - val_loss: 350.1687\n",
      "Epoch 422/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4750 - val_loss: 350.1680\n",
      "Epoch 423/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4753 - val_loss: 350.1674\n",
      "Epoch 424/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4756 - val_loss: 350.1671\n",
      "Epoch 425/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4758 - val_loss: 350.1663\n",
      "Epoch 426/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4761 - val_loss: 350.1656\n",
      "Epoch 427/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4764 - val_loss: 350.1649\n",
      "Epoch 428/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4767 - val_loss: 350.1642\n",
      "Epoch 429/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4770 - val_loss: 350.1635\n",
      "Epoch 430/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4772 - val_loss: 350.1627\n",
      "Epoch 431/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4775 - val_loss: 350.1619\n",
      "Epoch 432/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4778 - val_loss: 350.1610\n",
      "Epoch 433/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4781 - val_loss: 350.1604\n",
      "Epoch 434/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4783 - val_loss: 350.1595\n",
      "Epoch 435/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4786 - val_loss: 350.1589\n",
      "Epoch 436/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4789 - val_loss: 350.1581\n",
      "Epoch 437/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 18.4792 - val_loss: 350.1574\n",
      "Epoch 438/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4794 - val_loss: 350.1568\n",
      "Epoch 439/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4797 - val_loss: 350.1562\n",
      "Epoch 440/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4800 - val_loss: 350.1554\n",
      "Epoch 441/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4802 - val_loss: 350.1549\n",
      "Epoch 442/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4805 - val_loss: 350.1543\n",
      "Epoch 443/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4807 - val_loss: 350.1535\n",
      "Epoch 444/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4810 - val_loss: 350.1530\n",
      "Epoch 445/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4812 - val_loss: 350.1521\n",
      "Epoch 446/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4814 - val_loss: 350.1514\n",
      "Epoch 447/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4817 - val_loss: 350.1505\n",
      "Epoch 448/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4820 - val_loss: 350.1499\n",
      "Epoch 449/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4822 - val_loss: 350.1494\n",
      "Epoch 450/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4824 - val_loss: 350.1488\n",
      "Epoch 451/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4826 - val_loss: 350.1480\n",
      "Epoch 452/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4829 - val_loss: 350.1470\n",
      "Epoch 453/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4831 - val_loss: 350.1465\n",
      "Epoch 454/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4833 - val_loss: 350.1458\n",
      "Epoch 455/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4836 - val_loss: 350.1451\n",
      "Epoch 456/500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 18.4838 - val_loss: 350.1445\n",
      "Epoch 457/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4840 - val_loss: 350.1437\n",
      "Epoch 458/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4843 - val_loss: 350.1431\n",
      "Epoch 459/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4844 - val_loss: 350.1424\n",
      "Epoch 460/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4847 - val_loss: 350.1414\n",
      "Epoch 461/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4849 - val_loss: 350.1411\n",
      "Epoch 462/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4851 - val_loss: 350.1404\n",
      "Epoch 463/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4853 - val_loss: 350.1399\n",
      "Epoch 464/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4855 - val_loss: 350.1393\n",
      "Epoch 465/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4857 - val_loss: 350.1388\n",
      "Epoch 466/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4859 - val_loss: 350.1380\n",
      "Epoch 467/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4861 - val_loss: 350.1375\n",
      "Epoch 468/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4863 - val_loss: 350.1371\n",
      "Epoch 469/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4865 - val_loss: 350.1366\n",
      "Epoch 470/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4867 - val_loss: 350.1360\n",
      "Epoch 471/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4869 - val_loss: 350.1355\n",
      "Epoch 472/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4871 - val_loss: 350.1350\n",
      "Epoch 473/500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 18.4873 - val_loss: 350.1346\n",
      "Epoch 474/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4874 - val_loss: 350.1339\n",
      "Epoch 475/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4876 - val_loss: 350.1334\n",
      "Epoch 476/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4878 - val_loss: 350.1329\n",
      "Epoch 477/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4880 - val_loss: 350.1324\n",
      "Epoch 478/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4881 - val_loss: 350.1320\n",
      "Epoch 479/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4883 - val_loss: 350.1313\n",
      "Epoch 480/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4885 - val_loss: 350.1307\n",
      "Epoch 481/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4887 - val_loss: 350.1303\n",
      "Epoch 482/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4888 - val_loss: 350.1297\n",
      "Epoch 483/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4890 - val_loss: 350.1293\n",
      "Epoch 484/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4891 - val_loss: 350.1288\n",
      "Epoch 485/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4893 - val_loss: 350.1282\n",
      "Epoch 486/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4895 - val_loss: 350.1278\n",
      "Epoch 487/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4896 - val_loss: 350.1271\n",
      "Epoch 488/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4898 - val_loss: 350.1265\n",
      "Epoch 489/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4899 - val_loss: 350.1260\n",
      "Epoch 490/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4901 - val_loss: 350.1253\n",
      "Epoch 491/500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 18.4903 - val_loss: 350.1252\n",
      "Epoch 492/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4904 - val_loss: 350.1248\n",
      "Epoch 493/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4905 - val_loss: 350.1242\n",
      "Epoch 494/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4907 - val_loss: 350.1239\n",
      "Epoch 495/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4908 - val_loss: 350.1236\n",
      "Epoch 496/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4910 - val_loss: 350.1230\n",
      "Epoch 497/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4911 - val_loss: 350.1227\n",
      "Epoch 498/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4912 - val_loss: 350.1225\n",
      "Epoch 499/500\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.4914 - val_loss: 350.1222\n",
      "Epoch 500/500\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4915 - val_loss: 350.1221\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(85.7336, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0020, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0125, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3008, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 351ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.44253501e+01, 6.44085434e+01, 6.43917367e+01, 6.43749300e+01,\n",
       "        6.43581233e+01, 6.43413165e+01, 6.43245098e+01, 6.43077031e+01,\n",
       "        6.42939309e+01, 6.42827264e+01, 6.42715219e+01, 6.42603175e+01,\n",
       "        6.42491130e+01, 6.42379085e+01, 6.42267040e+01, 6.42154995e+01,\n",
       "        6.42042950e+01, 6.41930906e+01, 6.41818861e+01, 6.41706816e+01,\n",
       "        6.41594771e+01, 6.41482726e+01, 6.41370682e+01, 6.41258637e+01,\n",
       "        6.41146592e+01, 6.41034547e+01, 6.40922502e+01, 6.40810458e+01,\n",
       "        6.40698413e+01, 6.40586368e+01, 6.40474323e+01, 6.40362278e+01,\n",
       "        6.40250233e+01, 6.40138189e+01, 6.40026144e+01, 6.39914099e+01,\n",
       "        6.39802054e+01, 6.39690009e+01, 6.39577964e+01, 6.39465920e+01,\n",
       "        6.39353875e+01, 6.39241830e+01, 6.39129785e+01, 6.39017740e+01,\n",
       "        6.38717087e+01, 6.38380952e+01, 6.38044818e+01, 6.67063142e+01,\n",
       "        6.66264823e+01, 6.65466503e+01, 6.64668184e+01, 6.63869865e+01,\n",
       "        6.63071545e+01, 6.62273226e+01, 6.61474907e+01, 6.60676587e+01,\n",
       "        6.59869827e+01, 6.59079949e+01, 6.58281629e+01, 6.57510504e+01,\n",
       "        6.56754202e+01, 6.55978999e+01, 6.55241597e+01, 6.54485294e+01,\n",
       "        6.53728992e+01, 6.52972689e+01, 6.52216387e+01, 6.51460084e+01,\n",
       "        6.50703781e+01, 6.49947479e+01, 6.49191177e+01, 6.48623249e+01,\n",
       "        6.48119048e+01, 7.19575653e+01, 7.51617134e-01, 6.18574840e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        6.87894440e+01, 0.00000000e+00, 4.49371904e-01, 8.36256817e-02,\n",
       "        0.00000000e+00, 7.17031956e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.74329206e-01, 0.00000000e+00, 1.93096563e-01,\n",
       "        2.29679853e-01, 0.00000000e+00, 1.32399821e+00, 0.00000000e+00,\n",
       "        2.03691721e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([61.30994398, 61.30060691, 61.29126984, 61.28193277, 61.2725957 ,\n",
       "       61.26325864, 61.25392157, 61.2445845 , 61.23524743, 61.22591036,\n",
       "       61.2165733 , 61.20723623, 61.19684874, 61.18284314, 61.16883754,\n",
       "       61.15483193, 61.14082633, 61.12682073, 61.11281513, 61.09880952,\n",
       "       61.08480392, 61.07079832, 61.05679272, 61.04278711, 61.02878151,\n",
       "       61.01477591, 61.00077031, 60.98676471, 60.9727591 , 60.9587535 ,\n",
       "       60.9447479 , 60.9307423 , 60.91673669, 60.90273109, 60.88872549,\n",
       "       60.87471989, 60.86071429, 60.84670868, 60.83270308, 60.81869748,\n",
       "       60.80469188, 60.79068627, 60.77668067, 60.76267507, 60.74866947,\n",
       "       60.73466387, 60.72065826, 60.70665266, 60.69264706, 60.67864146,\n",
       "       60.66463585, 60.65063025, 60.63662465, 60.62261905, 60.60861345,\n",
       "       60.59460784, 60.58060224, 60.56659664, 60.55259104, 60.53858543,\n",
       "       60.52457983, 60.51057423, 60.49656863, 60.48256303, 60.46855742,\n",
       "       60.45455182, 60.44054622, 60.42654062, 60.41253501, 60.39852941,\n",
       "       60.38452381, 60.37051821, 60.35651261, 60.342507  , 60.3285014 ,\n",
       "       60.3144958 , 60.3004902 , 60.28648459, 60.27247899, 60.25847339,\n",
       "       60.24446779, 60.23046218, 60.21645658, 60.20245098, 60.18844538,\n",
       "       60.17443978, 60.16043417, 60.14642857, 60.13242297, 60.11841737,\n",
       "       60.10441176, 60.09040616, 60.07640056, 60.06239496, 60.04838936,\n",
       "       60.03438375, 60.02037815, 60.00637255, 59.99236695, 59.97836134])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.218262755494095\n",
      "18.00104434964106\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
