{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.140000\n",
       "1       88.911429\n",
       "2       88.682857\n",
       "3       88.454286\n",
       "4       88.225714\n",
       "          ...    \n",
       "1695    65.809034\n",
       "1696    65.808100\n",
       "1697    65.807166\n",
       "1698    65.806232\n",
       "1699    65.805299\n",
       "Name: C5, Length: 1700, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c5_interpolated_1600_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.140000\n",
       "1       88.911429\n",
       "2       88.682857\n",
       "3       88.454286\n",
       "4       88.225714\n",
       "          ...    \n",
       "1595     0.000000\n",
       "1596     0.000000\n",
       "1597     0.103676\n",
       "1598     0.000000\n",
       "1599     0.000000\n",
       "Name: C5, Length: 1600, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1600)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.140000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.911429</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.682857</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.454286</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.225714</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     89.140000  0.000298\n",
       "1     88.911429  0.000298\n",
       "2     88.682857  0.000297\n",
       "3     88.454286  0.000297\n",
       "4     88.225714  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.8407  # Value for C0\n",
    "K0 = -0.0015  # Value for K0\n",
    "K1 = -0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0118    # Value for b\n",
    "c = 2.5775    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    89.140000    88.911429    88.682857    88.454286    88.225714   \n",
      "351    88.911429    88.682857    88.454286    88.225714    87.997143   \n",
      "352    88.682857    88.454286    88.225714    87.997143    87.768571   \n",
      "353    88.454286    88.225714    87.997143    87.768571    87.540000   \n",
      "354    88.225714    87.997143    87.768571    87.540000    87.311429   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.997143    87.768571    87.540000    87.311429    87.095798  ...   \n",
      "351    87.768571    87.540000    87.311429    87.095798    87.039776  ...   \n",
      "352    87.540000    87.311429    87.095798    87.039776    86.983754  ...   \n",
      "353    87.311429    87.095798    87.039776    86.983754    86.927731  ...   \n",
      "354    87.095798    87.039776    86.983754    86.927731    86.871709  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   79.161181    0.000263   79.132236    0.000263   79.103291    0.000263   \n",
      "351   79.132236    0.000263   79.103291    0.000263   79.074346    0.000262   \n",
      "352   79.103291    0.000263   79.074346    0.000262   79.045401    0.000262   \n",
      "353   79.074346    0.000262   79.045401    0.000262   79.016457    0.000262   \n",
      "354   79.045401    0.000262   79.016457    0.000262   78.987512    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   79.074346    0.000262   79.045401    0.000262  \n",
      "351   79.045401    0.000262   79.016457    0.000262  \n",
      "352   79.016457    0.000262   78.987512    0.000262  \n",
      "353   78.987512    0.000262   78.958567    0.000262  \n",
      "354   78.958567    0.000262   78.929622    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1150, 1, 251) (1150, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "15/15 [==============================] - 2s 33ms/step - loss: 5453.7349 - val_loss: 4481.9238\n",
      "Epoch 2/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5317.8359 - val_loss: 4412.4048\n",
      "Epoch 3/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5242.5508 - val_loss: 4346.7773\n",
      "Epoch 4/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5171.0391 - val_loss: 4281.6743\n",
      "Epoch 5/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5100.2583 - val_loss: 4217.3926\n",
      "Epoch 6/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5030.3345 - val_loss: 4153.9229\n",
      "Epoch 7/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4961.2368 - val_loss: 4091.2297\n",
      "Epoch 8/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4882.7495 - val_loss: 4009.4990\n",
      "Epoch 9/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4802.2759 - val_loss: 3945.3359\n",
      "Epoch 10/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4732.3848 - val_loss: 3882.2559\n",
      "Epoch 11/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4663.6572 - val_loss: 3820.2441\n",
      "Epoch 12/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4595.9956 - val_loss: 3759.1836\n",
      "Epoch 13/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4529.2891 - val_loss: 3698.9929\n",
      "Epoch 14/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4463.4609 - val_loss: 3639.6162\n",
      "Epoch 15/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4398.4629 - val_loss: 3581.0178\n",
      "Epoch 16/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4334.2603 - val_loss: 3523.1711\n",
      "Epoch 17/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4270.8267 - val_loss: 3466.0540\n",
      "Epoch 18/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4208.1431 - val_loss: 3409.6521\n",
      "Epoch 19/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4146.1929 - val_loss: 3353.9509\n",
      "Epoch 20/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4084.9629 - val_loss: 3298.9382\n",
      "Epoch 21/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4024.4417 - val_loss: 3244.6042\n",
      "Epoch 22/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3964.6187 - val_loss: 3190.9395\n",
      "Epoch 23/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3905.4841 - val_loss: 3137.9355\n",
      "Epoch 24/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3847.0298 - val_loss: 3085.5840\n",
      "Epoch 25/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3789.2490 - val_loss: 3033.8789\n",
      "Epoch 26/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3732.1326 - val_loss: 2982.8123\n",
      "Epoch 27/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3675.6738 - val_loss: 2932.3767\n",
      "Epoch 28/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3619.8665 - val_loss: 2882.5669\n",
      "Epoch 29/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3564.7043 - val_loss: 2833.3767\n",
      "Epoch 30/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3510.1797 - val_loss: 2784.7988\n",
      "Epoch 31/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3456.2886 - val_loss: 2736.8289\n",
      "Epoch 32/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3403.0242 - val_loss: 2689.4607\n",
      "Epoch 33/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3350.3801 - val_loss: 2642.6890\n",
      "Epoch 34/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3298.3523 - val_loss: 2596.5073\n",
      "Epoch 35/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3246.9341 - val_loss: 2550.9119\n",
      "Epoch 36/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3196.1208 - val_loss: 2505.8960\n",
      "Epoch 37/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3145.9065 - val_loss: 2461.4565\n",
      "Epoch 38/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3096.2869 - val_loss: 2417.5862\n",
      "Epoch 39/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3047.2566 - val_loss: 2374.2810\n",
      "Epoch 40/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2998.8103 - val_loss: 2331.5361\n",
      "Epoch 41/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2950.9429 - val_loss: 2289.3459\n",
      "Epoch 42/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2903.6497 - val_loss: 2247.7063\n",
      "Epoch 43/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2856.9265 - val_loss: 2206.6135\n",
      "Epoch 44/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2810.7678 - val_loss: 2166.0601\n",
      "Epoch 45/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2765.1692 - val_loss: 2126.0420\n",
      "Epoch 46/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2720.1255 - val_loss: 2086.5566\n",
      "Epoch 47/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2675.6323 - val_loss: 2047.5984\n",
      "Epoch 48/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2631.6853 - val_loss: 2009.1624\n",
      "Epoch 49/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2588.2808 - val_loss: 1971.2440\n",
      "Epoch 50/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2545.4119 - val_loss: 1933.8389\n",
      "Epoch 51/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2503.0759 - val_loss: 1896.9430\n",
      "Epoch 52/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2461.2678 - val_loss: 1860.5511\n",
      "Epoch 53/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2419.9829 - val_loss: 1824.6595\n",
      "Epoch 54/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2379.2178 - val_loss: 1789.2637\n",
      "Epoch 55/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2338.9668 - val_loss: 1754.3594\n",
      "Epoch 56/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2299.2273 - val_loss: 1719.9415\n",
      "Epoch 57/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2259.9932 - val_loss: 1686.0070\n",
      "Epoch 58/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 2221.2610 - val_loss: 1652.5504\n",
      "Epoch 59/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2183.0266 - val_loss: 1619.5684\n",
      "Epoch 60/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2145.2856 - val_loss: 1587.0566\n",
      "Epoch 61/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2108.0342 - val_loss: 1555.0100\n",
      "Epoch 62/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2071.2678 - val_loss: 1523.4263\n",
      "Epoch 63/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2034.9824 - val_loss: 1492.2996\n",
      "Epoch 64/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1999.1741 - val_loss: 1461.6263\n",
      "Epoch 65/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1963.8383 - val_loss: 1431.4027\n",
      "Epoch 66/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1928.9714 - val_loss: 1401.6245\n",
      "Epoch 67/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1894.5695 - val_loss: 1372.2877\n",
      "Epoch 68/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1860.6277 - val_loss: 1343.3878\n",
      "Epoch 69/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1827.1427 - val_loss: 1314.9220\n",
      "Epoch 70/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1794.1106 - val_loss: 1286.8850\n",
      "Epoch 71/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1761.5273 - val_loss: 1259.2732\n",
      "Epoch 72/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1729.3885 - val_loss: 1232.0834\n",
      "Epoch 73/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1697.6909 - val_loss: 1205.3108\n",
      "Epoch 74/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1666.4302 - val_loss: 1178.9519\n",
      "Epoch 75/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1635.6025 - val_loss: 1153.0024\n",
      "Epoch 76/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1605.2042 - val_loss: 1127.4590\n",
      "Epoch 77/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1575.2310 - val_loss: 1102.3174\n",
      "Epoch 78/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1545.6796 - val_loss: 1077.5746\n",
      "Epoch 79/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1516.5464 - val_loss: 1053.2258\n",
      "Epoch 80/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1487.8269 - val_loss: 1029.2677\n",
      "Epoch 81/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 1459.5179 - val_loss: 1005.6967\n",
      "Epoch 82/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1431.6154 - val_loss: 982.5083\n",
      "Epoch 83/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1404.1155 - val_loss: 959.6994\n",
      "Epoch 84/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1377.0144 - val_loss: 937.2651\n",
      "Epoch 85/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1350.3085 - val_loss: 915.2033\n",
      "Epoch 86/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1323.9944 - val_loss: 893.5098\n",
      "Epoch 87/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1298.0684 - val_loss: 872.1807\n",
      "Epoch 88/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1272.5272 - val_loss: 851.2134\n",
      "Epoch 89/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1247.3667 - val_loss: 830.6017\n",
      "Epoch 90/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1222.5829 - val_loss: 810.3448\n",
      "Epoch 91/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1198.1731 - val_loss: 790.4373\n",
      "Epoch 92/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1174.1334 - val_loss: 770.8758\n",
      "Epoch 93/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1150.4596 - val_loss: 751.6575\n",
      "Epoch 94/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1127.1486 - val_loss: 732.7777\n",
      "Epoch 95/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1104.1973 - val_loss: 714.2336\n",
      "Epoch 96/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1081.6013 - val_loss: 696.0226\n",
      "Epoch 97/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 1059.3579 - val_loss: 678.1386\n",
      "Epoch 98/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 1037.4631 - val_loss: 660.5801\n",
      "Epoch 99/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1015.9136 - val_loss: 643.3427\n",
      "Epoch 100/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 994.7060 - val_loss: 626.4247\n",
      "Epoch 101/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 973.8368 - val_loss: 609.8199\n",
      "Epoch 102/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 953.3026 - val_loss: 593.5265\n",
      "Epoch 103/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 933.0995 - val_loss: 577.5410\n",
      "Epoch 104/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 913.2250 - val_loss: 561.8585\n",
      "Epoch 105/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 891.9858 - val_loss: 542.2933\n",
      "Epoch 106/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 867.6077 - val_loss: 524.3054\n",
      "Epoch 107/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 845.2477 - val_loss: 507.1367\n",
      "Epoch 108/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 823.8724 - val_loss: 490.7539\n",
      "Epoch 109/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 803.3430 - val_loss: 475.0132\n",
      "Epoch 110/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 783.5055 - val_loss: 459.8156\n",
      "Epoch 111/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 764.2572 - val_loss: 445.0981\n",
      "Epoch 112/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 745.5324 - val_loss: 430.8166\n",
      "Epoch 113/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 727.2847 - val_loss: 416.9404\n",
      "Epoch 114/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 709.4812 - val_loss: 403.4453\n",
      "Epoch 115/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 692.0963 - val_loss: 390.3131\n",
      "Epoch 116/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 675.1097 - val_loss: 377.5288\n",
      "Epoch 117/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 658.5047 - val_loss: 365.0784\n",
      "Epoch 118/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 642.2673 - val_loss: 352.9509\n",
      "Epoch 119/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 626.3847 - val_loss: 341.1370\n",
      "Epoch 120/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 610.8468 - val_loss: 329.6266\n",
      "Epoch 121/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 595.6436 - val_loss: 318.4132\n",
      "Epoch 122/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 580.7667 - val_loss: 307.4877\n",
      "Epoch 123/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 566.2073 - val_loss: 296.8439\n",
      "Epoch 124/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 551.9590 - val_loss: 286.4754\n",
      "Epoch 125/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 538.0143 - val_loss: 276.3757\n",
      "Epoch 126/500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 524.3669 - val_loss: 266.5392\n",
      "Epoch 127/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 511.0109 - val_loss: 256.9606\n",
      "Epoch 128/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 497.9402 - val_loss: 247.6342\n",
      "Epoch 129/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 485.1490 - val_loss: 238.5548\n",
      "Epoch 130/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 472.6323 - val_loss: 229.7180\n",
      "Epoch 131/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 460.3852 - val_loss: 221.1187\n",
      "Epoch 132/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 448.4025 - val_loss: 212.7527\n",
      "Epoch 133/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 436.6795 - val_loss: 204.6146\n",
      "Epoch 134/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 425.2111 - val_loss: 196.7008\n",
      "Epoch 135/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 413.9935 - val_loss: 189.0069\n",
      "Epoch 136/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 403.0219 - val_loss: 181.5284\n",
      "Epoch 137/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 392.2922 - val_loss: 174.2620\n",
      "Epoch 138/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 381.8004 - val_loss: 167.2024\n",
      "Epoch 139/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 371.5419 - val_loss: 160.3466\n",
      "Epoch 140/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 361.5128 - val_loss: 153.6911\n",
      "Epoch 141/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 351.7095 - val_loss: 147.2314\n",
      "Epoch 142/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 342.1281 - val_loss: 140.9634\n",
      "Epoch 143/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 332.7643 - val_loss: 134.8840\n",
      "Epoch 144/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 323.6148 - val_loss: 128.9897\n",
      "Epoch 145/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 314.6759 - val_loss: 123.2769\n",
      "Epoch 146/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 305.9440 - val_loss: 117.7418\n",
      "Epoch 147/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 297.4155 - val_loss: 112.3811\n",
      "Epoch 148/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 289.0869 - val_loss: 107.1913\n",
      "Epoch 149/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 280.9548 - val_loss: 102.1695\n",
      "Epoch 150/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 273.0159 - val_loss: 97.3115\n",
      "Epoch 151/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 265.2664 - val_loss: 92.6145\n",
      "Epoch 152/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 257.7033 - val_loss: 88.0754\n",
      "Epoch 153/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 250.3233 - val_loss: 83.6904\n",
      "Epoch 154/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 243.1231 - val_loss: 79.4568\n",
      "Epoch 155/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 236.0994 - val_loss: 75.3715\n",
      "Epoch 156/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 229.2492 - val_loss: 71.4311\n",
      "Epoch 157/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 222.5697 - val_loss: 67.6329\n",
      "Epoch 158/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 216.0571 - val_loss: 63.9737\n",
      "Epoch 159/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 209.7090 - val_loss: 60.4504\n",
      "Epoch 160/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 203.5219 - val_loss: 57.0600\n",
      "Epoch 161/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 197.4930 - val_loss: 53.7996\n",
      "Epoch 162/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 191.6193 - val_loss: 50.6666\n",
      "Epoch 163/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 185.8981 - val_loss: 47.6575\n",
      "Epoch 164/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 180.3263 - val_loss: 44.7700\n",
      "Epoch 165/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 174.9011 - val_loss: 42.0013\n",
      "Epoch 166/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 169.6197 - val_loss: 39.3485\n",
      "Epoch 167/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 164.4794 - val_loss: 36.8089\n",
      "Epoch 168/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 159.4773 - val_loss: 34.3796\n",
      "Epoch 169/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 154.6107 - val_loss: 32.0581\n",
      "Epoch 170/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 149.8768 - val_loss: 29.8418\n",
      "Epoch 171/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 145.2732 - val_loss: 27.7280\n",
      "Epoch 172/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 140.7971 - val_loss: 25.7140\n",
      "Epoch 173/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 136.4459 - val_loss: 23.7976\n",
      "Epoch 174/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 132.2172 - val_loss: 21.9760\n",
      "Epoch 175/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 128.1081 - val_loss: 20.2468\n",
      "Epoch 176/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 124.1165 - val_loss: 18.6076\n",
      "Epoch 177/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 120.2397 - val_loss: 17.0560\n",
      "Epoch 178/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 116.4752 - val_loss: 15.5893\n",
      "Epoch 179/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 112.8206 - val_loss: 14.2056\n",
      "Epoch 180/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 109.2736 - val_loss: 12.9022\n",
      "Epoch 181/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 105.8318 - val_loss: 11.6771\n",
      "Epoch 182/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 102.4928 - val_loss: 10.5278\n",
      "Epoch 183/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 99.2545 - val_loss: 9.4521\n",
      "Epoch 184/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 96.1144 - val_loss: 8.4481\n",
      "Epoch 185/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 93.0704 - val_loss: 7.5131\n",
      "Epoch 186/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 90.1200 - val_loss: 6.6455\n",
      "Epoch 187/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 87.2615 - val_loss: 5.8428\n",
      "Epoch 188/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 84.4923 - val_loss: 5.1032\n",
      "Epoch 189/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 81.8105 - val_loss: 4.4245\n",
      "Epoch 190/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 79.2140 - val_loss: 3.8048\n",
      "Epoch 191/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 76.7007 - val_loss: 3.2420\n",
      "Epoch 192/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 74.2686 - val_loss: 2.7343\n",
      "Epoch 193/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 71.9154 - val_loss: 2.2796\n",
      "Epoch 194/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 69.6397 - val_loss: 1.8762\n",
      "Epoch 195/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 67.4393 - val_loss: 1.5222\n",
      "Epoch 196/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 65.3119 - val_loss: 1.2158\n",
      "Epoch 197/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 63.2561 - val_loss: 0.9551\n",
      "Epoch 198/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 61.2698 - val_loss: 0.7385\n",
      "Epoch 199/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 59.3513 - val_loss: 0.5643\n",
      "Epoch 200/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 57.4987 - val_loss: 0.4307\n",
      "Epoch 201/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 55.7104 - val_loss: 0.3361\n",
      "Epoch 202/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 53.9844 - val_loss: 0.2789\n",
      "Epoch 203/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 52.3192 - val_loss: 0.2575\n",
      "Epoch 204/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 50.7130 - val_loss: 0.2704\n",
      "Epoch 205/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 49.1641 - val_loss: 0.3160\n",
      "Epoch 206/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 47.6712 - val_loss: 0.3929\n",
      "Epoch 207/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 46.2325 - val_loss: 0.4995\n",
      "Epoch 208/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 44.8464 - val_loss: 0.6346\n",
      "Epoch 209/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 43.5114 - val_loss: 0.7966\n",
      "Epoch 210/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 42.2260 - val_loss: 0.9842\n",
      "Epoch 211/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 40.9887 - val_loss: 1.1962\n",
      "Epoch 212/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 39.7981 - val_loss: 1.4312\n",
      "Epoch 213/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 38.6528 - val_loss: 1.6879\n",
      "Epoch 214/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 37.5514 - val_loss: 1.9652\n",
      "Epoch 215/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 36.4925 - val_loss: 2.2618\n",
      "Epoch 216/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 35.4747 - val_loss: 2.5766\n",
      "Epoch 217/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 34.4970 - val_loss: 2.9084\n",
      "Epoch 218/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 33.5579 - val_loss: 3.2561\n",
      "Epoch 219/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 32.6561 - val_loss: 3.6187\n",
      "Epoch 220/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 31.7905 - val_loss: 3.9951\n",
      "Epoch 221/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 30.9600 - val_loss: 4.3843\n",
      "Epoch 222/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 30.1632 - val_loss: 4.7854\n",
      "Epoch 223/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 29.3991 - val_loss: 5.1972\n",
      "Epoch 224/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 28.6667 - val_loss: 5.6191\n",
      "Epoch 225/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 27.9647 - val_loss: 6.0500\n",
      "Epoch 226/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 27.2922 - val_loss: 6.4892\n",
      "Epoch 227/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 26.6482 - val_loss: 6.9356\n",
      "Epoch 228/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 26.0316 - val_loss: 7.3887\n",
      "Epoch 229/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 25.4414 - val_loss: 7.8475\n",
      "Epoch 230/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.8767 - val_loss: 8.3113\n",
      "Epoch 231/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.3366 - val_loss: 8.7795\n",
      "Epoch 232/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 23.8202 - val_loss: 9.2511\n",
      "Epoch 233/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 23.3267 - val_loss: 9.7258\n",
      "Epoch 234/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 22.8550 - val_loss: 10.2029\n",
      "Epoch 235/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 22.4045 - val_loss: 10.6815\n",
      "Epoch 236/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 21.9743 - val_loss: 11.1612\n",
      "Epoch 237/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 21.5637 - val_loss: 11.6415\n",
      "Epoch 238/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 21.1718 - val_loss: 12.1217\n",
      "Epoch 239/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 20.7980 - val_loss: 12.6014\n",
      "Epoch 240/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 20.4416 - val_loss: 13.0801\n",
      "Epoch 241/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 20.1017 - val_loss: 13.5573\n",
      "Epoch 242/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 19.7778 - val_loss: 14.0324\n",
      "Epoch 243/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 19.4694 - val_loss: 14.5055\n",
      "Epoch 244/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 19.1755 - val_loss: 14.9755\n",
      "Epoch 245/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.8958 - val_loss: 15.4424\n",
      "Epoch 246/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 18.6296 - val_loss: 15.9057\n",
      "Epoch 247/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.3763 - val_loss: 16.3652\n",
      "Epoch 248/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.1354 - val_loss: 16.8206\n",
      "Epoch 249/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 17.9064 - val_loss: 17.2714\n",
      "Epoch 250/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 17.6887 - val_loss: 17.7175\n",
      "Epoch 251/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 17.4818 - val_loss: 18.1586\n",
      "Epoch 252/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 17.2853 - val_loss: 18.5945\n",
      "Epoch 253/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 17.0986 - val_loss: 19.0248\n",
      "Epoch 254/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.9215 - val_loss: 19.4495\n",
      "Epoch 255/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.7534 - val_loss: 19.8683\n",
      "Epoch 256/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 16.5939 - val_loss: 20.2808\n",
      "Epoch 257/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 16.4427 - val_loss: 20.6873\n",
      "Epoch 258/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 16.2993 - val_loss: 21.0871\n",
      "Epoch 259/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 16.1634 - val_loss: 21.4806\n",
      "Epoch 260/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 16.0346 - val_loss: 21.8675\n",
      "Epoch 261/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 15.9126 - val_loss: 22.2474\n",
      "Epoch 262/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 15.7970 - val_loss: 22.6208\n",
      "Epoch 263/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 15.6877 - val_loss: 22.9870\n",
      "Epoch 264/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 15.5842 - val_loss: 23.3463\n",
      "Epoch 265/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 15.4862 - val_loss: 23.6985\n",
      "Epoch 266/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 15.3936 - val_loss: 24.0438\n",
      "Epoch 267/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 15.3059 - val_loss: 24.3819\n",
      "Epoch 268/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 15.2231 - val_loss: 24.7129\n",
      "Epoch 269/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.1448 - val_loss: 25.0367\n",
      "Epoch 270/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.0708 - val_loss: 25.3531\n",
      "Epoch 271/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.0009 - val_loss: 25.6628\n",
      "Epoch 272/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.9348 - val_loss: 25.9649\n",
      "Epoch 273/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.8726 - val_loss: 26.2603\n",
      "Epoch 274/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 14.8137 - val_loss: 26.5485\n",
      "Epoch 275/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.7581 - val_loss: 26.8294\n",
      "Epoch 276/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.7057 - val_loss: 27.1037\n",
      "Epoch 277/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.6562 - val_loss: 27.3708\n",
      "Epoch 278/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.6096 - val_loss: 27.6310\n",
      "Epoch 279/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.5656 - val_loss: 27.8843\n",
      "Epoch 280/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.5241 - val_loss: 28.1309\n",
      "Epoch 281/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 14.4849 - val_loss: 28.3709\n",
      "Epoch 282/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4481 - val_loss: 28.6042\n",
      "Epoch 283/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 14.4133 - val_loss: 28.8310\n",
      "Epoch 284/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 14.3805 - val_loss: 29.0512\n",
      "Epoch 285/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 14.3497 - val_loss: 29.2653\n",
      "Epoch 286/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 14.3206 - val_loss: 29.4730\n",
      "Epoch 287/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 14.2932 - val_loss: 29.6745\n",
      "Epoch 288/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 14.2674 - val_loss: 29.8699\n",
      "Epoch 289/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 14.2431 - val_loss: 30.0595\n",
      "Epoch 290/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.2202 - val_loss: 30.2430\n",
      "Epoch 291/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.1987 - val_loss: 30.4207\n",
      "Epoch 292/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.1784 - val_loss: 30.5930\n",
      "Epoch 293/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.1593 - val_loss: 30.7597\n",
      "Epoch 294/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.1413 - val_loss: 30.9210\n",
      "Epoch 295/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 14.1244 - val_loss: 31.0769\n",
      "Epoch 296/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.1085 - val_loss: 31.2277\n",
      "Epoch 297/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.0935 - val_loss: 31.3734\n",
      "Epoch 298/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.0794 - val_loss: 31.5137\n",
      "Epoch 299/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.0661 - val_loss: 31.6495\n",
      "Epoch 300/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.0536 - val_loss: 31.7806\n",
      "Epoch 301/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.0418 - val_loss: 31.9068\n",
      "Epoch 302/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.0308 - val_loss: 32.0287\n",
      "Epoch 303/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.0204 - val_loss: 32.1460\n",
      "Epoch 304/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.0106 - val_loss: 32.2593\n",
      "Epoch 305/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.0013 - val_loss: 32.3682\n",
      "Epoch 306/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.9927 - val_loss: 32.4733\n",
      "Epoch 307/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.9845 - val_loss: 32.5742\n",
      "Epoch 308/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.9768 - val_loss: 32.6713\n",
      "Epoch 309/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 13.9695 - val_loss: 32.7645\n",
      "Epoch 310/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.9627 - val_loss: 32.8541\n",
      "Epoch 311/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.9563 - val_loss: 32.9402\n",
      "Epoch 312/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.9503 - val_loss: 33.0229\n",
      "Epoch 313/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.9447 - val_loss: 33.1024\n",
      "Epoch 314/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.9393 - val_loss: 33.1782\n",
      "Epoch 315/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.9343 - val_loss: 33.2514\n",
      "Epoch 316/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.9296 - val_loss: 33.3213\n",
      "Epoch 317/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.9252 - val_loss: 33.3885\n",
      "Epoch 318/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.9210 - val_loss: 33.4528\n",
      "Epoch 319/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.9171 - val_loss: 33.5142\n",
      "Epoch 320/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.9134 - val_loss: 33.5732\n",
      "Epoch 321/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.9099 - val_loss: 33.6296\n",
      "Epoch 322/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.9066 - val_loss: 33.6831\n",
      "Epoch 323/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.9036 - val_loss: 33.7346\n",
      "Epoch 324/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.9008 - val_loss: 33.7838\n",
      "Epoch 325/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8981 - val_loss: 33.8309\n",
      "Epoch 326/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 13.8955 - val_loss: 33.8758\n",
      "Epoch 327/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.8931 - val_loss: 33.9185\n",
      "Epoch 328/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.8909 - val_loss: 33.9592\n",
      "Epoch 329/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8889 - val_loss: 33.9979\n",
      "Epoch 330/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8869 - val_loss: 34.0352\n",
      "Epoch 331/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.8851 - val_loss: 34.0706\n",
      "Epoch 332/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8834 - val_loss: 34.1039\n",
      "Epoch 333/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.8818 - val_loss: 34.1362\n",
      "Epoch 334/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.8802 - val_loss: 34.1668\n",
      "Epoch 335/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8788 - val_loss: 34.1956\n",
      "Epoch 336/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8775 - val_loss: 34.2234\n",
      "Epoch 337/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.8763 - val_loss: 34.2492\n",
      "Epoch 338/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8752 - val_loss: 34.2740\n",
      "Epoch 339/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.8741 - val_loss: 34.2976\n",
      "Epoch 340/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 13.8731 - val_loss: 34.3198\n",
      "Epoch 341/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 13.8723 - val_loss: 34.3408\n",
      "Epoch 342/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8714 - val_loss: 34.3610\n",
      "Epoch 343/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.8706 - val_loss: 34.3800\n",
      "Epoch 344/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.8699 - val_loss: 34.3981\n",
      "Epoch 345/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.8693 - val_loss: 34.4153\n",
      "Epoch 346/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8686 - val_loss: 34.4313\n",
      "Epoch 347/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8681 - val_loss: 34.4465\n",
      "Epoch 348/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8676 - val_loss: 34.4608\n",
      "Epoch 349/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8671 - val_loss: 34.4744\n",
      "Epoch 350/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8667 - val_loss: 34.4873\n",
      "Epoch 351/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.8664 - val_loss: 34.4992\n",
      "Epoch 352/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8660 - val_loss: 34.5105\n",
      "Epoch 353/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8657 - val_loss: 34.5213\n",
      "Epoch 354/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8655 - val_loss: 34.5314\n",
      "Epoch 355/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8652 - val_loss: 34.5409\n",
      "Epoch 356/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8651 - val_loss: 34.5499\n",
      "Epoch 357/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8648 - val_loss: 34.5584\n",
      "Epoch 358/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8647 - val_loss: 34.5661\n",
      "Epoch 359/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8646 - val_loss: 34.5735\n",
      "Epoch 360/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 13.8645 - val_loss: 34.5804\n",
      "Epoch 361/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8644 - val_loss: 34.5871\n",
      "Epoch 362/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8644 - val_loss: 34.5930\n",
      "Epoch 363/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8644 - val_loss: 34.5987\n",
      "Epoch 364/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8644 - val_loss: 34.6042\n",
      "Epoch 365/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8643 - val_loss: 34.6090\n",
      "Epoch 366/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8644 - val_loss: 34.6137\n",
      "Epoch 367/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8645 - val_loss: 34.6184\n",
      "Epoch 368/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8645 - val_loss: 34.6222\n",
      "Epoch 369/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8646 - val_loss: 34.6260\n",
      "Epoch 370/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8647 - val_loss: 34.6294\n",
      "Epoch 371/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8648 - val_loss: 34.6326\n",
      "Epoch 372/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8650 - val_loss: 34.6355\n",
      "Epoch 373/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8651 - val_loss: 34.6386\n",
      "Epoch 374/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8652 - val_loss: 34.6412\n",
      "Epoch 375/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8654 - val_loss: 34.6435\n",
      "Epoch 376/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 13.8656 - val_loss: 34.6457\n",
      "Epoch 377/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8657 - val_loss: 34.6477\n",
      "Epoch 378/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8659 - val_loss: 34.6495\n",
      "Epoch 379/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8661 - val_loss: 34.6512\n",
      "Epoch 380/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8663 - val_loss: 34.6527\n",
      "Epoch 381/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8665 - val_loss: 34.6538\n",
      "Epoch 382/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8668 - val_loss: 34.6552\n",
      "Epoch 383/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8670 - val_loss: 34.6565\n",
      "Epoch 384/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8672 - val_loss: 34.6575\n",
      "Epoch 385/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8675 - val_loss: 34.6583\n",
      "Epoch 386/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8677 - val_loss: 34.6592\n",
      "Epoch 387/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8679 - val_loss: 34.6600\n",
      "Epoch 388/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 13.8682 - val_loss: 34.6608\n",
      "Epoch 389/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8684 - val_loss: 34.6612\n",
      "Epoch 390/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 13.8687 - val_loss: 34.6616\n",
      "Epoch 391/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8690 - val_loss: 34.6620\n",
      "Epoch 392/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8693 - val_loss: 34.6622\n",
      "Epoch 393/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8695 - val_loss: 34.6626\n",
      "Epoch 394/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.8698 - val_loss: 34.6629\n",
      "Epoch 395/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8701 - val_loss: 34.6629\n",
      "Epoch 396/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8704 - val_loss: 34.6630\n",
      "Epoch 397/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8707 - val_loss: 34.6630\n",
      "Epoch 398/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8709 - val_loss: 34.6632\n",
      "Epoch 399/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8712 - val_loss: 34.6631\n",
      "Epoch 400/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.8715 - val_loss: 34.6630\n",
      "Epoch 401/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8718 - val_loss: 34.6629\n",
      "Epoch 402/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8721 - val_loss: 34.6628\n",
      "Epoch 403/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.8724 - val_loss: 34.6626\n",
      "Epoch 404/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8727 - val_loss: 34.6623\n",
      "Epoch 405/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 13.8730 - val_loss: 34.6620\n",
      "Epoch 406/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8733 - val_loss: 34.6619\n",
      "Epoch 407/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8736 - val_loss: 34.6618\n",
      "Epoch 408/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8739 - val_loss: 34.6615\n",
      "Epoch 409/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8742 - val_loss: 34.6612\n",
      "Epoch 410/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8745 - val_loss: 34.6609\n",
      "Epoch 411/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8748 - val_loss: 34.6605\n",
      "Epoch 412/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 13.8752 - val_loss: 34.6602\n",
      "Epoch 413/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 13.8754 - val_loss: 34.6599\n",
      "Epoch 414/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8757 - val_loss: 34.6595\n",
      "Epoch 415/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8761 - val_loss: 34.6592\n",
      "Epoch 416/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8763 - val_loss: 34.6589\n",
      "Epoch 417/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 13.8767 - val_loss: 34.6584\n",
      "Epoch 418/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8770 - val_loss: 34.6579\n",
      "Epoch 419/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8773 - val_loss: 34.6575\n",
      "Epoch 420/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8776 - val_loss: 34.6573\n",
      "Epoch 421/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8779 - val_loss: 34.6569\n",
      "Epoch 422/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8782 - val_loss: 34.6561\n",
      "Epoch 423/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8785 - val_loss: 34.6558\n",
      "Epoch 424/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8788 - val_loss: 34.6554\n",
      "Epoch 425/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8791 - val_loss: 34.6550\n",
      "Epoch 426/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8794 - val_loss: 34.6547\n",
      "Epoch 427/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8797 - val_loss: 34.6540\n",
      "Epoch 428/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8800 - val_loss: 34.6535\n",
      "Epoch 429/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8803 - val_loss: 34.6531\n",
      "Epoch 430/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8806 - val_loss: 34.6526\n",
      "Epoch 431/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8809 - val_loss: 34.6517\n",
      "Epoch 432/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8813 - val_loss: 34.6513\n",
      "Epoch 433/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8816 - val_loss: 34.6509\n",
      "Epoch 434/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 13.8818 - val_loss: 34.6504\n",
      "Epoch 435/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8821 - val_loss: 34.6499\n",
      "Epoch 436/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8824 - val_loss: 34.6494\n",
      "Epoch 437/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8828 - val_loss: 34.6490\n",
      "Epoch 438/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8831 - val_loss: 34.6485\n",
      "Epoch 439/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8833 - val_loss: 34.6478\n",
      "Epoch 440/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8837 - val_loss: 34.6473\n",
      "Epoch 441/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8839 - val_loss: 34.6470\n",
      "Epoch 442/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8842 - val_loss: 34.6466\n",
      "Epoch 443/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8845 - val_loss: 34.6459\n",
      "Epoch 444/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8848 - val_loss: 34.6455\n",
      "Epoch 445/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8851 - val_loss: 34.6449\n",
      "Epoch 446/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8854 - val_loss: 34.6444\n",
      "Epoch 447/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8857 - val_loss: 34.6439\n",
      "Epoch 448/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8860 - val_loss: 34.6433\n",
      "Epoch 449/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8863 - val_loss: 34.6429\n",
      "Epoch 450/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8866 - val_loss: 34.6426\n",
      "Epoch 451/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8868 - val_loss: 34.6422\n",
      "Epoch 452/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 13.8871 - val_loss: 34.6419\n",
      "Epoch 453/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8874 - val_loss: 34.6414\n",
      "Epoch 454/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8877 - val_loss: 34.6410\n",
      "Epoch 455/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8880 - val_loss: 34.6409\n",
      "Epoch 456/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8882 - val_loss: 34.6400\n",
      "Epoch 457/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8885 - val_loss: 34.6397\n",
      "Epoch 458/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8888 - val_loss: 34.6391\n",
      "Epoch 459/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8891 - val_loss: 34.6388\n",
      "Epoch 460/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8893 - val_loss: 34.6385\n",
      "Epoch 461/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8896 - val_loss: 34.6380\n",
      "Epoch 462/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8899 - val_loss: 34.6376\n",
      "Epoch 463/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8902 - val_loss: 34.6372\n",
      "Epoch 464/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8904 - val_loss: 34.6367\n",
      "Epoch 465/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8907 - val_loss: 34.6359\n",
      "Epoch 466/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8909 - val_loss: 34.6354\n",
      "Epoch 467/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8912 - val_loss: 34.6350\n",
      "Epoch 468/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8915 - val_loss: 34.6346\n",
      "Epoch 469/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 13.8917 - val_loss: 34.6343\n",
      "Epoch 470/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8920 - val_loss: 34.6337\n",
      "Epoch 471/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8923 - val_loss: 34.6334\n",
      "Epoch 472/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8925 - val_loss: 34.6329\n",
      "Epoch 473/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8928 - val_loss: 34.6324\n",
      "Epoch 474/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8930 - val_loss: 34.6319\n",
      "Epoch 475/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8934 - val_loss: 34.6316\n",
      "Epoch 476/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8936 - val_loss: 34.6314\n",
      "Epoch 477/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8938 - val_loss: 34.6309\n",
      "Epoch 478/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8940 - val_loss: 34.6306\n",
      "Epoch 479/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8943 - val_loss: 34.6302\n",
      "Epoch 480/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8945 - val_loss: 34.6297\n",
      "Epoch 481/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.8948 - val_loss: 34.6295\n",
      "Epoch 482/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.8950 - val_loss: 34.6292\n",
      "Epoch 483/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 13.8952 - val_loss: 34.6288\n",
      "Epoch 484/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 13.8955 - val_loss: 34.6282\n",
      "Epoch 485/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 13.8957 - val_loss: 34.6277\n",
      "Epoch 486/500\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 13.8960 - val_loss: 34.6274\n",
      "Epoch 487/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.8962 - val_loss: 34.6269\n",
      "Epoch 488/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 13.8964 - val_loss: 34.6266\n",
      "Epoch 489/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 13.8966 - val_loss: 34.6259\n",
      "Epoch 490/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 13.8969 - val_loss: 34.6256\n",
      "Epoch 491/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 13.8971 - val_loss: 34.6253\n",
      "Epoch 492/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 13.8974 - val_loss: 34.6252\n",
      "Epoch 493/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8975 - val_loss: 34.6248\n",
      "Epoch 494/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8977 - val_loss: 34.6244\n",
      "Epoch 495/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 13.8980 - val_loss: 34.6236\n",
      "Epoch 496/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.8983 - val_loss: 34.6235\n",
      "Epoch 497/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.8984 - val_loss: 34.6230\n",
      "Epoch 498/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.8986 - val_loss: 34.6227\n",
      "Epoch 499/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.8989 - val_loss: 34.6224\n",
      "Epoch 500/500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 13.8991 - val_loss: 34.6219\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.8407, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0015, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0118, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.5775, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 409ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.97152661e+01, 6.97096639e+01, 6.97040616e+01, 6.96969188e+01,\n",
       "        6.96857143e+01, 6.96745098e+01, 6.96633053e+01, 6.96521008e+01,\n",
       "        6.96408964e+01, 6.96296919e+01, 6.96184874e+01, 6.96072829e+01,\n",
       "        6.95960784e+01, 6.95848740e+01, 6.95736695e+01, 6.95624650e+01,\n",
       "        6.95512605e+01, 6.95400560e+01, 6.95288515e+01, 6.95176471e+01,\n",
       "        6.95064426e+01, 6.94952381e+01, 6.94840336e+01, 6.94728291e+01,\n",
       "        6.94616247e+01, 6.94504202e+01, 6.94392157e+01, 6.94280112e+01,\n",
       "        6.94168067e+01, 6.94056022e+01, 6.93943978e+01, 6.93831933e+01,\n",
       "        6.93719888e+01, 6.93607843e+01, 6.93495798e+01, 6.93383754e+01,\n",
       "        6.93271709e+01, 6.93159664e+01, 6.93047619e+01, 6.92822829e+01,\n",
       "        6.92514706e+01, 6.92206583e+01, 6.91898459e+01, 6.91590336e+01,\n",
       "        6.91282213e+01, 6.90974090e+01, 6.90665966e+01, 6.90357843e+01,\n",
       "        6.90049720e+01, 6.89741597e+01, 6.89433473e+01, 6.89125350e+01,\n",
       "        6.88817227e+01, 6.88509104e+01, 6.88200980e+01, 6.87892857e+01,\n",
       "        6.87584734e+01, 6.87276611e+01, 6.86968487e+01, 6.86660364e+01,\n",
       "        6.86352241e+01, 6.86044118e+01, 6.85735994e+01, 6.85427871e+01,\n",
       "        6.85119748e+01, 6.84811625e+01, 6.84503501e+01, 6.84195378e+01,\n",
       "        6.83887255e+01, 6.83579132e+01, 6.83271008e+01, 6.82962885e+01,\n",
       "        6.82654762e+01, 6.82346639e+01, 6.82038515e+01, 6.81632353e+01,\n",
       "        6.81212185e+01, 6.80792017e+01, 6.80371849e+01, 6.79951681e+01,\n",
       "        7.49060364e+01, 6.45492792e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.67115811e-02, 0.00000000e+00, 5.82197547e-01, 7.12585807e-01,\n",
       "        0.00000000e+00, 1.22350466e+00, 0.00000000e+00, 4.79321927e-02,\n",
       "        6.37347624e-02, 0.00000000e+00, 0.00000000e+00, 4.10423160e-01,\n",
       "        2.52054334e-01, 0.00000000e+00, 3.47051203e-01, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([65.89773576, 65.89680205, 65.89586835, 65.89493464, 65.89400093,\n",
       "       65.89306723, 65.89213352, 65.89119981, 65.89026611, 65.8893324 ,\n",
       "       65.88839869, 65.88746499, 65.88653128, 65.88559757, 65.88466387,\n",
       "       65.88373016, 65.88279645, 65.88186275, 65.88092904, 65.87999533,\n",
       "       65.87906162, 65.87812792, 65.87719421, 65.8762605 , 65.8753268 ,\n",
       "       65.87439309, 65.87345938, 65.87252568, 65.87159197, 65.87065826,\n",
       "       65.86972456, 65.86879085, 65.86785714, 65.86692344, 65.86598973,\n",
       "       65.86505602, 65.86412232, 65.86318861, 65.8622549 , 65.8613212 ,\n",
       "       65.86038749, 65.85945378, 65.85852007, 65.85758637, 65.85665266,\n",
       "       65.85571895, 65.85478525, 65.85385154, 65.85291783, 65.85198413,\n",
       "       65.85105042, 65.85011671, 65.84918301, 65.8482493 , 65.84731559,\n",
       "       65.84638189, 65.84544818, 65.84451447, 65.84358077, 65.84264706,\n",
       "       65.84171335, 65.84077965, 65.83984594, 65.83891223, 65.83797852,\n",
       "       65.83704482, 65.83611111, 65.8351774 , 65.8342437 , 65.83330999,\n",
       "       65.83237628, 65.83144258, 65.83050887, 65.82957516, 65.82864146,\n",
       "       65.82770775, 65.82677404, 65.82584034, 65.82490663, 65.82397292,\n",
       "       65.82303922, 65.82210551, 65.8211718 , 65.8202381 , 65.81930439,\n",
       "       65.81837068, 65.81743697, 65.81650327, 65.81556956, 65.81463585,\n",
       "       65.81370215, 65.81276844, 65.81183473, 65.81090103, 65.80996732,\n",
       "       65.80903361, 65.80809991, 65.8071662 , 65.80623249, 65.80529879])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.75096263031488\n",
      "15.126589748158622\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
