{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.140000\n",
       "1       88.911429\n",
       "2       88.682857\n",
       "3       88.454286\n",
       "4       88.225714\n",
       "          ...    \n",
       "1795    65.209641\n",
       "1796    65.203105\n",
       "1797    65.196569\n",
       "1798    65.190033\n",
       "1799    65.183497\n",
       "Name: C5, Length: 1800, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c5_interpolated_1700_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.140000\n",
       "1       88.911429\n",
       "2       88.682857\n",
       "3       88.454286\n",
       "4       88.225714\n",
       "          ...    \n",
       "1695     0.410423\n",
       "1696     0.252054\n",
       "1697     0.000000\n",
       "1698     0.347051\n",
       "1699     0.000000\n",
       "Name: C5, Length: 1700, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1700)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.140000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.911429</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.682857</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.454286</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.225714</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     89.140000  0.000298\n",
       "1     88.911429  0.000298\n",
       "2     88.682857  0.000297\n",
       "3     88.454286  0.000297\n",
       "4     88.225714  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.8407  # Value for C0\n",
    "K0 = -0.0015  # Value for K0\n",
    "K1 = -0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0118    # Value for b\n",
    "c = 2.5775    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    89.140000    88.911429    88.682857    88.454286    88.225714   \n",
      "351    88.911429    88.682857    88.454286    88.225714    87.997143   \n",
      "352    88.682857    88.454286    88.225714    87.997143    87.768571   \n",
      "353    88.454286    88.225714    87.997143    87.768571    87.540000   \n",
      "354    88.225714    87.997143    87.768571    87.540000    87.311429   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.997143    87.768571    87.540000    87.311429    87.095798  ...   \n",
      "351    87.768571    87.540000    87.311429    87.095798    87.039776  ...   \n",
      "352    87.540000    87.311429    87.095798    87.039776    86.983754  ...   \n",
      "353    87.311429    87.095798    87.039776    86.983754    86.927731  ...   \n",
      "354    87.095798    87.039776    86.983754    86.927731    86.871709  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   79.161181    0.000263   79.132236    0.000263   79.103291    0.000263   \n",
      "351   79.132236    0.000263   79.103291    0.000263   79.074346    0.000262   \n",
      "352   79.103291    0.000263   79.074346    0.000262   79.045401    0.000262   \n",
      "353   79.074346    0.000262   79.045401    0.000262   79.016457    0.000262   \n",
      "354   79.045401    0.000262   79.016457    0.000262   78.987512    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   79.074346    0.000262   79.045401    0.000262  \n",
      "351   79.045401    0.000262   79.016457    0.000262  \n",
      "352   79.016457    0.000262   78.987512    0.000262  \n",
      "353   78.987512    0.000262   78.958567    0.000262  \n",
      "354   78.958567    0.000262   78.929622    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1250, 1, 251) (1250, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "16/16 [==============================] - 2s 33ms/step - loss: 5417.1440 - val_loss: 4443.7891\n",
      "Epoch 2/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5338.8828 - val_loss: 4382.9170\n",
      "Epoch 3/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5272.5347 - val_loss: 4322.6929\n",
      "Epoch 4/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5185.2002 - val_loss: 4235.5581\n",
      "Epoch 5/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5108.7495 - val_loss: 4172.0737\n",
      "Epoch 6/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5039.4150 - val_loss: 4109.7197\n",
      "Epoch 7/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4971.2627 - val_loss: 4048.4165\n",
      "Epoch 8/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4904.1484 - val_loss: 3988.0271\n",
      "Epoch 9/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4837.9482 - val_loss: 3928.4600\n",
      "Epoch 10/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4772.5786 - val_loss: 3869.6584\n",
      "Epoch 11/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4707.9868 - val_loss: 3811.5825\n",
      "Epoch 12/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4644.1372 - val_loss: 3754.2056\n",
      "Epoch 13/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4581.0034 - val_loss: 3697.5071\n",
      "Epoch 14/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4518.5669 - val_loss: 3641.4707\n",
      "Epoch 15/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4456.8105 - val_loss: 3586.0837\n",
      "Epoch 16/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4395.7231 - val_loss: 3531.3342\n",
      "Epoch 17/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4335.2910 - val_loss: 3477.2129\n",
      "Epoch 18/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4275.5049 - val_loss: 3423.7109\n",
      "Epoch 19/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4216.3579 - val_loss: 3370.8215\n",
      "Epoch 20/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4157.8413 - val_loss: 3318.5366\n",
      "Epoch 21/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4099.9482 - val_loss: 3266.8481\n",
      "Epoch 22/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4042.6704 - val_loss: 3215.7517\n",
      "Epoch 23/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3986.0034 - val_loss: 3165.2407\n",
      "Epoch 24/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3929.9390 - val_loss: 3115.3096\n",
      "Epoch 25/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 3874.4749 - val_loss: 3065.9519\n",
      "Epoch 26/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3819.6013 - val_loss: 3017.1643\n",
      "Epoch 27/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3765.3157 - val_loss: 2968.9395\n",
      "Epoch 28/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3711.6118 - val_loss: 2921.2729\n",
      "Epoch 29/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3658.4846 - val_loss: 2874.1616\n",
      "Epoch 30/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3605.9299 - val_loss: 2827.5979\n",
      "Epoch 31/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3553.9421 - val_loss: 2781.5796\n",
      "Epoch 32/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3502.5166 - val_loss: 2736.1001\n",
      "Epoch 33/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3451.6492 - val_loss: 2676.9346\n",
      "Epoch 34/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3383.2712 - val_loss: 2628.0100\n",
      "Epoch 35/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3328.1609 - val_loss: 2579.6797\n",
      "Epoch 36/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3274.3408 - val_loss: 2532.7163\n",
      "Epoch 37/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3221.8936 - val_loss: 2486.8838\n",
      "Epoch 38/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3170.5696 - val_loss: 2441.9973\n",
      "Epoch 39/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3120.1978 - val_loss: 2397.9397\n",
      "Epoch 40/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3070.6721 - val_loss: 2354.6360\n",
      "Epoch 41/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3021.9199 - val_loss: 2312.0347\n",
      "Epoch 42/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2973.8931 - val_loss: 2270.0991\n",
      "Epoch 43/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2926.5540 - val_loss: 2228.7996\n",
      "Epoch 44/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2879.8752 - val_loss: 2188.1133\n",
      "Epoch 45/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2831.1499 - val_loss: 2138.5183\n",
      "Epoch 46/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2775.0417 - val_loss: 2094.0078\n",
      "Epoch 47/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2724.8054 - val_loss: 2050.8301\n",
      "Epoch 48/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2676.1450 - val_loss: 2009.0255\n",
      "Epoch 49/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2628.8584 - val_loss: 1968.3375\n",
      "Epoch 50/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2582.6956 - val_loss: 1928.5928\n",
      "Epoch 51/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2537.4941 - val_loss: 1889.6814\n",
      "Epoch 52/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2493.1516 - val_loss: 1851.5294\n",
      "Epoch 53/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2449.5967 - val_loss: 1814.0857\n",
      "Epoch 54/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2406.7793 - val_loss: 1777.3121\n",
      "Epoch 55/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2364.6609 - val_loss: 1741.1781\n",
      "Epoch 56/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2323.2100 - val_loss: 1705.6580\n",
      "Epoch 57/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2282.4019 - val_loss: 1670.7338\n",
      "Epoch 58/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2242.2161 - val_loss: 1636.3857\n",
      "Epoch 59/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2202.6340 - val_loss: 1602.5995\n",
      "Epoch 60/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2163.6414 - val_loss: 1569.3618\n",
      "Epoch 61/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2125.2219 - val_loss: 1536.6599\n",
      "Epoch 62/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2087.0322 - val_loss: 1501.1038\n",
      "Epoch 63/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2043.8668 - val_loss: 1464.9100\n",
      "Epoch 64/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2001.8667 - val_loss: 1429.6801\n",
      "Epoch 65/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1961.2690 - val_loss: 1395.7545\n",
      "Epoch 66/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1922.0077 - val_loss: 1362.8959\n",
      "Epoch 67/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1883.8351 - val_loss: 1330.9310\n",
      "Epoch 68/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1846.5847 - val_loss: 1299.7487\n",
      "Epoch 69/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1810.1514 - val_loss: 1269.2773\n",
      "Epoch 70/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1774.4640 - val_loss: 1239.4652\n",
      "Epoch 71/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1739.4712 - val_loss: 1210.2750\n",
      "Epoch 72/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1705.1342 - val_loss: 1181.6754\n",
      "Epoch 73/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1671.4220 - val_loss: 1153.6440\n",
      "Epoch 74/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1638.3108 - val_loss: 1126.1584\n",
      "Epoch 75/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1605.7778 - val_loss: 1099.2040\n",
      "Epoch 76/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1573.8074 - val_loss: 1072.7642\n",
      "Epoch 77/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1542.3816 - val_loss: 1046.8257\n",
      "Epoch 78/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1511.4880 - val_loss: 1021.3757\n",
      "Epoch 79/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1481.1128 - val_loss: 996.4048\n",
      "Epoch 80/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1451.2448 - val_loss: 971.9015\n",
      "Epoch 81/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1421.8739 - val_loss: 947.8572\n",
      "Epoch 82/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1392.9899 - val_loss: 924.2632\n",
      "Epoch 83/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1364.5837 - val_loss: 901.1115\n",
      "Epoch 84/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1336.6478 - val_loss: 878.3940\n",
      "Epoch 85/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1309.1733 - val_loss: 856.1031\n",
      "Epoch 86/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1282.1527 - val_loss: 834.2315\n",
      "Epoch 87/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1255.5789 - val_loss: 812.7740\n",
      "Epoch 88/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1229.4447 - val_loss: 791.7234\n",
      "Epoch 89/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1203.7438 - val_loss: 771.0719\n",
      "Epoch 90/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1178.4688 - val_loss: 750.8159\n",
      "Epoch 91/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1153.6146 - val_loss: 730.9475\n",
      "Epoch 92/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1129.1750 - val_loss: 711.4630\n",
      "Epoch 93/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1105.1444 - val_loss: 692.3552\n",
      "Epoch 94/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1081.5170 - val_loss: 673.6203\n",
      "Epoch 95/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1058.2869 - val_loss: 655.2515\n",
      "Epoch 96/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1035.4489 - val_loss: 637.2446\n",
      "Epoch 97/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1012.9985 - val_loss: 619.5944\n",
      "Epoch 98/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 990.9298 - val_loss: 602.2957\n",
      "Epoch 99/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 969.2380 - val_loss: 585.3439\n",
      "Epoch 100/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 947.9187 - val_loss: 568.7347\n",
      "Epoch 101/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 926.9666 - val_loss: 552.4630\n",
      "Epoch 102/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 906.3771 - val_loss: 536.5240\n",
      "Epoch 103/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 886.1462 - val_loss: 520.9141\n",
      "Epoch 104/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 866.2685 - val_loss: 505.6280\n",
      "Epoch 105/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 846.7394 - val_loss: 490.6609\n",
      "Epoch 106/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 827.5552 - val_loss: 476.0097\n",
      "Epoch 107/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 808.7110 - val_loss: 461.6689\n",
      "Epoch 108/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 790.2028 - val_loss: 447.6356\n",
      "Epoch 109/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 772.0262 - val_loss: 433.9043\n",
      "Epoch 110/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 754.1771 - val_loss: 420.4712\n",
      "Epoch 111/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 736.6512 - val_loss: 407.3330\n",
      "Epoch 112/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 719.4449 - val_loss: 394.4849\n",
      "Epoch 113/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 702.5536 - val_loss: 381.9236\n",
      "Epoch 114/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 685.9736 - val_loss: 369.6443\n",
      "Epoch 115/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 669.7009 - val_loss: 357.6436\n",
      "Epoch 116/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 653.7318 - val_loss: 345.9181\n",
      "Epoch 117/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 638.0624 - val_loss: 334.4623\n",
      "Epoch 118/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 622.6881 - val_loss: 323.2733\n",
      "Epoch 119/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 607.6060 - val_loss: 312.3482\n",
      "Epoch 120/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 592.8120 - val_loss: 301.6823\n",
      "Epoch 121/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 578.3022 - val_loss: 291.2722\n",
      "Epoch 122/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 564.0731 - val_loss: 281.1132\n",
      "Epoch 123/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 550.1210 - val_loss: 271.2032\n",
      "Epoch 124/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 536.4421 - val_loss: 261.5374\n",
      "Epoch 125/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 523.0330 - val_loss: 252.1126\n",
      "Epoch 126/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 509.8899 - val_loss: 242.9255\n",
      "Epoch 127/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 497.0092 - val_loss: 233.9719\n",
      "Epoch 128/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 484.3877 - val_loss: 225.2487\n",
      "Epoch 129/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 472.0217 - val_loss: 216.7522\n",
      "Epoch 130/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 459.9076 - val_loss: 208.4794\n",
      "Epoch 131/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 448.0424 - val_loss: 200.4263\n",
      "Epoch 132/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 436.4221 - val_loss: 192.5893\n",
      "Epoch 133/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 425.0437 - val_loss: 184.9656\n",
      "Epoch 134/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 413.9035 - val_loss: 177.5515\n",
      "Epoch 135/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 402.9984 - val_loss: 170.3431\n",
      "Epoch 136/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 392.3247 - val_loss: 163.3380\n",
      "Epoch 137/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 381.8795 - val_loss: 156.5326\n",
      "Epoch 138/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 371.6596 - val_loss: 149.9230\n",
      "Epoch 139/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 361.6613 - val_loss: 143.5066\n",
      "Epoch 140/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 351.8814 - val_loss: 137.2796\n",
      "Epoch 141/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 342.3167 - val_loss: 131.2392\n",
      "Epoch 142/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 332.9643 - val_loss: 125.3818\n",
      "Epoch 143/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 323.8206 - val_loss: 119.7045\n",
      "Epoch 144/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 314.8828 - val_loss: 114.2042\n",
      "Epoch 145/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 306.1476 - val_loss: 108.8775\n",
      "Epoch 146/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 297.6119 - val_loss: 103.7212\n",
      "Epoch 147/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 289.2726 - val_loss: 98.7323\n",
      "Epoch 148/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 281.1266 - val_loss: 93.9081\n",
      "Epoch 149/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 273.1709 - val_loss: 89.2448\n",
      "Epoch 150/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 265.4023 - val_loss: 84.7397\n",
      "Epoch 151/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 257.8180 - val_loss: 80.3900\n",
      "Epoch 152/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 250.4149 - val_loss: 76.1923\n",
      "Epoch 153/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 243.1900 - val_loss: 72.1439\n",
      "Epoch 154/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 236.1404 - val_loss: 68.2417\n",
      "Epoch 155/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 229.2633 - val_loss: 64.4828\n",
      "Epoch 156/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 222.5556 - val_loss: 60.8643\n",
      "Epoch 157/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 216.0146 - val_loss: 57.3836\n",
      "Epoch 158/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 209.6372 - val_loss: 54.0371\n",
      "Epoch 159/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 203.4206 - val_loss: 50.8229\n",
      "Epoch 160/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 197.3622 - val_loss: 47.7371\n",
      "Epoch 161/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 191.4590 - val_loss: 44.7779\n",
      "Epoch 162/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 185.7083 - val_loss: 41.9418\n",
      "Epoch 163/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 180.1073 - val_loss: 39.2267\n",
      "Epoch 164/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 174.6533 - val_loss: 36.6291\n",
      "Epoch 165/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 169.3437 - val_loss: 34.1469\n",
      "Epoch 166/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 164.1757 - val_loss: 31.7774\n",
      "Epoch 167/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 159.1466 - val_loss: 29.5176\n",
      "Epoch 168/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 154.2538 - val_loss: 27.3651\n",
      "Epoch 169/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 149.4948 - val_loss: 25.3172\n",
      "Epoch 170/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 144.8668 - val_loss: 23.3716\n",
      "Epoch 171/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 140.3675 - val_loss: 21.5253\n",
      "Epoch 172/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 135.9940 - val_loss: 19.7761\n",
      "Epoch 173/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 131.7442 - val_loss: 18.1214\n",
      "Epoch 174/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 127.6152 - val_loss: 16.5587\n",
      "Epoch 175/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 123.6048 - val_loss: 15.0855\n",
      "Epoch 176/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 119.7104 - val_loss: 13.6997\n",
      "Epoch 177/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 115.9298 - val_loss: 12.3985\n",
      "Epoch 178/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 112.2602 - val_loss: 11.1796\n",
      "Epoch 179/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 108.6995 - val_loss: 10.0410\n",
      "Epoch 180/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 105.2454 - val_loss: 8.9801\n",
      "Epoch 181/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 101.8955 - val_loss: 7.9946\n",
      "Epoch 182/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 98.6474 - val_loss: 7.0825\n",
      "Epoch 183/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 95.4990 - val_loss: 6.2413\n",
      "Epoch 184/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 92.4480 - val_loss: 5.4689\n",
      "Epoch 185/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 89.4920 - val_loss: 4.7631\n",
      "Epoch 186/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 86.6291 - val_loss: 4.1220\n",
      "Epoch 187/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 83.8570 - val_loss: 3.5432\n",
      "Epoch 188/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 81.1736 - val_loss: 3.0248\n",
      "Epoch 189/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 78.5767 - val_loss: 2.5648\n",
      "Epoch 190/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 76.0644 - val_loss: 2.1610\n",
      "Epoch 191/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 73.6345 - val_loss: 1.8115\n",
      "Epoch 192/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 71.2849 - val_loss: 1.5144\n",
      "Epoch 193/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 69.0137 - val_loss: 1.2678\n",
      "Epoch 194/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 66.8191 - val_loss: 1.0698\n",
      "Epoch 195/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 64.6989 - val_loss: 0.9184\n",
      "Epoch 196/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 62.6513 - val_loss: 0.8120\n",
      "Epoch 197/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 60.6744 - val_loss: 0.7486\n",
      "Epoch 198/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 58.7663 - val_loss: 0.7266\n",
      "Epoch 199/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 56.9253 - val_loss: 0.7442\n",
      "Epoch 200/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 55.1493 - val_loss: 0.7998\n",
      "Epoch 201/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 53.4367 - val_loss: 0.8915\n",
      "Epoch 202/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 51.7858 - val_loss: 1.0180\n",
      "Epoch 203/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 50.1949 - val_loss: 1.1775\n",
      "Epoch 204/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 48.6623 - val_loss: 1.3684\n",
      "Epoch 205/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 47.1862 - val_loss: 1.5893\n",
      "Epoch 206/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 45.7653 - val_loss: 1.8386\n",
      "Epoch 207/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 44.3976 - val_loss: 2.1150\n",
      "Epoch 208/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 43.0818 - val_loss: 2.4168\n",
      "Epoch 209/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 41.8163 - val_loss: 2.7429\n",
      "Epoch 210/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 40.5996 - val_loss: 3.0917\n",
      "Epoch 211/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 39.4301 - val_loss: 3.4620\n",
      "Epoch 212/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 38.3065 - val_loss: 3.8525\n",
      "Epoch 213/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 37.2272 - val_loss: 4.2619\n",
      "Epoch 214/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 36.1910 - val_loss: 4.6889\n",
      "Epoch 215/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 35.1966 - val_loss: 5.1324\n",
      "Epoch 216/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 34.2423 - val_loss: 5.5912\n",
      "Epoch 217/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 33.3271 - val_loss: 6.0641\n",
      "Epoch 218/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 32.4496 - val_loss: 6.5501\n",
      "Epoch 219/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 31.6086 - val_loss: 7.0482\n",
      "Epoch 220/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 30.8028 - val_loss: 7.5570\n",
      "Epoch 221/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 30.0311 - val_loss: 8.0759\n",
      "Epoch 222/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 29.2922 - val_loss: 8.6037\n",
      "Epoch 223/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 28.5851 - val_loss: 9.1395\n",
      "Epoch 224/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 27.9086 - val_loss: 9.6824\n",
      "Epoch 225/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 27.2617 - val_loss: 10.2316\n",
      "Epoch 226/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.6433 - val_loss: 10.7859\n",
      "Epoch 227/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 26.0524 - val_loss: 11.3450\n",
      "Epoch 228/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 25.4879 - val_loss: 11.9077\n",
      "Epoch 229/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 24.9490 - val_loss: 12.4734\n",
      "Epoch 230/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 24.4345 - val_loss: 13.0414\n",
      "Epoch 231/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 23.9436 - val_loss: 13.6108\n",
      "Epoch 232/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 23.4754 - val_loss: 14.1811\n",
      "Epoch 233/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 23.0291 - val_loss: 14.7517\n",
      "Epoch 234/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 22.6037 - val_loss: 15.3218\n",
      "Epoch 235/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 22.1984 - val_loss: 15.8909\n",
      "Epoch 236/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 21.8125 - val_loss: 16.4584\n",
      "Epoch 237/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 21.4452 - val_loss: 17.0240\n",
      "Epoch 238/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 21.0956 - val_loss: 17.5867\n",
      "Epoch 239/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.7632 - val_loss: 18.1463\n",
      "Epoch 240/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.4471 - val_loss: 18.7024\n",
      "Epoch 241/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1467 - val_loss: 19.2544\n",
      "Epoch 242/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 19.8613 - val_loss: 19.8019\n",
      "Epoch 243/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 19.5903 - val_loss: 20.3447\n",
      "Epoch 244/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 19.3330 - val_loss: 20.8822\n",
      "Epoch 245/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 19.0889 - val_loss: 21.4140\n",
      "Epoch 246/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 18.8575 - val_loss: 21.9401\n",
      "Epoch 247/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 18.6380 - val_loss: 22.4601\n",
      "Epoch 248/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 18.4299 - val_loss: 22.9735\n",
      "Epoch 249/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 18.2328 - val_loss: 23.4801\n",
      "Epoch 250/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 18.0462 - val_loss: 23.9798\n",
      "Epoch 251/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 17.8696 - val_loss: 24.4723\n",
      "Epoch 252/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 17.7024 - val_loss: 24.9574\n",
      "Epoch 253/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 17.5443 - val_loss: 25.4348\n",
      "Epoch 254/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 17.3949 - val_loss: 25.9045\n",
      "Epoch 255/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 17.2536 - val_loss: 26.3661\n",
      "Epoch 256/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 17.1202 - val_loss: 26.8201\n",
      "Epoch 257/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.9941 - val_loss: 27.2656\n",
      "Epoch 258/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.8752 - val_loss: 27.7028\n",
      "Epoch 259/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 16.7630 - val_loss: 28.1317\n",
      "Epoch 260/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.6571 - val_loss: 28.5525\n",
      "Epoch 261/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.5572 - val_loss: 28.9646\n",
      "Epoch 262/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.4631 - val_loss: 29.3681\n",
      "Epoch 263/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.3745 - val_loss: 29.7626\n",
      "Epoch 264/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.2911 - val_loss: 30.1491\n",
      "Epoch 265/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.2124 - val_loss: 30.5268\n",
      "Epoch 266/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.1384 - val_loss: 30.8957\n",
      "Epoch 267/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.0689 - val_loss: 31.2561\n",
      "Epoch 268/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 16.0034 - val_loss: 31.6080\n",
      "Epoch 269/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.9420 - val_loss: 31.9515\n",
      "Epoch 270/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.8841 - val_loss: 32.2864\n",
      "Epoch 271/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.8298 - val_loss: 32.6127\n",
      "Epoch 272/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.7787 - val_loss: 32.9307\n",
      "Epoch 273/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.7309 - val_loss: 33.2403\n",
      "Epoch 274/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.6859 - val_loss: 33.5418\n",
      "Epoch 275/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.6437 - val_loss: 33.8348\n",
      "Epoch 276/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.6042 - val_loss: 34.1200\n",
      "Epoch 277/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.5671 - val_loss: 34.3970\n",
      "Epoch 278/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 15.5324 - val_loss: 34.6662\n",
      "Epoch 279/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.4997 - val_loss: 34.9276\n",
      "Epoch 280/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.4693 - val_loss: 35.1815\n",
      "Epoch 281/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.4407 - val_loss: 35.4278\n",
      "Epoch 282/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.4138 - val_loss: 35.6664\n",
      "Epoch 283/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.3888 - val_loss: 35.8976\n",
      "Epoch 284/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.3654 - val_loss: 36.1220\n",
      "Epoch 285/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.3434 - val_loss: 36.3391\n",
      "Epoch 286/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.3228 - val_loss: 36.5490\n",
      "Epoch 287/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.3037 - val_loss: 36.7522\n",
      "Epoch 288/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.2857 - val_loss: 36.9489\n",
      "Epoch 289/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.2690 - val_loss: 37.1388\n",
      "Epoch 290/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.2533 - val_loss: 37.3224\n",
      "Epoch 291/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.2386 - val_loss: 37.4999\n",
      "Epoch 292/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.2249 - val_loss: 37.6709\n",
      "Epoch 293/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.2121 - val_loss: 37.8362\n",
      "Epoch 294/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.2001 - val_loss: 37.9955\n",
      "Epoch 295/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.1890 - val_loss: 38.1488\n",
      "Epoch 296/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.1786 - val_loss: 38.2969\n",
      "Epoch 297/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.1689 - val_loss: 38.4393\n",
      "Epoch 298/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.1598 - val_loss: 38.5767\n",
      "Epoch 299/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.1513 - val_loss: 38.7089\n",
      "Epoch 300/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.1434 - val_loss: 38.8360\n",
      "Epoch 301/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.1360 - val_loss: 38.9579\n",
      "Epoch 302/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.1291 - val_loss: 39.0754\n",
      "Epoch 303/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 15.1227 - val_loss: 39.1881\n",
      "Epoch 304/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.1167 - val_loss: 39.2964\n",
      "Epoch 305/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.1111 - val_loss: 39.4002\n",
      "Epoch 306/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.1059 - val_loss: 39.4997\n",
      "Epoch 307/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.1011 - val_loss: 39.5953\n",
      "Epoch 308/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0966 - val_loss: 39.6869\n",
      "Epoch 309/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0923 - val_loss: 39.7747\n",
      "Epoch 310/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0884 - val_loss: 39.8588\n",
      "Epoch 311/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0848 - val_loss: 39.9393\n",
      "Epoch 312/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0813 - val_loss: 40.0164\n",
      "Epoch 313/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0782 - val_loss: 40.0901\n",
      "Epoch 314/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0752 - val_loss: 40.1603\n",
      "Epoch 315/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0725 - val_loss: 40.2276\n",
      "Epoch 316/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0699 - val_loss: 40.2917\n",
      "Epoch 317/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0675 - val_loss: 40.3531\n",
      "Epoch 318/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0653 - val_loss: 40.4112\n",
      "Epoch 319/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0633 - val_loss: 40.4671\n",
      "Epoch 320/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0614 - val_loss: 40.5203\n",
      "Epoch 321/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0596 - val_loss: 40.5708\n",
      "Epoch 322/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0580 - val_loss: 40.6190\n",
      "Epoch 323/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 15.0565 - val_loss: 40.6653\n",
      "Epoch 324/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0551 - val_loss: 40.7090\n",
      "Epoch 325/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0538 - val_loss: 40.7505\n",
      "Epoch 326/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0526 - val_loss: 40.7898\n",
      "Epoch 327/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0515 - val_loss: 40.8274\n",
      "Epoch 328/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0505 - val_loss: 40.8630\n",
      "Epoch 329/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0496 - val_loss: 40.8967\n",
      "Epoch 330/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0487 - val_loss: 40.9290\n",
      "Epoch 331/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0480 - val_loss: 40.9593\n",
      "Epoch 332/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0472 - val_loss: 40.9882\n",
      "Epoch 333/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0466 - val_loss: 41.0157\n",
      "Epoch 334/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0460 - val_loss: 41.0414\n",
      "Epoch 335/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0455 - val_loss: 41.0661\n",
      "Epoch 336/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0450 - val_loss: 41.0891\n",
      "Epoch 337/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0445 - val_loss: 41.1111\n",
      "Epoch 338/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0442 - val_loss: 41.1317\n",
      "Epoch 339/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0438 - val_loss: 41.1511\n",
      "Epoch 340/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 15.0435 - val_loss: 41.1697\n",
      "Epoch 341/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0433 - val_loss: 41.1870\n",
      "Epoch 342/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0430 - val_loss: 41.2033\n",
      "Epoch 343/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0429 - val_loss: 41.2189\n",
      "Epoch 344/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0427 - val_loss: 41.2335\n",
      "Epoch 345/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0426 - val_loss: 41.2470\n",
      "Epoch 346/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0425 - val_loss: 41.2601\n",
      "Epoch 347/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0424 - val_loss: 41.2720\n",
      "Epoch 348/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0424 - val_loss: 41.2832\n",
      "Epoch 349/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0424 - val_loss: 41.2939\n",
      "Epoch 350/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0424 - val_loss: 41.3038\n",
      "Epoch 351/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0425 - val_loss: 41.3133\n",
      "Epoch 352/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0425 - val_loss: 41.3219\n",
      "Epoch 353/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0425 - val_loss: 41.3300\n",
      "Epoch 354/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0427 - val_loss: 41.3375\n",
      "Epoch 355/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0428 - val_loss: 41.3447\n",
      "Epoch 356/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0429 - val_loss: 41.3514\n",
      "Epoch 357/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0430 - val_loss: 41.3575\n",
      "Epoch 358/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0432 - val_loss: 41.3630\n",
      "Epoch 359/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 15.0434 - val_loss: 41.3685\n",
      "Epoch 360/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0435 - val_loss: 41.3734\n",
      "Epoch 361/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0437 - val_loss: 41.3779\n",
      "Epoch 362/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0439 - val_loss: 41.3823\n",
      "Epoch 363/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0442 - val_loss: 41.3864\n",
      "Epoch 364/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0443 - val_loss: 41.3902\n",
      "Epoch 365/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0446 - val_loss: 41.3935\n",
      "Epoch 366/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0448 - val_loss: 41.3964\n",
      "Epoch 367/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0451 - val_loss: 41.3994\n",
      "Epoch 368/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0453 - val_loss: 41.4019\n",
      "Epoch 369/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0456 - val_loss: 41.4043\n",
      "Epoch 370/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0459 - val_loss: 41.4064\n",
      "Epoch 371/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0462 - val_loss: 41.4085\n",
      "Epoch 372/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0465 - val_loss: 41.4103\n",
      "Epoch 373/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0467 - val_loss: 41.4119\n",
      "Epoch 374/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0470 - val_loss: 41.4131\n",
      "Epoch 375/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0474 - val_loss: 41.4145\n",
      "Epoch 376/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0477 - val_loss: 41.4155\n",
      "Epoch 377/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0480 - val_loss: 41.4166\n",
      "Epoch 378/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 15.0483 - val_loss: 41.4174\n",
      "Epoch 379/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0486 - val_loss: 41.4184\n",
      "Epoch 380/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0489 - val_loss: 41.4191\n",
      "Epoch 381/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0492 - val_loss: 41.4197\n",
      "Epoch 382/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0496 - val_loss: 41.4202\n",
      "Epoch 383/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0499 - val_loss: 41.4205\n",
      "Epoch 384/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0502 - val_loss: 41.4209\n",
      "Epoch 385/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0506 - val_loss: 41.4212\n",
      "Epoch 386/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0509 - val_loss: 41.4215\n",
      "Epoch 387/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0513 - val_loss: 41.4216\n",
      "Epoch 388/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0516 - val_loss: 41.4217\n",
      "Epoch 389/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0520 - val_loss: 41.4218\n",
      "Epoch 390/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0523 - val_loss: 41.4218\n",
      "Epoch 391/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0526 - val_loss: 41.4217\n",
      "Epoch 392/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0530 - val_loss: 41.4217\n",
      "Epoch 393/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0534 - val_loss: 41.4215\n",
      "Epoch 394/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0537 - val_loss: 41.4214\n",
      "Epoch 395/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0540 - val_loss: 41.4210\n",
      "Epoch 396/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0544 - val_loss: 41.4209\n",
      "Epoch 397/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0547 - val_loss: 41.4205\n",
      "Epoch 398/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0551 - val_loss: 41.4202\n",
      "Epoch 399/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 15.0554 - val_loss: 41.4196\n",
      "Epoch 400/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0558 - val_loss: 41.4195\n",
      "Epoch 401/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0562 - val_loss: 41.4189\n",
      "Epoch 402/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0565 - val_loss: 41.4186\n",
      "Epoch 403/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0569 - val_loss: 41.4180\n",
      "Epoch 404/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0572 - val_loss: 41.4175\n",
      "Epoch 405/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0575 - val_loss: 41.4170\n",
      "Epoch 406/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0579 - val_loss: 41.4164\n",
      "Epoch 407/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0583 - val_loss: 41.4159\n",
      "Epoch 408/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0586 - val_loss: 41.4154\n",
      "Epoch 409/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0589 - val_loss: 41.4150\n",
      "Epoch 410/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0593 - val_loss: 41.4142\n",
      "Epoch 411/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0596 - val_loss: 41.4136\n",
      "Epoch 412/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0600 - val_loss: 41.4129\n",
      "Epoch 413/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0604 - val_loss: 41.4126\n",
      "Epoch 414/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0607 - val_loss: 41.4119\n",
      "Epoch 415/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0611 - val_loss: 41.4115\n",
      "Epoch 416/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 15.0614 - val_loss: 41.4111\n",
      "Epoch 417/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0617 - val_loss: 41.4101\n",
      "Epoch 418/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0621 - val_loss: 41.4095\n",
      "Epoch 419/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0624 - val_loss: 41.4089\n",
      "Epoch 420/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0628 - val_loss: 41.4083\n",
      "Epoch 421/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0631 - val_loss: 41.4076\n",
      "Epoch 422/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0634 - val_loss: 41.4072\n",
      "Epoch 423/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0638 - val_loss: 41.4066\n",
      "Epoch 424/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0642 - val_loss: 41.4062\n",
      "Epoch 425/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0644 - val_loss: 41.4056\n",
      "Epoch 426/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0648 - val_loss: 41.4053\n",
      "Epoch 427/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0651 - val_loss: 41.4045\n",
      "Epoch 428/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0655 - val_loss: 41.4042\n",
      "Epoch 429/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0658 - val_loss: 41.4034\n",
      "Epoch 430/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0661 - val_loss: 41.4028\n",
      "Epoch 431/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0664 - val_loss: 41.4024\n",
      "Epoch 432/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0668 - val_loss: 41.4017\n",
      "Epoch 433/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 15.0671 - val_loss: 41.4013\n",
      "Epoch 434/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0674 - val_loss: 41.4005\n",
      "Epoch 435/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0677 - val_loss: 41.4001\n",
      "Epoch 436/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0680 - val_loss: 41.3994\n",
      "Epoch 437/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0683 - val_loss: 41.3990\n",
      "Epoch 438/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0687 - val_loss: 41.3985\n",
      "Epoch 439/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0689 - val_loss: 41.3981\n",
      "Epoch 440/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0693 - val_loss: 41.3973\n",
      "Epoch 441/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0696 - val_loss: 41.3966\n",
      "Epoch 442/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0699 - val_loss: 41.3961\n",
      "Epoch 443/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0702 - val_loss: 41.3956\n",
      "Epoch 444/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0705 - val_loss: 41.3950\n",
      "Epoch 445/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0708 - val_loss: 41.3944\n",
      "Epoch 446/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0711 - val_loss: 41.3939\n",
      "Epoch 447/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0714 - val_loss: 41.3934\n",
      "Epoch 448/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0717 - val_loss: 41.3929\n",
      "Epoch 449/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0720 - val_loss: 41.3922\n",
      "Epoch 450/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0723 - val_loss: 41.3918\n",
      "Epoch 451/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 15.0726 - val_loss: 41.3910\n",
      "Epoch 452/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0729 - val_loss: 41.3908\n",
      "Epoch 453/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0731 - val_loss: 41.3903\n",
      "Epoch 454/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0734 - val_loss: 41.3895\n",
      "Epoch 455/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0737 - val_loss: 41.3889\n",
      "Epoch 456/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0740 - val_loss: 41.3882\n",
      "Epoch 457/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0743 - val_loss: 41.3878\n",
      "Epoch 458/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0746 - val_loss: 41.3872\n",
      "Epoch 459/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0749 - val_loss: 41.3869\n",
      "Epoch 460/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0751 - val_loss: 41.3865\n",
      "Epoch 461/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0754 - val_loss: 41.3859\n",
      "Epoch 462/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0756 - val_loss: 41.3853\n",
      "Epoch 463/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0759 - val_loss: 41.3848\n",
      "Epoch 464/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0762 - val_loss: 41.3846\n",
      "Epoch 465/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0765 - val_loss: 41.3840\n",
      "Epoch 466/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0767 - val_loss: 41.3836\n",
      "Epoch 467/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0770 - val_loss: 41.3832\n",
      "Epoch 468/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0772 - val_loss: 41.3827\n",
      "Epoch 469/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0775 - val_loss: 41.3824\n",
      "Epoch 470/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 15.0778 - val_loss: 41.3820\n",
      "Epoch 471/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0780 - val_loss: 41.3814\n",
      "Epoch 472/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0782 - val_loss: 41.3809\n",
      "Epoch 473/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0785 - val_loss: 41.3805\n",
      "Epoch 474/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0787 - val_loss: 41.3800\n",
      "Epoch 475/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0789 - val_loss: 41.3794\n",
      "Epoch 476/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0792 - val_loss: 41.3789\n",
      "Epoch 477/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0794 - val_loss: 41.3784\n",
      "Epoch 478/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0797 - val_loss: 41.3780\n",
      "Epoch 479/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0799 - val_loss: 41.3774\n",
      "Epoch 480/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0801 - val_loss: 41.3770\n",
      "Epoch 481/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0804 - val_loss: 41.3767\n",
      "Epoch 482/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0806 - val_loss: 41.3765\n",
      "Epoch 483/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0808 - val_loss: 41.3763\n",
      "Epoch 484/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0810 - val_loss: 41.3757\n",
      "Epoch 485/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0813 - val_loss: 41.3753\n",
      "Epoch 486/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.0815 - val_loss: 41.3749\n",
      "Epoch 487/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0817 - val_loss: 41.3746\n",
      "Epoch 488/500\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 15.0819 - val_loss: 41.3742\n",
      "Epoch 489/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0821 - val_loss: 41.3739\n",
      "Epoch 490/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0823 - val_loss: 41.3735\n",
      "Epoch 491/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0825 - val_loss: 41.3730\n",
      "Epoch 492/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0827 - val_loss: 41.3727\n",
      "Epoch 493/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0829 - val_loss: 41.3722\n",
      "Epoch 494/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0831 - val_loss: 41.3716\n",
      "Epoch 495/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0833 - val_loss: 41.3712\n",
      "Epoch 496/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 15.0836 - val_loss: 41.3708\n",
      "Epoch 497/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0837 - val_loss: 41.3704\n",
      "Epoch 498/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0839 - val_loss: 41.3700\n",
      "Epoch 499/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0841 - val_loss: 41.3697\n",
      "Epoch 500/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 15.0843 - val_loss: 41.3694\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.8407, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0015, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0118, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.5775, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 352ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[69.35704949, 69.345845  , 69.33464052, 69.32343604, 69.31223156,\n",
       "        69.30102708, 69.27201214, 69.24119981, 69.21038749, 69.17957516,\n",
       "        69.14876284, 69.11795051, 69.08713819, 69.05632586, 69.02551354,\n",
       "        68.99470121, 68.96388889, 68.93307656, 68.90226424, 68.87145191,\n",
       "        68.84063959, 68.80982726, 68.77901494, 68.74820261, 68.71739029,\n",
       "        68.68657796, 68.65576564, 68.62495331, 68.59414099, 68.56332866,\n",
       "        68.53251634, 68.50170401, 68.47089169, 68.44007937, 68.40926704,\n",
       "        68.37845472, 68.34764239, 68.31683007, 68.28601774, 68.25520542,\n",
       "        68.22439309, 68.1912465 , 68.14922969, 68.10721289, 68.06519608,\n",
       "        68.02317927, 67.98116246, 67.93914566, 67.89712885, 67.85511204,\n",
       "        67.81309524, 67.77107843, 67.72906162, 67.68704482, 67.64502801,\n",
       "        67.6030112 , 67.5609944 , 67.51897759, 67.47696078, 67.43494398,\n",
       "        67.39292717, 67.35091036, 67.30889356, 67.26687675, 67.22485994,\n",
       "        67.18284314, 67.14082633, 67.09880952, 67.05679272, 67.01477591,\n",
       "        66.9727591 , 66.9307423 , 66.88872549, 66.84670868, 66.80469188,\n",
       "        66.76267507, 66.72065826, 66.68860878, 66.66619981, 66.64379085,\n",
       "        74.55937195,  0.        ,  0.        ,  0.30658042,  0.29149154,\n",
       "         0.13243723,  0.        ,  0.09926027,  0.13731404,  0.        ,\n",
       "         0.77885544,  0.        ,  1.20305598,  0.        ,  0.26323679,\n",
       "         0.        ,  0.16698904,  0.2095235 ,  0.267838  ,  0.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([65.80436508, 65.80343137, 65.80249767, 65.80156396, 65.80063025,\n",
       "       65.79787582, 65.79133987, 65.78480392, 65.77826797, 65.77173203,\n",
       "       65.76519608, 65.75866013, 65.75212418, 65.74558824, 65.73905229,\n",
       "       65.73251634, 65.72598039, 65.71944444, 65.7129085 , 65.70637255,\n",
       "       65.6998366 , 65.69330065, 65.68676471, 65.68022876, 65.67369281,\n",
       "       65.66715686, 65.66062092, 65.65408497, 65.64754902, 65.64101307,\n",
       "       65.63447712, 65.62794118, 65.62140523, 65.61486928, 65.60833333,\n",
       "       65.60179739, 65.59526144, 65.58872549, 65.58218954, 65.57565359,\n",
       "       65.56911765, 65.5625817 , 65.55604575, 65.5495098 , 65.54297386,\n",
       "       65.53643791, 65.52990196, 65.52336601, 65.51683007, 65.51029412,\n",
       "       65.50375817, 65.49722222, 65.49068627, 65.48415033, 65.47761438,\n",
       "       65.47107843, 65.46454248, 65.45800654, 65.45147059, 65.44493464,\n",
       "       65.43839869, 65.43186275, 65.4253268 , 65.41879085, 65.4122549 ,\n",
       "       65.40571895, 65.39918301, 65.39264706, 65.38611111, 65.37957516,\n",
       "       65.37303922, 65.36650327, 65.35996732, 65.35343137, 65.34689542,\n",
       "       65.34035948, 65.33382353, 65.32728758, 65.32075163, 65.31421569,\n",
       "       65.30767974, 65.30114379, 65.29460784, 65.2880719 , 65.28153595,\n",
       "       65.275     , 65.26846405, 65.2619281 , 65.25539216, 65.24885621,\n",
       "       65.24232026, 65.23578431, 65.22924837, 65.22271242, 65.21617647,\n",
       "       65.20964052, 65.20310458, 65.19656863, 65.19003268, 65.18349673])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.46422388307013\n",
      "14.49857481514071\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
