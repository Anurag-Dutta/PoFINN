{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "1595    71.816830\n",
       "1596    71.810294\n",
       "1597    71.803758\n",
       "1598    71.799603\n",
       "1599    71.798669\n",
       "Name: C3, Length: 1600, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c3_interpolated_1500_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "1495    72.470425\n",
       "1496    72.463889\n",
       "1497    72.457353\n",
       "1498    72.450817\n",
       "1499    72.444281\n",
       "Name: C3, Length: 1500, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1500)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.100000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.875910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.651821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90.427731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90.203641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     91.100000  0.000298\n",
       "1     90.875910  0.000298\n",
       "2     90.651821  0.000297\n",
       "3     90.427731  0.000297\n",
       "4     90.203641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.0403  # Value for C0\n",
    "K0 = -0.0012  # Value for K0\n",
    "K1 = -0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0120    # Value for b\n",
    "c = 2.0334    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    91.100000    90.875910    90.651821    90.427731    90.203641   \n",
      "351    90.875910    90.651821    90.427731    90.203641    89.979552   \n",
      "352    90.651821    90.427731    90.203641    89.979552    89.755462   \n",
      "353    90.427731    90.203641    89.979552    89.755462    89.531373   \n",
      "354    90.203641    89.979552    89.755462    89.531373    89.307283   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    89.979552    89.755462    89.531373    89.307283    89.094118  ...   \n",
      "351    89.755462    89.531373    89.307283    89.094118    89.015686  ...   \n",
      "352    89.531373    89.307283    89.094118    89.015686    88.937255  ...   \n",
      "353    89.307283    89.094118    89.015686    88.937255    88.858824  ...   \n",
      "354    89.094118    89.015686    88.937255    88.858824    88.780392  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   81.423343    0.000263   81.404669    0.000263   81.385994    0.000263   \n",
      "351   81.404669    0.000263   81.385994    0.000263   81.367320    0.000262   \n",
      "352   81.385994    0.000263   81.367320    0.000262   81.348646    0.000262   \n",
      "353   81.367320    0.000262   81.348646    0.000262   81.329972    0.000262   \n",
      "354   81.348646    0.000262   81.329972    0.000262   81.311298    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   81.367320    0.000262   81.348646    0.000262  \n",
      "351   81.348646    0.000262   81.329972    0.000262  \n",
      "352   81.329972    0.000262   81.311298    0.000262  \n",
      "353   81.311298    0.000262   81.292624    0.000262  \n",
      "354   81.292624    0.000262   81.273950    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050, 1, 251) (1050, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "14/14 [==============================] - 2s 35ms/step - loss: 6071.9858 - val_loss: 5324.6475\n",
      "Epoch 2/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5989.7925 - val_loss: 5258.5234\n",
      "Epoch 3/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5926.8911 - val_loss: 5197.4214\n",
      "Epoch 4/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5866.4219 - val_loss: 5142.7046\n",
      "Epoch 5/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5808.4028 - val_loss: 5088.5190\n",
      "Epoch 6/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5750.9668 - val_loss: 5034.9043\n",
      "Epoch 7/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5694.1025 - val_loss: 4981.8169\n",
      "Epoch 8/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5637.5479 - val_loss: 4927.5439\n",
      "Epoch 9/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5569.2007 - val_loss: 4856.3604\n",
      "Epoch 10/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5501.5439 - val_loss: 4798.4312\n",
      "Epoch 11/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5440.2690 - val_loss: 4741.6758\n",
      "Epoch 12/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5380.2148 - val_loss: 4686.0044\n",
      "Epoch 13/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5321.2070 - val_loss: 4631.2314\n",
      "Epoch 14/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5263.0732 - val_loss: 4577.2266\n",
      "Epoch 15/500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 5205.6934 - val_loss: 4523.9014\n",
      "Epoch 16/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 5148.9897 - val_loss: 4471.1968\n",
      "Epoch 17/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5092.9097 - val_loss: 4419.0723\n",
      "Epoch 18/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5037.4126 - val_loss: 4367.4966\n",
      "Epoch 19/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4982.4707 - val_loss: 4316.4463\n",
      "Epoch 20/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4928.0615 - val_loss: 4265.9033\n",
      "Epoch 21/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4874.1675 - val_loss: 4215.8516\n",
      "Epoch 22/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4820.7739 - val_loss: 4166.2817\n",
      "Epoch 23/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4767.8677 - val_loss: 4117.1802\n",
      "Epoch 24/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4715.4395 - val_loss: 4068.5386\n",
      "Epoch 25/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4663.4800 - val_loss: 4020.3494\n",
      "Epoch 26/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4611.9814 - val_loss: 3972.6050\n",
      "Epoch 27/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4560.9365 - val_loss: 3925.2981\n",
      "Epoch 28/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4510.3379 - val_loss: 3878.4250\n",
      "Epoch 29/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4460.1816 - val_loss: 3831.9785\n",
      "Epoch 30/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4410.4609 - val_loss: 3785.9548\n",
      "Epoch 31/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4361.1709 - val_loss: 3740.3489\n",
      "Epoch 32/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4312.3076 - val_loss: 3695.1562\n",
      "Epoch 33/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4263.8662 - val_loss: 3650.3723\n",
      "Epoch 34/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4215.8418 - val_loss: 3605.9946\n",
      "Epoch 35/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4168.2329 - val_loss: 3562.0176\n",
      "Epoch 36/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4121.0322 - val_loss: 3518.4392\n",
      "Epoch 37/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4074.2400 - val_loss: 3475.2554\n",
      "Epoch 38/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4027.8494 - val_loss: 3432.4629\n",
      "Epoch 39/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3981.8591 - val_loss: 3390.0581\n",
      "Epoch 40/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3936.2661 - val_loss: 3348.0391\n",
      "Epoch 41/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3891.0657 - val_loss: 3306.4026\n",
      "Epoch 42/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3846.2563 - val_loss: 3265.1443\n",
      "Epoch 43/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3801.8342 - val_loss: 3224.2634\n",
      "Epoch 44/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3757.7971 - val_loss: 3183.7542\n",
      "Epoch 45/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3714.1409 - val_loss: 3143.6179\n",
      "Epoch 46/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3670.8645 - val_loss: 3103.8481\n",
      "Epoch 47/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3627.9646 - val_loss: 3064.4446\n",
      "Epoch 48/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3585.4380 - val_loss: 3025.4041\n",
      "Epoch 49/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3543.2827 - val_loss: 2986.7239\n",
      "Epoch 50/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 3501.4968 - val_loss: 2948.4026\n",
      "Epoch 51/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3460.0764 - val_loss: 2910.4351\n",
      "Epoch 52/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3419.0200 - val_loss: 2872.8215\n",
      "Epoch 53/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3378.3250 - val_loss: 2835.5583\n",
      "Epoch 54/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3337.9883 - val_loss: 2798.6440\n",
      "Epoch 55/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3298.0083 - val_loss: 2762.0757\n",
      "Epoch 56/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3258.3828 - val_loss: 2725.8496\n",
      "Epoch 57/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3219.1084 - val_loss: 2689.9666\n",
      "Epoch 58/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3177.2261 - val_loss: 2646.9482\n",
      "Epoch 59/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3131.0471 - val_loss: 2606.5764\n",
      "Epoch 60/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3087.4580 - val_loss: 2567.2515\n",
      "Epoch 61/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3045.0627 - val_loss: 2529.0037\n",
      "Epoch 62/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3003.7217 - val_loss: 2491.6318\n",
      "Epoch 63/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2963.2393 - val_loss: 2454.9910\n",
      "Epoch 64/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2923.4810 - val_loss: 2418.9819\n",
      "Epoch 65/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2884.3562 - val_loss: 2383.5405\n",
      "Epoch 66/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2845.8054 - val_loss: 2348.6211\n",
      "Epoch 67/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2807.7854 - val_loss: 2314.1897\n",
      "Epoch 68/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2770.2627 - val_loss: 2280.2209\n",
      "Epoch 69/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2733.2144 - val_loss: 2246.6946\n",
      "Epoch 70/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2696.6182 - val_loss: 2213.5933\n",
      "Epoch 71/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2660.4587 - val_loss: 2180.9053\n",
      "Epoch 72/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2624.7229 - val_loss: 2148.6165\n",
      "Epoch 73/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2589.3972 - val_loss: 2116.7180\n",
      "Epoch 74/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2554.4736 - val_loss: 2085.2009\n",
      "Epoch 75/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2519.9414 - val_loss: 2054.0574\n",
      "Epoch 76/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2485.7932 - val_loss: 2023.2799\n",
      "Epoch 77/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2452.0222 - val_loss: 1992.8619\n",
      "Epoch 78/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2418.6213 - val_loss: 1962.7980\n",
      "Epoch 79/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2385.5852 - val_loss: 1933.0830\n",
      "Epoch 80/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2352.9077 - val_loss: 1903.7119\n",
      "Epoch 81/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2320.5847 - val_loss: 1874.6787\n",
      "Epoch 82/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2288.6101 - val_loss: 1845.9803\n",
      "Epoch 83/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2256.9805 - val_loss: 1817.6129\n",
      "Epoch 84/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2225.6909 - val_loss: 1789.5708\n",
      "Epoch 85/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2194.7380 - val_loss: 1761.8527\n",
      "Epoch 86/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 2164.1174 - val_loss: 1734.4531\n",
      "Epoch 87/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2133.8262 - val_loss: 1707.3689\n",
      "Epoch 88/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2103.8601 - val_loss: 1680.5970\n",
      "Epoch 89/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2074.2156 - val_loss: 1654.1334\n",
      "Epoch 90/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2044.8898 - val_loss: 1627.9768\n",
      "Epoch 91/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2015.8798 - val_loss: 1602.1226\n",
      "Epoch 92/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1987.1826 - val_loss: 1576.5687\n",
      "Epoch 93/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1958.7954 - val_loss: 1551.3120\n",
      "Epoch 94/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1930.7148 - val_loss: 1526.3497\n",
      "Epoch 95/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1902.9380 - val_loss: 1501.6794\n",
      "Epoch 96/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1875.4626 - val_loss: 1477.2985\n",
      "Epoch 97/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1848.2863 - val_loss: 1453.2046\n",
      "Epoch 98/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1821.4058 - val_loss: 1429.3940\n",
      "Epoch 99/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1794.8193 - val_loss: 1405.8650\n",
      "Epoch 100/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1768.5229 - val_loss: 1382.6156\n",
      "Epoch 101/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1742.5159 - val_loss: 1359.6427\n",
      "Epoch 102/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1716.7949 - val_loss: 1336.9449\n",
      "Epoch 103/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1691.3579 - val_loss: 1314.5192\n",
      "Epoch 104/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1666.2026 - val_loss: 1292.3641\n",
      "Epoch 105/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1641.3269 - val_loss: 1270.4761\n",
      "Epoch 106/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1616.7281 - val_loss: 1248.8535\n",
      "Epoch 107/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1592.4043 - val_loss: 1227.4944\n",
      "Epoch 108/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1568.3531 - val_loss: 1206.3962\n",
      "Epoch 109/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1544.5723 - val_loss: 1185.5574\n",
      "Epoch 110/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1521.0593 - val_loss: 1164.9762\n",
      "Epoch 111/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1497.8138 - val_loss: 1144.6489\n",
      "Epoch 112/500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1474.8318 - val_loss: 1124.5748\n",
      "Epoch 113/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1452.1122 - val_loss: 1104.7520\n",
      "Epoch 114/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1429.6527 - val_loss: 1085.1769\n",
      "Epoch 115/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1407.4512 - val_loss: 1065.8497\n",
      "Epoch 116/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1385.5056 - val_loss: 1046.7667\n",
      "Epoch 117/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 1363.8143 - val_loss: 1027.9263\n",
      "Epoch 118/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1342.3749 - val_loss: 1009.3270\n",
      "Epoch 119/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1321.1857 - val_loss: 990.9664\n",
      "Epoch 120/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1300.2451 - val_loss: 972.8434\n",
      "Epoch 121/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1279.5504 - val_loss: 954.9552\n",
      "Epoch 122/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1259.1006 - val_loss: 937.2998\n",
      "Epoch 123/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1238.8932 - val_loss: 919.8768\n",
      "Epoch 124/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1218.9265 - val_loss: 902.6827\n",
      "Epoch 125/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1199.1989 - val_loss: 885.7156\n",
      "Epoch 126/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1179.7080 - val_loss: 868.9753\n",
      "Epoch 127/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1160.4525 - val_loss: 852.4587\n",
      "Epoch 128/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1141.4299 - val_loss: 836.1634\n",
      "Epoch 129/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1122.6389 - val_loss: 820.0890\n",
      "Epoch 130/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1104.0776 - val_loss: 804.2329\n",
      "Epoch 131/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1085.7444 - val_loss: 788.5935\n",
      "Epoch 132/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1067.6370 - val_loss: 773.1696\n",
      "Epoch 133/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1049.7544 - val_loss: 757.9583\n",
      "Epoch 134/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1032.0940 - val_loss: 742.9589\n",
      "Epoch 135/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1014.6548 - val_loss: 728.1681\n",
      "Epoch 136/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 997.4343 - val_loss: 713.5861\n",
      "Epoch 137/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 980.4313 - val_loss: 699.2095\n",
      "Epoch 138/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 963.6441 - val_loss: 685.0377\n",
      "Epoch 139/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 947.0709 - val_loss: 671.0685\n",
      "Epoch 140/500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 930.7097 - val_loss: 657.3000\n",
      "Epoch 141/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 914.5587 - val_loss: 643.7316\n",
      "Epoch 142/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 898.6169 - val_loss: 630.3602\n",
      "Epoch 143/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 882.8822 - val_loss: 617.1843\n",
      "Epoch 144/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 867.3530 - val_loss: 604.2028\n",
      "Epoch 145/500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 852.0278 - val_loss: 591.4135\n",
      "Epoch 146/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 836.9040 - val_loss: 578.8157\n",
      "Epoch 147/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 821.9810 - val_loss: 566.4062\n",
      "Epoch 148/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 807.2571 - val_loss: 554.1851\n",
      "Epoch 149/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 792.7299 - val_loss: 542.1490\n",
      "Epoch 150/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 778.3982 - val_loss: 530.2975\n",
      "Epoch 151/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 764.2609 - val_loss: 518.6281\n",
      "Epoch 152/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 750.3154 - val_loss: 507.1397\n",
      "Epoch 153/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 736.5607 - val_loss: 495.8310\n",
      "Epoch 154/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 722.9951 - val_loss: 484.6995\n",
      "Epoch 155/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 709.6166 - val_loss: 473.7439\n",
      "Epoch 156/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 696.4242 - val_loss: 462.9629\n",
      "Epoch 157/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 683.4161 - val_loss: 452.3545\n",
      "Epoch 158/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 670.5903 - val_loss: 441.9165\n",
      "Epoch 159/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 657.9454 - val_loss: 431.6483\n",
      "Epoch 160/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 645.4799 - val_loss: 421.5482\n",
      "Epoch 161/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 633.1926 - val_loss: 411.6146\n",
      "Epoch 162/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 621.0812 - val_loss: 401.8453\n",
      "Epoch 163/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 609.1447 - val_loss: 392.2392\n",
      "Epoch 164/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 597.3813 - val_loss: 382.7948\n",
      "Epoch 165/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 585.7897 - val_loss: 373.5105\n",
      "Epoch 166/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 574.3681 - val_loss: 364.3850\n",
      "Epoch 167/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 563.1151 - val_loss: 355.4158\n",
      "Epoch 168/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 552.0287 - val_loss: 346.6020\n",
      "Epoch 169/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 541.1080 - val_loss: 337.9420\n",
      "Epoch 170/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 530.3512 - val_loss: 329.4341\n",
      "Epoch 171/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 519.7566 - val_loss: 321.0769\n",
      "Epoch 172/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 509.3230 - val_loss: 312.8685\n",
      "Epoch 173/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 499.0486 - val_loss: 304.8078\n",
      "Epoch 174/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 488.9320 - val_loss: 296.8932\n",
      "Epoch 175/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 478.9717 - val_loss: 289.1228\n",
      "Epoch 176/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 469.1661 - val_loss: 281.4953\n",
      "Epoch 177/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 459.5136 - val_loss: 274.0088\n",
      "Epoch 178/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 450.0131 - val_loss: 266.6631\n",
      "Epoch 179/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 440.6625 - val_loss: 259.4551\n",
      "Epoch 180/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 431.4610 - val_loss: 252.3840\n",
      "Epoch 181/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 422.4067 - val_loss: 245.4490\n",
      "Epoch 182/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 413.4986 - val_loss: 238.6471\n",
      "Epoch 183/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 404.7344 - val_loss: 231.9776\n",
      "Epoch 184/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 396.1132 - val_loss: 225.4393\n",
      "Epoch 185/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 387.6335 - val_loss: 219.0297\n",
      "Epoch 186/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 379.2935 - val_loss: 212.7485\n",
      "Epoch 187/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 371.0922 - val_loss: 206.5939\n",
      "Epoch 188/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 363.0278 - val_loss: 200.5636\n",
      "Epoch 189/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 355.0988 - val_loss: 194.6569\n",
      "Epoch 190/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 347.3041 - val_loss: 188.8726\n",
      "Epoch 191/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 339.6421 - val_loss: 183.2082\n",
      "Epoch 192/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 332.1111 - val_loss: 177.6629\n",
      "Epoch 193/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 324.7099 - val_loss: 172.2355\n",
      "Epoch 194/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 317.4374 - val_loss: 166.9244\n",
      "Epoch 195/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 310.2918 - val_loss: 161.7276\n",
      "Epoch 196/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 303.2716 - val_loss: 156.6441\n",
      "Epoch 197/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 296.3753 - val_loss: 151.6725\n",
      "Epoch 198/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 289.6019 - val_loss: 146.8112\n",
      "Epoch 199/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 282.9497 - val_loss: 142.0588\n",
      "Epoch 200/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 276.4174 - val_loss: 137.4140\n",
      "Epoch 201/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 270.0038 - val_loss: 132.8754\n",
      "Epoch 202/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 263.7072 - val_loss: 128.4416\n",
      "Epoch 203/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 257.5264 - val_loss: 124.1109\n",
      "Epoch 204/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 251.4598 - val_loss: 119.8821\n",
      "Epoch 205/500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 245.5061 - val_loss: 115.7537\n",
      "Epoch 206/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 239.6639 - val_loss: 111.7245\n",
      "Epoch 207/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 233.9322 - val_loss: 107.7934\n",
      "Epoch 208/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 228.3092 - val_loss: 103.9583\n",
      "Epoch 209/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 222.7939 - val_loss: 100.2181\n",
      "Epoch 210/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 217.3846 - val_loss: 96.5718\n",
      "Epoch 211/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 212.0803 - val_loss: 93.0173\n",
      "Epoch 212/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 206.8791 - val_loss: 89.5542\n",
      "Epoch 213/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 201.7803 - val_loss: 86.1803\n",
      "Epoch 214/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 196.7821 - val_loss: 82.8943\n",
      "Epoch 215/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 191.8834 - val_loss: 79.6956\n",
      "Epoch 216/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 187.0831 - val_loss: 76.5825\n",
      "Epoch 217/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 182.3796 - val_loss: 73.5536\n",
      "Epoch 218/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 177.7717 - val_loss: 70.6073\n",
      "Epoch 219/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 173.2580 - val_loss: 67.7424\n",
      "Epoch 220/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 168.8371 - val_loss: 64.9579\n",
      "Epoch 221/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 164.5081 - val_loss: 62.2527\n",
      "Epoch 222/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 160.2694 - val_loss: 59.6247\n",
      "Epoch 223/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 156.1199 - val_loss: 57.0732\n",
      "Epoch 224/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 152.0579 - val_loss: 54.5969\n",
      "Epoch 225/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 148.0829 - val_loss: 52.1943\n",
      "Epoch 226/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 144.1930 - val_loss: 49.8643\n",
      "Epoch 227/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 140.3873 - val_loss: 47.6056\n",
      "Epoch 228/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 136.6644 - val_loss: 45.4172\n",
      "Epoch 229/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 133.0231 - val_loss: 43.2973\n",
      "Epoch 230/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 129.4624 - val_loss: 41.2449\n",
      "Epoch 231/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 125.9807 - val_loss: 39.2591\n",
      "Epoch 232/500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 122.5771 - val_loss: 37.3383\n",
      "Epoch 233/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 119.2500 - val_loss: 35.4816\n",
      "Epoch 234/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 115.9988 - val_loss: 33.6873\n",
      "Epoch 235/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 112.8217 - val_loss: 31.9548\n",
      "Epoch 236/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 109.7180 - val_loss: 30.2823\n",
      "Epoch 237/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 106.6860 - val_loss: 28.6691\n",
      "Epoch 238/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 103.7249 - val_loss: 27.1140\n",
      "Epoch 239/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 100.8335 - val_loss: 25.6154\n",
      "Epoch 240/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 98.0105 - val_loss: 24.1728\n",
      "Epoch 241/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 95.2550 - val_loss: 22.7845\n",
      "Epoch 242/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 92.5655 - val_loss: 21.4496\n",
      "Epoch 243/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 89.9412 - val_loss: 20.1669\n",
      "Epoch 244/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 87.3810 - val_loss: 18.9355\n",
      "Epoch 245/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 84.8837 - val_loss: 17.7541\n",
      "Epoch 246/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 82.4481 - val_loss: 16.6217\n",
      "Epoch 247/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 80.0733 - val_loss: 15.5372\n",
      "Epoch 248/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 77.7581 - val_loss: 14.4992\n",
      "Epoch 249/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 75.5012 - val_loss: 13.5072\n",
      "Epoch 250/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 73.3020 - val_loss: 12.5599\n",
      "Epoch 251/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 71.1593 - val_loss: 11.6562\n",
      "Epoch 252/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 69.0717 - val_loss: 10.7950\n",
      "Epoch 253/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 67.0387 - val_loss: 9.9755\n",
      "Epoch 254/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 65.0589 - val_loss: 9.1964\n",
      "Epoch 255/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 63.1313 - val_loss: 8.4568\n",
      "Epoch 256/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 61.2548 - val_loss: 7.7557\n",
      "Epoch 257/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 59.4288 - val_loss: 7.0924\n",
      "Epoch 258/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 57.6520 - val_loss: 6.4657\n",
      "Epoch 259/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 55.9236 - val_loss: 5.8745\n",
      "Epoch 260/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 54.2423 - val_loss: 5.3180\n",
      "Epoch 261/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 52.6076 - val_loss: 4.7953\n",
      "Epoch 262/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 51.0183 - val_loss: 4.3055\n",
      "Epoch 263/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 49.4734 - val_loss: 3.8475\n",
      "Epoch 264/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 47.9721 - val_loss: 3.4206\n",
      "Epoch 265/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 46.5134 - val_loss: 3.0237\n",
      "Epoch 266/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 45.0964 - val_loss: 2.6562\n",
      "Epoch 267/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 43.7202 - val_loss: 2.3170\n",
      "Epoch 268/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 42.3841 - val_loss: 2.0053\n",
      "Epoch 269/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 41.0869 - val_loss: 1.7203\n",
      "Epoch 270/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 39.8281 - val_loss: 1.4612\n",
      "Epoch 271/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 38.6066 - val_loss: 1.2272\n",
      "Epoch 272/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 37.4216 - val_loss: 1.0172\n",
      "Epoch 273/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 36.2722 - val_loss: 0.8308\n",
      "Epoch 274/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 35.1578 - val_loss: 0.6670\n",
      "Epoch 275/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 34.0774 - val_loss: 0.5251\n",
      "Epoch 276/500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 33.0301 - val_loss: 0.4043\n",
      "Epoch 277/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 32.0153 - val_loss: 0.3038\n",
      "Epoch 278/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 31.0323 - val_loss: 0.2230\n",
      "Epoch 279/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 30.0801 - val_loss: 0.1612\n",
      "Epoch 280/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 29.1582 - val_loss: 0.1175\n",
      "Epoch 281/500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 28.2657 - val_loss: 0.0914\n",
      "Epoch 282/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 27.4017 - val_loss: 0.0821\n",
      "Epoch 283/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 26.5659 - val_loss: 0.0889\n",
      "Epoch 284/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 25.7572 - val_loss: 0.1113\n",
      "Epoch 285/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 24.9751 - val_loss: 0.1485\n",
      "Epoch 286/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 24.2188 - val_loss: 0.2000\n",
      "Epoch 287/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 23.4877 - val_loss: 0.2650\n",
      "Epoch 288/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 22.7811 - val_loss: 0.3431\n",
      "Epoch 289/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.0985 - val_loss: 0.4336\n",
      "Epoch 290/500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 21.4390 - val_loss: 0.5359\n",
      "Epoch 291/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 20.8022 - val_loss: 0.6495\n",
      "Epoch 292/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 20.1873 - val_loss: 0.7738\n",
      "Epoch 293/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 19.5939 - val_loss: 0.9083\n",
      "Epoch 294/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 19.0211 - val_loss: 1.0524\n",
      "Epoch 295/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 18.4687 - val_loss: 1.2055\n",
      "Epoch 296/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 17.9358 - val_loss: 1.3674\n",
      "Epoch 297/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.4219 - val_loss: 1.5373\n",
      "Epoch 298/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 16.9265 - val_loss: 1.7148\n",
      "Epoch 299/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.4491 - val_loss: 1.8995\n",
      "Epoch 300/500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 15.9891 - val_loss: 2.0909\n",
      "Epoch 301/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 15.5461 - val_loss: 2.2886\n",
      "Epoch 302/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 15.1194 - val_loss: 2.4921\n",
      "Epoch 303/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.7085 - val_loss: 2.7010\n",
      "Epoch 304/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 14.3131 - val_loss: 2.9149\n",
      "Epoch 305/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 13.9327 - val_loss: 3.1334\n",
      "Epoch 306/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 13.5667 - val_loss: 3.3561\n",
      "Epoch 307/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 13.2148 - val_loss: 3.5826\n",
      "Epoch 308/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 12.8763 - val_loss: 3.8126\n",
      "Epoch 309/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 12.5510 - val_loss: 4.0458\n",
      "Epoch 310/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 12.2384 - val_loss: 4.2816\n",
      "Epoch 311/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 11.9381 - val_loss: 4.5201\n",
      "Epoch 312/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 11.6496 - val_loss: 4.7605\n",
      "Epoch 313/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 11.3727 - val_loss: 5.0030\n",
      "Epoch 314/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 11.1068 - val_loss: 5.2467\n",
      "Epoch 315/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 10.8517 - val_loss: 5.4918\n",
      "Epoch 316/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 10.6069 - val_loss: 5.7378\n",
      "Epoch 317/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 10.3722 - val_loss: 5.9846\n",
      "Epoch 318/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 10.1470 - val_loss: 6.2318\n",
      "Epoch 319/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.9312 - val_loss: 6.4792\n",
      "Epoch 320/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.7244 - val_loss: 6.7265\n",
      "Epoch 321/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.5262 - val_loss: 6.9736\n",
      "Epoch 322/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.3364 - val_loss: 7.2203\n",
      "Epoch 323/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.1545 - val_loss: 7.4661\n",
      "Epoch 324/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 8.9805 - val_loss: 7.7111\n",
      "Epoch 325/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 8.8140 - val_loss: 7.9550\n",
      "Epoch 326/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 8.6546 - val_loss: 8.1977\n",
      "Epoch 327/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 8.5022 - val_loss: 8.4391\n",
      "Epoch 328/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 8.3563 - val_loss: 8.6787\n",
      "Epoch 329/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 8.2168 - val_loss: 8.9166\n",
      "Epoch 330/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 8.0835 - val_loss: 9.1527\n",
      "Epoch 331/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 7.9561 - val_loss: 9.3868\n",
      "Epoch 332/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 7.8344 - val_loss: 9.6186\n",
      "Epoch 333/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 7.7182 - val_loss: 9.8482\n",
      "Epoch 334/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 7.6072 - val_loss: 10.0753\n",
      "Epoch 335/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 7.5013 - val_loss: 10.3002\n",
      "Epoch 336/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 7.4001 - val_loss: 10.5221\n",
      "Epoch 337/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 7.3036 - val_loss: 10.7414\n",
      "Epoch 338/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.2116 - val_loss: 10.9581\n",
      "Epoch 339/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.1238 - val_loss: 11.1716\n",
      "Epoch 340/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 7.0401 - val_loss: 11.3825\n",
      "Epoch 341/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 6.9602 - val_loss: 11.5902\n",
      "Epoch 342/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 6.8842 - val_loss: 11.7951\n",
      "Epoch 343/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 6.8116 - val_loss: 11.9965\n",
      "Epoch 344/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 6.7426 - val_loss: 12.1949\n",
      "Epoch 345/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 6.6768 - val_loss: 12.3900\n",
      "Epoch 346/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 6.6142 - val_loss: 12.5819\n",
      "Epoch 347/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 6.5545 - val_loss: 12.7705\n",
      "Epoch 348/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 6.4977 - val_loss: 12.9559\n",
      "Epoch 349/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 6.4437 - val_loss: 13.1378\n",
      "Epoch 350/500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 6.3923 - val_loss: 13.3163\n",
      "Epoch 351/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 6.3434 - val_loss: 13.4913\n",
      "Epoch 352/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 6.2969 - val_loss: 13.6631\n",
      "Epoch 353/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 6.2527 - val_loss: 13.8314\n",
      "Epoch 354/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 6.2106 - val_loss: 13.9964\n",
      "Epoch 355/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 6.1706 - val_loss: 14.1579\n",
      "Epoch 356/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 6.1326 - val_loss: 14.3160\n",
      "Epoch 357/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 6.0965 - val_loss: 14.4706\n",
      "Epoch 358/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 6.0622 - val_loss: 14.6218\n",
      "Epoch 359/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 6.0296 - val_loss: 14.7698\n",
      "Epoch 360/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 5.9986 - val_loss: 14.9142\n",
      "Epoch 361/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.9692 - val_loss: 15.0554\n",
      "Epoch 362/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.9413 - val_loss: 15.1933\n",
      "Epoch 363/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.9148 - val_loss: 15.3278\n",
      "Epoch 364/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.8896 - val_loss: 15.4590\n",
      "Epoch 365/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.8657 - val_loss: 15.5869\n",
      "Epoch 366/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.8430 - val_loss: 15.7118\n",
      "Epoch 367/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.8215 - val_loss: 15.8332\n",
      "Epoch 368/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.8011 - val_loss: 15.9519\n",
      "Epoch 369/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.7817 - val_loss: 16.0671\n",
      "Epoch 370/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.7633 - val_loss: 16.1796\n",
      "Epoch 371/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.7458 - val_loss: 16.2888\n",
      "Epoch 372/500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 5.7293 - val_loss: 16.3952\n",
      "Epoch 373/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.7135 - val_loss: 16.4986\n",
      "Epoch 374/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.6986 - val_loss: 16.5990\n",
      "Epoch 375/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.6845 - val_loss: 16.6967\n",
      "Epoch 376/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.6711 - val_loss: 16.7913\n",
      "Epoch 377/500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 5.6584 - val_loss: 16.8834\n",
      "Epoch 378/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.6463 - val_loss: 16.9727\n",
      "Epoch 379/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.6349 - val_loss: 17.0591\n",
      "Epoch 380/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.6240 - val_loss: 17.1433\n",
      "Epoch 381/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.6137 - val_loss: 17.2247\n",
      "Epoch 382/500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 5.6040 - val_loss: 17.3036\n",
      "Epoch 383/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 5.5947 - val_loss: 17.3800\n",
      "Epoch 384/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.5859 - val_loss: 17.4541\n",
      "Epoch 385/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.5776 - val_loss: 17.5257\n",
      "Epoch 386/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.5697 - val_loss: 17.5949\n",
      "Epoch 387/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.5623 - val_loss: 17.6618\n",
      "Epoch 388/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.5552 - val_loss: 17.7267\n",
      "Epoch 389/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.5484 - val_loss: 17.7894\n",
      "Epoch 390/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.5420 - val_loss: 17.8497\n",
      "Epoch 391/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.5360 - val_loss: 17.9083\n",
      "Epoch 392/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.5303 - val_loss: 17.9647\n",
      "Epoch 393/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.5248 - val_loss: 18.0192\n",
      "Epoch 394/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.5197 - val_loss: 18.0717\n",
      "Epoch 395/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.5147 - val_loss: 18.1222\n",
      "Epoch 396/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.5101 - val_loss: 18.1710\n",
      "Epoch 397/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.5057 - val_loss: 18.2181\n",
      "Epoch 398/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.5016 - val_loss: 18.2635\n",
      "Epoch 399/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4976 - val_loss: 18.3072\n",
      "Epoch 400/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4939 - val_loss: 18.3492\n",
      "Epoch 401/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4903 - val_loss: 18.3898\n",
      "Epoch 402/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4869 - val_loss: 18.4285\n",
      "Epoch 403/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4837 - val_loss: 18.4660\n",
      "Epoch 404/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4807 - val_loss: 18.5020\n",
      "Epoch 405/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4778 - val_loss: 18.5366\n",
      "Epoch 406/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 5.4751 - val_loss: 18.5696\n",
      "Epoch 407/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4725 - val_loss: 18.6017\n",
      "Epoch 408/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4701 - val_loss: 18.6322\n",
      "Epoch 409/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4677 - val_loss: 18.6616\n",
      "Epoch 410/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4655 - val_loss: 18.6896\n",
      "Epoch 411/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4635 - val_loss: 18.7165\n",
      "Epoch 412/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4615 - val_loss: 18.7423\n",
      "Epoch 413/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4597 - val_loss: 18.7671\n",
      "Epoch 414/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4579 - val_loss: 18.7909\n",
      "Epoch 415/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4562 - val_loss: 18.8135\n",
      "Epoch 416/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4546 - val_loss: 18.8352\n",
      "Epoch 417/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4531 - val_loss: 18.8560\n",
      "Epoch 418/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4517 - val_loss: 18.8759\n",
      "Epoch 419/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4503 - val_loss: 18.8949\n",
      "Epoch 420/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4490 - val_loss: 18.9128\n",
      "Epoch 421/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4479 - val_loss: 18.9300\n",
      "Epoch 422/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4467 - val_loss: 18.9467\n",
      "Epoch 423/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4457 - val_loss: 18.9624\n",
      "Epoch 424/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4447 - val_loss: 18.9774\n",
      "Epoch 425/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4437 - val_loss: 18.9917\n",
      "Epoch 426/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4428 - val_loss: 19.0055\n",
      "Epoch 427/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 5.4420 - val_loss: 19.0183\n",
      "Epoch 428/500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 5.4412 - val_loss: 19.0308\n",
      "Epoch 429/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4404 - val_loss: 19.0424\n",
      "Epoch 430/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4397 - val_loss: 19.0537\n",
      "Epoch 431/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4390 - val_loss: 19.0644\n",
      "Epoch 432/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4384 - val_loss: 19.0745\n",
      "Epoch 433/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4378 - val_loss: 19.0841\n",
      "Epoch 434/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4373 - val_loss: 19.0933\n",
      "Epoch 435/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4368 - val_loss: 19.1021\n",
      "Epoch 436/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4363 - val_loss: 19.1103\n",
      "Epoch 437/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4358 - val_loss: 19.1180\n",
      "Epoch 438/500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 5.4354 - val_loss: 19.1255\n",
      "Epoch 439/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4350 - val_loss: 19.1325\n",
      "Epoch 440/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4346 - val_loss: 19.1390\n",
      "Epoch 441/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4343 - val_loss: 19.1452\n",
      "Epoch 442/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4340 - val_loss: 19.1510\n",
      "Epoch 443/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4337 - val_loss: 19.1568\n",
      "Epoch 444/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4334 - val_loss: 19.1620\n",
      "Epoch 445/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4332 - val_loss: 19.1669\n",
      "Epoch 446/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4330 - val_loss: 19.1718\n",
      "Epoch 447/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4327 - val_loss: 19.1763\n",
      "Epoch 448/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 5.4325 - val_loss: 19.1804\n",
      "Epoch 449/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4324 - val_loss: 19.1845\n",
      "Epoch 450/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4322 - val_loss: 19.1881\n",
      "Epoch 451/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4321 - val_loss: 19.1916\n",
      "Epoch 452/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4320 - val_loss: 19.1950\n",
      "Epoch 453/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4318 - val_loss: 19.1980\n",
      "Epoch 454/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4317 - val_loss: 19.2009\n",
      "Epoch 455/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4316 - val_loss: 19.2036\n",
      "Epoch 456/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4316 - val_loss: 19.2062\n",
      "Epoch 457/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4315 - val_loss: 19.2086\n",
      "Epoch 458/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4314 - val_loss: 19.2108\n",
      "Epoch 459/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4314 - val_loss: 19.2129\n",
      "Epoch 460/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4314 - val_loss: 19.2147\n",
      "Epoch 461/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4313 - val_loss: 19.2164\n",
      "Epoch 462/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4313 - val_loss: 19.2182\n",
      "Epoch 463/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4313 - val_loss: 19.2195\n",
      "Epoch 464/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4313 - val_loss: 19.2211\n",
      "Epoch 465/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4314 - val_loss: 19.2226\n",
      "Epoch 466/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4313 - val_loss: 19.2238\n",
      "Epoch 467/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4314 - val_loss: 19.2248\n",
      "Epoch 468/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 5.4314 - val_loss: 19.2261\n",
      "Epoch 469/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4314 - val_loss: 19.2270\n",
      "Epoch 470/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4315 - val_loss: 19.2280\n",
      "Epoch 471/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4315 - val_loss: 19.2287\n",
      "Epoch 472/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4316 - val_loss: 19.2296\n",
      "Epoch 473/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4316 - val_loss: 19.2303\n",
      "Epoch 474/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4317 - val_loss: 19.2309\n",
      "Epoch 475/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4317 - val_loss: 19.2315\n",
      "Epoch 476/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4318 - val_loss: 19.2319\n",
      "Epoch 477/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4319 - val_loss: 19.2324\n",
      "Epoch 478/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4320 - val_loss: 19.2329\n",
      "Epoch 479/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4320 - val_loss: 19.2333\n",
      "Epoch 480/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4321 - val_loss: 19.2337\n",
      "Epoch 481/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4322 - val_loss: 19.2341\n",
      "Epoch 482/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4323 - val_loss: 19.2343\n",
      "Epoch 483/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4324 - val_loss: 19.2345\n",
      "Epoch 484/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4325 - val_loss: 19.2347\n",
      "Epoch 485/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4326 - val_loss: 19.2348\n",
      "Epoch 486/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4327 - val_loss: 19.2349\n",
      "Epoch 487/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4328 - val_loss: 19.2350\n",
      "Epoch 488/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4329 - val_loss: 19.2352\n",
      "Epoch 489/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4330 - val_loss: 19.2353\n",
      "Epoch 490/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4331 - val_loss: 19.2353\n",
      "Epoch 491/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4332 - val_loss: 19.2353\n",
      "Epoch 492/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4333 - val_loss: 19.2353\n",
      "Epoch 493/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4334 - val_loss: 19.2353\n",
      "Epoch 494/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4335 - val_loss: 19.2352\n",
      "Epoch 495/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 5.4337 - val_loss: 19.2351\n",
      "Epoch 496/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4338 - val_loss: 19.2351\n",
      "Epoch 497/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4339 - val_loss: 19.2350\n",
      "Epoch 498/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4340 - val_loss: 19.2350\n",
      "Epoch 499/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4341 - val_loss: 19.2349\n",
      "Epoch 500/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.4342 - val_loss: 19.2348\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.0403, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0012, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0120, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.0334, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1e-10\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 330ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[75.50802988, 75.48310458, 75.45677404, 75.43044351, 75.40411298,\n",
       "        75.37778245, 75.35145191, 75.32512138, 75.29879085, 75.27246032,\n",
       "        75.24612979, 75.21979925, 75.19346872, 75.16713819, 75.14080766,\n",
       "        75.11447712, 75.08814659, 75.06181606, 75.03548553, 75.009155  ,\n",
       "        74.98282446, 74.95649393, 74.9301634 , 74.90383287, 74.87750233,\n",
       "        74.8511718 , 74.82484127, 74.79851074, 74.77218021, 74.74584967,\n",
       "        74.71951914, 74.69318861, 74.66685808, 74.64052754, 74.61419701,\n",
       "        74.58786648, 74.56153595, 74.55314192, 74.54585901, 74.5385761 ,\n",
       "        74.53129318, 74.52401027, 74.51672736, 74.50944444, 74.50216153,\n",
       "        74.49487862, 74.4875957 , 74.48031279, 74.47302988, 74.46574697,\n",
       "        74.45846405, 74.45118114, 74.44389823, 74.43661531, 74.4293324 ,\n",
       "        74.42204949, 74.41476657, 74.40748366, 74.40020075, 74.39291783,\n",
       "        74.38563492, 74.37835201, 74.37106909, 74.36378618, 74.35650327,\n",
       "        74.34922035, 74.34193744, 74.33465453, 74.32737162, 74.3200887 ,\n",
       "        74.31280579, 74.30552288, 74.29661531, 74.28260971, 74.26860411,\n",
       "        74.25459851, 74.2405929 , 74.2265873 , 74.2125817 , 74.1985761 ,\n",
       "        78.96292114,  0.31688905,  0.        ,  1.20782113,  0.        ,\n",
       "         0.20382594,  0.15123923,  0.        ,  0.        ,  0.71398759,\n",
       "         0.31362844,  0.25656152,  0.        ,  0.97541744,  0.31501234,\n",
       "         0.17827012,  0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([72.4377451 , 72.43120915, 72.4246732 , 72.41813725, 72.41160131,\n",
       "       72.40506536, 72.39852941, 72.39199346, 72.38545752, 72.37892157,\n",
       "       72.37238562, 72.36584967, 72.35931373, 72.35277778, 72.34624183,\n",
       "       72.33970588, 72.33316993, 72.32663399, 72.32009804, 72.31356209,\n",
       "       72.30702614, 72.3004902 , 72.29395425, 72.2874183 , 72.28088235,\n",
       "       72.27434641, 72.26781046, 72.26127451, 72.25473856, 72.24820261,\n",
       "       72.24166667, 72.23513072, 72.22859477, 72.22205882, 72.21552288,\n",
       "       72.20898693, 72.20245098, 72.19591503, 72.18937908, 72.18284314,\n",
       "       72.17630719, 72.16977124, 72.16323529, 72.15669935, 72.1501634 ,\n",
       "       72.14362745, 72.1370915 , 72.13055556, 72.12401961, 72.11748366,\n",
       "       72.11094771, 72.10441176, 72.09787582, 72.09133987, 72.08480392,\n",
       "       72.07826797, 72.07173203, 72.06519608, 72.05866013, 72.05212418,\n",
       "       72.04558824, 72.03905229, 72.03251634, 72.02598039, 72.01944444,\n",
       "       72.0129085 , 72.00637255, 71.9998366 , 71.99330065, 71.98676471,\n",
       "       71.98022876, 71.97369281, 71.96715686, 71.96062092, 71.95408497,\n",
       "       71.94754902, 71.94101307, 71.93447712, 71.92794118, 71.92140523,\n",
       "       71.91486928, 71.90833333, 71.90179739, 71.89526144, 71.88872549,\n",
       "       71.88218954, 71.87565359, 71.86911765, 71.8625817 , 71.85604575,\n",
       "       71.8495098 , 71.84297386, 71.83643791, 71.82990196, 71.82336601,\n",
       "       71.81683007, 71.81029412, 71.80375817, 71.79960317, 71.79866947])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.302193637138473\n",
      "15.68519395358125\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
