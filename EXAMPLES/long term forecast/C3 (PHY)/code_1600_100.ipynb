{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "1695    71.709034\n",
       "1696    71.708100\n",
       "1697    71.707166\n",
       "1698    71.706232\n",
       "1699    71.705299\n",
       "Name: C3, Length: 1700, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c3_interpolated_1600_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "1595     0.178270\n",
       "1596     0.000000\n",
       "1597     0.000000\n",
       "1598     0.000000\n",
       "1599     0.000000\n",
       "Name: C3, Length: 1600, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1600)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.100000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.875910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.651821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90.427731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90.203641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     91.100000  0.000298\n",
       "1     90.875910  0.000298\n",
       "2     90.651821  0.000297\n",
       "3     90.427731  0.000297\n",
       "4     90.203641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.0403  # Value for C0\n",
    "K0 = -0.0012  # Value for K0\n",
    "K1 = -0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0120    # Value for b\n",
    "c = 2.0334    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    91.100000    90.875910    90.651821    90.427731    90.203641   \n",
      "351    90.875910    90.651821    90.427731    90.203641    89.979552   \n",
      "352    90.651821    90.427731    90.203641    89.979552    89.755462   \n",
      "353    90.427731    90.203641    89.979552    89.755462    89.531373   \n",
      "354    90.203641    89.979552    89.755462    89.531373    89.307283   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    89.979552    89.755462    89.531373    89.307283    89.094118  ...   \n",
      "351    89.755462    89.531373    89.307283    89.094118    89.015686  ...   \n",
      "352    89.531373    89.307283    89.094118    89.015686    88.937255  ...   \n",
      "353    89.307283    89.094118    89.015686    88.937255    88.858824  ...   \n",
      "354    89.094118    89.015686    88.937255    88.858824    88.780392  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   81.423343    0.000263   81.404669    0.000263   81.385994    0.000263   \n",
      "351   81.404669    0.000263   81.385994    0.000263   81.367320    0.000262   \n",
      "352   81.385994    0.000263   81.367320    0.000262   81.348646    0.000262   \n",
      "353   81.367320    0.000262   81.348646    0.000262   81.329972    0.000262   \n",
      "354   81.348646    0.000262   81.329972    0.000262   81.311298    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   81.367320    0.000262   81.348646    0.000262  \n",
      "351   81.348646    0.000262   81.329972    0.000262  \n",
      "352   81.329972    0.000262   81.311298    0.000262  \n",
      "353   81.311298    0.000262   81.292624    0.000262  \n",
      "354   81.292624    0.000262   81.273950    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1150, 1, 251) (1150, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "15/15 [==============================] - 3s 36ms/step - loss: 6004.8125 - val_loss: 5259.5103\n",
      "Epoch 2/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5930.2969 - val_loss: 5202.5596\n",
      "Epoch 3/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5876.5239 - val_loss: 5152.5542\n",
      "Epoch 4/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5822.1089 - val_loss: 5098.0625\n",
      "Epoch 5/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5764.4053 - val_loss: 5046.2007\n",
      "Epoch 6/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5709.3472 - val_loss: 4994.8379\n",
      "Epoch 7/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 5654.9150 - val_loss: 4944.1108\n",
      "Epoch 8/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5601.1118 - val_loss: 4893.9443\n",
      "Epoch 9/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5547.8574 - val_loss: 4844.2773\n",
      "Epoch 10/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5495.0996 - val_loss: 4795.0688\n",
      "Epoch 11/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5442.7993 - val_loss: 4746.2915\n",
      "Epoch 12/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5390.9336 - val_loss: 4697.9268\n",
      "Epoch 13/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5339.4854 - val_loss: 4649.9619\n",
      "Epoch 14/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5288.4414 - val_loss: 4602.3853\n",
      "Epoch 15/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5237.7930 - val_loss: 4555.1909\n",
      "Epoch 16/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5187.5317 - val_loss: 4508.3696\n",
      "Epoch 17/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5137.6494 - val_loss: 4461.9165\n",
      "Epoch 18/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5088.1426 - val_loss: 4415.8267\n",
      "Epoch 19/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5039.0063 - val_loss: 4370.0962\n",
      "Epoch 20/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4990.2349 - val_loss: 4324.7222\n",
      "Epoch 21/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4941.8267 - val_loss: 4279.6987\n",
      "Epoch 22/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4893.7759 - val_loss: 4235.0239\n",
      "Epoch 23/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4846.0811 - val_loss: 4190.6958\n",
      "Epoch 24/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4798.7383 - val_loss: 4146.7095\n",
      "Epoch 25/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4751.7446 - val_loss: 4103.0620\n",
      "Epoch 26/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4705.0977 - val_loss: 4059.7529\n",
      "Epoch 27/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4658.7944 - val_loss: 4016.7791\n",
      "Epoch 28/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4612.8325 - val_loss: 3974.1370\n",
      "Epoch 29/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4567.2100 - val_loss: 3931.8259\n",
      "Epoch 30/500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 4521.9238 - val_loss: 3889.8418\n",
      "Epoch 31/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4476.9727 - val_loss: 3848.1836\n",
      "Epoch 32/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4432.3525 - val_loss: 3806.8494\n",
      "Epoch 33/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4388.0635 - val_loss: 3765.8369\n",
      "Epoch 34/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4344.1030 - val_loss: 3725.1440\n",
      "Epoch 35/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4300.4683 - val_loss: 3684.7686\n",
      "Epoch 36/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4257.1577 - val_loss: 3644.7085\n",
      "Epoch 37/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4214.1694 - val_loss: 3604.9614\n",
      "Epoch 38/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4171.5005 - val_loss: 3565.5271\n",
      "Epoch 39/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4129.1504 - val_loss: 3526.4028\n",
      "Epoch 40/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4087.1162 - val_loss: 3487.5859\n",
      "Epoch 41/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4045.3972 - val_loss: 3449.0764\n",
      "Epoch 42/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4003.9905 - val_loss: 3410.8701\n",
      "Epoch 43/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3962.8945 - val_loss: 3372.9668\n",
      "Epoch 44/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3922.1077 - val_loss: 3335.3650\n",
      "Epoch 45/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3881.6282 - val_loss: 3298.0618\n",
      "Epoch 46/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3841.4543 - val_loss: 3261.0559\n",
      "Epoch 47/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3801.5845 - val_loss: 3224.3467\n",
      "Epoch 48/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3762.0164 - val_loss: 3187.9304\n",
      "Epoch 49/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3722.7493 - val_loss: 3151.8076\n",
      "Epoch 50/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3683.7810 - val_loss: 3115.9758\n",
      "Epoch 51/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3645.1099 - val_loss: 3080.4331\n",
      "Epoch 52/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3606.7339 - val_loss: 3045.1775\n",
      "Epoch 53/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3568.6523 - val_loss: 3010.2083\n",
      "Epoch 54/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3530.8623 - val_loss: 2975.5227\n",
      "Epoch 55/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3493.3633 - val_loss: 2941.1208\n",
      "Epoch 56/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3456.1533 - val_loss: 2906.9993\n",
      "Epoch 57/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3419.2310 - val_loss: 2873.1580\n",
      "Epoch 58/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3382.5945 - val_loss: 2839.5947\n",
      "Epoch 59/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3346.2422 - val_loss: 2806.3074\n",
      "Epoch 60/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3310.1729 - val_loss: 2773.2952\n",
      "Epoch 61/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3274.3845 - val_loss: 2740.5566\n",
      "Epoch 62/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3238.8760 - val_loss: 2708.0908\n",
      "Epoch 63/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3203.6460 - val_loss: 2675.8948\n",
      "Epoch 64/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3168.6921 - val_loss: 2643.9680\n",
      "Epoch 65/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3134.0144 - val_loss: 2612.3091\n",
      "Epoch 66/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3099.6104 - val_loss: 2580.9153\n",
      "Epoch 67/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3065.4783 - val_loss: 2549.7869\n",
      "Epoch 68/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3031.6169 - val_loss: 2518.9211\n",
      "Epoch 69/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2998.0254 - val_loss: 2488.3174\n",
      "Epoch 70/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2964.7014 - val_loss: 2457.9744\n",
      "Epoch 71/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2931.6438 - val_loss: 2427.8889\n",
      "Epoch 72/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 2898.8516 - val_loss: 2398.0620\n",
      "Epoch 73/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2866.3235 - val_loss: 2368.4912\n",
      "Epoch 74/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2834.0566 - val_loss: 2339.1746\n",
      "Epoch 75/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2802.0505 - val_loss: 2310.1116\n",
      "Epoch 76/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2770.3044 - val_loss: 2281.3000\n",
      "Epoch 77/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2738.8167 - val_loss: 2252.7393\n",
      "Epoch 78/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2707.5857 - val_loss: 2224.4270\n",
      "Epoch 79/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2673.9277 - val_loss: 2190.1643\n",
      "Epoch 80/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2636.3528 - val_loss: 2156.7791\n",
      "Epoch 81/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2600.1628 - val_loss: 2124.5442\n",
      "Epoch 82/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2565.2095 - val_loss: 2093.3528\n",
      "Epoch 83/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2531.2642 - val_loss: 2062.9724\n",
      "Epoch 84/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2498.1086 - val_loss: 2033.2472\n",
      "Epoch 85/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2465.6016 - val_loss: 2004.0773\n",
      "Epoch 86/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2433.6511 - val_loss: 1975.3987\n",
      "Epoch 87/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 2402.1978 - val_loss: 1947.1633\n",
      "Epoch 88/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2371.1963 - val_loss: 1919.3380\n",
      "Epoch 89/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2340.6152 - val_loss: 1891.8990\n",
      "Epoch 90/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2310.4285 - val_loss: 1864.8250\n",
      "Epoch 91/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2280.6174 - val_loss: 1838.0989\n",
      "Epoch 92/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2251.1643 - val_loss: 1811.7087\n",
      "Epoch 93/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2222.0569 - val_loss: 1785.6420\n",
      "Epoch 94/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2193.2832 - val_loss: 1759.8890\n",
      "Epoch 95/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2164.8335 - val_loss: 1734.4419\n",
      "Epoch 96/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2136.6982 - val_loss: 1709.2936\n",
      "Epoch 97/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2108.8718 - val_loss: 1684.4365\n",
      "Epoch 98/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2081.3457 - val_loss: 1659.8652\n",
      "Epoch 99/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 2054.1152 - val_loss: 1635.5745\n",
      "Epoch 100/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2027.1736 - val_loss: 1611.5580\n",
      "Epoch 101/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2000.5161 - val_loss: 1587.8135\n",
      "Epoch 102/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1974.1390 - val_loss: 1564.3353\n",
      "Epoch 103/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1948.0370 - val_loss: 1541.1201\n",
      "Epoch 104/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1922.2068 - val_loss: 1518.1642\n",
      "Epoch 105/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1896.6447 - val_loss: 1495.4647\n",
      "Epoch 106/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1871.3464 - val_loss: 1473.0172\n",
      "Epoch 107/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1846.3099 - val_loss: 1450.8190\n",
      "Epoch 108/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 1821.5309 - val_loss: 1428.8678\n",
      "Epoch 109/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1797.0070 - val_loss: 1407.1603\n",
      "Epoch 110/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1772.7351 - val_loss: 1385.6943\n",
      "Epoch 111/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1748.7126 - val_loss: 1364.4667\n",
      "Epoch 112/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1724.9371 - val_loss: 1343.4752\n",
      "Epoch 113/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1701.4055 - val_loss: 1322.7180\n",
      "Epoch 114/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1678.1166 - val_loss: 1302.1927\n",
      "Epoch 115/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1655.0673 - val_loss: 1281.8970\n",
      "Epoch 116/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1632.2552 - val_loss: 1261.8289\n",
      "Epoch 117/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1609.6790 - val_loss: 1241.9852\n",
      "Epoch 118/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1587.3352 - val_loss: 1222.3655\n",
      "Epoch 119/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1565.2230 - val_loss: 1202.9667\n",
      "Epoch 120/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1543.3400 - val_loss: 1183.7883\n",
      "Epoch 121/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1521.6841 - val_loss: 1164.8260\n",
      "Epoch 122/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1500.2534 - val_loss: 1146.0798\n",
      "Epoch 123/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1479.0465 - val_loss: 1127.5485\n",
      "Epoch 124/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1458.0610 - val_loss: 1109.2283\n",
      "Epoch 125/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1437.2953 - val_loss: 1091.1180\n",
      "Epoch 126/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1416.7477 - val_loss: 1073.2178\n",
      "Epoch 127/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1396.4166 - val_loss: 1055.5243\n",
      "Epoch 128/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1376.3005 - val_loss: 1038.0361\n",
      "Epoch 129/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1356.3971 - val_loss: 1020.7512\n",
      "Epoch 130/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1336.7056 - val_loss: 1003.6694\n",
      "Epoch 131/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1317.2238 - val_loss: 986.7875\n",
      "Epoch 132/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1297.9497 - val_loss: 970.1044\n",
      "Epoch 133/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1278.8831 - val_loss: 953.6196\n",
      "Epoch 134/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1260.0215 - val_loss: 937.3306\n",
      "Epoch 135/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1241.3634 - val_loss: 921.2362\n",
      "Epoch 136/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1222.9073 - val_loss: 905.3344\n",
      "Epoch 137/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1204.6527 - val_loss: 889.6255\n",
      "Epoch 138/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1186.5970 - val_loss: 874.1057\n",
      "Epoch 139/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 1168.7390 - val_loss: 858.7754\n",
      "Epoch 140/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1151.0778 - val_loss: 843.6320\n",
      "Epoch 141/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1133.6111 - val_loss: 828.6749\n",
      "Epoch 142/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1116.3381 - val_loss: 813.9017\n",
      "Epoch 143/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1099.2573 - val_loss: 799.3123\n",
      "Epoch 144/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1082.3673 - val_loss: 784.9037\n",
      "Epoch 145/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1065.6665 - val_loss: 770.6763\n",
      "Epoch 146/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1049.1539 - val_loss: 756.6275\n",
      "Epoch 147/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1032.8276 - val_loss: 742.7570\n",
      "Epoch 148/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1016.6871 - val_loss: 729.0621\n",
      "Epoch 149/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1000.7305 - val_loss: 715.5430\n",
      "Epoch 150/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 984.9567 - val_loss: 702.1973\n",
      "Epoch 151/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 969.3644 - val_loss: 689.0240\n",
      "Epoch 152/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 953.9523 - val_loss: 676.0225\n",
      "Epoch 153/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 938.7191 - val_loss: 663.1903\n",
      "Epoch 154/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 923.6633 - val_loss: 650.5267\n",
      "Epoch 155/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 908.7837 - val_loss: 638.0299\n",
      "Epoch 156/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 894.0793 - val_loss: 625.6996\n",
      "Epoch 157/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 879.5486 - val_loss: 613.5345\n",
      "Epoch 158/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 865.1907 - val_loss: 601.5324\n",
      "Epoch 159/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 851.0041 - val_loss: 589.6928\n",
      "Epoch 160/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 836.9874 - val_loss: 578.0137\n",
      "Epoch 161/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 823.1393 - val_loss: 566.4942\n",
      "Epoch 162/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 809.4587 - val_loss: 555.1342\n",
      "Epoch 163/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 795.9446 - val_loss: 543.9308\n",
      "Epoch 164/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 782.5956 - val_loss: 532.8838\n",
      "Epoch 165/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 769.4105 - val_loss: 521.9915\n",
      "Epoch 166/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 756.3887 - val_loss: 511.2525\n",
      "Epoch 167/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 743.5280 - val_loss: 500.6664\n",
      "Epoch 168/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 730.8275 - val_loss: 490.2318\n",
      "Epoch 169/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 718.2863 - val_loss: 479.9462\n",
      "Epoch 170/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 705.9031 - val_loss: 469.8098\n",
      "Epoch 171/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 693.6766 - val_loss: 459.8217\n",
      "Epoch 172/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 681.6059 - val_loss: 449.9796\n",
      "Epoch 173/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 669.6895 - val_loss: 440.2826\n",
      "Epoch 174/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 657.9263 - val_loss: 430.7292\n",
      "Epoch 175/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 646.3152 - val_loss: 421.3192\n",
      "Epoch 176/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 634.8552 - val_loss: 412.0507\n",
      "Epoch 177/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 623.5445 - val_loss: 402.9227\n",
      "Epoch 178/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 612.3826 - val_loss: 393.9334\n",
      "Epoch 179/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 601.3680 - val_loss: 385.0826\n",
      "Epoch 180/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 590.4998 - val_loss: 376.3686\n",
      "Epoch 181/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 579.7765 - val_loss: 367.7902\n",
      "Epoch 182/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 569.1972 - val_loss: 359.3468\n",
      "Epoch 183/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 558.7607 - val_loss: 351.0371\n",
      "Epoch 184/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 548.4663 - val_loss: 342.8592\n",
      "Epoch 185/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 538.3124 - val_loss: 334.8129\n",
      "Epoch 186/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 528.2979 - val_loss: 326.8965\n",
      "Epoch 187/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 518.4221 - val_loss: 319.1090\n",
      "Epoch 188/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 508.6830 - val_loss: 311.4492\n",
      "Epoch 189/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 499.0802 - val_loss: 303.9158\n",
      "Epoch 190/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 489.6123 - val_loss: 296.5072\n",
      "Epoch 191/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 480.2776 - val_loss: 289.2235\n",
      "Epoch 192/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 471.0759 - val_loss: 282.0628\n",
      "Epoch 193/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 462.0059 - val_loss: 275.0236\n",
      "Epoch 194/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 453.0663 - val_loss: 268.1058\n",
      "Epoch 195/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 444.2561 - val_loss: 261.3070\n",
      "Epoch 196/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 435.5742 - val_loss: 254.6272\n",
      "Epoch 197/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 427.0195 - val_loss: 248.0654\n",
      "Epoch 198/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 418.5907 - val_loss: 241.6190\n",
      "Epoch 199/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 410.2869 - val_loss: 235.2882\n",
      "Epoch 200/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 402.1070 - val_loss: 229.0713\n",
      "Epoch 201/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 394.0496 - val_loss: 222.9676\n",
      "Epoch 202/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 386.1140 - val_loss: 216.9755\n",
      "Epoch 203/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 378.2988 - val_loss: 211.0936\n",
      "Epoch 204/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 370.6033 - val_loss: 205.3218\n",
      "Epoch 205/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 363.0258 - val_loss: 199.6580\n",
      "Epoch 206/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 355.5659 - val_loss: 194.1022\n",
      "Epoch 207/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 348.2221 - val_loss: 188.6521\n",
      "Epoch 208/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 340.9933 - val_loss: 183.3067\n",
      "Epoch 209/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 333.8785 - val_loss: 178.0659\n",
      "Epoch 210/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 326.8765 - val_loss: 172.9278\n",
      "Epoch 211/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 319.9865 - val_loss: 167.8915\n",
      "Epoch 212/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 313.2072 - val_loss: 162.9557\n",
      "Epoch 213/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 306.5376 - val_loss: 158.1191\n",
      "Epoch 214/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 299.9765 - val_loss: 153.3815\n",
      "Epoch 215/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 293.5231 - val_loss: 148.7408\n",
      "Epoch 216/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 287.1758 - val_loss: 144.1966\n",
      "Epoch 217/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 280.9342 - val_loss: 139.7473\n",
      "Epoch 218/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 274.7968 - val_loss: 135.3925\n",
      "Epoch 219/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 268.7628 - val_loss: 131.1301\n",
      "Epoch 220/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 262.8309 - val_loss: 126.9599\n",
      "Epoch 221/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 256.9999 - val_loss: 122.8806\n",
      "Epoch 222/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 251.2692 - val_loss: 118.8906\n",
      "Epoch 223/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 245.6374 - val_loss: 114.9892\n",
      "Epoch 224/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 240.1033 - val_loss: 111.1755\n",
      "Epoch 225/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 234.6666 - val_loss: 107.4481\n",
      "Epoch 226/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 229.3256 - val_loss: 103.8063\n",
      "Epoch 227/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 224.0794 - val_loss: 100.2485\n",
      "Epoch 228/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 218.9270 - val_loss: 96.7743\n",
      "Epoch 229/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 213.8673 - val_loss: 93.3821\n",
      "Epoch 230/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 208.8992 - val_loss: 90.0707\n",
      "Epoch 231/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 204.0217 - val_loss: 86.8393\n",
      "Epoch 232/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 199.2339 - val_loss: 83.6871\n",
      "Epoch 233/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 194.5347 - val_loss: 80.6128\n",
      "Epoch 234/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 189.9230 - val_loss: 77.6153\n",
      "Epoch 235/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 185.3979 - val_loss: 74.6934\n",
      "Epoch 236/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 180.9582 - val_loss: 71.8463\n",
      "Epoch 237/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 176.6030 - val_loss: 69.0731\n",
      "Epoch 238/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 172.3313 - val_loss: 66.3722\n",
      "Epoch 239/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 168.1418 - val_loss: 63.7429\n",
      "Epoch 240/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 164.0337 - val_loss: 61.1844\n",
      "Epoch 241/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 160.0061 - val_loss: 58.6952\n",
      "Epoch 242/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 156.0580 - val_loss: 56.2745\n",
      "Epoch 243/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 152.1881 - val_loss: 53.9213\n",
      "Epoch 244/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 148.3958 - val_loss: 51.6345\n",
      "Epoch 245/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 144.6797 - val_loss: 49.4132\n",
      "Epoch 246/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 141.0390 - val_loss: 47.2564\n",
      "Epoch 247/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 137.4728 - val_loss: 45.1625\n",
      "Epoch 248/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 133.9800 - val_loss: 43.1313\n",
      "Epoch 249/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 130.5596 - val_loss: 41.1613\n",
      "Epoch 250/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 127.2106 - val_loss: 39.2518\n",
      "Epoch 251/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 123.9320 - val_loss: 37.4015\n",
      "Epoch 252/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 120.7230 - val_loss: 35.6099\n",
      "Epoch 253/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 117.5826 - val_loss: 33.8753\n",
      "Epoch 254/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 114.5095 - val_loss: 32.1971\n",
      "Epoch 255/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 111.5031 - val_loss: 30.5743\n",
      "Epoch 256/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 108.5623 - val_loss: 29.0060\n",
      "Epoch 257/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 105.6862 - val_loss: 27.4910\n",
      "Epoch 258/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 102.8737 - val_loss: 26.0288\n",
      "Epoch 259/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 100.1241 - val_loss: 24.6179\n",
      "Epoch 260/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 97.4362 - val_loss: 23.2574\n",
      "Epoch 261/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 94.8093 - val_loss: 21.9469\n",
      "Epoch 262/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 92.2425 - val_loss: 20.6849\n",
      "Epoch 263/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 89.7344 - val_loss: 19.4705\n",
      "Epoch 264/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 87.2846 - val_loss: 18.3029\n",
      "Epoch 265/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 84.8921 - val_loss: 17.1812\n",
      "Epoch 266/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 82.5555 - val_loss: 16.1045\n",
      "Epoch 267/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 80.2744 - val_loss: 15.0719\n",
      "Epoch 268/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 78.0480 - val_loss: 14.0823\n",
      "Epoch 269/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 75.8751 - val_loss: 13.1352\n",
      "Epoch 270/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 73.7551 - val_loss: 12.2291\n",
      "Epoch 271/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 71.6866 - val_loss: 11.3636\n",
      "Epoch 272/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 69.6692 - val_loss: 10.5375\n",
      "Epoch 273/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 67.7018 - val_loss: 9.7502\n",
      "Epoch 274/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 65.7837 - val_loss: 9.0007\n",
      "Epoch 275/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 63.9139 - val_loss: 8.2881\n",
      "Epoch 276/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 62.0914 - val_loss: 7.6114\n",
      "Epoch 277/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 60.3157 - val_loss: 6.9702\n",
      "Epoch 278/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 58.5857 - val_loss: 6.3633\n",
      "Epoch 279/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 56.9008 - val_loss: 5.7899\n",
      "Epoch 280/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 55.2599 - val_loss: 5.2493\n",
      "Epoch 281/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 53.6623 - val_loss: 4.7406\n",
      "Epoch 282/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 52.1072 - val_loss: 4.2630\n",
      "Epoch 283/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 50.5939 - val_loss: 3.8156\n",
      "Epoch 284/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 49.1213 - val_loss: 3.3978\n",
      "Epoch 285/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 47.6888 - val_loss: 3.0087\n",
      "Epoch 286/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 46.2957 - val_loss: 2.6475\n",
      "Epoch 287/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 44.9411 - val_loss: 2.3135\n",
      "Epoch 288/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 43.6244 - val_loss: 2.0059\n",
      "Epoch 289/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 42.3445 - val_loss: 1.7239\n",
      "Epoch 290/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 41.1008 - val_loss: 1.4669\n",
      "Epoch 291/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 39.8925 - val_loss: 1.2340\n",
      "Epoch 292/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 38.7191 - val_loss: 1.0246\n",
      "Epoch 293/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 37.5796 - val_loss: 0.8379\n",
      "Epoch 294/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 36.4735 - val_loss: 0.6732\n",
      "Epoch 295/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 35.3998 - val_loss: 0.5298\n",
      "Epoch 296/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 34.3580 - val_loss: 0.4071\n",
      "Epoch 297/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 33.3472 - val_loss: 0.3043\n",
      "Epoch 298/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 32.3670 - val_loss: 0.2208\n",
      "Epoch 299/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 31.4166 - val_loss: 0.1559\n",
      "Epoch 300/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 30.4951 - val_loss: 0.1090\n",
      "Epoch 301/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 29.6022 - val_loss: 0.0794\n",
      "Epoch 302/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 28.7369 - val_loss: 0.0666\n",
      "Epoch 303/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 27.8987 - val_loss: 0.0698\n",
      "Epoch 304/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 27.0871 - val_loss: 0.0884\n",
      "Epoch 305/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 26.3011 - val_loss: 0.1219\n",
      "Epoch 306/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.5404 - val_loss: 0.1697\n",
      "Epoch 307/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.8042 - val_loss: 0.2312\n",
      "Epoch 308/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.0921 - val_loss: 0.3058\n",
      "Epoch 309/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 23.4033 - val_loss: 0.3930\n",
      "Epoch 310/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 22.7373 - val_loss: 0.4921\n",
      "Epoch 311/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 22.0935 - val_loss: 0.6028\n",
      "Epoch 312/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 21.4714 - val_loss: 0.7243\n",
      "Epoch 313/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 20.8702 - val_loss: 0.8563\n",
      "Epoch 314/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 20.2895 - val_loss: 0.9982\n",
      "Epoch 315/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 19.7289 - val_loss: 1.1495\n",
      "Epoch 316/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 19.1875 - val_loss: 1.3097\n",
      "Epoch 317/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 18.6651 - val_loss: 1.4783\n",
      "Epoch 318/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.1611 - val_loss: 1.6549\n",
      "Epoch 319/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 17.6749 - val_loss: 1.8391\n",
      "Epoch 320/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 17.2060 - val_loss: 2.0303\n",
      "Epoch 321/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 16.7541 - val_loss: 2.2281\n",
      "Epoch 322/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 16.3185 - val_loss: 2.4322\n",
      "Epoch 323/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.8989 - val_loss: 2.6420\n",
      "Epoch 324/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 15.4947 - val_loss: 2.8572\n",
      "Epoch 325/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 15.1054 - val_loss: 3.0775\n",
      "Epoch 326/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 14.7307 - val_loss: 3.3023\n",
      "Epoch 327/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.3701 - val_loss: 3.5314\n",
      "Epoch 328/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.0231 - val_loss: 3.7642\n",
      "Epoch 329/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.6895 - val_loss: 4.0007\n",
      "Epoch 330/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.3686 - val_loss: 4.2403\n",
      "Epoch 331/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.0602 - val_loss: 4.4828\n",
      "Epoch 332/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 12.7638 - val_loss: 4.7278\n",
      "Epoch 333/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 12.4791 - val_loss: 4.9749\n",
      "Epoch 334/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 12.2056 - val_loss: 5.2240\n",
      "Epoch 335/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.9431 - val_loss: 5.4746\n",
      "Epoch 336/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.6911 - val_loss: 5.7267\n",
      "Epoch 337/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.4493 - val_loss: 5.9797\n",
      "Epoch 338/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.2173 - val_loss: 6.2336\n",
      "Epoch 339/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 10.9949 - val_loss: 6.4880\n",
      "Epoch 340/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 10.7816 - val_loss: 6.7427\n",
      "Epoch 341/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 10.5773 - val_loss: 6.9973\n",
      "Epoch 342/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 10.3816 - val_loss: 7.2517\n",
      "Epoch 343/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 10.1941 - val_loss: 7.5058\n",
      "Epoch 344/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 10.0145 - val_loss: 7.7593\n",
      "Epoch 345/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 9.8427 - val_loss: 8.0119\n",
      "Epoch 346/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 9.6782 - val_loss: 8.2636\n",
      "Epoch 347/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 9.5209 - val_loss: 8.5138\n",
      "Epoch 348/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 9.3705 - val_loss: 8.7629\n",
      "Epoch 349/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 9.2267 - val_loss: 9.0104\n",
      "Epoch 350/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 9.0893 - val_loss: 9.2561\n",
      "Epoch 351/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 8.9580 - val_loss: 9.5001\n",
      "Epoch 352/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 8.8325 - val_loss: 9.7420\n",
      "Epoch 353/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 8.7128 - val_loss: 9.9818\n",
      "Epoch 354/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 8.5984 - val_loss: 10.2192\n",
      "Epoch 355/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 8.4893 - val_loss: 10.4543\n",
      "Epoch 356/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 8.3853 - val_loss: 10.6869\n",
      "Epoch 357/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 8.2860 - val_loss: 10.9169\n",
      "Epoch 358/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 8.1913 - val_loss: 11.1442\n",
      "Epoch 359/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 8.1011 - val_loss: 11.3688\n",
      "Epoch 360/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 8.0152 - val_loss: 11.5904\n",
      "Epoch 361/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 7.9332 - val_loss: 11.8090\n",
      "Epoch 362/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 7.8553 - val_loss: 12.0245\n",
      "Epoch 363/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 7.7810 - val_loss: 12.2369\n",
      "Epoch 364/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 7.7104 - val_loss: 12.4463\n",
      "Epoch 365/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 7.6431 - val_loss: 12.6524\n",
      "Epoch 366/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 7.5791 - val_loss: 12.8553\n",
      "Epoch 367/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 7.5183 - val_loss: 13.0547\n",
      "Epoch 368/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 7.4604 - val_loss: 13.2507\n",
      "Epoch 369/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 7.4055 - val_loss: 13.4435\n",
      "Epoch 370/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 7.3532 - val_loss: 13.6327\n",
      "Epoch 371/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 7.3036 - val_loss: 13.8185\n",
      "Epoch 372/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 7.2564 - val_loss: 14.0007\n",
      "Epoch 373/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 7.2117 - val_loss: 14.1797\n",
      "Epoch 374/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 7.1692 - val_loss: 14.3549\n",
      "Epoch 375/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 7.1289 - val_loss: 14.5267\n",
      "Epoch 376/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 7.0906 - val_loss: 14.6949\n",
      "Epoch 377/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 7.0544 - val_loss: 14.8598\n",
      "Epoch 378/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 7.0199 - val_loss: 15.0210\n",
      "Epoch 379/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.9873 - val_loss: 15.1787\n",
      "Epoch 380/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.9564 - val_loss: 15.3329\n",
      "Epoch 381/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.9270 - val_loss: 15.4837\n",
      "Epoch 382/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.8993 - val_loss: 15.6310\n",
      "Epoch 383/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.8729 - val_loss: 15.7749\n",
      "Epoch 384/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.8480 - val_loss: 15.9151\n",
      "Epoch 385/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.8244 - val_loss: 16.0520\n",
      "Epoch 386/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.8021 - val_loss: 16.1857\n",
      "Epoch 387/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.7809 - val_loss: 16.3161\n",
      "Epoch 388/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 6.7609 - val_loss: 16.4428\n",
      "Epoch 389/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 6.7420 - val_loss: 16.5666\n",
      "Epoch 390/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.7240 - val_loss: 16.6872\n",
      "Epoch 391/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.7070 - val_loss: 16.8041\n",
      "Epoch 392/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.6910 - val_loss: 16.9183\n",
      "Epoch 393/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.6758 - val_loss: 17.0291\n",
      "Epoch 394/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.6615 - val_loss: 17.1369\n",
      "Epoch 395/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.6479 - val_loss: 17.2416\n",
      "Epoch 396/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.6351 - val_loss: 17.3435\n",
      "Epoch 397/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.6230 - val_loss: 17.4422\n",
      "Epoch 398/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.6115 - val_loss: 17.5382\n",
      "Epoch 399/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.6007 - val_loss: 17.6313\n",
      "Epoch 400/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.5904 - val_loss: 17.7214\n",
      "Epoch 401/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.5808 - val_loss: 17.8089\n",
      "Epoch 402/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.5717 - val_loss: 17.8940\n",
      "Epoch 403/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.5630 - val_loss: 17.9762\n",
      "Epoch 404/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.5549 - val_loss: 18.0558\n",
      "Epoch 405/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.5471 - val_loss: 18.1328\n",
      "Epoch 406/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 6.5399 - val_loss: 18.2074\n",
      "Epoch 407/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.5330 - val_loss: 18.2795\n",
      "Epoch 408/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.5265 - val_loss: 18.3494\n",
      "Epoch 409/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.5203 - val_loss: 18.4168\n",
      "Epoch 410/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.5145 - val_loss: 18.4819\n",
      "Epoch 411/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 6.5090 - val_loss: 18.5448\n",
      "Epoch 412/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.5039 - val_loss: 18.6056\n",
      "Epoch 413/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4990 - val_loss: 18.6642\n",
      "Epoch 414/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4944 - val_loss: 18.7208\n",
      "Epoch 415/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4900 - val_loss: 18.7754\n",
      "Epoch 416/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4859 - val_loss: 18.8280\n",
      "Epoch 417/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4820 - val_loss: 18.8786\n",
      "Epoch 418/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4783 - val_loss: 18.9275\n",
      "Epoch 419/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4749 - val_loss: 18.9744\n",
      "Epoch 420/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4716 - val_loss: 19.0197\n",
      "Epoch 421/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4685 - val_loss: 19.0632\n",
      "Epoch 422/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4656 - val_loss: 19.1048\n",
      "Epoch 423/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4628 - val_loss: 19.1450\n",
      "Epoch 424/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4603 - val_loss: 19.1838\n",
      "Epoch 425/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4578 - val_loss: 19.2210\n",
      "Epoch 426/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4555 - val_loss: 19.2565\n",
      "Epoch 427/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4533 - val_loss: 19.2909\n",
      "Epoch 428/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4512 - val_loss: 19.3236\n",
      "Epoch 429/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4493 - val_loss: 19.3551\n",
      "Epoch 430/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4474 - val_loss: 19.3852\n",
      "Epoch 431/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4457 - val_loss: 19.4142\n",
      "Epoch 432/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 6.4441 - val_loss: 19.4419\n",
      "Epoch 433/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4426 - val_loss: 19.4686\n",
      "Epoch 434/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4411 - val_loss: 19.4938\n",
      "Epoch 435/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4397 - val_loss: 19.5182\n",
      "Epoch 436/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4384 - val_loss: 19.5415\n",
      "Epoch 437/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4372 - val_loss: 19.5635\n",
      "Epoch 438/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4361 - val_loss: 19.5849\n",
      "Epoch 439/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4350 - val_loss: 19.6050\n",
      "Epoch 440/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4340 - val_loss: 19.6244\n",
      "Epoch 441/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4330 - val_loss: 19.6428\n",
      "Epoch 442/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4321 - val_loss: 19.6604\n",
      "Epoch 443/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4313 - val_loss: 19.6773\n",
      "Epoch 444/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4305 - val_loss: 19.6932\n",
      "Epoch 445/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4298 - val_loss: 19.7084\n",
      "Epoch 446/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4290 - val_loss: 19.7229\n",
      "Epoch 447/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4284 - val_loss: 19.7366\n",
      "Epoch 448/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4278 - val_loss: 19.7498\n",
      "Epoch 449/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4273 - val_loss: 19.7624\n",
      "Epoch 450/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4267 - val_loss: 19.7743\n",
      "Epoch 451/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4262 - val_loss: 19.7855\n",
      "Epoch 452/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 6.4258 - val_loss: 19.7962\n",
      "Epoch 453/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4253 - val_loss: 19.8064\n",
      "Epoch 454/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4249 - val_loss: 19.8160\n",
      "Epoch 455/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4245 - val_loss: 19.8251\n",
      "Epoch 456/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4242 - val_loss: 19.8339\n",
      "Epoch 457/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4239 - val_loss: 19.8420\n",
      "Epoch 458/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4236 - val_loss: 19.8499\n",
      "Epoch 459/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4233 - val_loss: 19.8572\n",
      "Epoch 460/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4231 - val_loss: 19.8640\n",
      "Epoch 461/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4229 - val_loss: 19.8706\n",
      "Epoch 462/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4226 - val_loss: 19.8769\n",
      "Epoch 463/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4225 - val_loss: 19.8826\n",
      "Epoch 464/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4223 - val_loss: 19.8882\n",
      "Epoch 465/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4222 - val_loss: 19.8934\n",
      "Epoch 466/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4220 - val_loss: 19.8985\n",
      "Epoch 467/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4219 - val_loss: 19.9030\n",
      "Epoch 468/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4218 - val_loss: 19.9074\n",
      "Epoch 469/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4217 - val_loss: 19.9115\n",
      "Epoch 470/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4216 - val_loss: 19.9152\n",
      "Epoch 471/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4216 - val_loss: 19.9189\n",
      "Epoch 472/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 6.4215 - val_loss: 19.9221\n",
      "Epoch 473/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4215 - val_loss: 19.9255\n",
      "Epoch 474/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4214 - val_loss: 19.9284\n",
      "Epoch 475/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4214 - val_loss: 19.9312\n",
      "Epoch 476/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4214 - val_loss: 19.9337\n",
      "Epoch 477/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4214 - val_loss: 19.9364\n",
      "Epoch 478/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4214 - val_loss: 19.9386\n",
      "Epoch 479/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4214 - val_loss: 19.9405\n",
      "Epoch 480/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4215 - val_loss: 19.9424\n",
      "Epoch 481/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4215 - val_loss: 19.9443\n",
      "Epoch 482/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4215 - val_loss: 19.9461\n",
      "Epoch 483/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4216 - val_loss: 19.9477\n",
      "Epoch 484/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4216 - val_loss: 19.9493\n",
      "Epoch 485/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4217 - val_loss: 19.9504\n",
      "Epoch 486/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4217 - val_loss: 19.9519\n",
      "Epoch 487/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4218 - val_loss: 19.9530\n",
      "Epoch 488/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4219 - val_loss: 19.9541\n",
      "Epoch 489/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4220 - val_loss: 19.9551\n",
      "Epoch 490/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 6.4220 - val_loss: 19.9561\n",
      "Epoch 491/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 6.4221 - val_loss: 19.9569\n",
      "Epoch 492/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4222 - val_loss: 19.9576\n",
      "Epoch 493/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 6.4223 - val_loss: 19.9584\n",
      "Epoch 494/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4224 - val_loss: 19.9591\n",
      "Epoch 495/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4225 - val_loss: 19.9597\n",
      "Epoch 496/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4226 - val_loss: 19.9603\n",
      "Epoch 497/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4227 - val_loss: 19.9606\n",
      "Epoch 498/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4228 - val_loss: 19.9610\n",
      "Epoch 499/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4229 - val_loss: 19.9614\n",
      "Epoch 500/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.4231 - val_loss: 19.9618\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.0403, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0012, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0120, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.0334, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 323ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[74.6317507 , 74.60542017, 74.57908964, 74.5579972 , 74.55071429,\n",
       "        74.54343137, 74.53614846, 74.52886555, 74.52158263, 74.51429972,\n",
       "        74.50701681, 74.49973389, 74.49245098, 74.48516807, 74.47788515,\n",
       "        74.47060224, 74.46331933, 74.45603641, 74.4487535 , 74.44147059,\n",
       "        74.43418768, 74.42690476, 74.41962185, 74.41233894, 74.40505602,\n",
       "        74.39777311, 74.3904902 , 74.38320728, 74.37592437, 74.36864146,\n",
       "        74.36135854, 74.35407563, 74.34679272, 74.3395098 , 74.33222689,\n",
       "        74.32494398, 74.31766106, 74.31037815, 74.30309524, 74.29194678,\n",
       "        74.27794118, 74.26393557, 74.24992997, 74.23592437, 74.22191877,\n",
       "        74.20791317, 74.19390756, 74.17990196, 74.16589636, 74.15189076,\n",
       "        74.13788515, 74.12387955, 74.10987395, 74.09586835, 74.08186275,\n",
       "        74.06785714, 74.05385154, 74.03984594, 74.02584034, 74.01183473,\n",
       "        73.99782913, 73.98382353, 73.96981793, 73.95581232, 73.94180672,\n",
       "        73.92780112, 73.91379552, 73.89978992, 73.88578431, 73.87177871,\n",
       "        73.85777311, 73.84376751, 73.8297619 , 73.8157563 , 73.8017507 ,\n",
       "        73.76813725, 73.73172269, 73.69530812, 73.65889356, 73.62247899,\n",
       "        78.65814972,  0.        ,  0.85138339,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.82520831,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([71.79773576, 71.79680205, 71.79586835, 71.79493464, 71.79400093,\n",
       "       71.79306723, 71.79213352, 71.79119981, 71.79026611, 71.7893324 ,\n",
       "       71.78839869, 71.78746499, 71.78653128, 71.78559757, 71.78466387,\n",
       "       71.78373016, 71.78279645, 71.78186275, 71.78092904, 71.77999533,\n",
       "       71.77906162, 71.77812792, 71.77719421, 71.7762605 , 71.7753268 ,\n",
       "       71.77439309, 71.77345938, 71.77252568, 71.77159197, 71.77065826,\n",
       "       71.76972456, 71.76879085, 71.76785714, 71.76692344, 71.76598973,\n",
       "       71.76505602, 71.76412232, 71.76318861, 71.7622549 , 71.7613212 ,\n",
       "       71.76038749, 71.75945378, 71.75852007, 71.75758637, 71.75665266,\n",
       "       71.75571895, 71.75478525, 71.75385154, 71.75291783, 71.75198413,\n",
       "       71.75105042, 71.75011671, 71.74918301, 71.7482493 , 71.74731559,\n",
       "       71.74638189, 71.74544818, 71.74451447, 71.74358077, 71.74264706,\n",
       "       71.74171335, 71.74077965, 71.73984594, 71.73891223, 71.73797852,\n",
       "       71.73704482, 71.73611111, 71.7351774 , 71.7342437 , 71.73330999,\n",
       "       71.73237628, 71.73144258, 71.73050887, 71.72957516, 71.72864146,\n",
       "       71.72770775, 71.72677404, 71.72584034, 71.72490663, 71.72397292,\n",
       "       71.72303922, 71.72210551, 71.7211718 , 71.7202381 , 71.71930439,\n",
       "       71.71837068, 71.71743697, 71.71650327, 71.71556956, 71.71463585,\n",
       "       71.71370215, 71.71276844, 71.71183473, 71.71090103, 71.70996732,\n",
       "       71.70903361, 71.70809991, 71.7071662 , 71.70623249, 71.70529879])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.30672419328527\n",
      "15.644295318748835\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
