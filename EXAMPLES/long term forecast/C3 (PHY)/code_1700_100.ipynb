{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "1795    71.278315\n",
       "1796    71.273646\n",
       "1797    71.268978\n",
       "1798    71.264309\n",
       "1799    71.259641\n",
       "Name: C3, Length: 1800, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c3_interpolated_1700_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "1695     0.000000\n",
       "1696     0.000000\n",
       "1697     0.000000\n",
       "1698     0.000000\n",
       "1699     0.000000\n",
       "Name: C3, Length: 1700, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1700)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.100000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.875910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.651821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90.427731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90.203641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     91.100000  0.000298\n",
       "1     90.875910  0.000298\n",
       "2     90.651821  0.000297\n",
       "3     90.427731  0.000297\n",
       "4     90.203641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAAsTAAALEwEAmpwYAAAcOUlEQVR4nO3de5BcZ3nn8e8z3TM999GMNCONpZFGvkkIe43kwfEG4gWcJbLDIoe4CAlLvCyJa2thC5JNEWep3VCprVrYJJCkSKAczGJSDjYYU3YIG1vYAhIqeDOyZMvWxZKFrtZcNNJcNPfLu3/0262eUWsu3af7nG79PlWq7j7T3fPMqdHvvPOct99jzjlERKS8VIRdgIiIBE/hLiJShhTuIiJlSOEuIlKGFO4iImUoXsxvtmbNGtfZ2VnMbykiUvL27t173jnXupLXFDXcOzs76e7uLua3FBEpeWZ2cqWvUVtGRKQMKdxFRMqQwl1EpAwp3EVEypDCXUSkDCncRUTKkMJdRKQMlUS4f++VN3nsxRVP8xQRuWaVRLh//8A5/vS515mamQu7FBGRklAS4f6rOzZwYXSKHx7pC7sUEZGSUBLhftfNraypr+I7L50JuxQRkZJQEuFeGatg19vW88LhPi6OToVdjohI5JVEuAPcf/sGpmcd//3pV9V7FxFZQsmE+1vaG/n0zi1875Vz/PY3uhmbmgm7JBGRyCqZcAf4z++6kf/1gVv5x6P9/PuvvsjgmFo0IiLZlFS4A/z6HRv5qw/v4NWzw7z3iz/mL54/Sv/IZNhliYhESsmFO8DOW9r55oN3srW9kS/sfp13fO4FfueJ/ew/PRh2aSIikWDOuaJ9s66uLhf0lZje6L/E3/zzSZ7ce4ZLkzPc1rGKD//cRm5qq6elroqWuirqE3HMLNDvKyJSLGa21znXtaLXlHq4p4xMTPPUS2d59J9PcLx/dN7XKmNGc20y6G9e28C7t7byb25uo6WuqiC1iIgE6ZoO95S5OcfBc8P0j0wyMDrFhdFJLoxOc2F0koFLU7x8ZpDzl6Ywg7d1rOI9W9p499Y23npdo0b3IhJJCvdlmJtzHDg7xJ4jfew50s8rZwZxDtoaErxrSyt3Xr+ark0tdLTUKOxFJBIU7jk4f2mSHx3pZ8+RPn78ej/DE8n5860NCW7f2ExXZzM7NjVzy3VNVMVL8vyziJQ4hXueZuccR/tG6D5xkb0nk/9OXRgDoCpewW0bmrh9Uwu3b2rm9k3N6tmLSFEo3Augb3giHfTdJy/y2ptDTM8m99n1a+rY2t5AR3MtG1pq2dhSS0dzDeuba0jEYyFXLiLlIpdwjxeqmHLR1ljNPbe2c8+t7QBMTM/yypkhuk9e4KWTFzl8boQfHOxjavbyejdmsK6xmo7mWjpaauloqUkGf0stHc21tDUkqKhQP19ECkfhvkLVlTHu2NzCHZtb0tvm5hy9IxOcvjDOqQtjnL4wxumLydufHDtPz/DEvPeoilewobmGjmY/2vfhv6E5Gf6NNZqXLyL5UbgHoKLCaG+qob2pZl7op0xMz3J2cDwZ+hfGOH0xef/UhTH2nbqYPombUl1ZQWtDgraGatoaEv5+8nFrYyK9bXVdgpj+AhCRLBTuRVBdGeOG1npuaK3P+vWhsen0SP/MxXH6RiboG5mkb3iSo32X+Mmx81ccAABiFcbquiraGn3w1yf8/QSda+q4rWMVjdWVhf7xRCSCFO4R0FRbSVNtE7esb7rqcyamZ+kfmaRvZMLfJsM/9bh3eIIDZ4cYuDTJnD9HbgY3tdWzvaOZHZtWsX1jMze21qvfL3INWFa4m9nvAL8FOOAA8FGgHXgcWA3sBT7inNMavAVSXRnzJ2drF33e7Jxj4NIkR3pH2HdqkJdOXeQfXuvhie7TADQk4rxtYzLot29cxfaOVayq1ZROkXKz5FRIM1sP/BOwzTk3bmbfAr4P3As85Zx73My+ArzsnPvyYu9VilMhy4FzjuPnR9Nhv+/UIEd6htMj/Otb69jhw37HxmZuXtugXr5IhBRyKmQcqDGzaaAWOAe8B/gN//VHgc8Ci4a7hMPM0j3/+2/fAMClyRleOTPIvlOD7Dt1kRcO9/Hk3uQFyOuqYtzWsSod9je01tNQHaehulKf0hUpEUuGu3PurJn9CXAKGAeeI9mGGXTOpc7ynQHWZ3u9mT0IPAiwcePGIGqWANQn4vz8DWv4+RvWAMnRfXL2zuXR/Vd+dJzZufl/2VVXVtBQXUmjD/vGmkoaquM0+m2ZjxuyPK6riqvnL1IES4a7mTUDu4DNwCDwbWDncr+Bc+5h4GFItmVyqlIKzszYtLqOTavruG978jg9PjXLgbNDnB0cY3h8hpGJaYYn/O34DMMT0wyNT3Pm4uWvTy5x8XKzZN9/qQNDc10VLXWVrKqtoqW2iua6KhqrNf9fCmtqZo7ZOUdNVel/wnw5bZlfBH7mnOsHMLOngHcAq8ws7kfvG4CzhStTwlBTFfPz9q+cu381kzOzjEzMMDw+nbydmJ73OHWAGPYHiJGJac4OjnN4Yjr5nMkZrnYaKFZhNNdmBn4lzT7419QnWNdYzbqmatqbqmltSFAZUwtJVmbnn/2Y4+dHOfG5X87rfVKrzd7WsSqYwnKwnHA/BdxpZrUk2zJ3A93AHuB+kjNmHgCeLlSRUjoS8RiJ+hhr6hM5vX5uznFpaobB0WkujE1xcXSKi2NTXBidYnBs/rYT58fYNzbIxbGp9Ho/KWbQWp9gXVN1OvQz77c31bCusbosRmgSnOPnR5d+0jK8/0s/Acj7IJGP5fTcXzSzJ4GXgBlgH8k2y98Dj5vZ//TbHilkoXJtqKgw36apZOPqxad9pjjnGBqf5tzQBD3DE/QMZfwbnuDkwBg/PT6Q9YNgTTWV6cBf31zDlrUNbFnXwNZ1DZoiKiVtWbNlnHN/CPzhgs3HgTsCr0hkhcyMVbVVrKqt4i3tjVd93tjUTDrwexYeCIYn2H96kL8dP5V+/trGBFvWNbJ1XUM69G9sq6e6UqN9iT59QlWuGbVVca5vref6qywD4Zyjd3iSwz3DHOkZ4UjPCId7Rvj6GwPpVT9jFUbn6lq2rmtky7rLo/yO5lrNApJIUbiLeGaW7s2/a0tbevvM7BwnBkY5nBH4B84O8fcHzqWfU1sV46a1DWz1I/wb2upZ25igtT5Bc22Vgl+KTuEusoR4rIIb2xq4sa2B9/2ry9tHJ2d4vfdy4B/pGWH3od70Ug8psQpjTX1VcnG3hkR6gbd59+uTX9MJXgmKwl0kR3WJuF+jpzm9zTlH/6VJTpwfo39kkv6RCfovJRd567+UXODt1bNDnM9Y4C1TQyJOa0OCNRlLPV8+CCRX/mxtSNBSV6UlIhbxpReOMjsH79naxluva7wm/3JSuIsEyMz8OvzViz5vds5xYXRq3kqfmQeB/pFJDr05zI9GJrk0ufRyz22pNf8bq+fdXovz/efmHH/y3OsAfPEHr9PakOBdN7fy7q1tvPOmNdfMMtgKd5EQxCosOSJvSLCNq8/wgeQsn/MjU/MOAv0ZSz73DE3wypkhBkYns34ArKWual7gN1ZXUl1ZQXVljEQ8eXv5cSzL1/y2eCy9Pcoj4dTJ7wfvup6t6xrYc6SfZ1/r4dt7zxCvMLo6m3l7Z0v6cw9r/b/VCy54Pzvn+L1vv0yswuhormVtY4K1TdWsbUiel2murYz0J6YV7iIRV1sVZ+Pq+JLz/mdm5xgYnUqHfq+/Ta393z8ywdHeEUYmZpiYnmUmW19omapiFSQqK+YFf31qKYma5HISTTWp+5U01lz+WlPN5bWJCrEQXWoJjLaGBB/YsYEP7NjAzOwc+04PsudwHy8c7uNLe45dcSCMLzhgDY5N8d19V//gfVWsgrbGBJ2r69h2XSPb2ht5S3sjN7TWBf4z5ULhLlIm4rGK9CgUrn7hl5SZ2TkmZ+aYmJ5lwt9OTs8xMTN7+f70LBMzmff9rX88OTPH5PQs49OzXJqcYXB8mlMXxhgeT647tNQBpKYylg7+1oYEm1bX0bm6ls41dXSurmNjS+2KTzJP+XBPZBw44rEK3t7Zwts7W/j0zq1Mz85x/tIkPUPJg2Dv8AS9wxP81Q/fuOL9/mjXW/m1t3ekL4rTO+xf5/9qeqP/El//yYn0XwxRWTlV4S5yjYrHKojHKqhLFCYGnHNMTM+lF5gbHp9Oryk07NcSSm6fYWh8mt6RCZ59rYcLo/Ov+dPeVM2m1bV0+oXtNq+p9Yvc1VJbdWXtywnZylhF+rrHmc5cHOfA2aErnp+Ix9jQnLyIfTbTs3Mc7x/l4LkhDr45zF//48+W3D+FpnAXkYIwM2qqYtRUxfxfE8szND7NqYExfjYwysnzo5wYGOPEwCg/ONTL+Uvzgz91veDO1bV+1F9HZSzZXsl3BL2SplVlrCL9obZf2Q4Hzw0zOb34CqmFpnAXkUhpqqnk1g1N3LrhytbSyMQ0J33YnxwY48T55O2eI/30j5yZ99yagJaJiO4p08Up3EWkZDRUV3LL+uwXkx+dnOHkwBgnB0Y5PzrFXTe35vQ9lrr0aKlQuItIWahLxJOzVq5bfGrpYoKc2Rj2ISIap3VFRCImnwG8RaCZo3AXEVlMhD+otBiFu4hIhqDaKWH37hXuIiJe5hjd5RHzURjsK9xFRBYRgZzOicJdRCRDmcyEVLiLiKQEucpj2McIhbuISDZhp3OeFO4iIouIwsnRXCjcRUQy5DNLJkoU7iIi3vypkPkJ+8Sswl1EZBG5LCUQhcvvKdxFRMqQwl1EJENQ7ZSwO/cKdxGRlIxuSn6rQoZP4S4isogItM9zonAXEckQ9iyXoCjcRUS8zJkxec9315K/IiLRlUtXJgqtHIW7iEgZUriLiJQhhbuIiGcBTYWEEpnnbmarzOxJMztsZofM7F+bWYuZ7Tazo/62udDFiogUWy798wi03Jc9cv9z4B+cc1uB24BDwEPA8865m4Dn/WMREYmAJcPdzJqAu4BHAJxzU865QWAX8Kh/2qPAfYUpUUSkeJzvx4TdVsnXckbum4F+4P+Y2T4z+6qZ1QFrnXPn/HN6gLXZXmxmD5pZt5l19/f3B1O1iEgBZGun5LIqJIQ+zX1Z4R4HdgBfds5tB0ZZ0IJxyUNd1h/FOfewc67LOdfV2tqab70iIpFXKkv+ngHOOOde9I+fJBn2vWbWDuBv+wpTooiIrNSS4e6c6wFOm9kWv+lu4CDwDPCA3/YA8HRBKhQRKaJUC8Ll2VcJ+3J98WU+778Aj5lZFXAc+CjJA8O3zOxjwEngg4UpUUSkOLJ2U0p0KuSywt05tx/oyvKluwOtRkREAqFPqIqIZEh1Y8Ke7ZIvhbuIiJdt2mOuLZawDw4KdxGRgEVgJqTCXUSkHCncRUQyhD2FMSgKdxERL1s7JddPm6rnLiJSdsJvuivcRUSyCHvknS+Fu4hIhoWhnvNUyLwryY/CXUTEC2oKo6ZCiohIQSjcRUQypFeFDL2xkh+Fu4hIWpblB3JsseS7ZHC+FO4iIgGLQMtd4S4iko2mQoqIlJErpkJGYRieA4W7iIhXqkGejcJdRCRgUThIKNxFRLIo8Za7wl1EZL75sZ7t6kylQOEuIuIFGeNhz7ZRuIuIZJHPh5CiMNpXuIuILCIKJ0dzoXAXEckQVDsl7LVpFO4iIp6W/BURKXOaCikiUkZKPdRTFO4iIl6Qs1w0FVJEJILyCWf13EVEIs6ikNQ5ULiLiGQI+wpKQVG4i4h4QQ7Swz5EKNxFRLLS8gMiImUr/JjOzbLD3cxiZrbPzL7nH282sxfN7JiZPWFmVYUrU0SkOIJqp4Tdu1/JyP2TwKGMx58HvuicuxG4CHwsyMJERIotc5SeVzZHYLi/rHA3sw3ALwNf9Y8NeA/wpH/Ko8B9BahPRCRUJToTctkj9z8DPg3M+cergUHn3Ix/fAZYn+2FZvagmXWbWXd/f38+tYqIFFyZzIRcOtzN7H1An3Nuby7fwDn3sHOuyznX1dramstbiIgURZAfWAr7GBFfxnPeAbzfzO4FqoFG4M+BVWYW96P3DcDZwpUpIlJcJd5yX3rk7pz7A+fcBudcJ/Ah4AXn3IeBPcD9/mkPAE8XrEoRkZBEYc56LvKZ5/77wO+a2TGSPfhHgilJRCQ8YU9hDMpy2jJpzrkfAj/0948DdwRfkohI+PLOeC35KyISXbmcY43CSpIKdxGRMqRwFxHJENjyAwG9T64U7iIiXmY3xYUez/lRuIuILCKX7nn4HXeFu4jIfKU9YE9TuIuIZJHvVMiw58sr3EVEvGyfRs1tKmQAxeRJ4S4iUoYU7iIiGcqk5a5wFxFJmTcVMt+ee34vz5vCXURkUStvoEeg5a5wFxEpRwp3EZEMYU9hDIrCXUTEy2yn5Lv8QNjHCIW7iMgitOSviIhEhsJdRCRDqpuS/1RILT8gIhIJ2bopWhVSREQiQ+EuIpIh7FkuQVG4i4h4Qc5yCfsgoXAXEVlEToEfgaa7wl1EpAwp3EVEMqSmMIbdVsmXwl1ExMvWTcm1wxL2wUHhLiISsGyX6ys2hbuISBlSuIuIZEi1U8JePiBfCncRkZRsyw/ktCpk/qXkS+EuIlKGFO4iIhmCWhUybAp3EZFF5NpiCftyfQp3EREvqCmMEWi5Lx3uZtZhZnvM7KCZvWZmn/TbW8xst5kd9bfNhS9XRESWYzkj9xngvzrntgF3Ah83s23AQ8DzzrmbgOf9YxGR0ubm3ZSsJcPdOXfOOfeSvz8CHALWA7uAR/3THgXuK1CNIiJFkf1KTLk1WcI+OKyo525mncB24EVgrXPunP9SD7A22NJEREpTSc1zN7N64DvAp5xzw5lfc8nTwlkPVGb2oJl1m1l3f39/XsWKiBRL2LNd8rWscDezSpLB/phz7im/udfM2v3X24G+bK91zj3snOtyznW1trYGUbOISMFcsexABEbhuVjObBkDHgEOOee+kPGlZ4AH/P0HgKeDL09EpHiCzPGwB/7xZTznHcBHgANmtt9v+2/A54BvmdnHgJPABwtSoYhIiYnCkr9Lhrtz7p+4+gHt7mDLERGJhtLuuOsTqiIi8yxsp+R8JaaQDw8KdxERL6gpjCU1FVJE5FoS9gnRfCncRUQyLMx0i8IwPAcKdxERL8hZLmGP/BXuIiIBi8JgX+EuIpJVaTfdFe4iIhkWrikTgUF4ThTuIiJekO2UsMf9CncRkSzyOyEa/nhf4S4isogonBzNhcJdRCRDUO0UTYUUEYmIoAbpURjtK9xFRLII+4RovhTuIiIZrlwVMgLD8Bwo3EVEUgLtp2jJXxGRyMnnhGgUxvoKdxGRRUTh5GguFO4iImVI4S4i4gXacdc8dxGR6Fm4gNhKRKGVo3AXEVlEBHI6Jwp3EZEF8hm1R4XCXUTEy2yn5BvvYR8eFO4iIovJoS8ThU+1KtxFRMqQwl1EZIEgWu5h9+0V7iIiXmY7Ja/lB8LvyijcRUQWE4X+eS4U7iIiC4Q90yUICncREW/+VMj8Ij7sA4TCXURkEbn0z6PQyFG4i4iUIYW7iMgCYU9jDILCXUTES7VTxqdn826ah318yCvczWynmR0xs2Nm9lBQRYmIhOG1N4cBuPWzz/EbX30RyK1//ubQBEPj04xNzfCF3a8zPjUbYJXLE8/1hWYWA/4S+LfAGeBfzOwZ59zBoIoTESmm4YnpQN5n98FeAH7/Owf4u5ffJBGv4OPvvjGQ916ufEbudwDHnHPHnXNTwOPArmDKEhEpvpa6qkDf7+9efhOAP372CKcGxgJ976XkE+7rgdMZj8/4bfOY2YNm1m1m3f39/Xl8OxGRwvqjXbek729d18CvdXWw7brGFb/PN3/7TgDuuWUdADvfuo6qeHFPcVquZ4XN7H5gp3Put/zjjwA/55z7xNVe09XV5bq7u3P6fiIi1yoz2+uc61rJa/I5lJwFOjIeb/DbREQkZPmE+78AN5nZZjOrAj4EPBNMWSIiko+cZ8s452bM7BPAs0AM+Jpz7rXAKhMRkZzlHO4AzrnvA98PqBYREQmIPqEqIlKGFO4iImVI4S4iUoYU7iIiZSjnDzHl9M3M+oGTOb58DXA+wHKKQTUXXqnVC6q5WMqp5k3OudaVvFFRwz0fZta90k9ohU01F16p1QuquViu9ZrVlhERKUMKdxGRMlRK4f5w2AXkQDUXXqnVC6q5WK7pmkum5y4iIstXSiN3ERFZJoW7iEgZKolwj+KFuM2sw8z2mNlBM3vNzD7pt3/WzM6a2X7/796M1/yB/xmOmNkvhVT3CTM74Gvr9ttazGy3mR31t81+u5nZX/iaXzGzHSHUuyVjX+43s2Ez+1TU9rOZfc3M+szs1YxtK96vZvaAf/5RM3ugyPX+sZkd9jV918xW+e2dZjaesa+/kvGa2/3v0zH/M+VyPel8al7x70Ex8+QqNT+RUe8JM9vvtwe7n51zkf5HcjnhN4DrgSrgZWBbBOpqB3b4+w3A68A24LPA72V5/jZfewLY7H+mWAh1nwDWLNj2v4GH/P2HgM/7+/cC/5fkBeDvBF6MwO9CD7ApavsZuAvYAbya634FWoDj/rbZ328uYr3vBeL+/ucz6u3MfN6C9/l//mcw/zPdU+R9vKLfg2LnSbaaF3z9T4H/UYj9XAoj90heiNs5d84595K/PwIcIss1ZDPsAh53zk06534GHCP5s0XBLuBRf/9R4L6M7d9wST8FVplZewj1pdwNvOGcW+xTzqHsZ+fcj4ELWWpZyX79JWC3c+6Cc+4isBvYWax6nXPPOedm/MOfkry62lX5mhudcz91yQT6Bpd/xsBdZR9fzdV+D4qaJ4vV7EffHwS+udh75LqfSyHcl3Uh7jCZWSewHXjRb/qE/9P2a6k/xYnOz+GA58xsr5k96Letdc6d8/d7gLX+flRqTvkQ8/8jRHk/w8r3a5Rq/48kR4gpm81sn5n9yMx+wW9bT7LGlLDqXcnvQZT28S8Avc65oxnbAtvPpRDukWZm9cB3gE8554aBLwM3AG8DzpH8sytK3umc2wHcA3zczO7K/KIfGURufqwlL+X4fuDbflPU9/M8Ud2v2ZjZZ4AZ4DG/6Ryw0Tm3Hfhd4G/NrDGs+hYoqd+DBX6d+YOVQPdzKYR7ZC/EbWaVJIP9MefcUwDOuV7n3Kxzbg74ay63BCLxczjnzvrbPuC7JOvrTbVb/G2ff3okavbuAV5yzvVC9Pezt9L9GnrtZvYfgPcBH/YHJHxrY8Df30uyZ32zry2zdVP0enP4PQh9HwOYWRz4APBEalvQ+7kUwj2SF+L2/bJHgEPOuS9kbM/sSf8KkDpL/gzwITNLmNlm4CaSJ0mKxszqzKwhdZ/kCbRXfW2pmRkPAE9n1PybfnbHncBQRpuh2OaNcqK8nzOsdL8+C7zXzJp9e+G9fltRmNlO4NPA+51zYxnbW80s5u9fT3KfHvc1D5vZnf7/w29m/IzFqnmlvwdRyZNfBA4759LtlsD3c6HOEgf5j+TsgtdJHsk+E3Y9vqZ3kvwz+xVgv/93L/A3wAG//RmgPeM1n/E/wxEKOKtgkZqvJzk74GXgtdS+BFYDzwNHgR8ALX67AX/paz4AdIW0r+uAAaApY1uk9jPJA885YJpkT/RjuexXkr3uY/7fR4tc7zGS/ejU7/NX/HN/1f++7AdeAv5dxvt0kQzUN4Av4T/1XsSaV/x7UMw8yVaz3/514D8teG6g+1nLD4iIlKFSaMuIiMgKKdxFRMqQwl1EpAwp3EVEypDCXUSkDCncRUTKkMJdRKQM/X8vR186jDUoZgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.0403  # Value for C0\n",
    "K0 = -0.0012  # Value for K0\n",
    "K1 = -0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0120    # Value for b\n",
    "c = 2.0334    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    91.100000    90.875910    90.651821    90.427731    90.203641   \n",
      "351    90.875910    90.651821    90.427731    90.203641    89.979552   \n",
      "352    90.651821    90.427731    90.203641    89.979552    89.755462   \n",
      "353    90.427731    90.203641    89.979552    89.755462    89.531373   \n",
      "354    90.203641    89.979552    89.755462    89.531373    89.307283   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    89.979552    89.755462    89.531373    89.307283    89.094118  ...   \n",
      "351    89.755462    89.531373    89.307283    89.094118    89.015686  ...   \n",
      "352    89.531373    89.307283    89.094118    89.015686    88.937255  ...   \n",
      "353    89.307283    89.094118    89.015686    88.937255    88.858824  ...   \n",
      "354    89.094118    89.015686    88.937255    88.858824    88.780392  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   81.423343    0.000263   81.404669    0.000263   81.385994    0.000263   \n",
      "351   81.404669    0.000263   81.385994    0.000263   81.367320    0.000262   \n",
      "352   81.385994    0.000263   81.367320    0.000262   81.348646    0.000262   \n",
      "353   81.367320    0.000262   81.348646    0.000262   81.329972    0.000262   \n",
      "354   81.348646    0.000262   81.329972    0.000262   81.311298    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   81.367320    0.000262   81.348646    0.000262  \n",
      "351   81.348646    0.000262   81.329972    0.000262  \n",
      "352   81.329972    0.000262   81.311298    0.000262  \n",
      "353   81.311298    0.000262   81.292624    0.000262  \n",
      "354   81.292624    0.000262   81.273950    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1250, 1, 251) (1250, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "16/16 [==============================] - 2s 31ms/step - loss: 5932.7285 - val_loss: 5159.1216\n",
      "Epoch 2/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5862.8271 - val_loss: 5111.4775\n",
      "Epoch 3/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5812.0068 - val_loss: 5064.0122\n",
      "Epoch 4/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5761.4697 - val_loss: 5016.8843\n",
      "Epoch 5/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5711.2886 - val_loss: 4970.1060\n",
      "Epoch 6/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5661.4590 - val_loss: 4923.6670\n",
      "Epoch 7/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5611.9761 - val_loss: 4877.5605\n",
      "Epoch 8/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5562.8281 - val_loss: 4831.7788\n",
      "Epoch 9/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5514.0127 - val_loss: 4786.3193\n",
      "Epoch 10/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5465.5244 - val_loss: 4741.1787\n",
      "Epoch 11/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5417.3589 - val_loss: 4696.3521\n",
      "Epoch 12/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5369.2993 - val_loss: 4650.3682\n",
      "Epoch 13/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5318.2500 - val_loss: 4601.6265\n",
      "Epoch 14/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5266.3325 - val_loss: 4553.6699\n",
      "Epoch 15/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5215.3960 - val_loss: 4506.6362\n",
      "Epoch 16/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5165.3359 - val_loss: 4460.3359\n",
      "Epoch 17/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5115.9780 - val_loss: 4414.6377\n",
      "Epoch 18/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 5067.2065 - val_loss: 4369.4614\n",
      "Epoch 19/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5018.9512 - val_loss: 4324.7559\n",
      "Epoch 20/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4971.1670 - val_loss: 4280.4868\n",
      "Epoch 21/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4923.8203 - val_loss: 4236.6279\n",
      "Epoch 22/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4876.8906 - val_loss: 4193.1616\n",
      "Epoch 23/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4830.3569 - val_loss: 4150.0732\n",
      "Epoch 24/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4784.2070 - val_loss: 4107.3516\n",
      "Epoch 25/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4738.4312 - val_loss: 4064.9863\n",
      "Epoch 26/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4693.0171 - val_loss: 4022.9702\n",
      "Epoch 27/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4647.9590 - val_loss: 3981.2959\n",
      "Epoch 28/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4603.2500 - val_loss: 3939.9585\n",
      "Epoch 29/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4558.8843 - val_loss: 3898.9519\n",
      "Epoch 30/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4514.8555 - val_loss: 3858.2717\n",
      "Epoch 31/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4471.1606 - val_loss: 3817.9146\n",
      "Epoch 32/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4422.0693 - val_loss: 3765.0210\n",
      "Epoch 33/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4367.9683 - val_loss: 3719.3743\n",
      "Epoch 34/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4319.2739 - val_loss: 3674.9854\n",
      "Epoch 35/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4271.9229 - val_loss: 3631.7468\n",
      "Epoch 36/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4225.6592 - val_loss: 3589.3960\n",
      "Epoch 37/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4180.2441 - val_loss: 3547.7620\n",
      "Epoch 38/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4135.5288 - val_loss: 3506.7395\n",
      "Epoch 39/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4091.4211 - val_loss: 3466.2600\n",
      "Epoch 40/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4047.8557 - val_loss: 3426.2754\n",
      "Epoch 41/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4004.7908 - val_loss: 3386.7527\n",
      "Epoch 42/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3962.1921 - val_loss: 3347.6631\n",
      "Epoch 43/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3920.0339 - val_loss: 3308.9880\n",
      "Epoch 44/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3878.2979 - val_loss: 3270.7102\n",
      "Epoch 45/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3836.9663 - val_loss: 3232.8154\n",
      "Epoch 46/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3796.0259 - val_loss: 3195.2927\n",
      "Epoch 47/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3755.4666 - val_loss: 3158.1323\n",
      "Epoch 48/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3715.2764 - val_loss: 3121.3240\n",
      "Epoch 49/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3675.4470 - val_loss: 3084.8621\n",
      "Epoch 50/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3635.9719 - val_loss: 3048.7388\n",
      "Epoch 51/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3596.8440 - val_loss: 3012.9495\n",
      "Epoch 52/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3558.0576 - val_loss: 2977.4868\n",
      "Epoch 53/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3519.6060 - val_loss: 2942.3469\n",
      "Epoch 54/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3481.4849 - val_loss: 2907.5249\n",
      "Epoch 55/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3443.6892 - val_loss: 2873.0164\n",
      "Epoch 56/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3406.2153 - val_loss: 2838.8174\n",
      "Epoch 57/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3369.0586 - val_loss: 2804.9248\n",
      "Epoch 58/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3332.2151 - val_loss: 2771.3342\n",
      "Epoch 59/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3295.6816 - val_loss: 2738.0435\n",
      "Epoch 60/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3259.4556 - val_loss: 2705.0486\n",
      "Epoch 61/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3223.5325 - val_loss: 2672.3467\n",
      "Epoch 62/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3187.9099 - val_loss: 2639.9355\n",
      "Epoch 63/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3152.5854 - val_loss: 2607.8115\n",
      "Epoch 64/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3117.5559 - val_loss: 2575.9729\n",
      "Epoch 65/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3082.8188 - val_loss: 2544.4175\n",
      "Epoch 66/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3048.3711 - val_loss: 2513.1404\n",
      "Epoch 67/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3014.2104 - val_loss: 2482.1426\n",
      "Epoch 68/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2980.3345 - val_loss: 2451.4189\n",
      "Epoch 69/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2946.7419 - val_loss: 2420.9700\n",
      "Epoch 70/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2913.4294 - val_loss: 2390.7915\n",
      "Epoch 71/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2880.3955 - val_loss: 2360.8826\n",
      "Epoch 72/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2847.6377 - val_loss: 2331.2405\n",
      "Epoch 73/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2815.1536 - val_loss: 2301.8640\n",
      "Epoch 74/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2782.9414 - val_loss: 2272.7498\n",
      "Epoch 75/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2750.9995 - val_loss: 2243.8977\n",
      "Epoch 76/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2719.3262 - val_loss: 2215.3047\n",
      "Epoch 77/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2687.9189 - val_loss: 2186.9695\n",
      "Epoch 78/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2656.7766 - val_loss: 2158.8899\n",
      "Epoch 79/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2625.8970 - val_loss: 2131.0649\n",
      "Epoch 80/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2595.2786 - val_loss: 2103.4922\n",
      "Epoch 81/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2564.9187 - val_loss: 2076.1707\n",
      "Epoch 82/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2534.8171 - val_loss: 2049.0984\n",
      "Epoch 83/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2504.9717 - val_loss: 2022.2731\n",
      "Epoch 84/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2475.3806 - val_loss: 1995.6942\n",
      "Epoch 85/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2446.0417 - val_loss: 1969.3589\n",
      "Epoch 86/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2416.9536 - val_loss: 1943.2670\n",
      "Epoch 87/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2388.1152 - val_loss: 1917.4158\n",
      "Epoch 88/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2359.5244 - val_loss: 1891.8048\n",
      "Epoch 89/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2331.1804 - val_loss: 1866.4313\n",
      "Epoch 90/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2303.0815 - val_loss: 1841.2949\n",
      "Epoch 91/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2275.2258 - val_loss: 1816.3931\n",
      "Epoch 92/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2247.6116 - val_loss: 1791.7252\n",
      "Epoch 93/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2220.2375 - val_loss: 1767.2903\n",
      "Epoch 94/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2193.1030 - val_loss: 1743.0853\n",
      "Epoch 95/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2166.2058 - val_loss: 1719.1102\n",
      "Epoch 96/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2139.5447 - val_loss: 1695.3632\n",
      "Epoch 97/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2113.1182 - val_loss: 1671.8428\n",
      "Epoch 98/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2086.9253 - val_loss: 1648.5477\n",
      "Epoch 99/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2060.9644 - val_loss: 1625.4768\n",
      "Epoch 100/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2035.2340 - val_loss: 1602.6283\n",
      "Epoch 101/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2009.7333 - val_loss: 1580.0015\n",
      "Epoch 102/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1984.4601 - val_loss: 1557.5942\n",
      "Epoch 103/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1959.4138 - val_loss: 1535.4061\n",
      "Epoch 104/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1934.5925 - val_loss: 1513.4347\n",
      "Epoch 105/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1909.9951 - val_loss: 1491.6790\n",
      "Epoch 106/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1885.6206 - val_loss: 1470.1393\n",
      "Epoch 107/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1861.4673 - val_loss: 1448.8121\n",
      "Epoch 108/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1837.5342 - val_loss: 1427.6971\n",
      "Epoch 109/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1813.8201 - val_loss: 1406.7935\n",
      "Epoch 110/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1790.3232 - val_loss: 1386.0992\n",
      "Epoch 111/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1767.0432 - val_loss: 1365.6139\n",
      "Epoch 112/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1743.9784 - val_loss: 1345.3356\n",
      "Epoch 113/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1721.1267 - val_loss: 1325.2627\n",
      "Epoch 114/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1698.4880 - val_loss: 1305.3950\n",
      "Epoch 115/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1676.0602 - val_loss: 1285.7305\n",
      "Epoch 116/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1653.8431 - val_loss: 1266.2687\n",
      "Epoch 117/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1631.8350 - val_loss: 1247.0077\n",
      "Epoch 118/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1610.0348 - val_loss: 1227.9468\n",
      "Epoch 119/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1588.4408 - val_loss: 1209.0840\n",
      "Epoch 120/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1567.0519 - val_loss: 1190.4192\n",
      "Epoch 121/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1545.8680 - val_loss: 1171.9514\n",
      "Epoch 122/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1524.8868 - val_loss: 1153.6782\n",
      "Epoch 123/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1504.1078 - val_loss: 1135.5988\n",
      "Epoch 124/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1483.5294 - val_loss: 1117.7130\n",
      "Epoch 125/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1463.1512 - val_loss: 1100.0188\n",
      "Epoch 126/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1442.9708 - val_loss: 1082.5149\n",
      "Epoch 127/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1422.9878 - val_loss: 1065.2008\n",
      "Epoch 128/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1403.2012 - val_loss: 1048.0743\n",
      "Epoch 129/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1383.6095 - val_loss: 1031.1355\n",
      "Epoch 130/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1364.2115 - val_loss: 1014.3821\n",
      "Epoch 131/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1345.0067 - val_loss: 997.8142\n",
      "Epoch 132/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1325.9933 - val_loss: 981.4301\n",
      "Epoch 133/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1307.1707 - val_loss: 965.2281\n",
      "Epoch 134/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1288.5374 - val_loss: 949.2082\n",
      "Epoch 135/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1270.0927 - val_loss: 933.3683\n",
      "Epoch 136/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1251.8351 - val_loss: 917.7080\n",
      "Epoch 137/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1233.7637 - val_loss: 902.2261\n",
      "Epoch 138/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1215.8771 - val_loss: 886.9209\n",
      "Epoch 139/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1198.1752 - val_loss: 871.7921\n",
      "Epoch 140/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1180.6558 - val_loss: 856.8383\n",
      "Epoch 141/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1163.3184 - val_loss: 842.0578\n",
      "Epoch 142/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1146.1615 - val_loss: 827.4501\n",
      "Epoch 143/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1129.1844 - val_loss: 813.0143\n",
      "Epoch 144/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1112.3861 - val_loss: 798.7495\n",
      "Epoch 145/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1095.7653 - val_loss: 784.6537\n",
      "Epoch 146/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1079.3208 - val_loss: 770.7261\n",
      "Epoch 147/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1063.0519 - val_loss: 756.9666\n",
      "Epoch 148/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1046.9574 - val_loss: 743.3731\n",
      "Epoch 149/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1031.0366 - val_loss: 729.9450\n",
      "Epoch 150/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1015.2874 - val_loss: 716.6803\n",
      "Epoch 151/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 999.7099 - val_loss: 703.5796\n",
      "Epoch 152/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 984.3022 - val_loss: 690.6406\n",
      "Epoch 153/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 969.0640 - val_loss: 677.8622\n",
      "Epoch 154/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 953.9940 - val_loss: 665.2446\n",
      "Epoch 155/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 939.0908 - val_loss: 652.7852\n",
      "Epoch 156/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 924.3539 - val_loss: 640.4836\n",
      "Epoch 157/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 909.7819 - val_loss: 628.3394\n",
      "Epoch 158/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 895.3739 - val_loss: 616.3503\n",
      "Epoch 159/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 881.1283 - val_loss: 604.5164\n",
      "Epoch 160/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 867.0452 - val_loss: 592.8353\n",
      "Epoch 161/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 853.1225 - val_loss: 581.3075\n",
      "Epoch 162/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 839.3604 - val_loss: 569.9307\n",
      "Epoch 163/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 825.7569 - val_loss: 558.7052\n",
      "Epoch 164/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 812.3115 - val_loss: 547.6293\n",
      "Epoch 165/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 799.0233 - val_loss: 536.7017\n",
      "Epoch 166/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 785.8906 - val_loss: 525.9213\n",
      "Epoch 167/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 772.9127 - val_loss: 515.2871\n",
      "Epoch 168/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 760.0886 - val_loss: 504.7991\n",
      "Epoch 169/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 747.4171 - val_loss: 494.4547\n",
      "Epoch 170/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 734.8976 - val_loss: 484.2535\n",
      "Epoch 171/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 722.5288 - val_loss: 474.1946\n",
      "Epoch 172/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 710.3101 - val_loss: 464.2771\n",
      "Epoch 173/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 698.2398 - val_loss: 454.5004\n",
      "Epoch 174/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 686.3177 - val_loss: 444.8622\n",
      "Epoch 175/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 674.5420 - val_loss: 435.3622\n",
      "Epoch 176/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 662.9122 - val_loss: 425.9996\n",
      "Epoch 177/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 651.4273 - val_loss: 416.7732\n",
      "Epoch 178/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 640.0861 - val_loss: 407.6814\n",
      "Epoch 179/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 628.8878 - val_loss: 398.7239\n",
      "Epoch 180/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 617.8306 - val_loss: 389.8991\n",
      "Epoch 181/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 606.9148 - val_loss: 381.2067\n",
      "Epoch 182/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 596.1389 - val_loss: 372.6458\n",
      "Epoch 183/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 585.5020 - val_loss: 364.2143\n",
      "Epoch 184/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 575.0028 - val_loss: 355.9120\n",
      "Epoch 185/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 564.6402 - val_loss: 347.7378\n",
      "Epoch 186/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 554.4132 - val_loss: 339.6895\n",
      "Epoch 187/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 544.3211 - val_loss: 331.7676\n",
      "Epoch 188/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 534.3632 - val_loss: 323.9709\n",
      "Epoch 189/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 524.5381 - val_loss: 316.2985\n",
      "Epoch 190/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 514.8452 - val_loss: 308.7486\n",
      "Epoch 191/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 505.2827 - val_loss: 301.3196\n",
      "Epoch 192/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 495.8500 - val_loss: 294.0125\n",
      "Epoch 193/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 486.5466 - val_loss: 286.8247\n",
      "Epoch 194/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 477.3712 - val_loss: 279.7563\n",
      "Epoch 195/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 468.3223 - val_loss: 272.8048\n",
      "Epoch 196/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 459.3998 - val_loss: 265.9708\n",
      "Epoch 197/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 450.6024 - val_loss: 259.2522\n",
      "Epoch 198/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 441.9289 - val_loss: 252.6488\n",
      "Epoch 199/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 433.3783 - val_loss: 246.1590\n",
      "Epoch 200/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 424.9502 - val_loss: 239.7816\n",
      "Epoch 201/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 416.6428 - val_loss: 233.5162\n",
      "Epoch 202/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 408.4556 - val_loss: 227.3615\n",
      "Epoch 203/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 400.3875 - val_loss: 221.3159\n",
      "Epoch 204/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 392.4376 - val_loss: 215.3794\n",
      "Epoch 205/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 384.6049 - val_loss: 209.5509\n",
      "Epoch 206/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 376.8885 - val_loss: 203.8289\n",
      "Epoch 207/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 369.2871 - val_loss: 198.2123\n",
      "Epoch 208/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 361.8001 - val_loss: 192.7002\n",
      "Epoch 209/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 354.4261 - val_loss: 187.2921\n",
      "Epoch 210/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 347.1643 - val_loss: 181.9860\n",
      "Epoch 211/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 340.0140 - val_loss: 176.7822\n",
      "Epoch 212/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 332.9742 - val_loss: 171.6785\n",
      "Epoch 213/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 326.0437 - val_loss: 166.6747\n",
      "Epoch 214/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 319.2212 - val_loss: 161.7687\n",
      "Epoch 215/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 312.5061 - val_loss: 156.9607\n",
      "Epoch 216/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 305.8975 - val_loss: 152.2493\n",
      "Epoch 217/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 299.3943 - val_loss: 147.6334\n",
      "Epoch 218/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 292.9957 - val_loss: 143.1118\n",
      "Epoch 219/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 286.7004 - val_loss: 138.6836\n",
      "Epoch 220/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 280.5074 - val_loss: 134.3477\n",
      "Epoch 221/500\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 274.4162 - val_loss: 130.1039\n",
      "Epoch 222/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 268.4256 - val_loss: 125.9499\n",
      "Epoch 223/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 262.5343 - val_loss: 121.8855\n",
      "Epoch 224/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 256.7419 - val_loss: 117.9098\n",
      "Epoch 225/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 251.0471 - val_loss: 114.0215\n",
      "Epoch 226/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 245.4491 - val_loss: 110.2198\n",
      "Epoch 227/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 239.9467 - val_loss: 106.5035\n",
      "Epoch 228/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 234.5391 - val_loss: 102.8713\n",
      "Epoch 229/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 229.2252 - val_loss: 99.3226\n",
      "Epoch 230/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 224.0040 - val_loss: 95.8563\n",
      "Epoch 231/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 218.8748 - val_loss: 92.4714\n",
      "Epoch 232/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 213.8363 - val_loss: 89.1671\n",
      "Epoch 233/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 208.8880 - val_loss: 85.9420\n",
      "Epoch 234/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 204.0282 - val_loss: 82.7954\n",
      "Epoch 235/500\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 199.2565 - val_loss: 79.7262\n",
      "Epoch 236/500\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 194.5719 - val_loss: 76.7335\n",
      "Epoch 237/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 189.9735 - val_loss: 73.8160\n",
      "Epoch 238/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 185.4600 - val_loss: 70.9730\n",
      "Epoch 239/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 181.0307 - val_loss: 68.2035\n",
      "Epoch 240/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 176.6847 - val_loss: 65.5065\n",
      "Epoch 241/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 172.4208 - val_loss: 62.8809\n",
      "Epoch 242/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 168.2384 - val_loss: 60.3257\n",
      "Epoch 243/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 164.1360 - val_loss: 57.8400\n",
      "Epoch 244/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 160.1130 - val_loss: 55.4227\n",
      "Epoch 245/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 156.1685 - val_loss: 53.0730\n",
      "Epoch 246/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 152.3013 - val_loss: 50.7897\n",
      "Epoch 247/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 148.5106 - val_loss: 48.5718\n",
      "Epoch 248/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 144.7955 - val_loss: 46.4188\n",
      "Epoch 249/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 141.1554 - val_loss: 44.3293\n",
      "Epoch 250/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 137.5889 - val_loss: 42.3026\n",
      "Epoch 251/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 134.0950 - val_loss: 40.3373\n",
      "Epoch 252/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 130.6730 - val_loss: 38.4328\n",
      "Epoch 253/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 127.3219 - val_loss: 36.5882\n",
      "Epoch 254/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 124.0409 - val_loss: 34.8021\n",
      "Epoch 255/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 120.8289 - val_loss: 33.0739\n",
      "Epoch 256/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 117.6850 - val_loss: 31.4026\n",
      "Epoch 257/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 114.6082 - val_loss: 29.7870\n",
      "Epoch 258/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 111.5976 - val_loss: 28.2267\n",
      "Epoch 259/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 108.6526 - val_loss: 26.7202\n",
      "Epoch 260/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 105.7719 - val_loss: 25.2668\n",
      "Epoch 261/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 102.9547 - val_loss: 23.8654\n",
      "Epoch 262/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 100.2002 - val_loss: 22.5154\n",
      "Epoch 263/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 97.5074 - val_loss: 21.2157\n",
      "Epoch 264/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 94.8754 - val_loss: 19.9650\n",
      "Epoch 265/500\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 92.3033 - val_loss: 18.7629\n",
      "Epoch 266/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 89.7901 - val_loss: 17.6083\n",
      "Epoch 267/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 87.3351 - val_loss: 16.5002\n",
      "Epoch 268/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 84.9372 - val_loss: 15.4378\n",
      "Epoch 269/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 82.5957 - val_loss: 14.4200\n",
      "Epoch 270/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 80.3098 - val_loss: 13.4463\n",
      "Epoch 271/500\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 78.0784 - val_loss: 12.5155\n",
      "Epoch 272/500\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 75.9008 - val_loss: 11.6268\n",
      "Epoch 273/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 73.7760 - val_loss: 10.7791\n",
      "Epoch 274/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 71.7030 - val_loss: 9.9719\n",
      "Epoch 275/500\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 69.6812 - val_loss: 9.2040\n",
      "Epoch 276/500\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 67.7099 - val_loss: 8.4748\n",
      "Epoch 277/500\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 65.7878 - val_loss: 7.7833\n",
      "Epoch 278/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 63.9146 - val_loss: 7.1285\n",
      "Epoch 279/500\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 62.0888 - val_loss: 6.5097\n",
      "Epoch 280/500\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 60.3100 - val_loss: 5.9261\n",
      "Epoch 281/500\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 58.5773 - val_loss: 5.3768\n",
      "Epoch 282/500\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 56.8898 - val_loss: 4.8610\n",
      "Epoch 283/500\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 55.2468 - val_loss: 4.3778\n",
      "Epoch 284/500\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 53.6474 - val_loss: 3.9266\n",
      "Epoch 285/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 52.0910 - val_loss: 3.5062\n",
      "Epoch 286/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 50.5765 - val_loss: 3.1162\n",
      "Epoch 287/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 49.1032 - val_loss: 2.7554\n",
      "Epoch 288/500\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 47.6702 - val_loss: 2.4234\n",
      "Epoch 289/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 46.2770 - val_loss: 2.1192\n",
      "Epoch 290/500\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 44.9226 - val_loss: 1.8421\n",
      "Epoch 291/500\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 43.6065 - val_loss: 1.5912\n",
      "Epoch 292/500\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 42.3276 - val_loss: 1.3659\n",
      "Epoch 293/500\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 41.0853 - val_loss: 1.1655\n",
      "Epoch 294/500\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 39.8790 - val_loss: 0.9890\n",
      "Epoch 295/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 38.7076 - val_loss: 0.8359\n",
      "Epoch 296/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 37.5707 - val_loss: 0.7053\n",
      "Epoch 297/500\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 36.4674 - val_loss: 0.5967\n",
      "Epoch 298/500\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 35.3970 - val_loss: 0.5091\n",
      "Epoch 299/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 34.3588 - val_loss: 0.4421\n",
      "Epoch 300/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 33.3520 - val_loss: 0.3948\n",
      "Epoch 301/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 32.3762 - val_loss: 0.3666\n",
      "Epoch 302/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 31.4304 - val_loss: 0.3568\n",
      "Epoch 303/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 30.5141 - val_loss: 0.3647\n",
      "Epoch 304/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 29.6264 - val_loss: 0.3898\n",
      "Epoch 305/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 28.7669 - val_loss: 0.4313\n",
      "Epoch 306/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 27.9348 - val_loss: 0.4886\n",
      "Epoch 307/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 27.1295 - val_loss: 0.5611\n",
      "Epoch 308/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 26.3503 - val_loss: 0.6482\n",
      "Epoch 309/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 25.5967 - val_loss: 0.7492\n",
      "Epoch 310/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 24.8679 - val_loss: 0.8636\n",
      "Epoch 311/500\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 24.1633 - val_loss: 0.9908\n",
      "Epoch 312/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 23.4824 - val_loss: 1.1302\n",
      "Epoch 313/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 22.8245 - val_loss: 1.2812\n",
      "Epoch 314/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 22.1892 - val_loss: 1.4433\n",
      "Epoch 315/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 21.5756 - val_loss: 1.6160\n",
      "Epoch 316/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 20.9833 - val_loss: 1.7986\n",
      "Epoch 317/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 20.4118 - val_loss: 1.9908\n",
      "Epoch 318/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 19.8605 - val_loss: 2.1919\n",
      "Epoch 319/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 19.3288 - val_loss: 2.4015\n",
      "Epoch 320/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 18.8161 - val_loss: 2.6190\n",
      "Epoch 321/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 18.3220 - val_loss: 2.8441\n",
      "Epoch 322/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 17.8459 - val_loss: 3.0761\n",
      "Epoch 323/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 17.3873 - val_loss: 3.3147\n",
      "Epoch 324/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 16.9458 - val_loss: 3.5594\n",
      "Epoch 325/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 16.5208 - val_loss: 3.8097\n",
      "Epoch 326/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 16.1118 - val_loss: 4.0654\n",
      "Epoch 327/500\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 15.7183 - val_loss: 4.3258\n",
      "Epoch 328/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 15.3400 - val_loss: 4.5907\n",
      "Epoch 329/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 14.9763 - val_loss: 4.8594\n",
      "Epoch 330/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 14.6268 - val_loss: 5.1320\n",
      "Epoch 331/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 14.2910 - val_loss: 5.4077\n",
      "Epoch 332/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 13.9685 - val_loss: 5.6863\n",
      "Epoch 333/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 13.6589 - val_loss: 5.9674\n",
      "Epoch 334/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 13.3618 - val_loss: 6.2509\n",
      "Epoch 335/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 13.0767 - val_loss: 6.5361\n",
      "Epoch 336/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 12.8033 - val_loss: 6.8228\n",
      "Epoch 337/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 12.5412 - val_loss: 7.1108\n",
      "Epoch 338/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 12.2900 - val_loss: 7.3997\n",
      "Epoch 339/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 12.0493 - val_loss: 7.6892\n",
      "Epoch 340/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 11.8188 - val_loss: 7.9792\n",
      "Epoch 341/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.5981 - val_loss: 8.2690\n",
      "Epoch 342/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 11.3869 - val_loss: 8.5588\n",
      "Epoch 343/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.1849 - val_loss: 8.8482\n",
      "Epoch 344/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 10.9916 - val_loss: 9.1368\n",
      "Epoch 345/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 10.8069 - val_loss: 9.4245\n",
      "Epoch 346/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 10.6303 - val_loss: 9.7109\n",
      "Epoch 347/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 10.4617 - val_loss: 9.9964\n",
      "Epoch 348/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 10.3006 - val_loss: 10.2799\n",
      "Epoch 349/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 10.1469 - val_loss: 10.5619\n",
      "Epoch 350/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 10.0001 - val_loss: 10.8421\n",
      "Epoch 351/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 9.8601 - val_loss: 11.1199\n",
      "Epoch 352/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 9.7267 - val_loss: 11.3956\n",
      "Epoch 353/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 9.5994 - val_loss: 11.6688\n",
      "Epoch 354/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 9.4782 - val_loss: 11.9394\n",
      "Epoch 355/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 9.3627 - val_loss: 12.2074\n",
      "Epoch 356/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 9.2527 - val_loss: 12.4724\n",
      "Epoch 357/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 9.1481 - val_loss: 12.7344\n",
      "Epoch 358/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 9.0485 - val_loss: 12.9933\n",
      "Epoch 359/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 8.9538 - val_loss: 13.2490\n",
      "Epoch 360/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 8.8638 - val_loss: 13.5013\n",
      "Epoch 361/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8.7782 - val_loss: 13.7504\n",
      "Epoch 362/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 8.6968 - val_loss: 13.9958\n",
      "Epoch 363/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8.6197 - val_loss: 14.2376\n",
      "Epoch 364/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 8.5463 - val_loss: 14.4759\n",
      "Epoch 365/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 8.4768 - val_loss: 14.7104\n",
      "Epoch 366/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8.4108 - val_loss: 14.9410\n",
      "Epoch 367/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8.3482 - val_loss: 15.1681\n",
      "Epoch 368/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 8.2889 - val_loss: 15.3909\n",
      "Epoch 369/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 8.2327 - val_loss: 15.6100\n",
      "Epoch 370/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 8.1794 - val_loss: 15.8250\n",
      "Epoch 371/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 8.1290 - val_loss: 16.0362\n",
      "Epoch 372/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8.0812 - val_loss: 16.2433\n",
      "Epoch 373/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8.0360 - val_loss: 16.4462\n",
      "Epoch 374/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.9933 - val_loss: 16.6451\n",
      "Epoch 375/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.9529 - val_loss: 16.8400\n",
      "Epoch 376/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.9147 - val_loss: 17.0307\n",
      "Epoch 377/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 7.8786 - val_loss: 17.2174\n",
      "Epoch 378/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.8445 - val_loss: 17.4001\n",
      "Epoch 379/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.8123 - val_loss: 17.5787\n",
      "Epoch 380/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 7.7819 - val_loss: 17.7531\n",
      "Epoch 381/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 7.7532 - val_loss: 17.9236\n",
      "Epoch 382/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.7262 - val_loss: 18.0902\n",
      "Epoch 383/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 7.7006 - val_loss: 18.2528\n",
      "Epoch 384/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.6765 - val_loss: 18.4113\n",
      "Epoch 385/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.6539 - val_loss: 18.5658\n",
      "Epoch 386/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.6325 - val_loss: 18.7166\n",
      "Epoch 387/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.6123 - val_loss: 18.8633\n",
      "Epoch 388/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.5934 - val_loss: 19.0063\n",
      "Epoch 389/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.5755 - val_loss: 19.1456\n",
      "Epoch 390/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.5587 - val_loss: 19.2811\n",
      "Epoch 391/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.5429 - val_loss: 19.4127\n",
      "Epoch 392/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.5281 - val_loss: 19.5410\n",
      "Epoch 393/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.5141 - val_loss: 19.6654\n",
      "Epoch 394/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.5009 - val_loss: 19.7863\n",
      "Epoch 395/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.4885 - val_loss: 19.9036\n",
      "Epoch 396/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.4769 - val_loss: 20.0175\n",
      "Epoch 397/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.4660 - val_loss: 20.1281\n",
      "Epoch 398/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.4558 - val_loss: 20.2353\n",
      "Epoch 399/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.4461 - val_loss: 20.3391\n",
      "Epoch 400/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.4371 - val_loss: 20.4397\n",
      "Epoch 401/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 7.4286 - val_loss: 20.5373\n",
      "Epoch 402/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.4207 - val_loss: 20.6319\n",
      "Epoch 403/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.4132 - val_loss: 20.7233\n",
      "Epoch 404/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.4062 - val_loss: 20.8117\n",
      "Epoch 405/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3996 - val_loss: 20.8972\n",
      "Epoch 406/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3934 - val_loss: 20.9799\n",
      "Epoch 407/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3877 - val_loss: 21.0600\n",
      "Epoch 408/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3822 - val_loss: 21.1370\n",
      "Epoch 409/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3771 - val_loss: 21.2115\n",
      "Epoch 410/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.3723 - val_loss: 21.2834\n",
      "Epoch 411/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3679 - val_loss: 21.3527\n",
      "Epoch 412/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3637 - val_loss: 21.4195\n",
      "Epoch 413/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.3598 - val_loss: 21.4840\n",
      "Epoch 414/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3561 - val_loss: 21.5459\n",
      "Epoch 415/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3526 - val_loss: 21.6057\n",
      "Epoch 416/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3494 - val_loss: 21.6633\n",
      "Epoch 417/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3464 - val_loss: 21.7184\n",
      "Epoch 418/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3436 - val_loss: 21.7718\n",
      "Epoch 419/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 7.3409 - val_loss: 21.8229\n",
      "Epoch 420/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.3384 - val_loss: 21.8719\n",
      "Epoch 421/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.3361 - val_loss: 21.9193\n",
      "Epoch 422/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3339 - val_loss: 21.9646\n",
      "Epoch 423/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.3319 - val_loss: 22.0081\n",
      "Epoch 424/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.3300 - val_loss: 22.0498\n",
      "Epoch 425/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.3282 - val_loss: 22.0899\n",
      "Epoch 426/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3265 - val_loss: 22.1282\n",
      "Epoch 427/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3250 - val_loss: 22.1652\n",
      "Epoch 428/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.3235 - val_loss: 22.2004\n",
      "Epoch 429/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3222 - val_loss: 22.2342\n",
      "Epoch 430/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3209 - val_loss: 22.2663\n",
      "Epoch 431/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3197 - val_loss: 22.2972\n",
      "Epoch 432/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3186 - val_loss: 22.3267\n",
      "Epoch 433/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3176 - val_loss: 22.3548\n",
      "Epoch 434/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3166 - val_loss: 22.3819\n",
      "Epoch 435/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3157 - val_loss: 22.4077\n",
      "Epoch 436/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.3149 - val_loss: 22.4323\n",
      "Epoch 437/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3141 - val_loss: 22.4557\n",
      "Epoch 438/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 7.3134 - val_loss: 22.4781\n",
      "Epoch 439/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.3127 - val_loss: 22.4994\n",
      "Epoch 440/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 7.3120 - val_loss: 22.5196\n",
      "Epoch 441/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.3115 - val_loss: 22.5390\n",
      "Epoch 442/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.3110 - val_loss: 22.5574\n",
      "Epoch 443/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3104 - val_loss: 22.5748\n",
      "Epoch 444/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.3100 - val_loss: 22.5914\n",
      "Epoch 445/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.3096 - val_loss: 22.6072\n",
      "Epoch 446/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3092 - val_loss: 22.6223\n",
      "Epoch 447/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3088 - val_loss: 22.6367\n",
      "Epoch 448/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.3085 - val_loss: 22.6499\n",
      "Epoch 449/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3082 - val_loss: 22.6627\n",
      "Epoch 450/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.3079 - val_loss: 22.6748\n",
      "Epoch 451/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3077 - val_loss: 22.6865\n",
      "Epoch 452/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3074 - val_loss: 22.6973\n",
      "Epoch 453/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3072 - val_loss: 22.7077\n",
      "Epoch 454/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 7.3071 - val_loss: 22.7173\n",
      "Epoch 455/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.3069 - val_loss: 22.7268\n",
      "Epoch 456/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3067 - val_loss: 22.7354\n",
      "Epoch 457/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3066 - val_loss: 22.7437\n",
      "Epoch 458/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3065 - val_loss: 22.7512\n",
      "Epoch 459/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3064 - val_loss: 22.7585\n",
      "Epoch 460/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.3064 - val_loss: 22.7657\n",
      "Epoch 461/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3063 - val_loss: 22.7722\n",
      "Epoch 462/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.3062 - val_loss: 22.7785\n",
      "Epoch 463/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.3062 - val_loss: 22.7841\n",
      "Epoch 464/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3062 - val_loss: 22.7897\n",
      "Epoch 465/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3061 - val_loss: 22.7947\n",
      "Epoch 466/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.3062 - val_loss: 22.7997\n",
      "Epoch 467/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.3061 - val_loss: 22.8041\n",
      "Epoch 468/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.3062 - val_loss: 22.8083\n",
      "Epoch 469/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3062 - val_loss: 22.8123\n",
      "Epoch 470/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.3062 - val_loss: 22.8160\n",
      "Epoch 471/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 7.3063 - val_loss: 22.8194\n",
      "Epoch 472/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.3063 - val_loss: 22.8228\n",
      "Epoch 473/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.3064 - val_loss: 22.8257\n",
      "Epoch 474/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.3065 - val_loss: 22.8287\n",
      "Epoch 475/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3065 - val_loss: 22.8315\n",
      "Epoch 476/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3066 - val_loss: 22.8339\n",
      "Epoch 477/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3066 - val_loss: 22.8360\n",
      "Epoch 478/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3068 - val_loss: 22.8382\n",
      "Epoch 479/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3068 - val_loss: 22.8402\n",
      "Epoch 480/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.3069 - val_loss: 22.8420\n",
      "Epoch 481/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3070 - val_loss: 22.8438\n",
      "Epoch 482/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.3071 - val_loss: 22.8453\n",
      "Epoch 483/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3072 - val_loss: 22.8466\n",
      "Epoch 484/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3074 - val_loss: 22.8480\n",
      "Epoch 485/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3075 - val_loss: 22.8491\n",
      "Epoch 486/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3076 - val_loss: 22.8503\n",
      "Epoch 487/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3077 - val_loss: 22.8511\n",
      "Epoch 488/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3079 - val_loss: 22.8521\n",
      "Epoch 489/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3080 - val_loss: 22.8530\n",
      "Epoch 490/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3081 - val_loss: 22.8538\n",
      "Epoch 491/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3083 - val_loss: 22.8546\n",
      "Epoch 492/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3084 - val_loss: 22.8551\n",
      "Epoch 493/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.3085 - val_loss: 22.8556\n",
      "Epoch 494/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3087 - val_loss: 22.8561\n",
      "Epoch 495/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.3088 - val_loss: 22.8565\n",
      "Epoch 496/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3090 - val_loss: 22.8569\n",
      "Epoch 497/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 7.3091 - val_loss: 22.8573\n",
      "Epoch 498/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.3093 - val_loss: 22.8575\n",
      "Epoch 499/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.3094 - val_loss: 22.8577\n",
      "Epoch 500/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.3096 - val_loss: 22.8580\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.0403, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0012, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0120, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.0334, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 345ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[74.33708217, 74.32979925, 74.32251634, 74.31523343, 74.30795051,\n",
       "        74.3006676 , 74.28727824, 74.27327264, 74.25926704, 74.24526144,\n",
       "        74.23125584, 74.21725023, 74.20324463, 74.18923903, 74.17523343,\n",
       "        74.16122782, 74.14722222, 74.13321662, 74.11921102, 74.10520542,\n",
       "        74.09119981, 74.07719421, 74.06318861, 74.04918301, 74.0351774 ,\n",
       "        74.0211718 , 74.0071662 , 73.9931606 , 73.979155  , 73.96514939,\n",
       "        73.95114379, 73.93713819, 73.92313259, 73.90912698, 73.89512138,\n",
       "        73.88111578, 73.86711018, 73.85310458, 73.83909897, 73.82509337,\n",
       "        73.81108777, 73.79241363, 73.75599907, 73.7195845 , 73.68316993,\n",
       "        73.64675537, 73.6103408 , 73.57392624, 73.53751167, 73.50109711,\n",
       "        73.46468254, 73.42826797, 73.39185341, 73.35543884, 73.31902428,\n",
       "        73.28260971, 73.24619514, 73.20978058, 73.17336601, 73.13695145,\n",
       "        73.10053688, 73.06412232, 73.02770775, 72.99129318, 72.95487862,\n",
       "        72.91846405, 72.88204949, 72.84563492, 72.80922035, 72.77280579,\n",
       "        72.73639122, 72.69997666, 72.66356209, 72.62714753, 72.59073296,\n",
       "        72.55431839, 72.51790383, 72.49003268, 72.47042484, 72.45081699,\n",
       "        78.38580322,  0.17704163,  0.        ,  0.09506555,  0.        ,\n",
       "         0.        ,  0.35351771,  0.        ,  0.22538763,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.46497834,  0.        ,\n",
       "         0.66569293,  0.        ,  0.1712544 ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([71.70436508, 71.70343137, 71.70249767, 71.70156396, 71.70063025,\n",
       "       71.69848273, 71.69381419, 71.68914566, 71.68447712, 71.67980859,\n",
       "       71.67514006, 71.67047152, 71.66580299, 71.66113445, 71.65646592,\n",
       "       71.65179739, 71.64712885, 71.64246032, 71.63779178, 71.63312325,\n",
       "       71.62845472, 71.62378618, 71.61911765, 71.61444911, 71.60978058,\n",
       "       71.60511204, 71.60044351, 71.59577498, 71.59110644, 71.58643791,\n",
       "       71.58176937, 71.57710084, 71.57243231, 71.56776377, 71.56309524,\n",
       "       71.5584267 , 71.55375817, 71.54908964, 71.5444211 , 71.53975257,\n",
       "       71.53508403, 71.5304155 , 71.52574697, 71.52107843, 71.5164099 ,\n",
       "       71.51174136, 71.50707283, 71.5024043 , 71.49773576, 71.49306723,\n",
       "       71.48839869, 71.48373016, 71.47906162, 71.47439309, 71.46972456,\n",
       "       71.46505602, 71.46038749, 71.45571895, 71.45105042, 71.44638189,\n",
       "       71.44171335, 71.43704482, 71.43237628, 71.42770775, 71.42303922,\n",
       "       71.41837068, 71.41370215, 71.40903361, 71.40436508, 71.39969655,\n",
       "       71.39502801, 71.39035948, 71.38569094, 71.38102241, 71.37635387,\n",
       "       71.37168534, 71.36701681, 71.36234827, 71.35767974, 71.3530112 ,\n",
       "       71.34834267, 71.34367414, 71.3390056 , 71.33433707, 71.32966853,\n",
       "       71.325     , 71.32033147, 71.31566293, 71.3109944 , 71.30632586,\n",
       "       71.30165733, 71.2969888 , 71.29232026, 71.28765173, 71.28298319,\n",
       "       71.27831466, 71.27364613, 71.26897759, 71.26430906, 71.25964052])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.096525155962063\n",
      "15.253041442155341\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
