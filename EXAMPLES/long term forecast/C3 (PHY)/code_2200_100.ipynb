{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "2295    68.282230\n",
       "2296    68.275945\n",
       "2297    68.269659\n",
       "2298    68.263374\n",
       "2299    68.257088\n",
       "Name: C3, Length: 2300, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c3_interpolated_2200_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "2195     0.000000\n",
       "2196     0.000000\n",
       "2197     0.224613\n",
       "2198     0.181440\n",
       "2199     0.702173\n",
       "Name: C3, Length: 2200, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2200)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.100000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.875910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.651821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90.427731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90.203641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     91.100000  0.000298\n",
       "1     90.875910  0.000298\n",
       "2     90.651821  0.000297\n",
       "3     90.427731  0.000297\n",
       "4     90.203641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.0403  # Value for C0\n",
    "K0 = -0.0012  # Value for K0\n",
    "K1 = -0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0120    # Value for b\n",
    "c = 2.0334    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    91.100000    90.875910    90.651821    90.427731    90.203641   \n",
      "351    90.875910    90.651821    90.427731    90.203641    89.979552   \n",
      "352    90.651821    90.427731    90.203641    89.979552    89.755462   \n",
      "353    90.427731    90.203641    89.979552    89.755462    89.531373   \n",
      "354    90.203641    89.979552    89.755462    89.531373    89.307283   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    89.979552    89.755462    89.531373    89.307283    89.094118  ...   \n",
      "351    89.755462    89.531373    89.307283    89.094118    89.015686  ...   \n",
      "352    89.531373    89.307283    89.094118    89.015686    88.937255  ...   \n",
      "353    89.307283    89.094118    89.015686    88.937255    88.858824  ...   \n",
      "354    89.094118    89.015686    88.937255    88.858824    88.780392  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   81.423343    0.000263   81.404669    0.000263   81.385994    0.000263   \n",
      "351   81.404669    0.000263   81.385994    0.000263   81.367320    0.000262   \n",
      "352   81.385994    0.000263   81.367320    0.000262   81.348646    0.000262   \n",
      "353   81.367320    0.000262   81.348646    0.000262   81.329972    0.000262   \n",
      "354   81.348646    0.000262   81.329972    0.000262   81.311298    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   81.367320    0.000262   81.348646    0.000262  \n",
      "351   81.348646    0.000262   81.329972    0.000262  \n",
      "352   81.329972    0.000262   81.311298    0.000262  \n",
      "353   81.311298    0.000262   81.292624    0.000262  \n",
      "354   81.292624    0.000262   81.273950    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1750, 1, 251) (1750, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 2s 22ms/step - loss: 5696.2817 - val_loss: 3837.0862\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5521.9609 - val_loss: 3710.3186\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5437.6362 - val_loss: 3651.2036\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5342.4849 - val_loss: 3590.6240\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 5265.1069 - val_loss: 3540.0415\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 5181.0151 - val_loss: 3483.4260\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5098.5547 - val_loss: 3427.5117\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 5022.4292 - val_loss: 3377.4272\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4948.0684 - val_loss: 3328.5315\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4875.1113 - val_loss: 3280.6306\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4803.3467 - val_loss: 3233.6147\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 4732.6470 - val_loss: 3187.4167\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4662.9336 - val_loss: 3141.9897\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4594.1523 - val_loss: 3097.2993\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4526.2603 - val_loss: 3053.3201\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4459.2261 - val_loss: 3010.0310\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4393.0234 - val_loss: 2967.4146\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 4327.6338 - val_loss: 2925.4568\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4263.0361 - val_loss: 2884.1443\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4199.2153 - val_loss: 2843.4651\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 4136.1587 - val_loss: 2803.4104\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4073.8535 - val_loss: 2763.9695\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 4012.2869 - val_loss: 2725.1340\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3951.4504 - val_loss: 2686.8962\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3891.3328 - val_loss: 2649.2480\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3831.9260 - val_loss: 2612.1819\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3773.2214 - val_loss: 2575.6919\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3715.2104 - val_loss: 2539.7703\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3657.8857 - val_loss: 2504.4111\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3601.2383 - val_loss: 2469.6084\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3545.2629 - val_loss: 2435.3557\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3489.9519 - val_loss: 2401.6475\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3435.2981 - val_loss: 2368.4783\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3381.2961 - val_loss: 2335.8425\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3327.9377 - val_loss: 2303.7351\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3275.2185 - val_loss: 2272.1514\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3223.1318 - val_loss: 2241.0923\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3171.6719 - val_loss: 2210.6333\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3120.8325 - val_loss: 2180.5720\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3070.6079 - val_loss: 2151.0249\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3020.9939 - val_loss: 2121.9744\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2971.9832 - val_loss: 2093.4119\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2923.5710 - val_loss: 2065.2126\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2868.5503 - val_loss: 2030.6644\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2812.5034 - val_loss: 1999.4094\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2759.3567 - val_loss: 1969.6407\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2708.1680 - val_loss: 1940.9741\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2658.4243 - val_loss: 1913.1949\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2609.8494 - val_loss: 1886.1860\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2562.2856 - val_loss: 1859.8734\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2515.6326 - val_loss: 1834.2080\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2469.8215 - val_loss: 1809.1532\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2424.8013 - val_loss: 1784.6818\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2380.5317 - val_loss: 1760.7711\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2336.9841 - val_loss: 1737.4034\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2294.1318 - val_loss: 1714.5624\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2251.9534 - val_loss: 1692.2350\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2210.4294 - val_loss: 1670.4087\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2169.5454 - val_loss: 1649.0740\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2129.2861 - val_loss: 1628.2200\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2089.6387 - val_loss: 1607.8380\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2050.5913 - val_loss: 1587.9200\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2012.1338 - val_loss: 1568.4576\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1974.2554 - val_loss: 1549.4436\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1936.9476 - val_loss: 1530.8699\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1900.2010 - val_loss: 1512.7285\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1864.0078 - val_loss: 1495.0087\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1828.3599 - val_loss: 1476.3318\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1785.9642 - val_loss: 1455.5125\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1744.3527 - val_loss: 1435.8492\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1704.9789 - val_loss: 1417.6343\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1667.2341 - val_loss: 1400.2919\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1630.7430 - val_loss: 1383.6711\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1595.2805 - val_loss: 1367.6837\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1560.7136 - val_loss: 1352.2733\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1526.9548 - val_loss: 1337.4001\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1493.9429 - val_loss: 1323.0343\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1461.6321 - val_loss: 1309.1527\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1429.9865 - val_loss: 1295.7356\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1398.9769 - val_loss: 1282.7675\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1368.5797 - val_loss: 1270.2339\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1338.7738 - val_loss: 1258.1227\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1309.5417 - val_loss: 1246.4231\n",
      "Epoch 84/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1280.8677 - val_loss: 1235.1239\n",
      "Epoch 85/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1252.7371 - val_loss: 1224.2169\n",
      "Epoch 86/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1225.1378 - val_loss: 1213.6930\n",
      "Epoch 87/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1198.0580 - val_loss: 1203.5441\n",
      "Epoch 88/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1171.4867 - val_loss: 1193.7625\n",
      "Epoch 89/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1145.4139 - val_loss: 1184.3411\n",
      "Epoch 90/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1119.8306 - val_loss: 1175.2728\n",
      "Epoch 91/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1094.7273 - val_loss: 1166.5509\n",
      "Epoch 92/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1070.0962 - val_loss: 1158.1691\n",
      "Epoch 93/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1045.9288 - val_loss: 1150.1212\n",
      "Epoch 94/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1022.2177 - val_loss: 1142.4008\n",
      "Epoch 95/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 998.9560 - val_loss: 1135.0027\n",
      "Epoch 96/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 976.1364 - val_loss: 1127.9205\n",
      "Epoch 97/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 953.7519 - val_loss: 1121.1492\n",
      "Epoch 98/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 931.7960 - val_loss: 1114.6832\n",
      "Epoch 99/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 910.2623 - val_loss: 1108.5170\n",
      "Epoch 100/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 889.1454 - val_loss: 1102.6453\n",
      "Epoch 101/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 868.4383 - val_loss: 1097.0634\n",
      "Epoch 102/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 848.1354 - val_loss: 1091.7656\n",
      "Epoch 103/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 828.2313 - val_loss: 1086.7472\n",
      "Epoch 104/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 808.7200 - val_loss: 1082.0035\n",
      "Epoch 105/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 789.5964 - val_loss: 1077.5294\n",
      "Epoch 106/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 770.8548 - val_loss: 1073.3199\n",
      "Epoch 107/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 752.4901 - val_loss: 1069.3704\n",
      "Epoch 108/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 734.4963 - val_loss: 1065.6764\n",
      "Epoch 109/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 716.8695 - val_loss: 1062.2332\n",
      "Epoch 110/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 699.6039 - val_loss: 1059.0358\n",
      "Epoch 111/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 682.6949 - val_loss: 1056.0796\n",
      "Epoch 112/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 666.1376 - val_loss: 1053.3608\n",
      "Epoch 113/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 649.9268 - val_loss: 1050.8741\n",
      "Epoch 114/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 634.0577 - val_loss: 1048.6155\n",
      "Epoch 115/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 618.5258 - val_loss: 1046.5803\n",
      "Epoch 116/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 603.3264 - val_loss: 1044.7643\n",
      "Epoch 117/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 588.4547 - val_loss: 1043.1630\n",
      "Epoch 118/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 573.9064 - val_loss: 1041.7719\n",
      "Epoch 119/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 559.6766 - val_loss: 1040.5869\n",
      "Epoch 120/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 545.7609 - val_loss: 1039.6034\n",
      "Epoch 121/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 532.1552 - val_loss: 1038.8176\n",
      "Epoch 122/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 518.8546 - val_loss: 1038.2247\n",
      "Epoch 123/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 505.8548 - val_loss: 1037.8207\n",
      "Epoch 124/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 493.1516 - val_loss: 1037.6014\n",
      "Epoch 125/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 480.7406 - val_loss: 1037.5626\n",
      "Epoch 126/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 468.6177 - val_loss: 1037.7003\n",
      "Epoch 127/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 456.7784 - val_loss: 1038.0101\n",
      "Epoch 128/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 445.2184 - val_loss: 1038.4879\n",
      "Epoch 129/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 433.9335 - val_loss: 1039.1294\n",
      "Epoch 130/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 422.9196 - val_loss: 1039.9308\n",
      "Epoch 131/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 412.1728 - val_loss: 1040.8879\n",
      "Epoch 132/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 401.6884 - val_loss: 1041.9968\n",
      "Epoch 133/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 391.4630 - val_loss: 1043.2528\n",
      "Epoch 134/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 381.4919 - val_loss: 1044.6531\n",
      "Epoch 135/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 371.7714 - val_loss: 1046.1924\n",
      "Epoch 136/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 362.2974 - val_loss: 1047.8676\n",
      "Epoch 137/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 353.0659 - val_loss: 1049.6741\n",
      "Epoch 138/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 344.0729 - val_loss: 1051.6084\n",
      "Epoch 139/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 335.3145 - val_loss: 1053.6663\n",
      "Epoch 140/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 326.7865 - val_loss: 1055.8440\n",
      "Epoch 141/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 318.4853 - val_loss: 1058.1378\n",
      "Epoch 142/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 310.4069 - val_loss: 1060.5437\n",
      "Epoch 143/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 302.5474 - val_loss: 1063.0576\n",
      "Epoch 144/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 294.9028 - val_loss: 1065.6758\n",
      "Epoch 145/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 287.4697 - val_loss: 1068.3947\n",
      "Epoch 146/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 280.2439 - val_loss: 1071.2103\n",
      "Epoch 147/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 273.2218 - val_loss: 1074.1189\n",
      "Epoch 148/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 266.3995 - val_loss: 1077.1168\n",
      "Epoch 149/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 259.7733 - val_loss: 1080.2003\n",
      "Epoch 150/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 253.3396 - val_loss: 1083.3656\n",
      "Epoch 151/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 247.0947 - val_loss: 1086.6093\n",
      "Epoch 152/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 241.0349 - val_loss: 1089.9274\n",
      "Epoch 153/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 235.1564 - val_loss: 1093.3167\n",
      "Epoch 154/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 229.4558 - val_loss: 1096.7732\n",
      "Epoch 155/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 223.9295 - val_loss: 1100.2937\n",
      "Epoch 156/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 218.5739 - val_loss: 1103.8745\n",
      "Epoch 157/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 213.3853 - val_loss: 1107.5122\n",
      "Epoch 158/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 208.3604 - val_loss: 1111.2032\n",
      "Epoch 159/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 203.4957 - val_loss: 1114.9446\n",
      "Epoch 160/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 198.7876 - val_loss: 1118.7322\n",
      "Epoch 161/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 194.2327 - val_loss: 1122.5631\n",
      "Epoch 162/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 189.8276 - val_loss: 1126.4344\n",
      "Epoch 163/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 185.5689 - val_loss: 1130.3423\n",
      "Epoch 164/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 181.4533 - val_loss: 1134.2834\n",
      "Epoch 165/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 177.4775 - val_loss: 1138.2551\n",
      "Epoch 166/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 173.6382 - val_loss: 1142.2539\n",
      "Epoch 167/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 169.9320 - val_loss: 1146.2765\n",
      "Epoch 168/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 166.3560 - val_loss: 1150.3204\n",
      "Epoch 169/500\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 162.9067 - val_loss: 1154.3821\n",
      "Epoch 170/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 159.5811 - val_loss: 1158.4586\n",
      "Epoch 171/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 156.3760 - val_loss: 1162.5479\n",
      "Epoch 172/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 153.2883 - val_loss: 1166.6455\n",
      "Epoch 173/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 150.3150 - val_loss: 1170.7500\n",
      "Epoch 174/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 147.4531 - val_loss: 1174.8579\n",
      "Epoch 175/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 144.6995 - val_loss: 1178.9663\n",
      "Epoch 176/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 142.0513 - val_loss: 1183.0732\n",
      "Epoch 177/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 139.5056 - val_loss: 1187.1760\n",
      "Epoch 178/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 137.0595 - val_loss: 1191.2714\n",
      "Epoch 179/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 134.7103 - val_loss: 1195.3575\n",
      "Epoch 180/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 132.4550 - val_loss: 1199.4316\n",
      "Epoch 181/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 130.2910 - val_loss: 1203.4916\n",
      "Epoch 182/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 128.2155 - val_loss: 1207.5344\n",
      "Epoch 183/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 126.2258 - val_loss: 1211.5587\n",
      "Epoch 184/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 124.3194 - val_loss: 1215.5613\n",
      "Epoch 185/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 122.4937 - val_loss: 1219.5411\n",
      "Epoch 186/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 120.7460 - val_loss: 1223.4954\n",
      "Epoch 187/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 119.0739 - val_loss: 1227.4221\n",
      "Epoch 188/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 117.4749 - val_loss: 1231.3197\n",
      "Epoch 189/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 115.9466 - val_loss: 1235.1857\n",
      "Epoch 190/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 114.4867 - val_loss: 1239.0192\n",
      "Epoch 191/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 113.0927 - val_loss: 1242.8173\n",
      "Epoch 192/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 111.7625 - val_loss: 1246.5787\n",
      "Epoch 193/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 110.4938 - val_loss: 1250.3026\n",
      "Epoch 194/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 109.2844 - val_loss: 1253.9865\n",
      "Epoch 195/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 108.1321 - val_loss: 1257.6289\n",
      "Epoch 196/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 107.0349 - val_loss: 1261.2284\n",
      "Epoch 197/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 105.9909 - val_loss: 1264.7842\n",
      "Epoch 198/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 104.9978 - val_loss: 1268.2944\n",
      "Epoch 199/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 104.0537 - val_loss: 1271.7585\n",
      "Epoch 200/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 103.1569 - val_loss: 1275.1752\n",
      "Epoch 201/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 102.3052 - val_loss: 1278.5425\n",
      "Epoch 202/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 101.4971 - val_loss: 1281.8606\n",
      "Epoch 203/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 100.7307 - val_loss: 1285.1278\n",
      "Epoch 204/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 100.0042 - val_loss: 1288.3435\n",
      "Epoch 205/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 99.3162 - val_loss: 1291.5068\n",
      "Epoch 206/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 98.6647 - val_loss: 1294.6168\n",
      "Epoch 207/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 98.0485 - val_loss: 1297.6733\n",
      "Epoch 208/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 97.4657 - val_loss: 1300.6754\n",
      "Epoch 209/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 96.9151 - val_loss: 1303.6228\n",
      "Epoch 210/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 96.3951 - val_loss: 1306.5144\n",
      "Epoch 211/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 95.9045 - val_loss: 1309.3506\n",
      "Epoch 212/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 95.4417 - val_loss: 1312.1302\n",
      "Epoch 213/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 95.0055 - val_loss: 1314.8540\n",
      "Epoch 214/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 94.5946 - val_loss: 1317.5211\n",
      "Epoch 215/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 94.2078 - val_loss: 1320.1311\n",
      "Epoch 216/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 93.8440 - val_loss: 1322.6842\n",
      "Epoch 217/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 93.5020 - val_loss: 1325.1804\n",
      "Epoch 218/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 93.1806 - val_loss: 1327.6200\n",
      "Epoch 219/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.8790 - val_loss: 1330.0022\n",
      "Epoch 220/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.5960 - val_loss: 1332.3284\n",
      "Epoch 221/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 92.3305 - val_loss: 1334.5978\n",
      "Epoch 222/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 92.0819 - val_loss: 1336.8115\n",
      "Epoch 223/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 91.8490 - val_loss: 1338.9688\n",
      "Epoch 224/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 91.6311 - val_loss: 1341.0704\n",
      "Epoch 225/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 91.4273 - val_loss: 1343.1172\n",
      "Epoch 226/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 91.2368 - val_loss: 1345.1089\n",
      "Epoch 227/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 91.0590 - val_loss: 1347.0458\n",
      "Epoch 228/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 90.8931 - val_loss: 1348.9291\n",
      "Epoch 229/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 90.7383 - val_loss: 1350.7598\n",
      "Epoch 230/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 90.5940 - val_loss: 1352.5375\n",
      "Epoch 231/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 90.4597 - val_loss: 1354.2628\n",
      "Epoch 232/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 90.3346 - val_loss: 1355.9364\n",
      "Epoch 233/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 90.2183 - val_loss: 1357.5592\n",
      "Epoch 234/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 90.1102 - val_loss: 1359.1327\n",
      "Epoch 235/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 90.0098 - val_loss: 1360.6565\n",
      "Epoch 236/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 89.9165 - val_loss: 1362.1322\n",
      "Epoch 237/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 89.8301 - val_loss: 1363.5593\n",
      "Epoch 238/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 89.7499 - val_loss: 1364.9398\n",
      "Epoch 239/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 89.6756 - val_loss: 1366.2739\n",
      "Epoch 240/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 89.6068 - val_loss: 1367.5629\n",
      "Epoch 241/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 89.5432 - val_loss: 1368.8080\n",
      "Epoch 242/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 89.4844 - val_loss: 1370.0092\n",
      "Epoch 243/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 89.4300 - val_loss: 1371.1676\n",
      "Epoch 244/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 89.3797 - val_loss: 1372.2844\n",
      "Epoch 245/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 89.3334 - val_loss: 1373.3605\n",
      "Epoch 246/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 89.2906 - val_loss: 1374.3967\n",
      "Epoch 247/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 89.2511 - val_loss: 1375.3944\n",
      "Epoch 248/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 89.2149 - val_loss: 1376.3539\n",
      "Epoch 249/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 89.1814 - val_loss: 1377.2765\n",
      "Epoch 250/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 89.1506 - val_loss: 1378.1635\n",
      "Epoch 251/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 89.1222 - val_loss: 1379.0145\n",
      "Epoch 252/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 89.0962 - val_loss: 1379.8318\n",
      "Epoch 253/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 89.0723 - val_loss: 1380.6163\n",
      "Epoch 254/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 89.0503 - val_loss: 1381.3684\n",
      "Epoch 255/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 89.0301 - val_loss: 1382.0890\n",
      "Epoch 256/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 89.0116 - val_loss: 1382.7788\n",
      "Epoch 257/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 88.9946 - val_loss: 1383.4396\n",
      "Epoch 258/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 88.9790 - val_loss: 1384.0714\n",
      "Epoch 259/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.9648 - val_loss: 1384.6760\n",
      "Epoch 260/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.9517 - val_loss: 1385.2532\n",
      "Epoch 261/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.9397 - val_loss: 1385.8051\n",
      "Epoch 262/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.9288 - val_loss: 1386.3322\n",
      "Epoch 263/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.9188 - val_loss: 1386.8350\n",
      "Epoch 264/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.9097 - val_loss: 1387.3143\n",
      "Epoch 265/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.9013 - val_loss: 1387.7708\n",
      "Epoch 266/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8937 - val_loss: 1388.2057\n",
      "Epoch 267/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8867 - val_loss: 1388.6202\n",
      "Epoch 268/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8804 - val_loss: 1389.0144\n",
      "Epoch 269/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8746 - val_loss: 1389.3896\n",
      "Epoch 270/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8693 - val_loss: 1389.7457\n",
      "Epoch 271/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8645 - val_loss: 1390.0840\n",
      "Epoch 272/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8602 - val_loss: 1390.4052\n",
      "Epoch 273/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8561 - val_loss: 1390.7098\n",
      "Epoch 274/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8526 - val_loss: 1390.9993\n",
      "Epoch 275/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8493 - val_loss: 1391.2729\n",
      "Epoch 276/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8463 - val_loss: 1391.5326\n",
      "Epoch 277/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8436 - val_loss: 1391.7782\n",
      "Epoch 278/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8411 - val_loss: 1392.0104\n",
      "Epoch 279/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8389 - val_loss: 1392.2299\n",
      "Epoch 280/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8370 - val_loss: 1392.4376\n",
      "Epoch 281/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8352 - val_loss: 1392.6344\n",
      "Epoch 282/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8335 - val_loss: 1392.8197\n",
      "Epoch 283/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8321 - val_loss: 1392.9941\n",
      "Epoch 284/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8308 - val_loss: 1393.1591\n",
      "Epoch 285/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8297 - val_loss: 1393.3145\n",
      "Epoch 286/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 88.8286 - val_loss: 1393.4602\n",
      "Epoch 287/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8277 - val_loss: 1393.5978\n",
      "Epoch 288/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8269 - val_loss: 1393.7273\n",
      "Epoch 289/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8262 - val_loss: 1393.8488\n",
      "Epoch 290/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8256 - val_loss: 1393.9630\n",
      "Epoch 291/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8250 - val_loss: 1394.0709\n",
      "Epoch 292/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8246 - val_loss: 1394.1718\n",
      "Epoch 293/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8242 - val_loss: 1394.2660\n",
      "Epoch 294/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8239 - val_loss: 1394.3542\n",
      "Epoch 295/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8236 - val_loss: 1394.4373\n",
      "Epoch 296/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8235 - val_loss: 1394.5151\n",
      "Epoch 297/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8233 - val_loss: 1394.5879\n",
      "Epoch 298/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8232 - val_loss: 1394.6558\n",
      "Epoch 299/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8231 - val_loss: 1394.7195\n",
      "Epoch 300/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8231 - val_loss: 1394.7789\n",
      "Epoch 301/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8232 - val_loss: 1394.8348\n",
      "Epoch 302/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8232 - val_loss: 1394.8868\n",
      "Epoch 303/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8233 - val_loss: 1394.9351\n",
      "Epoch 304/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8234 - val_loss: 1394.9802\n",
      "Epoch 305/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8235 - val_loss: 1395.0219\n",
      "Epoch 306/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8237 - val_loss: 1395.0609\n",
      "Epoch 307/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8239 - val_loss: 1395.0969\n",
      "Epoch 308/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8241 - val_loss: 1395.1311\n",
      "Epoch 309/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8243 - val_loss: 1395.1620\n",
      "Epoch 310/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8245 - val_loss: 1395.1910\n",
      "Epoch 311/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8248 - val_loss: 1395.2180\n",
      "Epoch 312/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 88.8250 - val_loss: 1395.2428\n",
      "Epoch 313/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8253 - val_loss: 1395.2655\n",
      "Epoch 314/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8256 - val_loss: 1395.2874\n",
      "Epoch 315/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8260 - val_loss: 1395.3077\n",
      "Epoch 316/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8263 - val_loss: 1395.3259\n",
      "Epoch 317/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8266 - val_loss: 1395.3429\n",
      "Epoch 318/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8269 - val_loss: 1395.3580\n",
      "Epoch 319/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8273 - val_loss: 1395.3728\n",
      "Epoch 320/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 88.8276 - val_loss: 1395.3859\n",
      "Epoch 321/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8279 - val_loss: 1395.3973\n",
      "Epoch 322/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8283 - val_loss: 1395.4091\n",
      "Epoch 323/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8287 - val_loss: 1395.4188\n",
      "Epoch 324/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8290 - val_loss: 1395.4291\n",
      "Epoch 325/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8294 - val_loss: 1395.4370\n",
      "Epoch 326/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8298 - val_loss: 1395.4453\n",
      "Epoch 327/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8302 - val_loss: 1395.4523\n",
      "Epoch 328/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8305 - val_loss: 1395.4596\n",
      "Epoch 329/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8309 - val_loss: 1395.4653\n",
      "Epoch 330/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8313 - val_loss: 1395.4709\n",
      "Epoch 331/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8316 - val_loss: 1395.4762\n",
      "Epoch 332/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8320 - val_loss: 1395.4802\n",
      "Epoch 333/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8324 - val_loss: 1395.4844\n",
      "Epoch 334/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8327 - val_loss: 1395.4880\n",
      "Epoch 335/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8331 - val_loss: 1395.4912\n",
      "Epoch 336/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8335 - val_loss: 1395.4943\n",
      "Epoch 337/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8339 - val_loss: 1395.4976\n",
      "Epoch 338/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8343 - val_loss: 1395.4998\n",
      "Epoch 339/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8346 - val_loss: 1395.5022\n",
      "Epoch 340/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8350 - val_loss: 1395.5042\n",
      "Epoch 341/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8353 - val_loss: 1395.5059\n",
      "Epoch 342/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 88.8357 - val_loss: 1395.5072\n",
      "Epoch 343/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8361 - val_loss: 1395.5078\n",
      "Epoch 344/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8364 - val_loss: 1395.5093\n",
      "Epoch 345/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8368 - val_loss: 1395.5099\n",
      "Epoch 346/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8371 - val_loss: 1395.5115\n",
      "Epoch 347/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8374 - val_loss: 1395.5116\n",
      "Epoch 348/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8378 - val_loss: 1395.5121\n",
      "Epoch 349/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8382 - val_loss: 1395.5128\n",
      "Epoch 350/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8385 - val_loss: 1395.5128\n",
      "Epoch 351/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8388 - val_loss: 1395.5134\n",
      "Epoch 352/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8392 - val_loss: 1395.5134\n",
      "Epoch 353/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8395 - val_loss: 1395.5132\n",
      "Epoch 354/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8398 - val_loss: 1395.5132\n",
      "Epoch 355/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8401 - val_loss: 1395.5137\n",
      "Epoch 356/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8405 - val_loss: 1395.5133\n",
      "Epoch 357/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8407 - val_loss: 1395.5131\n",
      "Epoch 358/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8410 - val_loss: 1395.5126\n",
      "Epoch 359/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8413 - val_loss: 1395.5121\n",
      "Epoch 360/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8417 - val_loss: 1395.5118\n",
      "Epoch 361/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8420 - val_loss: 1395.5116\n",
      "Epoch 362/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8423 - val_loss: 1395.5110\n",
      "Epoch 363/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8426 - val_loss: 1395.5109\n",
      "Epoch 364/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8428 - val_loss: 1395.5107\n",
      "Epoch 365/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8431 - val_loss: 1395.5099\n",
      "Epoch 366/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8434 - val_loss: 1395.5099\n",
      "Epoch 367/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 88.8437 - val_loss: 1395.5098\n",
      "Epoch 368/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8439 - val_loss: 1395.5093\n",
      "Epoch 369/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8442 - val_loss: 1395.5084\n",
      "Epoch 370/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8445 - val_loss: 1395.5082\n",
      "Epoch 371/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8447 - val_loss: 1395.5078\n",
      "Epoch 372/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8450 - val_loss: 1395.5077\n",
      "Epoch 373/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8452 - val_loss: 1395.5073\n",
      "Epoch 374/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8454 - val_loss: 1395.5062\n",
      "Epoch 375/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8457 - val_loss: 1395.5055\n",
      "Epoch 376/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8459 - val_loss: 1395.5055\n",
      "Epoch 377/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8462 - val_loss: 1395.5043\n",
      "Epoch 378/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8465 - val_loss: 1395.5043\n",
      "Epoch 379/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8467 - val_loss: 1395.5043\n",
      "Epoch 380/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8469 - val_loss: 1395.5040\n",
      "Epoch 381/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8471 - val_loss: 1395.5037\n",
      "Epoch 382/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8473 - val_loss: 1395.5038\n",
      "Epoch 383/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8475 - val_loss: 1395.5033\n",
      "Epoch 384/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8477 - val_loss: 1395.5031\n",
      "Epoch 385/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8480 - val_loss: 1395.5031\n",
      "Epoch 386/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8481 - val_loss: 1395.5028\n",
      "Epoch 387/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8483 - val_loss: 1395.5020\n",
      "Epoch 388/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8485 - val_loss: 1395.5011\n",
      "Epoch 389/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8487 - val_loss: 1395.5009\n",
      "Epoch 390/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8489 - val_loss: 1395.5000\n",
      "Epoch 391/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8491 - val_loss: 1395.4993\n",
      "Epoch 392/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8493 - val_loss: 1395.4990\n",
      "Epoch 393/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 88.8495 - val_loss: 1395.4987\n",
      "Epoch 394/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8496 - val_loss: 1395.4985\n",
      "Epoch 395/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8498 - val_loss: 1395.4973\n",
      "Epoch 396/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8500 - val_loss: 1395.4973\n",
      "Epoch 397/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8502 - val_loss: 1395.4969\n",
      "Epoch 398/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8504 - val_loss: 1395.4967\n",
      "Epoch 399/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8505 - val_loss: 1395.4965\n",
      "Epoch 400/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8507 - val_loss: 1395.4960\n",
      "Epoch 401/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8508 - val_loss: 1395.4952\n",
      "Epoch 402/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8510 - val_loss: 1395.4952\n",
      "Epoch 403/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8511 - val_loss: 1395.4949\n",
      "Epoch 404/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8513 - val_loss: 1395.4948\n",
      "Epoch 405/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8514 - val_loss: 1395.4943\n",
      "Epoch 406/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8516 - val_loss: 1395.4943\n",
      "Epoch 407/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8517 - val_loss: 1395.4933\n",
      "Epoch 408/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8519 - val_loss: 1395.4929\n",
      "Epoch 409/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8520 - val_loss: 1395.4924\n",
      "Epoch 410/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8521 - val_loss: 1395.4919\n",
      "Epoch 411/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8523 - val_loss: 1395.4918\n",
      "Epoch 412/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8524 - val_loss: 1395.4915\n",
      "Epoch 413/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8525 - val_loss: 1395.4912\n",
      "Epoch 414/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8526 - val_loss: 1395.4911\n",
      "Epoch 415/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8527 - val_loss: 1395.4907\n",
      "Epoch 416/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 88.8528 - val_loss: 1395.4905\n",
      "Epoch 417/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8529 - val_loss: 1395.4902\n",
      "Epoch 418/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8531 - val_loss: 1395.4901\n",
      "Epoch 419/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8532 - val_loss: 1395.4899\n",
      "Epoch 420/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8533 - val_loss: 1395.4897\n",
      "Epoch 421/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8534 - val_loss: 1395.4896\n",
      "Epoch 422/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8535 - val_loss: 1395.4893\n",
      "Epoch 423/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8536 - val_loss: 1395.4885\n",
      "Epoch 424/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8537 - val_loss: 1395.4883\n",
      "Epoch 425/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8538 - val_loss: 1395.4885\n",
      "Epoch 426/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8539 - val_loss: 1395.4883\n",
      "Epoch 427/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8540 - val_loss: 1395.4882\n",
      "Epoch 428/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8541 - val_loss: 1395.4880\n",
      "Epoch 429/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8542 - val_loss: 1395.4879\n",
      "Epoch 430/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8543 - val_loss: 1395.4875\n",
      "Epoch 431/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8543 - val_loss: 1395.4868\n",
      "Epoch 432/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8545 - val_loss: 1395.4875\n",
      "Epoch 433/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8545 - val_loss: 1395.4874\n",
      "Epoch 434/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8546 - val_loss: 1395.4869\n",
      "Epoch 435/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8547 - val_loss: 1395.4868\n",
      "Epoch 436/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8548 - val_loss: 1395.4869\n",
      "Epoch 437/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8548 - val_loss: 1395.4865\n",
      "Epoch 438/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8549 - val_loss: 1395.4861\n",
      "Epoch 439/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8550 - val_loss: 1395.4861\n",
      "Epoch 440/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8551 - val_loss: 1395.4862\n",
      "Epoch 441/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8552 - val_loss: 1395.4862\n",
      "Epoch 442/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8552 - val_loss: 1395.4862\n",
      "Epoch 443/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 88.8553 - val_loss: 1395.4862\n",
      "Epoch 444/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 88.8554 - val_loss: 1395.4862\n",
      "Epoch 445/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 88.8555 - val_loss: 1395.4862\n",
      "Epoch 446/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8555 - val_loss: 1395.4858\n",
      "Epoch 447/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8556 - val_loss: 1395.4858\n",
      "Epoch 448/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8556 - val_loss: 1395.4855\n",
      "Epoch 449/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8557 - val_loss: 1395.4855\n",
      "Epoch 450/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8558 - val_loss: 1395.4856\n",
      "Epoch 451/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8558 - val_loss: 1395.4855\n",
      "Epoch 452/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8559 - val_loss: 1395.4855\n",
      "Epoch 453/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8559 - val_loss: 1395.4849\n",
      "Epoch 454/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8560 - val_loss: 1395.4849\n",
      "Epoch 455/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8560 - val_loss: 1395.4849\n",
      "Epoch 456/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8561 - val_loss: 1395.4843\n",
      "Epoch 457/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8561 - val_loss: 1395.4849\n",
      "Epoch 458/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8561 - val_loss: 1395.4843\n",
      "Epoch 459/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8562 - val_loss: 1395.4841\n",
      "Epoch 460/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8563 - val_loss: 1395.4839\n",
      "Epoch 461/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8563 - val_loss: 1395.4836\n",
      "Epoch 462/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8564 - val_loss: 1395.4838\n",
      "Epoch 463/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8565 - val_loss: 1395.4839\n",
      "Epoch 464/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8565 - val_loss: 1395.4839\n",
      "Epoch 465/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8565 - val_loss: 1395.4835\n",
      "Epoch 466/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 88.8566 - val_loss: 1395.4835\n",
      "Epoch 467/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8566 - val_loss: 1395.4835\n",
      "Epoch 468/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8566 - val_loss: 1395.4832\n",
      "Epoch 469/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8567 - val_loss: 1395.4829\n",
      "Epoch 470/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8567 - val_loss: 1395.4825\n",
      "Epoch 471/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8568 - val_loss: 1395.4829\n",
      "Epoch 472/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8568 - val_loss: 1395.4829\n",
      "Epoch 473/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8568 - val_loss: 1395.4824\n",
      "Epoch 474/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8569 - val_loss: 1395.4824\n",
      "Epoch 475/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8569 - val_loss: 1395.4824\n",
      "Epoch 476/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8569 - val_loss: 1395.4823\n",
      "Epoch 477/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8569 - val_loss: 1395.4817\n",
      "Epoch 478/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8570 - val_loss: 1395.4816\n",
      "Epoch 479/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8570 - val_loss: 1395.4814\n",
      "Epoch 480/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8570 - val_loss: 1395.4816\n",
      "Epoch 481/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8571 - val_loss: 1395.4817\n",
      "Epoch 482/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 88.8571 - val_loss: 1395.4817\n",
      "Epoch 483/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8572 - val_loss: 1395.4817\n",
      "Epoch 484/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8572 - val_loss: 1395.4817\n",
      "Epoch 485/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8572 - val_loss: 1395.4814\n",
      "Epoch 486/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8572 - val_loss: 1395.4813\n",
      "Epoch 487/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8573 - val_loss: 1395.4812\n",
      "Epoch 488/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8573 - val_loss: 1395.4812\n",
      "Epoch 489/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 88.8573 - val_loss: 1395.4812\n",
      "Epoch 490/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8574 - val_loss: 1395.4812\n",
      "Epoch 491/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8574 - val_loss: 1395.4812\n",
      "Epoch 492/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8574 - val_loss: 1395.4812\n",
      "Epoch 493/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8575 - val_loss: 1395.4814\n",
      "Epoch 494/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8575 - val_loss: 1395.4812\n",
      "Epoch 495/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8575 - val_loss: 1395.4814\n",
      "Epoch 496/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8575 - val_loss: 1395.4812\n",
      "Epoch 497/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8575 - val_loss: 1395.4811\n",
      "Epoch 498/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8576 - val_loss: 1395.4814\n",
      "Epoch 499/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 88.8576 - val_loss: 1395.4814\n",
      "Epoch 500/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 88.8576 - val_loss: 1395.4814\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.0403, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0012, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0120, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.0334, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 336ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.30641223e+01, 7.29548786e+01, 7.28456349e+01, 7.27363912e+01,\n",
       "        7.26271475e+01, 7.25179038e+01, 7.24508170e+01, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.25387630e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.71254400e-01, 7.39044585e+01, 7.38624416e+01, 7.38204248e+01,\n",
       "        7.37438609e+01, 7.36346172e+01, 7.35253735e+01, 7.34161298e+01,\n",
       "        7.33068861e+01, 7.31976424e+01, 7.30883987e+01, 7.29791550e+01,\n",
       "        7.28699113e+01, 7.27606676e+01, 7.26514239e+01, 7.25421802e+01,\n",
       "        7.24638889e+01, 7.53777824e+01, 7.51408077e+01, 7.49038329e+01,\n",
       "        7.46668581e+01, 7.45240103e+01, 7.44584641e+01, 7.43929178e+01,\n",
       "        7.43273716e+01, 7.42265873e+01, 1.51239230e-01, 1.78270120e-01,\n",
       "        0.00000000e+00, 6.35062690e-01, 0.00000000e+00, 1.22059360e-01,\n",
       "        0.00000000e+00, 2.97363040e-01, 1.29221230e-01, 7.27849440e+01,\n",
       "        7.26757003e+01, 7.25664566e+01, 7.24769608e+01, 7.54304435e+01,\n",
       "        7.51934687e+01, 7.49564939e+01, 7.47195191e+01, 7.45385761e+01,\n",
       "        7.44730299e+01, 7.44074837e+01, 7.43419374e+01, 7.42545985e+01,\n",
       "        0.00000000e+00, 9.75417440e+01, 7.45790896e+01, 7.44997339e+01,\n",
       "        7.44341877e+01, 7.43686415e+01, 7.43030952e+01, 7.41799020e+01,\n",
       "        7.40538515e+01, 7.39278011e+01, 7.38017507e+01, 0.00000000e+00,\n",
       "        0.00000000e+00, 5.32386131e+01, 2.18327340e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        6.93092117e+01, 1.16420841e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.77224857e-01, 0.00000000e+00, 0.00000000e+00, 7.27132559e-01,\n",
       "        1.63542360e-01, 3.64421695e-01, 0.00000000e+00, 5.48217833e-01,\n",
       "        0.00000000e+00, 2.05416068e-01, 4.69413579e-01, 5.66975355e-01,\n",
       "        3.22637320e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([68.87934686, 68.87306142, 68.86677598, 68.86049054, 68.8542051 ,\n",
       "       68.84791966, 68.84163421, 68.83534877, 68.82906333, 68.82277789,\n",
       "       68.81649245, 68.81020701, 68.80392157, 68.79763613, 68.79135069,\n",
       "       68.78506525, 68.7787798 , 68.77249436, 68.76620892, 68.75992348,\n",
       "       68.75363804, 68.7473526 , 68.74106716, 68.73478172, 68.72849628,\n",
       "       68.72221084, 68.71592539, 68.70963995, 68.70335451, 68.69706907,\n",
       "       68.69078363, 68.68449819, 68.67821275, 68.67192731, 68.66564187,\n",
       "       68.65935643, 68.65307098, 68.64678554, 68.6405001 , 68.63421466,\n",
       "       68.62792922, 68.62164378, 68.61535834, 68.6090729 , 68.60278746,\n",
       "       68.59650202, 68.59021657, 68.58393113, 68.57764569, 68.57136025,\n",
       "       68.56507481, 68.55878937, 68.55250393, 68.54621849, 68.53993305,\n",
       "       68.53364761, 68.52736216, 68.52107672, 68.51479128, 68.50850584,\n",
       "       68.5022204 , 68.49593496, 68.48964952, 68.48336408, 68.47707864,\n",
       "       68.4707932 , 68.46450775, 68.45822231, 68.45193687, 68.44565143,\n",
       "       68.43936599, 68.43308055, 68.42679511, 68.42050967, 68.41422423,\n",
       "       68.40793879, 68.40165334, 68.3953679 , 68.38908246, 68.38279702,\n",
       "       68.37651158, 68.37022614, 68.3639407 , 68.35765526, 68.35136982,\n",
       "       68.34508438, 68.33879893, 68.33251349, 68.32622805, 68.31994261,\n",
       "       68.31365717, 68.30737173, 68.30108629, 68.29480085, 68.28851541,\n",
       "       68.28222997, 68.27594452, 68.26965908, 68.26337364, 68.2570882 ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.07554828227279\n",
      "32.5897874620911\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
