{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "2495    67.025142\n",
       "2496    67.018856\n",
       "2497    67.012571\n",
       "2498    67.006285\n",
       "2499    67.000000\n",
       "Name: C3, Length: 2500, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c3_interpolated_2400_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "2395     0.549756\n",
       "2396     0.226634\n",
       "2397     0.369678\n",
       "2398     0.067416\n",
       "2399     0.000000\n",
       "Name: C3, Length: 2400, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2400)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.100000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.875910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.651821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90.427731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90.203641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     91.100000  0.000298\n",
       "1     90.875910  0.000298\n",
       "2     90.651821  0.000297\n",
       "3     90.427731  0.000297\n",
       "4     90.203641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD4CAYAAAAQP7oXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAAsTAAALEwEAmpwYAAAu2klEQVR4nO3deXhU5dn48e892SAhCdlICAlJICAiO5FNRBEFXCruRfsqrVq01b61i321vlbra1u1q+3Ptmq1VWzF1g1UFEERF9ag7DshrAFCCFuAhCTP7485M0yGSTKTmWS2+3NduXLmnOfMPM9Mcu551iPGGJRSSilv2IKdAaWUUuFDg4ZSSimvadBQSinlNQ0aSimlvKZBQymllNdig52BtsjMzDSFhYXBzoZSSoWVFStWHDTGZPnzHGEZNAoLCyktLQ12NpRSKqyIyA5/n0Obp5RSSnlNg4ZSSimvBSRoiMhkEdkkIltF5AEPx8eJyJciUi8iN7gdaxCRldbP7EDkRymlVPvwu09DRGKAZ4DLgN3AchGZbYxZ75JsJ/BN4McenuKkMWaIv/lQSinV/gLRET4C2GqMKQMQkZnAFMAZNIwx5daxxgC8nlJKqSAJRPNUD2CXy+Pd1j5vdRKRUhFZIiLXNJdIRKZb6UorKyvbmFWllFL+CIWO8AJjTAlwC/AHEentKZEx5jljTIkxpiQry69hxkoppdooEEFjD5Dv8jjP2ucVY8we63cZ8AkwNAB5UkpFiZraen734SZW7joc7Kw06+9fbOeDtRXBzkZABCJoLAf6iEiRiMQDUwGvRkGJSJqIJFjbmcAFuPSFKKVUa2rq6vnjx1tZu+dIsLPSrH8sKueDtfuCnY2A8DtoGGPqgXuBucAG4N/GmHUi8piIXA0gIueLyG7gRuBZEVlnnX4uUCoiq4AFwBNuo66UUkqFkIAsI2KMmQPMcdv3M5ft5dibrdzPWwQMDEQelFIqlEXKPVJDoSNcKaUimgQ7AwGkQUMppZTXNGgopcJbmLT7mDDJZ2s0aCilIoKEcBuQhHLmfKRBQymllNc0aCilVAeIkNYpDRpKKdXeIqdxSoOGUkopH2jQUEqFtXBp9jERMnxKg4ZSKiJIKDcChXDWfKVBQymllNc0aCilVAeIjMYpDRpKKdXuIqh1SoOGUkop72nQUEqFtbAZlBQu+WyFBg2lVEQI5eWddO0ppZRSPjERUtXQoKGUUu0scuoZGjSUUkr5QIOGUkp1gLDpsG+FBg2lVFgLh76CCOoH16ChlIoMEXRdDmkaNJRSqgNo85RSSimvhPQKvD7SoKGUUsprURU0fj13Iy8vLg92NpRSUSgcOuy9EVVB4/MtB5m3fn+ws6GUCqBw6CvQ0VNuRGSyiGwSka0i8oCH4+NE5EsRqReRG9yOTRORLdbPtEDkpzkFGUmUV9W050sopYIkki7MoczvoCEiMcAzwOVAf+BmEenvlmwn8E3gX27npgOPACOBEcAjIpLmb56aU5iRyJ7qk9TVN7bXSyillEfhUCPyRiBqGiOArcaYMmNMHTATmOKawBhTboxZDbhfrScB84wxh4wx1cA8YHIA8uRRQUYSjQZ2V59or5dQSqmIFoig0QPY5fJ4t7Wvvc/1WWFmIgA7qjRoKKVUW4RNR7iITBeRUhEpraysbNNzFGQkAWi/hlKqw0VI61RAgsYeIN/lcZ61L6DnGmOeM8aUGGNKsrKy2pTRjKR4uiTEak1DqQgSDhdjvQlTU8uBPiJSJCLxwFRgtpfnzgUmikia1QE+0drXLkSEgoxErWkoFYEiadZ1KPM7aBhj6oF7sV/sNwD/NsasE5HHRORqABE5X0R2AzcCz4rIOuvcQ8D/YQ88y4HHrH3tpjAjifKDGjSUUh0rUkZPxQbiSYwxc4A5bvt+5rK9HHvTk6dzXwReDEQ+vNE/N4X31lSws+oEPTMSO+pllVJRLJLqQGHTER4o1w3rgU3gtdKdwc6KUkqFnagLGt1TO3PxOd34T+lu6ht0kp9SqqNERvtU1AUNgKnn53PgWC0LNrVt6K5SKnSYMOgsiKDBU9EZNC7p141uyQnMXKZNVEpFjAi6MIeyqAwasTE2bhiex4JNByirPB7s7CilokAYVIi8EpVBA+DW0QWkdI5j+owVHD11OtjZUUpFMG2eigDdUzvz1/8aTvnBGu7911faKa6UUl6I2qABMKpXBr+4dgCfbq7k8fc2BDs7SqkIFiGtU4GZ3BfOvn5+T7bsP87fPt9O725duHVUQbCzpJTyQTj0FUTSEidRHzQAHrziXMoO1vDo7HXUNzQyuncGxVldiI2J6oqYUmElci7LoU2DBhBjE56eOoRbnl/Kz99ZD0CnOBvndk9haH4a94zvTUaXhCDnUikVzsJhPok3NGhYkjvFMeueCyg7WMPaPUdYY/28smQHc9ft49lbhzOgR2qws6mUCkORNHpKg4YLm00o7taF4m5duGao/QaCa3Yf4a4ZpVz/l0U8ef0g536llPJFZNQzonz0lDcG5qUy+3tjGZzflfteW8nj767X4blKqailQcMLmV0S+OedI5k2uoC/fb6daX9fxoGjp4KdLaVUmIig1ikNGt6Ki7Hx8ykDeOqGQSwvr2bCbxcyY8kOGhsjpdKpVHgL9VuqRkg/uAYNX91Uks/c+8YxMC+Vh99ey/V/XcTGfUeDnS2lVCgL8YDmCw0abVCUmcQ/7xzJ724azI6qE1z1x8954v2NnKxrCHbWlFKqXWnQaCMR4bpheXz0w4u4flgef124jct+v5AFmw4EO2tKqRAUIa1TGjT8lZYUz5M3DOK16aNIiLXxrb8v555/fakd5Uopp8hpnNJ5GgEzslcGc75/Ic8tLONPC7by8YYDTDovmylDejC2TyZxuiSJUu0iUjqYw4UGjQBKiI3hexP6cNXgXJ77tIw5ayp4e+Ve0pPiuXJgd6YMyWVYzzRstkj63qFUaAj1/ypdRkQ1qygziV9dN5CfX30en26uZNaqvfxnxS5mLNlBj66duXpILlOG5NIvJyXYWVVKdYAIGjylQaM9xcfauLR/Npf2z+Z4bT3z1u/j7a/28tynZfzlk230y0nm6iG5XD04l7y0xGBnVymlWqVBo4N0SYjl2qF5XDs0j4PHa5mzpoJZK/fy1AebeOqDTZQUpDFlaA+uGJCjK+oqpUKWBo0gyOySwG2jC7ltdCG7Dp1g9qq9vP3VHh5+ey0/m7WWc7KTGdUrg5FF6YwoStcgolSYi6DWKQ0awZafnsg944v57sW92VBxjPkb9rN0exUzl+/kH4vKAejTrQsjitIZ2SuDUUXpdEvpFNxMKxVCTMTMgAgPGjRChIjQPzeF/rkpQB/q6htZs+cwS7cfYmnZId7+ag//XLoTsHe0jyxKZ2SvdEYUZdCja+fgZl6pEBDqnc0RMngqMEFDRCYDTwMxwN+MMU+4HU8AXgaGA1XA140x5SJSCGwANllJlxhj7g5EnsJdfKyN4QXpDC9I57sXQ31DI+v2HmXp9iqWlh3ivTUVzFy+C4CCjETuHV/M9cPydDivUiEo1BdT9IXfQUNEYoBngMuA3cByEZltjFnvkuwOoNoYUywiU4Enga9bx7YZY4b4m49IFxtjY3B+Vwbnd2X6uN40NBo27jvK0rJDzFq1l/tfX80rS3fy6Nf6M7RnWrCzq5SKUIGYpjwC2GqMKTPG1AEzgSluaaYAL1nbrwMTJJJCbxDE2ITzclO5fWwRb31nDL+7aTAVh09y7Z8X8cPXVrJflzFRKqRESt9LIIJGD2CXy+Pd1j6PaYwx9cARIMM6ViQiX4nIQhG5sLkXEZHpIlIqIqWVlZUByHbksNnsiyd+/OOL+e7FvXl3dQXjf/MJf/5kK6dO68q7SgVbJH1DDvaCSBVAT2PMUOCHwL9ExOM0aWPMc8aYEmNMSVZWVodmMlx0SYjlJ5P7Me+H47igOJOnPtjExN9/yofr9kXMEgZKudM/7Y4ViKCxB8h3eZxn7fOYRkRigVSgyhhTa4ypAjDGrAC2AX0DkKeoVpCRxPO3lTDjjhEkxNqYPmMFt724jC37jwU7a0q1m1Bv8I6U4BaIoLEc6CMiRSISD0wFZrulmQ1Ms7ZvAD42xhgRybI60hGRXkAfoCwAeVLAhX2ymPP9C3nka/1Zteswk5/+jEdnr+PIidPBzppSUSXUA5ov/A4aVh/FvcBc7MNn/22MWScij4nI1VayF4AMEdmKvRnqAWv/OGC1iKzE3kF+tzHmkL95UmfExdj41gVFfHL/eKaen8/Li8u5+DcLeGXJDmrrtb9DKeWbgMzTMMbMAea47fuZy/Yp4EYP570BvBGIPKiWpSfF84trB/KNkQX8/J11/K+1ZEleWiJFmUn0ykqiV1YXelnbOSmdImpsuVLBFinNUzojPMr0z01h5vRRfLK5kq92Hmb7wRrKKo+zvPwQJ1zucd45LoaizCSKspLobf3uldmFoqwkUjrFBbEESoUfiaDxUxo0opCIMP6cbow/p5tznzGG/UdrKas8TtnBGsoqayg7eJy1e47w/poKGl2+JWV2SXDWSHplJXFOTgrn5iSTlZygtRPV4SLkC3zY0KChAHsgyUntRE5qJ8YUZzY5VlvfwK5DJ9hWWeOsmZRV1jBv/X6qauqc6TKS4unXPZlzc1Lo1z2Fc7snU9ytCwmxMR1dHNUGDY2GhZsPMP6cbiEV/BdtPciwgjQ6xbX8dxTq3+bdJ/edOt3Aql2HGdkro5kzQpMGDdWqhNgYirslU9wt+axjh0/UsXHfMTZUHGVDxVE27jvGjCU7qK1vBCDWJvTO6kK/7skM7JHK6N4ZnJuTomtkhaDnPyvjifc38uytw5l0Xo5X55RVHufwydMMa6ela8oqj3PL35Zy/bA8fnvTYK/OOXD0FDsPnWBQXlfiY4M9Fc3i4c/9obfW8saXu/nkxxdTmJnU8XlqIw0ayi9dE+MZ1SuDUS7fluobGimvOmEFkaNsqDjG8u2HmLVyL2DvlB/dO4OxxZmMLc4kP13vWhgKdh46AUDlsVqvz3n+szI+2nCAZQ9d6vU5x2vriY+xeXVBP3LSPjx86wHv5xjNXbePh2eto/R/LyXTy3vROFZOaK02E0gbKo4C9vcjnGjQUAEXG2OjuFsXirt14WuDc5379x05xaJtB/l860G+2HqQ91ZXAJCf3pmxxZmM6Z3JmN4ZetOpIGnL6J7GRt/nIAx4ZC7nF6bxn7vHtJ4nx4YPL+Lof/MlWyN/+RFHTp6m/IkrfTjLNzp6Sikf5aR24rpheVw3LA9jDNsqj/PF1io+33qQd1dV8Ooy+xJm/buncEFxBhcUZzKiKJ3EeP0z7Rj2q5ovQcBgsLWh/2N5ebV3z9+GAOBYMseXfDlqNO3FU07CNYbof6MKChFx9pNMG1NIfUMja/Yc4Yut9prIS4t28Pxn20mKj+Gm8/P51pgiemZoM1Z7OnOB9u1bfVuChvdaD2Tu66o5ahrtmy//tSW4hQINGiokxMbYGNozjaE907j3kj6crGtgWfkh3vpyNzMW7+Afi8qZ2D+bO8b24vzCtJAa3RMpnEHDh7e20Zh2XSLDl5qGIx+N1kkSIn3gDu41i7a836FAg4YKSZ3jY7iobxYX9c3igcvP5eXF5fxr2U7mrtvPwB6p3DG2iCsGdg+d0TERxLemoPa96DkutL58G29Lk1Z7Ezm7T8O0oTkwFOh/nAp5Oamd+Mnkfix+YAKPXzOAmrp67nttJRc+9THPLNjK4RN1rT+JalVbbhJkTNv6NLzV2Ni2fhYIwWYf96DRhubAUKA1DRU2OsfH8F+jCrhlRE8Wbq7khc+38+u5m/jTx1u4flget48tondWl2BnM2y1rXmqYy7OvvazQAgGDTeOGBLi2TyLBg0Vdmw2YXy/bozv140NFUd58fPt/Kd0N/9cupPx52TxzQuKGFmU3qFj7iOB8yLm0wW6nfs0HBs+9rNAaF2M7e9p06qGoyM8hLLpFQ0aKqyd2z2FX984mJ9M7scrS3bwypIdTHtxGXEx9nuoDy9Io6QgjeEFaXRL6RTs7IY0cyZq+HROe170GttwYQ3VDmb35r8zNY0Qy2grNGioiJCVnMAPLuvLdy7uzWdbDrJiRzUrdhxixpIdvPD5dgDy0jo7A8iwgjT65aQQo8uZnMWnC3Qb52n48AJAK0Nu3R+Hy1DWEA1urdGgoSJKp7gYLuufzWX9swGoq29k3d4jVhCp5ottVbxtLWeSFB/D0J72AFJSkMaQnl0jctn3k3UNbN5/jMH5XVtM15aO8MbG9rk419Y3kBAb02yT2eb9xyjISPS4GGZH9Gk0NBqfvnB4yopO7lMqBMXHnpn/ceeF9m+hu6tP8uXOakrL7YHk/328hUZr6Og52cn2Jq3CNEYUZdCja+dgF8Fvr6/YxcOz1nHziJ488rX+zff1OL/5BrdPY976/dw38ys+uG+cs6nJ5jLO83RDI1/70+fcPKInj159nsc8AbRXJXLG4nIenrWOLx++jPSkeI9pfjlnA8N6dmV0r0wGP/YhACUFTRd1dNSIFm2rYuGmSm4fW9Q+GQ4wDRoqqogI+emJ5KcnMmVID8C+YNyqXYftQWRnNbNX7uWfS3cC9iatUb0yGFmUzqheGeSldQ67Nuga6+Zary7byerdh/nLN4Z7nF3fhi4NDIFvk684cpKaugZe/GI7l/TrZuXpzGucbmiktr6Rmct38t8T+px1fmMbgp8v3l+7D4A1e45wUd8sj2me+7QMgA9/MK7Z53G83w+/vRaAb44pDIvVnzVoqKjXJSGWC4ozucC6j0hDo2HTvmMs3V7F0rJDfLzxAK+v2A1AbmonRroEkYKMxJAPIg3WVfTP3xjGA2+s5so/fcavbxjM5AGelz8Xgb2HT5KeFN/qCDT7PI32ye9ry3cx3Pp27voWO46fOt3Iy4vLnYtiOmolppnaz46qGgoy/F+CvCAjkUXbqthRVQN4DhoOsS28Oe6T/fYcPhkWKz5r0FDKTYxN6J+bQv/cFL51QRGNjYatlcdZUmYPIp9tqeStr/YAkJ2SQFFmEulJ8aQlxjf9nRRPWmKc83FifExQAoxjgtzE/tkM7HEh3/3nl9z9ygrG9M5gbJ9MSgrSGZSX2mQNp++9+hUVh08y8bwcSgrTKClIJyf17NFnrvM0jp46HZA+IUdQOFHXwJ8XbPNQHvtvEXhpUTnn5aYCcN9rK7mgOBPjNndk/d6jrN1zhAffWsMztwxl8oDuXuW3tr6B2vrGs9I4mizLKmtaLcvJ02duoVzvcvvL2voG51L0Dhc+tYDPfjKewydO85eFW/ndTUOItQmxMaE1B1uDhlKtsNmEvtnJ9M1O5rbRhdYKvTUs3V7F8u2H2Hv4FJv3H6e6po7qE3VNbo3rKj7WRnqiPZikJ9mDSV5aIkWZiRRkJFGUmUS3drhlboPLaKL89ETe+M4Ynv+sjDe/3M1TH2yy5y3GRl2D/WosAj+a2Jc/L9jGa8t38Y9F5YC9qW5EUTo3DM9jdK8MRMTZp3G8tp6Sx+fTLyeZqwfncmNJPqmd2xZAHH0SU4bkOu/B4vqeOMpzw7A85m3Yz7dfLnUem/SHTynO6uKs/VQeq+WmZxdTmJlIQXoid7/yJd8Y2ZPvT+jDNc98wa2jC/nOxb095uPBN9ewbs9R/vXtkU2W63fkZdbKPTx05bnEtXBRf2dVhXN75a7DVB6rJSs5gViXTpq8tM7srj4JwO/nb+bNL+1fSOas+YBfXTeQm0f0bOUd61gaNJTykX2FXvv9Qr4xsqDJscZGw9FTp6k+cZpDNXVU19Rx6ITb75rTVJ+oY+2eI8xdt4/TDWeiTOe4GAoyEinMSKIwM4nCjETrdxLZKW0LKI6ahqO9PD7Wxj3ji7lnfDGHaupYsaOa0h2HeHahvR2+S0KcdW+TTE43NLKh4ijLy6spLT/ERxsO8OaXe+jTrQu3jS7g+Kl6RISGBsP3xhczf8N+Hn9vA7+bt5nrhvVg2uhC+mSfueOja23mwNFTvLemghtL8umScOZSZMUufnXdQEoK0nh41joKXJptHEFlYF4q9086hxG//AiA/76kmHdXV7Cs/JAzbVZyAo9efR4/fXMNT08dwood1fzt8+1kJSdQUpjOkx9spMCtf+dQTR1/mL+ZS/p1473VFXzv1a94+fYRNBqYs6bC+X5WnzhNdU1dk/k/K3YcoiAjiVibUN9oWFxW1eS53/xyN3dd1JsYm3DXRb14dmFZkybAeev3N0m/YOMBDRpKRTKbTeiaGE/XxHiKvLiFZ31DIxVHTrH9YA07qmoorzpB+cEathw4xscbDzi//QN0irMxJL8rVw7K5fIBOV7fla7BND88ND0p3jlEeWL/HK7/yyLiYs6kjYuxMSivK4PyunLH2CJOnW7gnVV7eXnxDh6etc6ZLjUxju9N6MP3JvRh7Z4j/GNROf8u3c0rS3YyZUguv7lxMHExtibt+HPX7+fn76zn6Y+28MK08539F46gEGMTrh2Wx8Oz1pGffmYUmzMIitAtpRO5qZ3Ye+QUxdnJvPDNPMb/5pMmZbxheB5jemeQ27UzkwfkcPB4LU9/tIVXvz2K3dUn+NG/VzVJv2x7FS8v3sHibVU8fs0A7n99NX9duI2s5AT+54019MtxCYJu7+f1f1ls5c3++LR12+Nnbx3O0/O38PHGA9x1kb1mM8BqVmtoNPRMT+Sui3rx0FtrmzzfRxsPWH00odNvpkFDqSCKjbE5R3O5d6o2NBr2Hj5JuRVMtlfW8OmWSh5+ey2PzFrLmN6ZXDmoO5PPyyGtmaGf9ueBGC8uOt5clzrFxXBjST43DM9j5a7D3Pz8Esb1aZrvAT1S+c2Ng/npFefywudlPLNgG3X1jfzx5qFNRmbdOqqAAbkp/OC1lUx7cRkz7hjB0J5pzj6NGBHqPcxmaHAJKu6KMpN4euoQFm6ubLI/1+qHEBF+ce1A5q3fz1c7D/Or6wYx6Q+fNknr6A/ZcuA4N5bk885q+w3CPv3JeJ6ev4WN+87cera5u/E5mijrrQ6YuBh7zaLR5QTH+13f2Ehq5zgu6J1Jt+QEDrjcbreh0XD4xOkWP9+OpkFDqRAVYzszPPhCa2SpMYZN+4/x7qoK3l29lwffXMPDb6/lguJMrhrUnYnn5ZzVl9BoTJN5Dq3xZtKZiDC0ZxprH53UbPr0pHjun9SPjKQEHnt3PffNXMlvbxrcJM3Qnmm8On0UU59bwm0vLGPGnSPPBA2XoOB6cXYNKp5MGdLDOZzak6SEWBb/dAIpneIwxtArK6lJp3ajWyT42qDu3P/6atbtPcLkAd158YvtZ/Jllb7qeC1VNWevtuxoerSJnJUnxzDihgZDjAgFGYks/ekEih6cA8Afvj6E+15byd4jJzVoKKXaRkTol5NCv5wUfjSxL+v2HuWd1Xt5d1UF97++mofeWsu4vplcNSiXCed2I7lTnH32sjc1DceGD1OVvRnZc/vYIhqN4fH3Njgv+K66p3bm1W/bA8etLyxleEEaIvayOrLtOOvzLQf5YttB4EwfTVuabhwjokSEywfk8IzLKC33LF7WP5tYmzBnzT4uH5jTNGhYaafPWMGKHWffwrbiiL2D21OtyJHtvUdOER9rO6scPdLstaOKw6ecI8RCgQYNpcKUiDCgRyoDeqTywOR+rNx1mHdXV/De6grmbzhAfKyN8edksWJHNbX1jV49H7RtOZHW3HlhL+obDU+8v9Hj8dyunXl1+ii+/uxiPtl0pmnJffmQlxeX86HVWeyIV+63e/XV5QO6Nwka7oGta2I8Y4ozeX9tBfdPOoes5AQqrSYkR8p9R0450yd3iuXYqXrAPpcEzg5E7sqrTpy1zzG01xF4QkVoDQBWSrWJo7no4av6s+iBS/jP3aO5+fx8vtx5mIPH68jo0nrzhuPy7Oc1uFl3X9SbH1zat9njPbraaxyeGGcfwZnJhInxTb/ztrWr+LzclCaP3ZunwF7b2FF1gj3VJ7namkwIZzrlR/ZKd+67cmD3syb1JcWfPUnSNUVnD5MoHQMdqk+cbr0QHSggQUNEJovIJhHZKiIPeDieICKvWceXikihy7EHrf2bRGRSIPKjVDSz2YTzC9P5+ZQBLHlwAm9+dwwv3z6y1fOcTUHtuJLe9y+1d86MtWbfu8tPT2T5Q5cy975xTfNkfaevbzQMyuvK7HsvOLPEiJ8ji0SEW0aeGdbqqfwZVp9CTV09/zO5Hw9e3q/pc7iEgB5dO/PhD8Y5R3wlxNooKUzHnSPbf//W+Sz48cVnHY+LsTfP1Te0XkvsSH43T4lIDPAMcBmwG1guIrONMetdkt0BVBtjikVkKvAk8HUR6Q9MBc4DcoH5ItLXGNOAUspvMTZhWM+01hNy5sLX3quvnpebQkIL93bPSk4gK7npcGJnTaOhkbgYYVBe14DmKbVzHPFWe5dr85RjuKuj5tDQaIiPtTkn+7kuXZKdksD+o7XYbEKvrC4kxtkvr72avZuk/Tm7JSd4nG0vIsTZbJxurW2rgwWipjEC2GqMKTPG1AEzgSluaaYAL1nbrwMTxP71YAow0xhTa4zZDmy1nk8p1cHO1DTa9yIl4n1gcq9E1Hu5JPnRU6d5ZsFWNu8/1mpasM+rcDRLuTZPOa7Xjlnfpx2z5q3jjhpQozHOABLj7KB3nGvfOHDsFM8u3GatWeVdzS4uRpxzPUJFIIJGD2CXy+Pd1j6PaYwx9cARIMPLcwEQkekiUioipZWVlZ6SKKUCoL2/1wrS5sDU0GiaLMHRnANHa/n13E1M/P2n7D3cekeyTewzuCuOnGyyRpQjgMRaF37HMdcL/pw1Fby9cq+zhuKIaY7g4ailVBw+xa/e38i2yuNelNTuVH0ju6tPsn7vUeav38+MxeXOwBUsYdMRbox5zhhTYowpycpqeWVJpZTvOqJPw/E6Xtc0rO/0NbX1zFhczspdh50X8JbU1NY7t+dv2N9CSkee7M85+lcfNznXvfZQ7zLvAuxBZZM12c/xHI5jZ4KG/TLrXmZvemIaGg0frNvHlX/6jDtfLuXhWes8dtR3pEAMud0D5Ls8zrP2eUqzW0RigVSgystzlVIdwHGBvvuVFTw9dQiX9c8+a4RSYF7H+8DkCGR//uTMkNjmlht3bcpyXPh/ee1Abh6R7zG9K9enbNo8Zd92NE85Zng7XuuS3y50pj1eax/lFOM2f8QR5By1q4PH63h12U7n6Chv3gvXNHG+zNRsB4F49eVAHxEpEpF47B3bs93SzAamWds3AB8b+zs4G5hqja4qAvoAywKQJ6WUj1wvut+fuZIjJ9tpqKdIm5vAijKTPN54yV12aiemj+vFuL6Zzov3qdPNj69xXUr9l3POnksS61bT8MQxJ8MRNPKsyXnuw2l/8vpqHnxzjXN13vKq5pdYv/2Cs+/mF+wbNfn9NcIYUy8i9wJzgRjgRWPMOhF5DCg1xswGXgBmiMhW4BD2wIKV7t/AeqAeuEdHTikVHO6dzi0t+e2PZ24Z6vX9td1TJcTavBo51TurCz+94lzn42/+fRknahv4992jPaZvLjuOmka/nBQ+/ME454S75ob5rnpkonNk2I8u68v89ftJtFbwbS7c/PTNNc4bSbk7diq05mhAgGaEG2PmAHPc9v3MZfsUcGMz5/4C+EUg8qGUajv32dftFTTy0ry/O12gVnfNSEpgQ0XzA2jcX+fKQd15b3WFc/RU5/gY+ros8d5crlzX/eqV1YXUznEktnL3wzi34cddE+M4bE3oc9xnI5ToMiJKKcA+P+KnV/RzNs/Eh8Ad49wvzp46gVuKK2t2H2HFjkNcN6wHwwvSml1m3Oa279ycZN5bXdHsKC/39M157a7RzppHc30X7n00c+8b57yr34m6ek+nBFXw/yqUUiEhPSmeO8b2cj6O82KUUkfztOBhSx3JCzcf4NF31jO0Z1duGdmz2ZrLnRcWcVNJnvOx48ZIzc2r87YCVJSZ5FyWvbkGKvcaXXZKJ863ZpAftzr0W7rXeEfToKGUcnLtLPa236E9uV+crxrkue0fzm5eA+iZkYQIrc7ViIuxcdvoQudjZ02iuaDR4rP5pqXg/NiUAfb8hMBn4aDNU0opp5MuQSMU7hbnnofvexg51VI2J52Xzcb/m0xCbMv9CmC/eZSD4xrd3JyItrw1zTZPtdAM6AhesTbh2pJ8+rrcNTBYtKahlHI6WRe6gxdHFqX7/I07ITbGq4DhTlwm7zWTwrmV0qlt371fut2+YtLE/tnNpkm2nrtr5zievGEQd4w9ewhuR9OahlLKqbY+dIPGa3d5Hi4bSL+9cTBdE+OczVnNhQzX2HXXRb2pqa13ztNoTnMzwiec263Zcwb0SGVYz67tMsmyrUInJ0qpoCvulszU8/OZv+FAsLPitUC2ol0/3N4ZPmPJDqCl5ilx2YafTO7nMZ0r16d6/rYSxhZnsvnxy1vt5I612TwOAAgWDRpKqSZON5gWly4PNe2xFFOPrp0Y1zer2WHH/sapvtldsNmEeC+a2749rldAO979pUFDKdXEaeueFeEmkDWOS/plc0m/5vsaXF/L06gtT1znfNT7UHO4rIU+j2DQoKGUauLBK/pxIoQ7xN0FY5CXv6/ZGELNTb7SoKGUaqJ7aufWE0U59z4Nb7iGCV9qGqFGg4ZSKqTdMrInVwzoHuxsNOE6zNbbSoejdeq20QUus8TDjwYNpVRI++W1A1s87m2fQiANL0hv87mXD+jeZGHDcBM+QySUUsoD0+43qG2Z981T4dsk5UqDhlIqIoTLeK8QWJ3FLxo0lFJhLRjNU67DZ71+/cioaGjQUEopX7kOfvJ19FSYVzQ0aCillK/8WdYjFFYP9ocGDaVUWAvGNbj51W+b1x7LnQSDBg2lVFgLxsW4U1wMqx6ZCPhecwjzioYGDaVUZOjwi7GPwUqH3CqllPJ5RniYVzQ0aCilwluwmnvaWnPQ5imllIpCzppDdE3T0KChlFL+8L3iEN5VDQ0aSinVBr7WHEyEjLnVoKGUihDB+Qbv7ZBb54zw8K5o+Bc0RCRdROaJyBbrd1oz6aZZabaIyDSX/Z+IyCYRWWn9dPMnP0op1VHaWnMI85jhd03jAeAjY0wf4CPrcRMikg48AowERgCPuAWXbxhjhlg/B/zMj1IqygTrIuxzzSEyWqf8DhpTgJes7ZeAazykmQTMM8YcMsZUA/OAyX6+rlJKhQRfg1a0rz2VbYypsLb3Adke0vQAdrk83m3tc/i71TT1sLTwborIdBEpFZHSyspKP7OtlFL+8bV1KlJmhLd6u1cRmQ/keDj0kOsDY4wREV/flW8YY/aISDLwBnAr8LKnhMaY54DnAEpKSiLj3VdKhT9vO8IjZEZ4q0HDGHNpc8dEZL+IdDfGVIhId8BTn8Qe4GKXx3nAJ9Zz77F+HxORf2Hv8/AYNJRSqiUd3eqjM8LbZjbgGA01DZjlIc1cYKKIpFkd4BOBuSISKyKZACISB1wFrPUzP0op1TF8rDlEyDQNv4PGE8BlIrIFuNR6jIiUiMjfAIwxh4D/A5ZbP49Z+xKwB4/VwErsNZLn/cyPUirKBLtj2deXD8btaQOp1eaplhhjqoAJHvaXAne6PH4ReNEtTQ0w3J/XV0qpYPF5Rni75KLj6YxwpZTyg7c1B8dkwGjv01BKqagUKX0UvtKgoZSKCB1/4z7fag6REmM0aCillB98nxHeLtnoMBo0lFJhLWhrT/k6IzxCqhoaNJRSqg18X+rcas4K8yG3GjSUUsoPvgYBbZ5SSqko5Ov9NLR5SimlQkjQZob7OiNcaxpKKRV9fF8aPTJo0FBKhbcgf3P3dcFC7QhXSqko5muzmDZPKaVUFIrWO/dp0FBKKT/4PCO8XXLRcfxaGl0ppUJFsNae8tZVg3K5alBuO+Wm42hNQyml2sDZsR3uVQcfadBQSoW1YF+zNWgopZRqVWrnOO4dX0zf7ORgZ6VDaZ+GUkq1QVpSPD+edE6ws9HhtKahlFLKaxo0lFIRIdr6FoJFg4ZSSimvadBQSoW1oK1uG6U0aCillPKaBg2llFJe06ChlFLKaxo0lFIRQbs2OoYGDaWUUl7zK2iISLqIzBORLdbvtGbSfSAih0XkXbf9RSKyVES2ishrIhLvT36UUtFHKxgdy9+axgPAR8aYPsBH1mNPfg3c6mH/k8DvjTHFQDVwh5/5UUop1Y78DRpTgJes7ZeAazwlMsZ8BBxz3Sf2wdWXAK+3dr5SSqnQ4G/QyDbGVFjb+4BsH87NAA4bY+qtx7uBHs0lFpHpIlIqIqWVlZVty61SSim/tLrKrYjMB3I8HHrI9YExxohIu90E1xjzHPAcQElJSWTcbFcpFTCivRsdotWgYYy5tLljIrJfRLobYypEpDtwwIfXrgK6ikisVdvIA/b4cL5SSqkO5m/z1GxgmrU9DZjl7YnGGAMsAG5oy/lKKaU6nr9B4wngMhHZAlxqPUZESkTkb45EIvIZ8B9ggojsFpFJ1qH/AX4oIlux93G84Gd+lFJRRif1dSy/7txnjKkCJnjYXwrc6fL4wmbOLwNG+JMHpZRSHUdnhCullPKaBg2lVGTQZqoOoUFDKaWU1zRoKKWU8poGDaVUWNNJfR1Lg4ZSSimvadBQSinlNQ0aSqmIoI1UHUODhlJKKa9p0FBKKeU1DRpKqbCma091LA0aSimlvKZBQykV1m4e0ROArOSEIOckOvi1yq1SSgXbtDGFTBtTGOxsRA2taSillPKaBg2llFJe06ChlFLKaxo0lFJKeU2DhlJKKa9p0FBKKeU1DRpKKaW8pkFDKaWU18QYE+w8+ExEKoEdbTw9EzgYwOyEk2guO0R3+aO57BDd5Xcte4ExJsufJwvLoOEPESk1xpQEOx/BEM1lh+gufzSXHaK7/IEuuzZPKaWU8poGDaWUUl6LxqDxXLAzEETRXHaI7vJHc9khussf0LJHXZ+GUkqptovGmoZSSqk20qChlFLKa1ETNERksohsEpGtIvJAsPPTXkSkXETWiMhKESm19qWLyDwR2WL9TrP2i4j80XpPVovIsODm3jci8qKIHBCRtS77fC6riEyz0m8RkWnBKEtbNFP+R0Vkj/X5rxSRK1yOPWiVf5OITHLZH3b/GyKSLyILRGS9iKwTke9b+yP+82+h7B3z2RtjIv4HiAG2Ab2AeGAV0D/Y+WqnspYDmW77ngIesLYfAJ60tq8A3gcEGAUsDXb+fSzrOGAYsLatZQXSgTLrd5q1nRbssvlR/keBH3tI29/6u08Aiqz/h5hw/d8AugPDrO1kYLNVxoj//Fsoe4d89tFS0xgBbDXGlBlj6oCZwJQg56kjTQFesrZfAq5x2f+ysVsCdBWR7kHIX5sYYz4FDrnt9rWsk4B5xphDxphqYB4wud0zHwDNlL85U4CZxphaY8x2YCv2/4uw/N8wxlQYY760to8BG4AeRMHn30LZmxPQzz5agkYPYJfL4920/CaHMwN8KCIrRGS6tS/bGFNhbe8Dsq3tSHxffC1rJL4H91pNMC86mmeI4PKLSCEwFFhKlH3+bmWHDvjsoyVoRJOxxphhwOXAPSIyzvWgsddXo2KcdTSV1cVfgN7AEKAC+G1Qc9PORKQL8AZwnzHmqOuxSP/8PZS9Qz77aAkae4B8l8d51r6IY4zZY/0+ALyFvQq639HsZP0+YCWPxPfF17JG1HtgjNlvjGkwxjQCz2P//CECyy8icdgvmv80xrxp7Y6Kz99T2Tvqs4+WoLEc6CMiRSISD0wFZgc5TwEnIkkikuzYBiYCa7GX1TEqZBowy9qeDdxmjSwZBRxxqdqHK1/LOheYKCJpVnV+orUvLLn1SV2L/fMHe/mnikiCiBQBfYBlhOn/hogI8AKwwRjzO5dDEf/5N1f2Dvvsgz0SoKN+sI+e2Ix9tMBDwc5PO5WxF/YREKuAdY5yAhnAR8AWYD6Qbu0X4BnrPVkDlAS7DD6W91Xs1fDT2Ntj72hLWYHbsXcObgW+Fexy+Vn+GVb5VlsXgO4u6R+yyr8JuNxlf9j9bwBjsTc9rQZWWj9XRMPn30LZO+Sz12VElFJKeS1amqeUUkoFgAYNpZRSXtOgoZRSymsaNJRSSnlNg4ZSSimvadBQSinlNQ0aSimlvPb/AcyYl2S2cHiiAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.0403  # Value for C0\n",
    "K0 = -0.0012  # Value for K0\n",
    "K1 = -0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0120    # Value for b\n",
    "c = 2.0334    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    91.100000    90.875910    90.651821    90.427731    90.203641   \n",
      "351    90.875910    90.651821    90.427731    90.203641    89.979552   \n",
      "352    90.651821    90.427731    90.203641    89.979552    89.755462   \n",
      "353    90.427731    90.203641    89.979552    89.755462    89.531373   \n",
      "354    90.203641    89.979552    89.755462    89.531373    89.307283   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    89.979552    89.755462    89.531373    89.307283    89.094118  ...   \n",
      "351    89.755462    89.531373    89.307283    89.094118    89.015686  ...   \n",
      "352    89.531373    89.307283    89.094118    89.015686    88.937255  ...   \n",
      "353    89.307283    89.094118    89.015686    88.937255    88.858824  ...   \n",
      "354    89.094118    89.015686    88.937255    88.858824    88.780392  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   81.423343    0.000263   81.404669    0.000263   81.385994    0.000263   \n",
      "351   81.404669    0.000263   81.385994    0.000263   81.367320    0.000262   \n",
      "352   81.385994    0.000263   81.367320    0.000262   81.348646    0.000262   \n",
      "353   81.367320    0.000262   81.348646    0.000262   81.329972    0.000262   \n",
      "354   81.348646    0.000262   81.329972    0.000262   81.311298    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   81.367320    0.000262   81.348646    0.000262  \n",
      "351   81.348646    0.000262   81.329972    0.000262  \n",
      "352   81.329972    0.000262   81.311298    0.000262  \n",
      "353   81.311298    0.000262   81.292624    0.000262  \n",
      "354   81.292624    0.000262   81.273950    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1950, 1, 251) (1950, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "25/25 [==============================] - 2s 19ms/step - loss: 5556.0669 - val_loss: 3405.7109\n",
      "Epoch 2/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 5255.6245 - val_loss: 3223.6438\n",
      "Epoch 3/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 5096.7515 - val_loss: 3145.3413\n",
      "Epoch 4/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4976.6919 - val_loss: 3071.1228\n",
      "Epoch 5/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4860.9468 - val_loss: 3001.9041\n",
      "Epoch 6/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4750.7612 - val_loss: 2935.0715\n",
      "Epoch 7/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4643.5732 - val_loss: 2871.0254\n",
      "Epoch 8/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4539.7217 - val_loss: 2809.0806\n",
      "Epoch 9/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4438.4326 - val_loss: 2749.0317\n",
      "Epoch 10/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4339.4512 - val_loss: 2690.7417\n",
      "Epoch 11/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4242.6079 - val_loss: 2634.1143\n",
      "Epoch 12/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 4147.7026 - val_loss: 2577.4438\n",
      "Epoch 13/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 4049.8938 - val_loss: 2520.2637\n",
      "Epoch 14/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3952.1504 - val_loss: 2464.9138\n",
      "Epoch 15/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3857.6953 - val_loss: 2411.7771\n",
      "Epoch 16/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3765.9895 - val_loss: 2360.5315\n",
      "Epoch 17/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3676.6206 - val_loss: 2310.9917\n",
      "Epoch 18/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3589.3594 - val_loss: 2263.0442\n",
      "Epoch 19/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3504.0591 - val_loss: 2216.6101\n",
      "Epoch 20/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3420.6206 - val_loss: 2171.6294\n",
      "Epoch 21/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3338.9663 - val_loss: 2128.0527\n",
      "Epoch 22/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3259.0339 - val_loss: 2085.8391\n",
      "Epoch 23/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3180.7732 - val_loss: 2044.9517\n",
      "Epoch 24/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 3104.1379 - val_loss: 2005.3575\n",
      "Epoch 25/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3029.0894 - val_loss: 1967.0273\n",
      "Epoch 26/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2955.5916 - val_loss: 1929.9327\n",
      "Epoch 27/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2883.6125 - val_loss: 1894.0471\n",
      "Epoch 28/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2813.1216 - val_loss: 1859.3462\n",
      "Epoch 29/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2744.0891 - val_loss: 1825.8060\n",
      "Epoch 30/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2676.4905 - val_loss: 1793.4042\n",
      "Epoch 31/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2610.2979 - val_loss: 1762.1174\n",
      "Epoch 32/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2545.4885 - val_loss: 1731.9264\n",
      "Epoch 33/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2482.0378 - val_loss: 1702.8080\n",
      "Epoch 34/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2419.9243 - val_loss: 1674.7440\n",
      "Epoch 35/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2359.1250 - val_loss: 1647.7140\n",
      "Epoch 36/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2299.6191 - val_loss: 1621.6979\n",
      "Epoch 37/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2241.3850 - val_loss: 1596.6776\n",
      "Epoch 38/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2184.4036 - val_loss: 1572.6342\n",
      "Epoch 39/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2128.6545 - val_loss: 1549.5493\n",
      "Epoch 40/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2074.1189 - val_loss: 1527.4052\n",
      "Epoch 41/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2020.7772 - val_loss: 1506.1835\n",
      "Epoch 42/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 1968.6111 - val_loss: 1485.8665\n",
      "Epoch 43/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1917.6016 - val_loss: 1466.4379\n",
      "Epoch 44/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1867.7317 - val_loss: 1447.8793\n",
      "Epoch 45/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1818.9830 - val_loss: 1430.1750\n",
      "Epoch 46/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1771.3386 - val_loss: 1413.3074\n",
      "Epoch 47/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1724.7808 - val_loss: 1397.2604\n",
      "Epoch 48/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1679.2920 - val_loss: 1382.0171\n",
      "Epoch 49/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1634.8569 - val_loss: 1367.5619\n",
      "Epoch 50/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1591.4576 - val_loss: 1353.8782\n",
      "Epoch 51/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1549.0787 - val_loss: 1340.9504\n",
      "Epoch 52/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1507.7034 - val_loss: 1328.7623\n",
      "Epoch 53/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1467.3157 - val_loss: 1317.2986\n",
      "Epoch 54/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1427.9000 - val_loss: 1306.5435\n",
      "Epoch 55/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1389.4406 - val_loss: 1296.4819\n",
      "Epoch 56/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1351.9215 - val_loss: 1287.0980\n",
      "Epoch 57/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1315.3278 - val_loss: 1278.3772\n",
      "Epoch 58/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1279.6443 - val_loss: 1270.3042\n",
      "Epoch 59/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1244.8557 - val_loss: 1262.8639\n",
      "Epoch 60/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1210.9467 - val_loss: 1256.0416\n",
      "Epoch 61/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1177.9036 - val_loss: 1249.8224\n",
      "Epoch 62/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1145.7104 - val_loss: 1244.1921\n",
      "Epoch 63/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1114.3534 - val_loss: 1239.1357\n",
      "Epoch 64/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1083.8181 - val_loss: 1234.6393\n",
      "Epoch 65/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1054.0900 - val_loss: 1230.6881\n",
      "Epoch 66/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1025.1549 - val_loss: 1227.2681\n",
      "Epoch 67/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 996.9987 - val_loss: 1224.3651\n",
      "Epoch 68/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 969.6079 - val_loss: 1221.9651\n",
      "Epoch 69/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 942.9680 - val_loss: 1220.0544\n",
      "Epoch 70/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 917.0654 - val_loss: 1218.6190\n",
      "Epoch 71/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 891.8867 - val_loss: 1217.6451\n",
      "Epoch 72/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 867.4182 - val_loss: 1217.1194\n",
      "Epoch 73/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 843.6468 - val_loss: 1217.0282\n",
      "Epoch 74/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 820.5589 - val_loss: 1217.3579\n",
      "Epoch 75/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 798.1415 - val_loss: 1218.0955\n",
      "Epoch 76/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 776.3813 - val_loss: 1219.2275\n",
      "Epoch 77/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 755.2658 - val_loss: 1220.7415\n",
      "Epoch 78/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 734.7817 - val_loss: 1222.6235\n",
      "Epoch 79/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 714.9161 - val_loss: 1224.8613\n",
      "Epoch 80/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 695.6564 - val_loss: 1227.4419\n",
      "Epoch 81/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 676.9905 - val_loss: 1230.3527\n",
      "Epoch 82/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 658.9055 - val_loss: 1233.5811\n",
      "Epoch 83/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 641.3892 - val_loss: 1237.1146\n",
      "Epoch 84/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 624.4293 - val_loss: 1240.9410\n",
      "Epoch 85/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 608.0137 - val_loss: 1245.0482\n",
      "Epoch 86/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 592.1304 - val_loss: 1249.4240\n",
      "Epoch 87/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 576.7676 - val_loss: 1254.0560\n",
      "Epoch 88/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 561.9134 - val_loss: 1258.9332\n",
      "Epoch 89/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 547.5560 - val_loss: 1264.0435\n",
      "Epoch 90/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 533.6839 - val_loss: 1269.3751\n",
      "Epoch 91/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 520.2858 - val_loss: 1274.9172\n",
      "Epoch 92/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 507.3498 - val_loss: 1280.6578\n",
      "Epoch 93/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 494.8651 - val_loss: 1286.5861\n",
      "Epoch 94/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 482.8207 - val_loss: 1292.6909\n",
      "Epoch 95/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 471.2053 - val_loss: 1298.9617\n",
      "Epoch 96/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 460.0082 - val_loss: 1305.3877\n",
      "Epoch 97/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 449.2184 - val_loss: 1311.9584\n",
      "Epoch 98/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 438.8256 - val_loss: 1318.6632\n",
      "Epoch 99/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 428.8189 - val_loss: 1325.4918\n",
      "Epoch 100/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 419.1884 - val_loss: 1332.4347\n",
      "Epoch 101/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 409.9235 - val_loss: 1339.4812\n",
      "Epoch 102/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 401.0141 - val_loss: 1346.6224\n",
      "Epoch 103/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 392.4504 - val_loss: 1353.8480\n",
      "Epoch 104/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 384.2224 - val_loss: 1361.1498\n",
      "Epoch 105/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 376.3204 - val_loss: 1368.5173\n",
      "Epoch 106/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 368.7350 - val_loss: 1375.9425\n",
      "Epoch 107/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 361.4565 - val_loss: 1383.4164\n",
      "Epoch 108/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 354.4757 - val_loss: 1390.9302\n",
      "Epoch 109/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 347.7835 - val_loss: 1398.4756\n",
      "Epoch 110/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 341.3710 - val_loss: 1406.0447\n",
      "Epoch 111/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 335.2291 - val_loss: 1413.6292\n",
      "Epoch 112/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 329.3492 - val_loss: 1421.2214\n",
      "Epoch 113/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 323.7229 - val_loss: 1428.8141\n",
      "Epoch 114/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 318.3416 - val_loss: 1436.3995\n",
      "Epoch 115/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 313.1972 - val_loss: 1443.9703\n",
      "Epoch 116/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 308.2817 - val_loss: 1451.5205\n",
      "Epoch 117/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 303.5870 - val_loss: 1459.0435\n",
      "Epoch 118/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 299.1053 - val_loss: 1466.5322\n",
      "Epoch 119/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 294.8290 - val_loss: 1473.9801\n",
      "Epoch 120/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 290.7509 - val_loss: 1481.3821\n",
      "Epoch 121/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 286.8636 - val_loss: 1488.7316\n",
      "Epoch 122/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 283.1600 - val_loss: 1496.0236\n",
      "Epoch 123/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 279.6331 - val_loss: 1503.2532\n",
      "Epoch 124/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 276.2760 - val_loss: 1510.4150\n",
      "Epoch 125/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 273.0823 - val_loss: 1517.5040\n",
      "Epoch 126/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 270.0454 - val_loss: 1524.5157\n",
      "Epoch 127/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 267.1592 - val_loss: 1531.4463\n",
      "Epoch 128/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 264.4173 - val_loss: 1538.2910\n",
      "Epoch 129/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 261.8140 - val_loss: 1545.0461\n",
      "Epoch 130/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 259.3433 - val_loss: 1551.7085\n",
      "Epoch 131/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 256.9996 - val_loss: 1558.2744\n",
      "Epoch 132/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 254.7777 - val_loss: 1564.7410\n",
      "Epoch 133/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 252.6719 - val_loss: 1571.1055\n",
      "Epoch 134/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 250.6773 - val_loss: 1577.3638\n",
      "Epoch 135/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 248.7890 - val_loss: 1583.5149\n",
      "Epoch 136/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 247.0020 - val_loss: 1589.5560\n",
      "Epoch 137/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 245.3117 - val_loss: 1595.4858\n",
      "Epoch 138/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 243.7137 - val_loss: 1601.3007\n",
      "Epoch 139/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 242.2035 - val_loss: 1607.0016\n",
      "Epoch 140/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 240.7769 - val_loss: 1612.5851\n",
      "Epoch 141/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 239.4301 - val_loss: 1618.0509\n",
      "Epoch 142/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 238.1589 - val_loss: 1623.3977\n",
      "Epoch 143/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 236.9597 - val_loss: 1628.6254\n",
      "Epoch 144/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 235.8287 - val_loss: 1633.7330\n",
      "Epoch 145/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 234.7628 - val_loss: 1638.7196\n",
      "Epoch 146/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 233.7583 - val_loss: 1643.5847\n",
      "Epoch 147/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 232.8123 - val_loss: 1648.3303\n",
      "Epoch 148/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 231.9216 - val_loss: 1652.9540\n",
      "Epoch 149/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 231.0833 - val_loss: 1657.4580\n",
      "Epoch 150/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 230.2945 - val_loss: 1661.8413\n",
      "Epoch 151/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 229.5526 - val_loss: 1666.1060\n",
      "Epoch 152/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 228.8551 - val_loss: 1670.2509\n",
      "Epoch 153/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 228.1995 - val_loss: 1674.2792\n",
      "Epoch 154/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 227.5835 - val_loss: 1678.1891\n",
      "Epoch 155/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 227.0048 - val_loss: 1681.9836\n",
      "Epoch 156/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 226.4614 - val_loss: 1685.6638\n",
      "Epoch 157/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 225.9510 - val_loss: 1689.2312\n",
      "Epoch 158/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 225.4721 - val_loss: 1692.6866\n",
      "Epoch 159/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 225.0226 - val_loss: 1696.0309\n",
      "Epoch 160/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 224.6009 - val_loss: 1699.2671\n",
      "Epoch 161/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 224.2053 - val_loss: 1702.3965\n",
      "Epoch 162/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 223.8344 - val_loss: 1705.4197\n",
      "Epoch 163/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 223.4865 - val_loss: 1708.3417\n",
      "Epoch 164/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 223.1602 - val_loss: 1711.1609\n",
      "Epoch 165/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.8544 - val_loss: 1713.8812\n",
      "Epoch 166/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 222.5678 - val_loss: 1716.5032\n",
      "Epoch 167/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.2991 - val_loss: 1719.0314\n",
      "Epoch 168/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 222.0472 - val_loss: 1721.4657\n",
      "Epoch 169/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 221.8113 - val_loss: 1723.8093\n",
      "Epoch 170/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 221.5902 - val_loss: 1726.0634\n",
      "Epoch 171/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 221.3831 - val_loss: 1728.2305\n",
      "Epoch 172/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 221.1890 - val_loss: 1730.3134\n",
      "Epoch 173/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 221.0072 - val_loss: 1732.3140\n",
      "Epoch 174/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 220.8367 - val_loss: 1734.2347\n",
      "Epoch 175/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 220.6771 - val_loss: 1736.0773\n",
      "Epoch 176/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 220.5276 - val_loss: 1737.8452\n",
      "Epoch 177/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 220.3875 - val_loss: 1739.5385\n",
      "Epoch 178/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 220.2562 - val_loss: 1741.1608\n",
      "Epoch 179/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 220.1333 - val_loss: 1742.7137\n",
      "Epoch 180/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 220.0180 - val_loss: 1744.2009\n",
      "Epoch 181/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 219.9100 - val_loss: 1745.6221\n",
      "Epoch 182/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 219.8089 - val_loss: 1746.9816\n",
      "Epoch 183/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 219.7140 - val_loss: 1748.2804\n",
      "Epoch 184/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 219.6251 - val_loss: 1749.5211\n",
      "Epoch 185/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 219.5418 - val_loss: 1750.7056\n",
      "Epoch 186/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 219.4637 - val_loss: 1751.8354\n",
      "Epoch 187/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 219.3906 - val_loss: 1752.9130\n",
      "Epoch 188/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 219.3220 - val_loss: 1753.9406\n",
      "Epoch 189/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 219.2578 - val_loss: 1754.9194\n",
      "Epoch 190/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 219.1974 - val_loss: 1755.8510\n",
      "Epoch 191/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 219.1410 - val_loss: 1756.7383\n",
      "Epoch 192/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 219.0880 - val_loss: 1757.5825\n",
      "Epoch 193/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 219.0384 - val_loss: 1758.3849\n",
      "Epoch 194/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.9918 - val_loss: 1759.1487\n",
      "Epoch 195/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.9482 - val_loss: 1759.8746\n",
      "Epoch 196/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.9073 - val_loss: 1760.5625\n",
      "Epoch 197/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.8689 - val_loss: 1761.2161\n",
      "Epoch 198/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.8330 - val_loss: 1761.8367\n",
      "Epoch 199/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.7993 - val_loss: 1762.4252\n",
      "Epoch 200/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.7677 - val_loss: 1762.9824\n",
      "Epoch 201/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.7382 - val_loss: 1763.5111\n",
      "Epoch 202/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.7104 - val_loss: 1764.0125\n",
      "Epoch 203/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.6844 - val_loss: 1764.4861\n",
      "Epoch 204/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.6601 - val_loss: 1764.9351\n",
      "Epoch 205/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 218.6373 - val_loss: 1765.3600\n",
      "Epoch 206/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 218.6160 - val_loss: 1765.7611\n",
      "Epoch 207/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.5959 - val_loss: 1766.1415\n",
      "Epoch 208/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.5773 - val_loss: 1766.5011\n",
      "Epoch 209/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.5597 - val_loss: 1766.8408\n",
      "Epoch 210/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.5432 - val_loss: 1767.1614\n",
      "Epoch 211/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.5279 - val_loss: 1767.4628\n",
      "Epoch 212/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.5135 - val_loss: 1767.7487\n",
      "Epoch 213/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.5001 - val_loss: 1768.0176\n",
      "Epoch 214/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.4875 - val_loss: 1768.2715\n",
      "Epoch 215/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.4758 - val_loss: 1768.5109\n",
      "Epoch 216/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.4649 - val_loss: 1768.7367\n",
      "Epoch 217/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.4545 - val_loss: 1768.9495\n",
      "Epoch 218/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.4449 - val_loss: 1769.1494\n",
      "Epoch 219/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.4360 - val_loss: 1769.3378\n",
      "Epoch 220/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.4277 - val_loss: 1769.5148\n",
      "Epoch 221/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.4200 - val_loss: 1769.6819\n",
      "Epoch 222/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.4127 - val_loss: 1769.8395\n",
      "Epoch 223/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.4060 - val_loss: 1769.9867\n",
      "Epoch 224/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3997 - val_loss: 1770.1249\n",
      "Epoch 225/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3939 - val_loss: 1770.2557\n",
      "Epoch 226/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3884 - val_loss: 1770.3784\n",
      "Epoch 227/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3834 - val_loss: 1770.4929\n",
      "Epoch 228/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3787 - val_loss: 1770.6010\n",
      "Epoch 229/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3744 - val_loss: 1770.7018\n",
      "Epoch 230/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3703 - val_loss: 1770.7981\n",
      "Epoch 231/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3665 - val_loss: 1770.8870\n",
      "Epoch 232/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3631 - val_loss: 1770.9705\n",
      "Epoch 233/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3599 - val_loss: 1771.0500\n",
      "Epoch 234/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3569 - val_loss: 1771.1234\n",
      "Epoch 235/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3541 - val_loss: 1771.1921\n",
      "Epoch 236/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3516 - val_loss: 1771.2561\n",
      "Epoch 237/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3494 - val_loss: 1771.3168\n",
      "Epoch 238/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3471 - val_loss: 1771.3734\n",
      "Epoch 239/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 218.3452 - val_loss: 1771.4279\n",
      "Epoch 240/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3434 - val_loss: 1771.4766\n",
      "Epoch 241/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3418 - val_loss: 1771.5236\n",
      "Epoch 242/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3402 - val_loss: 1771.5668\n",
      "Epoch 243/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3388 - val_loss: 1771.6085\n",
      "Epoch 244/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3376 - val_loss: 1771.6466\n",
      "Epoch 245/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3365 - val_loss: 1771.6830\n",
      "Epoch 246/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3354 - val_loss: 1771.7157\n",
      "Epoch 247/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3345 - val_loss: 1771.7471\n",
      "Epoch 248/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3336 - val_loss: 1771.7756\n",
      "Epoch 249/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3329 - val_loss: 1771.8047\n",
      "Epoch 250/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3321 - val_loss: 1771.8295\n",
      "Epoch 251/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3315 - val_loss: 1771.8531\n",
      "Epoch 252/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3310 - val_loss: 1771.8756\n",
      "Epoch 253/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3305 - val_loss: 1771.8964\n",
      "Epoch 254/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3301 - val_loss: 1771.9166\n",
      "Epoch 255/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3297 - val_loss: 1771.9341\n",
      "Epoch 256/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3295 - val_loss: 1771.9513\n",
      "Epoch 257/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3292 - val_loss: 1771.9674\n",
      "Epoch 258/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3289 - val_loss: 1771.9830\n",
      "Epoch 259/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3287 - val_loss: 1771.9963\n",
      "Epoch 260/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3286 - val_loss: 1772.0096\n",
      "Epoch 261/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3284 - val_loss: 1772.0216\n",
      "Epoch 262/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3284 - val_loss: 1772.0326\n",
      "Epoch 263/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3283 - val_loss: 1772.0432\n",
      "Epoch 264/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3283 - val_loss: 1772.0543\n",
      "Epoch 265/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3282 - val_loss: 1772.0636\n",
      "Epoch 266/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3282 - val_loss: 1772.0717\n",
      "Epoch 267/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 218.3283 - val_loss: 1772.0806\n",
      "Epoch 268/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3282 - val_loss: 1772.0873\n",
      "Epoch 269/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3283 - val_loss: 1772.0946\n",
      "Epoch 270/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3285 - val_loss: 1772.1016\n",
      "Epoch 271/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3285 - val_loss: 1772.1074\n",
      "Epoch 272/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3287 - val_loss: 1772.1133\n",
      "Epoch 273/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3288 - val_loss: 1772.1179\n",
      "Epoch 274/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3289 - val_loss: 1772.1234\n",
      "Epoch 275/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3290 - val_loss: 1772.1279\n",
      "Epoch 276/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3292 - val_loss: 1772.1316\n",
      "Epoch 277/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3294 - val_loss: 1772.1356\n",
      "Epoch 278/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3295 - val_loss: 1772.1399\n",
      "Epoch 279/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3297 - val_loss: 1772.1436\n",
      "Epoch 280/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3298 - val_loss: 1772.1472\n",
      "Epoch 281/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3300 - val_loss: 1772.1505\n",
      "Epoch 282/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3301 - val_loss: 1772.1542\n",
      "Epoch 283/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3303 - val_loss: 1772.1569\n",
      "Epoch 284/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3305 - val_loss: 1772.1592\n",
      "Epoch 285/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3306 - val_loss: 1772.1620\n",
      "Epoch 286/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3308 - val_loss: 1772.1636\n",
      "Epoch 287/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3310 - val_loss: 1772.1660\n",
      "Epoch 288/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3312 - val_loss: 1772.1676\n",
      "Epoch 289/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3314 - val_loss: 1772.1691\n",
      "Epoch 290/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3315 - val_loss: 1772.1705\n",
      "Epoch 291/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3318 - val_loss: 1772.1725\n",
      "Epoch 292/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3320 - val_loss: 1772.1747\n",
      "Epoch 293/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3321 - val_loss: 1772.1753\n",
      "Epoch 294/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3323 - val_loss: 1772.1768\n",
      "Epoch 295/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3324 - val_loss: 1772.1774\n",
      "Epoch 296/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3327 - val_loss: 1772.1793\n",
      "Epoch 297/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3328 - val_loss: 1772.1804\n",
      "Epoch 298/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3330 - val_loss: 1772.1820\n",
      "Epoch 299/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3332 - val_loss: 1772.1835\n",
      "Epoch 300/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 218.3333 - val_loss: 1772.1848\n",
      "Epoch 301/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 218.3334 - val_loss: 1772.1858\n",
      "Epoch 302/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3335 - val_loss: 1772.1866\n",
      "Epoch 303/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3337 - val_loss: 1772.1871\n",
      "Epoch 304/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3339 - val_loss: 1772.1876\n",
      "Epoch 305/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3342 - val_loss: 1772.1886\n",
      "Epoch 306/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3342 - val_loss: 1772.1895\n",
      "Epoch 307/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3344 - val_loss: 1772.1904\n",
      "Epoch 308/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3344 - val_loss: 1772.1909\n",
      "Epoch 309/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3346 - val_loss: 1772.1909\n",
      "Epoch 310/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3348 - val_loss: 1772.1909\n",
      "Epoch 311/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3350 - val_loss: 1772.1918\n",
      "Epoch 312/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3350 - val_loss: 1772.1923\n",
      "Epoch 313/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3352 - val_loss: 1772.1925\n",
      "Epoch 314/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3354 - val_loss: 1772.1930\n",
      "Epoch 315/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3355 - val_loss: 1772.1930\n",
      "Epoch 316/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3357 - val_loss: 1772.1932\n",
      "Epoch 317/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3358 - val_loss: 1772.1942\n",
      "Epoch 318/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3359 - val_loss: 1772.1942\n",
      "Epoch 319/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3360 - val_loss: 1772.1942\n",
      "Epoch 320/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3362 - val_loss: 1772.1942\n",
      "Epoch 321/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3363 - val_loss: 1772.1947\n",
      "Epoch 322/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3364 - val_loss: 1772.1953\n",
      "Epoch 323/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3365 - val_loss: 1772.1953\n",
      "Epoch 324/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3366 - val_loss: 1772.1953\n",
      "Epoch 325/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3367 - val_loss: 1772.1957\n",
      "Epoch 326/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3368 - val_loss: 1772.1962\n",
      "Epoch 327/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3369 - val_loss: 1772.1967\n",
      "Epoch 328/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 218.3370 - val_loss: 1772.1963\n",
      "Epoch 329/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3371 - val_loss: 1772.1965\n",
      "Epoch 330/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3372 - val_loss: 1772.1968\n",
      "Epoch 331/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3373 - val_loss: 1772.1969\n",
      "Epoch 332/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3374 - val_loss: 1772.1971\n",
      "Epoch 333/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3375 - val_loss: 1772.1971\n",
      "Epoch 334/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3376 - val_loss: 1772.1979\n",
      "Epoch 335/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3377 - val_loss: 1772.1982\n",
      "Epoch 336/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3377 - val_loss: 1772.1987\n",
      "Epoch 337/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3377 - val_loss: 1772.1992\n",
      "Epoch 338/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3378 - val_loss: 1772.1993\n",
      "Epoch 339/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3380 - val_loss: 1772.2002\n",
      "Epoch 340/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3381 - val_loss: 1772.2008\n",
      "Epoch 341/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3380 - val_loss: 1772.2013\n",
      "Epoch 342/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3381 - val_loss: 1772.2013\n",
      "Epoch 343/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3381 - val_loss: 1772.2012\n",
      "Epoch 344/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3382 - val_loss: 1772.2012\n",
      "Epoch 345/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3383 - val_loss: 1772.2017\n",
      "Epoch 346/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3384 - val_loss: 1772.2018\n",
      "Epoch 347/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3384 - val_loss: 1772.2025\n",
      "Epoch 348/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3384 - val_loss: 1772.2030\n",
      "Epoch 349/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3385 - val_loss: 1772.2024\n",
      "Epoch 350/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3386 - val_loss: 1772.2020\n",
      "Epoch 351/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3386 - val_loss: 1772.2013\n",
      "Epoch 352/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3387 - val_loss: 1772.2017\n",
      "Epoch 353/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3388 - val_loss: 1772.2023\n",
      "Epoch 354/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3389 - val_loss: 1772.2017\n",
      "Epoch 355/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3389 - val_loss: 1772.2017\n",
      "Epoch 356/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 218.3389 - val_loss: 1772.2019\n",
      "Epoch 357/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3390 - val_loss: 1772.2014\n",
      "Epoch 358/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 218.3391 - val_loss: 1772.2012\n",
      "Epoch 359/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3391 - val_loss: 1772.2014\n",
      "Epoch 360/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3392 - val_loss: 1772.2020\n",
      "Epoch 361/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3393 - val_loss: 1772.2024\n",
      "Epoch 362/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3393 - val_loss: 1772.2024\n",
      "Epoch 363/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3393 - val_loss: 1772.2017\n",
      "Epoch 364/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3393 - val_loss: 1772.2017\n",
      "Epoch 365/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3394 - val_loss: 1772.2017\n",
      "Epoch 366/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 218.3394 - val_loss: 1772.2017\n",
      "Epoch 367/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3394 - val_loss: 1772.2009\n",
      "Epoch 368/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3395 - val_loss: 1772.2009\n",
      "Epoch 369/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3396 - val_loss: 1772.2007\n",
      "Epoch 370/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3396 - val_loss: 1772.2002\n",
      "Epoch 371/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3397 - val_loss: 1772.2002\n",
      "Epoch 372/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3397 - val_loss: 1772.2003\n",
      "Epoch 373/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3398 - val_loss: 1772.2008\n",
      "Epoch 374/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3398 - val_loss: 1772.2009\n",
      "Epoch 375/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3399 - val_loss: 1772.2019\n",
      "Epoch 376/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3398 - val_loss: 1772.2023\n",
      "Epoch 377/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3398 - val_loss: 1772.2019\n",
      "Epoch 378/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3399 - val_loss: 1772.2024\n",
      "Epoch 379/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3399 - val_loss: 1772.2028\n",
      "Epoch 380/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 218.3399 - val_loss: 1772.2028\n",
      "Epoch 381/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 218.3400 - val_loss: 1772.2025\n",
      "Epoch 382/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3400 - val_loss: 1772.2025\n",
      "Epoch 383/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3400 - val_loss: 1772.2025\n",
      "Epoch 384/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3400 - val_loss: 1772.2025\n",
      "Epoch 385/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3400 - val_loss: 1772.2024\n",
      "Epoch 386/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3401 - val_loss: 1772.2014\n",
      "Epoch 387/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3401 - val_loss: 1772.2008\n",
      "Epoch 388/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3402 - val_loss: 1772.2007\n",
      "Epoch 389/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3403 - val_loss: 1772.2007\n",
      "Epoch 390/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3403 - val_loss: 1772.2004\n",
      "Epoch 391/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3403 - val_loss: 1772.2004\n",
      "Epoch 392/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3404 - val_loss: 1772.2017\n",
      "Epoch 393/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3403 - val_loss: 1772.2017\n",
      "Epoch 394/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3404 - val_loss: 1772.2017\n",
      "Epoch 395/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3404 - val_loss: 1772.2017\n",
      "Epoch 396/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3404 - val_loss: 1772.2019\n",
      "Epoch 397/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3404 - val_loss: 1772.2017\n",
      "Epoch 398/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3405 - val_loss: 1772.2023\n",
      "Epoch 399/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3404 - val_loss: 1772.2024\n",
      "Epoch 400/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3404 - val_loss: 1772.2028\n",
      "Epoch 401/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3404 - val_loss: 1772.2024\n",
      "Epoch 402/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3404 - val_loss: 1772.2017\n",
      "Epoch 403/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3405 - val_loss: 1772.2008\n",
      "Epoch 404/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3405 - val_loss: 1772.2002\n",
      "Epoch 405/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 218.3405 - val_loss: 1772.1993\n",
      "Epoch 406/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 218.3407 - val_loss: 1772.1993\n",
      "Epoch 407/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3407 - val_loss: 1772.1993\n",
      "Epoch 408/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3407 - val_loss: 1772.1993\n",
      "Epoch 409/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3407 - val_loss: 1772.1991\n",
      "Epoch 410/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3408 - val_loss: 1772.1991\n",
      "Epoch 411/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3408 - val_loss: 1772.1991\n",
      "Epoch 412/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3408 - val_loss: 1772.1991\n",
      "Epoch 413/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3408 - val_loss: 1772.1993\n",
      "Epoch 414/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3408 - val_loss: 1772.1993\n",
      "Epoch 415/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3408 - val_loss: 1772.1998\n",
      "Epoch 416/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3408 - val_loss: 1772.2004\n",
      "Epoch 417/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3407 - val_loss: 1772.2009\n",
      "Epoch 418/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3407 - val_loss: 1772.2017\n",
      "Epoch 419/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3407 - val_loss: 1772.2024\n",
      "Epoch 420/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3407 - val_loss: 1772.2025\n",
      "Epoch 421/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3408 - val_loss: 1772.2028\n",
      "Epoch 422/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3408 - val_loss: 1772.2032\n",
      "Epoch 423/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3408 - val_loss: 1772.2042\n",
      "Epoch 424/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3407 - val_loss: 1772.2042\n",
      "Epoch 425/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3407 - val_loss: 1772.2042\n",
      "Epoch 426/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3407 - val_loss: 1772.2042\n",
      "Epoch 427/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3407 - val_loss: 1772.2042\n",
      "Epoch 428/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3408 - val_loss: 1772.2043\n",
      "Epoch 429/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 218.3408 - val_loss: 1772.2042\n",
      "Epoch 430/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3408 - val_loss: 1772.2034\n",
      "Epoch 431/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3408 - val_loss: 1772.2030\n",
      "Epoch 432/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3408 - val_loss: 1772.2028\n",
      "Epoch 433/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3408 - val_loss: 1772.2025\n",
      "Epoch 434/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3408 - val_loss: 1772.2025\n",
      "Epoch 435/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3408 - val_loss: 1772.2025\n",
      "Epoch 436/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3408 - val_loss: 1772.2025\n",
      "Epoch 437/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3409 - val_loss: 1772.2025\n",
      "Epoch 438/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3408 - val_loss: 1772.2023\n",
      "Epoch 439/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3408 - val_loss: 1772.2019\n",
      "Epoch 440/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3409 - val_loss: 1772.2014\n",
      "Epoch 441/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3409 - val_loss: 1772.2008\n",
      "Epoch 442/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3409 - val_loss: 1772.2003\n",
      "Epoch 443/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3409 - val_loss: 1772.1997\n",
      "Epoch 444/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3409 - val_loss: 1772.1991\n",
      "Epoch 445/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3410 - val_loss: 1772.1984\n",
      "Epoch 446/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3410 - val_loss: 1772.1981\n",
      "Epoch 447/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3411 - val_loss: 1772.1981\n",
      "Epoch 448/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3411 - val_loss: 1772.1978\n",
      "Epoch 449/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3412 - val_loss: 1772.1978\n",
      "Epoch 450/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3412 - val_loss: 1772.1978\n",
      "Epoch 451/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3412 - val_loss: 1772.1978\n",
      "Epoch 452/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 218.3412 - val_loss: 1772.1979\n",
      "Epoch 453/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 218.3412 - val_loss: 1772.1984\n",
      "Epoch 454/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3411 - val_loss: 1772.1991\n",
      "Epoch 455/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3411 - val_loss: 1772.1998\n",
      "Epoch 456/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3411 - val_loss: 1772.2003\n",
      "Epoch 457/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3411 - val_loss: 1772.2007\n",
      "Epoch 458/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3411 - val_loss: 1772.2009\n",
      "Epoch 459/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3411 - val_loss: 1772.2017\n",
      "Epoch 460/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3411 - val_loss: 1772.2024\n",
      "Epoch 461/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3411 - val_loss: 1772.2030\n",
      "Epoch 462/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3411 - val_loss: 1772.2039\n",
      "Epoch 463/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3410 - val_loss: 1772.2045\n",
      "Epoch 464/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3410 - val_loss: 1772.2050\n",
      "Epoch 465/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3410 - val_loss: 1772.2056\n",
      "Epoch 466/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3409 - val_loss: 1772.2058\n",
      "Epoch 467/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3409 - val_loss: 1772.2069\n",
      "Epoch 468/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3409 - val_loss: 1772.2074\n",
      "Epoch 469/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3408 - val_loss: 1772.2079\n",
      "Epoch 470/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3408 - val_loss: 1772.2090\n",
      "Epoch 471/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3408 - val_loss: 1772.2096\n",
      "Epoch 472/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3408 - val_loss: 1772.2111\n",
      "Epoch 473/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3407 - val_loss: 1772.2115\n",
      "Epoch 474/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 218.3407 - val_loss: 1772.2115\n",
      "Epoch 475/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3407 - val_loss: 1772.2115\n",
      "Epoch 476/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3407 - val_loss: 1772.2115\n",
      "Epoch 477/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3407 - val_loss: 1772.2115\n",
      "Epoch 478/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3407 - val_loss: 1772.2115\n",
      "Epoch 479/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3407 - val_loss: 1772.2115\n",
      "Epoch 480/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3407 - val_loss: 1772.2115\n",
      "Epoch 481/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3407 - val_loss: 1772.2107\n",
      "Epoch 482/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3408 - val_loss: 1772.2104\n",
      "Epoch 483/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3408 - val_loss: 1772.2096\n",
      "Epoch 484/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3408 - val_loss: 1772.2091\n",
      "Epoch 485/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3407 - val_loss: 1772.2086\n",
      "Epoch 486/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3408 - val_loss: 1772.2086\n",
      "Epoch 487/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3408 - val_loss: 1772.2076\n",
      "Epoch 488/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3409 - val_loss: 1772.2076\n",
      "Epoch 489/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3408 - val_loss: 1772.2074\n",
      "Epoch 490/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3409 - val_loss: 1772.2069\n",
      "Epoch 491/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3409 - val_loss: 1772.2068\n",
      "Epoch 492/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3409 - val_loss: 1772.2061\n",
      "Epoch 493/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3409 - val_loss: 1772.2056\n",
      "Epoch 494/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3409 - val_loss: 1772.2056\n",
      "Epoch 495/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 218.3409 - val_loss: 1772.2054\n",
      "Epoch 496/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 218.3410 - val_loss: 1772.2050\n",
      "Epoch 497/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3410 - val_loss: 1772.2048\n",
      "Epoch 498/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3410 - val_loss: 1772.2046\n",
      "Epoch 499/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 218.3410 - val_loss: 1772.2045\n",
      "Epoch 500/500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 218.3411 - val_loss: 1772.2045\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.0403, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0012, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0120, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.0334, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 321ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.42219188e+01, 7.40958684e+01, 7.39698179e+01, 7.38437675e+01,\n",
       "        7.86581497e+01, 0.00000000e+00, 0.00000000e+00, 1.42701268e-01,\n",
       "        0.00000000e+00, 3.05788219e-01, 6.89214470e-02, 0.00000000e+00,\n",
       "        4.89864558e-01, 7.43783520e+01, 7.43128058e+01, 7.41985761e+01,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.45361485e+01, 7.44706022e+01,\n",
       "        7.44050560e+01, 7.43395098e+01, 7.42499300e+01, 7.41238795e+01,\n",
       "        7.39978291e+01, 7.38717787e+01, 7.36588936e+01, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.43006676e+01, 7.41752334e+01, 7.40491830e+01,\n",
       "        7.39231326e+01, 7.37924136e+01, 7.34646825e+01, 7.31369514e+01,\n",
       "        7.28092204e+01, 7.24900327e+01, 3.53517710e-01, 6.65692930e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.91051570e-01, 3.35132570e-01,\n",
       "        0.00000000e+00, 1.01646010e-01, 2.20811410e-01, 7.38997899e+01,\n",
       "        7.37317227e+01, 0.00000000e+00, 8.25208310e-01, 7.43152334e+01,\n",
       "        7.42032446e+01, 7.40771942e+01, 7.39511438e+01, 7.38250934e+01,\n",
       "        7.35375117e+01, 7.32107806e+01, 7.28830495e+01, 7.25553184e+01,\n",
       "        0.00000000e+00, 4.64978340e-01, 7.38764472e+01, 7.36710317e+01,\n",
       "        7.33433007e+01, 7.30169569e+01, 7.26887839e+01, 7.54567740e+01,\n",
       "        7.47458497e+01, 7.44147666e+01, 1.20782113e+00, 4.50416830e-01,\n",
       "        2.35461760e-01, 5.17919350e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.08036410e-01, 0.00000000e+00, 1.81440070e-01,\n",
       "        7.09058228e+01, 1.74980745e-01, 7.77529418e-01, 1.07850611e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.40312099e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.80752993e-01, 5.55040240e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.64313629e-01, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([67.62225866, 67.61597322, 67.60968778, 67.60340234, 67.5971169 ,\n",
       "       67.59083145, 67.58454601, 67.57826057, 67.57197513, 67.56568969,\n",
       "       67.55940425, 67.55311881, 67.54683337, 67.54054793, 67.53426249,\n",
       "       67.52797704, 67.5216916 , 67.51540616, 67.50912072, 67.50283528,\n",
       "       67.49654984, 67.4902644 , 67.48397896, 67.47769352, 67.47140808,\n",
       "       67.46512263, 67.45883719, 67.45255175, 67.44626631, 67.43998087,\n",
       "       67.43369543, 67.42740999, 67.42112455, 67.41483911, 67.40855367,\n",
       "       67.40226822, 67.39598278, 67.38969734, 67.3834119 , 67.37712646,\n",
       "       67.37084102, 67.36455558, 67.35827014, 67.3519847 , 67.34569926,\n",
       "       67.33941381, 67.33312837, 67.32684293, 67.32055749, 67.31427205,\n",
       "       67.30798661, 67.30170117, 67.29541573, 67.28913029, 67.28284485,\n",
       "       67.2765594 , 67.27027396, 67.26398852, 67.25770308, 67.25141764,\n",
       "       67.2451322 , 67.23884676, 67.23256132, 67.22627588, 67.21999044,\n",
       "       67.21370499, 67.20741955, 67.20113411, 67.19484867, 67.18856323,\n",
       "       67.18227779, 67.17599235, 67.16970691, 67.16342147, 67.15713603,\n",
       "       67.15085058, 67.14456514, 67.1382797 , 67.13199426, 67.12570882,\n",
       "       67.11942338, 67.11313794, 67.1068525 , 67.10056706, 67.09428162,\n",
       "       67.08799617, 67.08171073, 67.07542529, 67.06913985, 67.06285441,\n",
       "       67.05656897, 67.05028353, 67.04399809, 67.03771265, 67.03142721,\n",
       "       67.02514176, 67.01885632, 67.01257088, 67.00628544, 67.        ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.045208735950546\n",
      "38.693233976549756\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
