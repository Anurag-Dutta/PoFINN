{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.200000\n",
       "1       88.931092\n",
       "2       88.662185\n",
       "3       88.393277\n",
       "4       88.124370\n",
       "          ...    \n",
       "1695    60.441303\n",
       "1696    60.432899\n",
       "1697    60.424496\n",
       "1698    60.416092\n",
       "1699    60.407689\n",
       "Name: C1, Length: 1700, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c1_interpolated_1600_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.200000\n",
       "1       88.931092\n",
       "2       88.662185\n",
       "3       88.393277\n",
       "4       88.124370\n",
       "          ...    \n",
       "1595     0.000000\n",
       "1596     0.000000\n",
       "1597     0.936131\n",
       "1598     0.000000\n",
       "1599     0.227903\n",
       "Name: C1, Length: 1600, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1600)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.931092</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.662185</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.393277</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.124370</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     89.200000  0.000298\n",
       "1     88.931092  0.000298\n",
       "2     88.662185  0.000297\n",
       "3     88.393277  0.000297\n",
       "4     88.124370  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6465  # Value for C0\n",
    "K0 = -0.0029  # Value for K0\n",
    "K1 = -0.0003  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0168    # Value for b\n",
    "c = 2.3581    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    89.200000    88.931092    88.662185    88.393277    88.124370   \n",
      "351    88.931092    88.662185    88.393277    88.124370    87.855462   \n",
      "352    88.662185    88.393277    88.124370    87.855462    87.586555   \n",
      "353    88.393277    88.124370    87.855462    87.586555    87.317647   \n",
      "354    88.124370    87.855462    87.586555    87.317647    87.048739   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.855462    87.586555    87.317647    87.048739    86.794538  ...   \n",
      "351    87.586555    87.317647    87.048739    86.794538    86.721709  ...   \n",
      "352    87.317647    87.048739    86.794538    86.721709    86.648880  ...   \n",
      "353    87.048739    86.794538    86.721709    86.648880    86.576050  ...   \n",
      "354    86.794538    86.721709    86.648880    86.576050    86.503221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   73.989683    0.000263   73.957937    0.000263   73.926190    0.000263   \n",
      "351   73.957937    0.000263   73.926190    0.000263   73.894444    0.000262   \n",
      "352   73.926190    0.000263   73.894444    0.000262   73.862698    0.000262   \n",
      "353   73.894444    0.000262   73.862698    0.000262   73.830952    0.000262   \n",
      "354   73.862698    0.000262   73.830952    0.000262   73.799206    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   73.894444    0.000262   73.862698    0.000262  \n",
      "351   73.862698    0.000262   73.830952    0.000262  \n",
      "352   73.830952    0.000262   73.799206    0.000262  \n",
      "353   73.799206    0.000262   73.767460    0.000262  \n",
      "354   73.767460    0.000262   73.735714    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1150, 1, 251) (1150, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "15/15 [==============================] - 3s 34ms/step - loss: 4629.4429 - val_loss: 3688.2930\n",
      "Epoch 2/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4555.5552 - val_loss: 3637.0652\n",
      "Epoch 3/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4489.3198 - val_loss: 3573.5334\n",
      "Epoch 4/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4427.2842 - val_loss: 3519.9380\n",
      "Epoch 5/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4354.1719 - val_loss: 3453.9011\n",
      "Epoch 6/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4293.4629 - val_loss: 3400.8870\n",
      "Epoch 7/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4234.6777 - val_loss: 3348.7244\n",
      "Epoch 8/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4176.7725 - val_loss: 3297.3484\n",
      "Epoch 9/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 4119.6606 - val_loss: 3246.6846\n",
      "Epoch 10/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4063.2742 - val_loss: 3196.6824\n",
      "Epoch 11/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4007.5640 - val_loss: 3147.3083\n",
      "Epoch 12/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3952.5002 - val_loss: 3098.5398\n",
      "Epoch 13/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3898.0603 - val_loss: 3050.3589\n",
      "Epoch 14/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3844.2261 - val_loss: 3002.7522\n",
      "Epoch 15/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3790.9861 - val_loss: 2955.7085\n",
      "Epoch 16/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3738.3284 - val_loss: 2909.2185\n",
      "Epoch 17/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3686.2427 - val_loss: 2863.2747\n",
      "Epoch 18/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3634.7227 - val_loss: 2817.8691\n",
      "Epoch 19/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3583.7603 - val_loss: 2772.9956\n",
      "Epoch 20/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3533.3494 - val_loss: 2728.6482\n",
      "Epoch 21/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3483.4832 - val_loss: 2684.8206\n",
      "Epoch 22/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3434.1555 - val_loss: 2641.5085\n",
      "Epoch 23/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3385.3623 - val_loss: 2598.7058\n",
      "Epoch 24/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3337.0972 - val_loss: 2556.4080\n",
      "Epoch 25/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3289.3560 - val_loss: 2514.6104\n",
      "Epoch 26/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3242.1340 - val_loss: 2473.3091\n",
      "Epoch 27/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3195.4265 - val_loss: 2432.4983\n",
      "Epoch 28/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3149.2283 - val_loss: 2392.1746\n",
      "Epoch 29/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3103.5354 - val_loss: 2352.3335\n",
      "Epoch 30/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3058.3442 - val_loss: 2312.9709\n",
      "Epoch 31/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3013.6504 - val_loss: 2274.0823\n",
      "Epoch 32/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2969.4490 - val_loss: 2235.6648\n",
      "Epoch 33/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2925.7361 - val_loss: 2197.7134\n",
      "Epoch 34/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2882.5088 - val_loss: 2160.2244\n",
      "Epoch 35/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2839.7620 - val_loss: 2123.1943\n",
      "Epoch 36/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2797.4924 - val_loss: 2086.6194\n",
      "Epoch 37/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2755.6968 - val_loss: 2050.4949\n",
      "Epoch 38/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2714.3696 - val_loss: 2014.8185\n",
      "Epoch 39/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2673.5090 - val_loss: 1979.5856\n",
      "Epoch 40/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2633.1101 - val_loss: 1944.7930\n",
      "Epoch 41/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2593.1697 - val_loss: 1910.4363\n",
      "Epoch 42/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2553.6841 - val_loss: 1876.5129\n",
      "Epoch 43/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2514.6504 - val_loss: 1843.0193\n",
      "Epoch 44/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2476.0642 - val_loss: 1809.9512\n",
      "Epoch 45/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2437.9214 - val_loss: 1777.3054\n",
      "Epoch 46/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2400.2205 - val_loss: 1745.0785\n",
      "Epoch 47/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2362.9561 - val_loss: 1713.2681\n",
      "Epoch 48/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2326.1255 - val_loss: 1681.8687\n",
      "Epoch 49/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2289.7256 - val_loss: 1650.8784\n",
      "Epoch 50/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2253.7522 - val_loss: 1620.2933\n",
      "Epoch 51/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2218.2034 - val_loss: 1590.1099\n",
      "Epoch 52/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2183.0742 - val_loss: 1560.3254\n",
      "Epoch 53/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2148.3621 - val_loss: 1530.9366\n",
      "Epoch 54/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2114.0635 - val_loss: 1501.9390\n",
      "Epoch 55/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2080.1753 - val_loss: 1473.3313\n",
      "Epoch 56/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2046.6947 - val_loss: 1445.1083\n",
      "Epoch 57/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2013.6179 - val_loss: 1417.2681\n",
      "Epoch 58/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1980.9418 - val_loss: 1389.8066\n",
      "Epoch 59/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1948.6632 - val_loss: 1362.7217\n",
      "Epoch 60/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1916.7789 - val_loss: 1336.0094\n",
      "Epoch 61/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1885.2856 - val_loss: 1309.6667\n",
      "Epoch 62/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1854.1805 - val_loss: 1283.6904\n",
      "Epoch 63/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1823.4602 - val_loss: 1258.0773\n",
      "Epoch 64/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 1793.1218 - val_loss: 1232.8247\n",
      "Epoch 65/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1763.1622 - val_loss: 1207.9294\n",
      "Epoch 66/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1733.5782 - val_loss: 1183.3883\n",
      "Epoch 67/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1704.3668 - val_loss: 1159.1981\n",
      "Epoch 68/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1675.5249 - val_loss: 1135.3560\n",
      "Epoch 69/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1647.0502 - val_loss: 1111.8593\n",
      "Epoch 70/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1618.9384 - val_loss: 1088.7040\n",
      "Epoch 71/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1591.1875 - val_loss: 1065.8881\n",
      "Epoch 72/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1563.7936 - val_loss: 1043.4080\n",
      "Epoch 73/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1536.7546 - val_loss: 1021.2610\n",
      "Epoch 74/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1510.0675 - val_loss: 999.4442\n",
      "Epoch 75/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1483.7288 - val_loss: 977.9545\n",
      "Epoch 76/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1457.7360 - val_loss: 956.7889\n",
      "Epoch 77/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1432.0861 - val_loss: 935.9452\n",
      "Epoch 78/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1406.7762 - val_loss: 915.4193\n",
      "Epoch 79/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1375.7242 - val_loss: 887.5575\n",
      "Epoch 80/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1346.0242 - val_loss: 864.3965\n",
      "Epoch 81/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1318.0010 - val_loss: 842.1741\n",
      "Epoch 82/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1291.0070 - val_loss: 820.7512\n",
      "Epoch 83/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1264.8561 - val_loss: 799.9855\n",
      "Epoch 84/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 1239.4036 - val_loss: 779.7844\n",
      "Epoch 85/500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1214.5548 - val_loss: 760.0867\n",
      "Epoch 86/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 1190.2479 - val_loss: 740.8510\n",
      "Epoch 87/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1166.4398 - val_loss: 722.0467\n",
      "Epoch 88/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1143.0975 - val_loss: 703.6511\n",
      "Epoch 89/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1120.1974 - val_loss: 685.6452\n",
      "Epoch 90/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1097.7186 - val_loss: 668.0143\n",
      "Epoch 91/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1075.6458 - val_loss: 650.7454\n",
      "Epoch 92/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1053.9650 - val_loss: 633.8276\n",
      "Epoch 93/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1032.6643 - val_loss: 617.2518\n",
      "Epoch 94/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1011.7338 - val_loss: 601.0084\n",
      "Epoch 95/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 991.1636 - val_loss: 585.0906\n",
      "Epoch 96/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 970.9455 - val_loss: 569.4908\n",
      "Epoch 97/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 951.0727 - val_loss: 554.2026\n",
      "Epoch 98/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 931.5378 - val_loss: 539.2201\n",
      "Epoch 99/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 912.3340 - val_loss: 524.5371\n",
      "Epoch 100/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 893.4557 - val_loss: 510.1488\n",
      "Epoch 101/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 874.8971 - val_loss: 496.0500\n",
      "Epoch 102/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 856.6532 - val_loss: 482.2358\n",
      "Epoch 103/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 838.7183 - val_loss: 468.7009\n",
      "Epoch 104/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 821.0876 - val_loss: 455.4419\n",
      "Epoch 105/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 803.7572 - val_loss: 442.4540\n",
      "Epoch 106/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 786.7219 - val_loss: 429.7327\n",
      "Epoch 107/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 769.9776 - val_loss: 417.2745\n",
      "Epoch 108/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 753.5203 - val_loss: 405.0758\n",
      "Epoch 109/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 737.3457 - val_loss: 393.1313\n",
      "Epoch 110/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 721.4496 - val_loss: 381.4382\n",
      "Epoch 111/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 705.8286 - val_loss: 369.9933\n",
      "Epoch 112/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 690.4792 - val_loss: 358.7926\n",
      "Epoch 113/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 675.3976 - val_loss: 347.8321\n",
      "Epoch 114/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 660.5795 - val_loss: 337.1090\n",
      "Epoch 115/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 646.0222 - val_loss: 326.6196\n",
      "Epoch 116/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 631.7222 - val_loss: 316.3610\n",
      "Epoch 117/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 617.6756 - val_loss: 306.3290\n",
      "Epoch 118/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 603.8792 - val_loss: 296.5215\n",
      "Epoch 119/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 590.3301 - val_loss: 286.9342\n",
      "Epoch 120/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 577.0250 - val_loss: 277.5647\n",
      "Epoch 121/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 563.9607 - val_loss: 268.4095\n",
      "Epoch 122/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 551.1338 - val_loss: 259.4662\n",
      "Epoch 123/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 538.5417 - val_loss: 250.7309\n",
      "Epoch 124/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 526.1810 - val_loss: 242.2014\n",
      "Epoch 125/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 514.0484 - val_loss: 233.8742\n",
      "Epoch 126/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 502.1416 - val_loss: 225.7464\n",
      "Epoch 127/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 490.4572 - val_loss: 217.8152\n",
      "Epoch 128/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 478.9925 - val_loss: 210.0775\n",
      "Epoch 129/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 467.7448 - val_loss: 202.5304\n",
      "Epoch 130/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 456.7104 - val_loss: 195.1719\n",
      "Epoch 131/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 445.8876 - val_loss: 187.9983\n",
      "Epoch 132/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 435.2728 - val_loss: 181.0072\n",
      "Epoch 133/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 424.8636 - val_loss: 174.1958\n",
      "Epoch 134/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 414.6573 - val_loss: 167.5611\n",
      "Epoch 135/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 404.6508 - val_loss: 161.1009\n",
      "Epoch 136/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 394.8418 - val_loss: 154.8120\n",
      "Epoch 137/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 385.2274 - val_loss: 148.6922\n",
      "Epoch 138/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 375.8052 - val_loss: 142.7383\n",
      "Epoch 139/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 366.5721 - val_loss: 136.9482\n",
      "Epoch 140/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 357.5259 - val_loss: 131.3188\n",
      "Epoch 141/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 348.6639 - val_loss: 125.8482\n",
      "Epoch 142/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 339.9835 - val_loss: 120.5330\n",
      "Epoch 143/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 331.4821 - val_loss: 115.3711\n",
      "Epoch 144/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 323.1572 - val_loss: 110.3601\n",
      "Epoch 145/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 315.0064 - val_loss: 105.4975\n",
      "Epoch 146/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 307.0271 - val_loss: 100.7798\n",
      "Epoch 147/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 299.2166 - val_loss: 96.2057\n",
      "Epoch 148/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 291.5726 - val_loss: 91.7724\n",
      "Epoch 149/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 284.0927 - val_loss: 87.4773\n",
      "Epoch 150/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 276.7747 - val_loss: 83.3179\n",
      "Epoch 151/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 269.6159 - val_loss: 79.2921\n",
      "Epoch 152/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 262.6139 - val_loss: 75.3973\n",
      "Epoch 153/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 255.7665 - val_loss: 71.6311\n",
      "Epoch 154/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 249.0711 - val_loss: 67.9915\n",
      "Epoch 155/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 242.5255 - val_loss: 64.4759\n",
      "Epoch 156/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 236.1277 - val_loss: 61.0814\n",
      "Epoch 157/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 229.8748 - val_loss: 57.8067\n",
      "Epoch 158/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 223.7649 - val_loss: 54.6489\n",
      "Epoch 159/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 217.7957 - val_loss: 51.6058\n",
      "Epoch 160/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 211.9648 - val_loss: 48.6753\n",
      "Epoch 161/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 206.2700 - val_loss: 45.8552\n",
      "Epoch 162/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 200.7092 - val_loss: 43.1429\n",
      "Epoch 163/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 195.2799 - val_loss: 40.5367\n",
      "Epoch 164/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 189.9804 - val_loss: 38.0340\n",
      "Epoch 165/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 184.8081 - val_loss: 35.6330\n",
      "Epoch 166/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 179.7612 - val_loss: 33.3312\n",
      "Epoch 167/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 174.8372 - val_loss: 31.1267\n",
      "Epoch 168/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 170.0343 - val_loss: 29.0174\n",
      "Epoch 169/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 165.3500 - val_loss: 27.0009\n",
      "Epoch 170/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 160.7826 - val_loss: 25.0754\n",
      "Epoch 171/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 156.3300 - val_loss: 23.2390\n",
      "Epoch 172/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 151.9900 - val_loss: 21.4893\n",
      "Epoch 173/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 147.7607 - val_loss: 19.8244\n",
      "Epoch 174/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 143.6398 - val_loss: 18.2426\n",
      "Epoch 175/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 139.6259 - val_loss: 16.7415\n",
      "Epoch 176/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 135.7165 - val_loss: 15.3195\n",
      "Epoch 177/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 131.9098 - val_loss: 13.9742\n",
      "Epoch 178/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 128.2037 - val_loss: 12.7041\n",
      "Epoch 179/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 124.5966 - val_loss: 11.5070\n",
      "Epoch 180/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 121.0862 - val_loss: 10.3813\n",
      "Epoch 181/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 117.6708 - val_loss: 9.3251\n",
      "Epoch 182/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 114.3489 - val_loss: 8.3363\n",
      "Epoch 183/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 111.1181 - val_loss: 7.4134\n",
      "Epoch 184/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 107.9767 - val_loss: 6.5543\n",
      "Epoch 185/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 104.9230 - val_loss: 5.7574\n",
      "Epoch 186/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 101.9553 - val_loss: 5.0209\n",
      "Epoch 187/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 99.0714 - val_loss: 4.3432\n",
      "Epoch 188/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 96.2702 - val_loss: 3.7224\n",
      "Epoch 189/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 93.5493 - val_loss: 3.1569\n",
      "Epoch 190/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 90.9075 - val_loss: 2.6450\n",
      "Epoch 191/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 88.3427 - val_loss: 2.1850\n",
      "Epoch 192/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 85.8536 - val_loss: 1.7754\n",
      "Epoch 193/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 83.4382 - val_loss: 1.4145\n",
      "Epoch 194/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 81.0951 - val_loss: 1.1008\n",
      "Epoch 195/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 78.8225 - val_loss: 0.8326\n",
      "Epoch 196/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 76.6189 - val_loss: 0.6085\n",
      "Epoch 197/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 74.4827 - val_loss: 0.4270\n",
      "Epoch 198/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 72.4124 - val_loss: 0.2865\n",
      "Epoch 199/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 70.4064 - val_loss: 0.1856\n",
      "Epoch 200/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 68.4633 - val_loss: 0.1228\n",
      "Epoch 201/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 66.5814 - val_loss: 0.0968\n",
      "Epoch 202/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 64.7595 - val_loss: 0.1061\n",
      "Epoch 203/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 62.9957 - val_loss: 0.1493\n",
      "Epoch 204/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 61.2890 - val_loss: 0.2252\n",
      "Epoch 205/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 59.6379 - val_loss: 0.3323\n",
      "Epoch 206/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 58.0408 - val_loss: 0.4694\n",
      "Epoch 207/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 56.4966 - val_loss: 0.6352\n",
      "Epoch 208/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 55.0036 - val_loss: 0.8284\n",
      "Epoch 209/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 53.5609 - val_loss: 1.0479\n",
      "Epoch 210/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 52.1668 - val_loss: 1.2924\n",
      "Epoch 211/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 50.8203 - val_loss: 1.5607\n",
      "Epoch 212/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 49.5201 - val_loss: 1.8516\n",
      "Epoch 213/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 48.2647 - val_loss: 2.1641\n",
      "Epoch 214/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 47.0531 - val_loss: 2.4970\n",
      "Epoch 215/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 45.8841 - val_loss: 2.8492\n",
      "Epoch 216/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 44.7566 - val_loss: 3.2198\n",
      "Epoch 217/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 43.6692 - val_loss: 3.6075\n",
      "Epoch 218/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 42.6209 - val_loss: 4.0115\n",
      "Epoch 219/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 41.6105 - val_loss: 4.4307\n",
      "Epoch 220/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 40.6370 - val_loss: 4.8641\n",
      "Epoch 221/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 39.6992 - val_loss: 5.3109\n",
      "Epoch 222/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 38.7961 - val_loss: 5.7702\n",
      "Epoch 223/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 37.9267 - val_loss: 6.2409\n",
      "Epoch 224/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 37.0900 - val_loss: 6.7223\n",
      "Epoch 225/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 36.2850 - val_loss: 7.2134\n",
      "Epoch 226/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 35.5107 - val_loss: 7.7135\n",
      "Epoch 227/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 34.7661 - val_loss: 8.2218\n",
      "Epoch 228/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 34.0504 - val_loss: 8.7375\n",
      "Epoch 229/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 33.3625 - val_loss: 9.2599\n",
      "Epoch 230/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 32.7017 - val_loss: 9.7881\n",
      "Epoch 231/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 32.0670 - val_loss: 10.3214\n",
      "Epoch 232/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 31.4576 - val_loss: 10.8594\n",
      "Epoch 233/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 30.8727 - val_loss: 11.4012\n",
      "Epoch 234/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 30.3113 - val_loss: 11.9461\n",
      "Epoch 235/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 29.7729 - val_loss: 12.4937\n",
      "Epoch 236/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 29.2565 - val_loss: 13.0432\n",
      "Epoch 237/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 28.7615 - val_loss: 13.5942\n",
      "Epoch 238/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 28.2869 - val_loss: 14.1462\n",
      "Epoch 239/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 27.8323 - val_loss: 14.6982\n",
      "Epoch 240/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 27.3969 - val_loss: 15.2503\n",
      "Epoch 241/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 26.9798 - val_loss: 15.8014\n",
      "Epoch 242/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 26.5808 - val_loss: 16.3514\n",
      "Epoch 243/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 26.1989 - val_loss: 16.8999\n",
      "Epoch 244/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 25.8334 - val_loss: 17.4463\n",
      "Epoch 245/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.4839 - val_loss: 17.9902\n",
      "Epoch 246/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.1497 - val_loss: 18.5312\n",
      "Epoch 247/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 24.8304 - val_loss: 19.0691\n",
      "Epoch 248/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 24.5252 - val_loss: 19.6032\n",
      "Epoch 249/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 24.2337 - val_loss: 20.1334\n",
      "Epoch 250/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 23.9553 - val_loss: 20.6594\n",
      "Epoch 251/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 23.6896 - val_loss: 21.1809\n",
      "Epoch 252/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 23.4359 - val_loss: 21.6975\n",
      "Epoch 253/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 23.1939 - val_loss: 22.2088\n",
      "Epoch 254/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 22.9631 - val_loss: 22.7150\n",
      "Epoch 255/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 22.7430 - val_loss: 23.2153\n",
      "Epoch 256/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 22.5332 - val_loss: 23.7098\n",
      "Epoch 257/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 22.3333 - val_loss: 24.1981\n",
      "Epoch 258/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 22.1428 - val_loss: 24.6804\n",
      "Epoch 259/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 21.9614 - val_loss: 25.1561\n",
      "Epoch 260/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 21.7887 - val_loss: 25.6252\n",
      "Epoch 261/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 21.6242 - val_loss: 26.0877\n",
      "Epoch 262/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 21.4677 - val_loss: 26.5432\n",
      "Epoch 263/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 21.3189 - val_loss: 26.9916\n",
      "Epoch 264/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 21.1773 - val_loss: 27.4329\n",
      "Epoch 265/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 21.0427 - val_loss: 27.8671\n",
      "Epoch 266/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 20.9146 - val_loss: 28.2938\n",
      "Epoch 267/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 20.7930 - val_loss: 28.7130\n",
      "Epoch 268/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 20.6775 - val_loss: 29.1248\n",
      "Epoch 269/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 20.5678 - val_loss: 29.5291\n",
      "Epoch 270/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 20.4635 - val_loss: 29.9258\n",
      "Epoch 271/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 20.3646 - val_loss: 30.3150\n",
      "Epoch 272/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 20.2707 - val_loss: 30.6966\n",
      "Epoch 273/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 20.1815 - val_loss: 31.0705\n",
      "Epoch 274/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 20.0970 - val_loss: 31.4366\n",
      "Epoch 275/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 20.0168 - val_loss: 31.7950\n",
      "Epoch 276/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 19.9407 - val_loss: 32.1458\n",
      "Epoch 277/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 19.8687 - val_loss: 32.4889\n",
      "Epoch 278/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 19.8004 - val_loss: 32.8244\n",
      "Epoch 279/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 19.7356 - val_loss: 33.1522\n",
      "Epoch 280/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 19.6743 - val_loss: 33.4726\n",
      "Epoch 281/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 19.6162 - val_loss: 33.7855\n",
      "Epoch 282/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 19.5611 - val_loss: 34.0910\n",
      "Epoch 283/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 19.5090 - val_loss: 34.3890\n",
      "Epoch 284/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 19.4597 - val_loss: 34.6797\n",
      "Epoch 285/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 19.4129 - val_loss: 34.9632\n",
      "Epoch 286/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 19.3687 - val_loss: 35.2394\n",
      "Epoch 287/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 19.3269 - val_loss: 35.5086\n",
      "Epoch 288/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 19.2873 - val_loss: 35.7705\n",
      "Epoch 289/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 19.2499 - val_loss: 36.0257\n",
      "Epoch 290/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 19.2144 - val_loss: 36.2739\n",
      "Epoch 291/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 19.1809 - val_loss: 36.5153\n",
      "Epoch 292/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 19.1492 - val_loss: 36.7500\n",
      "Epoch 293/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 19.1192 - val_loss: 36.9783\n",
      "Epoch 294/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 19.0909 - val_loss: 37.1998\n",
      "Epoch 295/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 19.0642 - val_loss: 37.4152\n",
      "Epoch 296/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 19.0388 - val_loss: 37.6242\n",
      "Epoch 297/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 19.0149 - val_loss: 37.8268\n",
      "Epoch 298/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.9923 - val_loss: 38.0237\n",
      "Epoch 299/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.9709 - val_loss: 38.2144\n",
      "Epoch 300/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.9507 - val_loss: 38.3992\n",
      "Epoch 301/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.9317 - val_loss: 38.5787\n",
      "Epoch 302/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.9136 - val_loss: 38.7523\n",
      "Epoch 303/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.8965 - val_loss: 38.9202\n",
      "Epoch 304/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 18.8805 - val_loss: 39.0827\n",
      "Epoch 305/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 18.8653 - val_loss: 39.2400\n",
      "Epoch 306/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.8509 - val_loss: 39.3922\n",
      "Epoch 307/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.8374 - val_loss: 39.5395\n",
      "Epoch 308/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.8245 - val_loss: 39.6815\n",
      "Epoch 309/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.8124 - val_loss: 39.8188\n",
      "Epoch 310/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.8010 - val_loss: 39.9517\n",
      "Epoch 311/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.7902 - val_loss: 40.0796\n",
      "Epoch 312/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.7799 - val_loss: 40.2029\n",
      "Epoch 313/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 18.7704 - val_loss: 40.3225\n",
      "Epoch 314/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 18.7613 - val_loss: 40.4373\n",
      "Epoch 315/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.7526 - val_loss: 40.5480\n",
      "Epoch 316/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.7445 - val_loss: 40.6547\n",
      "Epoch 317/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.7369 - val_loss: 40.7576\n",
      "Epoch 318/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.7296 - val_loss: 40.8567\n",
      "Epoch 319/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.7228 - val_loss: 40.9520\n",
      "Epoch 320/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.7163 - val_loss: 41.0438\n",
      "Epoch 321/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 18.7102 - val_loss: 41.1321\n",
      "Epoch 322/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.7044 - val_loss: 41.2169\n",
      "Epoch 323/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6990 - val_loss: 41.2984\n",
      "Epoch 324/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6939 - val_loss: 41.3767\n",
      "Epoch 325/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6891 - val_loss: 41.4518\n",
      "Epoch 326/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6845 - val_loss: 41.5244\n",
      "Epoch 327/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6801 - val_loss: 41.5936\n",
      "Epoch 328/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6761 - val_loss: 41.6602\n",
      "Epoch 329/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6722 - val_loss: 41.7240\n",
      "Epoch 330/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6686 - val_loss: 41.7851\n",
      "Epoch 331/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6652 - val_loss: 41.8436\n",
      "Epoch 332/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6619 - val_loss: 41.8995\n",
      "Epoch 333/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6589 - val_loss: 41.9534\n",
      "Epoch 334/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6561 - val_loss: 42.0050\n",
      "Epoch 335/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 18.6534 - val_loss: 42.0540\n",
      "Epoch 336/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6508 - val_loss: 42.1011\n",
      "Epoch 337/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6484 - val_loss: 42.1460\n",
      "Epoch 338/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6462 - val_loss: 42.1890\n",
      "Epoch 339/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6440 - val_loss: 42.2299\n",
      "Epoch 340/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6421 - val_loss: 42.2690\n",
      "Epoch 341/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 18.6402 - val_loss: 42.3066\n",
      "Epoch 342/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6384 - val_loss: 42.3423\n",
      "Epoch 343/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6368 - val_loss: 42.3761\n",
      "Epoch 344/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6353 - val_loss: 42.4086\n",
      "Epoch 345/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6339 - val_loss: 42.4397\n",
      "Epoch 346/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6324 - val_loss: 42.4692\n",
      "Epoch 347/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6312 - val_loss: 42.4973\n",
      "Epoch 348/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6300 - val_loss: 42.5238\n",
      "Epoch 349/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 18.6289 - val_loss: 42.5492\n",
      "Epoch 350/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6279 - val_loss: 42.5734\n",
      "Epoch 351/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6269 - val_loss: 42.5965\n",
      "Epoch 352/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6260 - val_loss: 42.6183\n",
      "Epoch 353/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6252 - val_loss: 42.6389\n",
      "Epoch 354/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6244 - val_loss: 42.6587\n",
      "Epoch 355/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6237 - val_loss: 42.6774\n",
      "Epoch 356/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6231 - val_loss: 42.6952\n",
      "Epoch 357/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6224 - val_loss: 42.7120\n",
      "Epoch 358/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 18.6218 - val_loss: 42.7278\n",
      "Epoch 359/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6213 - val_loss: 42.7426\n",
      "Epoch 360/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6209 - val_loss: 42.7571\n",
      "Epoch 361/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6205 - val_loss: 42.7706\n",
      "Epoch 362/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6201 - val_loss: 42.7834\n",
      "Epoch 363/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 18.6197 - val_loss: 42.7954\n",
      "Epoch 364/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 18.6194 - val_loss: 42.8067\n",
      "Epoch 365/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6192 - val_loss: 42.8173\n",
      "Epoch 366/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6190 - val_loss: 42.8275\n",
      "Epoch 367/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6187 - val_loss: 42.8372\n",
      "Epoch 368/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 18.6185 - val_loss: 42.8464\n",
      "Epoch 369/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6183 - val_loss: 42.8547\n",
      "Epoch 370/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6183 - val_loss: 42.8629\n",
      "Epoch 371/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6181 - val_loss: 42.8703\n",
      "Epoch 372/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6181 - val_loss: 42.8773\n",
      "Epoch 373/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6180 - val_loss: 42.8839\n",
      "Epoch 374/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6180 - val_loss: 42.8903\n",
      "Epoch 375/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6180 - val_loss: 42.8961\n",
      "Epoch 376/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6179 - val_loss: 42.9017\n",
      "Epoch 377/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6179 - val_loss: 42.9068\n",
      "Epoch 378/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 18.6180 - val_loss: 42.9118\n",
      "Epoch 379/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6180 - val_loss: 42.9161\n",
      "Epoch 380/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6181 - val_loss: 42.9204\n",
      "Epoch 381/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6182 - val_loss: 42.9243\n",
      "Epoch 382/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6183 - val_loss: 42.9280\n",
      "Epoch 383/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6184 - val_loss: 42.9315\n",
      "Epoch 384/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6185 - val_loss: 42.9344\n",
      "Epoch 385/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 18.6187 - val_loss: 42.9375\n",
      "Epoch 386/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6188 - val_loss: 42.9402\n",
      "Epoch 387/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6190 - val_loss: 42.9426\n",
      "Epoch 388/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6191 - val_loss: 42.9451\n",
      "Epoch 389/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6193 - val_loss: 42.9474\n",
      "Epoch 390/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6195 - val_loss: 42.9496\n",
      "Epoch 391/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6196 - val_loss: 42.9515\n",
      "Epoch 392/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6198 - val_loss: 42.9527\n",
      "Epoch 393/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 18.6200 - val_loss: 42.9544\n",
      "Epoch 394/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 18.6203 - val_loss: 42.9561\n",
      "Epoch 395/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6204 - val_loss: 42.9571\n",
      "Epoch 396/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6207 - val_loss: 42.9584\n",
      "Epoch 397/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6209 - val_loss: 42.9594\n",
      "Epoch 398/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6211 - val_loss: 42.9603\n",
      "Epoch 399/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6214 - val_loss: 42.9612\n",
      "Epoch 400/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6216 - val_loss: 42.9620\n",
      "Epoch 401/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6218 - val_loss: 42.9625\n",
      "Epoch 402/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6221 - val_loss: 42.9629\n",
      "Epoch 403/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 18.6224 - val_loss: 42.9637\n",
      "Epoch 404/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6226 - val_loss: 42.9642\n",
      "Epoch 405/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6228 - val_loss: 42.9645\n",
      "Epoch 406/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6231 - val_loss: 42.9647\n",
      "Epoch 407/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6234 - val_loss: 42.9650\n",
      "Epoch 408/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6236 - val_loss: 42.9650\n",
      "Epoch 409/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6239 - val_loss: 42.9652\n",
      "Epoch 410/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6242 - val_loss: 42.9653\n",
      "Epoch 411/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6245 - val_loss: 42.9655\n",
      "Epoch 412/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 18.6247 - val_loss: 42.9653\n",
      "Epoch 413/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6250 - val_loss: 42.9653\n",
      "Epoch 414/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6252 - val_loss: 42.9652\n",
      "Epoch 415/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 18.6255 - val_loss: 42.9650\n",
      "Epoch 416/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 18.6258 - val_loss: 42.9650\n",
      "Epoch 417/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 18.6260 - val_loss: 42.9650\n",
      "Epoch 418/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6263 - val_loss: 42.9648\n",
      "Epoch 419/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6265 - val_loss: 42.9646\n",
      "Epoch 420/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6269 - val_loss: 42.9644\n",
      "Epoch 421/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 18.6271 - val_loss: 42.9641\n",
      "Epoch 422/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 18.6274 - val_loss: 42.9636\n",
      "Epoch 423/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6277 - val_loss: 42.9634\n",
      "Epoch 424/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6280 - val_loss: 42.9630\n",
      "Epoch 425/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 18.6283 - val_loss: 42.9626\n",
      "Epoch 426/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6285 - val_loss: 42.9624\n",
      "Epoch 427/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6288 - val_loss: 42.9623\n",
      "Epoch 428/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 18.6290 - val_loss: 42.9618\n",
      "Epoch 429/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6293 - val_loss: 42.9613\n",
      "Epoch 430/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 18.6296 - val_loss: 42.9609\n",
      "Epoch 431/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6299 - val_loss: 42.9606\n",
      "Epoch 432/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 18.6302 - val_loss: 42.9600\n",
      "Epoch 433/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6304 - val_loss: 42.9595\n",
      "Epoch 434/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 18.6307 - val_loss: 42.9590\n",
      "Epoch 435/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 18.6310 - val_loss: 42.9586\n",
      "Epoch 436/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6312 - val_loss: 42.9582\n",
      "Epoch 437/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 18.6315 - val_loss: 42.9578\n",
      "Epoch 438/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6318 - val_loss: 42.9574\n",
      "Epoch 439/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 18.6320 - val_loss: 42.9569\n",
      "Epoch 440/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6323 - val_loss: 42.9565\n",
      "Epoch 441/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 18.6325 - val_loss: 42.9559\n",
      "Epoch 442/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6328 - val_loss: 42.9554\n",
      "Epoch 443/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6331 - val_loss: 42.9549\n",
      "Epoch 444/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6334 - val_loss: 42.9547\n",
      "Epoch 445/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6336 - val_loss: 42.9544\n",
      "Epoch 446/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6338 - val_loss: 42.9537\n",
      "Epoch 447/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6341 - val_loss: 42.9533\n",
      "Epoch 448/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 18.6344 - val_loss: 42.9527\n",
      "Epoch 449/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6346 - val_loss: 42.9525\n",
      "Epoch 450/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6348 - val_loss: 42.9520\n",
      "Epoch 451/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6351 - val_loss: 42.9514\n",
      "Epoch 452/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6354 - val_loss: 42.9510\n",
      "Epoch 453/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6356 - val_loss: 42.9506\n",
      "Epoch 454/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6359 - val_loss: 42.9503\n",
      "Epoch 455/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 18.6361 - val_loss: 42.9499\n",
      "Epoch 456/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 18.6364 - val_loss: 42.9496\n",
      "Epoch 457/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6366 - val_loss: 42.9490\n",
      "Epoch 458/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6368 - val_loss: 42.9487\n",
      "Epoch 459/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6370 - val_loss: 42.9481\n",
      "Epoch 460/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6372 - val_loss: 42.9475\n",
      "Epoch 461/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6375 - val_loss: 42.9469\n",
      "Epoch 462/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6378 - val_loss: 42.9465\n",
      "Epoch 463/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6380 - val_loss: 42.9461\n",
      "Epoch 464/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6382 - val_loss: 42.9458\n",
      "Epoch 465/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6385 - val_loss: 42.9456\n",
      "Epoch 466/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6387 - val_loss: 42.9453\n",
      "Epoch 467/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6389 - val_loss: 42.9448\n",
      "Epoch 468/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 18.6391 - val_loss: 42.9445\n",
      "Epoch 469/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6393 - val_loss: 42.9443\n",
      "Epoch 470/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6395 - val_loss: 42.9437\n",
      "Epoch 471/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 18.6397 - val_loss: 42.9430\n",
      "Epoch 472/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6400 - val_loss: 42.9427\n",
      "Epoch 473/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6402 - val_loss: 42.9420\n",
      "Epoch 474/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6404 - val_loss: 42.9417\n",
      "Epoch 475/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6406 - val_loss: 42.9414\n",
      "Epoch 476/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6408 - val_loss: 42.9408\n",
      "Epoch 477/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6410 - val_loss: 42.9404\n",
      "Epoch 478/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6413 - val_loss: 42.9402\n",
      "Epoch 479/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6414 - val_loss: 42.9397\n",
      "Epoch 480/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6416 - val_loss: 42.9391\n",
      "Epoch 481/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6419 - val_loss: 42.9386\n",
      "Epoch 482/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6420 - val_loss: 42.9384\n",
      "Epoch 483/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6423 - val_loss: 42.9380\n",
      "Epoch 484/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6424 - val_loss: 42.9376\n",
      "Epoch 485/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6426 - val_loss: 42.9371\n",
      "Epoch 486/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6428 - val_loss: 42.9368\n",
      "Epoch 487/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6430 - val_loss: 42.9366\n",
      "Epoch 488/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 18.6432 - val_loss: 42.9364\n",
      "Epoch 489/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6433 - val_loss: 42.9358\n",
      "Epoch 490/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6436 - val_loss: 42.9353\n",
      "Epoch 491/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6438 - val_loss: 42.9349\n",
      "Epoch 492/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6440 - val_loss: 42.9345\n",
      "Epoch 493/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 18.6441 - val_loss: 42.9342\n",
      "Epoch 494/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6443 - val_loss: 42.9338\n",
      "Epoch 495/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.6446 - val_loss: 42.9336\n",
      "Epoch 496/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6447 - val_loss: 42.9334\n",
      "Epoch 497/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6448 - val_loss: 42.9329\n",
      "Epoch 498/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6450 - val_loss: 42.9326\n",
      "Epoch 499/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 18.6452 - val_loss: 42.9324\n",
      "Epoch 500/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 18.6453 - val_loss: 42.9321\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6465, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0029, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0003, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0168, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3581, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1e-10\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 530ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.33534314e+01, 6.33338235e+01, 6.33142157e+01, 6.32930672e+01,\n",
       "        6.32678571e+01, 6.32426471e+01, 6.32174370e+01, 6.31922269e+01,\n",
       "        6.31670168e+01, 6.31418067e+01, 6.31165966e+01, 6.30913866e+01,\n",
       "        6.30661765e+01, 6.30409664e+01, 6.30157563e+01, 6.29905462e+01,\n",
       "        6.29653361e+01, 6.29401261e+01, 6.29149160e+01, 6.28897059e+01,\n",
       "        6.28644958e+01, 6.28392857e+01, 6.28140756e+01, 6.27888655e+01,\n",
       "        6.27636555e+01, 6.27384454e+01, 6.27132353e+01, 6.26880252e+01,\n",
       "        6.26628151e+01, 6.26376050e+01, 6.26123950e+01, 6.25871849e+01,\n",
       "        6.25619748e+01, 6.25367647e+01, 6.25115546e+01, 6.24863445e+01,\n",
       "        6.24611345e+01, 6.24359244e+01, 6.24107143e+01, 6.23983894e+01,\n",
       "        6.23955882e+01, 6.23927871e+01, 6.23899860e+01, 6.23871849e+01,\n",
       "        6.23843837e+01, 6.23815826e+01, 6.23787815e+01, 6.23759804e+01,\n",
       "        6.23731793e+01, 6.23703782e+01, 6.23675770e+01, 6.23647759e+01,\n",
       "        6.23619748e+01, 6.23591737e+01, 6.23563726e+01, 6.23535714e+01,\n",
       "        6.23507703e+01, 6.23479692e+01, 6.23451681e+01, 6.23423669e+01,\n",
       "        6.23395658e+01, 6.23367647e+01, 6.23339636e+01, 6.23311625e+01,\n",
       "        6.23283613e+01, 6.23255602e+01, 6.23227591e+01, 6.23199580e+01,\n",
       "        6.23171569e+01, 6.23143557e+01, 6.23115546e+01, 6.23087535e+01,\n",
       "        6.23059524e+01, 6.23031513e+01, 6.23003501e+01, 6.22828431e+01,\n",
       "        6.22632353e+01, 6.22436274e+01, 6.22240196e+01, 6.22044118e+01,\n",
       "        6.91144409e+01, 9.19897199e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.91281509e-01, 5.98464787e-01,\n",
       "        5.62436104e-01, 3.32957834e-01, 1.17285824e+00, 2.44988352e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.97582459e-01, 0.00000000e+00,\n",
       "        1.02453041e+00, 1.18719362e-01, 2.89757133e-01, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([61.23962185, 61.23121849, 61.22281513, 61.21441176, 61.2060084 ,\n",
       "       61.19760504, 61.18920168, 61.18079832, 61.17239496, 61.1639916 ,\n",
       "       61.15558824, 61.14718487, 61.13878151, 61.13037815, 61.12197479,\n",
       "       61.11357143, 61.10516807, 61.09676471, 61.08836134, 61.07995798,\n",
       "       61.07155462, 61.06315126, 61.0547479 , 61.04634454, 61.03794118,\n",
       "       61.02953782, 61.02113445, 61.01273109, 61.00432773, 60.99592437,\n",
       "       60.98752101, 60.97911765, 60.97071429, 60.96231092, 60.95390756,\n",
       "       60.9455042 , 60.93710084, 60.92869748, 60.92029412, 60.91189076,\n",
       "       60.90348739, 60.89508403, 60.88668067, 60.87827731, 60.86987395,\n",
       "       60.86147059, 60.85306723, 60.84466387, 60.8362605 , 60.82785714,\n",
       "       60.81945378, 60.81105042, 60.80264706, 60.7942437 , 60.78584034,\n",
       "       60.77743697, 60.76903361, 60.76063025, 60.75222689, 60.74382353,\n",
       "       60.73542017, 60.72701681, 60.71861345, 60.71021008, 60.70180672,\n",
       "       60.69340336, 60.685     , 60.67659664, 60.66819328, 60.65978992,\n",
       "       60.65138655, 60.64298319, 60.63457983, 60.62617647, 60.61777311,\n",
       "       60.60936975, 60.60096639, 60.59256303, 60.58415966, 60.5757563 ,\n",
       "       60.56735294, 60.55894958, 60.55054622, 60.54214286, 60.5337395 ,\n",
       "       60.52533613, 60.51693277, 60.50852941, 60.50012605, 60.49172269,\n",
       "       60.48331933, 60.47491597, 60.46651261, 60.45810924, 60.44970588,\n",
       "       60.44130252, 60.43289916, 60.4244958 , 60.41609244, 60.40768908])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.282215025239033\n",
      "12.874470312571644\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
