{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.200000\n",
       "1       88.931092\n",
       "2       88.662185\n",
       "3       88.393277\n",
       "4       88.124370\n",
       "          ...    \n",
       "1795    59.972049\n",
       "1796    59.967754\n",
       "1797    59.963459\n",
       "1798    59.959164\n",
       "1799    59.954869\n",
       "Name: C1, Length: 1800, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c1_interpolated_1700_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.200000\n",
       "1       88.931092\n",
       "2       88.662185\n",
       "3       88.393277\n",
       "4       88.124370\n",
       "          ...    \n",
       "1695     0.000000\n",
       "1696     1.024530\n",
       "1697     0.118719\n",
       "1698     0.289757\n",
       "1699     0.000000\n",
       "Name: C1, Length: 1700, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1700)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.931092</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.662185</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.393277</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.124370</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     89.200000  0.000298\n",
       "1     88.931092  0.000298\n",
       "2     88.662185  0.000297\n",
       "3     88.393277  0.000297\n",
       "4     88.124370  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6465  # Value for C0\n",
    "K0 = -0.0029  # Value for K0\n",
    "K1 = -0.0003  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0168    # Value for b\n",
    "c = 2.3581    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    89.200000    88.931092    88.662185    88.393277    88.124370   \n",
      "351    88.931092    88.662185    88.393277    88.124370    87.855462   \n",
      "352    88.662185    88.393277    88.124370    87.855462    87.586555   \n",
      "353    88.393277    88.124370    87.855462    87.586555    87.317647   \n",
      "354    88.124370    87.855462    87.586555    87.317647    87.048739   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.855462    87.586555    87.317647    87.048739    86.794538  ...   \n",
      "351    87.586555    87.317647    87.048739    86.794538    86.721709  ...   \n",
      "352    87.317647    87.048739    86.794538    86.721709    86.648880  ...   \n",
      "353    87.048739    86.794538    86.721709    86.648880    86.576050  ...   \n",
      "354    86.794538    86.721709    86.648880    86.576050    86.503221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   73.989683    0.000263   73.957937    0.000263   73.926190    0.000263   \n",
      "351   73.957937    0.000263   73.926190    0.000263   73.894444    0.000262   \n",
      "352   73.926190    0.000263   73.894444    0.000262   73.862698    0.000262   \n",
      "353   73.894444    0.000262   73.862698    0.000262   73.830952    0.000262   \n",
      "354   73.862698    0.000262   73.830952    0.000262   73.799206    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   73.894444    0.000262   73.862698    0.000262  \n",
      "351   73.862698    0.000262   73.830952    0.000262  \n",
      "352   73.830952    0.000262   73.799206    0.000262  \n",
      "353   73.799206    0.000262   73.767460    0.000262  \n",
      "354   73.767460    0.000262   73.735714    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1250, 1, 251) (1250, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "16/16 [==============================] - 3s 40ms/step - loss: 4568.1445 - val_loss: 3656.2134\n",
      "Epoch 2/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4512.6489 - val_loss: 3620.6990\n",
      "Epoch 3/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4473.2573 - val_loss: 3585.3511\n",
      "Epoch 4/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4432.4678 - val_loss: 3546.9082\n",
      "Epoch 5/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4390.2451 - val_loss: 3509.6311\n",
      "Epoch 6/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4348.9761 - val_loss: 3472.8191\n",
      "Epoch 7/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4308.2236 - val_loss: 3436.4736\n",
      "Epoch 8/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4267.9360 - val_loss: 3400.5293\n",
      "Epoch 9/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4228.0493 - val_loss: 3364.9412\n",
      "Epoch 10/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4188.5225 - val_loss: 3329.6799\n",
      "Epoch 11/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4149.3311 - val_loss: 3294.7285\n",
      "Epoch 12/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4110.4561 - val_loss: 3260.0742\n",
      "Epoch 13/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4071.8879 - val_loss: 3225.7080\n",
      "Epoch 14/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4033.6162 - val_loss: 3191.6226\n",
      "Epoch 15/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3988.3091 - val_loss: 3140.3962\n",
      "Epoch 16/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3936.4802 - val_loss: 3102.9294\n",
      "Epoch 17/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3894.8962 - val_loss: 3066.2937\n",
      "Epoch 18/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3854.2527 - val_loss: 3030.4624\n",
      "Epoch 19/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3814.3972 - val_loss: 2995.2646\n",
      "Epoch 20/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3775.1670 - val_loss: 2960.5879\n",
      "Epoch 21/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3736.4583 - val_loss: 2926.3628\n",
      "Epoch 22/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3698.2095 - val_loss: 2892.5437\n",
      "Epoch 23/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3660.3765 - val_loss: 2859.1008\n",
      "Epoch 24/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3622.9302 - val_loss: 2826.0103\n",
      "Epoch 25/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3585.8479 - val_loss: 2793.2571\n",
      "Epoch 26/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3549.1140 - val_loss: 2760.8267\n",
      "Epoch 27/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3512.7148 - val_loss: 2728.7095\n",
      "Epoch 28/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3476.6394 - val_loss: 2696.8958\n",
      "Epoch 29/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3440.8799 - val_loss: 2665.3784\n",
      "Epoch 30/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3405.4275 - val_loss: 2634.1521\n",
      "Epoch 31/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3370.2759 - val_loss: 2603.2095\n",
      "Epoch 32/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3335.4199 - val_loss: 2572.5471\n",
      "Epoch 33/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3300.8550 - val_loss: 2542.1602\n",
      "Epoch 34/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3266.5762 - val_loss: 2512.0449\n",
      "Epoch 35/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3232.5796 - val_loss: 2482.1978\n",
      "Epoch 36/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3198.8601 - val_loss: 2452.6155\n",
      "Epoch 37/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3165.4160 - val_loss: 2423.2939\n",
      "Epoch 38/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3128.9653 - val_loss: 2387.1262\n",
      "Epoch 39/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3088.7004 - val_loss: 2353.2993\n",
      "Epoch 40/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3050.7461 - val_loss: 2320.5735\n",
      "Epoch 41/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3013.9873 - val_loss: 2288.8145\n",
      "Epoch 42/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2978.1797 - val_loss: 2257.7991\n",
      "Epoch 43/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2943.1133 - val_loss: 2227.3848\n",
      "Epoch 44/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2908.6558 - val_loss: 2197.4829\n",
      "Epoch 45/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2874.7241 - val_loss: 2168.0366\n",
      "Epoch 46/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2841.2620 - val_loss: 2139.0039\n",
      "Epoch 47/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2808.2300 - val_loss: 2110.3562\n",
      "Epoch 48/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2775.5986 - val_loss: 2082.0715\n",
      "Epoch 49/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2743.3464 - val_loss: 2054.1311\n",
      "Epoch 50/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2711.4543 - val_loss: 2026.5214\n",
      "Epoch 51/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2679.9070 - val_loss: 1999.2300\n",
      "Epoch 52/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2648.6934 - val_loss: 1972.2471\n",
      "Epoch 53/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2617.8022 - val_loss: 1945.5647\n",
      "Epoch 54/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2587.2251 - val_loss: 1919.1743\n",
      "Epoch 55/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2556.9543 - val_loss: 1893.0701\n",
      "Epoch 56/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2526.9824 - val_loss: 1867.2455\n",
      "Epoch 57/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2497.3037 - val_loss: 1841.6952\n",
      "Epoch 58/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2467.9124 - val_loss: 1816.4158\n",
      "Epoch 59/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2438.8032 - val_loss: 1791.4008\n",
      "Epoch 60/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2409.9719 - val_loss: 1766.6477\n",
      "Epoch 61/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2381.4148 - val_loss: 1742.1517\n",
      "Epoch 62/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2353.1265 - val_loss: 1717.9103\n",
      "Epoch 63/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2325.1038 - val_loss: 1693.9197\n",
      "Epoch 64/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2297.3445 - val_loss: 1670.1770\n",
      "Epoch 65/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2269.8445 - val_loss: 1646.6791\n",
      "Epoch 66/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2242.6006 - val_loss: 1623.4231\n",
      "Epoch 67/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2215.6101 - val_loss: 1600.4071\n",
      "Epoch 68/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2188.8701 - val_loss: 1577.6274\n",
      "Epoch 69/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2162.3784 - val_loss: 1555.0829\n",
      "Epoch 70/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2136.1323 - val_loss: 1532.7699\n",
      "Epoch 71/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2110.1289 - val_loss: 1510.6874\n",
      "Epoch 72/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2084.3667 - val_loss: 1488.8328\n",
      "Epoch 73/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2058.8428 - val_loss: 1467.2041\n",
      "Epoch 74/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2033.5560 - val_loss: 1445.7988\n",
      "Epoch 75/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2008.5034 - val_loss: 1424.6156\n",
      "Epoch 76/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1983.6829 - val_loss: 1403.6525\n",
      "Epoch 77/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1959.0929 - val_loss: 1382.9071\n",
      "Epoch 78/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1934.7314 - val_loss: 1362.3781\n",
      "Epoch 79/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1910.5968 - val_loss: 1342.0638\n",
      "Epoch 80/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1886.6866 - val_loss: 1321.9622\n",
      "Epoch 81/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1862.9996 - val_loss: 1302.0712\n",
      "Epoch 82/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1839.5341 - val_loss: 1282.3901\n",
      "Epoch 83/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1816.2882 - val_loss: 1262.9169\n",
      "Epoch 84/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1793.2601 - val_loss: 1243.6499\n",
      "Epoch 85/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1770.4486 - val_loss: 1224.5875\n",
      "Epoch 86/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1747.8518 - val_loss: 1205.7284\n",
      "Epoch 87/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1725.4686 - val_loss: 1187.0708\n",
      "Epoch 88/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1703.2965 - val_loss: 1168.6139\n",
      "Epoch 89/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1681.3352 - val_loss: 1150.3550\n",
      "Epoch 90/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1659.5822 - val_loss: 1132.2935\n",
      "Epoch 91/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1638.0369 - val_loss: 1114.4280\n",
      "Epoch 92/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1616.6970 - val_loss: 1096.7570\n",
      "Epoch 93/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1595.5615 - val_loss: 1079.2792\n",
      "Epoch 94/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1574.6294 - val_loss: 1061.9929\n",
      "Epoch 95/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1553.8986 - val_loss: 1044.8967\n",
      "Epoch 96/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1533.3678 - val_loss: 1027.9895\n",
      "Epoch 97/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1513.0358 - val_loss: 1011.2704\n",
      "Epoch 98/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1492.9014 - val_loss: 994.7374\n",
      "Epoch 99/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1472.9631 - val_loss: 978.3894\n",
      "Epoch 100/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1453.2205 - val_loss: 962.2254\n",
      "Epoch 101/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1433.6708 - val_loss: 946.2437\n",
      "Epoch 102/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1414.3136 - val_loss: 930.4434\n",
      "Epoch 103/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1395.1475 - val_loss: 914.8229\n",
      "Epoch 104/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1376.1710 - val_loss: 899.3810\n",
      "Epoch 105/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1357.3834 - val_loss: 884.1171\n",
      "Epoch 106/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1338.7826 - val_loss: 869.0290\n",
      "Epoch 107/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1320.3684 - val_loss: 854.1163\n",
      "Epoch 108/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1302.1388 - val_loss: 839.3774\n",
      "Epoch 109/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1284.0931 - val_loss: 824.8112\n",
      "Epoch 110/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1266.2297 - val_loss: 810.4164\n",
      "Epoch 111/500\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1248.5476 - val_loss: 796.1919\n",
      "Epoch 112/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1231.0458 - val_loss: 782.1368\n",
      "Epoch 113/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1213.7225 - val_loss: 768.2495\n",
      "Epoch 114/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1196.5775 - val_loss: 754.5294\n",
      "Epoch 115/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1179.6095 - val_loss: 740.9748\n",
      "Epoch 116/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1162.8167 - val_loss: 727.5845\n",
      "Epoch 117/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1146.1982 - val_loss: 714.3583\n",
      "Epoch 118/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1129.7531 - val_loss: 701.2941\n",
      "Epoch 119/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1113.4803 - val_loss: 688.3910\n",
      "Epoch 120/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1097.3784 - val_loss: 675.6479\n",
      "Epoch 121/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1081.4464 - val_loss: 663.0641\n",
      "Epoch 122/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1065.6833 - val_loss: 650.6379\n",
      "Epoch 123/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1050.0876 - val_loss: 638.3683\n",
      "Epoch 124/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1034.6592 - val_loss: 626.2550\n",
      "Epoch 125/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1019.3960 - val_loss: 614.2960\n",
      "Epoch 126/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1004.2974 - val_loss: 602.4899\n",
      "Epoch 127/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 989.3619 - val_loss: 590.8370\n",
      "Epoch 128/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 974.5892 - val_loss: 579.3352\n",
      "Epoch 129/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 959.9777 - val_loss: 567.9838\n",
      "Epoch 130/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 945.5266 - val_loss: 556.7816\n",
      "Epoch 131/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 931.2346 - val_loss: 545.7276\n",
      "Epoch 132/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 917.1010 - val_loss: 534.8209\n",
      "Epoch 133/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 903.1243 - val_loss: 524.0602\n",
      "Epoch 134/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 889.3039 - val_loss: 513.4445\n",
      "Epoch 135/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 875.6384 - val_loss: 502.9726\n",
      "Epoch 136/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 862.1271 - val_loss: 492.6437\n",
      "Epoch 137/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 848.7686 - val_loss: 482.4568\n",
      "Epoch 138/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 835.5623 - val_loss: 472.4107\n",
      "Epoch 139/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 822.5067 - val_loss: 462.5040\n",
      "Epoch 140/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 809.6012 - val_loss: 452.7366\n",
      "Epoch 141/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 796.8448 - val_loss: 443.1070\n",
      "Epoch 142/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 784.2361 - val_loss: 433.6137\n",
      "Epoch 143/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 771.7744 - val_loss: 424.2562\n",
      "Epoch 144/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 759.4585 - val_loss: 415.0334\n",
      "Epoch 145/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 747.2876 - val_loss: 405.9447\n",
      "Epoch 146/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 735.2609 - val_loss: 396.9881\n",
      "Epoch 147/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 723.3771 - val_loss: 388.1635\n",
      "Epoch 148/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 711.6353 - val_loss: 379.4698\n",
      "Epoch 149/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 700.0345 - val_loss: 370.9055\n",
      "Epoch 150/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 688.5737 - val_loss: 362.4698\n",
      "Epoch 151/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 677.2520 - val_loss: 354.1618\n",
      "Epoch 152/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 666.0685 - val_loss: 345.9806\n",
      "Epoch 153/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 655.0220 - val_loss: 337.9250\n",
      "Epoch 154/500\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 644.1118 - val_loss: 329.9939\n",
      "Epoch 155/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 633.3365 - val_loss: 322.1866\n",
      "Epoch 156/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 622.6956 - val_loss: 314.5019\n",
      "Epoch 157/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 612.1880 - val_loss: 306.9391\n",
      "Epoch 158/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 601.8124 - val_loss: 299.4969\n",
      "Epoch 159/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 591.5684 - val_loss: 292.1743\n",
      "Epoch 160/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 581.4546 - val_loss: 284.9706\n",
      "Epoch 161/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 571.4706 - val_loss: 277.8846\n",
      "Epoch 162/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 561.6149 - val_loss: 270.9156\n",
      "Epoch 163/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 551.8867 - val_loss: 264.0617\n",
      "Epoch 164/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 542.2849 - val_loss: 257.3231\n",
      "Epoch 165/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 532.8088 - val_loss: 250.6983\n",
      "Epoch 166/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 523.4572 - val_loss: 244.1861\n",
      "Epoch 167/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 514.2294 - val_loss: 237.7860\n",
      "Epoch 168/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 505.1243 - val_loss: 231.4966\n",
      "Epoch 169/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 496.1411 - val_loss: 225.3170\n",
      "Epoch 170/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 487.2787 - val_loss: 219.2466\n",
      "Epoch 171/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 478.5364 - val_loss: 213.2843\n",
      "Epoch 172/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 469.9132 - val_loss: 207.4290\n",
      "Epoch 173/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 461.4081 - val_loss: 201.6793\n",
      "Epoch 174/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 453.0201 - val_loss: 196.0352\n",
      "Epoch 175/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 444.7483 - val_loss: 190.4948\n",
      "Epoch 176/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 436.5919 - val_loss: 185.0581\n",
      "Epoch 177/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 428.5497 - val_loss: 179.7232\n",
      "Epoch 178/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 420.6214 - val_loss: 174.4896\n",
      "Epoch 179/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 412.8053 - val_loss: 169.3563\n",
      "Epoch 180/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 405.1011 - val_loss: 164.3224\n",
      "Epoch 181/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 397.5074 - val_loss: 159.3869\n",
      "Epoch 182/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 390.0234 - val_loss: 154.5486\n",
      "Epoch 183/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 382.6484 - val_loss: 149.8066\n",
      "Epoch 184/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 375.3812 - val_loss: 145.1601\n",
      "Epoch 185/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 368.2210 - val_loss: 140.6084\n",
      "Epoch 186/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 361.1668 - val_loss: 136.1500\n",
      "Epoch 187/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 354.2178 - val_loss: 131.7840\n",
      "Epoch 188/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 347.3730 - val_loss: 127.5098\n",
      "Epoch 189/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 340.6314 - val_loss: 123.3264\n",
      "Epoch 190/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 333.9925 - val_loss: 119.2327\n",
      "Epoch 191/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 327.4553 - val_loss: 115.2278\n",
      "Epoch 192/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 321.0184 - val_loss: 111.3108\n",
      "Epoch 193/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 314.6814 - val_loss: 107.4807\n",
      "Epoch 194/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 308.4432 - val_loss: 103.7364\n",
      "Epoch 195/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 302.3029 - val_loss: 100.0771\n",
      "Epoch 196/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 296.2594 - val_loss: 96.5020\n",
      "Epoch 197/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 290.3121 - val_loss: 93.0099\n",
      "Epoch 198/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 284.4601 - val_loss: 89.6001\n",
      "Epoch 199/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 278.7025 - val_loss: 86.2714\n",
      "Epoch 200/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 273.0382 - val_loss: 83.0231\n",
      "Epoch 201/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 267.4663 - val_loss: 79.8539\n",
      "Epoch 202/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 261.9858 - val_loss: 76.7631\n",
      "Epoch 203/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 256.5959 - val_loss: 73.7496\n",
      "Epoch 204/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 251.2959 - val_loss: 70.8128\n",
      "Epoch 205/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 246.0848 - val_loss: 67.9515\n",
      "Epoch 206/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 240.9616 - val_loss: 65.1648\n",
      "Epoch 207/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 235.9257 - val_loss: 62.4519\n",
      "Epoch 208/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 230.9758 - val_loss: 59.8116\n",
      "Epoch 209/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 226.1112 - val_loss: 57.2431\n",
      "Epoch 210/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 221.3309 - val_loss: 54.7455\n",
      "Epoch 211/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 216.6343 - val_loss: 52.3179\n",
      "Epoch 212/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 212.0203 - val_loss: 49.9593\n",
      "Epoch 213/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 207.4881 - val_loss: 47.6688\n",
      "Epoch 214/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 203.0367 - val_loss: 45.4454\n",
      "Epoch 215/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 198.6653 - val_loss: 43.2883\n",
      "Epoch 216/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 194.3731 - val_loss: 41.1966\n",
      "Epoch 217/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 190.1590 - val_loss: 39.1690\n",
      "Epoch 218/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 186.0220 - val_loss: 37.2050\n",
      "Epoch 219/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 181.9619 - val_loss: 35.3038\n",
      "Epoch 220/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 177.9773 - val_loss: 33.4640\n",
      "Epoch 221/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 174.0673 - val_loss: 31.6849\n",
      "Epoch 222/500\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 170.2311 - val_loss: 29.9657\n",
      "Epoch 223/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 166.4680 - val_loss: 28.3053\n",
      "Epoch 224/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 162.7769 - val_loss: 26.7031\n",
      "Epoch 225/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 159.1572 - val_loss: 25.1578\n",
      "Epoch 226/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 155.6078 - val_loss: 23.6688\n",
      "Epoch 227/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 152.1280 - val_loss: 22.2350\n",
      "Epoch 228/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 148.7168 - val_loss: 20.8556\n",
      "Epoch 229/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 145.3734 - val_loss: 19.5296\n",
      "Epoch 230/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 142.0969 - val_loss: 18.2564\n",
      "Epoch 231/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 138.8864 - val_loss: 17.0347\n",
      "Epoch 232/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 135.7412 - val_loss: 15.8639\n",
      "Epoch 233/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 132.6606 - val_loss: 14.7430\n",
      "Epoch 234/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 129.6433 - val_loss: 13.6711\n",
      "Epoch 235/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 126.6887 - val_loss: 12.6475\n",
      "Epoch 236/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 123.7959 - val_loss: 11.6710\n",
      "Epoch 237/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 120.9642 - val_loss: 10.7410\n",
      "Epoch 238/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 118.1927 - val_loss: 9.8566\n",
      "Epoch 239/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 115.4805 - val_loss: 9.0168\n",
      "Epoch 240/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 112.8268 - val_loss: 8.2209\n",
      "Epoch 241/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 110.2309 - val_loss: 7.4679\n",
      "Epoch 242/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 107.6918 - val_loss: 6.7570\n",
      "Epoch 243/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 105.2087 - val_loss: 6.0874\n",
      "Epoch 244/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 102.7809 - val_loss: 5.4582\n",
      "Epoch 245/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 100.4076 - val_loss: 4.8686\n",
      "Epoch 246/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 98.0879 - val_loss: 4.3177\n",
      "Epoch 247/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 95.8210 - val_loss: 3.8047\n",
      "Epoch 248/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 93.6061 - val_loss: 3.3288\n",
      "Epoch 249/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 91.4423 - val_loss: 2.8891\n",
      "Epoch 250/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 89.3290 - val_loss: 2.4849\n",
      "Epoch 251/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 87.2653 - val_loss: 2.1153\n",
      "Epoch 252/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 85.2505 - val_loss: 1.7794\n",
      "Epoch 253/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 83.2836 - val_loss: 1.4766\n",
      "Epoch 254/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 81.3641 - val_loss: 1.2060\n",
      "Epoch 255/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 79.4909 - val_loss: 0.9668\n",
      "Epoch 256/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 77.6636 - val_loss: 0.7583\n",
      "Epoch 257/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 75.8812 - val_loss: 0.5795\n",
      "Epoch 258/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 74.1431 - val_loss: 0.4299\n",
      "Epoch 259/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 72.4482 - val_loss: 0.3085\n",
      "Epoch 260/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 70.7962 - val_loss: 0.2147\n",
      "Epoch 261/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 69.1860 - val_loss: 0.1476\n",
      "Epoch 262/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 67.6170 - val_loss: 0.1066\n",
      "Epoch 263/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 66.0885 - val_loss: 0.0909\n",
      "Epoch 264/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 64.5997 - val_loss: 0.0997\n",
      "Epoch 265/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 63.1501 - val_loss: 0.1323\n",
      "Epoch 266/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 61.7387 - val_loss: 0.1881\n",
      "Epoch 267/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 60.3648 - val_loss: 0.2662\n",
      "Epoch 268/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 59.0278 - val_loss: 0.3659\n",
      "Epoch 269/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 57.7271 - val_loss: 0.4866\n",
      "Epoch 270/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 56.4617 - val_loss: 0.6276\n",
      "Epoch 271/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 55.2310 - val_loss: 0.7882\n",
      "Epoch 272/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 54.0345 - val_loss: 0.9676\n",
      "Epoch 273/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 52.8714 - val_loss: 1.1653\n",
      "Epoch 274/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 51.7410 - val_loss: 1.3806\n",
      "Epoch 275/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 50.6427 - val_loss: 1.6127\n",
      "Epoch 276/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 49.5758 - val_loss: 1.8611\n",
      "Epoch 277/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 48.5396 - val_loss: 2.1252\n",
      "Epoch 278/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 47.5336 - val_loss: 2.4042\n",
      "Epoch 279/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 46.5569 - val_loss: 2.6976\n",
      "Epoch 280/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 45.6091 - val_loss: 3.0047\n",
      "Epoch 281/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 44.6895 - val_loss: 3.3250\n",
      "Epoch 282/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 43.7975 - val_loss: 3.6578\n",
      "Epoch 283/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 42.9325 - val_loss: 4.0026\n",
      "Epoch 284/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 42.0937 - val_loss: 4.3588\n",
      "Epoch 285/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 41.2807 - val_loss: 4.7257\n",
      "Epoch 286/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 40.4930 - val_loss: 5.1030\n",
      "Epoch 287/500\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 39.7298 - val_loss: 5.4899\n",
      "Epoch 288/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 38.9906 - val_loss: 5.8860\n",
      "Epoch 289/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 38.2748 - val_loss: 6.2906\n",
      "Epoch 290/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 37.5819 - val_loss: 6.7035\n",
      "Epoch 291/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 36.9113 - val_loss: 7.1239\n",
      "Epoch 292/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 36.2625 - val_loss: 7.5513\n",
      "Epoch 293/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 35.6350 - val_loss: 7.9853\n",
      "Epoch 294/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 35.0281 - val_loss: 8.4255\n",
      "Epoch 295/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 34.4414 - val_loss: 8.8712\n",
      "Epoch 296/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 33.8743 - val_loss: 9.3221\n",
      "Epoch 297/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 33.3263 - val_loss: 9.7778\n",
      "Epoch 298/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 32.7970 - val_loss: 10.2377\n",
      "Epoch 299/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 32.2859 - val_loss: 10.7014\n",
      "Epoch 300/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 31.7923 - val_loss: 11.1686\n",
      "Epoch 301/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 31.3160 - val_loss: 11.6387\n",
      "Epoch 302/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 30.8564 - val_loss: 12.1113\n",
      "Epoch 303/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 30.4132 - val_loss: 12.5861\n",
      "Epoch 304/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 29.9857 - val_loss: 13.0628\n",
      "Epoch 305/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 29.5735 - val_loss: 13.5409\n",
      "Epoch 306/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 29.1762 - val_loss: 14.0199\n",
      "Epoch 307/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 28.7935 - val_loss: 14.4998\n",
      "Epoch 308/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 28.4247 - val_loss: 14.9800\n",
      "Epoch 309/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 28.0696 - val_loss: 15.4602\n",
      "Epoch 310/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 27.7278 - val_loss: 15.9402\n",
      "Epoch 311/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 27.3988 - val_loss: 16.4194\n",
      "Epoch 312/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 27.0823 - val_loss: 16.8979\n",
      "Epoch 313/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.7778 - val_loss: 17.3750\n",
      "Epoch 314/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.4851 - val_loss: 17.8507\n",
      "Epoch 315/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 26.2037 - val_loss: 18.3246\n",
      "Epoch 316/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 25.9333 - val_loss: 18.7964\n",
      "Epoch 317/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 25.6734 - val_loss: 19.2660\n",
      "Epoch 318/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 25.4239 - val_loss: 19.7331\n",
      "Epoch 319/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 25.1842 - val_loss: 20.1975\n",
      "Epoch 320/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 24.9542 - val_loss: 20.6587\n",
      "Epoch 321/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 24.7335 - val_loss: 21.1167\n",
      "Epoch 322/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 24.5218 - val_loss: 21.5713\n",
      "Epoch 323/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 24.3188 - val_loss: 22.0222\n",
      "Epoch 324/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 24.1242 - val_loss: 22.4693\n",
      "Epoch 325/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 23.9376 - val_loss: 22.9124\n",
      "Epoch 326/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 23.7589 - val_loss: 23.3514\n",
      "Epoch 327/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 23.5877 - val_loss: 23.7861\n",
      "Epoch 328/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 23.4238 - val_loss: 24.2162\n",
      "Epoch 329/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 23.2669 - val_loss: 24.6417\n",
      "Epoch 330/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 23.1167 - val_loss: 25.0625\n",
      "Epoch 331/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 22.9730 - val_loss: 25.4781\n",
      "Epoch 332/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 22.8357 - val_loss: 25.8888\n",
      "Epoch 333/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 22.7044 - val_loss: 26.2944\n",
      "Epoch 334/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 22.5788 - val_loss: 26.6948\n",
      "Epoch 335/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 22.4588 - val_loss: 27.0898\n",
      "Epoch 336/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 22.3442 - val_loss: 27.4794\n",
      "Epoch 337/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 22.2348 - val_loss: 27.8634\n",
      "Epoch 338/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 22.1303 - val_loss: 28.2419\n",
      "Epoch 339/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 22.0306 - val_loss: 28.6148\n",
      "Epoch 340/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 21.9354 - val_loss: 28.9819\n",
      "Epoch 341/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 21.8447 - val_loss: 29.3431\n",
      "Epoch 342/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 21.7581 - val_loss: 29.6985\n",
      "Epoch 343/500\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 21.6756 - val_loss: 30.0482\n",
      "Epoch 344/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 21.5970 - val_loss: 30.3917\n",
      "Epoch 345/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 21.5221 - val_loss: 30.7295\n",
      "Epoch 346/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 21.4508 - val_loss: 31.0612\n",
      "Epoch 347/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 21.3829 - val_loss: 31.3870\n",
      "Epoch 348/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 21.3182 - val_loss: 31.7067\n",
      "Epoch 349/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 21.2567 - val_loss: 32.0205\n",
      "Epoch 350/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 21.1981 - val_loss: 32.3284\n",
      "Epoch 351/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 21.1424 - val_loss: 32.6304\n",
      "Epoch 352/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 21.0894 - val_loss: 32.9262\n",
      "Epoch 353/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 21.0390 - val_loss: 33.2162\n",
      "Epoch 354/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.9912 - val_loss: 33.5002\n",
      "Epoch 355/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.9457 - val_loss: 33.7783\n",
      "Epoch 356/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.9025 - val_loss: 34.0504\n",
      "Epoch 357/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.8614 - val_loss: 34.3167\n",
      "Epoch 358/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.8225 - val_loss: 34.5774\n",
      "Epoch 359/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 20.7855 - val_loss: 34.8319\n",
      "Epoch 360/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.7504 - val_loss: 35.0809\n",
      "Epoch 361/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.7171 - val_loss: 35.3240\n",
      "Epoch 362/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.6855 - val_loss: 35.5615\n",
      "Epoch 363/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.6555 - val_loss: 35.7936\n",
      "Epoch 364/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.6271 - val_loss: 36.0199\n",
      "Epoch 365/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.6001 - val_loss: 36.2410\n",
      "Epoch 366/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.5746 - val_loss: 36.4565\n",
      "Epoch 367/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.5504 - val_loss: 36.6666\n",
      "Epoch 368/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.5274 - val_loss: 36.8714\n",
      "Epoch 369/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.5057 - val_loss: 37.0712\n",
      "Epoch 370/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.4852 - val_loss: 37.2656\n",
      "Epoch 371/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.4657 - val_loss: 37.4549\n",
      "Epoch 372/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.4472 - val_loss: 37.6392\n",
      "Epoch 373/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.4298 - val_loss: 37.8186\n",
      "Epoch 374/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.4132 - val_loss: 37.9932\n",
      "Epoch 375/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.3976 - val_loss: 38.1630\n",
      "Epoch 376/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.3827 - val_loss: 38.3279\n",
      "Epoch 377/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.3688 - val_loss: 38.4883\n",
      "Epoch 378/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.3555 - val_loss: 38.6442\n",
      "Epoch 379/500\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 20.3430 - val_loss: 38.7956\n",
      "Epoch 380/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.3311 - val_loss: 38.9425\n",
      "Epoch 381/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.3199 - val_loss: 39.0852\n",
      "Epoch 382/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.3093 - val_loss: 39.2236\n",
      "Epoch 383/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.2993 - val_loss: 39.3580\n",
      "Epoch 384/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.2898 - val_loss: 39.4884\n",
      "Epoch 385/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.2808 - val_loss: 39.6146\n",
      "Epoch 386/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.2724 - val_loss: 39.7368\n",
      "Epoch 387/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.2643 - val_loss: 39.8553\n",
      "Epoch 388/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.2568 - val_loss: 39.9700\n",
      "Epoch 389/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.2496 - val_loss: 40.0810\n",
      "Epoch 390/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.2429 - val_loss: 40.1884\n",
      "Epoch 391/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.2365 - val_loss: 40.2925\n",
      "Epoch 392/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.2305 - val_loss: 40.3931\n",
      "Epoch 393/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.2248 - val_loss: 40.4903\n",
      "Epoch 394/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.2194 - val_loss: 40.5844\n",
      "Epoch 395/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.2143 - val_loss: 40.6752\n",
      "Epoch 396/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.2095 - val_loss: 40.7628\n",
      "Epoch 397/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 20.2050 - val_loss: 40.8476\n",
      "Epoch 398/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.2008 - val_loss: 40.9295\n",
      "Epoch 399/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1967 - val_loss: 41.0085\n",
      "Epoch 400/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1929 - val_loss: 41.0848\n",
      "Epoch 401/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1893 - val_loss: 41.1583\n",
      "Epoch 402/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1859 - val_loss: 41.2290\n",
      "Epoch 403/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1827 - val_loss: 41.2975\n",
      "Epoch 404/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1797 - val_loss: 41.3633\n",
      "Epoch 405/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1768 - val_loss: 41.4267\n",
      "Epoch 406/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1741 - val_loss: 41.4878\n",
      "Epoch 407/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1716 - val_loss: 41.5464\n",
      "Epoch 408/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1692 - val_loss: 41.6029\n",
      "Epoch 409/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 20.1669 - val_loss: 41.6574\n",
      "Epoch 410/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1648 - val_loss: 41.7098\n",
      "Epoch 411/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1628 - val_loss: 41.7601\n",
      "Epoch 412/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1609 - val_loss: 41.8086\n",
      "Epoch 413/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1591 - val_loss: 41.8552\n",
      "Epoch 414/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1574 - val_loss: 41.8997\n",
      "Epoch 415/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 20.1558 - val_loss: 41.9424\n",
      "Epoch 416/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1544 - val_loss: 41.9838\n",
      "Epoch 417/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1530 - val_loss: 42.0233\n",
      "Epoch 418/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1516 - val_loss: 42.0612\n",
      "Epoch 419/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1504 - val_loss: 42.0975\n",
      "Epoch 420/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1493 - val_loss: 42.1325\n",
      "Epoch 421/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1482 - val_loss: 42.1658\n",
      "Epoch 422/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1471 - val_loss: 42.1980\n",
      "Epoch 423/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1462 - val_loss: 42.2286\n",
      "Epoch 424/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 20.1453 - val_loss: 42.2583\n",
      "Epoch 425/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1444 - val_loss: 42.2862\n",
      "Epoch 426/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1437 - val_loss: 42.3131\n",
      "Epoch 427/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1429 - val_loss: 42.3389\n",
      "Epoch 428/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1422 - val_loss: 42.3634\n",
      "Epoch 429/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1416 - val_loss: 42.3871\n",
      "Epoch 430/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1410 - val_loss: 42.4096\n",
      "Epoch 431/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1405 - val_loss: 42.4312\n",
      "Epoch 432/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1399 - val_loss: 42.4517\n",
      "Epoch 433/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1394 - val_loss: 42.4715\n",
      "Epoch 434/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1390 - val_loss: 42.4902\n",
      "Epoch 435/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1386 - val_loss: 42.5082\n",
      "Epoch 436/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 20.1382 - val_loss: 42.5251\n",
      "Epoch 437/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1379 - val_loss: 42.5415\n",
      "Epoch 438/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 20.1375 - val_loss: 42.5569\n",
      "Epoch 439/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1372 - val_loss: 42.5716\n",
      "Epoch 440/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1370 - val_loss: 42.5858\n",
      "Epoch 441/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1367 - val_loss: 42.5993\n",
      "Epoch 442/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1365 - val_loss: 42.6119\n",
      "Epoch 443/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1363 - val_loss: 42.6239\n",
      "Epoch 444/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1361 - val_loss: 42.6354\n",
      "Epoch 445/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1360 - val_loss: 42.6463\n",
      "Epoch 446/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1359 - val_loss: 42.6568\n",
      "Epoch 447/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1357 - val_loss: 42.6666\n",
      "Epoch 448/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1356 - val_loss: 42.6761\n",
      "Epoch 449/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1355 - val_loss: 42.6852\n",
      "Epoch 450/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1354 - val_loss: 42.6934\n",
      "Epoch 451/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1354 - val_loss: 42.7016\n",
      "Epoch 452/500\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 20.1353 - val_loss: 42.7091\n",
      "Epoch 453/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1353 - val_loss: 42.7163\n",
      "Epoch 454/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1353 - val_loss: 42.7230\n",
      "Epoch 455/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1353 - val_loss: 42.7294\n",
      "Epoch 456/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1353 - val_loss: 42.7356\n",
      "Epoch 457/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1353 - val_loss: 42.7413\n",
      "Epoch 458/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1354 - val_loss: 42.7467\n",
      "Epoch 459/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1354 - val_loss: 42.7520\n",
      "Epoch 460/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 20.1354 - val_loss: 42.7567\n",
      "Epoch 461/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1355 - val_loss: 42.7615\n",
      "Epoch 462/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1356 - val_loss: 42.7659\n",
      "Epoch 463/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1356 - val_loss: 42.7699\n",
      "Epoch 464/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1357 - val_loss: 42.7739\n",
      "Epoch 465/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1358 - val_loss: 42.7775\n",
      "Epoch 466/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 20.1359 - val_loss: 42.7809\n",
      "Epoch 467/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1360 - val_loss: 42.7844\n",
      "Epoch 468/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1361 - val_loss: 42.7874\n",
      "Epoch 469/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1362 - val_loss: 42.7904\n",
      "Epoch 470/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1362 - val_loss: 42.7930\n",
      "Epoch 471/500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 20.1364 - val_loss: 42.7956\n",
      "Epoch 472/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1365 - val_loss: 42.7977\n",
      "Epoch 473/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1366 - val_loss: 42.7999\n",
      "Epoch 474/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1368 - val_loss: 42.8021\n",
      "Epoch 475/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 20.1369 - val_loss: 42.8040\n",
      "Epoch 476/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1370 - val_loss: 42.8058\n",
      "Epoch 477/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1372 - val_loss: 42.8078\n",
      "Epoch 478/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 20.1373 - val_loss: 42.8093\n",
      "Epoch 479/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1374 - val_loss: 42.8107\n",
      "Epoch 480/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1375 - val_loss: 42.8121\n",
      "Epoch 481/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1377 - val_loss: 42.8136\n",
      "Epoch 482/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1378 - val_loss: 42.8148\n",
      "Epoch 483/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1380 - val_loss: 42.8159\n",
      "Epoch 484/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1381 - val_loss: 42.8168\n",
      "Epoch 485/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1383 - val_loss: 42.8178\n",
      "Epoch 486/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 20.1384 - val_loss: 42.8187\n",
      "Epoch 487/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1386 - val_loss: 42.8195\n",
      "Epoch 488/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1388 - val_loss: 42.8204\n",
      "Epoch 489/500\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 20.1389 - val_loss: 42.8211\n",
      "Epoch 490/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 20.1390 - val_loss: 42.8217\n",
      "Epoch 491/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1392 - val_loss: 42.8222\n",
      "Epoch 492/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1393 - val_loss: 42.8228\n",
      "Epoch 493/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1395 - val_loss: 42.8232\n",
      "Epoch 494/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1396 - val_loss: 42.8237\n",
      "Epoch 495/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1398 - val_loss: 42.8239\n",
      "Epoch 496/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1400 - val_loss: 42.8244\n",
      "Epoch 497/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1401 - val_loss: 42.8246\n",
      "Epoch 498/500\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.1402 - val_loss: 42.8247\n",
      "Epoch 499/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 20.1404 - val_loss: 42.8250\n",
      "Epoch 500/500\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 20.1406 - val_loss: 42.8253\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6465, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0029, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0003, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0168, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3581, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1e-10\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 341ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[62.52836134, 62.50315126, 62.47794118, 62.45273109, 62.42752101,\n",
       "        62.40231092, 62.39745565, 62.39465453, 62.39185341, 62.38905229,\n",
       "        62.38625117, 62.38345005, 62.38064893, 62.37784781, 62.37504669,\n",
       "        62.37224556, 62.36944444, 62.36664332, 62.3638422 , 62.36104108,\n",
       "        62.35823996, 62.35543884, 62.35263772, 62.3498366 , 62.34703548,\n",
       "        62.34423436, 62.34143324, 62.33863212, 62.335831  , 62.33302988,\n",
       "        62.33022876, 62.32742764, 62.32462652, 62.3218254 , 62.31902428,\n",
       "        62.31622316, 62.31342204, 62.31062092, 62.30781979, 62.30501867,\n",
       "        62.30221755, 62.29591503, 62.27630719, 62.25669935, 62.2370915 ,\n",
       "        62.21748366, 62.19787582, 62.17826797, 62.15866013, 62.13905229,\n",
       "        62.11944444, 62.0998366 , 62.08022876, 62.06062092, 62.04101307,\n",
       "        62.02140523, 62.00179739, 61.98218954, 61.9625817 , 61.94297386,\n",
       "        61.92336601, 61.90375817, 61.88415033, 61.86454248, 61.84493464,\n",
       "        61.8253268 , 61.80571895, 61.78611111, 61.76650327, 61.74689542,\n",
       "        61.72728758, 61.70767974, 61.6880719 , 61.66846405, 61.64885621,\n",
       "        61.62924837, 61.60964052, 61.59515873, 61.58563492, 61.57611111,\n",
       "        68.71121979,  0.        ,  0.30991882,  0.6269477 ,  0.10570034,\n",
       "         0.35429442,  0.21235818,  0.0749445 ,  0.32987547,  0.09256098,\n",
       "         0.        ,  0.3086533 ,  0.43633798,  0.        ,  0.08268929,\n",
       "         0.        ,  0.11638702,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([60.39928571, 60.39088235, 60.38247899, 60.37407563, 60.36567227,\n",
       "       60.35860411, 60.35430906, 60.35001401, 60.34571895, 60.3414239 ,\n",
       "       60.33712885, 60.3328338 , 60.32853875, 60.3242437 , 60.31994865,\n",
       "       60.31565359, 60.31135854, 60.30706349, 60.30276844, 60.29847339,\n",
       "       60.29417834, 60.28988329, 60.28558824, 60.28129318, 60.27699813,\n",
       "       60.27270308, 60.26840803, 60.26411298, 60.25981793, 60.25552288,\n",
       "       60.25122782, 60.24693277, 60.24263772, 60.23834267, 60.23404762,\n",
       "       60.22975257, 60.22545752, 60.22116246, 60.21686741, 60.21257236,\n",
       "       60.20827731, 60.20398226, 60.19968721, 60.19539216, 60.19109711,\n",
       "       60.18680205, 60.182507  , 60.17821195, 60.1739169 , 60.16962185,\n",
       "       60.1653268 , 60.16103175, 60.15673669, 60.15244164, 60.14814659,\n",
       "       60.14385154, 60.13955649, 60.13526144, 60.13096639, 60.12667134,\n",
       "       60.12237628, 60.11808123, 60.11378618, 60.10949113, 60.10519608,\n",
       "       60.10090103, 60.09660598, 60.09231092, 60.08801587, 60.08372082,\n",
       "       60.07942577, 60.07513072, 60.07083567, 60.06654062, 60.06224556,\n",
       "       60.05795051, 60.05365546, 60.04936041, 60.04506536, 60.04077031,\n",
       "       60.03647526, 60.03218021, 60.02788515, 60.0235901 , 60.01929505,\n",
       "       60.015     , 60.01070495, 60.0064099 , 60.00211485, 59.99781979,\n",
       "       59.99352474, 59.98922969, 59.98493464, 59.98063959, 59.97634454,\n",
       "       59.97204949, 59.96775444, 59.96345938, 59.95916433, 59.95486928])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.153237184987212\n",
      "13.007053137064224\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
