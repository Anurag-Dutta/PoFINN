{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.200000\n",
       "1       88.931092\n",
       "2       88.662185\n",
       "3       88.393277\n",
       "4       88.124370\n",
       "          ...    \n",
       "2495    52.234980\n",
       "2496    52.226235\n",
       "2497    52.217490\n",
       "2498    52.208745\n",
       "2499    52.200000\n",
       "Name: C1, Length: 2500, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c1_interpolated_2400_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.200000\n",
       "1       88.931092\n",
       "2       88.662185\n",
       "3       88.393277\n",
       "4       88.124370\n",
       "          ...    \n",
       "2395     0.000000\n",
       "2396     0.000000\n",
       "2397     0.409292\n",
       "2398     0.409488\n",
       "2399     0.000000\n",
       "Name: C1, Length: 2400, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2400)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.931092</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.662185</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.393277</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.124370</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     89.200000  0.000298\n",
       "1     88.931092  0.000298\n",
       "2     88.662185  0.000297\n",
       "3     88.393277  0.000297\n",
       "4     88.124370  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6465  # Value for C0\n",
    "K0 = -0.0029  # Value for K0\n",
    "K1 = -0.0003  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0168    # Value for b\n",
    "c = 2.3581    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    89.200000    88.931092    88.662185    88.393277    88.124370   \n",
      "351    88.931092    88.662185    88.393277    88.124370    87.855462   \n",
      "352    88.662185    88.393277    88.124370    87.855462    87.586555   \n",
      "353    88.393277    88.124370    87.855462    87.586555    87.317647   \n",
      "354    88.124370    87.855462    87.586555    87.317647    87.048739   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.855462    87.586555    87.317647    87.048739    86.794538  ...   \n",
      "351    87.586555    87.317647    87.048739    86.794538    86.721709  ...   \n",
      "352    87.317647    87.048739    86.794538    86.721709    86.648880  ...   \n",
      "353    87.048739    86.794538    86.721709    86.648880    86.576050  ...   \n",
      "354    86.794538    86.721709    86.648880    86.576050    86.503221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   73.989683    0.000263   73.957937    0.000263   73.926190    0.000263   \n",
      "351   73.957937    0.000263   73.926190    0.000263   73.894444    0.000262   \n",
      "352   73.926190    0.000263   73.894444    0.000262   73.862698    0.000262   \n",
      "353   73.894444    0.000262   73.862698    0.000262   73.830952    0.000262   \n",
      "354   73.862698    0.000262   73.830952    0.000262   73.799206    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   73.894444    0.000262   73.862698    0.000262  \n",
      "351   73.862698    0.000262   73.830952    0.000262  \n",
      "352   73.830952    0.000262   73.799206    0.000262  \n",
      "353   73.799206    0.000262   73.767460    0.000262  \n",
      "354   73.767460    0.000262   73.735714    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1950, 1, 251) (1950, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "25/25 [==============================] - 3s 23ms/step - loss: 4140.6348 - val_loss: 2360.3799\n",
      "Epoch 2/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 3871.0000 - val_loss: 2228.3621\n",
      "Epoch 3/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 3744.8435 - val_loss: 2162.0725\n",
      "Epoch 4/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3633.8872 - val_loss: 2095.4705\n",
      "Epoch 5/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3528.9070 - val_loss: 2030.4255\n",
      "Epoch 6/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 3415.2949 - val_loss: 1968.0024\n",
      "Epoch 7/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 3311.4888 - val_loss: 1909.6808\n",
      "Epoch 8/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 3212.5012 - val_loss: 1854.4535\n",
      "Epoch 9/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3117.3621 - val_loss: 1801.7793\n",
      "Epoch 10/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 3025.4006 - val_loss: 1751.3613\n",
      "Epoch 11/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 2936.2417 - val_loss: 1703.0159\n",
      "Epoch 12/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 2849.6516 - val_loss: 1656.6163\n",
      "Epoch 13/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 2765.4653 - val_loss: 1612.0659\n",
      "Epoch 14/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 2683.5598 - val_loss: 1569.2881\n",
      "Epoch 15/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 2603.8394 - val_loss: 1528.2178\n",
      "Epoch 16/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 2526.2217 - val_loss: 1488.7983\n",
      "Epoch 17/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 2450.6401 - val_loss: 1450.9796\n",
      "Epoch 18/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 2377.0334 - val_loss: 1414.7158\n",
      "Epoch 19/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 2305.3481 - val_loss: 1379.9642\n",
      "Epoch 20/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 2235.5349 - val_loss: 1346.6853\n",
      "Epoch 21/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 2167.5483 - val_loss: 1314.8406\n",
      "Epoch 22/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2101.3459 - val_loss: 1284.3945\n",
      "Epoch 23/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2036.8877 - val_loss: 1255.3120\n",
      "Epoch 24/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1974.1356 - val_loss: 1227.5601\n",
      "Epoch 25/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1913.0537 - val_loss: 1201.1061\n",
      "Epoch 26/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1853.6060 - val_loss: 1175.9181\n",
      "Epoch 27/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1795.7592 - val_loss: 1151.9657\n",
      "Epoch 28/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1739.4806 - val_loss: 1129.2186\n",
      "Epoch 29/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1684.7384 - val_loss: 1107.6472\n",
      "Epoch 30/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1631.5020 - val_loss: 1087.2223\n",
      "Epoch 31/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1579.7399 - val_loss: 1067.9155\n",
      "Epoch 32/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1529.4232 - val_loss: 1049.6990\n",
      "Epoch 33/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1480.5232 - val_loss: 1032.5441\n",
      "Epoch 34/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1433.0107 - val_loss: 1016.4240\n",
      "Epoch 35/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1386.8579 - val_loss: 1001.3115\n",
      "Epoch 36/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1342.0377 - val_loss: 987.1785\n",
      "Epoch 37/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1298.5227 - val_loss: 973.9970\n",
      "Epoch 38/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 1256.2861 - val_loss: 961.7349\n",
      "Epoch 39/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 1215.3019 - val_loss: 950.3268\n",
      "Epoch 40/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 1175.2593 - val_loss: 944.2390\n",
      "Epoch 41/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1185.9347 - val_loss: 942.3483\n",
      "Epoch 42/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 1147.5919 - val_loss: 932.6468\n",
      "Epoch 43/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1110.2930 - val_loss: 923.7557\n",
      "Epoch 44/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1074.1279 - val_loss: 915.6627\n",
      "Epoch 45/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1039.0803 - val_loss: 908.3437\n",
      "Epoch 46/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1005.1252 - val_loss: 901.7751\n",
      "Epoch 47/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 972.2382 - val_loss: 895.9337\n",
      "Epoch 48/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 940.3954 - val_loss: 890.7959\n",
      "Epoch 49/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 909.5726 - val_loss: 886.3394\n",
      "Epoch 50/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 879.7470 - val_loss: 882.5413\n",
      "Epoch 51/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 850.8948 - val_loss: 879.3796\n",
      "Epoch 52/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 822.9938 - val_loss: 876.8326\n",
      "Epoch 53/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 796.0212 - val_loss: 874.8784\n",
      "Epoch 54/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 769.9552 - val_loss: 873.4958\n",
      "Epoch 55/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 744.7741 - val_loss: 872.6637\n",
      "Epoch 56/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 720.4564 - val_loss: 872.3616\n",
      "Epoch 57/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 696.9808 - val_loss: 872.5688\n",
      "Epoch 58/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 674.3270 - val_loss: 873.2652\n",
      "Epoch 59/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 652.4738 - val_loss: 874.4308\n",
      "Epoch 60/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 631.4015 - val_loss: 876.0461\n",
      "Epoch 61/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 611.0896 - val_loss: 878.0916\n",
      "Epoch 62/500\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 591.5189 - val_loss: 880.5483\n",
      "Epoch 63/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 572.6696 - val_loss: 883.3975\n",
      "Epoch 64/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 554.5226 - val_loss: 886.6205\n",
      "Epoch 65/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 537.0591 - val_loss: 890.1992\n",
      "Epoch 66/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 520.2604 - val_loss: 894.1155\n",
      "Epoch 67/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 504.1083 - val_loss: 898.3520\n",
      "Epoch 68/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 488.5844 - val_loss: 902.8911\n",
      "Epoch 69/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 473.6711 - val_loss: 907.7155\n",
      "Epoch 70/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 459.3510 - val_loss: 912.8090\n",
      "Epoch 71/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 445.6065 - val_loss: 918.1549\n",
      "Epoch 72/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 432.4206 - val_loss: 923.7367\n",
      "Epoch 73/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 419.7766 - val_loss: 929.5390\n",
      "Epoch 74/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 407.6580 - val_loss: 935.5460\n",
      "Epoch 75/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 396.0485 - val_loss: 941.7424\n",
      "Epoch 76/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 384.9324 - val_loss: 948.1133\n",
      "Epoch 77/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 374.2938 - val_loss: 954.6445\n",
      "Epoch 78/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 364.1172 - val_loss: 961.3212\n",
      "Epoch 79/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 354.3877 - val_loss: 968.1298\n",
      "Epoch 80/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 345.0903 - val_loss: 975.0566\n",
      "Epoch 81/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 336.2103 - val_loss: 982.0886\n",
      "Epoch 82/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 327.7336 - val_loss: 989.2123\n",
      "Epoch 83/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 319.6461 - val_loss: 996.4160\n",
      "Epoch 84/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 311.9339 - val_loss: 1003.6868\n",
      "Epoch 85/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 304.5836 - val_loss: 1011.0133\n",
      "Epoch 86/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 297.5822 - val_loss: 1018.3838\n",
      "Epoch 87/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 290.9167 - val_loss: 1025.7875\n",
      "Epoch 88/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 284.5743 - val_loss: 1033.2130\n",
      "Epoch 89/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 278.5430 - val_loss: 1040.6506\n",
      "Epoch 90/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 272.8104 - val_loss: 1048.0902\n",
      "Epoch 91/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 267.3650 - val_loss: 1055.5217\n",
      "Epoch 92/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 262.1952 - val_loss: 1062.9359\n",
      "Epoch 93/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 257.2901 - val_loss: 1070.3247\n",
      "Epoch 94/500\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 252.6385 - val_loss: 1077.6790\n",
      "Epoch 95/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 248.2299 - val_loss: 1084.9905\n",
      "Epoch 96/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 244.0540 - val_loss: 1092.2520\n",
      "Epoch 97/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 240.1008 - val_loss: 1099.4561\n",
      "Epoch 98/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 236.3605 - val_loss: 1106.5946\n",
      "Epoch 99/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 232.8238 - val_loss: 1113.6627\n",
      "Epoch 100/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 229.4815 - val_loss: 1120.6527\n",
      "Epoch 101/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 226.3246 - val_loss: 1127.5593\n",
      "Epoch 102/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 223.3447 - val_loss: 1134.3774\n",
      "Epoch 103/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 220.5333 - val_loss: 1141.1016\n",
      "Epoch 104/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 217.8824 - val_loss: 1147.7256\n",
      "Epoch 105/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 215.3845 - val_loss: 1154.2476\n",
      "Epoch 106/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 213.0317 - val_loss: 1160.6616\n",
      "Epoch 107/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 210.8170 - val_loss: 1166.9642\n",
      "Epoch 108/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 208.7335 - val_loss: 1173.1533\n",
      "Epoch 109/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 206.7742 - val_loss: 1179.2236\n",
      "Epoch 110/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 204.9330 - val_loss: 1185.1742\n",
      "Epoch 111/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 203.2036 - val_loss: 1191.0015\n",
      "Epoch 112/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 201.5800 - val_loss: 1196.7039\n",
      "Epoch 113/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 200.0567 - val_loss: 1202.2795\n",
      "Epoch 114/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 198.6280 - val_loss: 1207.7271\n",
      "Epoch 115/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 197.2887 - val_loss: 1213.0446\n",
      "Epoch 116/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 196.0340 - val_loss: 1218.2313\n",
      "Epoch 117/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 194.8589 - val_loss: 1223.2869\n",
      "Epoch 118/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 193.7590 - val_loss: 1228.2104\n",
      "Epoch 119/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 192.7299 - val_loss: 1233.0027\n",
      "Epoch 120/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 191.7674 - val_loss: 1237.6627\n",
      "Epoch 121/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 190.8677 - val_loss: 1242.1920\n",
      "Epoch 122/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 190.0269 - val_loss: 1246.5887\n",
      "Epoch 123/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 189.2415 - val_loss: 1250.8555\n",
      "Epoch 124/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 188.5082 - val_loss: 1254.9919\n",
      "Epoch 125/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 187.8236 - val_loss: 1259.0007\n",
      "Epoch 126/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 187.1849 - val_loss: 1262.8818\n",
      "Epoch 127/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 186.5891 - val_loss: 1266.6368\n",
      "Epoch 128/500\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 186.0334 - val_loss: 1270.2676\n",
      "Epoch 129/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 185.5155 - val_loss: 1273.7753\n",
      "Epoch 130/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 185.0327 - val_loss: 1277.1617\n",
      "Epoch 131/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 184.5828 - val_loss: 1280.4299\n",
      "Epoch 132/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 184.1637 - val_loss: 1283.5808\n",
      "Epoch 133/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 183.7734 - val_loss: 1286.6166\n",
      "Epoch 134/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 183.4099 - val_loss: 1289.5399\n",
      "Epoch 135/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 183.0714 - val_loss: 1292.3529\n",
      "Epoch 136/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 182.7563 - val_loss: 1295.0581\n",
      "Epoch 137/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 182.4630 - val_loss: 1297.6573\n",
      "Epoch 138/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 182.1901 - val_loss: 1300.1537\n",
      "Epoch 139/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 181.9360 - val_loss: 1302.5499\n",
      "Epoch 140/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 181.6996 - val_loss: 1304.8477\n",
      "Epoch 141/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 181.4796 - val_loss: 1307.0503\n",
      "Epoch 142/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 181.2749 - val_loss: 1309.1606\n",
      "Epoch 143/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 181.0844 - val_loss: 1311.1804\n",
      "Epoch 144/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 180.9071 - val_loss: 1313.1127\n",
      "Epoch 145/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 180.7422 - val_loss: 1314.9607\n",
      "Epoch 146/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 180.5887 - val_loss: 1316.7268\n",
      "Epoch 147/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 180.4458 - val_loss: 1318.4125\n",
      "Epoch 148/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 180.3130 - val_loss: 1320.0216\n",
      "Epoch 149/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 180.1892 - val_loss: 1321.5570\n",
      "Epoch 150/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 180.0741 - val_loss: 1323.0201\n",
      "Epoch 151/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 179.9669 - val_loss: 1324.4147\n",
      "Epoch 152/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 179.8671 - val_loss: 1325.7424\n",
      "Epoch 153/500\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 179.7742 - val_loss: 1327.0055\n",
      "Epoch 154/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 179.6878 - val_loss: 1328.2070\n",
      "Epoch 155/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 179.6072 - val_loss: 1329.3490\n",
      "Epoch 156/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 179.5323 - val_loss: 1330.4341\n",
      "Epoch 157/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 179.4625 - val_loss: 1331.4642\n",
      "Epoch 158/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 179.3975 - val_loss: 1332.4415\n",
      "Epoch 159/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 179.3369 - val_loss: 1333.3689\n",
      "Epoch 160/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 179.2805 - val_loss: 1334.2476\n",
      "Epoch 161/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 179.2279 - val_loss: 1335.0806\n",
      "Epoch 162/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 179.1790 - val_loss: 1335.8690\n",
      "Epoch 163/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 179.1334 - val_loss: 1336.6152\n",
      "Epoch 164/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 179.0910 - val_loss: 1337.3214\n",
      "Epoch 165/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 179.0514 - val_loss: 1337.9889\n",
      "Epoch 166/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 179.0145 - val_loss: 1338.6200\n",
      "Epoch 167/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.9802 - val_loss: 1339.2153\n",
      "Epoch 168/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.9482 - val_loss: 1339.7780\n",
      "Epoch 169/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 178.9185 - val_loss: 1340.3085\n",
      "Epoch 170/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.8908 - val_loss: 1340.8085\n",
      "Epoch 171/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.8650 - val_loss: 1341.2803\n",
      "Epoch 172/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.8409 - val_loss: 1341.7246\n",
      "Epoch 173/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.8186 - val_loss: 1342.1434\n",
      "Epoch 174/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.7978 - val_loss: 1342.5369\n",
      "Epoch 175/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.7784 - val_loss: 1342.9080\n",
      "Epoch 176/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.7604 - val_loss: 1343.2562\n",
      "Epoch 177/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.7437 - val_loss: 1343.5836\n",
      "Epoch 178/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.7281 - val_loss: 1343.8912\n",
      "Epoch 179/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.7137 - val_loss: 1344.1802\n",
      "Epoch 180/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.7002 - val_loss: 1344.4519\n",
      "Epoch 181/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.6877 - val_loss: 1344.7063\n",
      "Epoch 182/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.6762 - val_loss: 1344.9452\n",
      "Epoch 183/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.6655 - val_loss: 1345.1687\n",
      "Epoch 184/500\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 178.6554 - val_loss: 1345.3790\n",
      "Epoch 185/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.6463 - val_loss: 1345.5757\n",
      "Epoch 186/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.6377 - val_loss: 1345.7598\n",
      "Epoch 187/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.6298 - val_loss: 1345.9318\n",
      "Epoch 188/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.6225 - val_loss: 1346.0928\n",
      "Epoch 189/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.6157 - val_loss: 1346.2432\n",
      "Epoch 190/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.6095 - val_loss: 1346.3838\n",
      "Epoch 191/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.6037 - val_loss: 1346.5162\n",
      "Epoch 192/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5984 - val_loss: 1346.6387\n",
      "Epoch 193/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5936 - val_loss: 1346.7535\n",
      "Epoch 194/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5891 - val_loss: 1346.8607\n",
      "Epoch 195/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5849 - val_loss: 1346.9600\n",
      "Epoch 196/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5811 - val_loss: 1347.0543\n",
      "Epoch 197/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5776 - val_loss: 1347.1400\n",
      "Epoch 198/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5744 - val_loss: 1347.2209\n",
      "Epoch 199/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5716 - val_loss: 1347.2961\n",
      "Epoch 200/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5689 - val_loss: 1347.3672\n",
      "Epoch 201/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5665 - val_loss: 1347.4323\n",
      "Epoch 202/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5643 - val_loss: 1347.4934\n",
      "Epoch 203/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5623 - val_loss: 1347.5492\n",
      "Epoch 204/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5605 - val_loss: 1347.6016\n",
      "Epoch 205/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5589 - val_loss: 1347.6508\n",
      "Epoch 206/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5575 - val_loss: 1347.6963\n",
      "Epoch 207/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5562 - val_loss: 1347.7383\n",
      "Epoch 208/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5551 - val_loss: 1347.7776\n",
      "Epoch 209/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5540 - val_loss: 1347.8140\n",
      "Epoch 210/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5531 - val_loss: 1347.8472\n",
      "Epoch 211/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5523 - val_loss: 1347.8784\n",
      "Epoch 212/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5516 - val_loss: 1347.9077\n",
      "Epoch 213/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5510 - val_loss: 1347.9348\n",
      "Epoch 214/500\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 178.5505 - val_loss: 1347.9590\n",
      "Epoch 215/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5501 - val_loss: 1347.9822\n",
      "Epoch 216/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5497 - val_loss: 1348.0035\n",
      "Epoch 217/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5494 - val_loss: 1348.0232\n",
      "Epoch 218/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 178.5492 - val_loss: 1348.0411\n",
      "Epoch 219/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5490 - val_loss: 1348.0586\n",
      "Epoch 220/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5489 - val_loss: 1348.0742\n",
      "Epoch 221/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5488 - val_loss: 1348.0885\n",
      "Epoch 222/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5489 - val_loss: 1348.1014\n",
      "Epoch 223/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5488 - val_loss: 1348.1139\n",
      "Epoch 224/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5489 - val_loss: 1348.1254\n",
      "Epoch 225/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5490 - val_loss: 1348.1356\n",
      "Epoch 226/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5492 - val_loss: 1348.1449\n",
      "Epoch 227/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5493 - val_loss: 1348.1542\n",
      "Epoch 228/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5494 - val_loss: 1348.1624\n",
      "Epoch 229/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5496 - val_loss: 1348.1697\n",
      "Epoch 230/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5498 - val_loss: 1348.1766\n",
      "Epoch 231/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5501 - val_loss: 1348.1827\n",
      "Epoch 232/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 178.5503 - val_loss: 1348.1886\n",
      "Epoch 233/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 178.5506 - val_loss: 1348.1942\n",
      "Epoch 234/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5508 - val_loss: 1348.1991\n",
      "Epoch 235/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5511 - val_loss: 1348.2034\n",
      "Epoch 236/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5514 - val_loss: 1348.2076\n",
      "Epoch 237/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5517 - val_loss: 1348.2114\n",
      "Epoch 238/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5520 - val_loss: 1348.2152\n",
      "Epoch 239/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5522 - val_loss: 1348.2181\n",
      "Epoch 240/500\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 178.5526 - val_loss: 1348.2207\n",
      "Epoch 241/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5529 - val_loss: 1348.2239\n",
      "Epoch 242/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5532 - val_loss: 1348.2261\n",
      "Epoch 243/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5535 - val_loss: 1348.2279\n",
      "Epoch 244/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5539 - val_loss: 1348.2301\n",
      "Epoch 245/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5542 - val_loss: 1348.2321\n",
      "Epoch 246/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5545 - val_loss: 1348.2333\n",
      "Epoch 247/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5548 - val_loss: 1348.2343\n",
      "Epoch 248/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5551 - val_loss: 1348.2358\n",
      "Epoch 249/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5555 - val_loss: 1348.2378\n",
      "Epoch 250/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5558 - val_loss: 1348.2389\n",
      "Epoch 251/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5561 - val_loss: 1348.2396\n",
      "Epoch 252/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5564 - val_loss: 1348.2407\n",
      "Epoch 253/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5567 - val_loss: 1348.2410\n",
      "Epoch 254/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5570 - val_loss: 1348.2415\n",
      "Epoch 255/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5574 - val_loss: 1348.2418\n",
      "Epoch 256/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5576 - val_loss: 1348.2423\n",
      "Epoch 257/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5580 - val_loss: 1348.2432\n",
      "Epoch 258/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5582 - val_loss: 1348.2439\n",
      "Epoch 259/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5585 - val_loss: 1348.2444\n",
      "Epoch 260/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5588 - val_loss: 1348.2444\n",
      "Epoch 261/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5591 - val_loss: 1348.2444\n",
      "Epoch 262/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5593 - val_loss: 1348.2445\n",
      "Epoch 263/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 178.5596 - val_loss: 1348.2452\n",
      "Epoch 264/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 178.5599 - val_loss: 1348.2450\n",
      "Epoch 265/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5601 - val_loss: 1348.2452\n",
      "Epoch 266/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5604 - val_loss: 1348.2454\n",
      "Epoch 267/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5607 - val_loss: 1348.2454\n",
      "Epoch 268/500\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 178.5609 - val_loss: 1348.2455\n",
      "Epoch 269/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5611 - val_loss: 1348.2455\n",
      "Epoch 270/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5613 - val_loss: 1348.2452\n",
      "Epoch 271/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5616 - val_loss: 1348.2452\n",
      "Epoch 272/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5619 - val_loss: 1348.2452\n",
      "Epoch 273/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5620 - val_loss: 1348.2455\n",
      "Epoch 274/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5623 - val_loss: 1348.2455\n",
      "Epoch 275/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5624 - val_loss: 1348.2454\n",
      "Epoch 276/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5627 - val_loss: 1348.2450\n",
      "Epoch 277/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5629 - val_loss: 1348.2450\n",
      "Epoch 278/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5631 - val_loss: 1348.2452\n",
      "Epoch 279/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5633 - val_loss: 1348.2450\n",
      "Epoch 280/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5635 - val_loss: 1348.2448\n",
      "Epoch 281/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5637 - val_loss: 1348.2444\n",
      "Epoch 282/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5639 - val_loss: 1348.2445\n",
      "Epoch 283/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5641 - val_loss: 1348.2448\n",
      "Epoch 284/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5642 - val_loss: 1348.2445\n",
      "Epoch 285/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5644 - val_loss: 1348.2444\n",
      "Epoch 286/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5646 - val_loss: 1348.2444\n",
      "Epoch 287/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5647 - val_loss: 1348.2443\n",
      "Epoch 288/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5649 - val_loss: 1348.2438\n",
      "Epoch 289/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5651 - val_loss: 1348.2435\n",
      "Epoch 290/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5652 - val_loss: 1348.2435\n",
      "Epoch 291/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5654 - val_loss: 1348.2435\n",
      "Epoch 292/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5655 - val_loss: 1348.2433\n",
      "Epoch 293/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5657 - val_loss: 1348.2433\n",
      "Epoch 294/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5658 - val_loss: 1348.2432\n",
      "Epoch 295/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5659 - val_loss: 1348.2419\n",
      "Epoch 296/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5661 - val_loss: 1348.2418\n",
      "Epoch 297/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5662 - val_loss: 1348.2418\n",
      "Epoch 298/500\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 178.5663 - val_loss: 1348.2418\n",
      "Epoch 299/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5665 - val_loss: 1348.2418\n",
      "Epoch 300/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5666 - val_loss: 1348.2418\n",
      "Epoch 301/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5668 - val_loss: 1348.2417\n",
      "Epoch 302/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5669 - val_loss: 1348.2417\n",
      "Epoch 303/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5670 - val_loss: 1348.2417\n",
      "Epoch 304/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5671 - val_loss: 1348.2417\n",
      "Epoch 305/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5672 - val_loss: 1348.2415\n",
      "Epoch 306/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5673 - val_loss: 1348.2412\n",
      "Epoch 307/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5674 - val_loss: 1348.2412\n",
      "Epoch 308/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5675 - val_loss: 1348.2412\n",
      "Epoch 309/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5676 - val_loss: 1348.2410\n",
      "Epoch 310/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5677 - val_loss: 1348.2407\n",
      "Epoch 311/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5678 - val_loss: 1348.2407\n",
      "Epoch 312/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5679 - val_loss: 1348.2407\n",
      "Epoch 313/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5679 - val_loss: 1348.2404\n",
      "Epoch 314/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5680 - val_loss: 1348.2404\n",
      "Epoch 315/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5682 - val_loss: 1348.2404\n",
      "Epoch 316/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5683 - val_loss: 1348.2404\n",
      "Epoch 317/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5683 - val_loss: 1348.2404\n",
      "Epoch 318/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5684 - val_loss: 1348.2404\n",
      "Epoch 319/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5685 - val_loss: 1348.2404\n",
      "Epoch 320/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5685 - val_loss: 1348.2402\n",
      "Epoch 321/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5686 - val_loss: 1348.2401\n",
      "Epoch 322/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5687 - val_loss: 1348.2396\n",
      "Epoch 323/500\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 178.5688 - val_loss: 1348.2389\n",
      "Epoch 324/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5689 - val_loss: 1348.2388\n",
      "Epoch 325/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5690 - val_loss: 1348.2388\n",
      "Epoch 326/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5690 - val_loss: 1348.2383\n",
      "Epoch 327/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5691 - val_loss: 1348.2388\n",
      "Epoch 328/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 178.5692 - val_loss: 1348.2391\n",
      "Epoch 329/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5692 - val_loss: 1348.2389\n",
      "Epoch 330/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5693 - val_loss: 1348.2391\n",
      "Epoch 331/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5693 - val_loss: 1348.2391\n",
      "Epoch 332/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5694 - val_loss: 1348.2389\n",
      "Epoch 333/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5694 - val_loss: 1348.2384\n",
      "Epoch 334/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5695 - val_loss: 1348.2383\n",
      "Epoch 335/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5695 - val_loss: 1348.2386\n",
      "Epoch 336/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5695 - val_loss: 1348.2383\n",
      "Epoch 337/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5696 - val_loss: 1348.2388\n",
      "Epoch 338/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5697 - val_loss: 1348.2388\n",
      "Epoch 339/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5697 - val_loss: 1348.2388\n",
      "Epoch 340/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5697 - val_loss: 1348.2384\n",
      "Epoch 341/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5698 - val_loss: 1348.2383\n",
      "Epoch 342/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 178.5699 - val_loss: 1348.2383\n",
      "Epoch 343/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5699 - val_loss: 1348.2382\n",
      "Epoch 344/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5699 - val_loss: 1348.2382\n",
      "Epoch 345/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5700 - val_loss: 1348.2382\n",
      "Epoch 346/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5701 - val_loss: 1348.2383\n",
      "Epoch 347/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5701 - val_loss: 1348.2383\n",
      "Epoch 348/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 178.5701 - val_loss: 1348.2383\n",
      "Epoch 349/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5701 - val_loss: 1348.2380\n",
      "Epoch 350/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5701 - val_loss: 1348.2377\n",
      "Epoch 351/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5702 - val_loss: 1348.2380\n",
      "Epoch 352/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5703 - val_loss: 1348.2382\n",
      "Epoch 353/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5703 - val_loss: 1348.2380\n",
      "Epoch 354/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5703 - val_loss: 1348.2377\n",
      "Epoch 355/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5703 - val_loss: 1348.2373\n",
      "Epoch 356/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5704 - val_loss: 1348.2371\n",
      "Epoch 357/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5705 - val_loss: 1348.2377\n",
      "Epoch 358/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5705 - val_loss: 1348.2380\n",
      "Epoch 359/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5705 - val_loss: 1348.2382\n",
      "Epoch 360/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5705 - val_loss: 1348.2382\n",
      "Epoch 361/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5705 - val_loss: 1348.2380\n",
      "Epoch 362/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5705 - val_loss: 1348.2382\n",
      "Epoch 363/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5706 - val_loss: 1348.2380\n",
      "Epoch 364/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5706 - val_loss: 1348.2380\n",
      "Epoch 365/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5706 - val_loss: 1348.2380\n",
      "Epoch 366/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5706 - val_loss: 1348.2377\n",
      "Epoch 367/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5707 - val_loss: 1348.2375\n",
      "Epoch 368/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5707 - val_loss: 1348.2371\n",
      "Epoch 369/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5707 - val_loss: 1348.2368\n",
      "Epoch 370/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5707 - val_loss: 1348.2368\n",
      "Epoch 371/500\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 178.5708 - val_loss: 1348.2371\n",
      "Epoch 372/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 178.5708 - val_loss: 1348.2371\n",
      "Epoch 373/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5708 - val_loss: 1348.2373\n",
      "Epoch 374/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5708 - val_loss: 1348.2377\n",
      "Epoch 375/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5708 - val_loss: 1348.2380\n",
      "Epoch 376/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5709 - val_loss: 1348.2382\n",
      "Epoch 377/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5709 - val_loss: 1348.2380\n",
      "Epoch 378/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5709 - val_loss: 1348.2380\n",
      "Epoch 379/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5709 - val_loss: 1348.2380\n",
      "Epoch 380/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5709 - val_loss: 1348.2377\n",
      "Epoch 381/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5709 - val_loss: 1348.2377\n",
      "Epoch 382/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5710 - val_loss: 1348.2380\n",
      "Epoch 383/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5710 - val_loss: 1348.2380\n",
      "Epoch 384/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5710 - val_loss: 1348.2382\n",
      "Epoch 385/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5710 - val_loss: 1348.2383\n",
      "Epoch 386/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5710 - val_loss: 1348.2383\n",
      "Epoch 387/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5710 - val_loss: 1348.2383\n",
      "Epoch 388/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5710 - val_loss: 1348.2382\n",
      "Epoch 389/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5710 - val_loss: 1348.2382\n",
      "Epoch 390/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5711 - val_loss: 1348.2382\n",
      "Epoch 391/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5710 - val_loss: 1348.2382\n",
      "Epoch 392/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5711 - val_loss: 1348.2380\n",
      "Epoch 393/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5711 - val_loss: 1348.2380\n",
      "Epoch 394/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5711 - val_loss: 1348.2377\n",
      "Epoch 395/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5711 - val_loss: 1348.2375\n",
      "Epoch 396/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 178.5711 - val_loss: 1348.2368\n",
      "Epoch 397/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 178.5712 - val_loss: 1348.2367\n",
      "Epoch 398/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5712 - val_loss: 1348.2366\n",
      "Epoch 399/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5712 - val_loss: 1348.2362\n",
      "Epoch 400/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5712 - val_loss: 1348.2356\n",
      "Epoch 401/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5712 - val_loss: 1348.2352\n",
      "Epoch 402/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5713 - val_loss: 1348.2346\n",
      "Epoch 403/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5713 - val_loss: 1348.2345\n",
      "Epoch 404/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5714 - val_loss: 1348.2345\n",
      "Epoch 405/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5714 - val_loss: 1348.2345\n",
      "Epoch 406/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 178.5714 - val_loss: 1348.2345\n",
      "Epoch 407/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5714 - val_loss: 1348.2346\n",
      "Epoch 408/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5714 - val_loss: 1348.2346\n",
      "Epoch 409/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5714 - val_loss: 1348.2346\n",
      "Epoch 410/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5714 - val_loss: 1348.2346\n",
      "Epoch 411/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5714 - val_loss: 1348.2355\n",
      "Epoch 412/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5714 - val_loss: 1348.2357\n",
      "Epoch 413/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5714 - val_loss: 1348.2361\n",
      "Epoch 414/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5714 - val_loss: 1348.2366\n",
      "Epoch 415/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5714 - val_loss: 1348.2366\n",
      "Epoch 416/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 178.5714 - val_loss: 1348.2366\n",
      "Epoch 417/500\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 178.5714 - val_loss: 1348.2366\n",
      "Epoch 418/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5714 - val_loss: 1348.2366\n",
      "Epoch 419/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5714 - val_loss: 1348.2366\n",
      "Epoch 420/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5714 - val_loss: 1348.2368\n",
      "Epoch 421/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5714 - val_loss: 1348.2368\n",
      "Epoch 422/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5714 - val_loss: 1348.2372\n",
      "Epoch 423/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5714 - val_loss: 1348.2375\n",
      "Epoch 424/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5714 - val_loss: 1348.2375\n",
      "Epoch 425/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5714 - val_loss: 1348.2377\n",
      "Epoch 426/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5714 - val_loss: 1348.2377\n",
      "Epoch 427/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5714 - val_loss: 1348.2378\n",
      "Epoch 428/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5714 - val_loss: 1348.2380\n",
      "Epoch 429/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5714 - val_loss: 1348.2380\n",
      "Epoch 430/500\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 178.5714 - val_loss: 1348.2380\n",
      "Epoch 431/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5714 - val_loss: 1348.2380\n",
      "Epoch 432/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5714 - val_loss: 1348.2377\n",
      "Epoch 433/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5714 - val_loss: 1348.2377\n",
      "Epoch 434/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5714 - val_loss: 1348.2375\n",
      "Epoch 435/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5714 - val_loss: 1348.2375\n",
      "Epoch 436/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5714 - val_loss: 1348.2375\n",
      "Epoch 437/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5714 - val_loss: 1348.2372\n",
      "Epoch 438/500\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 178.5714 - val_loss: 1348.2372\n",
      "Epoch 439/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5714 - val_loss: 1348.2366\n",
      "Epoch 440/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 178.5714 - val_loss: 1348.2361\n",
      "Epoch 441/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5715 - val_loss: 1348.2358\n",
      "Epoch 442/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5715 - val_loss: 1348.2352\n",
      "Epoch 443/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5715 - val_loss: 1348.2346\n",
      "Epoch 444/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5715 - val_loss: 1348.2345\n",
      "Epoch 445/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5716 - val_loss: 1348.2345\n",
      "Epoch 446/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5716 - val_loss: 1348.2345\n",
      "Epoch 447/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5716 - val_loss: 1348.2345\n",
      "Epoch 448/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5716 - val_loss: 1348.2345\n",
      "Epoch 449/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5716 - val_loss: 1348.2343\n",
      "Epoch 450/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5716 - val_loss: 1348.2343\n",
      "Epoch 451/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5716 - val_loss: 1348.2341\n",
      "Epoch 452/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5716 - val_loss: 1348.2341\n",
      "Epoch 453/500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 178.5716 - val_loss: 1348.2340\n",
      "Epoch 454/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5716 - val_loss: 1348.2338\n",
      "Epoch 455/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5716 - val_loss: 1348.2340\n",
      "Epoch 456/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5716 - val_loss: 1348.2340\n",
      "Epoch 457/500\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 178.5716 - val_loss: 1348.2340\n",
      "Epoch 458/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5716 - val_loss: 1348.2340\n",
      "Epoch 459/500\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 178.5716 - val_loss: 1348.2340\n",
      "Epoch 460/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5717 - val_loss: 1348.2341\n",
      "Epoch 461/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5716 - val_loss: 1348.2341\n",
      "Epoch 462/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5717 - val_loss: 1348.2341\n",
      "Epoch 463/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5717 - val_loss: 1348.2341\n",
      "Epoch 464/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5717 - val_loss: 1348.2346\n",
      "Epoch 465/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5717 - val_loss: 1348.2346\n",
      "Epoch 466/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5717 - val_loss: 1348.2350\n",
      "Epoch 467/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5717 - val_loss: 1348.2350\n",
      "Epoch 468/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5717 - val_loss: 1348.2352\n",
      "Epoch 469/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5716 - val_loss: 1348.2352\n",
      "Epoch 470/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5716 - val_loss: 1348.2356\n",
      "Epoch 471/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5716 - val_loss: 1348.2357\n",
      "Epoch 472/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5716 - val_loss: 1348.2361\n",
      "Epoch 473/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5716 - val_loss: 1348.2366\n",
      "Epoch 474/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5716 - val_loss: 1348.2367\n",
      "Epoch 475/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5716 - val_loss: 1348.2371\n",
      "Epoch 476/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 178.5716 - val_loss: 1348.2375\n",
      "Epoch 477/500\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 178.5716 - val_loss: 1348.2377\n",
      "Epoch 478/500\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 178.5716 - val_loss: 1348.2378\n",
      "Epoch 479/500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 178.5715 - val_loss: 1348.2378\n",
      "Epoch 480/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5715 - val_loss: 1348.2380\n",
      "Epoch 481/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5715 - val_loss: 1348.2382\n",
      "Epoch 482/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5715 - val_loss: 1348.2383\n",
      "Epoch 483/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5715 - val_loss: 1348.2384\n",
      "Epoch 484/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5715 - val_loss: 1348.2388\n",
      "Epoch 485/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5715 - val_loss: 1348.2389\n",
      "Epoch 486/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5715 - val_loss: 1348.2389\n",
      "Epoch 487/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5715 - val_loss: 1348.2393\n",
      "Epoch 488/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5715 - val_loss: 1348.2394\n",
      "Epoch 489/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5715 - val_loss: 1348.2397\n",
      "Epoch 490/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5714 - val_loss: 1348.2401\n",
      "Epoch 491/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5714 - val_loss: 1348.2401\n",
      "Epoch 492/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5714 - val_loss: 1348.2401\n",
      "Epoch 493/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5714 - val_loss: 1348.2401\n",
      "Epoch 494/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5714 - val_loss: 1348.2401\n",
      "Epoch 495/500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 178.5714 - val_loss: 1348.2401\n",
      "Epoch 496/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5714 - val_loss: 1348.2401\n",
      "Epoch 497/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5714 - val_loss: 1348.2401\n",
      "Epoch 498/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5714 - val_loss: 1348.2401\n",
      "Epoch 499/500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 178.5714 - val_loss: 1348.2401\n",
      "Epoch 500/500\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 178.5714 - val_loss: 1348.2401\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6465, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0029, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0003, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0168, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3581, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 477ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.23843837e+01, 6.23591737e+01, 6.23339636e+01, 6.23087535e+01,\n",
       "        6.91144409e+01, 3.32957830e-01, 2.89757130e-01, 5.09254280e-01,\n",
       "        2.84271660e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.48248720e-01, 6.26712185e+01, 6.24443277e+01, 6.23797152e+01,\n",
       "        0.00000000e+00, 9.36131420e-01, 6.32174370e+01, 6.29905462e+01,\n",
       "        6.27636555e+01, 6.25367647e+01, 6.23899860e+01, 6.23647759e+01,\n",
       "        6.23395658e+01, 6.23143557e+01, 6.22240196e+01, 5.98464790e-01,\n",
       "        1.02453041e+00, 6.24023109e+01, 6.23750467e+01, 6.23498366e+01,\n",
       "        6.23246265e+01, 6.22959150e+01, 6.21194444e+01, 6.19429739e+01,\n",
       "        6.17665033e+01, 6.15951587e+01, 2.12358180e-01, 0.00000000e+00,\n",
       "        4.00981660e-01, 0.00000000e+00, 0.00000000e+00, 4.54650400e-01,\n",
       "        0.00000000e+00, 7.51809900e-02, 2.82301490e-01, 6.23199580e+01,\n",
       "        6.22632353e+01, 0.00000000e+00, 5.97582459e-01, 6.24527311e+01,\n",
       "        6.23806489e+01, 6.23554388e+01, 6.23302288e+01, 6.23050187e+01,\n",
       "        6.21586601e+01, 6.19821895e+01, 6.18057189e+01, 6.16292484e+01,\n",
       "        1.05700340e-01, 0.00000000e+00, 6.23152895e+01, 6.22305556e+01,\n",
       "        6.22054085e+01, 6.21877614e+01, 6.21701144e+01, 6.39678105e+01,\n",
       "        6.34383987e+01, 6.27972689e+01, 0.00000000e+00, 1.03494644e-01,\n",
       "        6.49816620e-02, 5.22042122e+01, 1.86739072e-01, 0.00000000e+00,\n",
       "        6.10181570e-01, 4.14854854e-01, 0.00000000e+00, 3.23886007e-01,\n",
       "        6.05423584e+01, 4.09309447e-01, 1.38536859e+00, 5.56463540e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.40732422e-01, 1.76612988e-01,\n",
       "        0.00000000e+00, 9.70736593e-02, 0.00000000e+00, 6.97304964e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.20219469e-01, 7.64393032e-01,\n",
       "        0.00000000e+00, 6.98342770e-02, 0.00000000e+00, 6.10190690e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([53.06575118, 53.05700622, 53.04826126, 53.03951629, 53.03077133,\n",
       "       53.02202637, 53.01328141, 53.00453645, 52.99579149, 52.98704653,\n",
       "       52.97830156, 52.9695566 , 52.96081164, 52.95206668, 52.94332172,\n",
       "       52.93457676, 52.9258318 , 52.91708683, 52.90834187, 52.89959691,\n",
       "       52.89085195, 52.88210699, 52.87336203, 52.86461707, 52.8558721 ,\n",
       "       52.84712714, 52.83838218, 52.82963722, 52.82089226, 52.8121473 ,\n",
       "       52.80340234, 52.79465738, 52.78591241, 52.77716745, 52.76842249,\n",
       "       52.75967753, 52.75093257, 52.74218761, 52.73344265, 52.72469768,\n",
       "       52.71595272, 52.70720776, 52.6984628 , 52.68971784, 52.68097288,\n",
       "       52.67222792, 52.66348295, 52.65473799, 52.64599303, 52.63724807,\n",
       "       52.62850311, 52.61975815, 52.61101319, 52.60226822, 52.59352326,\n",
       "       52.5847783 , 52.57603334, 52.56728838, 52.55854342, 52.54979846,\n",
       "       52.54105349, 52.53230853, 52.52356357, 52.51481861, 52.50607365,\n",
       "       52.49732869, 52.48858373, 52.47983876, 52.4710938 , 52.46234884,\n",
       "       52.45360388, 52.44485892, 52.43611396, 52.427369  , 52.41862403,\n",
       "       52.40987907, 52.40113411, 52.39238915, 52.38364419, 52.37489923,\n",
       "       52.36615427, 52.35740931, 52.34866434, 52.33991938, 52.33117442,\n",
       "       52.32242946, 52.3136845 , 52.30493954, 52.29619458, 52.28744961,\n",
       "       52.27870465, 52.26995969, 52.26121473, 52.25246977, 52.24372481,\n",
       "       52.23497985, 52.22623488, 52.21748992, 52.20874496, 52.2       ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.64810520635306\n",
      "32.20147122957222\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
