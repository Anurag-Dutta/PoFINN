{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.200000\n",
       "1       88.931092\n",
       "2       88.662185\n",
       "3       88.393277\n",
       "4       88.124370\n",
       "          ...    \n",
       "1595    61.268175\n",
       "1596    61.265000\n",
       "1597    61.261825\n",
       "1598    61.256429\n",
       "1599    61.248025\n",
       "Name: C1, Length: 1600, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c1_interpolated_1500_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.200000\n",
       "1       88.931092\n",
       "2       88.662185\n",
       "3       88.393277\n",
       "4       88.124370\n",
       "          ...    \n",
       "1495    61.585635\n",
       "1496    61.582460\n",
       "1497    61.579286\n",
       "1498    61.576111\n",
       "1499    61.572937\n",
       "Name: C1, Length: 1500, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1500)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.931092</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.662185</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.393277</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.124370</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     89.200000  0.000298\n",
       "1     88.931092  0.000298\n",
       "2     88.662185  0.000297\n",
       "3     88.393277  0.000297\n",
       "4     88.124370  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6465  # Value for C0\n",
    "K0 = -0.0029  # Value for K0\n",
    "K1 = -0.0003  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0168    # Value for b\n",
    "c = 2.3581    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    89.200000    88.931092    88.662185    88.393277    88.124370   \n",
      "351    88.931092    88.662185    88.393277    88.124370    87.855462   \n",
      "352    88.662185    88.393277    88.124370    87.855462    87.586555   \n",
      "353    88.393277    88.124370    87.855462    87.586555    87.317647   \n",
      "354    88.124370    87.855462    87.586555    87.317647    87.048739   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.855462    87.586555    87.317647    87.048739    86.794538  ...   \n",
      "351    87.586555    87.317647    87.048739    86.794538    86.721709  ...   \n",
      "352    87.317647    87.048739    86.794538    86.721709    86.648880  ...   \n",
      "353    87.048739    86.794538    86.721709    86.648880    86.576050  ...   \n",
      "354    86.794538    86.721709    86.648880    86.576050    86.503221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   73.989683    0.000263   73.957937    0.000263   73.926190    0.000263   \n",
      "351   73.957937    0.000263   73.926190    0.000263   73.894444    0.000262   \n",
      "352   73.926190    0.000263   73.894444    0.000262   73.862698    0.000262   \n",
      "353   73.894444    0.000262   73.862698    0.000262   73.830952    0.000262   \n",
      "354   73.862698    0.000262   73.830952    0.000262   73.799206    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   73.894444    0.000262   73.862698    0.000262  \n",
      "351   73.862698    0.000262   73.830952    0.000262  \n",
      "352   73.830952    0.000262   73.799206    0.000262  \n",
      "353   73.799206    0.000262   73.767460    0.000262  \n",
      "354   73.767460    0.000262   73.735714    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050, 1, 251) (1050, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "14/14 [==============================] - 3s 40ms/step - loss: 4776.4727 - val_loss: 3833.7598\n",
      "Epoch 2/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 4732.3491 - val_loss: 3783.8650\n",
      "Epoch 3/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4676.4351 - val_loss: 3748.2102\n",
      "Epoch 4/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4636.6904 - val_loss: 3712.5291\n",
      "Epoch 5/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4597.1187 - val_loss: 3677.1411\n",
      "Epoch 6/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4557.8667 - val_loss: 3642.0576\n",
      "Epoch 7/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 4518.9277 - val_loss: 3607.2625\n",
      "Epoch 8/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4480.2832 - val_loss: 3572.7395\n",
      "Epoch 9/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 4441.9175 - val_loss: 3538.4778\n",
      "Epoch 10/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 4403.8198 - val_loss: 3504.4702\n",
      "Epoch 11/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 4365.9849 - val_loss: 3470.7109\n",
      "Epoch 12/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 4328.4058 - val_loss: 3437.1968\n",
      "Epoch 13/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4291.0796 - val_loss: 3403.9231\n",
      "Epoch 14/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 4254.0029 - val_loss: 3370.8882\n",
      "Epoch 15/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4217.1719 - val_loss: 3338.0894\n",
      "Epoch 16/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4180.5850 - val_loss: 3305.5244\n",
      "Epoch 17/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4144.2402 - val_loss: 3273.1914\n",
      "Epoch 18/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4108.1357 - val_loss: 3241.0889\n",
      "Epoch 19/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 4072.2688 - val_loss: 3209.2146\n",
      "Epoch 20/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4036.6387 - val_loss: 3177.5676\n",
      "Epoch 21/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4001.2434 - val_loss: 3146.1467\n",
      "Epoch 22/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3966.0820 - val_loss: 3114.9492\n",
      "Epoch 23/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3931.1514 - val_loss: 3083.9751\n",
      "Epoch 24/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3896.4529 - val_loss: 3053.2219\n",
      "Epoch 25/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3861.9827 - val_loss: 3022.6899\n",
      "Epoch 26/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3827.7410 - val_loss: 2992.3760\n",
      "Epoch 27/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3793.7253 - val_loss: 2962.2803\n",
      "Epoch 28/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3759.9348 - val_loss: 2932.4009\n",
      "Epoch 29/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3726.3694 - val_loss: 2902.7366\n",
      "Epoch 30/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3692.9636 - val_loss: 2872.2361\n",
      "Epoch 31/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3656.8535 - val_loss: 2838.8577\n",
      "Epoch 32/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3619.2078 - val_loss: 2805.8547\n",
      "Epoch 33/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3582.3230 - val_loss: 2773.6726\n",
      "Epoch 34/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3546.2812 - val_loss: 2742.1699\n",
      "Epoch 35/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 3510.8779 - val_loss: 2706.8872\n",
      "Epoch 36/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3469.3977 - val_loss: 2672.2866\n",
      "Epoch 37/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3430.2212 - val_loss: 2638.1201\n",
      "Epoch 38/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3391.9746 - val_loss: 2604.9602\n",
      "Epoch 39/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3354.7693 - val_loss: 2572.6384\n",
      "Epoch 40/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3318.4075 - val_loss: 2540.9971\n",
      "Epoch 41/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3282.7368 - val_loss: 2509.9270\n",
      "Epoch 42/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3247.6531 - val_loss: 2479.3564\n",
      "Epoch 43/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3213.0845 - val_loss: 2449.2341\n",
      "Epoch 44/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3178.9822 - val_loss: 2419.5220\n",
      "Epoch 45/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3145.3074 - val_loss: 2390.1912\n",
      "Epoch 46/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3112.0315 - val_loss: 2361.2197\n",
      "Epoch 47/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3079.1321 - val_loss: 2332.5901\n",
      "Epoch 48/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3046.5894 - val_loss: 2304.2856\n",
      "Epoch 49/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 3014.3882 - val_loss: 2276.2949\n",
      "Epoch 50/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2982.5154 - val_loss: 2248.6072\n",
      "Epoch 51/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2950.9604 - val_loss: 2221.2124\n",
      "Epoch 52/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2919.7131 - val_loss: 2194.1033\n",
      "Epoch 53/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2888.7644 - val_loss: 2167.2717\n",
      "Epoch 54/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2851.2434 - val_loss: 2125.7280\n",
      "Epoch 55/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2808.3660 - val_loss: 2094.9822\n",
      "Epoch 56/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 2773.3923 - val_loss: 2065.1226\n",
      "Epoch 57/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2739.4717 - val_loss: 2036.1639\n",
      "Epoch 58/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2706.4692 - val_loss: 2007.9288\n",
      "Epoch 59/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2674.2029 - val_loss: 1980.2848\n",
      "Epoch 60/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2642.5447 - val_loss: 1953.1461\n",
      "Epoch 61/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2611.4114 - val_loss: 1926.4558\n",
      "Epoch 62/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2580.7449 - val_loss: 1900.1703\n",
      "Epoch 63/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2550.5049 - val_loss: 1874.2617\n",
      "Epoch 64/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2520.6611 - val_loss: 1848.7063\n",
      "Epoch 65/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2491.1887 - val_loss: 1823.4855\n",
      "Epoch 66/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2462.0693 - val_loss: 1798.5836\n",
      "Epoch 67/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2433.2864 - val_loss: 1773.9890\n",
      "Epoch 68/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2404.8281 - val_loss: 1749.6904\n",
      "Epoch 69/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2376.6824 - val_loss: 1725.6793\n",
      "Epoch 70/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2348.8398 - val_loss: 1701.9475\n",
      "Epoch 71/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2321.2920 - val_loss: 1678.4872\n",
      "Epoch 72/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2294.0315 - val_loss: 1655.2935\n",
      "Epoch 73/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2267.0515 - val_loss: 1632.3596\n",
      "Epoch 74/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2240.3457 - val_loss: 1609.6801\n",
      "Epoch 75/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2213.9087 - val_loss: 1587.2511\n",
      "Epoch 76/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 2187.7354 - val_loss: 1565.0675\n",
      "Epoch 77/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2161.8218 - val_loss: 1543.1265\n",
      "Epoch 78/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2136.1631 - val_loss: 1521.4232\n",
      "Epoch 79/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2110.7551 - val_loss: 1499.9537\n",
      "Epoch 80/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2085.5947 - val_loss: 1478.7158\n",
      "Epoch 81/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2060.6777 - val_loss: 1457.7058\n",
      "Epoch 82/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2036.0013 - val_loss: 1436.9210\n",
      "Epoch 83/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2011.5620 - val_loss: 1416.3585\n",
      "Epoch 84/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1987.3578 - val_loss: 1396.0164\n",
      "Epoch 85/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1963.3848 - val_loss: 1375.8901\n",
      "Epoch 86/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1939.6407 - val_loss: 1355.9791\n",
      "Epoch 87/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1916.1232 - val_loss: 1336.2802\n",
      "Epoch 88/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1892.8296 - val_loss: 1316.7919\n",
      "Epoch 89/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1869.7573 - val_loss: 1297.5107\n",
      "Epoch 90/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1846.9038 - val_loss: 1278.4351\n",
      "Epoch 91/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1824.2678 - val_loss: 1259.5638\n",
      "Epoch 92/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1801.8466 - val_loss: 1240.8939\n",
      "Epoch 93/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1779.6385 - val_loss: 1222.4248\n",
      "Epoch 94/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1757.1302 - val_loss: 1198.0616\n",
      "Epoch 95/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1726.7640 - val_loss: 1176.1948\n",
      "Epoch 96/500\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 1700.5917 - val_loss: 1154.7510\n",
      "Epoch 97/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1675.2290 - val_loss: 1134.1199\n",
      "Epoch 98/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1650.7396 - val_loss: 1114.1550\n",
      "Epoch 99/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1626.9475 - val_loss: 1094.7271\n",
      "Epoch 100/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1603.7203 - val_loss: 1075.7494\n",
      "Epoch 101/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1580.9716 - val_loss: 1057.1635\n",
      "Epoch 102/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1558.6425 - val_loss: 1038.9305\n",
      "Epoch 103/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1536.6921 - val_loss: 1021.0211\n",
      "Epoch 104/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1515.0891 - val_loss: 1003.4140\n",
      "Epoch 105/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1493.8120 - val_loss: 986.0916\n",
      "Epoch 106/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1472.8413 - val_loss: 969.0403\n",
      "Epoch 107/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1452.1626 - val_loss: 952.2486\n",
      "Epoch 108/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1431.7640 - val_loss: 935.7078\n",
      "Epoch 109/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1411.6348 - val_loss: 919.4089\n",
      "Epoch 110/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1391.7661 - val_loss: 903.3444\n",
      "Epoch 111/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1372.1500 - val_loss: 887.5084\n",
      "Epoch 112/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1352.7799 - val_loss: 871.8953\n",
      "Epoch 113/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1333.6488 - val_loss: 856.4998\n",
      "Epoch 114/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1314.7526 - val_loss: 841.3176\n",
      "Epoch 115/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1296.0850 - val_loss: 826.3448\n",
      "Epoch 116/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1277.6427 - val_loss: 811.5767\n",
      "Epoch 117/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1259.4204 - val_loss: 797.0107\n",
      "Epoch 118/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 1241.4150 - val_loss: 782.6423\n",
      "Epoch 119/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1223.6228 - val_loss: 768.4695\n",
      "Epoch 120/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1206.0399 - val_loss: 754.4885\n",
      "Epoch 121/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1188.6637 - val_loss: 740.6966\n",
      "Epoch 122/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1171.4906 - val_loss: 727.0917\n",
      "Epoch 123/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1154.5181 - val_loss: 713.6702\n",
      "Epoch 124/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1137.7437 - val_loss: 700.4315\n",
      "Epoch 125/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1121.1646 - val_loss: 687.3713\n",
      "Epoch 126/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1104.7784 - val_loss: 674.4885\n",
      "Epoch 127/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1088.5826 - val_loss: 661.7809\n",
      "Epoch 128/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1072.5752 - val_loss: 649.2463\n",
      "Epoch 129/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1056.7538 - val_loss: 636.8824\n",
      "Epoch 130/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1041.1167 - val_loss: 624.6875\n",
      "Epoch 131/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1025.6611 - val_loss: 612.6598\n",
      "Epoch 132/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1010.3859 - val_loss: 600.7980\n",
      "Epoch 133/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 995.2888 - val_loss: 589.0994\n",
      "Epoch 134/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 980.3679 - val_loss: 577.5624\n",
      "Epoch 135/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 965.6219 - val_loss: 566.1863\n",
      "Epoch 136/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 951.0483 - val_loss: 554.9684\n",
      "Epoch 137/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 936.6459 - val_loss: 543.9073\n",
      "Epoch 138/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 922.4125 - val_loss: 533.0015\n",
      "Epoch 139/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 908.3471 - val_loss: 522.2495\n",
      "Epoch 140/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 894.4476 - val_loss: 511.6500\n",
      "Epoch 141/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 880.7130 - val_loss: 501.2007\n",
      "Epoch 142/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 867.1412 - val_loss: 490.9012\n",
      "Epoch 143/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 853.7307 - val_loss: 480.7495\n",
      "Epoch 144/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 840.4807 - val_loss: 470.7441\n",
      "Epoch 145/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 827.3889 - val_loss: 460.8835\n",
      "Epoch 146/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 814.4542 - val_loss: 451.1670\n",
      "Epoch 147/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 801.6757 - val_loss: 441.5927\n",
      "Epoch 148/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 789.0514 - val_loss: 432.1594\n",
      "Epoch 149/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 776.5801 - val_loss: 422.8656\n",
      "Epoch 150/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 764.2607 - val_loss: 413.7104\n",
      "Epoch 151/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 752.0914 - val_loss: 404.6916\n",
      "Epoch 152/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 740.0711 - val_loss: 395.8091\n",
      "Epoch 153/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 728.1989 - val_loss: 387.0612\n",
      "Epoch 154/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 716.4727 - val_loss: 378.4459\n",
      "Epoch 155/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 704.8917 - val_loss: 369.9633\n",
      "Epoch 156/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 693.4548 - val_loss: 361.6107\n",
      "Epoch 157/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 682.1606 - val_loss: 353.3882\n",
      "Epoch 158/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 671.0079 - val_loss: 345.2938\n",
      "Epoch 159/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 659.9952 - val_loss: 337.3263\n",
      "Epoch 160/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 649.1219 - val_loss: 329.4850\n",
      "Epoch 161/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 638.3862 - val_loss: 321.7687\n",
      "Epoch 162/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 627.7874 - val_loss: 314.1753\n",
      "Epoch 163/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 617.3238 - val_loss: 306.7050\n",
      "Epoch 164/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 606.9950 - val_loss: 299.3557\n",
      "Epoch 165/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 596.7992 - val_loss: 292.1262\n",
      "Epoch 166/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 586.7354 - val_loss: 285.0162\n",
      "Epoch 167/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 576.8026 - val_loss: 278.0237\n",
      "Epoch 168/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 566.9997 - val_loss: 271.1480\n",
      "Epoch 169/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 557.3254 - val_loss: 264.3878\n",
      "Epoch 170/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 547.7787 - val_loss: 257.7425\n",
      "Epoch 171/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 538.3586 - val_loss: 251.2104\n",
      "Epoch 172/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 529.0638 - val_loss: 244.7905\n",
      "Epoch 173/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 519.8932 - val_loss: 238.4820\n",
      "Epoch 174/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 510.8461 - val_loss: 232.2836\n",
      "Epoch 175/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 501.9211 - val_loss: 226.1944\n",
      "Epoch 176/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 493.1170 - val_loss: 220.2129\n",
      "Epoch 177/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 484.4331 - val_loss: 214.3384\n",
      "Epoch 178/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 475.8682 - val_loss: 208.5694\n",
      "Epoch 179/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 467.4211 - val_loss: 202.9056\n",
      "Epoch 180/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 459.0909 - val_loss: 197.3454\n",
      "Epoch 181/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 450.8765 - val_loss: 191.8879\n",
      "Epoch 182/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 442.7774 - val_loss: 186.5321\n",
      "Epoch 183/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 434.7917 - val_loss: 181.2769\n",
      "Epoch 184/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 426.9189 - val_loss: 176.1210\n",
      "Epoch 185/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 419.1578 - val_loss: 171.0638\n",
      "Epoch 186/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 411.5072 - val_loss: 166.1041\n",
      "Epoch 187/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 403.9664 - val_loss: 161.2410\n",
      "Epoch 188/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 396.5345 - val_loss: 156.4735\n",
      "Epoch 189/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 389.2104 - val_loss: 151.8000\n",
      "Epoch 190/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 381.9929 - val_loss: 147.2202\n",
      "Epoch 191/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 374.8811 - val_loss: 142.7329\n",
      "Epoch 192/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 367.8745 - val_loss: 138.3372\n",
      "Epoch 193/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 360.9715 - val_loss: 134.0316\n",
      "Epoch 194/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 354.1714 - val_loss: 129.8156\n",
      "Epoch 195/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 347.4733 - val_loss: 125.6883\n",
      "Epoch 196/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 340.8759 - val_loss: 121.6481\n",
      "Epoch 197/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 334.3784 - val_loss: 117.6945\n",
      "Epoch 198/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 327.9800 - val_loss: 113.8266\n",
      "Epoch 199/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 321.6794 - val_loss: 110.0424\n",
      "Epoch 200/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 315.4756 - val_loss: 106.3424\n",
      "Epoch 201/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 309.3682 - val_loss: 102.7248\n",
      "Epoch 202/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 303.3559 - val_loss: 99.1888\n",
      "Epoch 203/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 297.4380 - val_loss: 95.7335\n",
      "Epoch 204/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 291.6130 - val_loss: 92.3577\n",
      "Epoch 205/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 285.8806 - val_loss: 89.0607\n",
      "Epoch 206/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 280.2395 - val_loss: 85.8414\n",
      "Epoch 207/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 274.6889 - val_loss: 82.6990\n",
      "Epoch 208/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 269.2278 - val_loss: 79.6323\n",
      "Epoch 209/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 263.8553 - val_loss: 76.6403\n",
      "Epoch 210/500\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 258.5703 - val_loss: 73.7225\n",
      "Epoch 211/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 253.3721 - val_loss: 70.8776\n",
      "Epoch 212/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 248.2597 - val_loss: 68.1046\n",
      "Epoch 213/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 243.2322 - val_loss: 65.4030\n",
      "Epoch 214/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 238.2889 - val_loss: 62.7714\n",
      "Epoch 215/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 233.4286 - val_loss: 60.2090\n",
      "Epoch 216/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 228.6506 - val_loss: 57.7150\n",
      "Epoch 217/500\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 223.9538 - val_loss: 55.2885\n",
      "Epoch 218/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 219.3375 - val_loss: 52.9284\n",
      "Epoch 219/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 214.8006 - val_loss: 50.6340\n",
      "Epoch 220/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 210.3424 - val_loss: 48.4039\n",
      "Epoch 221/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 205.9619 - val_loss: 46.2379\n",
      "Epoch 222/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 201.6582 - val_loss: 44.1346\n",
      "Epoch 223/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 197.4306 - val_loss: 42.0933\n",
      "Epoch 224/500\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 193.2783 - val_loss: 40.1131\n",
      "Epoch 225/500\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 189.1999 - val_loss: 38.1930\n",
      "Epoch 226/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 185.1951 - val_loss: 36.3321\n",
      "Epoch 227/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 181.2628 - val_loss: 34.5295\n",
      "Epoch 228/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 177.4019 - val_loss: 32.7846\n",
      "Epoch 229/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 173.6121 - val_loss: 31.0963\n",
      "Epoch 230/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 169.8924 - val_loss: 29.4637\n",
      "Epoch 231/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 166.2416 - val_loss: 27.8859\n",
      "Epoch 232/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 162.6589 - val_loss: 26.3622\n",
      "Epoch 233/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 159.1438 - val_loss: 24.8914\n",
      "Epoch 234/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 155.6952 - val_loss: 23.4732\n",
      "Epoch 235/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 152.3123 - val_loss: 22.1061\n",
      "Epoch 236/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 148.9941 - val_loss: 20.7895\n",
      "Epoch 237/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 145.7398 - val_loss: 19.5226\n",
      "Epoch 238/500\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 142.5490 - val_loss: 18.3046\n",
      "Epoch 239/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 139.4204 - val_loss: 17.1347\n",
      "Epoch 240/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 136.3535 - val_loss: 16.0120\n",
      "Epoch 241/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 133.3475 - val_loss: 14.9356\n",
      "Epoch 242/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 130.4014 - val_loss: 13.9045\n",
      "Epoch 243/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 127.5142 - val_loss: 12.9182\n",
      "Epoch 244/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 124.6854 - val_loss: 11.9758\n",
      "Epoch 245/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 121.9141 - val_loss: 11.0763\n",
      "Epoch 246/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 119.1995 - val_loss: 10.2192\n",
      "Epoch 247/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 116.5409 - val_loss: 9.4034\n",
      "Epoch 248/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 113.9374 - val_loss: 8.6282\n",
      "Epoch 249/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 111.3885 - val_loss: 7.8929\n",
      "Epoch 250/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 108.8930 - val_loss: 7.1966\n",
      "Epoch 251/500\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 106.4503 - val_loss: 6.5386\n",
      "Epoch 252/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 104.0597 - val_loss: 5.9180\n",
      "Epoch 253/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 101.7203 - val_loss: 5.3340\n",
      "Epoch 254/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 99.4314 - val_loss: 4.7861\n",
      "Epoch 255/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 97.1923 - val_loss: 4.2732\n",
      "Epoch 256/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 95.0021 - val_loss: 3.7947\n",
      "Epoch 257/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 92.8601 - val_loss: 3.3497\n",
      "Epoch 258/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 90.7655 - val_loss: 2.9377\n",
      "Epoch 259/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 88.7178 - val_loss: 2.5579\n",
      "Epoch 260/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 86.7161 - val_loss: 2.2094\n",
      "Epoch 261/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 84.7597 - val_loss: 1.8915\n",
      "Epoch 262/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 82.8478 - val_loss: 1.6036\n",
      "Epoch 263/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 80.9798 - val_loss: 1.3450\n",
      "Epoch 264/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 79.1549 - val_loss: 1.1148\n",
      "Epoch 265/500\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 77.3725 - val_loss: 0.9124\n",
      "Epoch 266/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 75.6317 - val_loss: 0.7371\n",
      "Epoch 267/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 73.9319 - val_loss: 0.5881\n",
      "Epoch 268/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 72.2725 - val_loss: 0.4649\n",
      "Epoch 269/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 70.6527 - val_loss: 0.3667\n",
      "Epoch 270/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 69.0718 - val_loss: 0.2928\n",
      "Epoch 271/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 67.5293 - val_loss: 0.2426\n",
      "Epoch 272/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 66.0244 - val_loss: 0.2154\n",
      "Epoch 273/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 64.5564 - val_loss: 0.2106\n",
      "Epoch 274/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 63.1245 - val_loss: 0.2274\n",
      "Epoch 275/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 61.7284 - val_loss: 0.2654\n",
      "Epoch 276/500\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 60.3673 - val_loss: 0.3237\n",
      "Epoch 277/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 59.0405 - val_loss: 0.4018\n",
      "Epoch 278/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 57.7473 - val_loss: 0.4991\n",
      "Epoch 279/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 56.4873 - val_loss: 0.6149\n",
      "Epoch 280/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 55.2596 - val_loss: 0.7487\n",
      "Epoch 281/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 54.0637 - val_loss: 0.8998\n",
      "Epoch 282/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 52.8991 - val_loss: 1.0677\n",
      "Epoch 283/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 51.7649 - val_loss: 1.2518\n",
      "Epoch 284/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 50.6609 - val_loss: 1.4514\n",
      "Epoch 285/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 49.5862 - val_loss: 1.6660\n",
      "Epoch 286/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 48.5403 - val_loss: 1.8952\n",
      "Epoch 287/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 47.5226 - val_loss: 2.1383\n",
      "Epoch 288/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 46.5325 - val_loss: 2.3946\n",
      "Epoch 289/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 45.5697 - val_loss: 2.6638\n",
      "Epoch 290/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 44.6332 - val_loss: 2.9453\n",
      "Epoch 291/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 43.7227 - val_loss: 3.2386\n",
      "Epoch 292/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 42.8375 - val_loss: 3.5431\n",
      "Epoch 293/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 41.9773 - val_loss: 3.8584\n",
      "Epoch 294/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 41.1414 - val_loss: 4.1839\n",
      "Epoch 295/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 40.3292 - val_loss: 4.5191\n",
      "Epoch 296/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 39.5405 - val_loss: 4.8637\n",
      "Epoch 297/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 38.7744 - val_loss: 5.2170\n",
      "Epoch 298/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 38.0306 - val_loss: 5.5786\n",
      "Epoch 299/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 37.3085 - val_loss: 5.9482\n",
      "Epoch 300/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 36.6077 - val_loss: 6.3252\n",
      "Epoch 301/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 35.9275 - val_loss: 6.7091\n",
      "Epoch 302/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 35.2677 - val_loss: 7.0996\n",
      "Epoch 303/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 34.6277 - val_loss: 7.4963\n",
      "Epoch 304/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 34.0069 - val_loss: 7.8987\n",
      "Epoch 305/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 33.4051 - val_loss: 8.3063\n",
      "Epoch 306/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 32.8217 - val_loss: 8.7190\n",
      "Epoch 307/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 32.2562 - val_loss: 9.1360\n",
      "Epoch 308/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 31.7082 - val_loss: 9.5573\n",
      "Epoch 309/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 31.1774 - val_loss: 9.9823\n",
      "Epoch 310/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 30.6631 - val_loss: 10.4108\n",
      "Epoch 311/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 30.1652 - val_loss: 10.8421\n",
      "Epoch 312/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 29.6831 - val_loss: 11.2763\n",
      "Epoch 313/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 29.2164 - val_loss: 11.7129\n",
      "Epoch 314/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 28.7646 - val_loss: 12.1513\n",
      "Epoch 315/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 28.3275 - val_loss: 12.5916\n",
      "Epoch 316/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 27.9047 - val_loss: 13.0333\n",
      "Epoch 317/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 27.4957 - val_loss: 13.4760\n",
      "Epoch 318/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 27.1003 - val_loss: 13.9193\n",
      "Epoch 319/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 26.7180 - val_loss: 14.3633\n",
      "Epoch 320/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 26.3484 - val_loss: 14.8075\n",
      "Epoch 321/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 25.9912 - val_loss: 15.2516\n",
      "Epoch 322/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 25.6460 - val_loss: 15.6956\n",
      "Epoch 323/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 25.3127 - val_loss: 16.1388\n",
      "Epoch 324/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 24.9906 - val_loss: 16.5814\n",
      "Epoch 325/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 24.6796 - val_loss: 17.0225\n",
      "Epoch 326/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 24.3796 - val_loss: 17.4624\n",
      "Epoch 327/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 24.0900 - val_loss: 17.9010\n",
      "Epoch 328/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 23.8104 - val_loss: 18.3377\n",
      "Epoch 329/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 23.5406 - val_loss: 18.7722\n",
      "Epoch 330/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 23.2805 - val_loss: 19.2049\n",
      "Epoch 331/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 23.0296 - val_loss: 19.6351\n",
      "Epoch 332/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.7877 - val_loss: 20.0627\n",
      "Epoch 333/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.5545 - val_loss: 20.4877\n",
      "Epoch 334/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.3297 - val_loss: 20.9096\n",
      "Epoch 335/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 22.1130 - val_loss: 21.3288\n",
      "Epoch 336/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.9043 - val_loss: 21.7445\n",
      "Epoch 337/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.7033 - val_loss: 22.1569\n",
      "Epoch 338/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 21.5097 - val_loss: 22.5657\n",
      "Epoch 339/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.3233 - val_loss: 22.9708\n",
      "Epoch 340/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.1439 - val_loss: 23.3723\n",
      "Epoch 341/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 20.9712 - val_loss: 23.7698\n",
      "Epoch 342/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 20.8049 - val_loss: 24.1634\n",
      "Epoch 343/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 20.6451 - val_loss: 24.5527\n",
      "Epoch 344/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 20.4913 - val_loss: 24.9377\n",
      "Epoch 345/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 20.3433 - val_loss: 25.3184\n",
      "Epoch 346/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 20.2011 - val_loss: 25.6947\n",
      "Epoch 347/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 20.0644 - val_loss: 26.0666\n",
      "Epoch 348/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 19.9329 - val_loss: 26.4339\n",
      "Epoch 349/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 19.8066 - val_loss: 26.7964\n",
      "Epoch 350/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 19.6852 - val_loss: 27.1543\n",
      "Epoch 351/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 19.5686 - val_loss: 27.5073\n",
      "Epoch 352/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 19.4566 - val_loss: 27.8554\n",
      "Epoch 353/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 19.3490 - val_loss: 28.1985\n",
      "Epoch 354/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 19.2457 - val_loss: 28.5369\n",
      "Epoch 355/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 19.1465 - val_loss: 28.8702\n",
      "Epoch 356/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 19.0514 - val_loss: 29.1986\n",
      "Epoch 357/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 18.9600 - val_loss: 29.5217\n",
      "Epoch 358/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 18.8723 - val_loss: 29.8398\n",
      "Epoch 359/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 18.7882 - val_loss: 30.1530\n",
      "Epoch 360/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 18.7075 - val_loss: 30.4607\n",
      "Epoch 361/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 18.6301 - val_loss: 30.7636\n",
      "Epoch 362/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 18.5559 - val_loss: 31.0611\n",
      "Epoch 363/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 18.4847 - val_loss: 31.3537\n",
      "Epoch 364/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 18.4165 - val_loss: 31.6409\n",
      "Epoch 365/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 18.3510 - val_loss: 31.9229\n",
      "Epoch 366/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 18.2883 - val_loss: 32.1999\n",
      "Epoch 367/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 18.2283 - val_loss: 32.4717\n",
      "Epoch 368/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 18.1707 - val_loss: 32.7383\n",
      "Epoch 369/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 18.1155 - val_loss: 33.0000\n",
      "Epoch 370/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 18.0626 - val_loss: 33.2565\n",
      "Epoch 371/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 18.0120 - val_loss: 33.5077\n",
      "Epoch 372/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.9636 - val_loss: 33.7542\n",
      "Epoch 373/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.9171 - val_loss: 33.9955\n",
      "Epoch 374/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.8726 - val_loss: 34.2320\n",
      "Epoch 375/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.8300 - val_loss: 34.4634\n",
      "Epoch 376/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.7892 - val_loss: 34.6899\n",
      "Epoch 377/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.7502 - val_loss: 34.9116\n",
      "Epoch 378/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.7128 - val_loss: 35.1283\n",
      "Epoch 379/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.6771 - val_loss: 35.3405\n",
      "Epoch 380/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.6428 - val_loss: 35.5477\n",
      "Epoch 381/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.6101 - val_loss: 35.7503\n",
      "Epoch 382/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.5787 - val_loss: 35.9483\n",
      "Epoch 383/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 17.5487 - val_loss: 36.1417\n",
      "Epoch 384/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.5200 - val_loss: 36.3304\n",
      "Epoch 385/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 17.4925 - val_loss: 36.5146\n",
      "Epoch 386/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 17.4663 - val_loss: 36.6949\n",
      "Epoch 387/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.4411 - val_loss: 36.8704\n",
      "Epoch 388/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 17.4170 - val_loss: 37.0415\n",
      "Epoch 389/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 17.3940 - val_loss: 37.2085\n",
      "Epoch 390/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 17.3720 - val_loss: 37.3710\n",
      "Epoch 391/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 17.3510 - val_loss: 37.5298\n",
      "Epoch 392/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.3308 - val_loss: 37.6841\n",
      "Epoch 393/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.3115 - val_loss: 37.8348\n",
      "Epoch 394/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.2931 - val_loss: 37.9812\n",
      "Epoch 395/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.2755 - val_loss: 38.1236\n",
      "Epoch 396/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.2587 - val_loss: 38.2623\n",
      "Epoch 397/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.2426 - val_loss: 38.3977\n",
      "Epoch 398/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.2271 - val_loss: 38.5290\n",
      "Epoch 399/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.2124 - val_loss: 38.6568\n",
      "Epoch 400/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 17.1983 - val_loss: 38.7811\n",
      "Epoch 401/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 17.1848 - val_loss: 38.9018\n",
      "Epoch 402/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.1719 - val_loss: 39.0191\n",
      "Epoch 403/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.1595 - val_loss: 39.1326\n",
      "Epoch 404/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 17.1478 - val_loss: 39.2435\n",
      "Epoch 405/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.1365 - val_loss: 39.3509\n",
      "Epoch 406/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 17.1257 - val_loss: 39.4550\n",
      "Epoch 407/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.1154 - val_loss: 39.5561\n",
      "Epoch 408/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.1056 - val_loss: 39.6544\n",
      "Epoch 409/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 17.0961 - val_loss: 39.7493\n",
      "Epoch 410/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 17.0871 - val_loss: 39.8415\n",
      "Epoch 411/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.0785 - val_loss: 39.9310\n",
      "Epoch 412/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.0702 - val_loss: 40.0172\n",
      "Epoch 413/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 17.0624 - val_loss: 40.1012\n",
      "Epoch 414/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 17.0549 - val_loss: 40.1826\n",
      "Epoch 415/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 17.0477 - val_loss: 40.2612\n",
      "Epoch 416/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.0408 - val_loss: 40.3375\n",
      "Epoch 417/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 17.0342 - val_loss: 40.4110\n",
      "Epoch 418/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 17.0279 - val_loss: 40.4822\n",
      "Epoch 419/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.0219 - val_loss: 40.5509\n",
      "Epoch 420/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 17.0161 - val_loss: 40.6178\n",
      "Epoch 421/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 17.0106 - val_loss: 40.6822\n",
      "Epoch 422/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 17.0054 - val_loss: 40.7443\n",
      "Epoch 423/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.0003 - val_loss: 40.8044\n",
      "Epoch 424/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9955 - val_loss: 40.8625\n",
      "Epoch 425/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9910 - val_loss: 40.9186\n",
      "Epoch 426/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9866 - val_loss: 40.9727\n",
      "Epoch 427/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 16.9824 - val_loss: 41.0249\n",
      "Epoch 428/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.9784 - val_loss: 41.0754\n",
      "Epoch 429/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 16.9746 - val_loss: 41.1237\n",
      "Epoch 430/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.9709 - val_loss: 41.1706\n",
      "Epoch 431/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9674 - val_loss: 41.2157\n",
      "Epoch 432/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9641 - val_loss: 41.2592\n",
      "Epoch 433/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9610 - val_loss: 41.3011\n",
      "Epoch 434/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9579 - val_loss: 41.3415\n",
      "Epoch 435/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.9550 - val_loss: 41.3802\n",
      "Epoch 436/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 16.9523 - val_loss: 41.4175\n",
      "Epoch 437/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9497 - val_loss: 41.4536\n",
      "Epoch 438/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9471 - val_loss: 41.4879\n",
      "Epoch 439/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.9448 - val_loss: 41.5214\n",
      "Epoch 440/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9424 - val_loss: 41.5531\n",
      "Epoch 441/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9403 - val_loss: 41.5839\n",
      "Epoch 442/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9382 - val_loss: 41.6135\n",
      "Epoch 443/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 16.9362 - val_loss: 41.6418\n",
      "Epoch 444/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9344 - val_loss: 41.6690\n",
      "Epoch 445/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9326 - val_loss: 41.6954\n",
      "Epoch 446/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9309 - val_loss: 41.7204\n",
      "Epoch 447/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9293 - val_loss: 41.7443\n",
      "Epoch 448/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9278 - val_loss: 41.7674\n",
      "Epoch 449/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9263 - val_loss: 41.7897\n",
      "Epoch 450/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9249 - val_loss: 41.8108\n",
      "Epoch 451/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9236 - val_loss: 41.8311\n",
      "Epoch 452/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9223 - val_loss: 41.8510\n",
      "Epoch 453/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9211 - val_loss: 41.8697\n",
      "Epoch 454/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9200 - val_loss: 41.8874\n",
      "Epoch 455/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9190 - val_loss: 41.9045\n",
      "Epoch 456/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9179 - val_loss: 41.9210\n",
      "Epoch 457/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9170 - val_loss: 41.9367\n",
      "Epoch 458/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.9160 - val_loss: 41.9517\n",
      "Epoch 459/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 16.9152 - val_loss: 41.9661\n",
      "Epoch 460/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9144 - val_loss: 41.9797\n",
      "Epoch 461/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9136 - val_loss: 41.9928\n",
      "Epoch 462/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9129 - val_loss: 42.0054\n",
      "Epoch 463/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9122 - val_loss: 42.0173\n",
      "Epoch 464/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9116 - val_loss: 42.0287\n",
      "Epoch 465/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9110 - val_loss: 42.0396\n",
      "Epoch 466/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.9104 - val_loss: 42.0499\n",
      "Epoch 467/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9099 - val_loss: 42.0598\n",
      "Epoch 468/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.9094 - val_loss: 42.0692\n",
      "Epoch 469/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9090 - val_loss: 42.0785\n",
      "Epoch 470/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9085 - val_loss: 42.0868\n",
      "Epoch 471/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9081 - val_loss: 42.0952\n",
      "Epoch 472/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 16.9077 - val_loss: 42.1027\n",
      "Epoch 473/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9074 - val_loss: 42.1101\n",
      "Epoch 474/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9070 - val_loss: 42.1172\n",
      "Epoch 475/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9067 - val_loss: 42.1238\n",
      "Epoch 476/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.9064 - val_loss: 42.1301\n",
      "Epoch 477/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9062 - val_loss: 42.1362\n",
      "Epoch 478/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 16.9060 - val_loss: 42.1420\n",
      "Epoch 479/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9058 - val_loss: 42.1473\n",
      "Epoch 480/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.9056 - val_loss: 42.1523\n",
      "Epoch 481/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9054 - val_loss: 42.1571\n",
      "Epoch 482/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.9052 - val_loss: 42.1617\n",
      "Epoch 483/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9051 - val_loss: 42.1663\n",
      "Epoch 484/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9050 - val_loss: 42.1705\n",
      "Epoch 485/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9049 - val_loss: 42.1742\n",
      "Epoch 486/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 16.9048 - val_loss: 42.1783\n",
      "Epoch 487/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 16.9047 - val_loss: 42.1815\n",
      "Epoch 488/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9046 - val_loss: 42.1849\n",
      "Epoch 489/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9046 - val_loss: 42.1878\n",
      "Epoch 490/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9046 - val_loss: 42.1909\n",
      "Epoch 491/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9045 - val_loss: 42.1937\n",
      "Epoch 492/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9045 - val_loss: 42.1961\n",
      "Epoch 493/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9045 - val_loss: 42.1988\n",
      "Epoch 494/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.9045 - val_loss: 42.2010\n",
      "Epoch 495/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9045 - val_loss: 42.2033\n",
      "Epoch 496/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9045 - val_loss: 42.2051\n",
      "Epoch 497/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9045 - val_loss: 42.2070\n",
      "Epoch 498/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9046 - val_loss: 42.2091\n",
      "Epoch 499/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9046 - val_loss: 42.2110\n",
      "Epoch 500/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 16.9047 - val_loss: 42.2127\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6465, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0029, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0003, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0168, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3581, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1e-10\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 383ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[64.0200747 , 63.9874183 , 63.96781046, 63.94820261, 63.92859477,\n",
       "        63.90898693, 63.88937908, 63.86977124, 63.8501634 , 63.83055556,\n",
       "        63.81094771, 63.79133987, 63.77173203, 63.75212418, 63.73251634,\n",
       "        63.7129085 , 63.69330065, 63.67369281, 63.65408497, 63.63447712,\n",
       "        63.61486928, 63.59526144, 63.57565359, 63.55604575, 63.53643791,\n",
       "        63.51683007, 63.49722222, 63.47761438, 63.45800654, 63.43839869,\n",
       "        63.41879085, 63.39918301, 63.37957516, 63.35996732, 63.34035948,\n",
       "        63.32075163, 63.30114379, 63.2762605 , 63.25105042, 63.22584034,\n",
       "        63.20063025, 63.17542017, 63.15021008, 63.125     , 63.09978992,\n",
       "        63.07457983, 63.04936975, 63.02415966, 62.99894958, 62.9737395 ,\n",
       "        62.94852941, 62.92331933, 62.89810924, 62.87289916, 62.84768908,\n",
       "        62.82247899, 62.79726891, 62.77205882, 62.74684874, 62.72163866,\n",
       "        62.69642857, 62.67121849, 62.6460084 , 62.62079832, 62.59558824,\n",
       "        62.57037815, 62.54516807, 62.51995798, 62.4947479 , 62.46953782,\n",
       "        62.44432773, 62.41911765, 62.39932306, 62.39652194, 62.39372082,\n",
       "        62.3909197 , 62.38811858, 62.38531746, 62.38251634, 62.37971522,\n",
       "        69.54266357,  0.        ,  0.59277862,  0.        ,  0.20421855,\n",
       "         0.13026485,  0.45828229,  0.61028612,  0.        ,  0.18071297,\n",
       "         0.28336877,  0.07421523,  0.80560917,  0.27068555,  0.        ,\n",
       "         0.        ,  0.        ,  0.93613142,  0.        ,  0.22790316]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([61.5697619 , 61.5665873 , 61.5634127 , 61.5602381 , 61.55706349,\n",
       "       61.55388889, 61.55071429, 61.54753968, 61.54436508, 61.54119048,\n",
       "       61.53801587, 61.53484127, 61.53166667, 61.52849206, 61.52531746,\n",
       "       61.52214286, 61.51896825, 61.51579365, 61.51261905, 61.50944444,\n",
       "       61.50626984, 61.50309524, 61.49992063, 61.49674603, 61.49357143,\n",
       "       61.49039683, 61.48722222, 61.48404762, 61.48087302, 61.47769841,\n",
       "       61.47452381, 61.47134921, 61.4681746 , 61.465     , 61.4618254 ,\n",
       "       61.45865079, 61.45547619, 61.45230159, 61.44912698, 61.44595238,\n",
       "       61.44277778, 61.43960317, 61.43642857, 61.43325397, 61.43007937,\n",
       "       61.42690476, 61.42373016, 61.42055556, 61.41738095, 61.41420635,\n",
       "       61.41103175, 61.40785714, 61.40468254, 61.40150794, 61.39833333,\n",
       "       61.39515873, 61.39198413, 61.38880952, 61.38563492, 61.38246032,\n",
       "       61.37928571, 61.37611111, 61.37293651, 61.3697619 , 61.3665873 ,\n",
       "       61.3634127 , 61.3602381 , 61.35706349, 61.35388889, 61.35071429,\n",
       "       61.34753968, 61.34436508, 61.34119048, 61.33801587, 61.33484127,\n",
       "       61.33166667, 61.32849206, 61.32531746, 61.32214286, 61.31896825,\n",
       "       61.31579365, 61.31261905, 61.30944444, 61.30626984, 61.30309524,\n",
       "       61.29992063, 61.29674603, 61.29357143, 61.29039683, 61.28722222,\n",
       "       61.28404762, 61.28087302, 61.27769841, 61.27452381, 61.27134921,\n",
       "       61.2681746 , 61.265     , 61.2618254 , 61.25642857, 61.24802521])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.664235516060003\n",
      "13.062899015594965\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
