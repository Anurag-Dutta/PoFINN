{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.200000\n",
       "1       88.931092\n",
       "2       88.662185\n",
       "3       88.393277\n",
       "4       88.124370\n",
       "          ...    \n",
       "1545    61.426905\n",
       "1546    61.423730\n",
       "1547    61.420556\n",
       "1548    61.417381\n",
       "1549    61.414206\n",
       "Name: C1, Length: 1550, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c1_interpolated_1450_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.200000\n",
       "1       88.931092\n",
       "2       88.662185\n",
       "3       88.393277\n",
       "4       88.124370\n",
       "          ...    \n",
       "1445     0.000000\n",
       "1446     0.471157\n",
       "1447     0.352841\n",
       "1448     0.346431\n",
       "1449     0.000000\n",
       "Name: C1, Length: 1450, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1450)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.931092</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.662185</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.393277</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.124370</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     89.200000  0.000298\n",
       "1     88.931092  0.000298\n",
       "2     88.662185  0.000297\n",
       "3     88.393277  0.000297\n",
       "4     88.124370  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6465  # Value for C0\n",
    "K0 = -0.0029  # Value for K0\n",
    "K1 = -0.0003  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0168    # Value for b\n",
    "c = 2.3581    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    89.200000    88.931092    88.662185    88.393277    88.124370   \n",
      "351    88.931092    88.662185    88.393277    88.124370    87.855462   \n",
      "352    88.662185    88.393277    88.124370    87.855462    87.586555   \n",
      "353    88.393277    88.124370    87.855462    87.586555    87.317647   \n",
      "354    88.124370    87.855462    87.586555    87.317647    87.048739   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.855462    87.586555    87.317647    87.048739    86.794538  ...   \n",
      "351    87.586555    87.317647    87.048739    86.794538    86.721709  ...   \n",
      "352    87.317647    87.048739    86.794538    86.721709    86.648880  ...   \n",
      "353    87.048739    86.794538    86.721709    86.648880    86.576050  ...   \n",
      "354    86.794538    86.721709    86.648880    86.576050    86.503221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   73.989683    0.000263   73.957937    0.000263   73.926190    0.000263   \n",
      "351   73.957937    0.000263   73.926190    0.000263   73.894444    0.000262   \n",
      "352   73.926190    0.000263   73.894444    0.000262   73.862698    0.000262   \n",
      "353   73.894444    0.000262   73.862698    0.000262   73.830952    0.000262   \n",
      "354   73.862698    0.000262   73.830952    0.000262   73.799206    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   73.894444    0.000262   73.862698    0.000262  \n",
      "351   73.862698    0.000262   73.830952    0.000262  \n",
      "352   73.830952    0.000262   73.799206    0.000262  \n",
      "353   73.799206    0.000262   73.767460    0.000262  \n",
      "354   73.767460    0.000262   73.735714    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1, 251) (1000, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "13/13 [==============================] - 3s 46ms/step - loss: 4730.9717 - val_loss: 3794.1687\n",
      "Epoch 2/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 4675.8960 - val_loss: 3753.1230\n",
      "Epoch 3/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4632.8555 - val_loss: 3716.3245\n",
      "Epoch 4/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4580.4185 - val_loss: 3663.0635\n",
      "Epoch 5/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4532.1104 - val_loss: 3625.0774\n",
      "Epoch 6/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4489.9937 - val_loss: 3573.0393\n",
      "Epoch 7/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4431.4756 - val_loss: 3534.0510\n",
      "Epoch 8/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4387.9761 - val_loss: 3495.2102\n",
      "Epoch 9/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4344.9282 - val_loss: 3456.9333\n",
      "Epoch 10/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 4302.4731 - val_loss: 3419.1787\n",
      "Epoch 11/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4260.5454 - val_loss: 3381.8831\n",
      "Epoch 12/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4219.0811 - val_loss: 3344.9980\n",
      "Epoch 13/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4178.0366 - val_loss: 3308.4915\n",
      "Epoch 14/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4132.5254 - val_loss: 3266.1130\n",
      "Epoch 15/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4088.7236 - val_loss: 3227.4502\n",
      "Epoch 16/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4045.7458 - val_loss: 3175.8291\n",
      "Epoch 17/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3987.0615 - val_loss: 3135.8040\n",
      "Epoch 18/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3942.1582 - val_loss: 3096.0974\n",
      "Epoch 19/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3898.0132 - val_loss: 3057.2634\n",
      "Epoch 20/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3854.7783 - val_loss: 3019.1997\n",
      "Epoch 21/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 3812.3203 - val_loss: 2981.7878\n",
      "Epoch 22/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3770.5220 - val_loss: 2944.9431\n",
      "Epoch 23/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3729.3013 - val_loss: 2908.6057\n",
      "Epoch 24/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3688.6025 - val_loss: 2872.7324\n",
      "Epoch 25/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3648.3838 - val_loss: 2837.2944\n",
      "Epoch 26/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3608.6143 - val_loss: 2802.2668\n",
      "Epoch 27/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3569.2712 - val_loss: 2767.6306\n",
      "Epoch 28/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3530.3340 - val_loss: 2733.3708\n",
      "Epoch 29/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 3491.7891 - val_loss: 2699.4758\n",
      "Epoch 30/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3453.6230 - val_loss: 2665.9338\n",
      "Epoch 31/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3415.8245 - val_loss: 2632.7363\n",
      "Epoch 32/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3378.3840 - val_loss: 2599.8755\n",
      "Epoch 33/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3341.2944 - val_loss: 2567.3438\n",
      "Epoch 34/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3304.5471 - val_loss: 2535.1353\n",
      "Epoch 35/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3268.1370 - val_loss: 2503.2441\n",
      "Epoch 36/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3232.0579 - val_loss: 2471.6660\n",
      "Epoch 37/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3196.3035 - val_loss: 2440.3955\n",
      "Epoch 38/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3160.8704 - val_loss: 2409.4292\n",
      "Epoch 39/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 3125.7537 - val_loss: 2378.7625\n",
      "Epoch 40/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 3090.9490 - val_loss: 2348.3914\n",
      "Epoch 41/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3056.4524 - val_loss: 2318.3127\n",
      "Epoch 42/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3022.2607 - val_loss: 2288.5232\n",
      "Epoch 43/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2988.3701 - val_loss: 2259.0198\n",
      "Epoch 44/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2954.7778 - val_loss: 2229.7993\n",
      "Epoch 45/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2921.4807 - val_loss: 2200.8591\n",
      "Epoch 46/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2888.4753 - val_loss: 2172.1973\n",
      "Epoch 47/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2855.7593 - val_loss: 2143.8096\n",
      "Epoch 48/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2823.3301 - val_loss: 2115.6943\n",
      "Epoch 49/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 2791.1846 - val_loss: 2087.8489\n",
      "Epoch 50/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2759.3206 - val_loss: 2060.2720\n",
      "Epoch 51/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2727.7363 - val_loss: 2032.9597\n",
      "Epoch 52/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2696.4285 - val_loss: 2005.9109\n",
      "Epoch 53/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 2665.3950 - val_loss: 1979.1232\n",
      "Epoch 54/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2634.6343 - val_loss: 1952.5945\n",
      "Epoch 55/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2604.1440 - val_loss: 1926.3231\n",
      "Epoch 56/500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 2573.9216 - val_loss: 1900.3063\n",
      "Epoch 57/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2543.9651 - val_loss: 1874.5426\n",
      "Epoch 58/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2514.2732 - val_loss: 1849.0300\n",
      "Epoch 59/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2484.8433 - val_loss: 1823.7668\n",
      "Epoch 60/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2455.6733 - val_loss: 1798.7510\n",
      "Epoch 61/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2426.7625 - val_loss: 1773.9800\n",
      "Epoch 62/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2398.1077 - val_loss: 1749.4539\n",
      "Epoch 63/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2369.7078 - val_loss: 1725.1697\n",
      "Epoch 64/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2341.5615 - val_loss: 1701.1250\n",
      "Epoch 65/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2313.1643 - val_loss: 1665.0054\n",
      "Epoch 66/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2270.0376 - val_loss: 1637.9436\n",
      "Epoch 67/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2238.3743 - val_loss: 1611.1199\n",
      "Epoch 68/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2207.3994 - val_loss: 1585.0950\n",
      "Epoch 69/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2177.2810 - val_loss: 1559.7644\n",
      "Epoch 70/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2147.8774 - val_loss: 1535.0073\n",
      "Epoch 71/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2119.0676 - val_loss: 1510.7393\n",
      "Epoch 72/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2090.7681 - val_loss: 1486.9030\n",
      "Epoch 73/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2062.9211 - val_loss: 1463.4581\n",
      "Epoch 74/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2035.4871 - val_loss: 1440.3767\n",
      "Epoch 75/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2008.4360 - val_loss: 1417.6362\n",
      "Epoch 76/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1981.7468 - val_loss: 1395.2192\n",
      "Epoch 77/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1955.4001 - val_loss: 1373.1122\n",
      "Epoch 78/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1929.3821 - val_loss: 1351.3042\n",
      "Epoch 79/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1903.6807 - val_loss: 1329.7858\n",
      "Epoch 80/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1878.2858 - val_loss: 1308.5481\n",
      "Epoch 81/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1853.1887 - val_loss: 1287.5839\n",
      "Epoch 82/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1828.3822 - val_loss: 1266.8872\n",
      "Epoch 83/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1803.8588 - val_loss: 1246.4526\n",
      "Epoch 84/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1779.6132 - val_loss: 1226.2744\n",
      "Epoch 85/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1755.6396 - val_loss: 1206.3488\n",
      "Epoch 86/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1731.9329 - val_loss: 1186.6703\n",
      "Epoch 87/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1708.4889 - val_loss: 1167.2363\n",
      "Epoch 88/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1685.3040 - val_loss: 1148.0425\n",
      "Epoch 89/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1662.3739 - val_loss: 1129.0854\n",
      "Epoch 90/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1639.6946 - val_loss: 1110.3618\n",
      "Epoch 91/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 1617.2628 - val_loss: 1091.8690\n",
      "Epoch 92/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1595.0757 - val_loss: 1073.6042\n",
      "Epoch 93/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1573.1301 - val_loss: 1055.5638\n",
      "Epoch 94/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1551.4230 - val_loss: 1037.7460\n",
      "Epoch 95/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1529.9520 - val_loss: 1020.1484\n",
      "Epoch 96/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1508.7142 - val_loss: 1002.7679\n",
      "Epoch 97/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1487.7069 - val_loss: 985.6025\n",
      "Epoch 98/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1466.9280 - val_loss: 968.6500\n",
      "Epoch 99/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1446.3756 - val_loss: 951.9080\n",
      "Epoch 100/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1426.0465 - val_loss: 935.3744\n",
      "Epoch 101/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1405.9387 - val_loss: 919.0472\n",
      "Epoch 102/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1386.0504 - val_loss: 902.9244\n",
      "Epoch 103/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1366.3789 - val_loss: 887.0042\n",
      "Epoch 104/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1346.9229 - val_loss: 871.2840\n",
      "Epoch 105/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1327.6804 - val_loss: 855.7631\n",
      "Epoch 106/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1308.6492 - val_loss: 840.4390\n",
      "Epoch 107/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1289.8274 - val_loss: 825.3092\n",
      "Epoch 108/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1271.2128 - val_loss: 810.3730\n",
      "Epoch 109/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 1252.8046 - val_loss: 795.6288\n",
      "Epoch 110/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 1234.6006 - val_loss: 781.0747\n",
      "Epoch 111/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1216.5984 - val_loss: 766.7071\n",
      "Epoch 112/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1198.7968 - val_loss: 752.5271\n",
      "Epoch 113/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 1181.1942 - val_loss: 738.5316\n",
      "Epoch 114/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 1163.7887 - val_loss: 724.7194\n",
      "Epoch 115/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1146.5791 - val_loss: 711.0887\n",
      "Epoch 116/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1129.5634 - val_loss: 697.6385\n",
      "Epoch 117/500\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 1112.7399 - val_loss: 684.3661\n",
      "Epoch 118/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 1096.1078 - val_loss: 671.2710\n",
      "Epoch 119/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 1079.6644 - val_loss: 658.3514\n",
      "Epoch 120/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1063.4092 - val_loss: 645.6061\n",
      "Epoch 121/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 1047.3402 - val_loss: 633.0328\n",
      "Epoch 122/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1031.4556 - val_loss: 620.6301\n",
      "Epoch 123/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 1015.7546 - val_loss: 608.3976\n",
      "Epoch 124/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 1000.2353 - val_loss: 596.3331\n",
      "Epoch 125/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 984.8967 - val_loss: 584.4349\n",
      "Epoch 126/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 969.7368 - val_loss: 572.7022\n",
      "Epoch 127/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 954.7542 - val_loss: 561.1335\n",
      "Epoch 128/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 939.9479 - val_loss: 549.7271\n",
      "Epoch 129/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 925.3164 - val_loss: 538.4816\n",
      "Epoch 130/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 910.8582 - val_loss: 527.3956\n",
      "Epoch 131/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 896.5715 - val_loss: 516.4675\n",
      "Epoch 132/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 882.4554 - val_loss: 505.6975\n",
      "Epoch 133/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 868.5092 - val_loss: 495.0825\n",
      "Epoch 134/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 854.7303 - val_loss: 484.6215\n",
      "Epoch 135/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 841.1180 - val_loss: 474.3130\n",
      "Epoch 136/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 827.6707 - val_loss: 464.1575\n",
      "Epoch 137/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 814.3876 - val_loss: 454.1515\n",
      "Epoch 138/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 801.2673 - val_loss: 444.2940\n",
      "Epoch 139/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 788.3079 - val_loss: 434.5848\n",
      "Epoch 140/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 775.5087 - val_loss: 425.0211\n",
      "Epoch 141/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 762.8678 - val_loss: 415.6031\n",
      "Epoch 142/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 750.3842 - val_loss: 406.3291\n",
      "Epoch 143/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 738.0570 - val_loss: 397.1968\n",
      "Epoch 144/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 725.8848 - val_loss: 388.2066\n",
      "Epoch 145/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 713.8660 - val_loss: 379.3555\n",
      "Epoch 146/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 701.9996 - val_loss: 370.6433\n",
      "Epoch 147/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 690.2844 - val_loss: 362.0690\n",
      "Epoch 148/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 678.7189 - val_loss: 353.6303\n",
      "Epoch 149/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 667.3019 - val_loss: 345.3268\n",
      "Epoch 150/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 656.0325 - val_loss: 337.1575\n",
      "Epoch 151/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 644.9095 - val_loss: 329.1203\n",
      "Epoch 152/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 633.9313 - val_loss: 321.2142\n",
      "Epoch 153/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 623.0967 - val_loss: 313.4384\n",
      "Epoch 154/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 612.4048 - val_loss: 305.7915\n",
      "Epoch 155/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 601.8546 - val_loss: 298.2722\n",
      "Epoch 156/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 591.4445 - val_loss: 290.8792\n",
      "Epoch 157/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 581.1732 - val_loss: 283.6118\n",
      "Epoch 158/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 571.0398 - val_loss: 276.4679\n",
      "Epoch 159/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 561.0432 - val_loss: 269.4474\n",
      "Epoch 160/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 551.1819 - val_loss: 262.5481\n",
      "Epoch 161/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 541.4550 - val_loss: 255.7692\n",
      "Epoch 162/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 531.8610 - val_loss: 249.1105\n",
      "Epoch 163/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 522.3994 - val_loss: 242.5690\n",
      "Epoch 164/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 513.0681 - val_loss: 236.1444\n",
      "Epoch 165/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 503.8666 - val_loss: 229.8358\n",
      "Epoch 166/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 494.7937 - val_loss: 223.6417\n",
      "Epoch 167/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 485.8480 - val_loss: 217.5615\n",
      "Epoch 168/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 477.0288 - val_loss: 211.5930\n",
      "Epoch 169/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 468.3346 - val_loss: 205.7357\n",
      "Epoch 170/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 459.7645 - val_loss: 199.9888\n",
      "Epoch 171/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 451.3171 - val_loss: 194.3506\n",
      "Epoch 172/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 442.9916 - val_loss: 188.8199\n",
      "Epoch 173/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 434.7867 - val_loss: 183.3960\n",
      "Epoch 174/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 426.7011 - val_loss: 178.0776\n",
      "Epoch 175/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 418.7340 - val_loss: 172.8632\n",
      "Epoch 176/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 410.8841 - val_loss: 167.7522\n",
      "Epoch 177/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 403.1505 - val_loss: 162.7431\n",
      "Epoch 178/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 395.5320 - val_loss: 157.8350\n",
      "Epoch 179/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 388.0272 - val_loss: 153.0271\n",
      "Epoch 180/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 380.6358 - val_loss: 148.3171\n",
      "Epoch 181/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 373.3558 - val_loss: 143.7054\n",
      "Epoch 182/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 366.1866 - val_loss: 139.1900\n",
      "Epoch 183/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 359.1272 - val_loss: 134.7702\n",
      "Epoch 184/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 352.1765 - val_loss: 130.4444\n",
      "Epoch 185/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 345.3330 - val_loss: 126.2119\n",
      "Epoch 186/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 338.5961 - val_loss: 122.0717\n",
      "Epoch 187/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 331.9646 - val_loss: 118.0224\n",
      "Epoch 188/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 325.4372 - val_loss: 114.0630\n",
      "Epoch 189/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 319.0132 - val_loss: 110.1926\n",
      "Epoch 190/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 312.6915 - val_loss: 106.4100\n",
      "Epoch 191/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 306.4709 - val_loss: 102.7143\n",
      "Epoch 192/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 300.3504 - val_loss: 99.1041\n",
      "Epoch 193/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 294.3289 - val_loss: 95.5783\n",
      "Epoch 194/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 288.4054 - val_loss: 92.1363\n",
      "Epoch 195/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 282.5788 - val_loss: 88.7766\n",
      "Epoch 196/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 276.8482 - val_loss: 85.4986\n",
      "Epoch 197/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 271.2128 - val_loss: 82.3009\n",
      "Epoch 198/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 265.6711 - val_loss: 79.1824\n",
      "Epoch 199/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 260.2222 - val_loss: 76.1423\n",
      "Epoch 200/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 254.8654 - val_loss: 73.1793\n",
      "Epoch 201/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 249.5993 - val_loss: 70.2929\n",
      "Epoch 202/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 244.4231 - val_loss: 67.4816\n",
      "Epoch 203/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 239.3358 - val_loss: 64.7444\n",
      "Epoch 204/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 234.3362 - val_loss: 62.0803\n",
      "Epoch 205/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 229.4236 - val_loss: 59.4884\n",
      "Epoch 206/500\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 224.5968 - val_loss: 56.9677\n",
      "Epoch 207/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 219.8547 - val_loss: 54.5170\n",
      "Epoch 208/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 215.1965 - val_loss: 52.1354\n",
      "Epoch 209/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 210.6214 - val_loss: 49.8220\n",
      "Epoch 210/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 206.1282 - val_loss: 47.5757\n",
      "Epoch 211/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 201.7157 - val_loss: 45.3955\n",
      "Epoch 212/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 197.3834 - val_loss: 43.2806\n",
      "Epoch 213/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 193.1302 - val_loss: 41.2296\n",
      "Epoch 214/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 188.9550 - val_loss: 39.2420\n",
      "Epoch 215/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 184.8567 - val_loss: 37.3165\n",
      "Epoch 216/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 180.8345 - val_loss: 35.4521\n",
      "Epoch 217/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 176.8876 - val_loss: 33.6482\n",
      "Epoch 218/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 173.0149 - val_loss: 31.9035\n",
      "Epoch 219/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 169.2154 - val_loss: 30.2170\n",
      "Epoch 220/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 165.4883 - val_loss: 28.5880\n",
      "Epoch 221/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 161.8327 - val_loss: 27.0155\n",
      "Epoch 222/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 158.2474 - val_loss: 25.4985\n",
      "Epoch 223/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 154.7319 - val_loss: 24.0361\n",
      "Epoch 224/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 151.2851 - val_loss: 22.6273\n",
      "Epoch 225/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 147.9059 - val_loss: 21.2711\n",
      "Epoch 226/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 144.5936 - val_loss: 19.9668\n",
      "Epoch 227/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 141.3469 - val_loss: 18.7134\n",
      "Epoch 228/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 138.1655 - val_loss: 17.5098\n",
      "Epoch 229/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 135.0480 - val_loss: 16.3555\n",
      "Epoch 230/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 131.9940 - val_loss: 15.2494\n",
      "Epoch 231/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 129.0023 - val_loss: 14.1904\n",
      "Epoch 232/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 126.0721 - val_loss: 13.1778\n",
      "Epoch 233/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 123.2024 - val_loss: 12.2109\n",
      "Epoch 234/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 120.3926 - val_loss: 11.2885\n",
      "Epoch 235/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 117.6415 - val_loss: 10.4098\n",
      "Epoch 236/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 114.9485 - val_loss: 9.5741\n",
      "Epoch 237/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 112.3127 - val_loss: 8.7804\n",
      "Epoch 238/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 109.7332 - val_loss: 8.0280\n",
      "Epoch 239/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 107.2091 - val_loss: 7.3158\n",
      "Epoch 240/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 104.7396 - val_loss: 6.6432\n",
      "Epoch 241/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 102.3241 - val_loss: 6.0092\n",
      "Epoch 242/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 99.9613 - val_loss: 5.4130\n",
      "Epoch 243/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 97.6507 - val_loss: 4.8539\n",
      "Epoch 244/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 95.3915 - val_loss: 4.3310\n",
      "Epoch 245/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 93.1827 - val_loss: 3.8435\n",
      "Epoch 246/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 91.0236 - val_loss: 3.3906\n",
      "Epoch 247/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 88.9134 - val_loss: 2.9714\n",
      "Epoch 248/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 86.8514 - val_loss: 2.5854\n",
      "Epoch 249/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 84.8368 - val_loss: 2.2315\n",
      "Epoch 250/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 82.8687 - val_loss: 1.9092\n",
      "Epoch 251/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 80.9464 - val_loss: 1.6175\n",
      "Epoch 252/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 79.0690 - val_loss: 1.3557\n",
      "Epoch 253/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 77.2359 - val_loss: 1.1232\n",
      "Epoch 254/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 75.4463 - val_loss: 0.9191\n",
      "Epoch 255/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 73.6993 - val_loss: 0.7427\n",
      "Epoch 256/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 71.9945 - val_loss: 0.5933\n",
      "Epoch 257/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 70.3309 - val_loss: 0.4701\n",
      "Epoch 258/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 68.7078 - val_loss: 0.3725\n",
      "Epoch 259/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 67.1245 - val_loss: 0.2998\n",
      "Epoch 260/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 65.5804 - val_loss: 0.2511\n",
      "Epoch 261/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 64.0745 - val_loss: 0.2259\n",
      "Epoch 262/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 62.6063 - val_loss: 0.2235\n",
      "Epoch 263/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 61.1752 - val_loss: 0.2432\n",
      "Epoch 264/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 59.7803 - val_loss: 0.2843\n",
      "Epoch 265/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 58.4209 - val_loss: 0.3461\n",
      "Epoch 266/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 57.0965 - val_loss: 0.4281\n",
      "Epoch 267/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 55.8063 - val_loss: 0.5295\n",
      "Epoch 268/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 54.5498 - val_loss: 0.6497\n",
      "Epoch 269/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 53.3262 - val_loss: 0.7882\n",
      "Epoch 270/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 52.1348 - val_loss: 0.9443\n",
      "Epoch 271/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 50.9750 - val_loss: 1.1173\n",
      "Epoch 272/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 49.8464 - val_loss: 1.3066\n",
      "Epoch 273/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 48.7481 - val_loss: 1.5118\n",
      "Epoch 274/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 47.6796 - val_loss: 1.7322\n",
      "Epoch 275/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 46.6402 - val_loss: 1.9671\n",
      "Epoch 276/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 45.6293 - val_loss: 2.2162\n",
      "Epoch 277/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 44.6463 - val_loss: 2.4788\n",
      "Epoch 278/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 43.6907 - val_loss: 2.7542\n",
      "Epoch 279/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 42.7619 - val_loss: 3.0421\n",
      "Epoch 280/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 41.8592 - val_loss: 3.3418\n",
      "Epoch 281/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 40.9822 - val_loss: 3.6529\n",
      "Epoch 282/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 40.1302 - val_loss: 3.9747\n",
      "Epoch 283/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 39.3028 - val_loss: 4.3070\n",
      "Epoch 284/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 38.4992 - val_loss: 4.6490\n",
      "Epoch 285/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 37.7192 - val_loss: 5.0004\n",
      "Epoch 286/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 36.9619 - val_loss: 5.3606\n",
      "Epoch 287/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 36.2270 - val_loss: 5.7292\n",
      "Epoch 288/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 35.5138 - val_loss: 6.1058\n",
      "Epoch 289/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 34.8220 - val_loss: 6.4898\n",
      "Epoch 290/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 34.1511 - val_loss: 6.8809\n",
      "Epoch 291/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 33.5005 - val_loss: 7.2784\n",
      "Epoch 292/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 32.8697 - val_loss: 7.6822\n",
      "Epoch 293/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 32.2583 - val_loss: 8.0917\n",
      "Epoch 294/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 31.6658 - val_loss: 8.5066\n",
      "Epoch 295/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 31.0917 - val_loss: 8.9264\n",
      "Epoch 296/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.5355 - val_loss: 9.3508\n",
      "Epoch 297/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 29.9969 - val_loss: 9.7791\n",
      "Epoch 298/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 29.4754 - val_loss: 10.2114\n",
      "Epoch 299/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 28.9704 - val_loss: 10.6471\n",
      "Epoch 300/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 28.4818 - val_loss: 11.0858\n",
      "Epoch 301/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 28.0088 - val_loss: 11.5273\n",
      "Epoch 302/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 27.5512 - val_loss: 11.9711\n",
      "Epoch 303/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 27.1086 - val_loss: 12.4169\n",
      "Epoch 304/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 26.6807 - val_loss: 12.8644\n",
      "Epoch 305/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 26.2669 - val_loss: 13.3135\n",
      "Epoch 306/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 25.8668 - val_loss: 13.7634\n",
      "Epoch 307/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 25.4803 - val_loss: 14.2143\n",
      "Epoch 308/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 25.1067 - val_loss: 14.6658\n",
      "Epoch 309/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 24.7458 - val_loss: 15.1173\n",
      "Epoch 310/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 24.3973 - val_loss: 15.5688\n",
      "Epoch 311/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 24.0608 - val_loss: 16.0201\n",
      "Epoch 312/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 23.7359 - val_loss: 16.4708\n",
      "Epoch 313/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 23.4223 - val_loss: 16.9208\n",
      "Epoch 314/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 23.1197 - val_loss: 17.3697\n",
      "Epoch 315/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 22.8277 - val_loss: 17.8174\n",
      "Epoch 316/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 22.5461 - val_loss: 18.2634\n",
      "Epoch 317/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 22.2745 - val_loss: 18.7078\n",
      "Epoch 318/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 22.0127 - val_loss: 19.1501\n",
      "Epoch 319/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 21.7604 - val_loss: 19.5905\n",
      "Epoch 320/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 21.5171 - val_loss: 20.0285\n",
      "Epoch 321/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 21.2828 - val_loss: 20.4640\n",
      "Epoch 322/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 21.0571 - val_loss: 20.8967\n",
      "Epoch 323/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 20.8397 - val_loss: 21.3267\n",
      "Epoch 324/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 20.6304 - val_loss: 21.7537\n",
      "Epoch 325/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 20.4288 - val_loss: 22.1775\n",
      "Epoch 326/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.2349 - val_loss: 22.5979\n",
      "Epoch 327/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.0483 - val_loss: 23.0149\n",
      "Epoch 328/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 19.8688 - val_loss: 23.4283\n",
      "Epoch 329/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 19.6961 - val_loss: 23.8381\n",
      "Epoch 330/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 19.5300 - val_loss: 24.2437\n",
      "Epoch 331/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 19.3704 - val_loss: 24.6456\n",
      "Epoch 332/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 19.2169 - val_loss: 25.0433\n",
      "Epoch 333/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 19.0695 - val_loss: 25.4369\n",
      "Epoch 334/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 18.9278 - val_loss: 25.8263\n",
      "Epoch 335/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 18.7917 - val_loss: 26.2113\n",
      "Epoch 336/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 18.6610 - val_loss: 26.5919\n",
      "Epoch 337/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 18.5355 - val_loss: 26.9677\n",
      "Epoch 338/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 18.4151 - val_loss: 27.3391\n",
      "Epoch 339/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 18.2994 - val_loss: 27.7057\n",
      "Epoch 340/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 18.1885 - val_loss: 28.0676\n",
      "Epoch 341/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 18.0821 - val_loss: 28.4248\n",
      "Epoch 342/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 17.9800 - val_loss: 28.7771\n",
      "Epoch 343/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 17.8821 - val_loss: 29.1243\n",
      "Epoch 344/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 17.7882 - val_loss: 29.4668\n",
      "Epoch 345/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 17.6982 - val_loss: 29.8044\n",
      "Epoch 346/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 17.6119 - val_loss: 30.1369\n",
      "Epoch 347/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 17.5293 - val_loss: 30.4644\n",
      "Epoch 348/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 17.4501 - val_loss: 30.7867\n",
      "Epoch 349/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 17.3743 - val_loss: 31.1041\n",
      "Epoch 350/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 17.3016 - val_loss: 31.4161\n",
      "Epoch 351/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 17.2321 - val_loss: 31.7231\n",
      "Epoch 352/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 17.1655 - val_loss: 32.0249\n",
      "Epoch 353/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 17.1018 - val_loss: 32.3220\n",
      "Epoch 354/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 17.0408 - val_loss: 32.6137\n",
      "Epoch 355/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.9825 - val_loss: 32.9003\n",
      "Epoch 356/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.9266 - val_loss: 33.1818\n",
      "Epoch 357/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 16.8732 - val_loss: 33.4581\n",
      "Epoch 358/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 16.8222 - val_loss: 33.7294\n",
      "Epoch 359/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.7734 - val_loss: 33.9956\n",
      "Epoch 360/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.7267 - val_loss: 34.2567\n",
      "Epoch 361/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 16.6821 - val_loss: 34.5130\n",
      "Epoch 362/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.6395 - val_loss: 34.7641\n",
      "Epoch 363/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.5987 - val_loss: 35.0104\n",
      "Epoch 364/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 16.5598 - val_loss: 35.2517\n",
      "Epoch 365/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 16.5226 - val_loss: 35.4879\n",
      "Epoch 366/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.4871 - val_loss: 35.7193\n",
      "Epoch 367/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 16.4532 - val_loss: 35.9459\n",
      "Epoch 368/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.4208 - val_loss: 36.1675\n",
      "Epoch 369/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.3899 - val_loss: 36.3847\n",
      "Epoch 370/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.3603 - val_loss: 36.5970\n",
      "Epoch 371/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.3322 - val_loss: 36.8046\n",
      "Epoch 372/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.3053 - val_loss: 37.0076\n",
      "Epoch 373/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.2796 - val_loss: 37.2061\n",
      "Epoch 374/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.2551 - val_loss: 37.4001\n",
      "Epoch 375/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.2318 - val_loss: 37.5897\n",
      "Epoch 376/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.2095 - val_loss: 37.7747\n",
      "Epoch 377/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 16.1882 - val_loss: 37.9555\n",
      "Epoch 378/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 16.1679 - val_loss: 38.1318\n",
      "Epoch 379/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 16.1486 - val_loss: 38.3041\n",
      "Epoch 380/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 16.1301 - val_loss: 38.4720\n",
      "Epoch 381/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 16.1125 - val_loss: 38.6356\n",
      "Epoch 382/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 16.0957 - val_loss: 38.7954\n",
      "Epoch 383/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 16.0797 - val_loss: 38.9512\n",
      "Epoch 384/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 16.0644 - val_loss: 39.1027\n",
      "Epoch 385/500\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 16.0500 - val_loss: 39.2508\n",
      "Epoch 386/500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 16.0361 - val_loss: 39.3950\n",
      "Epoch 387/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 16.0228 - val_loss: 39.5353\n",
      "Epoch 388/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 16.0103 - val_loss: 39.6721\n",
      "Epoch 389/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 15.9982 - val_loss: 39.8050\n",
      "Epoch 390/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 15.9868 - val_loss: 39.9345\n",
      "Epoch 391/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 15.9758 - val_loss: 40.0601\n",
      "Epoch 392/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 15.9655 - val_loss: 40.1826\n",
      "Epoch 393/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 15.9556 - val_loss: 40.3017\n",
      "Epoch 394/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 15.9461 - val_loss: 40.4174\n",
      "Epoch 395/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 15.9371 - val_loss: 40.5299\n",
      "Epoch 396/500\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 15.9285 - val_loss: 40.6389\n",
      "Epoch 397/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 15.9204 - val_loss: 40.7453\n",
      "Epoch 398/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 15.9125 - val_loss: 40.8481\n",
      "Epoch 399/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 15.9051 - val_loss: 40.9481\n",
      "Epoch 400/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 15.8981 - val_loss: 41.0452\n",
      "Epoch 401/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.8913 - val_loss: 41.1398\n",
      "Epoch 402/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.8849 - val_loss: 41.2309\n",
      "Epoch 403/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 15.8788 - val_loss: 41.3195\n",
      "Epoch 404/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.8730 - val_loss: 41.4058\n",
      "Epoch 405/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.8674 - val_loss: 41.4890\n",
      "Epoch 406/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 15.8621 - val_loss: 41.5699\n",
      "Epoch 407/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 15.8570 - val_loss: 41.6479\n",
      "Epoch 408/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 15.8523 - val_loss: 41.7237\n",
      "Epoch 409/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 15.8476 - val_loss: 41.7968\n",
      "Epoch 410/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 15.8433 - val_loss: 41.8681\n",
      "Epoch 411/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.8391 - val_loss: 41.9367\n",
      "Epoch 412/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.8351 - val_loss: 42.0032\n",
      "Epoch 413/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.8314 - val_loss: 42.0677\n",
      "Epoch 414/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.8278 - val_loss: 42.1296\n",
      "Epoch 415/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 15.8243 - val_loss: 42.1902\n",
      "Epoch 416/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 15.8211 - val_loss: 42.2481\n",
      "Epoch 417/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 15.8180 - val_loss: 42.3043\n",
      "Epoch 418/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 15.8150 - val_loss: 42.3585\n",
      "Epoch 419/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 15.8122 - val_loss: 42.4108\n",
      "Epoch 420/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.8095 - val_loss: 42.4615\n",
      "Epoch 421/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.8070 - val_loss: 42.5105\n",
      "Epoch 422/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 15.8045 - val_loss: 42.5574\n",
      "Epoch 423/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 15.8022 - val_loss: 42.6029\n",
      "Epoch 424/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.8000 - val_loss: 42.6467\n",
      "Epoch 425/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 15.7979 - val_loss: 42.6891\n",
      "Epoch 426/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.7959 - val_loss: 42.7298\n",
      "Epoch 427/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.7940 - val_loss: 42.7690\n",
      "Epoch 428/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 15.7922 - val_loss: 42.8070\n",
      "Epoch 429/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.7905 - val_loss: 42.8432\n",
      "Epoch 430/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.7889 - val_loss: 42.8783\n",
      "Epoch 431/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.7873 - val_loss: 42.9120\n",
      "Epoch 432/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 15.7858 - val_loss: 42.9447\n",
      "Epoch 433/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 15.7844 - val_loss: 42.9761\n",
      "Epoch 434/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.7831 - val_loss: 43.0062\n",
      "Epoch 435/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.7818 - val_loss: 43.0351\n",
      "Epoch 436/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 15.7806 - val_loss: 43.0627\n",
      "Epoch 437/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 15.7795 - val_loss: 43.0895\n",
      "Epoch 438/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.7783 - val_loss: 43.1154\n",
      "Epoch 439/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.7773 - val_loss: 43.1400\n",
      "Epoch 440/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 15.7764 - val_loss: 43.1637\n",
      "Epoch 441/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.7754 - val_loss: 43.1864\n",
      "Epoch 442/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.7746 - val_loss: 43.2083\n",
      "Epoch 443/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.7737 - val_loss: 43.2294\n",
      "Epoch 444/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.7729 - val_loss: 43.2492\n",
      "Epoch 445/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.7722 - val_loss: 43.2685\n",
      "Epoch 446/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 15.7715 - val_loss: 43.2870\n",
      "Epoch 447/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 15.7708 - val_loss: 43.3048\n",
      "Epoch 448/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.7702 - val_loss: 43.3218\n",
      "Epoch 449/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 15.7697 - val_loss: 43.3379\n",
      "Epoch 450/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 15.7691 - val_loss: 43.3536\n",
      "Epoch 451/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.7685 - val_loss: 43.3683\n",
      "Epoch 452/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.7681 - val_loss: 43.3825\n",
      "Epoch 453/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.7676 - val_loss: 43.3961\n",
      "Epoch 454/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.7672 - val_loss: 43.4092\n",
      "Epoch 455/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.7668 - val_loss: 43.4218\n",
      "Epoch 456/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.7664 - val_loss: 43.4338\n",
      "Epoch 457/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.7660 - val_loss: 43.4452\n",
      "Epoch 458/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.7657 - val_loss: 43.4562\n",
      "Epoch 459/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.7654 - val_loss: 43.4664\n",
      "Epoch 460/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.7651 - val_loss: 43.4764\n",
      "Epoch 461/500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 15.7648 - val_loss: 43.4856\n",
      "Epoch 462/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 15.7646 - val_loss: 43.4946\n",
      "Epoch 463/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 15.7644 - val_loss: 43.5033\n",
      "Epoch 464/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 15.7642 - val_loss: 43.5115\n",
      "Epoch 465/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 15.7640 - val_loss: 43.5193\n",
      "Epoch 466/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 15.7639 - val_loss: 43.5267\n",
      "Epoch 467/500\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 15.7637 - val_loss: 43.5337\n",
      "Epoch 468/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 15.7636 - val_loss: 43.5405\n",
      "Epoch 469/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 15.7634 - val_loss: 43.5470\n",
      "Epoch 470/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 15.7633 - val_loss: 43.5530\n",
      "Epoch 471/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 15.7632 - val_loss: 43.5587\n",
      "Epoch 472/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 15.7632 - val_loss: 43.5641\n",
      "Epoch 473/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 15.7631 - val_loss: 43.5694\n",
      "Epoch 474/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 15.7630 - val_loss: 43.5745\n",
      "Epoch 475/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 15.7630 - val_loss: 43.5792\n",
      "Epoch 476/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 15.7629 - val_loss: 43.5837\n",
      "Epoch 477/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 15.7629 - val_loss: 43.5878\n",
      "Epoch 478/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 15.7629 - val_loss: 43.5920\n",
      "Epoch 479/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.7629 - val_loss: 43.5956\n",
      "Epoch 480/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.7629 - val_loss: 43.5993\n",
      "Epoch 481/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 15.7629 - val_loss: 43.6028\n",
      "Epoch 482/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 15.7629 - val_loss: 43.6059\n",
      "Epoch 483/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 15.7629 - val_loss: 43.6089\n",
      "Epoch 484/500\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 15.7630 - val_loss: 43.6118\n",
      "Epoch 485/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 15.7630 - val_loss: 43.6145\n",
      "Epoch 486/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 15.7631 - val_loss: 43.6171\n",
      "Epoch 487/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 15.7631 - val_loss: 43.6194\n",
      "Epoch 488/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.7631 - val_loss: 43.6217\n",
      "Epoch 489/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 15.7632 - val_loss: 43.6235\n",
      "Epoch 490/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 15.7633 - val_loss: 43.6255\n",
      "Epoch 491/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 15.7633 - val_loss: 43.6272\n",
      "Epoch 492/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.7634 - val_loss: 43.6287\n",
      "Epoch 493/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.7636 - val_loss: 43.6307\n",
      "Epoch 494/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 15.7636 - val_loss: 43.6320\n",
      "Epoch 495/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 15.7637 - val_loss: 43.6335\n",
      "Epoch 496/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.7639 - val_loss: 43.6350\n",
      "Epoch 497/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.7639 - val_loss: 43.6363\n",
      "Epoch 498/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.7640 - val_loss: 43.6375\n",
      "Epoch 499/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.7641 - val_loss: 43.6386\n",
      "Epoch 500/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.7642 - val_loss: 43.6396\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6465, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0029, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0003, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0168, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3581, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 0.1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 409ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.49537815e+01, 6.48977591e+01, 6.48417367e+01, 6.47857143e+01,\n",
       "        6.47296919e+01, 6.46736695e+01, 6.46176471e+01, 6.45616246e+01,\n",
       "        6.45056022e+01, 6.44495798e+01, 6.43935574e+01, 6.43375350e+01,\n",
       "        6.42815126e+01, 6.42254902e+01, 6.41694678e+01, 6.41134454e+01,\n",
       "        6.40574230e+01, 6.40014006e+01, 6.39808824e+01, 6.39612745e+01,\n",
       "        6.39416667e+01, 6.39220588e+01, 6.39024510e+01, 6.38828431e+01,\n",
       "        6.38632353e+01, 6.38436274e+01, 6.38240196e+01, 6.38044118e+01,\n",
       "        6.37848039e+01, 6.37651961e+01, 6.37455882e+01, 6.37259804e+01,\n",
       "        6.37063725e+01, 6.36867647e+01, 6.36671569e+01, 6.36475490e+01,\n",
       "        6.36279412e+01, 6.36083333e+01, 6.35887255e+01, 6.35691177e+01,\n",
       "        6.35495098e+01, 6.35299020e+01, 6.35102941e+01, 6.34906863e+01,\n",
       "        6.34710784e+01, 6.34514706e+01, 6.34318628e+01, 6.34122549e+01,\n",
       "        6.33926471e+01, 6.33730392e+01, 6.33534314e+01, 6.33338235e+01,\n",
       "        6.33142157e+01, 6.32930672e+01, 6.32678571e+01, 6.32426471e+01,\n",
       "        6.32174370e+01, 6.31922269e+01, 6.31670168e+01, 6.31418067e+01,\n",
       "        6.31165966e+01, 6.30913866e+01, 6.30661765e+01, 6.30409664e+01,\n",
       "        6.30157563e+01, 6.29905462e+01, 6.29653361e+01, 6.29401261e+01,\n",
       "        6.29149160e+01, 6.28897059e+01, 6.28644958e+01, 6.28392857e+01,\n",
       "        6.28140756e+01, 6.27888655e+01, 6.27636555e+01, 6.27384454e+01,\n",
       "        6.27132353e+01, 6.26880252e+01, 6.26628151e+01, 6.26376050e+01,\n",
       "        7.00034103e+01, 0.00000000e+00, 0.00000000e+00, 1.69796750e-01,\n",
       "        2.26345703e-01, 4.73187238e-01, 2.75125384e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.17714518e-01, 3.69451851e-01, 0.00000000e+00,\n",
       "        4.59393412e-01, 8.40820149e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.59561169e-01, 6.64002374e-02, 1.25769787e-02]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([61.86454248, 61.85800654, 61.85147059, 61.84493464, 61.83839869,\n",
       "       61.83186275, 61.8253268 , 61.81879085, 61.8122549 , 61.80571895,\n",
       "       61.79918301, 61.79264706, 61.78611111, 61.77957516, 61.77303922,\n",
       "       61.76650327, 61.75996732, 61.75343137, 61.74689542, 61.74035948,\n",
       "       61.73382353, 61.72728758, 61.72075163, 61.71421569, 61.70767974,\n",
       "       61.70114379, 61.69460784, 61.6880719 , 61.68153595, 61.675     ,\n",
       "       61.66846405, 61.6619281 , 61.65539216, 61.64885621, 61.64232026,\n",
       "       61.63578431, 61.62924837, 61.62271242, 61.61617647, 61.60964052,\n",
       "       61.60310458, 61.59833333, 61.59515873, 61.59198413, 61.58880952,\n",
       "       61.58563492, 61.58246032, 61.57928571, 61.57611111, 61.57293651,\n",
       "       61.5697619 , 61.5665873 , 61.5634127 , 61.5602381 , 61.55706349,\n",
       "       61.55388889, 61.55071429, 61.54753968, 61.54436508, 61.54119048,\n",
       "       61.53801587, 61.53484127, 61.53166667, 61.52849206, 61.52531746,\n",
       "       61.52214286, 61.51896825, 61.51579365, 61.51261905, 61.50944444,\n",
       "       61.50626984, 61.50309524, 61.49992063, 61.49674603, 61.49357143,\n",
       "       61.49039683, 61.48722222, 61.48404762, 61.48087302, 61.47769841,\n",
       "       61.47452381, 61.47134921, 61.4681746 , 61.465     , 61.4618254 ,\n",
       "       61.45865079, 61.45547619, 61.45230159, 61.44912698, 61.44595238,\n",
       "       61.44277778, 61.43960317, 61.43642857, 61.43325397, 61.43007937,\n",
       "       61.42690476, 61.42373016, 61.42055556, 61.41738095, 61.41420635])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.792740410492407\n",
      "13.303835752732141\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
