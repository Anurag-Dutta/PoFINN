{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.200000\n",
       "1       88.931092\n",
       "2       88.662185\n",
       "3       88.393277\n",
       "4       88.124370\n",
       "          ...    \n",
       "1445    61.897222\n",
       "1446    61.890686\n",
       "1447    61.884150\n",
       "1448    61.877614\n",
       "1449    61.871078\n",
       "Name: C1, Length: 1450, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c1_interpolated_1350_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.200000\n",
       "1       88.931092\n",
       "2       88.662185\n",
       "3       88.393277\n",
       "4       88.124370\n",
       "          ...    \n",
       "1345     0.759480\n",
       "1346     0.000000\n",
       "1347     0.714415\n",
       "1348     1.127849\n",
       "1349     0.025774\n",
       "Name: C1, Length: 1350, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1350)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.931092</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.662185</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.393277</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.124370</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     89.200000  0.000298\n",
       "1     88.931092  0.000298\n",
       "2     88.662185  0.000297\n",
       "3     88.393277  0.000297\n",
       "4     88.124370  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6465  # Value for C0\n",
    "K0 = -0.0029  # Value for K0\n",
    "K1 = -0.0003  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0168    # Value for b\n",
    "c = 2.3581    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    89.200000    88.931092    88.662185    88.393277    88.124370   \n",
      "351    88.931092    88.662185    88.393277    88.124370    87.855462   \n",
      "352    88.662185    88.393277    88.124370    87.855462    87.586555   \n",
      "353    88.393277    88.124370    87.855462    87.586555    87.317647   \n",
      "354    88.124370    87.855462    87.586555    87.317647    87.048739   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.855462    87.586555    87.317647    87.048739    86.794538  ...   \n",
      "351    87.586555    87.317647    87.048739    86.794538    86.721709  ...   \n",
      "352    87.317647    87.048739    86.794538    86.721709    86.648880  ...   \n",
      "353    87.048739    86.794538    86.721709    86.648880    86.576050  ...   \n",
      "354    86.794538    86.721709    86.648880    86.576050    86.503221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   73.989683    0.000263   73.957937    0.000263   73.926190    0.000263   \n",
      "351   73.957937    0.000263   73.926190    0.000263   73.894444    0.000262   \n",
      "352   73.926190    0.000263   73.894444    0.000262   73.862698    0.000262   \n",
      "353   73.894444    0.000262   73.862698    0.000262   73.830952    0.000262   \n",
      "354   73.862698    0.000262   73.830952    0.000262   73.799206    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   73.894444    0.000262   73.862698    0.000262  \n",
      "351   73.862698    0.000262   73.830952    0.000262  \n",
      "352   73.830952    0.000262   73.799206    0.000262  \n",
      "353   73.799206    0.000262   73.767460    0.000262  \n",
      "354   73.767460    0.000262   73.735714    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 1, 251) (900, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "12/12 [==============================] - 2s 48ms/step - loss: 4915.9834 - val_loss: 3989.3396\n",
      "Epoch 2/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4854.7432 - val_loss: 3956.9326\n",
      "Epoch 3/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4815.6514 - val_loss: 3917.0713\n",
      "Epoch 4/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4773.6377 - val_loss: 3881.7935\n",
      "Epoch 5/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4734.6904 - val_loss: 3846.6721\n",
      "Epoch 6/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4696.0464 - val_loss: 3811.9268\n",
      "Epoch 7/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4657.8022 - val_loss: 3777.5471\n",
      "Epoch 8/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4619.9302 - val_loss: 3743.4978\n",
      "Epoch 9/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4582.3931 - val_loss: 3709.7493\n",
      "Epoch 10/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4545.1621 - val_loss: 3676.2798\n",
      "Epoch 11/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4508.2051 - val_loss: 3642.4978\n",
      "Epoch 12/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4469.9111 - val_loss: 3607.1980\n",
      "Epoch 13/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4430.7080 - val_loss: 3571.9746\n",
      "Epoch 14/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4391.8711 - val_loss: 3537.2559\n",
      "Epoch 15/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4353.5718 - val_loss: 3503.0139\n",
      "Epoch 16/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4315.7559 - val_loss: 3469.1897\n",
      "Epoch 17/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4278.3604 - val_loss: 3435.7327\n",
      "Epoch 18/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4241.3394 - val_loss: 3402.6096\n",
      "Epoch 19/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4204.6592 - val_loss: 3369.7971\n",
      "Epoch 20/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 4168.2988 - val_loss: 3337.2761\n",
      "Epoch 21/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 4132.2373 - val_loss: 3305.0337\n",
      "Epoch 22/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4096.4629 - val_loss: 3273.0598\n",
      "Epoch 23/500\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 4060.9663 - val_loss: 3241.3452\n",
      "Epoch 24/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 4025.7385 - val_loss: 3209.8833\n",
      "Epoch 25/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3988.4480 - val_loss: 3172.0635\n",
      "Epoch 26/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3947.1606 - val_loss: 3137.7458\n",
      "Epoch 27/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3909.0146 - val_loss: 3103.9041\n",
      "Epoch 28/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3871.5181 - val_loss: 3070.7122\n",
      "Epoch 29/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 3834.6931 - val_loss: 3038.0879\n",
      "Epoch 30/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 3798.4409 - val_loss: 3005.9458\n",
      "Epoch 31/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 3762.6770 - val_loss: 2974.2236\n",
      "Epoch 32/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3727.3416 - val_loss: 2942.8784\n",
      "Epoch 33/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3685.3799 - val_loss: 2903.1265\n",
      "Epoch 34/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3646.3440 - val_loss: 2868.9709\n",
      "Epoch 35/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3608.3669 - val_loss: 2835.5586\n",
      "Epoch 36/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 3571.2031 - val_loss: 2802.8528\n",
      "Epoch 37/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 3534.7581 - val_loss: 2770.7456\n",
      "Epoch 38/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 3498.9209 - val_loss: 2739.1528\n",
      "Epoch 39/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3463.6091 - val_loss: 2708.0129\n",
      "Epoch 40/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 3428.7632 - val_loss: 2677.2839\n",
      "Epoch 41/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 3394.3416 - val_loss: 2646.9329\n",
      "Epoch 42/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 3360.3125 - val_loss: 2616.9351\n",
      "Epoch 43/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 3326.6511 - val_loss: 2587.2732\n",
      "Epoch 44/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 3293.3379 - val_loss: 2557.9302\n",
      "Epoch 45/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 3260.3577 - val_loss: 2528.8948\n",
      "Epoch 46/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 3227.6975 - val_loss: 2500.1553\n",
      "Epoch 47/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 3195.3469 - val_loss: 2471.7024\n",
      "Epoch 48/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 3163.2952 - val_loss: 2443.5288\n",
      "Epoch 49/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 3131.5347 - val_loss: 2415.6277\n",
      "Epoch 50/500\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 3100.0583 - val_loss: 2387.9927\n",
      "Epoch 51/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 3068.8601 - val_loss: 2360.6179\n",
      "Epoch 52/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 3037.9331 - val_loss: 2333.4990\n",
      "Epoch 53/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 3007.2732 - val_loss: 2306.6311\n",
      "Epoch 54/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 2976.8743 - val_loss: 2280.0100\n",
      "Epoch 55/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 2946.7341 - val_loss: 2253.6316\n",
      "Epoch 56/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 2916.8469 - val_loss: 2227.4932\n",
      "Epoch 57/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 2887.2100 - val_loss: 2201.5911\n",
      "Epoch 58/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 2857.8193 - val_loss: 2175.9216\n",
      "Epoch 59/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2828.6721 - val_loss: 2150.4834\n",
      "Epoch 60/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2799.7654 - val_loss: 2125.2717\n",
      "Epoch 61/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2771.0955 - val_loss: 2100.2854\n",
      "Epoch 62/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2742.6604 - val_loss: 2075.5210\n",
      "Epoch 63/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 2714.4575 - val_loss: 2050.9775\n",
      "Epoch 64/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2686.4844 - val_loss: 2026.6517\n",
      "Epoch 65/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 2658.7388 - val_loss: 2002.5413\n",
      "Epoch 66/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 2631.2180 - val_loss: 1978.6455\n",
      "Epoch 67/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 2603.9204 - val_loss: 1954.9606\n",
      "Epoch 68/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 2576.8440 - val_loss: 1921.1115\n",
      "Epoch 69/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2536.7083 - val_loss: 1894.5839\n",
      "Epoch 70/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2505.9482 - val_loss: 1867.9659\n",
      "Epoch 71/500\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 2475.6440 - val_loss: 1842.0535\n",
      "Epoch 72/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 2446.1150 - val_loss: 1816.7924\n",
      "Epoch 73/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 2417.2588 - val_loss: 1792.0760\n",
      "Epoch 74/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 2388.9636 - val_loss: 1767.8223\n",
      "Epoch 75/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2361.1470 - val_loss: 1743.9736\n",
      "Epoch 76/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2333.7534 - val_loss: 1720.4900\n",
      "Epoch 77/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 2306.7417 - val_loss: 1697.3413\n",
      "Epoch 78/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2280.0818 - val_loss: 1674.5057\n",
      "Epoch 79/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2253.7510 - val_loss: 1651.9653\n",
      "Epoch 80/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2227.7317 - val_loss: 1629.7063\n",
      "Epoch 81/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2202.0093 - val_loss: 1607.7177\n",
      "Epoch 82/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2176.5718 - val_loss: 1585.9896\n",
      "Epoch 83/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 2151.4097 - val_loss: 1564.5143\n",
      "Epoch 84/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 2126.5134 - val_loss: 1543.2842\n",
      "Epoch 85/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 2101.8765 - val_loss: 1522.2935\n",
      "Epoch 86/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 2077.4924 - val_loss: 1501.5366\n",
      "Epoch 87/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 2053.3550 - val_loss: 1481.0089\n",
      "Epoch 88/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2029.4589 - val_loss: 1460.7052\n",
      "Epoch 89/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2005.7993 - val_loss: 1440.6227\n",
      "Epoch 90/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1982.3726 - val_loss: 1420.7557\n",
      "Epoch 91/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1959.1743 - val_loss: 1401.1033\n",
      "Epoch 92/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1936.2012 - val_loss: 1381.6606\n",
      "Epoch 93/500\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 1913.4497 - val_loss: 1362.4247\n",
      "Epoch 94/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 1890.9161 - val_loss: 1343.3933\n",
      "Epoch 95/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 1868.5983 - val_loss: 1324.5637\n",
      "Epoch 96/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 1846.4934 - val_loss: 1305.9337\n",
      "Epoch 97/500\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 1824.5986 - val_loss: 1287.4999\n",
      "Epoch 98/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1802.9116 - val_loss: 1269.2610\n",
      "Epoch 99/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1781.4298 - val_loss: 1251.2144\n",
      "Epoch 100/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1760.1505 - val_loss: 1233.3580\n",
      "Epoch 101/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1739.0720 - val_loss: 1215.6903\n",
      "Epoch 102/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1718.1931 - val_loss: 1198.2087\n",
      "Epoch 103/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1697.5100 - val_loss: 1180.9120\n",
      "Epoch 104/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 1677.0223 - val_loss: 1163.7982\n",
      "Epoch 105/500\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 1656.7279 - val_loss: 1146.8661\n",
      "Epoch 106/500\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 1636.6243 - val_loss: 1130.1128\n",
      "Epoch 107/500\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 1616.7098 - val_loss: 1113.5370\n",
      "Epoch 108/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 1596.9836 - val_loss: 1097.1379\n",
      "Epoch 109/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 1577.4434 - val_loss: 1080.9133\n",
      "Epoch 110/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1558.0875 - val_loss: 1064.8619\n",
      "Epoch 111/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1538.9149 - val_loss: 1048.9824\n",
      "Epoch 112/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 1519.9236 - val_loss: 1033.2726\n",
      "Epoch 113/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 1501.1122 - val_loss: 1017.7321\n",
      "Epoch 114/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1482.4791 - val_loss: 1002.3589\n",
      "Epoch 115/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 1464.0234 - val_loss: 987.1517\n",
      "Epoch 116/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 1445.7433 - val_loss: 972.1090\n",
      "Epoch 117/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1427.6375 - val_loss: 957.2301\n",
      "Epoch 118/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1409.7048 - val_loss: 942.5134\n",
      "Epoch 119/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1391.9434 - val_loss: 927.9574\n",
      "Epoch 120/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1374.3527 - val_loss: 913.5615\n",
      "Epoch 121/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1356.9308 - val_loss: 899.3231\n",
      "Epoch 122/500\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 1339.6765 - val_loss: 885.2428\n",
      "Epoch 123/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1322.5891 - val_loss: 871.3177\n",
      "Epoch 124/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1305.6670 - val_loss: 857.5473\n",
      "Epoch 125/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1288.9092 - val_loss: 843.9311\n",
      "Epoch 126/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1272.3140 - val_loss: 830.4672\n",
      "Epoch 127/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1255.8810 - val_loss: 817.1540\n",
      "Epoch 128/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1239.6083 - val_loss: 803.9918\n",
      "Epoch 129/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1223.4954 - val_loss: 790.9785\n",
      "Epoch 130/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1207.5402 - val_loss: 778.1127\n",
      "Epoch 131/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1191.7430 - val_loss: 765.3942\n",
      "Epoch 132/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1176.1019 - val_loss: 752.8212\n",
      "Epoch 133/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1160.6158 - val_loss: 740.3937\n",
      "Epoch 134/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1145.2836 - val_loss: 728.1090\n",
      "Epoch 135/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1130.1040 - val_loss: 715.9667\n",
      "Epoch 136/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1115.0764 - val_loss: 703.9665\n",
      "Epoch 137/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1100.1996 - val_loss: 692.1066\n",
      "Epoch 138/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1085.4722 - val_loss: 680.3862\n",
      "Epoch 139/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1070.8942 - val_loss: 668.8044\n",
      "Epoch 140/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1056.4633 - val_loss: 657.3596\n",
      "Epoch 141/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1042.1790 - val_loss: 646.0518\n",
      "Epoch 142/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1028.0408 - val_loss: 634.8788\n",
      "Epoch 143/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1014.0467 - val_loss: 623.8403\n",
      "Epoch 144/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1000.1964 - val_loss: 612.9352\n",
      "Epoch 145/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 986.4887 - val_loss: 602.1628\n",
      "Epoch 146/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 972.9229 - val_loss: 591.5215\n",
      "Epoch 147/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 959.4974 - val_loss: 581.0109\n",
      "Epoch 148/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 946.2119 - val_loss: 570.6306\n",
      "Epoch 149/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 933.0657 - val_loss: 560.3782\n",
      "Epoch 150/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 920.0570 - val_loss: 550.2537\n",
      "Epoch 151/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 907.1852 - val_loss: 540.2560\n",
      "Epoch 152/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 894.4492 - val_loss: 530.3835\n",
      "Epoch 153/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 881.8477 - val_loss: 520.6364\n",
      "Epoch 154/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 869.3811 - val_loss: 511.0128\n",
      "Epoch 155/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 857.0473 - val_loss: 501.5121\n",
      "Epoch 156/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 844.8455 - val_loss: 492.1342\n",
      "Epoch 157/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 832.7755 - val_loss: 482.8765\n",
      "Epoch 158/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 820.8356 - val_loss: 473.7391\n",
      "Epoch 159/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 809.0249 - val_loss: 464.7212\n",
      "Epoch 160/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 797.3430 - val_loss: 455.8219\n",
      "Epoch 161/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 785.7891 - val_loss: 447.0398\n",
      "Epoch 162/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 774.3616 - val_loss: 438.3746\n",
      "Epoch 163/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 763.0604 - val_loss: 429.8250\n",
      "Epoch 164/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 751.8837 - val_loss: 421.3901\n",
      "Epoch 165/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 740.8315 - val_loss: 413.0688\n",
      "Epoch 166/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 729.9023 - val_loss: 404.8610\n",
      "Epoch 167/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 719.0958 - val_loss: 396.7655\n",
      "Epoch 168/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 708.4107 - val_loss: 388.7808\n",
      "Epoch 169/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 697.8461 - val_loss: 380.9066\n",
      "Epoch 170/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 687.4015 - val_loss: 373.1423\n",
      "Epoch 171/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 677.0756 - val_loss: 365.4865\n",
      "Epoch 172/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 666.8679 - val_loss: 357.9379\n",
      "Epoch 173/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 656.7773 - val_loss: 350.4968\n",
      "Epoch 174/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 646.8031 - val_loss: 343.1617\n",
      "Epoch 175/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 636.9443 - val_loss: 335.9318\n",
      "Epoch 176/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 627.2001 - val_loss: 328.8061\n",
      "Epoch 177/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 617.5700 - val_loss: 321.7840\n",
      "Epoch 178/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 608.0529 - val_loss: 314.8645\n",
      "Epoch 179/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 598.6475 - val_loss: 308.0473\n",
      "Epoch 180/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 589.3539 - val_loss: 301.3307\n",
      "Epoch 181/500\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 580.1705 - val_loss: 294.7141\n",
      "Epoch 182/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 571.0970 - val_loss: 288.1972\n",
      "Epoch 183/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 562.1323 - val_loss: 281.7782\n",
      "Epoch 184/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 553.2758 - val_loss: 275.4576\n",
      "Epoch 185/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 544.5266 - val_loss: 269.2333\n",
      "Epoch 186/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 535.8834 - val_loss: 263.1053\n",
      "Epoch 187/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 527.3461 - val_loss: 257.0721\n",
      "Epoch 188/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 518.9136 - val_loss: 251.1331\n",
      "Epoch 189/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 510.5848 - val_loss: 245.2883\n",
      "Epoch 190/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 502.3594 - val_loss: 239.5356\n",
      "Epoch 191/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 494.2364 - val_loss: 233.8753\n",
      "Epoch 192/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 486.2149 - val_loss: 228.3058\n",
      "Epoch 193/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 478.2940 - val_loss: 222.8264\n",
      "Epoch 194/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 470.4732 - val_loss: 217.4365\n",
      "Epoch 195/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 462.7516 - val_loss: 212.1353\n",
      "Epoch 196/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 455.1285 - val_loss: 206.9220\n",
      "Epoch 197/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 447.6029 - val_loss: 201.7957\n",
      "Epoch 198/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 440.1742 - val_loss: 196.7556\n",
      "Epoch 199/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 432.8416 - val_loss: 191.8010\n",
      "Epoch 200/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 425.6042 - val_loss: 186.9312\n",
      "Epoch 201/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 418.4613 - val_loss: 182.1449\n",
      "Epoch 202/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 411.4120 - val_loss: 177.4419\n",
      "Epoch 203/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 404.4558 - val_loss: 172.8209\n",
      "Epoch 204/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 397.5915 - val_loss: 168.2816\n",
      "Epoch 205/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 390.8189 - val_loss: 163.8230\n",
      "Epoch 206/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 384.1368 - val_loss: 159.4443\n",
      "Epoch 207/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 377.5448 - val_loss: 155.1448\n",
      "Epoch 208/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 371.0417 - val_loss: 150.9234\n",
      "Epoch 209/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 364.6270 - val_loss: 146.7799\n",
      "Epoch 210/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 358.2998 - val_loss: 142.7129\n",
      "Epoch 211/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 352.0594 - val_loss: 138.7220\n",
      "Epoch 212/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 345.9049 - val_loss: 134.8061\n",
      "Epoch 213/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 339.8359 - val_loss: 130.9651\n",
      "Epoch 214/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 333.8515 - val_loss: 127.1975\n",
      "Epoch 215/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 327.9508 - val_loss: 123.5031\n",
      "Epoch 216/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 322.1333 - val_loss: 119.8806\n",
      "Epoch 217/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 316.3981 - val_loss: 116.3296\n",
      "Epoch 218/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 310.7444 - val_loss: 112.8495\n",
      "Epoch 219/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 305.1715 - val_loss: 109.4388\n",
      "Epoch 220/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 299.6788 - val_loss: 106.0980\n",
      "Epoch 221/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 294.2654 - val_loss: 102.8249\n",
      "Epoch 222/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 288.9305 - val_loss: 99.6196\n",
      "Epoch 223/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 283.6737 - val_loss: 96.4812\n",
      "Epoch 224/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 278.4941 - val_loss: 93.4091\n",
      "Epoch 225/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 273.3908 - val_loss: 90.4023\n",
      "Epoch 226/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 268.3633 - val_loss: 87.4600\n",
      "Epoch 227/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 263.4107 - val_loss: 84.5818\n",
      "Epoch 228/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 258.5323 - val_loss: 81.7666\n",
      "Epoch 229/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 253.7276 - val_loss: 79.0142\n",
      "Epoch 230/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 248.9956 - val_loss: 76.3233\n",
      "Epoch 231/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 244.3361 - val_loss: 73.6935\n",
      "Epoch 232/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 239.7477 - val_loss: 71.1238\n",
      "Epoch 233/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 235.2299 - val_loss: 68.6135\n",
      "Epoch 234/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 230.7823 - val_loss: 66.1622\n",
      "Epoch 235/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 226.4038 - val_loss: 63.7691\n",
      "Epoch 236/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 222.0941 - val_loss: 61.4332\n",
      "Epoch 237/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 217.8521 - val_loss: 59.1538\n",
      "Epoch 238/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 213.6773 - val_loss: 56.9305\n",
      "Epoch 239/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 209.5690 - val_loss: 54.7621\n",
      "Epoch 240/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 205.5264 - val_loss: 52.6487\n",
      "Epoch 241/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 201.5490 - val_loss: 50.5886\n",
      "Epoch 242/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 197.6359 - val_loss: 48.5819\n",
      "Epoch 243/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 193.7866 - val_loss: 46.6275\n",
      "Epoch 244/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 190.0004 - val_loss: 44.7248\n",
      "Epoch 245/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 186.2762 - val_loss: 42.8730\n",
      "Epoch 246/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 182.6139 - val_loss: 41.0716\n",
      "Epoch 247/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 179.0126 - val_loss: 39.3198\n",
      "Epoch 248/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 175.4714 - val_loss: 37.6167\n",
      "Epoch 249/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 171.9900 - val_loss: 35.9620\n",
      "Epoch 250/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 168.5675 - val_loss: 34.3549\n",
      "Epoch 251/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 165.2034 - val_loss: 32.7947\n",
      "Epoch 252/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 161.8969 - val_loss: 31.2805\n",
      "Epoch 253/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 158.6475 - val_loss: 29.8120\n",
      "Epoch 254/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 155.4543 - val_loss: 28.3883\n",
      "Epoch 255/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 152.3168 - val_loss: 27.0086\n",
      "Epoch 256/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 149.2341 - val_loss: 25.6725\n",
      "Epoch 257/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 146.2058 - val_loss: 24.3793\n",
      "Epoch 258/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 143.2312 - val_loss: 23.1282\n",
      "Epoch 259/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 140.3097 - val_loss: 21.9187\n",
      "Epoch 260/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 137.4407 - val_loss: 20.7501\n",
      "Epoch 261/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 134.6235 - val_loss: 19.6215\n",
      "Epoch 262/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 131.8572 - val_loss: 18.5328\n",
      "Epoch 263/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 129.1416 - val_loss: 17.4829\n",
      "Epoch 264/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 126.4758 - val_loss: 16.4712\n",
      "Epoch 265/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 123.8592 - val_loss: 15.4973\n",
      "Epoch 266/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 121.2912 - val_loss: 14.5603\n",
      "Epoch 267/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 118.7713 - val_loss: 13.6599\n",
      "Epoch 268/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 116.2988 - val_loss: 12.7951\n",
      "Epoch 269/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 113.8729 - val_loss: 11.9655\n",
      "Epoch 270/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 111.4932 - val_loss: 11.1705\n",
      "Epoch 271/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 109.1589 - val_loss: 10.4093\n",
      "Epoch 272/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 106.8697 - val_loss: 9.6815\n",
      "Epoch 273/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 104.6247 - val_loss: 8.9863\n",
      "Epoch 274/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 102.4236 - val_loss: 8.3233\n",
      "Epoch 275/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 100.2655 - val_loss: 7.6918\n",
      "Epoch 276/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 98.1500 - val_loss: 7.0912\n",
      "Epoch 277/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 96.0763 - val_loss: 6.5207\n",
      "Epoch 278/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 94.0440 - val_loss: 5.9802\n",
      "Epoch 279/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 92.0524 - val_loss: 5.4687\n",
      "Epoch 280/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 90.1011 - val_loss: 4.9858\n",
      "Epoch 281/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 88.1894 - val_loss: 4.5309\n",
      "Epoch 282/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 86.3167 - val_loss: 4.1033\n",
      "Epoch 283/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 84.4825 - val_loss: 3.7028\n",
      "Epoch 284/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 82.6863 - val_loss: 3.3284\n",
      "Epoch 285/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 80.9274 - val_loss: 2.9798\n",
      "Epoch 286/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 79.2054 - val_loss: 2.6564\n",
      "Epoch 287/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 77.5196 - val_loss: 2.3576\n",
      "Epoch 288/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 75.8695 - val_loss: 2.0829\n",
      "Epoch 289/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 74.2546 - val_loss: 1.8318\n",
      "Epoch 290/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 72.6742 - val_loss: 1.6036\n",
      "Epoch 291/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 71.1279 - val_loss: 1.3980\n",
      "Epoch 292/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 69.6152 - val_loss: 1.2143\n",
      "Epoch 293/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 68.1355 - val_loss: 1.0520\n",
      "Epoch 294/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 66.6883 - val_loss: 0.9107\n",
      "Epoch 295/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 65.2730 - val_loss: 0.7898\n",
      "Epoch 296/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 63.8893 - val_loss: 0.6887\n",
      "Epoch 297/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 62.5365 - val_loss: 0.6071\n",
      "Epoch 298/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 61.2139 - val_loss: 0.5444\n",
      "Epoch 299/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 59.9213 - val_loss: 0.5001\n",
      "Epoch 300/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 58.6583 - val_loss: 0.4737\n",
      "Epoch 301/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 57.4241 - val_loss: 0.4647\n",
      "Epoch 302/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 56.2183 - val_loss: 0.4728\n",
      "Epoch 303/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 55.0405 - val_loss: 0.4973\n",
      "Epoch 304/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 53.8901 - val_loss: 0.5379\n",
      "Epoch 305/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 52.7668 - val_loss: 0.5940\n",
      "Epoch 306/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 51.6699 - val_loss: 0.6653\n",
      "Epoch 307/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 50.5991 - val_loss: 0.7512\n",
      "Epoch 308/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 49.5538 - val_loss: 0.8513\n",
      "Epoch 309/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 48.5338 - val_loss: 0.9653\n",
      "Epoch 310/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 47.5383 - val_loss: 1.0925\n",
      "Epoch 311/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 46.5671 - val_loss: 1.2328\n",
      "Epoch 312/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 45.6196 - val_loss: 1.3854\n",
      "Epoch 313/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 44.6955 - val_loss: 1.5502\n",
      "Epoch 314/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 43.7943 - val_loss: 1.7266\n",
      "Epoch 315/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 42.9155 - val_loss: 1.9143\n",
      "Epoch 316/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 42.0588 - val_loss: 2.1129\n",
      "Epoch 317/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 41.2236 - val_loss: 2.3220\n",
      "Epoch 318/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 40.4095 - val_loss: 2.5411\n",
      "Epoch 319/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 39.6162 - val_loss: 2.7699\n",
      "Epoch 320/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 38.8433 - val_loss: 3.0080\n",
      "Epoch 321/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 38.0904 - val_loss: 3.2550\n",
      "Epoch 322/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 37.3571 - val_loss: 3.5107\n",
      "Epoch 323/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 36.6428 - val_loss: 3.7745\n",
      "Epoch 324/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 35.9474 - val_loss: 4.0462\n",
      "Epoch 325/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 35.2703 - val_loss: 4.3254\n",
      "Epoch 326/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 34.6112 - val_loss: 4.6118\n",
      "Epoch 327/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 33.9698 - val_loss: 4.9050\n",
      "Epoch 328/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 33.3457 - val_loss: 5.2047\n",
      "Epoch 329/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 32.7384 - val_loss: 5.5105\n",
      "Epoch 330/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 32.1477 - val_loss: 5.8222\n",
      "Epoch 331/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 31.5732 - val_loss: 6.1394\n",
      "Epoch 332/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 31.0145 - val_loss: 6.4618\n",
      "Epoch 333/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 30.4713 - val_loss: 6.7891\n",
      "Epoch 334/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 29.9432 - val_loss: 7.1211\n",
      "Epoch 335/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 29.4299 - val_loss: 7.4573\n",
      "Epoch 336/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 28.9312 - val_loss: 7.7976\n",
      "Epoch 337/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 28.4465 - val_loss: 8.1417\n",
      "Epoch 338/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 27.9757 - val_loss: 8.4892\n",
      "Epoch 339/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 27.5183 - val_loss: 8.8400\n",
      "Epoch 340/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 27.0741 - val_loss: 9.1936\n",
      "Epoch 341/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 26.6428 - val_loss: 9.5500\n",
      "Epoch 342/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 26.2241 - val_loss: 9.9088\n",
      "Epoch 343/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 25.8177 - val_loss: 10.2699\n",
      "Epoch 344/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 25.4232 - val_loss: 10.6328\n",
      "Epoch 345/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 25.0405 - val_loss: 10.9974\n",
      "Epoch 346/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 24.6693 - val_loss: 11.3635\n",
      "Epoch 347/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 24.3091 - val_loss: 11.7309\n",
      "Epoch 348/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 23.9599 - val_loss: 12.0992\n",
      "Epoch 349/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 23.6212 - val_loss: 12.4685\n",
      "Epoch 350/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 23.2928 - val_loss: 12.8383\n",
      "Epoch 351/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 22.9745 - val_loss: 13.2085\n",
      "Epoch 352/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 22.6661 - val_loss: 13.5790\n",
      "Epoch 353/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 22.3671 - val_loss: 13.9495\n",
      "Epoch 354/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 22.0776 - val_loss: 14.3199\n",
      "Epoch 355/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 21.7970 - val_loss: 14.6899\n",
      "Epoch 356/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 21.5253 - val_loss: 15.0593\n",
      "Epoch 357/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 21.2622 - val_loss: 15.4281\n",
      "Epoch 358/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 21.0075 - val_loss: 15.7959\n",
      "Epoch 359/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 20.7609 - val_loss: 16.1628\n",
      "Epoch 360/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 20.5223 - val_loss: 16.5284\n",
      "Epoch 361/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 20.2915 - val_loss: 16.8928\n",
      "Epoch 362/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 20.0681 - val_loss: 17.2558\n",
      "Epoch 363/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 19.8519 - val_loss: 17.6171\n",
      "Epoch 364/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 19.6429 - val_loss: 17.9766\n",
      "Epoch 365/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 19.4408 - val_loss: 18.3344\n",
      "Epoch 366/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 19.2454 - val_loss: 18.6901\n",
      "Epoch 367/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 19.0564 - val_loss: 19.0437\n",
      "Epoch 368/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 18.8738 - val_loss: 19.3950\n",
      "Epoch 369/500\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 18.6973 - val_loss: 19.7440\n",
      "Epoch 370/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 18.5268 - val_loss: 20.0906\n",
      "Epoch 371/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 18.3622 - val_loss: 20.4346\n",
      "Epoch 372/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 18.2031 - val_loss: 20.7759\n",
      "Epoch 373/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 18.0495 - val_loss: 21.1145\n",
      "Epoch 374/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 17.9011 - val_loss: 21.4502\n",
      "Epoch 375/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 17.7580 - val_loss: 21.7830\n",
      "Epoch 376/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 17.6198 - val_loss: 22.1128\n",
      "Epoch 377/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 17.4864 - val_loss: 22.4395\n",
      "Epoch 378/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 17.3577 - val_loss: 22.7630\n",
      "Epoch 379/500\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 17.2335 - val_loss: 23.0834\n",
      "Epoch 380/500\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 17.1137 - val_loss: 23.4004\n",
      "Epoch 381/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 16.9982 - val_loss: 23.7141\n",
      "Epoch 382/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 16.8868 - val_loss: 24.0244\n",
      "Epoch 383/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 16.7794 - val_loss: 24.3311\n",
      "Epoch 384/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 16.6759 - val_loss: 24.6344\n",
      "Epoch 385/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 16.5761 - val_loss: 24.9341\n",
      "Epoch 386/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 16.4799 - val_loss: 25.2302\n",
      "Epoch 387/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 16.3872 - val_loss: 25.5227\n",
      "Epoch 388/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 16.2980 - val_loss: 25.8114\n",
      "Epoch 389/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 16.2119 - val_loss: 26.0965\n",
      "Epoch 390/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 16.1291 - val_loss: 26.3779\n",
      "Epoch 391/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 16.0493 - val_loss: 26.6553\n",
      "Epoch 392/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 15.9725 - val_loss: 26.9289\n",
      "Epoch 393/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 15.8986 - val_loss: 27.1990\n",
      "Epoch 394/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 15.8274 - val_loss: 27.4649\n",
      "Epoch 395/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 15.7590 - val_loss: 27.7272\n",
      "Epoch 396/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 15.6930 - val_loss: 27.9856\n",
      "Epoch 397/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 15.6296 - val_loss: 28.2401\n",
      "Epoch 398/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 15.5686 - val_loss: 28.4906\n",
      "Epoch 399/500\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 15.5099 - val_loss: 28.7373\n",
      "Epoch 400/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 15.4534 - val_loss: 28.9803\n",
      "Epoch 401/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 15.3990 - val_loss: 29.2192\n",
      "Epoch 402/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 15.3468 - val_loss: 29.4544\n",
      "Epoch 403/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 15.2966 - val_loss: 29.6856\n",
      "Epoch 404/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 15.2483 - val_loss: 29.9130\n",
      "Epoch 405/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 15.2019 - val_loss: 30.1365\n",
      "Epoch 406/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 15.1573 - val_loss: 30.3560\n",
      "Epoch 407/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 15.1144 - val_loss: 30.5720\n",
      "Epoch 408/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 15.0732 - val_loss: 30.7839\n",
      "Epoch 409/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 15.0336 - val_loss: 30.9922\n",
      "Epoch 410/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.9955 - val_loss: 31.1965\n",
      "Epoch 411/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 14.9590 - val_loss: 31.3974\n",
      "Epoch 412/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.9238 - val_loss: 31.5942\n",
      "Epoch 413/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.8901 - val_loss: 31.7875\n",
      "Epoch 414/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.8577 - val_loss: 31.9770\n",
      "Epoch 415/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.8266 - val_loss: 32.1628\n",
      "Epoch 416/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.7967 - val_loss: 32.3449\n",
      "Epoch 417/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.7680 - val_loss: 32.5235\n",
      "Epoch 418/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.7405 - val_loss: 32.6985\n",
      "Epoch 419/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.7140 - val_loss: 32.8698\n",
      "Epoch 420/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 14.6886 - val_loss: 33.0380\n",
      "Epoch 421/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.6642 - val_loss: 33.2022\n",
      "Epoch 422/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 14.6409 - val_loss: 33.3632\n",
      "Epoch 423/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.6184 - val_loss: 33.5210\n",
      "Epoch 424/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.5968 - val_loss: 33.6749\n",
      "Epoch 425/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.5762 - val_loss: 33.8257\n",
      "Epoch 426/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.5563 - val_loss: 33.9735\n",
      "Epoch 427/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.5372 - val_loss: 34.1178\n",
      "Epoch 428/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 14.5190 - val_loss: 34.2590\n",
      "Epoch 429/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.5014 - val_loss: 34.3969\n",
      "Epoch 430/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.4846 - val_loss: 34.5318\n",
      "Epoch 431/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.4684 - val_loss: 34.6634\n",
      "Epoch 432/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.4529 - val_loss: 34.7922\n",
      "Epoch 433/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.4380 - val_loss: 34.9175\n",
      "Epoch 434/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 14.4238 - val_loss: 35.0402\n",
      "Epoch 435/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.4101 - val_loss: 35.1597\n",
      "Epoch 436/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.3970 - val_loss: 35.2767\n",
      "Epoch 437/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.3844 - val_loss: 35.3907\n",
      "Epoch 438/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 14.3723 - val_loss: 35.5021\n",
      "Epoch 439/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.3607 - val_loss: 35.6107\n",
      "Epoch 440/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 14.3495 - val_loss: 35.7163\n",
      "Epoch 441/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.3389 - val_loss: 35.8196\n",
      "Epoch 442/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 14.3286 - val_loss: 35.9200\n",
      "Epoch 443/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.3188 - val_loss: 36.0179\n",
      "Epoch 444/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.3094 - val_loss: 36.1134\n",
      "Epoch 445/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.3003 - val_loss: 36.2062\n",
      "Epoch 446/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.2917 - val_loss: 36.2965\n",
      "Epoch 447/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.2834 - val_loss: 36.3848\n",
      "Epoch 448/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.2753 - val_loss: 36.4704\n",
      "Epoch 449/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.2677 - val_loss: 36.5540\n",
      "Epoch 450/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.2604 - val_loss: 36.6349\n",
      "Epoch 451/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 14.2533 - val_loss: 36.7139\n",
      "Epoch 452/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.2466 - val_loss: 36.7905\n",
      "Epoch 453/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.2401 - val_loss: 36.8653\n",
      "Epoch 454/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.2339 - val_loss: 36.9377\n",
      "Epoch 455/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.2279 - val_loss: 37.0083\n",
      "Epoch 456/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.2222 - val_loss: 37.0767\n",
      "Epoch 457/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.2167 - val_loss: 37.1433\n",
      "Epoch 458/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.2115 - val_loss: 37.2080\n",
      "Epoch 459/500\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 14.2064 - val_loss: 37.2708\n",
      "Epoch 460/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.2015 - val_loss: 37.3316\n",
      "Epoch 461/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1969 - val_loss: 37.3907\n",
      "Epoch 462/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1924 - val_loss: 37.4481\n",
      "Epoch 463/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 14.1882 - val_loss: 37.5037\n",
      "Epoch 464/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 14.1841 - val_loss: 37.5577\n",
      "Epoch 465/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1802 - val_loss: 37.6101\n",
      "Epoch 466/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1764 - val_loss: 37.6608\n",
      "Epoch 467/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 14.1728 - val_loss: 37.7101\n",
      "Epoch 468/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 14.1693 - val_loss: 37.7576\n",
      "Epoch 469/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 14.1660 - val_loss: 37.8039\n",
      "Epoch 470/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1628 - val_loss: 37.8485\n",
      "Epoch 471/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1597 - val_loss: 37.8919\n",
      "Epoch 472/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1568 - val_loss: 37.9337\n",
      "Epoch 473/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1540 - val_loss: 37.9742\n",
      "Epoch 474/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1513 - val_loss: 38.0136\n",
      "Epoch 475/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1487 - val_loss: 38.0517\n",
      "Epoch 476/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 14.1462 - val_loss: 38.0881\n",
      "Epoch 477/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1439 - val_loss: 38.1237\n",
      "Epoch 478/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1417 - val_loss: 38.1581\n",
      "Epoch 479/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1395 - val_loss: 38.1913\n",
      "Epoch 480/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1374 - val_loss: 38.2233\n",
      "Epoch 481/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1354 - val_loss: 38.2544\n",
      "Epoch 482/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1335 - val_loss: 38.2843\n",
      "Epoch 483/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 14.1316 - val_loss: 38.3130\n",
      "Epoch 484/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1299 - val_loss: 38.3407\n",
      "Epoch 485/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 14.1282 - val_loss: 38.3678\n",
      "Epoch 486/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 14.1266 - val_loss: 38.3937\n",
      "Epoch 487/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 14.1251 - val_loss: 38.4186\n",
      "Epoch 488/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1236 - val_loss: 38.4428\n",
      "Epoch 489/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1222 - val_loss: 38.4663\n",
      "Epoch 490/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 14.1209 - val_loss: 38.4887\n",
      "Epoch 491/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1196 - val_loss: 38.5102\n",
      "Epoch 492/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1184 - val_loss: 38.5312\n",
      "Epoch 493/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1172 - val_loss: 38.5513\n",
      "Epoch 494/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1161 - val_loss: 38.5708\n",
      "Epoch 495/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1150 - val_loss: 38.5893\n",
      "Epoch 496/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1140 - val_loss: 38.6074\n",
      "Epoch 497/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 14.1130 - val_loss: 38.6245\n",
      "Epoch 498/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1121 - val_loss: 38.6412\n",
      "Epoch 499/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1112 - val_loss: 38.6573\n",
      "Epoch 500/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.1104 - val_loss: 38.6727\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6465, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0029, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0003, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0168, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3581, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 0.1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 424ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.64105976e+01, 6.63825864e+01, 6.63545752e+01, 6.63265640e+01,\n",
       "        6.62985527e+01, 6.62705416e+01, 6.62425303e+01, 6.62145191e+01,\n",
       "        6.61865079e+01, 6.61584967e+01, 6.61304855e+01, 6.61024743e+01,\n",
       "        6.60744631e+01, 6.60464519e+01, 6.60184407e+01, 6.59808590e+01,\n",
       "        6.59248366e+01, 6.58688142e+01, 6.58127918e+01, 6.57567694e+01,\n",
       "        6.57007470e+01, 6.56447246e+01, 6.55887022e+01, 6.55326797e+01,\n",
       "        6.54766573e+01, 6.54206349e+01, 6.53646125e+01, 6.53085901e+01,\n",
       "        6.52525677e+01, 6.51965453e+01, 6.51405229e+01, 6.50845005e+01,\n",
       "        6.50284781e+01, 6.49724557e+01, 6.49164332e+01, 6.48604108e+01,\n",
       "        6.48043884e+01, 6.47483660e+01, 6.46923436e+01, 6.46363212e+01,\n",
       "        6.45802988e+01, 6.45242764e+01, 6.44682540e+01, 6.44122316e+01,\n",
       "        6.43562091e+01, 6.43001867e+01, 6.42441643e+01, 6.41881419e+01,\n",
       "        6.41321195e+01, 6.40760971e+01, 6.40200747e+01, 6.39874183e+01,\n",
       "        6.39678105e+01, 6.39482026e+01, 6.39285948e+01, 6.39089869e+01,\n",
       "        6.38893791e+01, 6.38697712e+01, 6.38501634e+01, 6.38305556e+01,\n",
       "        6.38109477e+01, 6.37913399e+01, 6.37717320e+01, 6.37521242e+01,\n",
       "        6.37325163e+01, 6.37129085e+01, 6.36933006e+01, 6.36736928e+01,\n",
       "        6.36540850e+01, 6.36344771e+01, 6.36148693e+01, 6.35952614e+01,\n",
       "        6.35756536e+01, 6.35560458e+01, 6.35364379e+01, 6.35168301e+01,\n",
       "        6.34972222e+01, 6.34776144e+01, 6.34580065e+01, 6.34383987e+01,\n",
       "        7.04742966e+01, 0.00000000e+00, 4.67904925e-01, 0.00000000e+00,\n",
       "        6.00856692e-02, 0.00000000e+00, 1.74641982e-01, 9.60636795e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.38679010e-01, 6.18165255e-01,\n",
       "        3.81849140e-01, 1.59787571e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.74751890e-01, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([62.33116246, 62.33022876, 62.32929505, 62.32836134, 62.32742764,\n",
       "       62.32649393, 62.32556022, 62.32462652, 62.32369281, 62.3227591 ,\n",
       "       62.3218254 , 62.32089169, 62.31995798, 62.31902428, 62.31809057,\n",
       "       62.31715686, 62.31622316, 62.31528945, 62.31435574, 62.31342204,\n",
       "       62.31248833, 62.31155462, 62.31062092, 62.30968721, 62.3087535 ,\n",
       "       62.30781979, 62.30688609, 62.30595238, 62.30501867, 62.30408497,\n",
       "       62.30315126, 62.30221755, 62.30128385, 62.30035014, 62.29591503,\n",
       "       62.28937908, 62.28284314, 62.27630719, 62.26977124, 62.26323529,\n",
       "       62.25669935, 62.2501634 , 62.24362745, 62.2370915 , 62.23055556,\n",
       "       62.22401961, 62.21748366, 62.21094771, 62.20441176, 62.19787582,\n",
       "       62.19133987, 62.18480392, 62.17826797, 62.17173203, 62.16519608,\n",
       "       62.15866013, 62.15212418, 62.14558824, 62.13905229, 62.13251634,\n",
       "       62.12598039, 62.11944444, 62.1129085 , 62.10637255, 62.0998366 ,\n",
       "       62.09330065, 62.08676471, 62.08022876, 62.07369281, 62.06715686,\n",
       "       62.06062092, 62.05408497, 62.04754902, 62.04101307, 62.03447712,\n",
       "       62.02794118, 62.02140523, 62.01486928, 62.00833333, 62.00179739,\n",
       "       61.99526144, 61.98872549, 61.98218954, 61.97565359, 61.96911765,\n",
       "       61.9625817 , 61.95604575, 61.9495098 , 61.94297386, 61.93643791,\n",
       "       61.92990196, 61.92336601, 61.91683007, 61.91029412, 61.90375817,\n",
       "       61.89722222, 61.89068627, 61.88415033, 61.87761438, 61.87107843])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.99773425589188\n",
      "13.832984223308246\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
