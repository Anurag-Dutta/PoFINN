{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "1445    73.051984\n",
       "1446    73.039846\n",
       "1447    73.027708\n",
       "1448    73.015570\n",
       "1449    73.003431\n",
       "Name: C3, Length: 1450, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c3_interpolated_1350_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "1345     0.000000\n",
       "1346     0.107673\n",
       "1347     0.000000\n",
       "1348     0.000000\n",
       "1349     0.369774\n",
       "Name: C3, Length: 1350, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1350)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.100000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.875910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.651821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90.427731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90.203641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     91.100000  0.000298\n",
       "1     90.875910  0.000298\n",
       "2     90.651821  0.000297\n",
       "3     90.427731  0.000297\n",
       "4     90.203641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.0403  # Value for C0\n",
    "K0 = -0.0012  # Value for K0\n",
    "K1 = -0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0120    # Value for b\n",
    "c = 2.0334    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    91.100000    90.875910    90.651821    90.427731    90.203641   \n",
      "351    90.875910    90.651821    90.427731    90.203641    89.979552   \n",
      "352    90.651821    90.427731    90.203641    89.979552    89.755462   \n",
      "353    90.427731    90.203641    89.979552    89.755462    89.531373   \n",
      "354    90.203641    89.979552    89.755462    89.531373    89.307283   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    89.979552    89.755462    89.531373    89.307283    89.094118  ...   \n",
      "351    89.755462    89.531373    89.307283    89.094118    89.015686  ...   \n",
      "352    89.531373    89.307283    89.094118    89.015686    88.937255  ...   \n",
      "353    89.307283    89.094118    89.015686    88.937255    88.858824  ...   \n",
      "354    89.094118    89.015686    88.937255    88.858824    88.780392  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   81.423343    0.000263   81.404669    0.000263   81.385994    0.000263   \n",
      "351   81.404669    0.000263   81.385994    0.000263   81.367320    0.000262   \n",
      "352   81.385994    0.000263   81.367320    0.000262   81.348646    0.000262   \n",
      "353   81.367320    0.000262   81.348646    0.000262   81.329972    0.000262   \n",
      "354   81.348646    0.000262   81.329972    0.000262   81.311298    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   81.367320    0.000262   81.348646    0.000262  \n",
      "351   81.348646    0.000262   81.329972    0.000262  \n",
      "352   81.329972    0.000262   81.311298    0.000262  \n",
      "353   81.311298    0.000262   81.292624    0.000262  \n",
      "354   81.292624    0.000262   81.273950    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 1, 251) (900, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "12/12 [==============================] - 2s 44ms/step - loss: 6164.2866 - val_loss: 5457.0415\n",
      "Epoch 2/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 6099.7495 - val_loss: 5420.1938\n",
      "Epoch 3/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 6050.2900 - val_loss: 5370.1646\n",
      "Epoch 4/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 5996.9165 - val_loss: 5309.7612\n",
      "Epoch 5/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 5942.4019 - val_loss: 5269.8120\n",
      "Epoch 6/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 5900.1890 - val_loss: 5230.1631\n",
      "Epoch 7/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 5858.3618 - val_loss: 5190.9229\n",
      "Epoch 8/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 5816.9443 - val_loss: 5152.0547\n",
      "Epoch 9/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 5775.8950 - val_loss: 5113.5176\n",
      "Epoch 10/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 5735.1704 - val_loss: 5075.2778\n",
      "Epoch 11/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 5694.7437 - val_loss: 5037.3130\n",
      "Epoch 12/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 5654.5923 - val_loss: 4999.6074\n",
      "Epoch 13/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 5614.7012 - val_loss: 4962.1499\n",
      "Epoch 14/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 5575.0596 - val_loss: 4924.9312\n",
      "Epoch 15/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 5535.6606 - val_loss: 4887.9424\n",
      "Epoch 16/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 5494.8320 - val_loss: 4846.1855\n",
      "Epoch 17/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 5450.7324 - val_loss: 4806.2026\n",
      "Epoch 18/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 5408.3960 - val_loss: 4766.6328\n",
      "Epoch 19/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 5366.6318 - val_loss: 4727.6704\n",
      "Epoch 20/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 5325.4736 - val_loss: 4689.2383\n",
      "Epoch 21/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 5284.8311 - val_loss: 4651.2549\n",
      "Epoch 22/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 5244.6279 - val_loss: 4613.6621\n",
      "Epoch 23/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 5204.8120 - val_loss: 4576.4209\n",
      "Epoch 24/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 5165.3438 - val_loss: 4539.4980\n",
      "Epoch 25/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 5126.1963 - val_loss: 4502.8745\n",
      "Epoch 26/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 5087.3496 - val_loss: 4466.5347\n",
      "Epoch 27/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 5048.7900 - val_loss: 4430.4648\n",
      "Epoch 28/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 5010.5039 - val_loss: 4394.6553\n",
      "Epoch 29/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4972.4810 - val_loss: 4359.0986\n",
      "Epoch 30/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4934.7148 - val_loss: 4323.7866\n",
      "Epoch 31/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4897.1987 - val_loss: 4288.7153\n",
      "Epoch 32/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4859.9258 - val_loss: 4253.8779\n",
      "Epoch 33/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4822.8911 - val_loss: 4219.2710\n",
      "Epoch 34/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4786.0918 - val_loss: 4184.8911\n",
      "Epoch 35/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4749.5225 - val_loss: 4150.7344\n",
      "Epoch 36/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4713.1812 - val_loss: 4116.7974\n",
      "Epoch 37/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4677.0635 - val_loss: 4083.0767\n",
      "Epoch 38/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4641.1665 - val_loss: 4049.5718\n",
      "Epoch 39/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4605.4893 - val_loss: 4016.2786\n",
      "Epoch 40/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4570.0273 - val_loss: 3983.1951\n",
      "Epoch 41/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4534.7808 - val_loss: 3950.3201\n",
      "Epoch 42/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4499.7441 - val_loss: 3917.6514\n",
      "Epoch 43/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4464.9194 - val_loss: 3885.1865\n",
      "Epoch 44/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 4430.3022 - val_loss: 3852.9241\n",
      "Epoch 45/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4395.8921 - val_loss: 3820.8638\n",
      "Epoch 46/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4361.6870 - val_loss: 3789.0015\n",
      "Epoch 47/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4327.6846 - val_loss: 3757.3374\n",
      "Epoch 48/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4293.8838 - val_loss: 3725.8701\n",
      "Epoch 49/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4260.2842 - val_loss: 3694.5979\n",
      "Epoch 50/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4226.8828 - val_loss: 3663.5198\n",
      "Epoch 51/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4193.6802 - val_loss: 3632.6340\n",
      "Epoch 52/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4160.6738 - val_loss: 3601.9397\n",
      "Epoch 53/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4127.8623 - val_loss: 3571.4353\n",
      "Epoch 54/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4095.2444 - val_loss: 3541.1201\n",
      "Epoch 55/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4062.8193 - val_loss: 3510.9932\n",
      "Epoch 56/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4030.5864 - val_loss: 3481.0535\n",
      "Epoch 57/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3998.5444 - val_loss: 3451.2983\n",
      "Epoch 58/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3966.6917 - val_loss: 3421.7285\n",
      "Epoch 59/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3935.0271 - val_loss: 3392.3420\n",
      "Epoch 60/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3903.5508 - val_loss: 3363.1384\n",
      "Epoch 61/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3872.2598 - val_loss: 3334.1167\n",
      "Epoch 62/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3841.1548 - val_loss: 3305.2756\n",
      "Epoch 63/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3810.2341 - val_loss: 3276.6145\n",
      "Epoch 64/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3779.4973 - val_loss: 3248.1316\n",
      "Epoch 65/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3748.9419 - val_loss: 3219.8274\n",
      "Epoch 66/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3718.5698 - val_loss: 3191.7000\n",
      "Epoch 67/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3688.3772 - val_loss: 3163.7490\n",
      "Epoch 68/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3658.3645 - val_loss: 3135.9731\n",
      "Epoch 69/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3628.5315 - val_loss: 3108.3716\n",
      "Epoch 70/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3598.8767 - val_loss: 3080.9441\n",
      "Epoch 71/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3569.3979 - val_loss: 3053.6885\n",
      "Epoch 72/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3540.0962 - val_loss: 3026.6052\n",
      "Epoch 73/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3510.9705 - val_loss: 2999.6926\n",
      "Epoch 74/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3482.0188 - val_loss: 2972.9514\n",
      "Epoch 75/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3453.2412 - val_loss: 2946.3784\n",
      "Epoch 76/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3424.6367 - val_loss: 2919.9751\n",
      "Epoch 77/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3396.2041 - val_loss: 2893.7393\n",
      "Epoch 78/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3367.9434 - val_loss: 2867.6699\n",
      "Epoch 79/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3339.8535 - val_loss: 2841.7683\n",
      "Epoch 80/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3311.9333 - val_loss: 2816.0308\n",
      "Epoch 81/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3284.1824 - val_loss: 2790.4587\n",
      "Epoch 82/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3256.5994 - val_loss: 2765.0508\n",
      "Epoch 83/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3229.1841 - val_loss: 2739.8062\n",
      "Epoch 84/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3201.9365 - val_loss: 2714.7241\n",
      "Epoch 85/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3174.8542 - val_loss: 2689.8032\n",
      "Epoch 86/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3147.9373 - val_loss: 2665.0442\n",
      "Epoch 87/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3121.1853 - val_loss: 2640.4453\n",
      "Epoch 88/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3094.5964 - val_loss: 2616.0061\n",
      "Epoch 89/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3068.1726 - val_loss: 2591.7263\n",
      "Epoch 90/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3041.9104 - val_loss: 2567.6047\n",
      "Epoch 91/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3015.8098 - val_loss: 2543.6404\n",
      "Epoch 92/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2989.8708 - val_loss: 2519.8335\n",
      "Epoch 93/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2964.0916 - val_loss: 2496.1824\n",
      "Epoch 94/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2938.4727 - val_loss: 2472.6863\n",
      "Epoch 95/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2913.0117 - val_loss: 2449.3452\n",
      "Epoch 96/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2887.7097 - val_loss: 2426.1580\n",
      "Epoch 97/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2862.5654 - val_loss: 2403.1250\n",
      "Epoch 98/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2837.5781 - val_loss: 2380.2441\n",
      "Epoch 99/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2812.7466 - val_loss: 2357.5159\n",
      "Epoch 100/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2788.0713 - val_loss: 2334.9390\n",
      "Epoch 101/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2763.5508 - val_loss: 2312.5129\n",
      "Epoch 102/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2739.1843 - val_loss: 2290.2361\n",
      "Epoch 103/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2714.9717 - val_loss: 2268.1101\n",
      "Epoch 104/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2690.9121 - val_loss: 2246.1326\n",
      "Epoch 105/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2667.0051 - val_loss: 2224.3025\n",
      "Epoch 106/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2643.2495 - val_loss: 2202.6206\n",
      "Epoch 107/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2619.6450 - val_loss: 2181.0857\n",
      "Epoch 108/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2596.1912 - val_loss: 2159.6973\n",
      "Epoch 109/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2572.8872 - val_loss: 2138.4539\n",
      "Epoch 110/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2549.7317 - val_loss: 2117.3562\n",
      "Epoch 111/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 2526.7251 - val_loss: 2096.4026\n",
      "Epoch 112/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2503.8665 - val_loss: 2075.5930\n",
      "Epoch 113/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2481.1550 - val_loss: 2054.9268\n",
      "Epoch 114/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2458.5906 - val_loss: 2034.4026\n",
      "Epoch 115/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2436.1716 - val_loss: 2014.0205\n",
      "Epoch 116/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2413.8984 - val_loss: 1993.7799\n",
      "Epoch 117/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2391.7705 - val_loss: 1973.6798\n",
      "Epoch 118/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2369.7861 - val_loss: 1953.7205\n",
      "Epoch 119/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2347.9460 - val_loss: 1933.9001\n",
      "Epoch 120/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2326.2483 - val_loss: 1914.2191\n",
      "Epoch 121/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2304.6934 - val_loss: 1894.6757\n",
      "Epoch 122/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2283.2798 - val_loss: 1875.2714\n",
      "Epoch 123/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2262.0081 - val_loss: 1856.0034\n",
      "Epoch 124/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2240.8772 - val_loss: 1836.8724\n",
      "Epoch 125/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2219.8865 - val_loss: 1817.8767\n",
      "Epoch 126/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2199.0347 - val_loss: 1799.0170\n",
      "Epoch 127/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2178.3218 - val_loss: 1780.2924\n",
      "Epoch 128/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2157.7473 - val_loss: 1761.7015\n",
      "Epoch 129/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2137.3110 - val_loss: 1743.2448\n",
      "Epoch 130/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2117.0112 - val_loss: 1724.9205\n",
      "Epoch 131/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2096.8484 - val_loss: 1706.7294\n",
      "Epoch 132/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2076.8215 - val_loss: 1688.6705\n",
      "Epoch 133/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2056.9307 - val_loss: 1670.7427\n",
      "Epoch 134/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2037.1743 - val_loss: 1652.9460\n",
      "Epoch 135/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2017.5521 - val_loss: 1635.2791\n",
      "Epoch 136/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1998.0642 - val_loss: 1617.7421\n",
      "Epoch 137/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1978.7090 - val_loss: 1600.3344\n",
      "Epoch 138/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1959.4868 - val_loss: 1583.0548\n",
      "Epoch 139/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1940.3965 - val_loss: 1565.9042\n",
      "Epoch 140/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1921.4379 - val_loss: 1548.8802\n",
      "Epoch 141/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 1902.6105 - val_loss: 1531.9836\n",
      "Epoch 142/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1883.9133 - val_loss: 1515.2139\n",
      "Epoch 143/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1865.3459 - val_loss: 1498.5698\n",
      "Epoch 144/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1846.9087 - val_loss: 1482.0507\n",
      "Epoch 145/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1828.5994 - val_loss: 1465.6566\n",
      "Epoch 146/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1810.4186 - val_loss: 1449.3862\n",
      "Epoch 147/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1792.3656 - val_loss: 1433.2401\n",
      "Epoch 148/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1774.4399 - val_loss: 1417.2170\n",
      "Epoch 149/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1756.6411 - val_loss: 1401.3174\n",
      "Epoch 150/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1732.6917 - val_loss: 1373.7980\n",
      "Epoch 151/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1706.4995 - val_loss: 1354.2656\n",
      "Epoch 152/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1684.9082 - val_loss: 1335.3019\n",
      "Epoch 153/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1664.0200 - val_loss: 1316.9851\n",
      "Epoch 154/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1643.6632 - val_loss: 1293.4996\n",
      "Epoch 155/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1616.1879 - val_loss: 1272.6362\n",
      "Epoch 156/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1592.7781 - val_loss: 1252.0551\n",
      "Epoch 157/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1570.0884 - val_loss: 1232.3174\n",
      "Epoch 158/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1548.2871 - val_loss: 1213.3159\n",
      "Epoch 159/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1527.2302 - val_loss: 1194.9166\n",
      "Epoch 160/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1506.7817 - val_loss: 1177.0188\n",
      "Epoch 161/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1486.8439 - val_loss: 1159.5497\n",
      "Epoch 162/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1467.3472 - val_loss: 1142.4584\n",
      "Epoch 163/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1448.2408 - val_loss: 1125.7056\n",
      "Epoch 164/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1429.4862 - val_loss: 1109.2625\n",
      "Epoch 165/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1411.0542 - val_loss: 1093.1062\n",
      "Epoch 166/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1392.9211 - val_loss: 1077.2175\n",
      "Epoch 167/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1375.0675 - val_loss: 1061.5807\n",
      "Epoch 168/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1357.4779 - val_loss: 1046.1826\n",
      "Epoch 169/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 1340.1382 - val_loss: 1031.0120\n",
      "Epoch 170/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1323.0365 - val_loss: 1016.0594\n",
      "Epoch 171/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1306.1639 - val_loss: 1001.3154\n",
      "Epoch 172/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1289.5103 - val_loss: 986.7745\n",
      "Epoch 173/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1273.0685 - val_loss: 972.4286\n",
      "Epoch 174/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1256.8319 - val_loss: 958.2724\n",
      "Epoch 175/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1240.7936 - val_loss: 944.2991\n",
      "Epoch 176/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1224.9487 - val_loss: 930.5056\n",
      "Epoch 177/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1209.2916 - val_loss: 916.8873\n",
      "Epoch 178/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1193.8176 - val_loss: 903.4396\n",
      "Epoch 179/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1178.5228 - val_loss: 890.1586\n",
      "Epoch 180/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1163.4033 - val_loss: 877.0405\n",
      "Epoch 181/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1148.4547 - val_loss: 864.0833\n",
      "Epoch 182/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1133.6744 - val_loss: 851.2825\n",
      "Epoch 183/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1119.0585 - val_loss: 838.6371\n",
      "Epoch 184/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1104.6050 - val_loss: 826.1432\n",
      "Epoch 185/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1090.3103 - val_loss: 813.7972\n",
      "Epoch 186/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1076.1719 - val_loss: 801.5988\n",
      "Epoch 187/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1062.1874 - val_loss: 789.5444\n",
      "Epoch 188/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1048.3541 - val_loss: 777.6326\n",
      "Epoch 189/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1034.6704 - val_loss: 765.8607\n",
      "Epoch 190/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1021.1338 - val_loss: 754.2278\n",
      "Epoch 191/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1007.7422 - val_loss: 742.7310\n",
      "Epoch 192/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 994.4938 - val_loss: 731.3686\n",
      "Epoch 193/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 981.3871 - val_loss: 720.1404\n",
      "Epoch 194/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 968.4200 - val_loss: 709.0423\n",
      "Epoch 195/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 955.5901 - val_loss: 698.0748\n",
      "Epoch 196/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 942.8970 - val_loss: 687.2346\n",
      "Epoch 197/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 930.3380 - val_loss: 676.5220\n",
      "Epoch 198/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 917.9122 - val_loss: 665.9343\n",
      "Epoch 199/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 905.6178 - val_loss: 655.4704\n",
      "Epoch 200/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 893.4539 - val_loss: 645.1299\n",
      "Epoch 201/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 881.4185 - val_loss: 634.9093\n",
      "Epoch 202/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 869.5106 - val_loss: 624.8096\n",
      "Epoch 203/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 857.7282 - val_loss: 614.8282\n",
      "Epoch 204/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 846.0712 - val_loss: 604.9645\n",
      "Epoch 205/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 834.5376 - val_loss: 595.2181\n",
      "Epoch 206/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 823.1267 - val_loss: 585.5860\n",
      "Epoch 207/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 811.8368 - val_loss: 576.0685\n",
      "Epoch 208/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 800.6674 - val_loss: 566.6634\n",
      "Epoch 209/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 789.6161 - val_loss: 557.3705\n",
      "Epoch 210/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 778.6832 - val_loss: 548.1891\n",
      "Epoch 211/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 767.8668 - val_loss: 539.1180\n",
      "Epoch 212/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 757.1661 - val_loss: 530.1542\n",
      "Epoch 213/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 746.5800 - val_loss: 521.2999\n",
      "Epoch 214/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 736.1073 - val_loss: 512.5510\n",
      "Epoch 215/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 725.7474 - val_loss: 503.9079\n",
      "Epoch 216/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 715.4990 - val_loss: 495.3708\n",
      "Epoch 217/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 705.3613 - val_loss: 486.9379\n",
      "Epoch 218/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 695.3338 - val_loss: 478.6075\n",
      "Epoch 219/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 685.4146 - val_loss: 470.3797\n",
      "Epoch 220/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 675.6035 - val_loss: 462.2531\n",
      "Epoch 221/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 665.8995 - val_loss: 454.2268\n",
      "Epoch 222/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 656.3016 - val_loss: 446.3011\n",
      "Epoch 223/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 646.8092 - val_loss: 438.4737\n",
      "Epoch 224/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 637.4208 - val_loss: 430.7436\n",
      "Epoch 225/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 628.1359 - val_loss: 423.1112\n",
      "Epoch 226/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 618.9541 - val_loss: 415.5751\n",
      "Epoch 227/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 609.8740 - val_loss: 408.1343\n",
      "Epoch 228/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 600.8950 - val_loss: 400.7887\n",
      "Epoch 229/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 592.0162 - val_loss: 393.5367\n",
      "Epoch 230/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 583.2369 - val_loss: 386.3773\n",
      "Epoch 231/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 574.5561 - val_loss: 379.3102\n",
      "Epoch 232/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 565.9730 - val_loss: 372.3353\n",
      "Epoch 233/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 557.4873 - val_loss: 365.4508\n",
      "Epoch 234/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 549.0976 - val_loss: 358.6567\n",
      "Epoch 235/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 540.8038 - val_loss: 351.9510\n",
      "Epoch 236/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 532.6047 - val_loss: 345.3347\n",
      "Epoch 237/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 524.4995 - val_loss: 338.8059\n",
      "Epoch 238/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 516.4879 - val_loss: 332.3639\n",
      "Epoch 239/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 508.5685 - val_loss: 326.0082\n",
      "Epoch 240/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 500.7412 - val_loss: 319.7387\n",
      "Epoch 241/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 493.0051 - val_loss: 313.5537\n",
      "Epoch 242/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 485.3593 - val_loss: 307.4528\n",
      "Epoch 243/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 477.8032 - val_loss: 301.4360\n",
      "Epoch 244/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 470.3364 - val_loss: 295.5013\n",
      "Epoch 245/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 462.9578 - val_loss: 289.6488\n",
      "Epoch 246/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 455.6665 - val_loss: 283.8775\n",
      "Epoch 247/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 448.4624 - val_loss: 278.1876\n",
      "Epoch 248/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 441.3445 - val_loss: 272.5771\n",
      "Epoch 249/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 434.3123 - val_loss: 267.0459\n",
      "Epoch 250/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 427.3646 - val_loss: 261.5938\n",
      "Epoch 251/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 420.5014 - val_loss: 256.2189\n",
      "Epoch 252/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 413.7214 - val_loss: 250.9220\n",
      "Epoch 253/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 407.0251 - val_loss: 245.7022\n",
      "Epoch 254/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 400.4105 - val_loss: 240.5573\n",
      "Epoch 255/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 393.8774 - val_loss: 235.4890\n",
      "Epoch 256/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 387.4258 - val_loss: 230.4941\n",
      "Epoch 257/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 381.0538 - val_loss: 225.5738\n",
      "Epoch 258/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 374.7618 - val_loss: 220.7271\n",
      "Epoch 259/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 368.5486 - val_loss: 215.9525\n",
      "Epoch 260/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 362.4138 - val_loss: 211.2505\n",
      "Epoch 261/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 356.3567 - val_loss: 206.6198\n",
      "Epoch 262/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 350.3767 - val_loss: 202.0603\n",
      "Epoch 263/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 344.4733 - val_loss: 197.5704\n",
      "Epoch 264/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 338.6456 - val_loss: 193.1504\n",
      "Epoch 265/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 332.8930 - val_loss: 188.7990\n",
      "Epoch 266/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 327.2148 - val_loss: 184.5161\n",
      "Epoch 267/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 321.6108 - val_loss: 180.3007\n",
      "Epoch 268/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 316.0800 - val_loss: 176.1523\n",
      "Epoch 269/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 310.6218 - val_loss: 172.0708\n",
      "Epoch 270/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 305.2360 - val_loss: 168.0546\n",
      "Epoch 271/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 299.9215 - val_loss: 164.1037\n",
      "Epoch 272/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 294.6780 - val_loss: 160.2176\n",
      "Epoch 273/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 289.5046 - val_loss: 156.3952\n",
      "Epoch 274/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 284.4013 - val_loss: 152.6364\n",
      "Epoch 275/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 279.3670 - val_loss: 148.9402\n",
      "Epoch 276/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 274.4010 - val_loss: 145.3063\n",
      "Epoch 277/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 269.5031 - val_loss: 141.7337\n",
      "Epoch 278/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 264.6723 - val_loss: 138.2220\n",
      "Epoch 279/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 259.9083 - val_loss: 134.7713\n",
      "Epoch 280/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 255.2105 - val_loss: 131.3795\n",
      "Epoch 281/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 250.5781 - val_loss: 128.0475\n",
      "Epoch 282/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 246.0108 - val_loss: 124.7738\n",
      "Epoch 283/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 241.5078 - val_loss: 121.5578\n",
      "Epoch 284/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 237.0687 - val_loss: 118.3997\n",
      "Epoch 285/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 232.6928 - val_loss: 115.2980\n",
      "Epoch 286/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 228.3794 - val_loss: 112.2527\n",
      "Epoch 287/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 224.1281 - val_loss: 109.2628\n",
      "Epoch 288/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 219.9384 - val_loss: 106.3282\n",
      "Epoch 289/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 215.8095 - val_loss: 103.4482\n",
      "Epoch 290/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 211.7412 - val_loss: 100.6217\n",
      "Epoch 291/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 207.7324 - val_loss: 97.8490\n",
      "Epoch 292/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 203.7831 - val_loss: 95.1286\n",
      "Epoch 293/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 199.8922 - val_loss: 92.4607\n",
      "Epoch 294/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 196.0597 - val_loss: 89.8443\n",
      "Epoch 295/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 192.2844 - val_loss: 87.2786\n",
      "Epoch 296/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 188.5661 - val_loss: 84.7636\n",
      "Epoch 297/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 184.9043 - val_loss: 82.2986\n",
      "Epoch 298/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 181.2984 - val_loss: 79.8827\n",
      "Epoch 299/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 177.7476 - val_loss: 77.5156\n",
      "Epoch 300/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 174.2517 - val_loss: 75.1967\n",
      "Epoch 301/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 170.8100 - val_loss: 72.9255\n",
      "Epoch 302/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 167.4220 - val_loss: 70.7016\n",
      "Epoch 303/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 164.0874 - val_loss: 68.5241\n",
      "Epoch 304/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 160.8051 - val_loss: 66.3928\n",
      "Epoch 305/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 157.5750 - val_loss: 64.3066\n",
      "Epoch 306/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 154.3962 - val_loss: 62.2655\n",
      "Epoch 307/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 151.2684 - val_loss: 60.2686\n",
      "Epoch 308/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 148.1910 - val_loss: 58.3156\n",
      "Epoch 309/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 145.1636 - val_loss: 56.4058\n",
      "Epoch 310/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 142.1855 - val_loss: 54.5388\n",
      "Epoch 311/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 139.2561 - val_loss: 52.7139\n",
      "Epoch 312/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 136.3752 - val_loss: 50.9308\n",
      "Epoch 313/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 133.5422 - val_loss: 49.1889\n",
      "Epoch 314/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 130.7563 - val_loss: 47.4875\n",
      "Epoch 315/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 128.0174 - val_loss: 45.8259\n",
      "Epoch 316/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 125.3243 - val_loss: 44.2043\n",
      "Epoch 317/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 122.6773 - val_loss: 42.6214\n",
      "Epoch 318/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 120.0753 - val_loss: 41.0771\n",
      "Epoch 319/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 117.5179 - val_loss: 39.5707\n",
      "Epoch 320/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 115.0048 - val_loss: 38.1019\n",
      "Epoch 321/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 112.5353 - val_loss: 36.6697\n",
      "Epoch 322/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 110.1089 - val_loss: 35.2743\n",
      "Epoch 323/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 107.7252 - val_loss: 33.9145\n",
      "Epoch 324/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 105.3837 - val_loss: 32.5902\n",
      "Epoch 325/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 103.0837 - val_loss: 31.3008\n",
      "Epoch 326/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 100.8249 - val_loss: 30.0457\n",
      "Epoch 327/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 98.6067 - val_loss: 28.8245\n",
      "Epoch 328/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 96.4286 - val_loss: 27.6368\n",
      "Epoch 329/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 94.2902 - val_loss: 26.4818\n",
      "Epoch 330/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 92.1909 - val_loss: 25.3591\n",
      "Epoch 331/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 90.1302 - val_loss: 24.2684\n",
      "Epoch 332/500\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 88.1077 - val_loss: 23.2090\n",
      "Epoch 333/500\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 86.1227 - val_loss: 22.1806\n",
      "Epoch 334/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 84.1753 - val_loss: 21.1827\n",
      "Epoch 335/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 82.2645 - val_loss: 20.2146\n",
      "Epoch 336/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 80.3899 - val_loss: 19.2761\n",
      "Epoch 337/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 78.5512 - val_loss: 18.3664\n",
      "Epoch 338/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 76.7476 - val_loss: 17.4853\n",
      "Epoch 339/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 74.9790 - val_loss: 16.6322\n",
      "Epoch 340/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 73.2445 - val_loss: 15.8068\n",
      "Epoch 341/500\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 71.5443 - val_loss: 15.0082\n",
      "Epoch 342/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 69.8774 - val_loss: 14.2365\n",
      "Epoch 343/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 68.2435 - val_loss: 13.4908\n",
      "Epoch 344/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 66.6421 - val_loss: 12.7709\n",
      "Epoch 345/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 65.0727 - val_loss: 12.0763\n",
      "Epoch 346/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 63.5351 - val_loss: 11.4065\n",
      "Epoch 347/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 62.0287 - val_loss: 10.7609\n",
      "Epoch 348/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 60.5528 - val_loss: 10.1395\n",
      "Epoch 349/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 59.1074 - val_loss: 9.5413\n",
      "Epoch 350/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 57.6917 - val_loss: 8.9662\n",
      "Epoch 351/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 56.3055 - val_loss: 8.4138\n",
      "Epoch 352/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 54.9484 - val_loss: 7.8834\n",
      "Epoch 353/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 53.6197 - val_loss: 7.3749\n",
      "Epoch 354/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 52.3190 - val_loss: 6.8877\n",
      "Epoch 355/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 51.0463 - val_loss: 6.4213\n",
      "Epoch 356/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 49.8007 - val_loss: 5.9753\n",
      "Epoch 357/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 48.5819 - val_loss: 5.5495\n",
      "Epoch 358/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 47.3896 - val_loss: 5.1433\n",
      "Epoch 359/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 46.2234 - val_loss: 4.7562\n",
      "Epoch 360/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 45.0827 - val_loss: 4.3881\n",
      "Epoch 361/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 43.9673 - val_loss: 4.0383\n",
      "Epoch 362/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 42.8767 - val_loss: 3.7065\n",
      "Epoch 363/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 41.8104 - val_loss: 3.3924\n",
      "Epoch 364/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 40.7681 - val_loss: 3.0956\n",
      "Epoch 365/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 39.7495 - val_loss: 2.8155\n",
      "Epoch 366/500\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 38.7541 - val_loss: 2.5519\n",
      "Epoch 367/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 37.7815 - val_loss: 2.3044\n",
      "Epoch 368/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 36.8313 - val_loss: 2.0726\n",
      "Epoch 369/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 35.9033 - val_loss: 1.8561\n",
      "Epoch 370/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 34.9968 - val_loss: 1.6546\n",
      "Epoch 371/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 34.1117 - val_loss: 1.4677\n",
      "Epoch 372/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 33.2476 - val_loss: 1.2950\n",
      "Epoch 373/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 32.4039 - val_loss: 1.1362\n",
      "Epoch 374/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 31.5805 - val_loss: 0.9909\n",
      "Epoch 375/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 30.7769 - val_loss: 0.8588\n",
      "Epoch 376/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 29.9929 - val_loss: 0.7395\n",
      "Epoch 377/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 29.2279 - val_loss: 0.6327\n",
      "Epoch 378/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 28.4816 - val_loss: 0.5380\n",
      "Epoch 379/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 27.7539 - val_loss: 0.4551\n",
      "Epoch 380/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 27.0442 - val_loss: 0.3838\n",
      "Epoch 381/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 26.3523 - val_loss: 0.3236\n",
      "Epoch 382/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 25.6777 - val_loss: 0.2742\n",
      "Epoch 383/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 25.0203 - val_loss: 0.2353\n",
      "Epoch 384/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 24.3796 - val_loss: 0.2067\n",
      "Epoch 385/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 23.7554 - val_loss: 0.1879\n",
      "Epoch 386/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 23.1472 - val_loss: 0.1788\n",
      "Epoch 387/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 22.5548 - val_loss: 0.1789\n",
      "Epoch 388/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 21.9780 - val_loss: 0.1880\n",
      "Epoch 389/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 21.4162 - val_loss: 0.2059\n",
      "Epoch 390/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 20.8694 - val_loss: 0.2321\n",
      "Epoch 391/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 20.3370 - val_loss: 0.2665\n",
      "Epoch 392/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 19.8188 - val_loss: 0.3087\n",
      "Epoch 393/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 19.3146 - val_loss: 0.3585\n",
      "Epoch 394/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 18.8241 - val_loss: 0.4156\n",
      "Epoch 395/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 18.3470 - val_loss: 0.4797\n",
      "Epoch 396/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 17.8829 - val_loss: 0.5507\n",
      "Epoch 397/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 17.4317 - val_loss: 0.6281\n",
      "Epoch 398/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 16.9929 - val_loss: 0.7118\n",
      "Epoch 399/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 16.5664 - val_loss: 0.8015\n",
      "Epoch 400/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 16.1519 - val_loss: 0.8970\n",
      "Epoch 401/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 15.7491 - val_loss: 0.9980\n",
      "Epoch 402/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 15.3578 - val_loss: 1.1043\n",
      "Epoch 403/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 14.9776 - val_loss: 1.2157\n",
      "Epoch 404/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 14.6085 - val_loss: 1.3319\n",
      "Epoch 405/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 14.2499 - val_loss: 1.4527\n",
      "Epoch 406/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 13.9018 - val_loss: 1.5779\n",
      "Epoch 407/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 13.5640 - val_loss: 1.7073\n",
      "Epoch 408/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 13.2360 - val_loss: 1.8406\n",
      "Epoch 409/500\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 12.9179 - val_loss: 1.9777\n",
      "Epoch 410/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 12.6092 - val_loss: 2.1183\n",
      "Epoch 411/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 12.3098 - val_loss: 2.2623\n",
      "Epoch 412/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 12.0194 - val_loss: 2.4094\n",
      "Epoch 413/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 11.7378 - val_loss: 2.5596\n",
      "Epoch 414/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 11.4649 - val_loss: 2.7125\n",
      "Epoch 415/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 11.2002 - val_loss: 2.8680\n",
      "Epoch 416/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 10.9439 - val_loss: 3.0259\n",
      "Epoch 417/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 10.6954 - val_loss: 3.1861\n",
      "Epoch 418/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 10.4547 - val_loss: 3.3484\n",
      "Epoch 419/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 10.2216 - val_loss: 3.5125\n",
      "Epoch 420/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 9.9959 - val_loss: 3.6785\n",
      "Epoch 421/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 9.7774 - val_loss: 3.8460\n",
      "Epoch 422/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 9.5658 - val_loss: 4.0150\n",
      "Epoch 423/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 9.3610 - val_loss: 4.1853\n",
      "Epoch 424/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 9.1629 - val_loss: 4.3567\n",
      "Epoch 425/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 8.9712 - val_loss: 4.5292\n",
      "Epoch 426/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 8.7857 - val_loss: 4.7025\n",
      "Epoch 427/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 8.6064 - val_loss: 4.8766\n",
      "Epoch 428/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 8.4330 - val_loss: 5.0513\n",
      "Epoch 429/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 8.2654 - val_loss: 5.2263\n",
      "Epoch 430/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 8.1034 - val_loss: 5.4019\n",
      "Epoch 431/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 7.9468 - val_loss: 5.5776\n",
      "Epoch 432/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 7.7956 - val_loss: 5.7535\n",
      "Epoch 433/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 7.6494 - val_loss: 5.9294\n",
      "Epoch 434/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 7.5083 - val_loss: 6.1052\n",
      "Epoch 435/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 7.3720 - val_loss: 6.2808\n",
      "Epoch 436/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 7.2404 - val_loss: 6.4561\n",
      "Epoch 437/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 7.1134 - val_loss: 6.6311\n",
      "Epoch 438/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 6.9908 - val_loss: 6.8055\n",
      "Epoch 439/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 6.8725 - val_loss: 6.9794\n",
      "Epoch 440/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 6.7584 - val_loss: 7.1526\n",
      "Epoch 441/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 6.6483 - val_loss: 7.3252\n",
      "Epoch 442/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 6.5421 - val_loss: 7.4968\n",
      "Epoch 443/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 6.4398 - val_loss: 7.6675\n",
      "Epoch 444/500\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 6.3411 - val_loss: 7.8373\n",
      "Epoch 445/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 6.2459 - val_loss: 8.0060\n",
      "Epoch 446/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 6.1543 - val_loss: 8.1737\n",
      "Epoch 447/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 6.0660 - val_loss: 8.3401\n",
      "Epoch 448/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 5.9809 - val_loss: 8.5053\n",
      "Epoch 449/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 5.8990 - val_loss: 8.6691\n",
      "Epoch 450/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 5.8201 - val_loss: 8.8316\n",
      "Epoch 451/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 5.7441 - val_loss: 8.9926\n",
      "Epoch 452/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 5.6710 - val_loss: 9.1523\n",
      "Epoch 453/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 5.6006 - val_loss: 9.3104\n",
      "Epoch 454/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 5.5329 - val_loss: 9.4670\n",
      "Epoch 455/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 5.4677 - val_loss: 9.6219\n",
      "Epoch 456/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 5.4050 - val_loss: 9.7753\n",
      "Epoch 457/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 5.3447 - val_loss: 9.9269\n",
      "Epoch 458/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 5.2868 - val_loss: 10.0769\n",
      "Epoch 459/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 5.2310 - val_loss: 10.2249\n",
      "Epoch 460/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 5.1775 - val_loss: 10.3714\n",
      "Epoch 461/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 5.1260 - val_loss: 10.5160\n",
      "Epoch 462/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 5.0765 - val_loss: 10.6587\n",
      "Epoch 463/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 5.0289 - val_loss: 10.7997\n",
      "Epoch 464/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4.9832 - val_loss: 10.9387\n",
      "Epoch 465/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4.9393 - val_loss: 11.0758\n",
      "Epoch 466/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4.8972 - val_loss: 11.2110\n",
      "Epoch 467/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 4.8567 - val_loss: 11.3442\n",
      "Epoch 468/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4.8179 - val_loss: 11.4755\n",
      "Epoch 469/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4.7806 - val_loss: 11.6050\n",
      "Epoch 470/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4.7448 - val_loss: 11.7324\n",
      "Epoch 471/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4.7104 - val_loss: 11.8578\n",
      "Epoch 472/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4.6775 - val_loss: 11.9812\n",
      "Epoch 473/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4.6459 - val_loss: 12.1026\n",
      "Epoch 474/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4.6155 - val_loss: 12.2220\n",
      "Epoch 475/500\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 4.5864 - val_loss: 12.3395\n",
      "Epoch 476/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 4.5585 - val_loss: 12.4549\n",
      "Epoch 477/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 4.5317 - val_loss: 12.5685\n",
      "Epoch 478/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4.5061 - val_loss: 12.6798\n",
      "Epoch 479/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4.4815 - val_loss: 12.7894\n",
      "Epoch 480/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4.4579 - val_loss: 12.8970\n",
      "Epoch 481/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4.4353 - val_loss: 13.0024\n",
      "Epoch 482/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4.4137 - val_loss: 13.1059\n",
      "Epoch 483/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4.3929 - val_loss: 13.2076\n",
      "Epoch 484/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4.3730 - val_loss: 13.3070\n",
      "Epoch 485/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4.3540 - val_loss: 13.4046\n",
      "Epoch 486/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4.3358 - val_loss: 13.5005\n",
      "Epoch 487/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4.3183 - val_loss: 13.5943\n",
      "Epoch 488/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4.3015 - val_loss: 13.6862\n",
      "Epoch 489/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4.2855 - val_loss: 13.7762\n",
      "Epoch 490/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4.2701 - val_loss: 13.8642\n",
      "Epoch 491/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4.2554 - val_loss: 13.9505\n",
      "Epoch 492/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4.2414 - val_loss: 14.0350\n",
      "Epoch 493/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4.2279 - val_loss: 14.1176\n",
      "Epoch 494/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4.2150 - val_loss: 14.1984\n",
      "Epoch 495/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4.2026 - val_loss: 14.2774\n",
      "Epoch 496/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4.1908 - val_loss: 14.3545\n",
      "Epoch 497/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4.1795 - val_loss: 14.4300\n",
      "Epoch 498/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4.1687 - val_loss: 14.5036\n",
      "Epoch 499/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4.1583 - val_loss: 14.5756\n",
      "Epoch 500/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4.1484 - val_loss: 14.6460\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.0403, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0012, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0120, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.0334, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 389ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.69569561e+01, 7.69121382e+01, 7.68673203e+01, 7.68225023e+01,\n",
       "        7.67776844e+01, 7.67328665e+01, 7.66880486e+01, 7.66432306e+01,\n",
       "        7.65984127e+01, 7.65535948e+01, 7.65087768e+01, 7.64639589e+01,\n",
       "        7.64191410e+01, 7.63743231e+01, 7.63295051e+01, 7.62923436e+01,\n",
       "        7.62699346e+01, 7.62475257e+01, 7.62251167e+01, 7.62027078e+01,\n",
       "        7.61802988e+01, 7.61578898e+01, 7.61354809e+01, 7.61130719e+01,\n",
       "        7.60906629e+01, 7.60682540e+01, 7.60458450e+01, 7.60234360e+01,\n",
       "        7.60010271e+01, 7.59786181e+01, 7.59562092e+01, 7.59338002e+01,\n",
       "        7.59113912e+01, 7.58889823e+01, 7.58665733e+01, 7.58441643e+01,\n",
       "        7.58217554e+01, 7.57993464e+01, 7.57769374e+01, 7.57545285e+01,\n",
       "        7.57321195e+01, 7.57097105e+01, 7.56873016e+01, 7.56648926e+01,\n",
       "        7.56424837e+01, 7.56200747e+01, 7.55976657e+01, 7.55752568e+01,\n",
       "        7.55528478e+01, 7.55304388e+01, 7.55080299e+01, 7.54831046e+01,\n",
       "        7.54567740e+01, 7.54304435e+01, 7.54041130e+01, 7.53777824e+01,\n",
       "        7.53514519e+01, 7.53251214e+01, 7.52987909e+01, 7.52724603e+01,\n",
       "        7.52461298e+01, 7.52197992e+01, 7.51934687e+01, 7.51671382e+01,\n",
       "        7.51408077e+01, 7.51144771e+01, 7.50881466e+01, 7.50618161e+01,\n",
       "        7.50354855e+01, 7.50091550e+01, 7.49828245e+01, 7.49564939e+01,\n",
       "        7.49301634e+01, 7.49038329e+01, 7.48775023e+01, 7.48511718e+01,\n",
       "        7.48248413e+01, 7.47985107e+01, 7.47721802e+01, 7.47458497e+01,\n",
       "        7.92870941e+01, 0.00000000e+00, 2.17284888e-01, 5.90972528e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.82775307e-01, 4.43340153e-01,\n",
       "        3.00090790e-01, 6.86986327e-01, 2.13404834e-01, 0.00000000e+00,\n",
       "        4.47119743e-01, 1.30534396e-01, 1.82285339e-01, 0.00000000e+00,\n",
       "        4.37472165e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([73.95581232, 73.95114379, 73.94647526, 73.94180672, 73.93713819,\n",
       "       73.93246965, 73.92780112, 73.92313259, 73.91846405, 73.91379552,\n",
       "       73.90912698, 73.90445845, 73.89978992, 73.89512138, 73.89045285,\n",
       "       73.88578431, 73.88111578, 73.87644725, 73.87177871, 73.86711018,\n",
       "       73.86244164, 73.85777311, 73.85310458, 73.84843604, 73.84376751,\n",
       "       73.83909897, 73.83443044, 73.8297619 , 73.82509337, 73.82042484,\n",
       "       73.8157563 , 73.81108777, 73.80641923, 73.8017507 , 73.79241363,\n",
       "       73.78027544, 73.76813725, 73.75599907, 73.74386088, 73.73172269,\n",
       "       73.7195845 , 73.70744631, 73.69530812, 73.68316993, 73.67103175,\n",
       "       73.65889356, 73.64675537, 73.63461718, 73.62247899, 73.6103408 ,\n",
       "       73.59820261, 73.58606443, 73.57392624, 73.56178805, 73.54964986,\n",
       "       73.53751167, 73.52537348, 73.51323529, 73.50109711, 73.48895892,\n",
       "       73.47682073, 73.46468254, 73.45254435, 73.44040616, 73.42826797,\n",
       "       73.41612979, 73.4039916 , 73.39185341, 73.37971522, 73.36757703,\n",
       "       73.35543884, 73.34330065, 73.33116246, 73.31902428, 73.30688609,\n",
       "       73.2947479 , 73.28260971, 73.27047152, 73.25833333, 73.24619514,\n",
       "       73.23405696, 73.22191877, 73.20978058, 73.19764239, 73.1855042 ,\n",
       "       73.17336601, 73.16122782, 73.14908964, 73.13695145, 73.12481326,\n",
       "       73.11267507, 73.10053688, 73.08839869, 73.0762605 , 73.06412232,\n",
       "       73.05198413, 73.03984594, 73.02770775, 73.01556956, 73.00343137])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.85108701999399\n",
      "15.583019177134767\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
