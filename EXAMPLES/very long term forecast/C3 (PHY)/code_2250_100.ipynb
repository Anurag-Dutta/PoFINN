{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "2345    67.967958\n",
       "2346    67.961672\n",
       "2347    67.955387\n",
       "2348    67.949102\n",
       "2349    67.942816\n",
       "Name: C3, Length: 2350, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c3_interpolated_2250_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "2245     0.000000\n",
       "2246     0.000000\n",
       "2247     0.059959\n",
       "2248     0.000000\n",
       "2249     0.965788\n",
       "Name: C3, Length: 2250, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2250)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.100000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.875910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.651821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90.427731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90.203641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     91.100000  0.000298\n",
       "1     90.875910  0.000298\n",
       "2     90.651821  0.000297\n",
       "3     90.427731  0.000297\n",
       "4     90.203641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.0403  # Value for C0\n",
    "K0 = -0.0012  # Value for K0\n",
    "K1 = -0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0120    # Value for b\n",
    "c = 2.0334    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    91.100000    90.875910    90.651821    90.427731    90.203641   \n",
      "351    90.875910    90.651821    90.427731    90.203641    89.979552   \n",
      "352    90.651821    90.427731    90.203641    89.979552    89.755462   \n",
      "353    90.427731    90.203641    89.979552    89.755462    89.531373   \n",
      "354    90.203641    89.979552    89.755462    89.531373    89.307283   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    89.979552    89.755462    89.531373    89.307283    89.094118  ...   \n",
      "351    89.755462    89.531373    89.307283    89.094118    89.015686  ...   \n",
      "352    89.531373    89.307283    89.094118    89.015686    88.937255  ...   \n",
      "353    89.307283    89.094118    89.015686    88.937255    88.858824  ...   \n",
      "354    89.094118    89.015686    88.937255    88.858824    88.780392  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   81.423343    0.000263   81.404669    0.000263   81.385994    0.000263   \n",
      "351   81.404669    0.000263   81.385994    0.000263   81.367320    0.000262   \n",
      "352   81.385994    0.000263   81.367320    0.000262   81.348646    0.000262   \n",
      "353   81.367320    0.000262   81.348646    0.000262   81.329972    0.000262   \n",
      "354   81.348646    0.000262   81.329972    0.000262   81.311298    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   81.367320    0.000262   81.348646    0.000262  \n",
      "351   81.348646    0.000262   81.329972    0.000262  \n",
      "352   81.329972    0.000262   81.311298    0.000262  \n",
      "353   81.311298    0.000262   81.292624    0.000262  \n",
      "354   81.292624    0.000262   81.273950    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 1, 251) (1800, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "23/23 [==============================] - 3s 25ms/step - loss: 5649.3247 - val_loss: 3103.7927\n",
      "Epoch 2/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 5259.4741 - val_loss: 2956.8472\n",
      "Epoch 3/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 5105.6851 - val_loss: 2887.7681\n",
      "Epoch 4/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4993.9883 - val_loss: 2827.3877\n",
      "Epoch 5/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4890.2705 - val_loss: 2770.9246\n",
      "Epoch 6/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4790.3013 - val_loss: 2716.7507\n",
      "Epoch 7/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4693.2651 - val_loss: 2664.4441\n",
      "Epoch 8/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4598.6265 - val_loss: 2613.6157\n",
      "Epoch 9/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4506.0781 - val_loss: 2564.6663\n",
      "Epoch 10/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4415.4312 - val_loss: 2516.9375\n",
      "Epoch 11/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4326.5596 - val_loss: 2470.5544\n",
      "Epoch 12/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4239.3696 - val_loss: 2425.4656\n",
      "Epoch 13/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4153.7915 - val_loss: 2381.6284\n",
      "Epoch 14/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4069.7683 - val_loss: 2339.0073\n",
      "Epoch 15/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3987.2532 - val_loss: 2297.5713\n",
      "Epoch 16/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3906.2070 - val_loss: 2257.2937\n",
      "Epoch 17/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3826.5940 - val_loss: 2218.1494\n",
      "Epoch 18/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3748.3857 - val_loss: 2180.1157\n",
      "Epoch 19/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3671.5513 - val_loss: 2143.1721\n",
      "Epoch 20/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3596.0681 - val_loss: 2107.2979\n",
      "Epoch 21/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3521.9106 - val_loss: 2072.4746\n",
      "Epoch 22/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3449.0581 - val_loss: 2038.6841\n",
      "Epoch 23/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3377.4888 - val_loss: 2005.9088\n",
      "Epoch 24/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3307.1826 - val_loss: 1974.1327\n",
      "Epoch 25/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3238.1211 - val_loss: 1943.3381\n",
      "Epoch 26/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3170.2861 - val_loss: 1913.5106\n",
      "Epoch 27/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3103.6597 - val_loss: 1884.6342\n",
      "Epoch 28/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3038.2253 - val_loss: 1856.6937\n",
      "Epoch 29/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2973.9653 - val_loss: 1829.6747\n",
      "Epoch 30/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2910.8650 - val_loss: 1803.5618\n",
      "Epoch 31/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2848.9075 - val_loss: 1778.3417\n",
      "Epoch 32/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2788.0781 - val_loss: 1754.0000\n",
      "Epoch 33/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2728.3616 - val_loss: 1730.5226\n",
      "Epoch 34/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2669.7432 - val_loss: 1707.8964\n",
      "Epoch 35/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2612.2083 - val_loss: 1686.1073\n",
      "Epoch 36/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2555.7424 - val_loss: 1665.1423\n",
      "Epoch 37/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2500.3320 - val_loss: 1644.9884\n",
      "Epoch 38/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2445.9636 - val_loss: 1625.6324\n",
      "Epoch 39/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2392.6228 - val_loss: 1607.0618\n",
      "Epoch 40/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2340.2964 - val_loss: 1589.2628\n",
      "Epoch 41/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2288.9712 - val_loss: 1572.2240\n",
      "Epoch 42/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2238.6343 - val_loss: 1555.9321\n",
      "Epoch 43/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2189.2727 - val_loss: 1540.3750\n",
      "Epoch 44/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2140.8733 - val_loss: 1525.5403\n",
      "Epoch 45/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2093.4241 - val_loss: 1511.4158\n",
      "Epoch 46/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2046.9111 - val_loss: 1497.9897\n",
      "Epoch 47/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2001.3241 - val_loss: 1485.2496\n",
      "Epoch 48/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1956.6493 - val_loss: 1473.1838\n",
      "Epoch 49/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1912.8750 - val_loss: 1461.7802\n",
      "Epoch 50/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1869.9896 - val_loss: 1451.0276\n",
      "Epoch 51/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1827.9807 - val_loss: 1440.9136\n",
      "Epoch 52/500\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1786.8363 - val_loss: 1431.4271\n",
      "Epoch 53/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1746.5452 - val_loss: 1422.5566\n",
      "Epoch 54/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1707.0957 - val_loss: 1414.2905\n",
      "Epoch 55/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1668.4764 - val_loss: 1406.6176\n",
      "Epoch 56/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1630.6752 - val_loss: 1399.5262\n",
      "Epoch 57/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1593.6813 - val_loss: 1393.0055\n",
      "Epoch 58/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1557.4840 - val_loss: 1387.0441\n",
      "Epoch 59/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1522.0714 - val_loss: 1381.6310\n",
      "Epoch 60/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1487.4325 - val_loss: 1376.7550\n",
      "Epoch 61/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1453.5569 - val_loss: 1372.4055\n",
      "Epoch 62/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1420.4332 - val_loss: 1368.5710\n",
      "Epoch 63/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1388.0509 - val_loss: 1365.2413\n",
      "Epoch 64/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1356.3992 - val_loss: 1362.4052\n",
      "Epoch 65/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1325.4668 - val_loss: 1360.0520\n",
      "Epoch 66/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1295.2440 - val_loss: 1358.1710\n",
      "Epoch 67/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1265.7198 - val_loss: 1356.7517\n",
      "Epoch 68/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1236.8839 - val_loss: 1355.7834\n",
      "Epoch 69/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1208.7256 - val_loss: 1355.2559\n",
      "Epoch 70/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1181.2352 - val_loss: 1355.1584\n",
      "Epoch 71/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1154.4023 - val_loss: 1355.4807\n",
      "Epoch 72/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1128.2164 - val_loss: 1356.2123\n",
      "Epoch 73/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1102.6680 - val_loss: 1357.3429\n",
      "Epoch 74/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1077.7465 - val_loss: 1358.8623\n",
      "Epoch 75/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1053.4420 - val_loss: 1360.7604\n",
      "Epoch 76/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1029.7448 - val_loss: 1363.0272\n",
      "Epoch 77/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1006.6450 - val_loss: 1365.6523\n",
      "Epoch 78/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 984.1326 - val_loss: 1368.6259\n",
      "Epoch 79/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 962.1982 - val_loss: 1371.9380\n",
      "Epoch 80/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 940.8318 - val_loss: 1375.5787\n",
      "Epoch 81/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 920.0241 - val_loss: 1379.5382\n",
      "Epoch 82/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 899.7657 - val_loss: 1383.8066\n",
      "Epoch 83/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 880.0468 - val_loss: 1388.3743\n",
      "Epoch 84/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 860.8578 - val_loss: 1393.2316\n",
      "Epoch 85/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 842.1899 - val_loss: 1398.3688\n",
      "Epoch 86/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 824.0337 - val_loss: 1403.7764\n",
      "Epoch 87/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 806.3795 - val_loss: 1409.4448\n",
      "Epoch 88/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 789.2185 - val_loss: 1415.3647\n",
      "Epoch 89/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 772.5419 - val_loss: 1421.5269\n",
      "Epoch 90/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 756.3401 - val_loss: 1427.9220\n",
      "Epoch 91/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 740.6044 - val_loss: 1434.5408\n",
      "Epoch 92/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 725.3256 - val_loss: 1441.3738\n",
      "Epoch 93/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 710.4951 - val_loss: 1448.4124\n",
      "Epoch 94/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 696.1041 - val_loss: 1455.6473\n",
      "Epoch 95/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 682.1440 - val_loss: 1463.0699\n",
      "Epoch 96/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 668.6059 - val_loss: 1470.6708\n",
      "Epoch 97/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 655.4814 - val_loss: 1478.4417\n",
      "Epoch 98/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 642.7618 - val_loss: 1486.3737\n",
      "Epoch 99/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 630.4386 - val_loss: 1494.4580\n",
      "Epoch 100/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 618.5037 - val_loss: 1502.6862\n",
      "Epoch 101/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 606.9485 - val_loss: 1511.0500\n",
      "Epoch 102/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 595.7650 - val_loss: 1519.5410\n",
      "Epoch 103/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 584.9447 - val_loss: 1528.1506\n",
      "Epoch 104/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 574.4796 - val_loss: 1536.8716\n",
      "Epoch 105/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 564.3618 - val_loss: 1545.6941\n",
      "Epoch 106/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 554.5834 - val_loss: 1554.6119\n",
      "Epoch 107/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 545.1362 - val_loss: 1563.6160\n",
      "Epoch 108/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 536.0126 - val_loss: 1572.6987\n",
      "Epoch 109/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 527.2047 - val_loss: 1581.8533\n",
      "Epoch 110/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 518.7051 - val_loss: 1591.0715\n",
      "Epoch 111/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 510.5059 - val_loss: 1600.3459\n",
      "Epoch 112/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 502.5997 - val_loss: 1609.6687\n",
      "Epoch 113/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 494.9795 - val_loss: 1619.0333\n",
      "Epoch 114/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 487.6375 - val_loss: 1628.4325\n",
      "Epoch 115/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 480.5665 - val_loss: 1637.8589\n",
      "Epoch 116/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 473.7595 - val_loss: 1647.3058\n",
      "Epoch 117/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 467.2094 - val_loss: 1656.7660\n",
      "Epoch 118/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 460.9092 - val_loss: 1666.2343\n",
      "Epoch 119/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 454.8520 - val_loss: 1675.7031\n",
      "Epoch 120/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 449.0309 - val_loss: 1685.1661\n",
      "Epoch 121/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 443.4393 - val_loss: 1694.6177\n",
      "Epoch 122/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 438.0706 - val_loss: 1704.0513\n",
      "Epoch 123/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 432.9183 - val_loss: 1713.4601\n",
      "Epoch 124/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 427.9758 - val_loss: 1722.8397\n",
      "Epoch 125/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 423.2371 - val_loss: 1732.1838\n",
      "Epoch 126/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 418.6956 - val_loss: 1741.4875\n",
      "Epoch 127/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 414.3456 - val_loss: 1750.7451\n",
      "Epoch 128/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 410.1805 - val_loss: 1759.9506\n",
      "Epoch 129/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 406.1950 - val_loss: 1769.1002\n",
      "Epoch 130/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 402.3828 - val_loss: 1778.1879\n",
      "Epoch 131/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 398.7385 - val_loss: 1787.2101\n",
      "Epoch 132/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 395.2564 - val_loss: 1796.1613\n",
      "Epoch 133/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 391.9309 - val_loss: 1805.0377\n",
      "Epoch 134/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 388.7569 - val_loss: 1813.8347\n",
      "Epoch 135/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 385.7286 - val_loss: 1822.5485\n",
      "Epoch 136/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 382.8414 - val_loss: 1831.1752\n",
      "Epoch 137/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 380.0899 - val_loss: 1839.7108\n",
      "Epoch 138/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 377.4691 - val_loss: 1848.1516\n",
      "Epoch 139/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 374.9744 - val_loss: 1856.4946\n",
      "Epoch 140/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 372.6009 - val_loss: 1864.7357\n",
      "Epoch 141/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 370.3440 - val_loss: 1872.8734\n",
      "Epoch 142/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 368.1991 - val_loss: 1880.9033\n",
      "Epoch 143/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 366.1620 - val_loss: 1888.8235\n",
      "Epoch 144/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 364.2283 - val_loss: 1896.6312\n",
      "Epoch 145/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 362.3937 - val_loss: 1904.3241\n",
      "Epoch 146/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 360.6544 - val_loss: 1911.8998\n",
      "Epoch 147/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 359.0062 - val_loss: 1919.3562\n",
      "Epoch 148/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 357.4453 - val_loss: 1926.6915\n",
      "Epoch 149/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 355.9681 - val_loss: 1933.9041\n",
      "Epoch 150/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 354.5706 - val_loss: 1940.9929\n",
      "Epoch 151/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 353.2499 - val_loss: 1947.9548\n",
      "Epoch 152/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 352.0020 - val_loss: 1954.7911\n",
      "Epoch 153/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 350.8238 - val_loss: 1961.4987\n",
      "Epoch 154/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 349.7122 - val_loss: 1968.0770\n",
      "Epoch 155/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 348.6639 - val_loss: 1974.5259\n",
      "Epoch 156/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 347.6762 - val_loss: 1980.8445\n",
      "Epoch 157/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 346.7458 - val_loss: 1987.0320\n",
      "Epoch 158/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 345.8703 - val_loss: 1993.0889\n",
      "Epoch 159/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 345.0468 - val_loss: 1999.0135\n",
      "Epoch 160/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 344.2727 - val_loss: 2004.8064\n",
      "Epoch 161/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 343.5456 - val_loss: 2010.4692\n",
      "Epoch 162/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 342.8630 - val_loss: 2015.9996\n",
      "Epoch 163/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 342.2227 - val_loss: 2021.4010\n",
      "Epoch 164/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 341.6223 - val_loss: 2026.6711\n",
      "Epoch 165/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 341.0597 - val_loss: 2031.8108\n",
      "Epoch 166/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 340.5330 - val_loss: 2036.8231\n",
      "Epoch 167/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 340.0402 - val_loss: 2041.7068\n",
      "Epoch 168/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 339.5795 - val_loss: 2046.4620\n",
      "Epoch 169/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 339.1489 - val_loss: 2051.0916\n",
      "Epoch 170/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 338.7468 - val_loss: 2055.5977\n",
      "Epoch 171/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 338.3715 - val_loss: 2059.9788\n",
      "Epoch 172/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 338.0216 - val_loss: 2064.2373\n",
      "Epoch 173/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 337.6955 - val_loss: 2068.3752\n",
      "Epoch 174/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 337.3917 - val_loss: 2072.3931\n",
      "Epoch 175/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 337.1090 - val_loss: 2076.2922\n",
      "Epoch 176/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 336.8460 - val_loss: 2080.0769\n",
      "Epoch 177/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 336.6016 - val_loss: 2083.7461\n",
      "Epoch 178/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 336.3746 - val_loss: 2087.3015\n",
      "Epoch 179/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 336.1638 - val_loss: 2090.7463\n",
      "Epoch 180/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 335.9683 - val_loss: 2094.0815\n",
      "Epoch 181/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 335.7871 - val_loss: 2097.3105\n",
      "Epoch 182/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 335.6192 - val_loss: 2100.4329\n",
      "Epoch 183/500\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 335.4637 - val_loss: 2103.4526\n",
      "Epoch 184/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 335.3200 - val_loss: 2106.3711\n",
      "Epoch 185/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 335.1870 - val_loss: 2109.1902\n",
      "Epoch 186/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 335.0640 - val_loss: 2111.9114\n",
      "Epoch 187/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 334.9506 - val_loss: 2114.5386\n",
      "Epoch 188/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 334.8458 - val_loss: 2117.0718\n",
      "Epoch 189/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 334.7495 - val_loss: 2119.5149\n",
      "Epoch 190/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 334.6604 - val_loss: 2121.8694\n",
      "Epoch 191/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 334.5784 - val_loss: 2124.1365\n",
      "Epoch 192/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 334.5031 - val_loss: 2126.3213\n",
      "Epoch 193/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 334.4336 - val_loss: 2128.4226\n",
      "Epoch 194/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 334.3699 - val_loss: 2130.4438\n",
      "Epoch 195/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 334.3113 - val_loss: 2132.3884\n",
      "Epoch 196/500\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 334.2574 - val_loss: 2134.2568\n",
      "Epoch 197/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 334.2081 - val_loss: 2136.0513\n",
      "Epoch 198/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 334.1629 - val_loss: 2137.7712\n",
      "Epoch 199/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 334.1221 - val_loss: 2139.4297\n",
      "Epoch 200/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 334.0834 - val_loss: 2141.0164\n",
      "Epoch 201/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 334.0487 - val_loss: 2142.5388\n",
      "Epoch 202/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 334.0169 - val_loss: 2143.9973\n",
      "Epoch 203/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 333.9879 - val_loss: 2145.3950\n",
      "Epoch 204/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 333.9614 - val_loss: 2146.7336\n",
      "Epoch 205/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 333.9371 - val_loss: 2148.0154\n",
      "Epoch 206/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 333.9152 - val_loss: 2149.2419\n",
      "Epoch 207/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 333.8950 - val_loss: 2150.4136\n",
      "Epoch 208/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 333.8767 - val_loss: 2151.5356\n",
      "Epoch 209/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 333.8600 - val_loss: 2152.6067\n",
      "Epoch 210/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.8449 - val_loss: 2153.6304\n",
      "Epoch 211/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 333.8311 - val_loss: 2154.6084\n",
      "Epoch 212/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 333.8186 - val_loss: 2155.5405\n",
      "Epoch 213/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.8073 - val_loss: 2156.4304\n",
      "Epoch 214/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7971 - val_loss: 2157.2795\n",
      "Epoch 215/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 333.7878 - val_loss: 2158.0884\n",
      "Epoch 216/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 333.7794 - val_loss: 2158.8591\n",
      "Epoch 217/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7718 - val_loss: 2159.5938\n",
      "Epoch 218/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7650 - val_loss: 2160.2920\n",
      "Epoch 219/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7589 - val_loss: 2160.9575\n",
      "Epoch 220/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7534 - val_loss: 2161.5903\n",
      "Epoch 221/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7485 - val_loss: 2162.1926\n",
      "Epoch 222/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 333.7440 - val_loss: 2162.7649\n",
      "Epoch 223/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7401 - val_loss: 2163.3093\n",
      "Epoch 224/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7366 - val_loss: 2163.8257\n",
      "Epoch 225/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7335 - val_loss: 2164.3159\n",
      "Epoch 226/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7306 - val_loss: 2164.7815\n",
      "Epoch 227/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7282 - val_loss: 2165.2239\n",
      "Epoch 228/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7261 - val_loss: 2165.6428\n",
      "Epoch 229/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7242 - val_loss: 2166.0400\n",
      "Epoch 230/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 333.7227 - val_loss: 2166.4167\n",
      "Epoch 231/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7212 - val_loss: 2166.7729\n",
      "Epoch 232/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7200 - val_loss: 2167.1118\n",
      "Epoch 233/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7191 - val_loss: 2167.4319\n",
      "Epoch 234/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7182 - val_loss: 2167.7351\n",
      "Epoch 235/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7175 - val_loss: 2168.0220\n",
      "Epoch 236/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 333.7169 - val_loss: 2168.2920\n",
      "Epoch 237/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7165 - val_loss: 2168.5479\n",
      "Epoch 238/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7162 - val_loss: 2168.7905\n",
      "Epoch 239/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7161 - val_loss: 2169.0200\n",
      "Epoch 240/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7160 - val_loss: 2169.2361\n",
      "Epoch 241/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7159 - val_loss: 2169.4399\n",
      "Epoch 242/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 333.7159 - val_loss: 2169.6321\n",
      "Epoch 243/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7161 - val_loss: 2169.8140\n",
      "Epoch 244/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7163 - val_loss: 2169.9846\n",
      "Epoch 245/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7165 - val_loss: 2170.1462\n",
      "Epoch 246/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7168 - val_loss: 2170.2983\n",
      "Epoch 247/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 333.7173 - val_loss: 2170.4417\n",
      "Epoch 248/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 333.7176 - val_loss: 2170.5762\n",
      "Epoch 249/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 333.7181 - val_loss: 2170.7041\n",
      "Epoch 250/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 333.7186 - val_loss: 2170.8247\n",
      "Epoch 251/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7191 - val_loss: 2170.9368\n",
      "Epoch 252/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7195 - val_loss: 2171.0430\n",
      "Epoch 253/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 333.7202 - val_loss: 2171.1426\n",
      "Epoch 254/500\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 333.7206 - val_loss: 2171.2354\n",
      "Epoch 255/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 333.7213 - val_loss: 2171.3230\n",
      "Epoch 256/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 333.7220 - val_loss: 2171.4065\n",
      "Epoch 257/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7225 - val_loss: 2171.4834\n",
      "Epoch 258/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 333.7231 - val_loss: 2171.5557\n",
      "Epoch 259/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7238 - val_loss: 2171.6226\n",
      "Epoch 260/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7245 - val_loss: 2171.6868\n",
      "Epoch 261/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7251 - val_loss: 2171.7463\n",
      "Epoch 262/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7258 - val_loss: 2171.8013\n",
      "Epoch 263/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 333.7264 - val_loss: 2171.8530\n",
      "Epoch 264/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7271 - val_loss: 2171.8994\n",
      "Epoch 265/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7278 - val_loss: 2171.9409\n",
      "Epoch 266/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7285 - val_loss: 2171.9417\n",
      "Epoch 267/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7292 - val_loss: 2172.0725\n",
      "Epoch 268/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7299 - val_loss: 2172.1116\n",
      "Epoch 269/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 333.7305 - val_loss: 2172.1465\n",
      "Epoch 270/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7313 - val_loss: 2172.1807\n",
      "Epoch 271/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7319 - val_loss: 2172.2109\n",
      "Epoch 272/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7326 - val_loss: 2172.2410\n",
      "Epoch 273/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7331 - val_loss: 2172.2666\n",
      "Epoch 274/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7339 - val_loss: 2172.2922\n",
      "Epoch 275/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7346 - val_loss: 2172.3159\n",
      "Epoch 276/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 333.7352 - val_loss: 2172.3364\n",
      "Epoch 277/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 333.7359 - val_loss: 2172.3586\n",
      "Epoch 278/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7365 - val_loss: 2172.3774\n",
      "Epoch 279/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7371 - val_loss: 2172.3950\n",
      "Epoch 280/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7377 - val_loss: 2172.4111\n",
      "Epoch 281/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7383 - val_loss: 2172.4253\n",
      "Epoch 282/500\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 333.7390 - val_loss: 2172.4387\n",
      "Epoch 283/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 333.7396 - val_loss: 2172.4512\n",
      "Epoch 284/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 333.7402 - val_loss: 2172.4636\n",
      "Epoch 285/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 333.7408 - val_loss: 2172.4753\n",
      "Epoch 286/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7414 - val_loss: 2172.4849\n",
      "Epoch 287/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7419 - val_loss: 2172.4941\n",
      "Epoch 288/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7425 - val_loss: 2172.5022\n",
      "Epoch 289/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7431 - val_loss: 2172.5093\n",
      "Epoch 290/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7437 - val_loss: 2172.5154\n",
      "Epoch 291/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7442 - val_loss: 2172.5222\n",
      "Epoch 292/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7448 - val_loss: 2172.5278\n",
      "Epoch 293/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 333.7454 - val_loss: 2172.5339\n",
      "Epoch 294/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 333.7458 - val_loss: 2172.5369\n",
      "Epoch 295/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 333.7463 - val_loss: 2172.5400\n",
      "Epoch 296/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 333.7469 - val_loss: 2172.5444\n",
      "Epoch 297/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7473 - val_loss: 2172.5461\n",
      "Epoch 298/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7478 - val_loss: 2172.5476\n",
      "Epoch 299/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7482 - val_loss: 2172.5486\n",
      "Epoch 300/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7487 - val_loss: 2172.5500\n",
      "Epoch 301/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7493 - val_loss: 2172.5508\n",
      "Epoch 302/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7496 - val_loss: 2172.5505\n",
      "Epoch 303/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 333.7502 - val_loss: 2172.5496\n",
      "Epoch 304/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 333.7506 - val_loss: 2172.5503\n",
      "Epoch 305/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7509 - val_loss: 2172.5491\n",
      "Epoch 306/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7514 - val_loss: 2172.5459\n",
      "Epoch 307/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7518 - val_loss: 2172.5442\n",
      "Epoch 308/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7523 - val_loss: 2172.5405\n",
      "Epoch 309/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 333.7527 - val_loss: 2172.5366\n",
      "Epoch 310/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 333.7531 - val_loss: 2172.5322\n",
      "Epoch 311/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 333.7534 - val_loss: 2172.5264\n",
      "Epoch 312/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7538 - val_loss: 2172.5195\n",
      "Epoch 313/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7543 - val_loss: 2172.5134\n",
      "Epoch 314/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7546 - val_loss: 2172.5059\n",
      "Epoch 315/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7550 - val_loss: 2172.4993\n",
      "Epoch 316/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 333.7553 - val_loss: 2172.4907\n",
      "Epoch 317/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7558 - val_loss: 2172.4805\n",
      "Epoch 318/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7560 - val_loss: 2172.4673\n",
      "Epoch 319/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7564 - val_loss: 2172.4529\n",
      "Epoch 320/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7567 - val_loss: 2172.4368\n",
      "Epoch 321/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7569 - val_loss: 2172.4185\n",
      "Epoch 322/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7573 - val_loss: 2172.3975\n",
      "Epoch 323/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7576 - val_loss: 2172.3723\n",
      "Epoch 324/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7579 - val_loss: 2172.3416\n",
      "Epoch 325/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7583 - val_loss: 2172.3042\n",
      "Epoch 326/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 333.7586 - val_loss: 2172.2593\n",
      "Epoch 327/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7588 - val_loss: 2172.2026\n",
      "Epoch 328/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7591 - val_loss: 2172.1313\n",
      "Epoch 329/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7595 - val_loss: 2172.0413\n",
      "Epoch 330/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7596 - val_loss: 2171.9229\n",
      "Epoch 331/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7599 - val_loss: 2171.7712\n",
      "Epoch 332/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7601 - val_loss: 2171.5833\n",
      "Epoch 333/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7603 - val_loss: 2171.3550\n",
      "Epoch 334/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7606 - val_loss: 2167.9617\n",
      "Epoch 335/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 326.0220 - val_loss: 1993.3190\n",
      "Epoch 336/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 345.8669 - val_loss: 1998.9462\n",
      "Epoch 337/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 344.7604 - val_loss: 2010.2024\n",
      "Epoch 338/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 343.3691 - val_loss: 2021.0991\n",
      "Epoch 339/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 342.1231 - val_loss: 2031.3408\n",
      "Epoch 340/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 341.0392 - val_loss: 2040.9319\n",
      "Epoch 341/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 340.0987 - val_loss: 2049.9119\n",
      "Epoch 342/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 339.2825 - val_loss: 2058.3120\n",
      "Epoch 343/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 338.5740 - val_loss: 2066.1680\n",
      "Epoch 344/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 337.9587 - val_loss: 2073.5115\n",
      "Epoch 345/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 337.4243 - val_loss: 2080.3748\n",
      "Epoch 346/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 336.9598 - val_loss: 2086.7839\n",
      "Epoch 347/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 336.5562 - val_loss: 2092.7703\n",
      "Epoch 348/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 336.2051 - val_loss: 2098.3584\n",
      "Epoch 349/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 335.8996 - val_loss: 2103.5750\n",
      "Epoch 350/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 335.6338 - val_loss: 2108.4419\n",
      "Epoch 351/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 335.4022 - val_loss: 2112.9817\n",
      "Epoch 352/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 335.2006 - val_loss: 2117.2144\n",
      "Epoch 353/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 335.0247 - val_loss: 2121.1606\n",
      "Epoch 354/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 334.8716 - val_loss: 2124.8401\n",
      "Epoch 355/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 334.7378 - val_loss: 2128.2678\n",
      "Epoch 356/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 334.6210 - val_loss: 2131.4614\n",
      "Epoch 357/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 334.5190 - val_loss: 2134.4360\n",
      "Epoch 358/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 334.4300 - val_loss: 2137.2070\n",
      "Epoch 359/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 334.3521 - val_loss: 2139.7876\n",
      "Epoch 360/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 334.2838 - val_loss: 2142.1892\n",
      "Epoch 361/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 334.2241 - val_loss: 2144.4260\n",
      "Epoch 362/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 334.1717 - val_loss: 2146.5083\n",
      "Epoch 363/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 334.1256 - val_loss: 2148.4456\n",
      "Epoch 364/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 334.0854 - val_loss: 2150.2473\n",
      "Epoch 365/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 334.0498 - val_loss: 2151.9246\n",
      "Epoch 366/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 334.0186 - val_loss: 2153.4849\n",
      "Epoch 367/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.9912 - val_loss: 2154.9355\n",
      "Epoch 368/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 333.9671 - val_loss: 2156.2856\n",
      "Epoch 369/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 333.9457 - val_loss: 2157.5405\n",
      "Epoch 370/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.9269 - val_loss: 2158.7083\n",
      "Epoch 371/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.9101 - val_loss: 2159.7942\n",
      "Epoch 372/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.8954 - val_loss: 2160.8032\n",
      "Epoch 373/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.8823 - val_loss: 2161.7417\n",
      "Epoch 374/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 333.8707 - val_loss: 2162.6133\n",
      "Epoch 375/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 333.8605 - val_loss: 2163.4246\n",
      "Epoch 376/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 333.8513 - val_loss: 2164.1782\n",
      "Epoch 377/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 333.8430 - val_loss: 2164.8772\n",
      "Epoch 378/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.8359 - val_loss: 2165.5281\n",
      "Epoch 379/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.8292 - val_loss: 2166.1318\n",
      "Epoch 380/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.8236 - val_loss: 2166.6934\n",
      "Epoch 381/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.8183 - val_loss: 2167.2148\n",
      "Epoch 382/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.8138 - val_loss: 2167.7000\n",
      "Epoch 383/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.8097 - val_loss: 2168.1519\n",
      "Epoch 384/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.8058 - val_loss: 2168.5701\n",
      "Epoch 385/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.8025 - val_loss: 2168.9595\n",
      "Epoch 386/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7993 - val_loss: 2169.3201\n",
      "Epoch 387/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7967 - val_loss: 2169.6562\n",
      "Epoch 388/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7943 - val_loss: 2169.9678\n",
      "Epoch 389/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7921 - val_loss: 2170.2581\n",
      "Epoch 390/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 333.7900 - val_loss: 2170.5271\n",
      "Epoch 391/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7882 - val_loss: 2170.7764\n",
      "Epoch 392/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7867 - val_loss: 2171.0083\n",
      "Epoch 393/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7851 - val_loss: 2171.2227\n",
      "Epoch 394/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7839 - val_loss: 2171.4231\n",
      "Epoch 395/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7827 - val_loss: 2171.6091\n",
      "Epoch 396/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 333.7815 - val_loss: 2171.7812\n",
      "Epoch 397/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7805 - val_loss: 2171.9414\n",
      "Epoch 398/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7796 - val_loss: 2172.0903\n",
      "Epoch 399/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7788 - val_loss: 2172.2278\n",
      "Epoch 400/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7780 - val_loss: 2172.3577\n",
      "Epoch 401/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7773 - val_loss: 2172.4751\n",
      "Epoch 402/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7766 - val_loss: 2172.5859\n",
      "Epoch 403/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7762 - val_loss: 2172.6887\n",
      "Epoch 404/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7756 - val_loss: 2172.7844\n",
      "Epoch 405/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7752 - val_loss: 2172.8735\n",
      "Epoch 406/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7747 - val_loss: 2172.9541\n",
      "Epoch 407/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7744 - val_loss: 2173.0315\n",
      "Epoch 408/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7739 - val_loss: 2173.0996\n",
      "Epoch 409/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7736 - val_loss: 2173.1650\n",
      "Epoch 410/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7735 - val_loss: 2173.2275\n",
      "Epoch 411/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7730 - val_loss: 2173.2837\n",
      "Epoch 412/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7728 - val_loss: 2173.3350\n",
      "Epoch 413/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7726 - val_loss: 2173.3840\n",
      "Epoch 414/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7725 - val_loss: 2173.4302\n",
      "Epoch 415/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 333.7723 - val_loss: 2173.4717\n",
      "Epoch 416/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 333.7720 - val_loss: 2173.5100\n",
      "Epoch 417/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 333.7719 - val_loss: 2173.5457\n",
      "Epoch 418/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7718 - val_loss: 2173.5791\n",
      "Epoch 419/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7718 - val_loss: 2173.6108\n",
      "Epoch 420/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7715 - val_loss: 2173.6396\n",
      "Epoch 421/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7715 - val_loss: 2173.6663\n",
      "Epoch 422/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7713 - val_loss: 2173.6912\n",
      "Epoch 423/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7714 - val_loss: 2173.7144\n",
      "Epoch 424/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7712 - val_loss: 2173.7358\n",
      "Epoch 425/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7712 - val_loss: 2173.7559\n",
      "Epoch 426/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7711 - val_loss: 2173.7744\n",
      "Epoch 427/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7711 - val_loss: 2173.7915\n",
      "Epoch 428/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7711 - val_loss: 2173.8076\n",
      "Epoch 429/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 333.7711 - val_loss: 2173.8218\n",
      "Epoch 430/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7710 - val_loss: 2173.8362\n",
      "Epoch 431/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7709 - val_loss: 2173.8496\n",
      "Epoch 432/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7709 - val_loss: 2173.8616\n",
      "Epoch 433/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7709 - val_loss: 2173.8716\n",
      "Epoch 434/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7708 - val_loss: 2173.8818\n",
      "Epoch 435/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7708 - val_loss: 2173.8906\n",
      "Epoch 436/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7708 - val_loss: 2173.8987\n",
      "Epoch 437/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7709 - val_loss: 2173.9072\n",
      "Epoch 438/500\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 333.7708 - val_loss: 2173.9143\n",
      "Epoch 439/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7709 - val_loss: 2173.9211\n",
      "Epoch 440/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 333.7708 - val_loss: 2173.9268\n",
      "Epoch 441/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7708 - val_loss: 2173.9309\n",
      "Epoch 442/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7708 - val_loss: 2173.9363\n",
      "Epoch 443/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7708 - val_loss: 2173.9407\n",
      "Epoch 444/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7708 - val_loss: 2173.9431\n",
      "Epoch 445/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7708 - val_loss: 2173.9446\n",
      "Epoch 446/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7709 - val_loss: 2173.9365\n",
      "Epoch 447/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7717 - val_loss: 2173.9658\n",
      "Epoch 448/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7709 - val_loss: 2173.9829\n",
      "Epoch 449/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7708 - val_loss: 2173.9871\n",
      "Epoch 450/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7708 - val_loss: 2173.9907\n",
      "Epoch 451/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7708 - val_loss: 2173.9932\n",
      "Epoch 452/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7709 - val_loss: 2173.9961\n",
      "Epoch 453/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7709 - val_loss: 2173.9983\n",
      "Epoch 454/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7709 - val_loss: 2174.0002\n",
      "Epoch 455/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7709 - val_loss: 2174.0022\n",
      "Epoch 456/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 333.7709 - val_loss: 2174.0039\n",
      "Epoch 457/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7710 - val_loss: 2174.0059\n",
      "Epoch 458/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7710 - val_loss: 2174.0083\n",
      "Epoch 459/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7710 - val_loss: 2174.0100\n",
      "Epoch 460/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7711 - val_loss: 2174.0112\n",
      "Epoch 461/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 333.7710 - val_loss: 2174.0125\n",
      "Epoch 462/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7710 - val_loss: 2174.0132\n",
      "Epoch 463/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7710 - val_loss: 2174.0139\n",
      "Epoch 464/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 333.7710 - val_loss: 2174.0149\n",
      "Epoch 465/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7711 - val_loss: 2174.0166\n",
      "Epoch 466/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7711 - val_loss: 2174.0178\n",
      "Epoch 467/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7711 - val_loss: 2174.0181\n",
      "Epoch 468/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7711 - val_loss: 2174.0183\n",
      "Epoch 469/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7711 - val_loss: 2174.0183\n",
      "Epoch 470/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7711 - val_loss: 2174.0188\n",
      "Epoch 471/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 333.7711 - val_loss: 2174.0198\n",
      "Epoch 472/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7711 - val_loss: 2174.0205\n",
      "Epoch 473/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7711 - val_loss: 2174.0212\n",
      "Epoch 474/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7711 - val_loss: 2174.0220\n",
      "Epoch 475/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7711 - val_loss: 2174.0229\n",
      "Epoch 476/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 333.7712 - val_loss: 2174.0237\n",
      "Epoch 477/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7712 - val_loss: 2174.0232\n",
      "Epoch 478/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7712 - val_loss: 2174.0232\n",
      "Epoch 479/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7711 - val_loss: 2174.0229\n",
      "Epoch 480/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7712 - val_loss: 2174.0225\n",
      "Epoch 481/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7712 - val_loss: 2174.0225\n",
      "Epoch 482/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7712 - val_loss: 2174.0227\n",
      "Epoch 483/500\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 333.7713 - val_loss: 2174.0229\n",
      "Epoch 484/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 333.7713 - val_loss: 2174.0232\n",
      "Epoch 485/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 333.7713 - val_loss: 2174.0232\n",
      "Epoch 486/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7713 - val_loss: 2174.0237\n",
      "Epoch 487/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7713 - val_loss: 2174.0249\n",
      "Epoch 488/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7713 - val_loss: 2174.0254\n",
      "Epoch 489/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 333.7714 - val_loss: 2174.0266\n",
      "Epoch 490/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7715 - val_loss: 2174.0278\n",
      "Epoch 491/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 333.7715 - val_loss: 2174.0286\n",
      "Epoch 492/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 333.7715 - val_loss: 2174.0291\n",
      "Epoch 493/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7715 - val_loss: 2174.0300\n",
      "Epoch 494/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7715 - val_loss: 2174.0322\n",
      "Epoch 495/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7715 - val_loss: 2174.0332\n",
      "Epoch 496/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7715 - val_loss: 2174.0334\n",
      "Epoch 497/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7715 - val_loss: 2174.0334\n",
      "Epoch 498/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7715 - val_loss: 2174.0327\n",
      "Epoch 499/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7715 - val_loss: 2174.0327\n",
      "Epoch 500/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.7715 - val_loss: 2174.0315\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.0403, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0012, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0120, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.0334, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 406ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.47897339e+01, 7.45579972e+01, 7.44924510e+01, 7.44269048e+01,\n",
       "        7.92353516e+01, 0.00000000e+00, 0.00000000e+00, 5.85864365e-01,\n",
       "        3.53939682e-01, 1.07423425e+00, 2.92525411e-01, 0.00000000e+00,\n",
       "        2.52372533e-01, 7.52197992e+01, 7.49828245e+01, 7.47458497e+01,\n",
       "        3.00090790e-01, 0.00000000e+00, 7.57470588e+01, 7.55453782e+01,\n",
       "        7.53163445e+01, 7.50793697e+01, 7.48423950e+01, 7.46054202e+01,\n",
       "        7.45070168e+01, 7.44414706e+01, 7.43759244e+01, 0.00000000e+00,\n",
       "        2.42449280e-01, 7.49389402e+01, 7.47019655e+01, 7.45337208e+01,\n",
       "        7.44681746e+01, 7.44026284e+01, 7.79325630e+01, 7.67478058e+01,\n",
       "        7.59188609e+01, 0.00000000e+00, 4.78058785e-01, 0.00000000e+00,\n",
       "        4.87217605e-01, 5.77322320e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.82079101e-01, 0.00000000e+00, 0.00000000e+00, 7.44556364e+01,\n",
       "        7.43904902e+01, 0.00000000e+00, 5.08463920e-01, 7.49916013e+01,\n",
       "        7.47546265e+01, 7.45482866e+01, 7.44827404e+01, 7.44171942e+01,\n",
       "        7.80838235e+01, 7.70167134e+01, 7.60533147e+01, 9.95775300e-03,\n",
       "        0.00000000e+00, 3.95178497e-01, 7.44438982e+01, 7.43783520e+01,\n",
       "        7.75097105e+01, 7.62998133e+01, 7.56947712e+01, 7.68673203e+01,\n",
       "        7.59786181e+01, 7.53514519e+01, 5.90972530e-02, 5.18556535e+01,\n",
       "        0.00000000e+00, 6.85069580e+01, 3.11801523e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.14011917e+01, 0.00000000e+00, 1.32718444e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.98814183e-01, 0.00000000e+00, 1.20423712e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.94432819e-01, 0.00000000e+00, 3.83667201e-01,\n",
       "        5.83870232e-01, 5.99391997e-01, 9.58728909e-01, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([68.56507481, 68.55878937, 68.55250393, 68.54621849, 68.53993305,\n",
       "       68.53364761, 68.52736216, 68.52107672, 68.51479128, 68.50850584,\n",
       "       68.5022204 , 68.49593496, 68.48964952, 68.48336408, 68.47707864,\n",
       "       68.4707932 , 68.46450775, 68.45822231, 68.45193687, 68.44565143,\n",
       "       68.43936599, 68.43308055, 68.42679511, 68.42050967, 68.41422423,\n",
       "       68.40793879, 68.40165334, 68.3953679 , 68.38908246, 68.38279702,\n",
       "       68.37651158, 68.37022614, 68.3639407 , 68.35765526, 68.35136982,\n",
       "       68.34508438, 68.33879893, 68.33251349, 68.32622805, 68.31994261,\n",
       "       68.31365717, 68.30737173, 68.30108629, 68.29480085, 68.28851541,\n",
       "       68.28222997, 68.27594452, 68.26965908, 68.26337364, 68.2570882 ,\n",
       "       68.25080276, 68.24451732, 68.23823188, 68.23194644, 68.225661  ,\n",
       "       68.21937556, 68.21309011, 68.20680467, 68.20051923, 68.19423379,\n",
       "       68.18794835, 68.18166291, 68.17537747, 68.16909203, 68.16280659,\n",
       "       68.15652115, 68.1502357 , 68.14395026, 68.13766482, 68.13137938,\n",
       "       68.12509394, 68.1188085 , 68.11252306, 68.10623762, 68.09995218,\n",
       "       68.09366673, 68.08738129, 68.08109585, 68.07481041, 68.06852497,\n",
       "       68.06223953, 68.05595409, 68.04966865, 68.04338321, 68.03709777,\n",
       "       68.03081232, 68.02452688, 68.01824144, 68.011956  , 68.00567056,\n",
       "       67.99938512, 67.99309968, 67.98681424, 67.9805288 , 67.97424336,\n",
       "       67.96795791, 67.96167247, 67.95538703, 67.94910159, 67.94281615])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.21528326438763\n",
      "39.93803822089259\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
