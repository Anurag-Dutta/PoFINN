{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "1545    72.143627\n",
       "1546    72.137091\n",
       "1547    72.130556\n",
       "1548    72.124020\n",
       "1549    72.117484\n",
       "Name: C3, Length: 1550, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c3_interpolated_1450_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "1445     0.000000\n",
       "1446     0.437472\n",
       "1447     0.000000\n",
       "1448     0.000000\n",
       "1449     0.000000\n",
       "Name: C3, Length: 1450, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1450)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.100000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.875910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.651821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90.427731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90.203641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     91.100000  0.000298\n",
       "1     90.875910  0.000298\n",
       "2     90.651821  0.000297\n",
       "3     90.427731  0.000297\n",
       "4     90.203641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.0403  # Value for C0\n",
    "K0 = -0.0012  # Value for K0\n",
    "K1 = -0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0120    # Value for b\n",
    "c = 2.0334    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    91.100000    90.875910    90.651821    90.427731    90.203641   \n",
      "351    90.875910    90.651821    90.427731    90.203641    89.979552   \n",
      "352    90.651821    90.427731    90.203641    89.979552    89.755462   \n",
      "353    90.427731    90.203641    89.979552    89.755462    89.531373   \n",
      "354    90.203641    89.979552    89.755462    89.531373    89.307283   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    89.979552    89.755462    89.531373    89.307283    89.094118  ...   \n",
      "351    89.755462    89.531373    89.307283    89.094118    89.015686  ...   \n",
      "352    89.531373    89.307283    89.094118    89.015686    88.937255  ...   \n",
      "353    89.307283    89.094118    89.015686    88.937255    88.858824  ...   \n",
      "354    89.094118    89.015686    88.937255    88.858824    88.780392  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   81.423343    0.000263   81.404669    0.000263   81.385994    0.000263   \n",
      "351   81.404669    0.000263   81.385994    0.000263   81.367320    0.000262   \n",
      "352   81.385994    0.000263   81.367320    0.000262   81.348646    0.000262   \n",
      "353   81.367320    0.000262   81.348646    0.000262   81.329972    0.000262   \n",
      "354   81.348646    0.000262   81.329972    0.000262   81.311298    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   81.367320    0.000262   81.348646    0.000262  \n",
      "351   81.348646    0.000262   81.329972    0.000262  \n",
      "352   81.329972    0.000262   81.311298    0.000262  \n",
      "353   81.311298    0.000262   81.292624    0.000262  \n",
      "354   81.292624    0.000262   81.273950    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1, 251) (1000, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "13/13 [==============================] - 3s 62ms/step - loss: 6099.0088 - val_loss: 5363.1182\n",
      "Epoch 2/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 6034.7700 - val_loss: 5317.8354\n",
      "Epoch 3/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 5986.7305 - val_loss: 5272.7407\n",
      "Epoch 4/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 5929.3755 - val_loss: 5212.1904\n",
      "Epoch 5/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 5873.7441 - val_loss: 5165.5776\n",
      "Epoch 6/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5821.2104 - val_loss: 5107.9824\n",
      "Epoch 7/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 5762.1523 - val_loss: 5059.6924\n",
      "Epoch 8/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5710.9351 - val_loss: 5011.8613\n",
      "Epoch 9/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5660.3242 - val_loss: 4959.4795\n",
      "Epoch 10/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 5603.7163 - val_loss: 4910.1499\n",
      "Epoch 11/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5551.1089 - val_loss: 4861.0063\n",
      "Epoch 12/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 5499.1182 - val_loss: 4812.6411\n",
      "Epoch 13/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 5447.9160 - val_loss: 4764.9824\n",
      "Epoch 14/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5397.4043 - val_loss: 4717.9292\n",
      "Epoch 15/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 5347.4868 - val_loss: 4671.4111\n",
      "Epoch 16/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 5298.0981 - val_loss: 4625.3740\n",
      "Epoch 17/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5249.1914 - val_loss: 4579.7842\n",
      "Epoch 18/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 5200.7329 - val_loss: 4534.6167\n",
      "Epoch 19/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5152.6992 - val_loss: 4489.8501\n",
      "Epoch 20/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5105.0723 - val_loss: 4445.4692\n",
      "Epoch 21/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5057.8359 - val_loss: 4401.4609\n",
      "Epoch 22/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5010.9795 - val_loss: 4357.8174\n",
      "Epoch 23/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 4964.4917 - val_loss: 4314.5283\n",
      "Epoch 24/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4918.3657 - val_loss: 4271.5854\n",
      "Epoch 25/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4872.5928 - val_loss: 4228.9863\n",
      "Epoch 26/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4827.1680 - val_loss: 4186.7207\n",
      "Epoch 27/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 4782.0845 - val_loss: 4144.7871\n",
      "Epoch 28/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 4737.3379 - val_loss: 4103.1792\n",
      "Epoch 29/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 4692.9238 - val_loss: 4061.8931\n",
      "Epoch 30/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4648.8379 - val_loss: 4020.9260\n",
      "Epoch 31/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4605.0767 - val_loss: 3980.2725\n",
      "Epoch 32/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4561.6367 - val_loss: 3939.9312\n",
      "Epoch 33/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4518.5146 - val_loss: 3899.8992\n",
      "Epoch 34/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4475.7070 - val_loss: 3860.1719\n",
      "Epoch 35/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4433.2109 - val_loss: 3820.7485\n",
      "Epoch 36/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4391.0254 - val_loss: 3781.6252\n",
      "Epoch 37/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4349.1455 - val_loss: 3742.7993\n",
      "Epoch 38/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4307.5698 - val_loss: 3704.2693\n",
      "Epoch 39/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4266.2964 - val_loss: 3666.0332\n",
      "Epoch 40/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4225.3218 - val_loss: 3628.0874\n",
      "Epoch 41/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4184.6445 - val_loss: 3590.4312\n",
      "Epoch 42/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4144.2627 - val_loss: 3553.0627\n",
      "Epoch 43/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4104.1738 - val_loss: 3515.9795\n",
      "Epoch 44/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4064.3762 - val_loss: 3479.1785\n",
      "Epoch 45/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4024.8679 - val_loss: 3442.6587\n",
      "Epoch 46/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3985.6462 - val_loss: 3406.4187\n",
      "Epoch 47/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3946.7109 - val_loss: 3370.4563\n",
      "Epoch 48/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3908.0581 - val_loss: 3334.7703\n",
      "Epoch 49/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3869.6885 - val_loss: 3299.3582\n",
      "Epoch 50/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3831.5981 - val_loss: 3264.2185\n",
      "Epoch 51/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3793.7866 - val_loss: 3229.3496\n",
      "Epoch 52/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3756.2515 - val_loss: 3194.7507\n",
      "Epoch 53/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 3718.9919 - val_loss: 3160.4177\n",
      "Epoch 54/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3682.0049 - val_loss: 3126.3523\n",
      "Epoch 55/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3645.2903 - val_loss: 3092.5510\n",
      "Epoch 56/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3608.8459 - val_loss: 3059.0122\n",
      "Epoch 57/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3572.6707 - val_loss: 3025.7351\n",
      "Epoch 58/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3536.7620 - val_loss: 2992.7175\n",
      "Epoch 59/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3501.1191 - val_loss: 2959.9578\n",
      "Epoch 60/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3465.7407 - val_loss: 2927.4568\n",
      "Epoch 61/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3430.6248 - val_loss: 2895.2097\n",
      "Epoch 62/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3395.7703 - val_loss: 2863.2163\n",
      "Epoch 63/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3361.1753 - val_loss: 2831.4766\n",
      "Epoch 64/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3326.8391 - val_loss: 2799.9878\n",
      "Epoch 65/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3292.7598 - val_loss: 2768.7488\n",
      "Epoch 66/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3258.9355 - val_loss: 2737.7585\n",
      "Epoch 67/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3225.3657 - val_loss: 2707.0146\n",
      "Epoch 68/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3192.0491 - val_loss: 2676.5166\n",
      "Epoch 69/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3158.9841 - val_loss: 2646.2627\n",
      "Epoch 70/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3126.1685 - val_loss: 2616.2522\n",
      "Epoch 71/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3093.6016 - val_loss: 2586.4836\n",
      "Epoch 72/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3061.2822 - val_loss: 2556.9551\n",
      "Epoch 73/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3029.2090 - val_loss: 2527.6660\n",
      "Epoch 74/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2997.3809 - val_loss: 2498.6138\n",
      "Epoch 75/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2965.7957 - val_loss: 2469.7983\n",
      "Epoch 76/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2934.4529 - val_loss: 2441.2180\n",
      "Epoch 77/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2903.3508 - val_loss: 2412.8716\n",
      "Epoch 78/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2872.4888 - val_loss: 2384.7581\n",
      "Epoch 79/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2841.8650 - val_loss: 2356.8762\n",
      "Epoch 80/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2811.4778 - val_loss: 2329.2246\n",
      "Epoch 81/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2781.3269 - val_loss: 2301.8010\n",
      "Epoch 82/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2751.4104 - val_loss: 2274.6055\n",
      "Epoch 83/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2721.7278 - val_loss: 2247.6370\n",
      "Epoch 84/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2692.2769 - val_loss: 2220.8936\n",
      "Epoch 85/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2663.0576 - val_loss: 2194.3738\n",
      "Epoch 86/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2634.0671 - val_loss: 2168.0774\n",
      "Epoch 87/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2605.3057 - val_loss: 2142.0017\n",
      "Epoch 88/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2576.7712 - val_loss: 2116.1472\n",
      "Epoch 89/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2548.4631 - val_loss: 2090.5115\n",
      "Epoch 90/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2520.3804 - val_loss: 2065.0947\n",
      "Epoch 91/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2492.5210 - val_loss: 2039.8945\n",
      "Epoch 92/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2464.8848 - val_loss: 2014.9108\n",
      "Epoch 93/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2437.4695 - val_loss: 1990.1405\n",
      "Epoch 94/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2410.2749 - val_loss: 1965.5844\n",
      "Epoch 95/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2383.2993 - val_loss: 1941.2400\n",
      "Epoch 96/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2356.5417 - val_loss: 1917.1078\n",
      "Epoch 97/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2330.0010 - val_loss: 1893.1848\n",
      "Epoch 98/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2303.6758 - val_loss: 1869.4705\n",
      "Epoch 99/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2277.5654 - val_loss: 1845.9648\n",
      "Epoch 100/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2251.6687 - val_loss: 1822.6656\n",
      "Epoch 101/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2225.9841 - val_loss: 1799.5720\n",
      "Epoch 102/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2200.5110 - val_loss: 1776.6830\n",
      "Epoch 103/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2175.2480 - val_loss: 1753.9968\n",
      "Epoch 104/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2150.1941 - val_loss: 1731.5135\n",
      "Epoch 105/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2125.3484 - val_loss: 1709.2311\n",
      "Epoch 106/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2100.7095 - val_loss: 1687.1495\n",
      "Epoch 107/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2076.2769 - val_loss: 1665.2667\n",
      "Epoch 108/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2052.0481 - val_loss: 1643.5811\n",
      "Epoch 109/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2028.0236 - val_loss: 1622.0933\n",
      "Epoch 110/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2004.2019 - val_loss: 1600.8004\n",
      "Epoch 111/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1980.5811 - val_loss: 1579.7030\n",
      "Epoch 112/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1957.1611 - val_loss: 1558.7996\n",
      "Epoch 113/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1933.9403 - val_loss: 1538.0867\n",
      "Epoch 114/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1910.9181 - val_loss: 1517.5680\n",
      "Epoch 115/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 1888.0930 - val_loss: 1497.2389\n",
      "Epoch 116/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1865.4645 - val_loss: 1477.0984\n",
      "Epoch 117/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1843.0312 - val_loss: 1457.1479\n",
      "Epoch 118/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1820.7920 - val_loss: 1437.3844\n",
      "Epoch 119/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1798.7466 - val_loss: 1417.8069\n",
      "Epoch 120/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1776.8928 - val_loss: 1398.4141\n",
      "Epoch 121/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 1755.2295 - val_loss: 1379.2063\n",
      "Epoch 122/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1733.7572 - val_loss: 1360.1824\n",
      "Epoch 123/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1712.4734 - val_loss: 1341.3395\n",
      "Epoch 124/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1691.3779 - val_loss: 1322.6785\n",
      "Epoch 125/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1670.4692 - val_loss: 1304.1982\n",
      "Epoch 126/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1649.7466 - val_loss: 1285.8960\n",
      "Epoch 127/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1629.2091 - val_loss: 1267.7731\n",
      "Epoch 128/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1608.8558 - val_loss: 1249.8263\n",
      "Epoch 129/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1588.6852 - val_loss: 1232.0564\n",
      "Epoch 130/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1568.6965 - val_loss: 1214.4615\n",
      "Epoch 131/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1548.8896 - val_loss: 1197.0414\n",
      "Epoch 132/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1529.2623 - val_loss: 1179.7939\n",
      "Epoch 133/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1509.8141 - val_loss: 1162.7186\n",
      "Epoch 134/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1490.5439 - val_loss: 1145.8156\n",
      "Epoch 135/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1471.4509 - val_loss: 1129.0820\n",
      "Epoch 136/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1452.5342 - val_loss: 1112.5177\n",
      "Epoch 137/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1433.7925 - val_loss: 1096.1221\n",
      "Epoch 138/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1415.2251 - val_loss: 1079.8933\n",
      "Epoch 139/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1396.8309 - val_loss: 1063.8313\n",
      "Epoch 140/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1378.6094 - val_loss: 1047.9351\n",
      "Epoch 141/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1360.5591 - val_loss: 1032.2026\n",
      "Epoch 142/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 1342.6790 - val_loss: 1016.6339\n",
      "Epoch 143/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1324.9684 - val_loss: 1001.2277\n",
      "Epoch 144/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1307.4265 - val_loss: 985.9833\n",
      "Epoch 145/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1290.0519 - val_loss: 970.8998\n",
      "Epoch 146/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1272.8439 - val_loss: 955.9756\n",
      "Epoch 147/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1255.8015 - val_loss: 941.2100\n",
      "Epoch 148/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1238.9240 - val_loss: 926.6018\n",
      "Epoch 149/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1222.2100 - val_loss: 912.1510\n",
      "Epoch 150/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1205.6588 - val_loss: 897.8558\n",
      "Epoch 151/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1189.2699 - val_loss: 883.7160\n",
      "Epoch 152/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 1173.0420 - val_loss: 869.7307\n",
      "Epoch 153/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1156.9739 - val_loss: 855.8975\n",
      "Epoch 154/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1141.0648 - val_loss: 842.2167\n",
      "Epoch 155/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1125.3141 - val_loss: 828.6870\n",
      "Epoch 156/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1109.7207 - val_loss: 815.3082\n",
      "Epoch 157/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1094.2838 - val_loss: 802.0778\n",
      "Epoch 158/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1079.0022 - val_loss: 788.9966\n",
      "Epoch 159/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1063.8749 - val_loss: 776.0627\n",
      "Epoch 160/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1048.9016 - val_loss: 763.2746\n",
      "Epoch 161/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1034.0807 - val_loss: 750.6334\n",
      "Epoch 162/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1019.4118 - val_loss: 738.1360\n",
      "Epoch 163/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1004.8938 - val_loss: 725.7823\n",
      "Epoch 164/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 990.5251 - val_loss: 713.5720\n",
      "Epoch 165/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 976.3063 - val_loss: 701.5036\n",
      "Epoch 166/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 962.2352 - val_loss: 689.5767\n",
      "Epoch 167/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 948.3118 - val_loss: 677.7891\n",
      "Epoch 168/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 934.5342 - val_loss: 666.1406\n",
      "Epoch 169/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 920.9025 - val_loss: 654.6312\n",
      "Epoch 170/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 907.4154 - val_loss: 643.2587\n",
      "Epoch 171/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 894.0720 - val_loss: 632.0235\n",
      "Epoch 172/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 880.8710 - val_loss: 620.9231\n",
      "Epoch 173/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 867.8123 - val_loss: 609.9578\n",
      "Epoch 174/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 854.8945 - val_loss: 599.1259\n",
      "Epoch 175/500\n",
      "13/13 [==============================] - 0s 21ms/step - loss: 842.1169 - val_loss: 588.4274\n",
      "Epoch 176/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 829.4785 - val_loss: 577.8607\n",
      "Epoch 177/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 816.9786 - val_loss: 567.4255\n",
      "Epoch 178/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 804.6160 - val_loss: 557.1203\n",
      "Epoch 179/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 792.3903 - val_loss: 546.9440\n",
      "Epoch 180/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 780.2997 - val_loss: 536.8970\n",
      "Epoch 181/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 768.3445 - val_loss: 526.9763\n",
      "Epoch 182/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 756.5232 - val_loss: 517.1833\n",
      "Epoch 183/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 744.8348 - val_loss: 507.5153\n",
      "Epoch 184/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 733.2787 - val_loss: 497.9729\n",
      "Epoch 185/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 721.8540 - val_loss: 488.5544\n",
      "Epoch 186/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 710.5598 - val_loss: 479.2585\n",
      "Epoch 187/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 699.3951 - val_loss: 470.0850\n",
      "Epoch 188/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 688.3593 - val_loss: 461.0327\n",
      "Epoch 189/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 677.4515 - val_loss: 452.1013\n",
      "Epoch 190/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 666.6705 - val_loss: 443.2894\n",
      "Epoch 191/500\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 656.0159 - val_loss: 434.5964\n",
      "Epoch 192/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 645.4866 - val_loss: 426.0209\n",
      "Epoch 193/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 635.0814 - val_loss: 417.5618\n",
      "Epoch 194/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 624.7997 - val_loss: 409.2196\n",
      "Epoch 195/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 614.6411 - val_loss: 400.9922\n",
      "Epoch 196/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 604.6042 - val_loss: 392.8795\n",
      "Epoch 197/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 594.6882 - val_loss: 384.8800\n",
      "Epoch 198/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 584.8924 - val_loss: 376.9929\n",
      "Epoch 199/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 575.2158 - val_loss: 369.2173\n",
      "Epoch 200/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 565.6578 - val_loss: 361.5530\n",
      "Epoch 201/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 556.2175 - val_loss: 353.9987\n",
      "Epoch 202/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 546.8938 - val_loss: 346.5531\n",
      "Epoch 203/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 537.6859 - val_loss: 339.2162\n",
      "Epoch 204/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 528.5933 - val_loss: 331.9864\n",
      "Epoch 205/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 519.6146 - val_loss: 324.8631\n",
      "Epoch 206/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 510.7494 - val_loss: 317.8453\n",
      "Epoch 207/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 501.9967 - val_loss: 310.9329\n",
      "Epoch 208/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 493.3560 - val_loss: 304.1239\n",
      "Epoch 209/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 484.8258 - val_loss: 297.4182\n",
      "Epoch 210/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 476.4055 - val_loss: 290.8141\n",
      "Epoch 211/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 468.0944 - val_loss: 284.3123\n",
      "Epoch 212/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 459.8918 - val_loss: 277.9102\n",
      "Epoch 213/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 451.7963 - val_loss: 271.6079\n",
      "Epoch 214/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 443.8076 - val_loss: 265.4049\n",
      "Epoch 215/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 435.9251 - val_loss: 259.2995\n",
      "Epoch 216/500\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 428.1472 - val_loss: 253.2908\n",
      "Epoch 217/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 420.4735 - val_loss: 247.3790\n",
      "Epoch 218/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 412.9032 - val_loss: 241.5619\n",
      "Epoch 219/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 405.4349 - val_loss: 235.8395\n",
      "Epoch 220/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 398.0685 - val_loss: 230.2106\n",
      "Epoch 221/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 390.8028 - val_loss: 224.6744\n",
      "Epoch 222/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 383.6370 - val_loss: 219.2305\n",
      "Epoch 223/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 376.5705 - val_loss: 213.8777\n",
      "Epoch 224/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 369.6024 - val_loss: 208.6150\n",
      "Epoch 225/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 362.7317 - val_loss: 203.4422\n",
      "Epoch 226/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 355.9577 - val_loss: 198.3576\n",
      "Epoch 227/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 349.2795 - val_loss: 193.3605\n",
      "Epoch 228/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 342.6966 - val_loss: 188.4508\n",
      "Epoch 229/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 336.2078 - val_loss: 183.6272\n",
      "Epoch 230/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 329.8125 - val_loss: 178.8889\n",
      "Epoch 231/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 323.5099 - val_loss: 174.2349\n",
      "Epoch 232/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 317.2989 - val_loss: 169.6645\n",
      "Epoch 233/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 311.1788 - val_loss: 165.1767\n",
      "Epoch 234/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 305.1489 - val_loss: 160.7708\n",
      "Epoch 235/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 299.2083 - val_loss: 156.4457\n",
      "Epoch 236/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 293.3561 - val_loss: 152.2012\n",
      "Epoch 237/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 287.5916 - val_loss: 148.0358\n",
      "Epoch 238/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 281.9140 - val_loss: 143.9493\n",
      "Epoch 239/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 276.3225 - val_loss: 139.9403\n",
      "Epoch 240/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 270.8163 - val_loss: 136.0082\n",
      "Epoch 241/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 265.3946 - val_loss: 132.1521\n",
      "Epoch 242/500\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 260.0564 - val_loss: 128.3716\n",
      "Epoch 243/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 254.8014 - val_loss: 124.6658\n",
      "Epoch 244/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 249.6284 - val_loss: 121.0329\n",
      "Epoch 245/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 244.5364 - val_loss: 117.4734\n",
      "Epoch 246/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 239.5250 - val_loss: 113.9856\n",
      "Epoch 247/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 234.5933 - val_loss: 110.5695\n",
      "Epoch 248/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 229.7408 - val_loss: 107.2233\n",
      "Epoch 249/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 224.9662 - val_loss: 103.9468\n",
      "Epoch 250/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 220.2688 - val_loss: 100.7393\n",
      "Epoch 251/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 215.6480 - val_loss: 97.5994\n",
      "Epoch 252/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 211.1028 - val_loss: 94.5269\n",
      "Epoch 253/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 206.6328 - val_loss: 91.5207\n",
      "Epoch 254/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 202.2368 - val_loss: 88.5799\n",
      "Epoch 255/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 197.9142 - val_loss: 85.7042\n",
      "Epoch 256/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 193.6644 - val_loss: 82.8921\n",
      "Epoch 257/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 189.4861 - val_loss: 80.1436\n",
      "Epoch 258/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 185.3791 - val_loss: 77.4568\n",
      "Epoch 259/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 181.3423 - val_loss: 74.8323\n",
      "Epoch 260/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 177.3750 - val_loss: 72.2682\n",
      "Epoch 261/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 173.4766 - val_loss: 69.7640\n",
      "Epoch 262/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 169.6460 - val_loss: 67.3195\n",
      "Epoch 263/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 165.8828 - val_loss: 64.9331\n",
      "Epoch 264/500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 162.1860 - val_loss: 62.6046\n",
      "Epoch 265/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 158.5549 - val_loss: 60.3332\n",
      "Epoch 266/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 154.9886 - val_loss: 58.1173\n",
      "Epoch 267/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 151.4865 - val_loss: 55.9572\n",
      "Epoch 268/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 148.0479 - val_loss: 53.8517\n",
      "Epoch 269/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 144.6722 - val_loss: 51.8000\n",
      "Epoch 270/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 141.3582 - val_loss: 49.8013\n",
      "Epoch 271/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 138.1054 - val_loss: 47.8551\n",
      "Epoch 272/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 134.9131 - val_loss: 45.9602\n",
      "Epoch 273/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 131.7806 - val_loss: 44.1161\n",
      "Epoch 274/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 128.7070 - val_loss: 42.3224\n",
      "Epoch 275/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 125.6915 - val_loss: 40.5776\n",
      "Epoch 276/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 122.7335 - val_loss: 38.8813\n",
      "Epoch 277/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 119.8323 - val_loss: 37.2330\n",
      "Epoch 278/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 116.9869 - val_loss: 35.6316\n",
      "Epoch 279/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 114.1972 - val_loss: 34.0769\n",
      "Epoch 280/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 111.4621 - val_loss: 32.5676\n",
      "Epoch 281/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 108.7806 - val_loss: 31.1032\n",
      "Epoch 282/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 106.1525 - val_loss: 29.6829\n",
      "Epoch 283/500\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 103.5768 - val_loss: 28.3062\n",
      "Epoch 284/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 101.0528 - val_loss: 26.9721\n",
      "Epoch 285/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 98.5798 - val_loss: 25.6798\n",
      "Epoch 286/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 96.1571 - val_loss: 24.4292\n",
      "Epoch 287/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 93.7840 - val_loss: 23.2190\n",
      "Epoch 288/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 91.4599 - val_loss: 22.0487\n",
      "Epoch 289/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 89.1843 - val_loss: 20.9175\n",
      "Epoch 290/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 86.9561 - val_loss: 19.8249\n",
      "Epoch 291/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 84.7747 - val_loss: 18.7702\n",
      "Epoch 292/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 82.6397 - val_loss: 17.7526\n",
      "Epoch 293/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 80.5501 - val_loss: 16.7715\n",
      "Epoch 294/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 78.5053 - val_loss: 15.8262\n",
      "Epoch 295/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 76.5050 - val_loss: 14.9160\n",
      "Epoch 296/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 74.5483 - val_loss: 14.0401\n",
      "Epoch 297/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 72.6341 - val_loss: 13.1980\n",
      "Epoch 298/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 70.7624 - val_loss: 12.3892\n",
      "Epoch 299/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 68.9321 - val_loss: 11.6128\n",
      "Epoch 300/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 67.1427 - val_loss: 10.8682\n",
      "Epoch 301/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 65.3937 - val_loss: 10.1548\n",
      "Epoch 302/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 63.6842 - val_loss: 9.4718\n",
      "Epoch 303/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 62.0137 - val_loss: 8.8186\n",
      "Epoch 304/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 60.3816 - val_loss: 8.1950\n",
      "Epoch 305/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 58.7873 - val_loss: 7.6000\n",
      "Epoch 306/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 57.2302 - val_loss: 7.0330\n",
      "Epoch 307/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 55.7096 - val_loss: 6.4933\n",
      "Epoch 308/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 54.2247 - val_loss: 5.9805\n",
      "Epoch 309/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 52.7752 - val_loss: 5.4940\n",
      "Epoch 310/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 51.3604 - val_loss: 5.0329\n",
      "Epoch 311/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 49.9796 - val_loss: 4.5970\n",
      "Epoch 312/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 48.6324 - val_loss: 4.1855\n",
      "Epoch 313/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 47.3180 - val_loss: 3.7978\n",
      "Epoch 314/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 46.0360 - val_loss: 3.4334\n",
      "Epoch 315/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 44.7857 - val_loss: 3.0918\n",
      "Epoch 316/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 43.5666 - val_loss: 2.7722\n",
      "Epoch 317/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 42.3781 - val_loss: 2.4743\n",
      "Epoch 318/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 41.2196 - val_loss: 2.1975\n",
      "Epoch 319/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 40.0907 - val_loss: 1.9411\n",
      "Epoch 320/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 38.9906 - val_loss: 1.7047\n",
      "Epoch 321/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 37.9189 - val_loss: 1.4876\n",
      "Epoch 322/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 36.8750 - val_loss: 1.2896\n",
      "Epoch 323/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 35.8586 - val_loss: 1.1098\n",
      "Epoch 324/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 34.8687 - val_loss: 0.9480\n",
      "Epoch 325/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 33.9053 - val_loss: 0.8034\n",
      "Epoch 326/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 32.9676 - val_loss: 0.6758\n",
      "Epoch 327/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 32.0551 - val_loss: 0.5645\n",
      "Epoch 328/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 31.1672 - val_loss: 0.4691\n",
      "Epoch 329/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.3037 - val_loss: 0.3891\n",
      "Epoch 330/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 29.4637 - val_loss: 0.3239\n",
      "Epoch 331/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 28.6470 - val_loss: 0.2732\n",
      "Epoch 332/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 27.8531 - val_loss: 0.2365\n",
      "Epoch 333/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 27.0815 - val_loss: 0.2133\n",
      "Epoch 334/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 26.3316 - val_loss: 0.2032\n",
      "Epoch 335/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 25.6030 - val_loss: 0.2057\n",
      "Epoch 336/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 24.8953 - val_loss: 0.2204\n",
      "Epoch 337/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 24.2079 - val_loss: 0.2468\n",
      "Epoch 338/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 23.5404 - val_loss: 0.2846\n",
      "Epoch 339/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 22.8925 - val_loss: 0.3333\n",
      "Epoch 340/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 22.2636 - val_loss: 0.3925\n",
      "Epoch 341/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 21.6533 - val_loss: 0.4619\n",
      "Epoch 342/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 21.0612 - val_loss: 0.5409\n",
      "Epoch 343/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.4869 - val_loss: 0.6292\n",
      "Epoch 344/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.9301 - val_loss: 0.7265\n",
      "Epoch 345/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.3900 - val_loss: 0.8324\n",
      "Epoch 346/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 18.8666 - val_loss: 0.9464\n",
      "Epoch 347/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 18.3593 - val_loss: 1.0683\n",
      "Epoch 348/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 17.8678 - val_loss: 1.1976\n",
      "Epoch 349/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 17.3917 - val_loss: 1.3341\n",
      "Epoch 350/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 16.9305 - val_loss: 1.4774\n",
      "Epoch 351/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 16.4839 - val_loss: 1.6272\n",
      "Epoch 352/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 16.0515 - val_loss: 1.7830\n",
      "Epoch 353/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 15.6331 - val_loss: 1.9447\n",
      "Epoch 354/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 15.2281 - val_loss: 2.1118\n",
      "Epoch 355/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 14.8365 - val_loss: 2.2842\n",
      "Epoch 356/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 14.4576 - val_loss: 2.4614\n",
      "Epoch 357/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 14.0912 - val_loss: 2.6432\n",
      "Epoch 358/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 13.7371 - val_loss: 2.8294\n",
      "Epoch 359/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 13.3947 - val_loss: 3.0195\n",
      "Epoch 360/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 13.0639 - val_loss: 3.2134\n",
      "Epoch 361/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.7443 - val_loss: 3.4107\n",
      "Epoch 362/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 12.4356 - val_loss: 3.6113\n",
      "Epoch 363/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 12.1376 - val_loss: 3.8149\n",
      "Epoch 364/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 11.8497 - val_loss: 4.0212\n",
      "Epoch 365/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 11.5720 - val_loss: 4.2300\n",
      "Epoch 366/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.3040 - val_loss: 4.4409\n",
      "Epoch 367/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 11.0454 - val_loss: 4.6539\n",
      "Epoch 368/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 10.7960 - val_loss: 4.8687\n",
      "Epoch 369/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 10.5555 - val_loss: 5.0851\n",
      "Epoch 370/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 10.3237 - val_loss: 5.3028\n",
      "Epoch 371/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 10.1003 - val_loss: 5.5219\n",
      "Epoch 372/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.8849 - val_loss: 5.7418\n",
      "Epoch 373/500\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 9.6775 - val_loss: 5.9624\n",
      "Epoch 374/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 9.4778 - val_loss: 6.1838\n",
      "Epoch 375/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 9.2854 - val_loss: 6.4054\n",
      "Epoch 376/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.1003 - val_loss: 6.6273\n",
      "Epoch 377/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 8.9222 - val_loss: 6.8492\n",
      "Epoch 378/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 8.7507 - val_loss: 7.0712\n",
      "Epoch 379/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 8.5858 - val_loss: 7.2928\n",
      "Epoch 380/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 8.4272 - val_loss: 7.5141\n",
      "Epoch 381/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 8.2747 - val_loss: 7.7350\n",
      "Epoch 382/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 8.1281 - val_loss: 7.9551\n",
      "Epoch 383/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 7.9873 - val_loss: 8.1744\n",
      "Epoch 384/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 7.8520 - val_loss: 8.3927\n",
      "Epoch 385/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 7.7220 - val_loss: 8.6100\n",
      "Epoch 386/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 7.5972 - val_loss: 8.8261\n",
      "Epoch 387/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 7.4774 - val_loss: 9.0409\n",
      "Epoch 388/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 7.3625 - val_loss: 9.2542\n",
      "Epoch 389/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 7.2521 - val_loss: 9.4663\n",
      "Epoch 390/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 7.1463 - val_loss: 9.6766\n",
      "Epoch 391/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 7.0448 - val_loss: 9.8852\n",
      "Epoch 392/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 6.9474 - val_loss: 10.0923\n",
      "Epoch 393/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 6.8541 - val_loss: 10.2973\n",
      "Epoch 394/500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 6.7646 - val_loss: 10.5003\n",
      "Epoch 395/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 6.6789 - val_loss: 10.7014\n",
      "Epoch 396/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 6.5969 - val_loss: 10.9006\n",
      "Epoch 397/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 6.5183 - val_loss: 11.0972\n",
      "Epoch 398/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 6.4431 - val_loss: 11.2920\n",
      "Epoch 399/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 6.3710 - val_loss: 11.4844\n",
      "Epoch 400/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 6.3021 - val_loss: 11.6745\n",
      "Epoch 401/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 6.2361 - val_loss: 11.8622\n",
      "Epoch 402/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 6.1731 - val_loss: 12.0473\n",
      "Epoch 403/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 6.1128 - val_loss: 12.2302\n",
      "Epoch 404/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 6.0551 - val_loss: 12.4106\n",
      "Epoch 405/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.9999 - val_loss: 12.5883\n",
      "Epoch 406/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.9473 - val_loss: 12.7637\n",
      "Epoch 407/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.8969 - val_loss: 12.9362\n",
      "Epoch 408/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 5.8489 - val_loss: 13.1061\n",
      "Epoch 409/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 5.8030 - val_loss: 13.2733\n",
      "Epoch 410/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.7592 - val_loss: 13.4381\n",
      "Epoch 411/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 5.7174 - val_loss: 13.5999\n",
      "Epoch 412/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 5.6775 - val_loss: 13.7592\n",
      "Epoch 413/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 5.6394 - val_loss: 13.9158\n",
      "Epoch 414/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 5.6031 - val_loss: 14.0695\n",
      "Epoch 415/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 5.5684 - val_loss: 14.2206\n",
      "Epoch 416/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 5.5354 - val_loss: 14.3689\n",
      "Epoch 417/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 5.5040 - val_loss: 14.5145\n",
      "Epoch 418/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 5.4739 - val_loss: 14.6573\n",
      "Epoch 419/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5.4454 - val_loss: 14.7975\n",
      "Epoch 420/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5.4181 - val_loss: 14.9349\n",
      "Epoch 421/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.3922 - val_loss: 15.0696\n",
      "Epoch 422/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5.3675 - val_loss: 15.2018\n",
      "Epoch 423/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.3439 - val_loss: 15.3309\n",
      "Epoch 424/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 5.3215 - val_loss: 15.4576\n",
      "Epoch 425/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5.3002 - val_loss: 15.5816\n",
      "Epoch 426/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5.2799 - val_loss: 15.7030\n",
      "Epoch 427/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 5.2606 - val_loss: 15.8218\n",
      "Epoch 428/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5.2423 - val_loss: 15.9379\n",
      "Epoch 429/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5.2248 - val_loss: 16.0515\n",
      "Epoch 430/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 5.2082 - val_loss: 16.1623\n",
      "Epoch 431/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.1924 - val_loss: 16.2710\n",
      "Epoch 432/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.1773 - val_loss: 16.3766\n",
      "Epoch 433/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 5.1631 - val_loss: 16.4800\n",
      "Epoch 434/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.1495 - val_loss: 16.5810\n",
      "Epoch 435/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.1366 - val_loss: 16.6793\n",
      "Epoch 436/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 5.1244 - val_loss: 16.7755\n",
      "Epoch 437/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.1127 - val_loss: 16.8691\n",
      "Epoch 438/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.1017 - val_loss: 16.9606\n",
      "Epoch 439/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.0911 - val_loss: 17.0496\n",
      "Epoch 440/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5.0811 - val_loss: 17.1364\n",
      "Epoch 441/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5.0717 - val_loss: 17.2207\n",
      "Epoch 442/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 5.0627 - val_loss: 17.3029\n",
      "Epoch 443/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5.0541 - val_loss: 17.3831\n",
      "Epoch 444/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5.0460 - val_loss: 17.4612\n",
      "Epoch 445/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 5.0383 - val_loss: 17.5370\n",
      "Epoch 446/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5.0310 - val_loss: 17.6108\n",
      "Epoch 447/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5.0240 - val_loss: 17.6826\n",
      "Epoch 448/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 5.0174 - val_loss: 17.7521\n",
      "Epoch 449/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 5.0112 - val_loss: 17.8200\n",
      "Epoch 450/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5.0052 - val_loss: 17.8858\n",
      "Epoch 451/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 4.9996 - val_loss: 17.9495\n",
      "Epoch 452/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 4.9943 - val_loss: 18.0118\n",
      "Epoch 453/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 4.9892 - val_loss: 18.0717\n",
      "Epoch 454/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 4.9843 - val_loss: 18.1301\n",
      "Epoch 455/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.9798 - val_loss: 18.1867\n",
      "Epoch 456/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.9754 - val_loss: 18.2416\n",
      "Epoch 457/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.9713 - val_loss: 18.2949\n",
      "Epoch 458/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.9674 - val_loss: 18.3464\n",
      "Epoch 459/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 4.9637 - val_loss: 18.3962\n",
      "Epoch 460/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.9602 - val_loss: 18.4446\n",
      "Epoch 461/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4.9569 - val_loss: 18.4916\n",
      "Epoch 462/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.9537 - val_loss: 18.5367\n",
      "Epoch 463/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.9507 - val_loss: 18.5805\n",
      "Epoch 464/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.9478 - val_loss: 18.6231\n",
      "Epoch 465/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.9451 - val_loss: 18.6639\n",
      "Epoch 466/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.9426 - val_loss: 18.7037\n",
      "Epoch 467/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.9401 - val_loss: 18.7420\n",
      "Epoch 468/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 4.9378 - val_loss: 18.7788\n",
      "Epoch 469/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.9356 - val_loss: 18.8146\n",
      "Epoch 470/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4.9335 - val_loss: 18.8492\n",
      "Epoch 471/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.9316 - val_loss: 18.8825\n",
      "Epoch 472/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 4.9297 - val_loss: 18.9144\n",
      "Epoch 473/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4.9279 - val_loss: 18.9458\n",
      "Epoch 474/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.9262 - val_loss: 18.9756\n",
      "Epoch 475/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.9247 - val_loss: 19.0043\n",
      "Epoch 476/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.9231 - val_loss: 19.0322\n",
      "Epoch 477/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.9217 - val_loss: 19.0590\n",
      "Epoch 478/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 4.9203 - val_loss: 19.0847\n",
      "Epoch 479/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.9190 - val_loss: 19.1095\n",
      "Epoch 480/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4.9178 - val_loss: 19.1335\n",
      "Epoch 481/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4.9166 - val_loss: 19.1564\n",
      "Epoch 482/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4.9155 - val_loss: 19.1783\n",
      "Epoch 483/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4.9145 - val_loss: 19.1997\n",
      "Epoch 484/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.9135 - val_loss: 19.2203\n",
      "Epoch 485/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4.9125 - val_loss: 19.2398\n",
      "Epoch 486/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4.9116 - val_loss: 19.2583\n",
      "Epoch 487/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4.9108 - val_loss: 19.2765\n",
      "Epoch 488/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4.9100 - val_loss: 19.2943\n",
      "Epoch 489/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.9092 - val_loss: 19.3108\n",
      "Epoch 490/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4.9085 - val_loss: 19.3268\n",
      "Epoch 491/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 4.9078 - val_loss: 19.3422\n",
      "Epoch 492/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4.9072 - val_loss: 19.3568\n",
      "Epoch 493/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4.9066 - val_loss: 19.3713\n",
      "Epoch 494/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4.9060 - val_loss: 19.3849\n",
      "Epoch 495/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4.9054 - val_loss: 19.3977\n",
      "Epoch 496/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4.9049 - val_loss: 19.4103\n",
      "Epoch 497/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4.9044 - val_loss: 19.4221\n",
      "Epoch 498/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4.9040 - val_loss: 19.4333\n",
      "Epoch 499/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4.9035 - val_loss: 19.4439\n",
      "Epoch 500/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4.9031 - val_loss: 19.4543\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.0403, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0012, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0120, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.0334, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 375ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[75.88151261, 75.85910364, 75.83669468, 75.81428571, 75.79187675,\n",
       "        75.76946779, 75.74705882, 75.72464986, 75.7022409 , 75.67983193,\n",
       "        75.65742297, 75.63501401, 75.61260504, 75.59019608, 75.56778711,\n",
       "        75.54537815, 75.52296919, 75.50056022, 75.47432773, 75.4479972 ,\n",
       "        75.42166667, 75.39533613, 75.3690056 , 75.34267507, 75.31634454,\n",
       "        75.29001401, 75.26368347, 75.23735294, 75.21102241, 75.18469188,\n",
       "        75.15836134, 75.13203081, 75.10570028, 75.07936975, 75.05303922,\n",
       "        75.02670868, 75.00037815, 74.97404762, 74.94771709, 74.92138655,\n",
       "        74.89505602, 74.86872549, 74.84239496, 74.81606443, 74.78973389,\n",
       "        74.76340336, 74.73707283, 74.7107423 , 74.68441176, 74.65808123,\n",
       "        74.6317507 , 74.60542017, 74.57908964, 74.5579972 , 74.55071429,\n",
       "        74.54343137, 74.53614846, 74.52886555, 74.52158263, 74.51429972,\n",
       "        74.50701681, 74.49973389, 74.49245098, 74.48516807, 74.47788515,\n",
       "        74.47060224, 74.46331933, 74.45603641, 74.4487535 , 74.44147059,\n",
       "        74.43418768, 74.42690476, 74.41962185, 74.41233894, 74.40505602,\n",
       "        74.39777311, 74.3904902 , 74.38320728, 74.37592437, 74.36864146,\n",
       "        79.23535156,  0.41352707,  0.        ,  0.        ,  0.5165478 ,\n",
       "         0.        ,  0.39089096,  0.        ,  0.        ,  0.        ,\n",
       "         0.30069247,  0.12899898,  0.46311682,  0.        ,  0.50846392,\n",
       "         0.        ,  0.24244928,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([72.99129318, 72.979155  , 72.96701681, 72.95487862, 72.94274043,\n",
       "       72.93060224, 72.91846405, 72.90632586, 72.89418768, 72.88204949,\n",
       "       72.8699113 , 72.85777311, 72.84563492, 72.83349673, 72.82135854,\n",
       "       72.80922035, 72.79708217, 72.78494398, 72.77280579, 72.7606676 ,\n",
       "       72.74852941, 72.73639122, 72.72425303, 72.71211485, 72.69997666,\n",
       "       72.68783847, 72.67570028, 72.66356209, 72.6514239 , 72.63928571,\n",
       "       72.62714753, 72.61500934, 72.60287115, 72.59073296, 72.57859477,\n",
       "       72.56645658, 72.55431839, 72.54218021, 72.53004202, 72.51790383,\n",
       "       72.50576564, 72.49656863, 72.49003268, 72.48349673, 72.47696078,\n",
       "       72.47042484, 72.46388889, 72.45735294, 72.45081699, 72.44428105,\n",
       "       72.4377451 , 72.43120915, 72.4246732 , 72.41813725, 72.41160131,\n",
       "       72.40506536, 72.39852941, 72.39199346, 72.38545752, 72.37892157,\n",
       "       72.37238562, 72.36584967, 72.35931373, 72.35277778, 72.34624183,\n",
       "       72.33970588, 72.33316993, 72.32663399, 72.32009804, 72.31356209,\n",
       "       72.30702614, 72.3004902 , 72.29395425, 72.2874183 , 72.28088235,\n",
       "       72.27434641, 72.26781046, 72.26127451, 72.25473856, 72.24820261,\n",
       "       72.24166667, 72.23513072, 72.22859477, 72.22205882, 72.21552288,\n",
       "       72.20898693, 72.20245098, 72.19591503, 72.18937908, 72.18284314,\n",
       "       72.17630719, 72.16977124, 72.16323529, 72.15669935, 72.1501634 ,\n",
       "       72.14362745, 72.1370915 , 72.13055556, 72.12401961, 72.11748366])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.476316545380232\n",
      "15.690491703297912\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
