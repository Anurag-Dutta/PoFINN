{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "1745    71.511741\n",
       "1746    71.507073\n",
       "1747    71.502404\n",
       "1748    71.497736\n",
       "1749    71.493067\n",
       "Name: C3, Length: 1750, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c3_interpolated_1650_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "1645     0.000000\n",
       "1646     1.046385\n",
       "1647     0.000000\n",
       "1648     0.938433\n",
       "1649     0.385843\n",
       "Name: C3, Length: 1650, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1650)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.100000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.875910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.651821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90.427731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90.203641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     91.100000  0.000298\n",
       "1     90.875910  0.000298\n",
       "2     90.651821  0.000297\n",
       "3     90.427731  0.000297\n",
       "4     90.203641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAAsTAAALEwEAmpwYAAAgsklEQVR4nO3daXhc1Z3n8e9fKu3WaslGeAcvYHajsIQATcjCkoSkSTJJZ4gnkwzdzyTprJOB5EWn0/1Mk56e7kmmMyEkZJqk6aYhIQ0JkIQmYHbSAmw24xWMd5VtyZI3yZLOvKhbcqlUsmq5dRf593kePVV1VXX0V1n+3VPnnnuuOecQEZH4qQi7ABERKY4CXEQkphTgIiIxpQAXEYkpBbiISEwlgvxh7e3tbuHChUH+SBGR2Hv++ef3OOc6srcHGuALFy6ku7s7yB8pIhJ7ZrYl13YNoYiIxJQCXEQkphTgIiIxpQAXEYkpBbiISEwpwEVEYkoBLiISU7EI8F+9tIM7n8s5DVJE5IQViwB/6OVd/M1v1jE0PBp2KSIikRGLAP9I11x6Dx3lkbW7wy5FRCQyYhHgly7p4KSmWu7u3hp2KSIikRGLAK+sMK4/fw6r1ifZ3X8k7HJERCIhFgEO8JHz5zHq4Ov3vszhoZGwyxERCV1sAnxhewN/cd0Z/G5dD5/40bP0HhwKuyQRkVDFJsABbrh4Id//xApe2dHP9bc+zdZ9h8IuSUQkNLEKcICrzuzkzs9cyN4DQ1zznSf481++yqbkgbDLEhEJnDnnAvthXV1dzq8LOmxKHuC7j2zgwZd3cnTE8fZTZ3LDRQt41/LZVFXGbr8kIjIpM3veOdc1YXtcAzwtOTDI3d1b+afn3mJ732FmN9XwsbfN5+2nzqS1oZrW+mpa6qsU6iISW9M2wNNGRh2Prevhp89uYdX6JNm/VlNtgtaGajpm1HDBojYuX9rBigWtCnYRibxpH+CZdvQd5o09B+k9NETvwSH2HTxK76Eh9h0cYnvfYVZv7WNk1NFYk+CSxe1cvqyDy5d2cHJLXdlrExEp1GQBHuhFjYNyckvdccO4/8hRnt64h1Xrkzy2LsmvX90FwJJZM7h8aQddC1tZMb+VWU21QZUsIlKwadkDL4Rzjg09B1i1Lsmq9Ul+/8Y+hkZSi2bNaaljxYJWVsxvYcX8Vk7vbKI6oSEXEQnWCTWEUorB4RFe2d7Pi2/18sJbvbywpY9d3un7NYkKzprTPC7U1UsXkXJTgJdg5/7DvLClLxXob/Xy6vb+cb300zubmNdWx7zWeua21jG3tZ55bXU01laFXLmITAcn1Bi43zqb67j27DquPbsTmNhL39RzkKc37eFQ1hotzXVVzGurY25LKtjntR0L+LmtdTTU6O0XkeIpQYpQk6jk/AWtnL+gdWybc47eQ0fZuu8Q23oPs633EFt7U/c39Azw6LoeBrMuSNHWUM28jECf6wX8vNZ65rTUUVddGfSvJiIxogD3iZnR1lBNW0M158xrmfB95xx7DgyNhXpm0L+2s5+HX9s9NiyT1liToKOxhvbGGjoaa+iY4d1mPJ7VWENbQzUJzWcXOeEowANiZmPBu2J+64Tvj446kgcGx4J9e99h9hwYJDmQ+lq7o5/HBwYZGBzO0TbMbKimPSvgZzXW0tlcy1lzmpnbWoeZBfGrikhAFOARUVFhzG6qZXZTLV0LJ3/e4aER9hwYpMcL9mRGyKcfb04eJDkwOK5H3z6jmnPntXhfrZw9r5kmHWQVibW8AtzMvgR8BnDAy8CngE7gLmAm8Dxwg3NOi3SXWV11JfPa6pnXVn/c5znn6D88zJZ9B1mztY8Xt/axemsf/7a2Z+w5p3Y0cO68Vs6d38J581pYdlKjlhYQiZEppxGa2RzgSWC5c+6wmd0NPAhcA9zrnLvLzG4F1jjnvn+8tuI6jXA62X/4KC9t62P1W6lAX721j73exTFqEhWcOac5o6feoqEXkQgodRphAqgzs6NAPbATeCfwR9737wC+CRw3wCV8zXVVXLqkg0uXdACpnvq23sNjYb56ax//+OwWbn/yDWD80Ms581qY31ZPY20VjbUJ9dZFQjZlgDvntpvZ3wBvAYeB35IaMulzzqWPqG0D5uR6vZndCNwIMH/+fD9qFh+Z2diQzPvPORmAoyOjrNs1kBp2eauP1Vt7xw29pNVWVYyFeWNtFU21idT9mmPbGtPbxr4/fpuWJhAp3pQBbmatwHXAIqAPuAe4Kt8f4Jy7DbgNUkMoRVUpgaqqTA2lnDmnmRsuWgCkhl5e3raf3f1HGDhylIEjwwwMDjNw5Cj9R4ZTj48cZef+Y9/PPrEpl5pExfjwzwr4xtoEzXVVY+u6t9Z7a7w3VNFYk9DwjvDN+1+lpqqCm68+vaR2egaO8D8eWMst159NbVU8zsHIZwjlXcAbzrkkgJndC1wCtJhZwuuFzwW2l69MCVtzXRXvWNJe0GuGR0Y5MJgK934v1PsPHx0L+8l2Arv689sJJCqMlvoqWuqrac0K95kN1cxuquUkb2bPSc21sflPKYX5h6ffBCg5wG956HX+dfUOLlnczke65vlQWfnlE+BvAReZWT2pIZQrgW7gUeDDpGairATuK1eREk+Jygpa6qtpqa8uuo3hkVH6jwzTe2iIvkND9Hpru/cdSt32Hjqa2n5oiLf2HWLNtj56Dx1lKOusV0jthE5qqmVWUw0neaGenrp5UlMts5traG+ooaJCvfp83bd6O1WVFVyxbFbszxweGU0NEBRybOfpTXtYMLOBOSFdSyCfMfDnzOxnwAvAMPAiqSGRB4C7zOwvvW23l7NQOTElKivGznDNl3OOgcFhevqPsGv/ILv6j7Db+9q1P3W7YfcBegaOMJo1qJeoSJ1wlQ71Be31LJvdyNLZjSyeNUO9+AzOOb70L6sZdVBXVcm7ls/mv1y6iLPntoRdWlGGR1J/DJUF7MD/6IfPAfDE166YcmpvOeQ1C8U592fAn2Vt3gxc4HtFIiUyM5pqq2iqrWLxrMZJnzcy6thzYHAs1Hf3H2GXF/o9A0fYmDzA717vGTshqsJgwcwGlsyawbKTUqG+dHYji9obTsiDscOjjlEH7zu7k+a6Kn710k5+uWYH714+my+/eymndzaFXWJBhkdT/86JIj6B3Xzvy/z00xcEfkxGZ2LKCasy4+zXyQyPjPLm3kOs3z0w9rVu1wCPvN4z9pE7UWGc0tHAktmNY731pbNnsGBmQ0G9ubg56u3YzpzTzJ9cfio3XX0a//DUm9z2xGau/s4TXHtWJ1981xKWzJ58J5r25IY93PHMm/zHixZw2ZL2UA5Op/898/03c85hBq311Ty5cQ/3dG/jo28LduxcAS5yHInKChbPmsHiWTO45qzOse2DwyNsTh4cC/T1uw/w8rb9PPDSzrHn1CRSr12aMQQzuym1Ts3MhprY99qPjowfM26sreLzVy7hkxcv5EdPbubHT77Bg6/s5LpzTuYzl57C8s6mSY8vPLExycOv7ebh13Zzztxm/usVi7l8aUegQ1bDBY6Bj4w6nIOVFy/kqU17+IsHXuPyZR3H7RD4TQEuUoSaRCWndzZNGCY4NDTMxp4DXqingv3ZzXv5xYsTJ2m11FfRMaNmbBGyY7fV41acjOpqk+keeHXl+FBurq/iK+9ZxqcuWcQPHt/ET57ewr+u3kFbQzUXnzqTS05t5x2L25k/c/yYcXVlBd+67gz+/tGN/PFPn6cmUcGFp8zksiXtXLa0gyWzZuTVM79/zQ4aaxNcfMrMgnYAhfbA0zuw6kQF377+bK7+zuN89AfPcNsNXSw7aepPHX5QgIv4qL46wdlzWyYcyNt/+CibkwdIDgyy58CQdzs4drtmWx/JgcGc0ybNoK2+elzIZwZ95g6grb46sFk06QCfbOfS1lDNzVefzo2XnsKq9Ume2riXpzbuGfuUsryzibv++KKxRdXM4GMXzOf68+fy5IbURccf35DkLx9YCw+spbO5lkuXtHPpkg7esbid1hwHtvcfPsqf/vOLAGM7gD9Y2sEfLOtgUXvDcXcA6YOY+Y6Bp4+NVFUai9obuPMzF/In//gCH/q/T/HbL13G3NbyH9RUgIsEoLmuivNyLCOc7eDgMHsODI5bSjiZFfhv7k2tNpl9gRBI9R5nNlRnLCl8bGnh7MelTvsbHslvyGHmjBr+cMVc/nDFXJxzbEoe5KfPvMkdz2xhzda+1LIOGbOBqioruOK0WVxx2iwAtvUe4okNe3hiQ5Jfv7KLu7u3YQbnzWvhPWecNO5npXvRHz5/Lo21CVatT/KtX73Gt34F89rquGJZqt1cvfPsHviq9UnuX72Di0+dyeVLO+horMn6/b1PIN5Q2PkL2vjhJ7v44PeeovvNXgW4yImmoSZBQ02CBTMbjvu89FTJPQODGb36I+w5METPwJGxpYXX7uxnz4GhsXDKNKMmwSzvgiHpC4PUVlVSXVlBdSL1VePdVldWUON9L71tz4FBINUDzZeZsXjWDD5w7hzueGbLhGmcucxtrefjF8zn4xfMZ3hklDXb9vP4+iSPvL6bWx56PedrzprTzMq3LwRg675DPLY+yWOv93BP9zZ+8swWahIVvP3UmZzW2cTMhmpmzqhm3e4B4Ngnil+u2cHPX9jGz1/YBsAZJzfxtoVtzGpKnS+Q7swnKo7twFrqUp8mHMGcdK4AF4mhzKmSp3TMOO5zR0YdvYeG6OlPhXpP/5GxdeTT68q/uqOffQeHGBoeZXB4JK9gTWus9SdG8pl4kqisGLuc4ZfevZTtfYe55JbfscAbT8+1uuq8tnpuuGgBN1y0gCNHR/j9G/t4dF0Pq9YleWLDnrGDl2mZY+CdzbX8aGUXj61Lsmpdkru7t04Y5mqqCy9GFeAi01xlhdHuHSzN1/DIKEMjowweTd2mg31wOH0/dVtZYVy4qK3o2tKBW2x/dU5LHRcsaiPfYf/aqkouW9rBZUs74P3H1s3fe3CQu7u3ceuqTePaqjDjjJObOePkZj57xWIgdaB674Eh9h0c4uDQMG9bOPH3n2KVbt8owEVkgkRlBYnKCkpYBeG4yj3NO9/2zYzm+iqa66t428JWbl117HuThXB9dYL6tkTOMy+Dnr4evblJIiKSFwW4iESCUXz3Nd1b9mPkIqjhDz8owEUkNOmsnOrSjsfj16hF9vBHKTNJgtoJKMBFJHDlHioOawWaUj5FFEMBLiKR4McBQL97vsXW9JV71vBXD631t5gcFOAiEh6fAtePZtK9Z7/2AT9YtdmnlianABeRwGWvSVJKzzlnL9mX7nzhL9E0QhERyYsCXEQioaTO69g0wtIHQDJnxIRwXYmCKMBFJDTpwC0ldn2b+TFhGmH0KcBFJHDTdRph0BTgIhIJvlwH0+9phBHfFSjARSQ0fs3b9mPsOx3VcRg6SVOAi0jgJpy27vM0Qn9OCiq8KE0jFBGRvCjARSQS/Oi8+j38oWmEIiKTcD7N3/ZjLD19ENXPpWnLTQEuIoHzc3ZHzjHwkGaP+DKTpgAKcBGJhiiuRuhvc75TgItIaPzKW39zOw6DJykKcBEJnK/TCHP0k4sZych+STE1Bd1jV4CLiMSUAlxEIsGfaYT+Dn8EfVCyUApwEQlNKRcz9ruddFZrGqGISEByTyMMh06lF5ETkh/DFZpGmIOZtZjZz8zsdTNba2YXm1mbmT1sZhu829ZyFysi00sUhymiWNNk8u2Bfwf4tXPuNOAcYC1wE/CIc24J8Ij3WERkShOnEZZ4Kv0U7ecjezpiUasRBtxnnzLAzawZuAy4HcA5N+Sc6wOuA+7wnnYH8MHylCgiIrnk0wNfBCSB/2dmL5rZj8ysAZjtnNvpPWcXMDvXi83sRjPrNrPuZDLpT9UiMu1E8II8kR8EzyfAE8AK4PvOufOAg2QNl7jUZ42c751z7jbnXJdzrqujo6PUekVkGvFryp4/qxGObysOY+H5BPg2YJtz7jnv8c9IBfpuM+sE8G57ylOiiEw3/q5GmONU+tBWIwz2500Z4M65XcBWM1vmbboSeA24H1jpbVsJ3FeWCkXkhODLmZg+zyOM+AgKiTyf93ngTjOrBjYDnyIV/neb2aeBLcBHy1OiiExfcRioiK68Atw5txroyvGtK32tRkROCH6uRgg5dgMlrEboShgE12qEIiIFKGdoajErEZFJZPa8o3gqfdQpwEUkcBOGUKIwFp6eRkj6NgI1TUEBLiLxl9X1Dm3gI2rTCEVEglBs9pVzmDraI+AKcBEJUfQHKaJNAS4igZu48l9p7U1cjbDwvnO6prFZhEVNI4zYaoQiIlFW3mmEZWzcBwpwEQnN+GmE/rZ3IlCAi0jgJk4jDJ/fZ4cGQQEuIrGXHbaldOZLmf8dudUIRUSCUVz6ZR6w9Pvkm7CWpc2XAlxEQhOHsx2jTAEuIoHL7tf6Pd5c3EWNPWOLERZzUeNgKcBFJPbK1ZPXNEIRkUn4MY0w82VxmDniJwW4iASu3D3bsOaUB71+uAJcRCKgtK6zP1elNx8qCZYCXEQiwY/VCOMUvn5QgItIaKIcuFGuLU0BLiIh8Hc1womtF7EaoQ/D15pGKCJSID93AH5fp7OcFOAiEhqXkZbFZ2XGqfQn2DxCBbiIBG76TiMs/ecWQgEuIqGLQsc5nb1xWp9FAS4ikVDKyn8u69Yv0R4BV4CLSMyVb9gi+j1xBbiIBG7CaoQRCEt/phHqVHoRkYL4OfvE7+t0lpMCXERCo9UIS6MAF5HAlfsEGT/aL2pnoGmEInKiiUbPeWL6aghFRCQP/mRl6XuCSOxL8qQAF5HQ+DH7pFy95DgEed4BbmaVZvaimf3Ke7zIzJ4zs41m9i9mVl2+MkVkOpk4jbC87ef1Gj+mEUZ4DPwLwNqMx98G/s45txjoBT7tZ2EiImEYt8BWxM/FzCvAzWwucC3wI++xAe8EfuY95Q7gg2WoT0SmMb+Wbk23E42DocHJtwf+v4GvAaPe45lAn3Nu2Hu8DZiT64VmdqOZdZtZdzKZLKVWEZkm/BxqyNVL9mc1wsL3BpG7oIOZvQ/occ49X8wPcM7d5pzrcs51dXR0FNOEiExzUeg55wrfqE8jTOTxnEuAD5jZNUAt0AR8B2gxs4TXC58LbC9fmSIik0vPZvFjPxCBfUnepuyBO+duds7Ndc4tBD4G/M459wngUeDD3tNWAveVrUoRmZb86HlrGmFx/jvwZTPbSGpM/HZ/ShKR6S573Nrv1QjDmj0S9DU08xlCGeOcewx4zLu/GbjA/5JERII3Fr6ZM2PCKSVvOhNTREKT2e8upfOqaYQiIgHxdRphjrZCu6hx6T+2IApwEQlfBHrOOcM34vMIFeAiEgm+9Jp9WY0wAnuTPCnARSQ0fl0KrRyRG4cYV4CLSOhKCcucp9KX0F4porwaoYjItJXzYGjwZRREAS4ioRk3jbCEuEwPxfgxIhOnqYgKcBEJnK9DDWWbRljMaoTB9tkV4CISOr8OZpaiXMvSlpMCXEQiIayTb8rRRlAU4CISHp/CMkaZ6ysFuIgELnvVvtKmEea/tdw0jVBEJASaRigiUoDM09b9CEt/TqWPDwW4iATO955tVupG5YBouSnARSR0pYRlOa+CE/QVdgqlABeRSPAjLOPQa/aTAlxEQuNX4PqZ22On5cdgNFwBLiKBy+5s+z2N0K+Bj0Lb0TRCEZEQRHy4OycFuIiEZvxqhNGQrikO4+kKcBEJnN+r9mUvhhXW7BGtRigiJ5xSViMsZ1ZHfVhFAS4i0RDyyTdB9579oAAXkdBEcZw5XVMUa8umABeRwPk9NJGdtf5NIyysJU0jFJETjv/LyRbRTvxGUBTgIhKeKK5GWI62ykUBLiKB87uzmz1eXVpvuoQZMaX82CIowEUkfBFdjTDqE1MU4CISmsyec9irEWoMXEQkHxEOS00jFBEJUPYBR79604WvRhixU+nNbJ6ZPWpmr5nZq2b2BW97m5k9bGYbvNvW8pcrItNRKTM+wprzHQX59MCHga8455YDFwGfNbPlwE3AI865JcAj3mMRkbz5vRqhrxd28LGtcpkywJ1zO51zL3j3B4C1wBzgOuAO72l3AB8sU40iMs34vxqhf+1H4aSifBU0Bm5mC4HzgOeA2c65nd63dgGz/S1NRE4UJR0w1GqEUzOzGcDPgS865/ozv+dSa0Hm/CcwsxvNrNvMupPJZEnFisg0k5HcfoRlVJelLZe8AtzMqkiF953OuXu9zbvNrNP7fifQk+u1zrnbnHNdzrmujo4OP2oWkZiLcliO7QNiMAiezywUA24H1jrn/jbjW/cDK737K4H7/C9PRGRqEzrevk0jjPZqhIk8nnMJcAPwspmt9rZ9HbgFuNvMPg1sAT5algpFZNqLwoUYIvyhYFJTBrhz7kkm/92u9LccETmRjJ9G6MOp9CW3kNmWI+qxrjMxRSRw5Y7FUtov6aSiqJ2JKSJSblFdezvKB1tBAS4iIfJjwajMkNVqhCIiZZZrqCFqAarVCEVEApB9Ak8pY9Hj1ygvuplAKMBFJHSlTSP0S8TTOgcFuIiEppRT3ydpMYItlY8CXEQCF+1phJntFN5SkMMuCnARCV2pvd0o9ZaDPPipABeR0PiRdZpGKCISoFxhGfRZjFPxf3zefwpwEZl2StkXOJ/XKC8nBbiIhK7Uzm4MOstloQAXkdCMO2mmyDYyZ4r4eWW2OOwTFOAiEji/L2ocdPtRoQAXkQgorb8b1dUMy00BLiKhidY0wvj12hXgIhK8nNMIgy/jeOJwYFQBLiLTTmnTCDPbidheJYsCXERCp2mExVGAi0ho/DhpZvwYeAnXs8x6HId9ggJcRAJX7pEJv5qP9gCKAlxEIiBKvd04TUlUgItI7Lms22JE/HhlTgpwEQlcrqws/uzJXHMSi2wqUwyOjCrARUQmEfVeuQJcREJX6trbfnaWY9DxHqMAF5HQjD9pprg2fDuVPmvcJQ45rgAXkcCV+wxHv1YjjPgIigJcRMIXpd6uhlBERPLg35xrV3J72R8K4hDkCnARCVzuaYQ+thX1sQ+fKMBFRCah1QhFRKYQpeGKCJUyJQW4iITGOdi67xCvbN9f0riH8+Nc+uw2fWpsdNSVPM99MiUFuJldZWbrzGyjmd3kV1EiMr2ls/qvHnqdS//6UfYeHCq5rVe27+foaCooo7Ia4as79nPK1x9k0c0P+lJPtkSxLzSzSuB7wLuBbcC/m9n9zrnX/CpORGQqow72Hhziff/nSV/a++o9a/jqPWsAmH1abUltXfvdYzXtOzhEW0N1Se1lK6UHfgGw0Tm32Tk3BNwFXOdPWSIynVXkGC6pTRQXR6vWJUstB4DWHOGaPDDoS9sAvYeK/5QxmVICfA6wNePxNm/bOGZ2o5l1m1l3MunPGy0i8VZbVcmff+CMcdv+9MolRbX1d//hXKoTFXzovFT8nDmnieUnNxXczoyaBJ+4cP64bZ9/Z+E1/bf3LuPaszo54+QmzpyTquOmq0/j1I4ZBbc1FSt2cN3MPgxc5Zz7jPf4BuBC59znJntNV1eX6+7uLurniYicqMzseedcV/b2Unrg24F5GY/nettERCQApQT4vwNLzGyRmVUDHwPu96csERGZStGzUJxzw2b2OeA3QCXwY+fcq75VJiIix1V0gAM45x4EyjPBUUREjktnYoqIxJQCXEQkphTgIiIxpQAXEYmpok/kKeqHmSWBLUW+vB3Y42M5QVHdwYpr3RDf2lV3+S1wznVkbww0wEthZt25zkSKOtUdrLjWDfGtXXWHR0MoIiIxpQAXEYmpOAX4bWEXUCTVHay41g3xrV11hyQ2Y+AiIjJenHrgIiKSQQEuIhJTsQjwqF482czmmdmjZvaamb1qZl/wtn/TzLab2Wrv65qM19zs/R7rzOy94VUPZvammb3s1djtbWszs4fNbIN32+ptNzP7rlf7S2a2IqSal2W8r6vNrN/MvhjF99zMfmxmPWb2Ssa2gt9fM1vpPX+Dma0Mqe7/aWave7X9wsxavO0Lzexwxvt+a8Zrzvf+vjZ6v5tf1xoupO6C/y6imjc5Oeci/UVqqdpNwClANbAGWB52XV5tncAK734jsB5YDnwT+GqO5y/36q8BFnm/V2WI9b8JtGdt+2vgJu/+TcC3vfvXAA+RulD3RcBzEXj/K4FdwIIovufAZcAK4JVi31+gDdjs3bZ691tDqPs9QMK7/+2MuhdmPi+rnd97v4t5v9vVIdRd0N9FlPMm11cceuCRvXiyc26nc+4F7/4AsJYc1wXNcB1wl3Nu0Dn3BrCR1O8XJdcBd3j37wA+mLH9Jy7lWaDFzDpDqC/TlcAm59zxzu4N7T13zj0O7MtRTyHv73uBh51z+5xzvcDDwFVB1+2c+61zbth7+CypK3BNyqu9yTn3rEsl5k849ruWxSTv92Qm+7uIbN7kEocAz+viyWEzs4XAecBz3qbPeR83f5z+mEz0fhcH/NbMnjezG71ts51zO737u4DZ3v2o1Q6pq0D9c8bjOLznhb6/Uasf4D+T6lGnLTKzF81slZld6m2bQ6rWtDDrLuTvIorv96TiEOCRZ2YzgJ8DX3TO9QPfB04FzgV2Av8rvOqO6x3OuRXA1cBnzeyyzG96PadIzjO11GX8PgDc422Ky3s+Jsrv72TM7BvAMHCnt2knMN85dx7wZeCfzKzwS8KXT+z+LgoRhwCP9MWTzayKVHjf6Zy7F8A5t9s5N+KcGwV+yLGP7JH6XZxz273bHuAXpOrcnR4a8W57vKdHqnZSO50XnHO7IT7vOYW/v5Gp38z+E/A+4BPezgdvCGKvd/95UuPHS70aM4dZQqm7iL+LyLzf+YhDgEf24sneUfXbgbXOub/N2J45NvwhIH1U/H7gY2ZWY2aLgCWkDvQEzswazKwxfZ/UQapXvBrTMx1WAvd59+8HPunNlrgI2J8xFBCGj5MxfBKH9zyjnkLe398A7zGzVu/j/3u8bYEys6uArwEfcM4dytjeYWaV3v1TSL2/m73a+83sIu//ySc59rsGWXehfxeRzZucwj6Kms8XqSP060nt3b8Rdj0Zdb2D1Efgl4DV3tc1wE+Bl73t9wOdGa/5hvd7rKPMR+WnqP0UUkfY1wCvpt9XYCbwCLAB+DegzdtuwPe82l8GukKsvQHYCzRnbIvce05qB7MTOEpqLPXTxby/pMacN3pfnwqp7o2kxobTf+e3es+93vv7WQ28ALw/o50uUoG5Cfh7vDO/A6674L+LqOZNri+dSi8iElNxGEIREZEcFOAiIjGlABcRiSkFuIhITCnARURiSgEuIhJTCnARkZj6/40KnIpmFU0sAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAAsTAAALEwEAmpwYAAAvEklEQVR4nO3deXhU5dn48e89k30hgSQEyAKBsCNrQBBBFATUCu5iteJWWqutS21LX/uzvtS2alu3qlWqKPpWca+ouICoCCIQdtlMCFsAISQsYcn+/P6YMzAMAySZSc4Mc3+uK1dmzjznnHuGcO55lvM8YoxBKaVU+HLYHYBSSil7aSJQSqkwp4lAKaXCnCYCpZQKc5oIlFIqzEXYHUBjpKammg4dOtgdhlJKhZSlS5fuMcakeW8PyUTQoUMH8vPz7Q5DKaVCiohs8bVdm4aUUirMaSJQSqkwp4lAKaXCnCYCpZQKc5oIlFIqzGkiUEqpMKeJQCmlwlxYJYLp32xm5soddoehlFJBJawSweuLtzJzxXa7w1BKqaASVokgLTGakoNVdoehlFJBJbwSQUI0e8or7Q5DKaWCSlglgtTEaEoOVqLLcyql1DFhlQjSEqKpqqnjQEWN3aEopVTQCKtEkJoYBcCeg9o8pJRSbgFJBCIyVkQ2iEihiEz28fpwEVkmIjUicpXXaxNFpMD6mRiIeE4mLSEGQPsJlFLKg9+JQEScwDPARUAP4DoR6eFVbCtwE/Ca176tgD8CZwODgD+KSEt/YzoZd42gRGsESil1VCBqBIOAQmNMkTGmCpgBjPcsYIzZbIxZBdR57TsGmG2MKTPG7AVmA2MDEJNPaQnRgNYIlFLKUyASQQawzeN5sbWtqfdtsJZxUTgdojUCpZTyEDKdxSIySUTyRSS/pKSkUcdwOISU+Cj2lOtNZUop5RaIRLAdyPJ4nmltC+i+xpipxpg8Y0xeWtoJay/XW2pCtNYIlFLKQyASwRKgs4jkiEgUMAGYWc99PwVGi0hLq5N4tLWtyaQlRuvwUaWU8uB3IjDG1AB34rqArwPeNMasEZEpIjIOQEQGikgxcDXwvIissfYtA/6EK5ksAaZY25pMqk4zoZRSx4kIxEGMMbOAWV7bHvB4vARXs4+vfacB0wIRR32kJkax52AVxhhEpLlOq5RSQStkOosDJS0hmqraOg4c0WkmlFIKwjERJLruJSg5WGFzJEopFRzCLxFYN5WV6BBSpZQCwjARtE2OBWBL6SGbI1FKqeAQdomgQ0ocaYnRfLOx1O5QlFIqKIRdIhARzs1NZUHhHurqdIEapZQKu0QAMDQ3ldJDVWzYVW53KEopZbswTQQpAMwv2GNzJEopZb+wTARtk2LplBbP/EJNBEopFZaJAODc3FQWbyqjsqbW7lCUUspWYZsIhuamcqS6luVb99kdilJK2SpsE8HgTik4BBZo85BSKsyFbSJoERNJn6xk7SdQSoW9sE0EAMNyU1m5bR8HKqrtDkUppWwT1olgaG4qdQa+1buMlVJhLKwTQb/slsRGOpm7frfdoSillG3COhFERTi4rF873l5aTOHug3aHo5RStgjrRADw69FdiY10MuXDtRijcw8ppcJP2CeC1IRo7hrVmXnfl/D5Om0iUkqFn7BPBAATz+lAbusE/vTRWr3TWCkVdjQRAJFOBw/8qAdbSg8zbf5mu8NRSqlmpYnAMrxLGqO6p/PPuQXsOqDrGSulwkdAEoGIjBWRDSJSKCKTfbweLSJvWK8vEpEO1vZIEZkuIqtFZJ2I/D4Q8TTW//tRd2pqDY98vN7OMJRSqln5nQhExAk8A1wE9ACuE5EeXsVuBfYaY3KBx4FHrO1XA9HGmLOAAcDP3EnCDu1T4rltWA7vLt/O0i177QpDKaWaVSBqBIOAQmNMkTGmCpgBjPcqMx6Ybj1+GxgpIgIYIF5EIoBYoAo4EICYGu2O83NJbxHN/36wRpeyVEqFhUAkggxgm8fzYmubzzLGmBpgP5CCKykcAnYCW4G/G2PKfJ1ERCaJSL6I5JeUlAQgbN/ioyP4/UXdWVW8n7eXFjfZeZRSKljY3Vk8CKgF2gE5wK9FpKOvgsaYqcaYPGNMXlpaWpMGNb5vOwa0b8kjn6xnQeEeKqp1SKlS6swVEYBjbAeyPJ5nWtt8lSm2moGSgFLgx8AnxphqYLeILADygKIAxNVoIsL/juvJtc8v5PoXFhEd4WBQTiuG5qZybm4qPdq2wOEQO0NUSqmACUQiWAJ0FpEcXBf8Cbgu8J5mAhOBhcBVwFxjjBGRrcAFwKsiEg8MBp4IQEx+65WRxKL7R7F4UynzC0pZULiHh63RRC3jIrlxSAfuHtUZV1eHUkqFLr8TgTGmRkTuBD4FnMA0Y8waEZkC5BtjZgIv4rrYFwJluJIFuEYbvSQiawABXjLGrPI3pkBJiI7ggm7pXNAtHYDdByr4ZmMpH67awZOfF5AYE8Ftw3y2ZCmlVMiQUJxoLS8vz+Tn59t2/ro6wx2vLeOTNT/w/A0DGN2zjW2xKKVUfYnIUmNMnvd2uzuLQ5LDITx2TV96ZyZz14wVrC7eb3dISinVaJoIGik2ysm/bxxAq/gobp2+hB37jtgdklJKNYomAj+0Toxh2k0DOVJVyy0vL+FgZY3dISmlVINpIvBT1zaJPHN9fwp2H+TO15ZRU1tnd0hKKdUgmggCYHiXNKaM78mXG0p0pTOlVMgJxH0ECrj+7PZsKT3M1HlFZLaMZdLwTnaHpJRS9aKJIIAmj+1G8d7D/GXWeg5W1nKP3nCmlAoBmggCyOEQnprQj/io1Tz1eQF7Dlbyp/G9cOp0FEqpIKaJIMAinA4evao3qYnR/OvLjZQdrOKJCX2JiXTaHZpSSvmkncVNQET43dhu/L8f9eCTNT9w00uLOVBRbXdYSinlkyaCJnTruTk8OaEv+Zv3cu3z37Jb10JWSgUhTQRNbHzfDKbdNJAtpYe48rlv2LznkN0hKaXUcTQRNIPhXdJ47aeDOVRZy5X/+kbnJlJKBRVNBM2kb1Yyb/18CDGRTiZMXcj8gj12h6SUUoAmgmbVKS2Bd39xDlmt4rj55cV8uGqH3SEppZQmguaW3iKGN342hH5ZLfnl68t5ZeFmu0NSSoU5TQQ2SIqN5JVbBzGqezoPvL+Gf3y2QecnUkrZRhOBTWIinfzr+v5MGJjFP+cW8vP/W0rh7oN2h6WUCkN6Z7GNIpwO/nrFWWSnxPH03EJmr/2Ky/pmcNeozrRPibc7PKVUmNA1i4NE6cFKnvtqI68s3EJNneGq/pn8cmQumS3j7A5NKXWGONmaxZoIgszuAxU8++VGXlu0FYNhwsBs7jg/lzZJMXaHppQKcU26eL2IjBWRDSJSKCKTfbweLSJvWK8vEpEOHq/1FpGFIrJGRFaLSFhf8Vq3iOHBcT358jcjuDovi9cXb2X4375gygdrKSmvtDs8pdQZyO8agYg4ge+BC4FiYAlwnTFmrUeZXwC9jTE/F5EJwOXGmGtFJAJYBvzEGLNSRFKAfcaY2lOd80yuEXjbVnaYpz4v4N3l24lyOrjxnPb8bHgnWsVH2R2aUirENGWNYBBQaIwpMsZUATOA8V5lxgPTrcdvAyPFtWLLaGCVMWYlgDGm9HRJINxktYrjb1f3Yc695zGmZzpT5xUx7JG5/OOzDew/ojOaKqX8F4hEkAFs83hebG3zWcYYUwPsB1KALoARkU9FZJmI/DYA8ZyRclLjeWJCPz67ezgjurbmn3MLOfeRuTz1eQHlOsW1UsoPdt9HEAGcC1xv/b5cREb6Kigik0QkX0TyS0pKmjPGoNI5PZFnru/PR786l7NzUnhs9vcMe/QL/vXlRg5X1dgdnlIqBAUiEWwHsjyeZ1rbfJax+gWSgFJctYd5xpg9xpjDwCygv6+TGGOmGmPyjDF5aWlpAQg7tPVsl8QLE/N4/46h9MlM5pFP1jP80S944esiKqq1dU0pVX+BSARLgM4ikiMiUcAEYKZXmZnAROvxVcBc4+ql/hQ4S0TirARxHrAWVW99spKZfssg3rl9CF3SE3noo3Wc97cveOaLQraVHbY7PKVUCAjIfQQicjHwBOAEphlj/iwiU4B8Y8xMa0joq0A/oAyYYIwpsva9Afg9YIBZxpjT9hOE06ihhlq4sZQn5nzPok1lAPTLTmZcn3Zc0rstrRPDemSuUmFPbygLM9vKDvPBqh3MXLGD9T+U4xAY0imFcX3aMbZnW5LiIu0OUSnVzDQRhLGCXeXMXLmDmSt3sKX0MJFO4bwuaVzapx0X9kgnLkqnnFIqHGgiUBhjWL19PzNX7ODDVTv54UAFsZFORvVIZ1yfdgzvkkp0hNPuMJVSTUQTgTpOXZ1hyeYyZq7cwazVO9l7uJoWMRGM7dWGcX0yGNIpBadD7A5TKRVAmgjUSVXX1jG/cA8frNjBZ2t3cbCyhtSEaH7Uuy2X9mlH/+xkXDeCK6VCmSYCVS8V1bV8sX43M1fu4PP1u6mqqSOzZSxTxvfkgm7pdoenlPKDJgLVYOUV1cxeu4t/f72J9T8c4P6Lu3PruTlaO1AqRDXpNNTqzJQYE8kV/TN59/ZzuKhXGx76aB2T31lNVU2d3aEppQJIE4E6rdgoJ09f159fXZDLG/nbuOHFRZQdqrI7LKVUgGgiUPXicAj3ju7KkxP6smLbPi57ZgEFu8rtDkspFQCaCFSDjO+bwRuTBnO4qpYrnv2GLzfstjskpZSfNBGoBuuX3ZL37xxKZqs4bnl5CS8v2EQoDjpQSrloIlCNkpEcy9s/H8LI7uk8+MFa/vDf76iu1U5kpUKRJgLVaPHRETx/wwBuH9GJ/yzaysRpi9l3WDuRlQo1mgiUXxwO4Xdju/GPq/uQv3kvlz/7DUUlB+0OSynVAJoIVEBcOSCT1356NgeOVHPZMwtYULjH7pCUUvWkiUAFTF6HVvz3jqG0TYrlxmmL+b9vt9gdklKqHjQRqIDKahXH27cP4bwuafzhv9/x4Mw11GgnslJBTROBCrjEmEj+fWMePx2Ww8vfbObml5ew/0i13WEppU5CE4FqEk6HcP8lPXjkyrNYuLGUK55dwOY9h+wOSynlgyYC1aSuHZjN/912NqWHqrjs2QXML9BOZKWCjSYC1eQGd0zh/TuG0joxmhunLeLRT9aTv7mMI1W1doemlELXI1DN6FBlDb97ZxUfrtoJuJqPuqYn0icrmX5ZyfTJSia3dYIukalUE2nShWlEZCzwJOAEXjDGPOz1ejTwCjAAKAWuNcZs9ng9G1gLPGiM+fvpzqeJILTtLq9g1bb9rCzex4ptrp/yihoA4qKcnJWRRN+sZPpayaFtUowuhqNUAJwsEUQE4MBO4BngQqAYWCIiM40xaz2K3QrsNcbkisgE4BHgWo/XHwM+9jcWFRpaJ8YwqkcMo3q4lr6sqzNsLj3kSgxb97GieD8vLdhMlTXsNC0xmj6ZyfTNSqJvVkvOykwiKTbSzreg1BnF70QADAIKjTFFACIyAxiP6xu+23jgQevx28DTIiLGGCMilwGbAB1SEqYcDqFjWgId0xK4vF8mAJU1tazbWc7KbftYuW0fK4r3MWfdrqP7dEyLp2+mq8bQNyuZbm0TiY5w2vUWlAppgUgEGcA2j+fFwNknK2OMqRGR/UCKiFQAv8NVm7jvVCcRkUnAJIDs7OwAhK2CWXSE82jzkNv+w9Ws2m4lhm37mVewh3eXbwcgyumge7sW5LVvyeCOKQzKaaW1BtUgMxZvJbNlHOd2TrU7lGYXiETgjweBx40xB0/XBmyMmQpMBVcfQdOHpoJNUlwkwzqnMaxzGgDGGHbsrzhaa1i+dR+vfruFF+dvwiHQs10Sgzu2YkinFAZ2aEVijCYGdXKPz/meEV1a+50I3lyyjd++s4p1U8YSGxUatdRAJILtQJbH80xrm68yxSISASTh6jQ+G7hKRB4FkoE6EakwxjwdgLjUGU5EyEiOJSM5lovPagtARXUty7fuY2FRKd8WlTL9my38+2tXYjgrI4nBnVIY0tGVGOKj7f4epIKJMeAIwID6Jz8vAGDPwUqyWsX5f8BmEIj/CUuAziKSg+uCPwH4sVeZmcBEYCFwFTDXuIYrDXMXEJEHgYOaBJQ/YiKdDOmUwpBOKQAcqapl+da9LCwqZeHGUqbN38TzXxXhdAi9M5MY0tFVdkD7lsRFaWIIZ3UGIDxHp/n9l2+1+d8JfIpr+Og0Y8waEZkC5BtjZgIvAq+KSCFQhitZKNXkYqOcnJObyjm5rur+4aoalm7Zy8KNrhrD1HlFPPvlRiKdQp/MZFcS6ZjCwJxWRDr1fsvwYgjXUcoB+QpkjJkFzPLa9oDH4wrg6tMc48FAxKLUqcRFRRzXz3CosoYlm8v4tqiMhUWlPPNFIf+cW0hqQhTj+mRw5YAMerZLsjlq1RyMgUDcy+i+NyuUkorWhVVYi4+OYETX1ozo2hqA8opqvtlYynvLtvPqt5uZtmAT3dokctWATMb3zSAtMdrmiFVTqTMGCWDTUCjdBKmJQCkPiTGRjOnZhjE927D3UBUfrNrBO0uLeeijdfz14/Wc1yWNK/tnMrJ7a2IiQ2NEiKofQ2C+xYfikEZNBEqdRMv4KG4c0oEbh3SgcHc5by/dznvLi5m7fjctYiK4tE87rhyQSb+s5JD69qd8czUNBbBGELAjNT3tDVOqHnJbJzL5om58M3kkr9wyiPO7teadZcVc8ew3jHzsK575opAd+47YHWZIqq6t41evLyd/c5mtcdQFaALOxhxmdfF+Kmvsm41XawRKNYDTIQzvksbwLmmUV1Qza/VO3lm6nb99uoG/f7aBczqlcGX/TMb2aqPDUetp14EKZq7cwZcbdvPuL84ht3WiPYGYQDUNNayz+GBlDZc+PZ8RXdN4+eZB/gfQCFojUKqREmMiuXZgNm/+fAjzfnM+v7qgM1vLDnPvmysZ+NAc7ntrJQs3llJXF4qtxs3H/Q36QEUNE6ctYfeBCnvigMB2FtfzWNU1rskVv9xQErBzN5QmAqUCIDsljnsu7MJX953PG5MGc0nvtnzy3Q9c9+9vufr5hRTsKrc7xKDlTgQ3ndOBvYeruPO15dixTkqdMQEZPtqY87qVV9iztrcmAqUCyOEQzu6YwqNX9WHJ/aP46xVnsbHkIBc/9TVPzPne1nbgYOVuSumdmcT9l3Rn8eYyvtiwu/njCFTTkHVdr++xPCuMq7fv9z+ARtBEoFQTiY1yct2gbObcex4Xn9WWJ+YU8KOn5rN0i72dosGmzuPCeU1eFu1T4vj7p983e5OawQRk9Jc76voeyXgMOF2/056aoyYCpZpYakI0T07ox0s3D+RwVS1XPbeQB97/zrZmgGDjbgZyiBDpdHDPqC6s3XmAj7/7oVnjqAtQjcCtvmnMsxVs/Q8HAhdAA2giUKqZnN+1NZ/dM5ybzunAq99u4cLH5jFn7a7T73iG8/7if2mfdnRJT+AfszdQY61S1yxMYDqLG9q9cXwi0BqBUme8+OgI/nhpT969/RySYiO57ZV87nhtGSXllXaHZhvPGgG4huj+enRXikoO8d5y7xntmzCOgE0653o/9U0I7s7ixOgIvt9VTq0No8w0EShlg37ZLfngl+dy3+guzF6zi1GPfcWb+dtsGS1jN/c79ryrd3SPdPpkJvHEnIJm62CvC9ikcw09r2uH7m1bUFFdx5bS5l+1VxOBUjaJinBw5wWdmXXXMLqmJ/Lbt1dx/QuLbLkQ2KnOx2ydIsJ9Y7qyfd8R3liy7SR7nui95cX8Zda6RsVhAjzpnKlnL4E7cfRo1wKwp3lIE4FSNsttncCMSYP58+W9WF28n9GPz+O5rzY2b/u4jdwXQu9v4+fmpnJ2Tiv+ObeQI1X1qxXMWbebqfOKWFRU2vA4CHBncT1rBu5yndMTcAis39n8HcaaCJQKAg6HcP3Z7Zl973mc1yWNhz9ez/hnFvCdTePKm9OxG6qOvwqLCL8Z05WS8kqmL9xcv4NZh3r00w0NbmZz3UcQuOGj9eV+/7GRTnJS47VGoFS4a5MUw9Qb83juhv7sLq9k3NPz+cusdfX+RhyKTlYjAMjr0Irzu6bxry83cqAew23dzTFLt+xl7vr635R2dDGZeu9x+mPVe/io9VsEurVtoYlAKeUytldb5tx7HtcOzGLqvCLGPDGP+QV77A6rSRy7E9f3ZfjXo7uy/0g1L3y9qV7HykmNp0NKHI9+sqHeI3Aaejdw/Y5Zv3PXeYya6t4mka1lhzlYWRO4QOpBE4FSQSopNpK/XtGbGZMG43QIN7y4iDv+s4w5a3dRUX3m1BCOXQh9v94rI4lLzmrLi18Xsf00U30bAxHW8NMNu8p5Z2lxA2No/qYhz0TYrY2rw3hDM9cKNBEoFeQGd0zh47uGcef5ucwrKOG2V/IZ8KfZ3PnaMj5atZNDzfztMdA8m0ZO5teju+AQ4ZrnFp5yVJX7XoBLzmpLXvuW3P/f1XxSjzuUGzotRH3Uv7P4WLOUe+TQ8q17AxjJ6WkiUCoExEQ6uW9MV5b+4UKm3zKIcX3bsXBjKXe8tox+f5rNbdPzeWdpMfsPh960FceGj578MtwxLYHXfjqYw1U1XP3cQr4/yWyuxro72OEQXrxpIL0ykrjjtWV8sHLHKWMIZNNQw+8jcP12iNAuOZZeGS1OG2+gaSJQKoRERTg4r0saf72iN4vvH8Ubkwbz40HZrNmxn1+/tZIBD83mJy8u4j+LtoTM3crG40J4KmdlJvHmz4YAcM3zC1lVvM9nOfdhkmIjefXWsxmQ3ZK7Zizn7VM0E3kno9o6w7T5mxp1M1uDRyt5LWQzvk8GK4v3s2lP891PEpBEICJjRWSDiBSKyGQfr0eLyBvW64tEpIO1/UIRWSoiq63fFwQiHqXCgdOa8vrBcT35ZvIFvH/HUG4b1pFtZYe5/73vGPSXOVzz3EKmzd902rZ1OzVkxE7n9ETe/vk5JERH8ON/L2LxpuNncvW+BCdER/DyLQM5p1Mq9721ktcWbT3l8d0X48Wbypjy4VomvbK00f0x9Z5iwrpdxN1HcmmfdojA+yuab3oNvxOBiDiBZ4CLgB7AdSLSw6vYrcBeY0wu8DjwiLV9D3CpMeYsYCLwqr/xKBWORIQ+WclMvqgbX9w3gk/uHsZdIztzoKKaKR+uZejDcxn39Hye/bKQopKDdod7HF9TTJxKdkocb/18COktorlx2iK+9Fi7wNe9AHFREbwwMY/zu6bxP++t5qUFJ44+Oto0ZKWjIZ1SePTK3q4+men5DRq+2+DOYo6vjbRJimFwTgozV+xotilHAlEjGAQUGmOKjDFVwAxgvFeZ8cB06/HbwEgREWPMcmOMuzFsDRArItEBiEmpsOUefXL3qC58cvdwvrhvBL8b2w0R4dFPNnDBP75izOPzeGz296zbecD2+Y3c6w40pH2+bVIsb/xsCB1S4vnl68s91i4wPmsWMZFOnv9JHmN6pvO/H6zlua82Hh+Dj2kurhmYxd+v6sM3G/dw88uL698pb9y/Gjh01WPbyO6tKdpziLJDVfU7p58CkQgyAM/JQIqtbT7LGGNqgP1AileZK4FlxhifDZsiMklE8kUkv6TEvrU9lQo1Oanx3D6iE+/fMZRvJl/AHy/tQXJcJE/PLeCiJ79mxN+/5K+z1vHd9v02LRHp+t3QjtrUhGiuPzub8ooaSg66LhunCj8qwsHTP+7PpX3a8fDH63lizvcn3PzlPYT1ygGZPH5tX5Zs3svEaYsbNL6/obOPetaIMpJjAfihmdZvjmiWs5yGiPTE1Vw0+mRljDFTgakAeXl54TdFo1IB0C45lpuH5nDz0Bz2HKxk9tpdfPzdD7w4fxPPzyuiS3oCl/fL5LJ+7WibFNssMR1tGmnE4M0WsZEAlFfUkN7i9PMFRTodPHFtX6KcDp6YU0BspJOfndfJo5/ixJ3H980gwuHgVzOWM+mVfF66eSDREc5TvJ+G8TViqUNqPMO7pAXkvob6CEQi2A5keTzPtLb5KlMsIhFAElAKICKZwHvAjcaYjSilmkVqQjTXDcrmukHZ7DtcxYerdvLe8u088sl6Hv10Ped0SuHyfpmM7dWGhOim+854qikmTudYf4B7DYDTryngdAh/u6o3lTW1/PXj9aQkRHNhj3TreL73uaR3Wyprarn3zZXcPWMFT/+4P87TBOxOCPsOV1FVW0frxBif5XzVCLq3bcErtww69RsJoED86y4BOotIDq4L/gTgx15lZuLqDF4IXAXMNcYYEUkGPgImG2MWBCAWpVQjJMdFccPg9twwuD2b97gWhHlv+Xbue2slf/jvasb0bMPl/TI4NzeVCGdgR50fTQSNyATuXTybYepTs3A4hH9c04d9h6v53TurcDp6u/Y9RRa5on8mZYeqeOijdfzhv9/xl8t7+Sx/tLnJGIwx3PzyEo5U1fLGpCEkxUWeWP5Y4Lbx+1/UavO/E/gUWAe8aYxZIyJTRGScVexFIEVECoF7AfcQ0zuBXOABEVlh/bT2NyalVON1SI3nngu78NVvRvDO7UO4sn8mX24o4aaXljDk4bk89OFa1uwIXH9CXQOGj3pzX/TrjnbQ1r+vITrCyXM/GUDPdi34zVur6hXDbcM68osRnXh98VYem/29zzKen4qIcO+FXSgqOcTNLy/mcNWJfQzeK7TZISD1PWPMLGCW17YHPB5XAFf72O8h4KFAxKCUCiwRYUD7Vgxo34oHLu3BF+t38+6y7UxfuJkX5m+ia3oil/fP4LK+GbRJ8t3sUR/Hppho+IXQvYs52jTUsISSEB3BSzcN5KrnFrJpz6F6JZHfjOlK2aEq/jm3kFbxUdw8NMdnOff7GtY5jaeu68sv/rOMn726lBcm5h3Xx1DnR9NYoOidxUqp04qOcDK2V1um3pjH4v8ZxZ8u60VctJOHP17PkIc/54YXFvHO0mK2lR1u8N24voZu1pd305BpxIFSEqJ55ZZB9MtOpkfbFqctLyI8dFmvo0NR31vu+45lzwrT2F5tefjK3nxdsId73lhx3KJD3vcw2CEoRg0ppUJHy/gofjK4PT8Z3J5NR/sTivn1WyuPlYmLJL1FDK1bxNA6MZr0FtGu54kxRx+nJUYT6XT42TTibhry6CxuxFGyWsXx3i+G1rt8hNPBkxP6cfNLS7jnjZU888VGRnZvzYXd06k5ydTX1+RlceBINQ99tI5FRZ9zfrfWjOremqgI1/dxO2sEmgiUUo2WkxrPvRd24Z5RnVm+bR+Fuw6y60AFu8sr2XWggl3llRTsKmd3eaXPtQFS4qOIiXQ1kzQqDfjoLG4uMZFOXrwpjzeWbGPOul28+PUmnv+qyKPEiUHdNqwj7VPi+XDVDj5b88Px8x9pIlBKhTIRoX92S/pnt/T5el2dofRQlZUkKth9oJJdByrZVV7B7gMV5LZOICctvsHn9a5FuKaYaNRbaJS4qIij92UcqKjmqw0lrqafUyyIc2GPdC7skU51bR35m/fy+bpdrP+hnK7pic0XuBdNBEqpJudwCGmJ0aQlRuO6jSgw3Nf8Oo8qgV1frFvERHJpn3YA/PL15aetpUQ6HQzplMKQTt6TLDQ/7SxWSoUs76Yh18I0Nrax4DmSKXRoIlBKhSx309DRKecaOHy0Kdg5+qexNBEopUKXdc09NmqoefsITsXmSV0bRBOBUipkHZ1pyLNpyOZv5N43uYUCTQRKqZDlOGHSOfticQuSCkmDaCJQSoUsOdo05PrturPYrmiOFwxJqb40ESilQpa7Gej42UftZedNbo2liUApFbKOzTV0rEoQLJ3FoUQTgVIqdJ3QNGR/Z7E7KO0sVkqpZiBeF91gGD5q9/kbQxOBUipkOY4fNNSghWmamvYRKKVUM3BPJ3G0acjY3zQUJHmoQTQRKKVC1gkrlNkYi5vdcx01hiYCpVTI8rl4fZBch7VpSCmlmoX3CmV2xuJyrNsiCIKpJ00ESqmQ5T3ls6uzODjmGgolAUkEIjJWRDaISKGITPbxerSIvGG9vkhEOni89ntr+wYRGROIeJRS4cFxQiZo3JrFTSEYaif15XciEBEn8AxwEdADuE5EengVuxXYa4zJBR4HHrH27QFMAHoCY4FnreMppdRpea9QFgzDR+0+f2MEokYwCCg0xhQZY6qAGcB4rzLjgenW47eBkeKqv40HZhhjKo0xm4BC63hKKXVa7ovu3sPVQHAsTOMWQhWCgKxZnAFs83heDJx9sjLGmBoR2Q+kWNu/9do3w9dJRGQSMAkgOzs7AGErpUKdu2novrdWkhIfFRxLVR6dCC90UkHIdBYbY6YaY/KMMXlpaWl2h6OUCjI3v7yE77YfsDuM4KmSNEAgEsF2IMvjeaa1zWcZEYkAkoDSeu6rlFI++fry39jr8MKNpUyYupBtZYf9isktdOoDgUkES4DOIpIjIlG4On9nepWZCUy0Hl8FzDWuetNMYII1qigH6AwsDkBMSqkw4PCRCRrbMrS7vIJvi8qorq3zKybv5TP9ccdry3h/RdN/N/Y7ERhjaoA7gU+BdcCbxpg1IjJFRMZZxV4EUkSkELgXmGztuwZ4E1gLfALcYYyp9TcmpVR48H3Rb1wmqKx2JYCoCP8ui4Hso5i9dhdrdzZ9c1dA+giMMbOMMV2MMZ2MMX+2tj1gjJlpPa4wxlxtjMk1xgwyxhR57Ptna7+uxpiPAxGPUio8+JpgrrHX4coa13fQdTvL+eP73/nd2bu6eB8/eXERVTWNr2FEOx1UVtdR42ct5XRCprNYKaW8xUSeeAlr7PfxSuuCPenVfKYv3MKBIzWNOo77/A9+sJavC/ZQuPtgIyOCyAgHL3+zmdz7m/Y7siYCpVTIap8Sz5TxPY/b1vgagSsRpCZEA1BysLJRx/E+v9PR+KaiKGfzXKI1ESilQpqvDuPGcCeCknJXAvDnAu7Jr0Tg0V9RV9d045A0ESilQpp3ImjswjQRDjnuwpuTGt+o4wRyYRzPeN5d3nSjhzQRKKVCmnfrSWMrCL8a2Zkl/zPK/4C81PrxTT7S481FBKiG4osmAqVUSPMerulPS1HB7nI/oznx/DV1jR/xE+U8drCmnDlDE4FSKqRFe43796dp5tbp+f6Gc8LZ/akRNNe8SZoIlFIhzTsR+NNEv/9ItX/BeLh9RCcAavxIBE3YGnT8eZrnNEop1TS8r7NOuxcEsE7vbtOvqfUnERx7L3fNWEHO7z+iojrwky9oIlBKhTTPG4Cv7J/JU9f1sy8YjjVNuYeN+tNH4PCqEhgTuGGtngKxHoFSStmmziMTBMOC8bmtE3hyQt+jTVb+9BH4uuY3RY1HawRKqZB2XCLwMw+42/W7tUls9DHSEqMZ3zeDrFZxdEyN9+vuYF83y3nXEgJBawRKqTOGvxPFxUW6lkwf2b2137H0bJfE3PtG+HUM70TQFM1CoDUCpVSI86wR+DsLg/u6GyyrTJ4wb1ETdYRrIlBKhTT3Rdsh/q8K5h63HyR54ISE5GiiK7YmAqVUSHPXAiIcjoAtGB8sNYL5hXsAODc3FQjcBHveNBEopUKau2nI4fD/An60aSho6gQu7s7rprpDQhOBUiq0Wddsp4jfF/BBHVoBMLRTqr9RBZS/y2eejo4aUkqFtGM1AsGPe7cAyOvQinVTxhIb5QxAZIHT1IlAawRKqZDmrgM4Hf7XCICgSwKgiUAppU7JXSNwivg9fDRYuW9Ka6q3p4lAKRXS3B3EIhI0o30CrSkXpQE/E4GItBKR2SJSYP1ueZJyE60yBSIy0doWJyIfich6EVkjIg/7E4tSKjy5h4y6LpZnZiZoqjuK3fytEUwGPjfGdAY+t54fR0RaAX8EzgYGAX/0SBh/N8Z0A/oBQ0XkIj/jUUqFGXdzkNNx5jYNuecXCtbho+OB6dbj6cBlPsqMAWYbY8qMMXuB2cBYY8xhY8wXAMaYKmAZkOlnPEqpMGOOu4/gzMwEiTGRAFzap12THN/f4aPpxpid1uMfgHQfZTKAbR7Pi61tR4lIMnAp8OTJTiQik4BJANnZ2Y2PWCl1Rsnr0Io/XNKdt5cWn7E1gnF92jGuiZIA1CMRiMgcoI2Pl+73fGKMMSLS4H8GEYkAXgeeMsYUnaycMWYqMBUgLy/vDP3nVko1VK+MJHplJPHBqp1naA9B0zttIjDGjDrZayKyS0TaGmN2ikhbYLePYtuBER7PM4EvPZ5PBQqMMU/UJ2CllPLFIWde01D/7GSWbd3X5Ofxt2loJjAReNj6/b6PMp8Cf/HoIB4N/B5ARB4CkoDb/IxDKRXmhOCZLC5QZkwaQlWtn7dL14O/ncUPAxeKSAEwynqOiOSJyAsAxpgy4E/AEutnijGmTEQycTUv9QCWicgKEdGEoJRqFEcA5hoKNlERDhKim34mIL/OYIwpBUb62J6Px7d8Y8w0YJpXmWKabjSUUirMnNMpBWdTTdh/htNJ55RSZ4R7R3e1O4SQpelTKaXCnCYCpZQKc5oIlFIqzGkiUEqpMKeJQCmlwpwmAqWUCnOaCJRSKsxpIlBKqTAnoThJk4iUAFsauXsqsCeA4TQXjbt5adzNL1RjD6W42xtj0rw3hmQi8IeI5Btj8uyOo6E07ualcTe/UI09VOP2pE1DSikV5jQRKKVUmAvHRDDV7gAaSeNuXhp38wvV2EM17qPCro9AKaXU8cKxRqCUUsqDJgKllApzYZMIRGSsiGwQkUIRmWx3PJ5EJEtEvhCRtSKyRkTusrY/KCLbrWU8V4jIxR77/N56LxtEZIx90YOIbBaR1VaM+da2ViIyW0QKrN8tre0iIk9Zsa8Skf42xdzV43NdISIHROTuYPzMRWSaiOwWke88tjX48xWRiVb5AhGZaFPcfxOR9VZs74lIsrW9g4gc8fjcn/PYZ4D191VovbcmXdnwJHE3+O8imK85JzDGnPE/gBPYCHQEooCVQA+74/KIry3Q33qcCHyPay3nB4H7fJTvYb2HaCDHem9OG+PfDKR6bXsUmGw9ngw8Yj2+GPgY1zKlg4FFQfD5O4EfgPbB+JkDw4H+wHeN/XyBVkCR9bul9bilDXGPBiKsx494xN3Bs5zXcRZb70Ws93aRDXE36O8i2K853j/hUiMYBBQaY4qMMVXADGC8zTEdZYzZaYxZZj0uB9YBGafYZTwwwxhTaYzZBBTieo/BZDww3Xo8HbjMY/srxuVbIFlE2toQn6eRwEZjzKnuVrftMzfGzAPKfMTTkM93DDDbGFNmjNkLzAbGNnfcxpjPjDE11tNvgcxTHcOKvYUx5lvjuvK+wrH32iRO8nmfzMn+LoL6muMtXBJBBrDN43kxp77Q2kZEOgD9gEXWpjutavQ0d/Wf4Hs/BvhMRJaKyCRrW7oxZqf1+Acg3XocbLEDTABe93geCp95Qz/fYIsf4BZc3/DdckRkuYh8JSLDrG0ZuGJ1szPuhvxdBOPnfVLhkghCgogkAO8AdxtjDgD/AjoBfYGdwD/si+6UzjXG9AcuAu4QkeGeL1rf5IJynLKIRAHjgLesTaHymR8VzJ/vyYjI/UAN8B9r004g2xjTD7gXeE1EWtgVnw8h93fREOGSCLYDWR7PM61tQUNEInElgf8YY94FMMbsMsbUGmPqgH9zrCkiqN6PMWa79Xs38B6uOHe5m3ys37ut4kEVO67ktcwYswtC5zOn4Z9v0MQvIjcBPwKut5IYVtNKqfV4Ka729S5WjJ7NR7bE3Yi/i6D5vOsjXBLBEqCziORY3wAnADNtjukoaxTEi8A6Y8xjHts9284vB9yjGGYCE0QkWkRygM64OtSanYjEi0ii+zGuzsDvrBjdI1MmAu9bj2cCN1qjWwYD+z2aOOxwHR7NQqHwmXvE05DP91NgtIi0tJo1RlvbmpWIjAV+C4wzxhz22J4mIk7rcUdcn2+RFfsBERls/T+5kWPvtTnjbujfRVBfc05gd291c/3gGk3xPa5vGvfbHY9XbOfiqtqvAlZYPxcDrwKrre0zgbYe+9xvvZcNNPEoitPE3hHXiIiVwBr3ZwukAJ8DBcAcoJW1XYBnrNhXA3k2xh4PlAJJHtuC7jPHlah2AtW42ppvbczni6tNvtD6udmmuAtxtZ27/86fs8peaf39rACWAZd6HCcP14V3I/A01owIzRx3g/8ugvma4/2jU0wopVSYC5emIaWUUiehiUAppcKcJgKllApzmgiUUirMaSJQSqkwp4lAKaXCnCYCpZQKc/8fhiRlFHjri4oAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.0403  # Value for C0\n",
    "K0 = -0.0012  # Value for K0\n",
    "K1 = -0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0120    # Value for b\n",
    "c = 2.0334    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    91.100000    90.875910    90.651821    90.427731    90.203641   \n",
      "351    90.875910    90.651821    90.427731    90.203641    89.979552   \n",
      "352    90.651821    90.427731    90.203641    89.979552    89.755462   \n",
      "353    90.427731    90.203641    89.979552    89.755462    89.531373   \n",
      "354    90.203641    89.979552    89.755462    89.531373    89.307283   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    89.979552    89.755462    89.531373    89.307283    89.094118  ...   \n",
      "351    89.755462    89.531373    89.307283    89.094118    89.015686  ...   \n",
      "352    89.531373    89.307283    89.094118    89.015686    88.937255  ...   \n",
      "353    89.307283    89.094118    89.015686    88.937255    88.858824  ...   \n",
      "354    89.094118    89.015686    88.937255    88.858824    88.780392  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   81.423343    0.000263   81.404669    0.000263   81.385994    0.000263   \n",
      "351   81.404669    0.000263   81.385994    0.000263   81.367320    0.000262   \n",
      "352   81.385994    0.000263   81.367320    0.000262   81.348646    0.000262   \n",
      "353   81.367320    0.000262   81.348646    0.000262   81.329972    0.000262   \n",
      "354   81.348646    0.000262   81.329972    0.000262   81.311298    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   81.367320    0.000262   81.348646    0.000262  \n",
      "351   81.348646    0.000262   81.329972    0.000262  \n",
      "352   81.329972    0.000262   81.311298    0.000262  \n",
      "353   81.311298    0.000262   81.292624    0.000262  \n",
      "354   81.292624    0.000262   81.273950    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 1, 251) (1200, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "15/15 [==============================] - 2s 42ms/step - loss: 6084.1768 - val_loss: 4727.9268\n",
      "Epoch 2/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6006.4175 - val_loss: 4686.3076\n",
      "Epoch 3/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5952.8901 - val_loss: 4644.9688\n",
      "Epoch 4/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5899.6812 - val_loss: 4603.9678\n",
      "Epoch 5/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5846.8525 - val_loss: 4563.2988\n",
      "Epoch 6/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5794.4033 - val_loss: 4522.9473\n",
      "Epoch 7/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5742.3232 - val_loss: 4482.9058\n",
      "Epoch 8/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5690.6074 - val_loss: 4443.1685\n",
      "Epoch 9/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5639.2490 - val_loss: 4403.7310\n",
      "Epoch 10/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5588.2446 - val_loss: 4364.5894\n",
      "Epoch 11/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5537.5908 - val_loss: 4325.7417\n",
      "Epoch 12/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5487.2832 - val_loss: 4287.1846\n",
      "Epoch 13/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5437.3198 - val_loss: 4248.9160\n",
      "Epoch 14/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5387.6978 - val_loss: 4210.9351\n",
      "Epoch 15/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5338.4165 - val_loss: 4173.2368\n",
      "Epoch 16/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5289.4712 - val_loss: 4135.8228\n",
      "Epoch 17/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 5240.8608 - val_loss: 4098.6885\n",
      "Epoch 18/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5192.5840 - val_loss: 4061.8337\n",
      "Epoch 19/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5144.6382 - val_loss: 4025.2566\n",
      "Epoch 20/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 5097.0210 - val_loss: 3988.9553\n",
      "Epoch 21/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 5049.7300 - val_loss: 3952.9280\n",
      "Epoch 22/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5002.7661 - val_loss: 3917.1733\n",
      "Epoch 23/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4956.1250 - val_loss: 3881.6890\n",
      "Epoch 24/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4909.8057 - val_loss: 3846.4751\n",
      "Epoch 25/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4863.8057 - val_loss: 3811.5286\n",
      "Epoch 26/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4818.1250 - val_loss: 3776.8484\n",
      "Epoch 27/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4772.7598 - val_loss: 3742.4338\n",
      "Epoch 28/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4727.7095 - val_loss: 3708.2820\n",
      "Epoch 29/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4682.9731 - val_loss: 3674.3931\n",
      "Epoch 30/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4638.5464 - val_loss: 3640.7642\n",
      "Epoch 31/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4594.4302 - val_loss: 3607.3940\n",
      "Epoch 32/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4550.6226 - val_loss: 3574.2825\n",
      "Epoch 33/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4507.1206 - val_loss: 3541.4265\n",
      "Epoch 34/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4463.9233 - val_loss: 3508.8259\n",
      "Epoch 35/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4421.0303 - val_loss: 3476.4795\n",
      "Epoch 36/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 4378.4385 - val_loss: 3444.3838\n",
      "Epoch 37/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4336.1455 - val_loss: 3412.5398\n",
      "Epoch 38/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4294.1533 - val_loss: 3380.9456\n",
      "Epoch 39/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4252.4565 - val_loss: 3349.5994\n",
      "Epoch 40/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4211.0552 - val_loss: 3318.5000\n",
      "Epoch 41/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4169.9473 - val_loss: 3287.6458\n",
      "Epoch 42/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4129.1323 - val_loss: 3257.0361\n",
      "Epoch 43/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4088.6084 - val_loss: 3226.6689\n",
      "Epoch 44/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4048.3728 - val_loss: 3196.5439\n",
      "Epoch 45/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4008.4258 - val_loss: 3166.6589\n",
      "Epoch 46/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3968.7649 - val_loss: 3137.0129\n",
      "Epoch 47/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3929.3882 - val_loss: 3107.6050\n",
      "Epoch 48/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3890.2957 - val_loss: 3078.4336\n",
      "Epoch 49/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3851.4844 - val_loss: 3049.4968\n",
      "Epoch 50/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3812.9541 - val_loss: 3020.7952\n",
      "Epoch 51/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3774.7029 - val_loss: 2992.3254\n",
      "Epoch 52/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3736.7290 - val_loss: 2964.0874\n",
      "Epoch 53/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3699.0312 - val_loss: 2936.0796\n",
      "Epoch 54/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3661.6086 - val_loss: 2908.3010\n",
      "Epoch 55/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3624.4592 - val_loss: 2880.7505\n",
      "Epoch 56/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3587.5815 - val_loss: 2853.4265\n",
      "Epoch 57/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3550.9741 - val_loss: 2826.3279\n",
      "Epoch 58/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3514.6367 - val_loss: 2799.4539\n",
      "Epoch 59/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3478.5667 - val_loss: 2772.8025\n",
      "Epoch 60/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3442.7632 - val_loss: 2746.3735\n",
      "Epoch 61/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3407.2249 - val_loss: 2720.1650\n",
      "Epoch 62/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3371.9497 - val_loss: 2694.1768\n",
      "Epoch 63/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3336.9375 - val_loss: 2668.4062\n",
      "Epoch 64/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3302.1870 - val_loss: 2642.8533\n",
      "Epoch 65/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3267.6953 - val_loss: 2617.5161\n",
      "Epoch 66/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3233.4622 - val_loss: 2592.3948\n",
      "Epoch 67/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3199.4866 - val_loss: 2567.4871\n",
      "Epoch 68/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3165.7666 - val_loss: 2542.7920\n",
      "Epoch 69/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3132.3020 - val_loss: 2518.3081\n",
      "Epoch 70/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3099.0896 - val_loss: 2494.0354\n",
      "Epoch 71/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3066.1294 - val_loss: 2469.9719\n",
      "Epoch 72/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3033.4197 - val_loss: 2446.1169\n",
      "Epoch 73/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 3000.9602 - val_loss: 2422.4692\n",
      "Epoch 74/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2968.7485 - val_loss: 2399.0276\n",
      "Epoch 75/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2936.7839 - val_loss: 2375.7905\n",
      "Epoch 76/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2905.0647 - val_loss: 2352.7583\n",
      "Epoch 77/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 2873.5906 - val_loss: 2329.9282\n",
      "Epoch 78/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2842.3591 - val_loss: 2307.3005\n",
      "Epoch 79/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2811.3699 - val_loss: 2284.8735\n",
      "Epoch 80/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2780.6211 - val_loss: 2262.6467\n",
      "Epoch 81/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2750.1123 - val_loss: 2240.6172\n",
      "Epoch 82/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 2719.8423 - val_loss: 2218.7869\n",
      "Epoch 83/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2689.8086 - val_loss: 2197.1519\n",
      "Epoch 84/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 2660.0112 - val_loss: 2175.7134\n",
      "Epoch 85/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2630.4487 - val_loss: 2154.4688\n",
      "Epoch 86/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2601.1204 - val_loss: 2133.4185\n",
      "Epoch 87/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2572.0237 - val_loss: 2112.5596\n",
      "Epoch 88/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2543.1584 - val_loss: 2091.8931\n",
      "Epoch 89/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2514.5232 - val_loss: 2071.4160\n",
      "Epoch 90/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2486.1162 - val_loss: 2051.1292\n",
      "Epoch 91/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2457.9380 - val_loss: 2031.0294\n",
      "Epoch 92/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2429.9858 - val_loss: 2011.1190\n",
      "Epoch 93/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 2402.2590 - val_loss: 1991.3933\n",
      "Epoch 94/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 2374.7573 - val_loss: 1971.8542\n",
      "Epoch 95/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2347.4783 - val_loss: 1952.4987\n",
      "Epoch 96/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2320.4216 - val_loss: 1933.3278\n",
      "Epoch 97/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2293.5864 - val_loss: 1914.3384\n",
      "Epoch 98/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2266.9697 - val_loss: 1895.5305\n",
      "Epoch 99/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2240.5728 - val_loss: 1876.9036\n",
      "Epoch 100/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2214.3936 - val_loss: 1858.4562\n",
      "Epoch 101/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2188.4304 - val_loss: 1840.1870\n",
      "Epoch 102/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2162.6829 - val_loss: 1822.0955\n",
      "Epoch 103/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2137.1497 - val_loss: 1804.1810\n",
      "Epoch 104/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 2111.8298 - val_loss: 1786.4421\n",
      "Epoch 105/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 2086.7224 - val_loss: 1768.8778\n",
      "Epoch 106/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2061.8262 - val_loss: 1751.4882\n",
      "Epoch 107/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2037.1400 - val_loss: 1734.2708\n",
      "Epoch 108/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2012.6627 - val_loss: 1717.2252\n",
      "Epoch 109/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1988.3931 - val_loss: 1700.3508\n",
      "Epoch 110/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1964.3302 - val_loss: 1683.6467\n",
      "Epoch 111/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1940.4733 - val_loss: 1667.1115\n",
      "Epoch 112/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1916.8214 - val_loss: 1650.7450\n",
      "Epoch 113/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1893.3730 - val_loss: 1634.5450\n",
      "Epoch 114/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1870.1274 - val_loss: 1618.5121\n",
      "Epoch 115/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1847.0834 - val_loss: 1602.6447\n",
      "Epoch 116/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1824.2396 - val_loss: 1586.9417\n",
      "Epoch 117/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1801.5956 - val_loss: 1571.4023\n",
      "Epoch 118/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1779.1501 - val_loss: 1556.0255\n",
      "Epoch 119/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1756.9022 - val_loss: 1540.8107\n",
      "Epoch 120/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1734.8500 - val_loss: 1525.7562\n",
      "Epoch 121/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1712.9940 - val_loss: 1510.8625\n",
      "Epoch 122/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1691.3323 - val_loss: 1496.1276\n",
      "Epoch 123/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1669.8640 - val_loss: 1481.5519\n",
      "Epoch 124/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 1648.5886 - val_loss: 1467.1328\n",
      "Epoch 125/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 1627.5040 - val_loss: 1452.8700\n",
      "Epoch 126/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1606.6099 - val_loss: 1438.7626\n",
      "Epoch 127/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1585.9055 - val_loss: 1424.8107\n",
      "Epoch 128/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1565.3893 - val_loss: 1411.0121\n",
      "Epoch 129/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1545.0604 - val_loss: 1397.3661\n",
      "Epoch 130/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1524.9178 - val_loss: 1383.8728\n",
      "Epoch 131/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1504.9612 - val_loss: 1370.5304\n",
      "Epoch 132/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1485.1888 - val_loss: 1357.3385\n",
      "Epoch 133/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1465.5999 - val_loss: 1344.2960\n",
      "Epoch 134/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1446.1937 - val_loss: 1331.4022\n",
      "Epoch 135/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1426.9694 - val_loss: 1318.6564\n",
      "Epoch 136/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1407.9254 - val_loss: 1306.0573\n",
      "Epoch 137/500\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1389.0616 - val_loss: 1293.6041\n",
      "Epoch 138/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1370.3759 - val_loss: 1281.2960\n",
      "Epoch 139/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1351.8680 - val_loss: 1269.1324\n",
      "Epoch 140/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1333.5369 - val_loss: 1257.1121\n",
      "Epoch 141/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1315.3815 - val_loss: 1245.2343\n",
      "Epoch 142/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1297.4010 - val_loss: 1233.4982\n",
      "Epoch 143/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1279.5948 - val_loss: 1221.9032\n",
      "Epoch 144/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1261.9609 - val_loss: 1210.4484\n",
      "Epoch 145/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1244.4995 - val_loss: 1199.1324\n",
      "Epoch 146/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1227.2092 - val_loss: 1187.9550\n",
      "Epoch 147/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1210.0890 - val_loss: 1176.9152\n",
      "Epoch 148/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1193.1378 - val_loss: 1166.0117\n",
      "Epoch 149/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1176.3551 - val_loss: 1155.2443\n",
      "Epoch 150/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1159.7397 - val_loss: 1144.6115\n",
      "Epoch 151/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1143.2906 - val_loss: 1134.1133\n",
      "Epoch 152/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1127.0072 - val_loss: 1123.7484\n",
      "Epoch 153/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1110.8882 - val_loss: 1113.5160\n",
      "Epoch 154/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1094.9325 - val_loss: 1103.4149\n",
      "Epoch 155/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1079.1399 - val_loss: 1093.4446\n",
      "Epoch 156/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1063.5090 - val_loss: 1083.6047\n",
      "Epoch 157/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1048.0389 - val_loss: 1073.8937\n",
      "Epoch 158/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1032.7289 - val_loss: 1064.3108\n",
      "Epoch 159/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1017.5775 - val_loss: 1054.8557\n",
      "Epoch 160/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1002.5847 - val_loss: 1045.5271\n",
      "Epoch 161/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 987.7487 - val_loss: 1036.3245\n",
      "Epoch 162/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 973.0692 - val_loss: 1027.2468\n",
      "Epoch 163/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 958.5450 - val_loss: 1018.2931\n",
      "Epoch 164/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 944.1751 - val_loss: 1009.4630\n",
      "Epoch 165/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 929.9589 - val_loss: 1000.7557\n",
      "Epoch 166/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 915.8954 - val_loss: 992.1697\n",
      "Epoch 167/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 901.9834 - val_loss: 983.7044\n",
      "Epoch 168/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 888.2225 - val_loss: 975.3594\n",
      "Epoch 169/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 874.6113 - val_loss: 967.1339\n",
      "Epoch 170/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 861.1492 - val_loss: 959.0264\n",
      "Epoch 171/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 847.8354 - val_loss: 951.0370\n",
      "Epoch 172/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 834.6688 - val_loss: 943.1643\n",
      "Epoch 173/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 821.6484 - val_loss: 935.4072\n",
      "Epoch 174/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 808.7734 - val_loss: 927.7655\n",
      "Epoch 175/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 796.0428 - val_loss: 920.2382\n",
      "Epoch 176/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 783.4561 - val_loss: 912.8243\n",
      "Epoch 177/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 771.0123 - val_loss: 905.5230\n",
      "Epoch 178/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 758.7101 - val_loss: 898.3340\n",
      "Epoch 179/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 746.5491 - val_loss: 891.2560\n",
      "Epoch 180/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 734.5283 - val_loss: 884.2881\n",
      "Epoch 181/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 722.6464 - val_loss: 877.4297\n",
      "Epoch 182/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 710.9025 - val_loss: 870.6796\n",
      "Epoch 183/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 699.2965 - val_loss: 864.0378\n",
      "Epoch 184/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 687.8270 - val_loss: 857.5031\n",
      "Epoch 185/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 676.4934 - val_loss: 851.0744\n",
      "Epoch 186/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 665.2942 - val_loss: 844.7510\n",
      "Epoch 187/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 654.2289 - val_loss: 838.5323\n",
      "Epoch 188/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 643.2963 - val_loss: 832.4171\n",
      "Epoch 189/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 632.4959 - val_loss: 826.4053\n",
      "Epoch 190/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 621.8271 - val_loss: 820.4952\n",
      "Epoch 191/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 611.2885 - val_loss: 814.6868\n",
      "Epoch 192/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 600.8796 - val_loss: 808.9789\n",
      "Epoch 193/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 590.5989 - val_loss: 803.3702\n",
      "Epoch 194/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 580.4460 - val_loss: 797.8608\n",
      "Epoch 195/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 570.4198 - val_loss: 792.4495\n",
      "Epoch 196/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 560.5198 - val_loss: 787.1356\n",
      "Epoch 197/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 550.7449 - val_loss: 781.9178\n",
      "Epoch 198/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 541.0936 - val_loss: 776.7958\n",
      "Epoch 199/500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 531.5660 - val_loss: 771.7684\n",
      "Epoch 200/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 522.1608 - val_loss: 766.8353\n",
      "Epoch 201/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 512.8768 - val_loss: 761.9953\n",
      "Epoch 202/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 503.7137 - val_loss: 757.2477\n",
      "Epoch 203/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 494.6703 - val_loss: 752.5918\n",
      "Epoch 204/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 485.7461 - val_loss: 748.0267\n",
      "Epoch 205/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 476.9398 - val_loss: 743.5515\n",
      "Epoch 206/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 468.2506 - val_loss: 739.1652\n",
      "Epoch 207/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 459.6774 - val_loss: 734.8676\n",
      "Epoch 208/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 451.2203 - val_loss: 730.6573\n",
      "Epoch 209/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 442.8771 - val_loss: 726.5338\n",
      "Epoch 210/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 434.6479 - val_loss: 722.4963\n",
      "Epoch 211/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 426.5317 - val_loss: 718.5440\n",
      "Epoch 212/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 418.5271 - val_loss: 714.6758\n",
      "Epoch 213/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 410.6337 - val_loss: 710.8913\n",
      "Epoch 214/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 402.8506 - val_loss: 707.1891\n",
      "Epoch 215/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 395.1766 - val_loss: 703.5692\n",
      "Epoch 216/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 387.6112 - val_loss: 700.0302\n",
      "Epoch 217/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 380.1533 - val_loss: 696.5712\n",
      "Epoch 218/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 372.8019 - val_loss: 693.1917\n",
      "Epoch 219/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 365.5562 - val_loss: 689.8910\n",
      "Epoch 220/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 358.4156 - val_loss: 686.6679\n",
      "Epoch 221/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 351.3789 - val_loss: 683.5217\n",
      "Epoch 222/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 344.4454 - val_loss: 680.4519\n",
      "Epoch 223/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 337.6147 - val_loss: 677.4574\n",
      "Epoch 224/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 330.8852 - val_loss: 674.5378\n",
      "Epoch 225/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 324.2562 - val_loss: 671.6914\n",
      "Epoch 226/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 317.7270 - val_loss: 668.9182\n",
      "Epoch 227/500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 311.2968 - val_loss: 666.2173\n",
      "Epoch 228/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 304.9646 - val_loss: 663.5876\n",
      "Epoch 229/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 298.7294 - val_loss: 661.0283\n",
      "Epoch 230/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 292.5904 - val_loss: 658.5387\n",
      "Epoch 231/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 286.5469 - val_loss: 656.1182\n",
      "Epoch 232/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 280.5979 - val_loss: 653.7656\n",
      "Epoch 233/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 274.7425 - val_loss: 651.4803\n",
      "Epoch 234/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 268.9799 - val_loss: 649.2614\n",
      "Epoch 235/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 263.3090 - val_loss: 647.1083\n",
      "Epoch 236/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 257.7295 - val_loss: 645.0201\n",
      "Epoch 237/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 252.2401 - val_loss: 642.9959\n",
      "Epoch 238/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 246.8401 - val_loss: 641.0349\n",
      "Epoch 239/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 241.5284 - val_loss: 639.1363\n",
      "Epoch 240/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 236.3045 - val_loss: 637.2994\n",
      "Epoch 241/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 231.1673 - val_loss: 635.5233\n",
      "Epoch 242/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 226.1159 - val_loss: 633.8073\n",
      "Epoch 243/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 221.1498 - val_loss: 632.1505\n",
      "Epoch 244/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 216.2677 - val_loss: 630.5521\n",
      "Epoch 245/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 211.4686 - val_loss: 629.0111\n",
      "Epoch 246/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 206.7520 - val_loss: 627.5271\n",
      "Epoch 247/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 202.1171 - val_loss: 626.0993\n",
      "Epoch 248/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 197.5631 - val_loss: 624.7265\n",
      "Epoch 249/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 193.0890 - val_loss: 623.4083\n",
      "Epoch 250/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 188.6938 - val_loss: 622.1436\n",
      "Epoch 251/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 184.3769 - val_loss: 620.9318\n",
      "Epoch 252/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 180.1374 - val_loss: 619.7722\n",
      "Epoch 253/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 175.9741 - val_loss: 618.6635\n",
      "Epoch 254/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 171.8866 - val_loss: 617.6055\n",
      "Epoch 255/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 167.8738 - val_loss: 616.5971\n",
      "Epoch 256/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 163.9351 - val_loss: 615.6378\n",
      "Epoch 257/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 160.0693 - val_loss: 614.7264\n",
      "Epoch 258/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 156.2757 - val_loss: 613.8623\n",
      "Epoch 259/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 152.5537 - val_loss: 613.0448\n",
      "Epoch 260/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 148.9024 - val_loss: 612.2731\n",
      "Epoch 261/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 145.3209 - val_loss: 611.5463\n",
      "Epoch 262/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 141.8082 - val_loss: 610.8639\n",
      "Epoch 263/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 138.3635 - val_loss: 610.2248\n",
      "Epoch 264/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 134.9861 - val_loss: 609.6284\n",
      "Epoch 265/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 131.6754 - val_loss: 609.0739\n",
      "Epoch 266/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 128.4301 - val_loss: 608.5605\n",
      "Epoch 267/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 125.2496 - val_loss: 608.0875\n",
      "Epoch 268/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 122.1329 - val_loss: 607.6541\n",
      "Epoch 269/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 119.0795 - val_loss: 607.2595\n",
      "Epoch 270/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 116.0882 - val_loss: 606.9031\n",
      "Epoch 271/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 113.1585 - val_loss: 606.5839\n",
      "Epoch 272/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 110.2895 - val_loss: 606.3015\n",
      "Epoch 273/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 107.4803 - val_loss: 606.0546\n",
      "Epoch 274/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 104.7301 - val_loss: 605.8430\n",
      "Epoch 275/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 102.0382 - val_loss: 605.6656\n",
      "Epoch 276/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 99.4039 - val_loss: 605.5219\n",
      "Epoch 277/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 96.8260 - val_loss: 605.4111\n",
      "Epoch 278/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 94.3042 - val_loss: 605.3323\n",
      "Epoch 279/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 91.8374 - val_loss: 605.2850\n",
      "Epoch 280/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 89.4248 - val_loss: 605.2684\n",
      "Epoch 281/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 87.0658 - val_loss: 605.2818\n",
      "Epoch 282/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 84.7594 - val_loss: 605.3244\n",
      "Epoch 283/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 82.5048 - val_loss: 605.3955\n",
      "Epoch 284/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 80.3015 - val_loss: 605.4944\n",
      "Epoch 285/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 78.1483 - val_loss: 605.6205\n",
      "Epoch 286/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 76.0449 - val_loss: 605.7731\n",
      "Epoch 287/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 73.9902 - val_loss: 605.9513\n",
      "Epoch 288/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 71.9836 - val_loss: 606.1545\n",
      "Epoch 289/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 70.0244 - val_loss: 606.3820\n",
      "Epoch 290/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 68.1117 - val_loss: 606.6334\n",
      "Epoch 291/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 66.2448 - val_loss: 606.9077\n",
      "Epoch 292/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 64.4228 - val_loss: 607.2042\n",
      "Epoch 293/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 62.6453 - val_loss: 607.5225\n",
      "Epoch 294/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 60.9113 - val_loss: 607.8616\n",
      "Epoch 295/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 59.2201 - val_loss: 608.2211\n",
      "Epoch 296/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 57.5712 - val_loss: 608.6004\n",
      "Epoch 297/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 55.9636 - val_loss: 608.9985\n",
      "Epoch 298/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 54.3966 - val_loss: 609.4152\n",
      "Epoch 299/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 52.8695 - val_loss: 609.8495\n",
      "Epoch 300/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 51.3818 - val_loss: 610.3010\n",
      "Epoch 301/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 49.9325 - val_loss: 610.7691\n",
      "Epoch 302/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 48.5211 - val_loss: 611.2530\n",
      "Epoch 303/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 47.1469 - val_loss: 611.7521\n",
      "Epoch 304/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 45.8091 - val_loss: 612.2659\n",
      "Epoch 305/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 44.5071 - val_loss: 612.7939\n",
      "Epoch 306/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 43.2400 - val_loss: 613.3353\n",
      "Epoch 307/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 42.0075 - val_loss: 613.8896\n",
      "Epoch 308/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 40.8088 - val_loss: 614.4562\n",
      "Epoch 309/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 39.6432 - val_loss: 615.0348\n",
      "Epoch 310/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 38.5099 - val_loss: 615.6243\n",
      "Epoch 311/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 37.4086 - val_loss: 616.2245\n",
      "Epoch 312/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 36.3384 - val_loss: 616.8348\n",
      "Epoch 313/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 35.2985 - val_loss: 617.4548\n",
      "Epoch 314/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 34.2886 - val_loss: 618.0837\n",
      "Epoch 315/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 33.3081 - val_loss: 618.7210\n",
      "Epoch 316/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 32.3562 - val_loss: 619.3663\n",
      "Epoch 317/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 31.4323 - val_loss: 620.0191\n",
      "Epoch 318/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 30.5357 - val_loss: 620.6788\n",
      "Epoch 319/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 29.6660 - val_loss: 621.3449\n",
      "Epoch 320/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 28.8227 - val_loss: 622.0172\n",
      "Epoch 321/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 28.0049 - val_loss: 622.6946\n",
      "Epoch 322/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 27.2123 - val_loss: 623.3772\n",
      "Epoch 323/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 26.4442 - val_loss: 624.0643\n",
      "Epoch 324/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 25.7000 - val_loss: 624.7555\n",
      "Epoch 325/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.9793 - val_loss: 625.4503\n",
      "Epoch 326/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 24.2813 - val_loss: 626.1483\n",
      "Epoch 327/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 23.6057 - val_loss: 626.8491\n",
      "Epoch 328/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 22.9517 - val_loss: 627.5523\n",
      "Epoch 329/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 22.3190 - val_loss: 628.2574\n",
      "Epoch 330/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 21.7071 - val_loss: 628.9638\n",
      "Epoch 331/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 21.1153 - val_loss: 629.6715\n",
      "Epoch 332/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 20.5432 - val_loss: 630.3799\n",
      "Epoch 333/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 19.9904 - val_loss: 631.0886\n",
      "Epoch 334/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 19.4562 - val_loss: 631.7972\n",
      "Epoch 335/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.9403 - val_loss: 632.5057\n",
      "Epoch 336/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.4421 - val_loss: 633.2133\n",
      "Epoch 337/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 17.9611 - val_loss: 633.9198\n",
      "Epoch 338/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 17.4969 - val_loss: 634.6249\n",
      "Epoch 339/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 17.0491 - val_loss: 635.3281\n",
      "Epoch 340/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.6173 - val_loss: 636.0293\n",
      "Epoch 341/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.2009 - val_loss: 636.7281\n",
      "Epoch 342/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.7995 - val_loss: 637.4242\n",
      "Epoch 343/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.4128 - val_loss: 638.1174\n",
      "Epoch 344/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.0403 - val_loss: 638.8074\n",
      "Epoch 345/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.6815 - val_loss: 639.4938\n",
      "Epoch 346/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.3361 - val_loss: 640.1765\n",
      "Epoch 347/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.0037 - val_loss: 640.8548\n",
      "Epoch 348/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 13.6839 - val_loss: 641.5289\n",
      "Epoch 349/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.3764 - val_loss: 642.1984\n",
      "Epoch 350/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.0808 - val_loss: 642.8632\n",
      "Epoch 351/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.7966 - val_loss: 643.5229\n",
      "Epoch 352/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.5235 - val_loss: 644.1774\n",
      "Epoch 353/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.2613 - val_loss: 644.8265\n",
      "Epoch 354/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 12.0094 - val_loss: 645.4698\n",
      "Epoch 355/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.7677 - val_loss: 646.1074\n",
      "Epoch 356/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.5359 - val_loss: 646.7387\n",
      "Epoch 357/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.3135 - val_loss: 647.3641\n",
      "Epoch 358/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.1002 - val_loss: 647.9831\n",
      "Epoch 359/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 10.8958 - val_loss: 648.5956\n",
      "Epoch 360/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 10.6999 - val_loss: 649.2014\n",
      "Epoch 361/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 10.5123 - val_loss: 649.8005\n",
      "Epoch 362/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 10.3326 - val_loss: 650.3927\n",
      "Epoch 363/500\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 10.1606 - val_loss: 650.9778\n",
      "Epoch 364/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 9.9960 - val_loss: 651.5558\n",
      "Epoch 365/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 9.8387 - val_loss: 652.1264\n",
      "Epoch 366/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 9.6882 - val_loss: 652.6898\n",
      "Epoch 367/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 9.5444 - val_loss: 653.2454\n",
      "Epoch 368/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 9.4070 - val_loss: 653.7938\n",
      "Epoch 369/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 9.2758 - val_loss: 654.3344\n",
      "Epoch 370/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 9.1505 - val_loss: 654.8672\n",
      "Epoch 371/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 9.0310 - val_loss: 655.3924\n",
      "Epoch 372/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 8.9169 - val_loss: 655.9096\n",
      "Epoch 373/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 8.8081 - val_loss: 656.4191\n",
      "Epoch 374/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 8.7045 - val_loss: 656.9207\n",
      "Epoch 375/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 8.6056 - val_loss: 657.4141\n",
      "Epoch 376/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 8.5116 - val_loss: 657.8996\n",
      "Epoch 377/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 8.4220 - val_loss: 658.3771\n",
      "Epoch 378/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 8.3367 - val_loss: 658.8466\n",
      "Epoch 379/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 8.2556 - val_loss: 659.3080\n",
      "Epoch 380/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 8.1784 - val_loss: 659.7615\n",
      "Epoch 381/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 8.1051 - val_loss: 660.2067\n",
      "Epoch 382/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 8.0354 - val_loss: 660.6437\n",
      "Epoch 383/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 7.9693 - val_loss: 661.0729\n",
      "Epoch 384/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 7.9064 - val_loss: 661.4940\n",
      "Epoch 385/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 7.8468 - val_loss: 661.9070\n",
      "Epoch 386/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 7.7902 - val_loss: 662.3122\n",
      "Epoch 387/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 7.7366 - val_loss: 662.7092\n",
      "Epoch 388/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 7.6857 - val_loss: 663.0984\n",
      "Epoch 389/500\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 7.6375 - val_loss: 663.4795\n",
      "Epoch 390/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 7.5919 - val_loss: 663.8528\n",
      "Epoch 391/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 7.5488 - val_loss: 664.2184\n",
      "Epoch 392/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 7.5079 - val_loss: 664.5762\n",
      "Epoch 393/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 7.4692 - val_loss: 664.9263\n",
      "Epoch 394/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 7.4327 - val_loss: 665.2687\n",
      "Epoch 395/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 7.3981 - val_loss: 665.6034\n",
      "Epoch 396/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 7.3655 - val_loss: 665.9306\n",
      "Epoch 397/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 7.3347 - val_loss: 666.2501\n",
      "Epoch 398/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 7.3056 - val_loss: 666.5624\n",
      "Epoch 399/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 7.2782 - val_loss: 666.8676\n",
      "Epoch 400/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 7.2523 - val_loss: 667.1653\n",
      "Epoch 401/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 7.2280 - val_loss: 667.4560\n",
      "Epoch 402/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 7.2050 - val_loss: 667.7394\n",
      "Epoch 403/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 7.1834 - val_loss: 668.0157\n",
      "Epoch 404/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 7.1630 - val_loss: 668.2854\n",
      "Epoch 405/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 7.1439 - val_loss: 668.5482\n",
      "Epoch 406/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 7.1259 - val_loss: 668.8043\n",
      "Epoch 407/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 7.1089 - val_loss: 669.0539\n",
      "Epoch 408/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 7.0930 - val_loss: 669.2968\n",
      "Epoch 409/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 7.0780 - val_loss: 669.5331\n",
      "Epoch 410/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 7.0640 - val_loss: 669.7631\n",
      "Epoch 411/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 7.0508 - val_loss: 669.9869\n",
      "Epoch 412/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 7.0385 - val_loss: 670.2045\n",
      "Epoch 413/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 7.0270 - val_loss: 670.4164\n",
      "Epoch 414/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 7.0161 - val_loss: 670.6221\n",
      "Epoch 415/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 7.0059 - val_loss: 670.8219\n",
      "Epoch 416/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.9964 - val_loss: 671.0160\n",
      "Epoch 417/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.9875 - val_loss: 671.2044\n",
      "Epoch 418/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.9792 - val_loss: 671.3874\n",
      "Epoch 419/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.9714 - val_loss: 671.5647\n",
      "Epoch 420/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.9641 - val_loss: 671.7367\n",
      "Epoch 421/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.9573 - val_loss: 671.9037\n",
      "Epoch 422/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.9509 - val_loss: 672.0655\n",
      "Epoch 423/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 6.9450 - val_loss: 672.2221\n",
      "Epoch 424/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.9395 - val_loss: 672.3741\n",
      "Epoch 425/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.9343 - val_loss: 672.5215\n",
      "Epoch 426/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.9295 - val_loss: 672.6638\n",
      "Epoch 427/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.9250 - val_loss: 672.8018\n",
      "Epoch 428/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.9209 - val_loss: 672.9348\n",
      "Epoch 429/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.9170 - val_loss: 673.0637\n",
      "Epoch 430/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.9134 - val_loss: 673.1885\n",
      "Epoch 431/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.9100 - val_loss: 673.3091\n",
      "Epoch 432/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.9069 - val_loss: 673.4252\n",
      "Epoch 433/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.9040 - val_loss: 673.5378\n",
      "Epoch 434/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.9013 - val_loss: 673.6462\n",
      "Epoch 435/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.8988 - val_loss: 673.7512\n",
      "Epoch 436/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.8965 - val_loss: 673.8522\n",
      "Epoch 437/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.8943 - val_loss: 673.9494\n",
      "Epoch 438/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 6.8924 - val_loss: 674.0435\n",
      "Epoch 439/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 6.8905 - val_loss: 674.1340\n",
      "Epoch 440/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 6.8888 - val_loss: 674.2213\n",
      "Epoch 441/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.8872 - val_loss: 674.3054\n",
      "Epoch 442/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.8857 - val_loss: 674.3861\n",
      "Epoch 443/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.8844 - val_loss: 674.4641\n",
      "Epoch 444/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.8831 - val_loss: 674.5389\n",
      "Epoch 445/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.8820 - val_loss: 674.6110\n",
      "Epoch 446/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.8809 - val_loss: 674.6802\n",
      "Epoch 447/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 6.8800 - val_loss: 674.7469\n",
      "Epoch 448/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.8790 - val_loss: 674.8107\n",
      "Epoch 449/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.8782 - val_loss: 674.8721\n",
      "Epoch 450/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.8774 - val_loss: 674.9309\n",
      "Epoch 451/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.8768 - val_loss: 674.9875\n",
      "Epoch 452/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 6.8761 - val_loss: 675.0418\n",
      "Epoch 453/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.8755 - val_loss: 675.0936\n",
      "Epoch 454/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.8750 - val_loss: 675.1436\n",
      "Epoch 455/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.8745 - val_loss: 675.1915\n",
      "Epoch 456/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.8740 - val_loss: 675.2372\n",
      "Epoch 457/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.8736 - val_loss: 675.2809\n",
      "Epoch 458/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 6.8732 - val_loss: 675.3231\n",
      "Epoch 459/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 6.8729 - val_loss: 675.3631\n",
      "Epoch 460/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.8726 - val_loss: 675.4015\n",
      "Epoch 461/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.8723 - val_loss: 675.4382\n",
      "Epoch 462/500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 6.8721 - val_loss: 675.4734\n",
      "Epoch 463/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 6.8718 - val_loss: 675.5068\n",
      "Epoch 464/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 6.8716 - val_loss: 675.5389\n",
      "Epoch 465/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 6.8714 - val_loss: 675.5696\n",
      "Epoch 466/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.8713 - val_loss: 675.5988\n",
      "Epoch 467/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.8711 - val_loss: 675.6267\n",
      "Epoch 468/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.8710 - val_loss: 675.6532\n",
      "Epoch 469/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.8709 - val_loss: 675.6785\n",
      "Epoch 470/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.8708 - val_loss: 675.7025\n",
      "Epoch 471/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.8707 - val_loss: 675.7255\n",
      "Epoch 472/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.8707 - val_loss: 675.7471\n",
      "Epoch 473/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.8706 - val_loss: 675.7678\n",
      "Epoch 474/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.8705 - val_loss: 675.7874\n",
      "Epoch 475/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.8705 - val_loss: 675.8063\n",
      "Epoch 476/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 6.8705 - val_loss: 675.8240\n",
      "Epoch 477/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 6.8705 - val_loss: 675.8406\n",
      "Epoch 478/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 6.8705 - val_loss: 675.8568\n",
      "Epoch 479/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 6.8705 - val_loss: 675.8719\n",
      "Epoch 480/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 6.8705 - val_loss: 675.8864\n",
      "Epoch 481/500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 6.8705 - val_loss: 675.9001\n",
      "Epoch 482/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 6.8706 - val_loss: 675.9129\n",
      "Epoch 483/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 6.8706 - val_loss: 675.9251\n",
      "Epoch 484/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 6.8706 - val_loss: 675.9366\n",
      "Epoch 485/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 6.8707 - val_loss: 675.9475\n",
      "Epoch 486/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.8707 - val_loss: 675.9581\n",
      "Epoch 487/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.8708 - val_loss: 675.9681\n",
      "Epoch 488/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.8708 - val_loss: 675.9775\n",
      "Epoch 489/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 6.8709 - val_loss: 675.9861\n",
      "Epoch 490/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 6.8709 - val_loss: 675.9945\n",
      "Epoch 491/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 6.8710 - val_loss: 676.0021\n",
      "Epoch 492/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 6.8711 - val_loss: 676.0095\n",
      "Epoch 493/500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 6.8711 - val_loss: 676.0164\n",
      "Epoch 494/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 6.8712 - val_loss: 676.0231\n",
      "Epoch 495/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 6.8713 - val_loss: 676.0291\n",
      "Epoch 496/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 6.8714 - val_loss: 676.0349\n",
      "Epoch 497/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 6.8715 - val_loss: 676.0400\n",
      "Epoch 498/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 6.8716 - val_loss: 676.0452\n",
      "Epoch 499/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 6.8716 - val_loss: 676.0496\n",
      "Epoch 500/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 6.8717 - val_loss: 676.0541\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.0403, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0012, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0120, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.0334, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 596ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.44584641e+01, 7.44511811e+01, 7.44438982e+01, 7.44366153e+01,\n",
       "        7.44293324e+01, 7.44220495e+01, 7.44147666e+01, 7.44074837e+01,\n",
       "        7.44002007e+01, 7.43929178e+01, 7.43856349e+01, 7.43783520e+01,\n",
       "        7.43710691e+01, 7.82098740e+01, 7.81342437e+01, 7.80586134e+01,\n",
       "        7.79829832e+01, 7.79073529e+01, 7.77786181e+01, 7.76441643e+01,\n",
       "        7.75097105e+01, 7.73752568e+01, 7.72408030e+01, 7.71063492e+01,\n",
       "        7.69718954e+01, 7.68374416e+01, 7.67029879e+01, 7.65685341e+01,\n",
       "        7.64340803e+01, 7.62998133e+01, 7.62325864e+01, 7.61653595e+01,\n",
       "        7.60981326e+01, 7.60309057e+01, 7.59636788e+01, 7.58964519e+01,\n",
       "        7.58292250e+01, 7.57619981e+01, 7.56947712e+01, 7.56275443e+01,\n",
       "        1.33370146e-01, 0.00000000e+00, 3.10590327e-01, 0.00000000e+00,\n",
       "        3.39932400e-02, 1.07672811e-01, 3.69773686e-01, 7.68673203e+01,\n",
       "        7.67328665e+01, 7.65984127e+01, 7.64639589e+01, 7.63295051e+01,\n",
       "        7.62475257e+01, 7.61802988e+01, 7.61130719e+01, 7.60458450e+01,\n",
       "        7.59786181e+01, 7.59113912e+01, 7.58441643e+01, 7.57769374e+01,\n",
       "        7.57097105e+01, 7.56424837e+01, 7.55752568e+01, 7.55080299e+01,\n",
       "        7.54304435e+01, 7.53514519e+01, 7.52724603e+01, 7.51934687e+01,\n",
       "        7.51144771e+01, 7.50354855e+01, 7.49564939e+01, 7.48775023e+01,\n",
       "        7.47985107e+01, 7.92870941e+01, 5.90972530e-02, 3.82775307e-01,\n",
       "        6.86986327e-01, 4.47119743e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.52969475e+01, 0.00000000e+00, 9.08673465e-01, 5.18556535e-01,\n",
       "        0.00000000e+00, 1.88238751e-02, 2.49879092e-01, 0.00000000e+00,\n",
       "        1.82585105e-01, 7.02037811e-01, 0.00000000e+00, 1.43628478e-01,\n",
       "        0.00000000e+00, 5.13238549e-01, 4.37101305e-01, 2.20264018e-01,\n",
       "        0.00000000e+00, 1.37467071e-01, 0.00000000e+00, 5.85591793e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([71.75105042, 71.75011671, 71.74918301, 71.7482493 , 71.74731559,\n",
       "       71.74638189, 71.74544818, 71.74451447, 71.74358077, 71.74264706,\n",
       "       71.74171335, 71.74077965, 71.73984594, 71.73891223, 71.73797852,\n",
       "       71.73704482, 71.73611111, 71.7351774 , 71.7342437 , 71.73330999,\n",
       "       71.73237628, 71.73144258, 71.73050887, 71.72957516, 71.72864146,\n",
       "       71.72770775, 71.72677404, 71.72584034, 71.72490663, 71.72397292,\n",
       "       71.72303922, 71.72210551, 71.7211718 , 71.7202381 , 71.71930439,\n",
       "       71.71837068, 71.71743697, 71.71650327, 71.71556956, 71.71463585,\n",
       "       71.71370215, 71.71276844, 71.71183473, 71.71090103, 71.70996732,\n",
       "       71.70903361, 71.70809991, 71.7071662 , 71.70623249, 71.70529879,\n",
       "       71.70436508, 71.70343137, 71.70249767, 71.70156396, 71.70063025,\n",
       "       71.69848273, 71.69381419, 71.68914566, 71.68447712, 71.67980859,\n",
       "       71.67514006, 71.67047152, 71.66580299, 71.66113445, 71.65646592,\n",
       "       71.65179739, 71.64712885, 71.64246032, 71.63779178, 71.63312325,\n",
       "       71.62845472, 71.62378618, 71.61911765, 71.61444911, 71.60978058,\n",
       "       71.60511204, 71.60044351, 71.59577498, 71.59110644, 71.58643791,\n",
       "       71.58176937, 71.57710084, 71.57243231, 71.56776377, 71.56309524,\n",
       "       71.5584267 , 71.55375817, 71.54908964, 71.5444211 , 71.53975257,\n",
       "       71.53508403, 71.5304155 , 71.52574697, 71.52107843, 71.5164099 ,\n",
       "       71.51174136, 71.50707283, 71.5024043 , 71.49773576, 71.49306723])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.56340191019133\n",
      "25.85747516922715\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
