{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "1945    70.265838\n",
       "1946    70.260703\n",
       "1947    70.255567\n",
       "1948    70.250432\n",
       "1949    70.245296\n",
       "Name: C3, Length: 1950, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c3_interpolated_1850_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       91.100000\n",
       "1       90.875910\n",
       "2       90.651821\n",
       "3       90.427731\n",
       "4       90.203641\n",
       "          ...    \n",
       "1845     0.000000\n",
       "1846     0.000000\n",
       "1847     0.252373\n",
       "1848     0.039135\n",
       "1849     0.000000\n",
       "Name: C3, Length: 1850, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1850)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.100000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.875910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.651821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90.427731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90.203641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     91.100000  0.000298\n",
       "1     90.875910  0.000298\n",
       "2     90.651821  0.000297\n",
       "3     90.427731  0.000297\n",
       "4     90.203641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.0403  # Value for C0\n",
    "K0 = -0.0012  # Value for K0\n",
    "K1 = -0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0120    # Value for b\n",
    "c = 2.0334    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    91.100000    90.875910    90.651821    90.427731    90.203641   \n",
      "351    90.875910    90.651821    90.427731    90.203641    89.979552   \n",
      "352    90.651821    90.427731    90.203641    89.979552    89.755462   \n",
      "353    90.427731    90.203641    89.979552    89.755462    89.531373   \n",
      "354    90.203641    89.979552    89.755462    89.531373    89.307283   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    89.979552    89.755462    89.531373    89.307283    89.094118  ...   \n",
      "351    89.755462    89.531373    89.307283    89.094118    89.015686  ...   \n",
      "352    89.531373    89.307283    89.094118    89.015686    88.937255  ...   \n",
      "353    89.307283    89.094118    89.015686    88.937255    88.858824  ...   \n",
      "354    89.094118    89.015686    88.937255    88.858824    88.780392  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   81.423343    0.000263   81.404669    0.000263   81.385994    0.000263   \n",
      "351   81.404669    0.000263   81.385994    0.000263   81.367320    0.000262   \n",
      "352   81.385994    0.000263   81.367320    0.000262   81.348646    0.000262   \n",
      "353   81.367320    0.000262   81.348646    0.000262   81.329972    0.000262   \n",
      "354   81.348646    0.000262   81.329972    0.000262   81.311298    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   81.367320    0.000262   81.348646    0.000262  \n",
      "351   81.348646    0.000262   81.329972    0.000262  \n",
      "352   81.329972    0.000262   81.311298    0.000262  \n",
      "353   81.311298    0.000262   81.292624    0.000262  \n",
      "354   81.292624    0.000262   81.273950    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 1, 251) (1400, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "18/18 [==============================] - 3s 39ms/step - loss: 5972.6680 - val_loss: 4322.4785\n",
      "Epoch 2/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5766.2578 - val_loss: 4167.7944\n",
      "Epoch 3/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5653.6191 - val_loss: 4087.2510\n",
      "Epoch 4/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5564.1807 - val_loss: 4025.9548\n",
      "Epoch 5/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 5483.3740 - val_loss: 3969.5322\n",
      "Epoch 6/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5404.5474 - val_loss: 3914.5156\n",
      "Epoch 7/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 5327.3364 - val_loss: 3860.6455\n",
      "Epoch 8/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5251.4634 - val_loss: 3807.7629\n",
      "Epoch 9/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 5176.7563 - val_loss: 3755.7695\n",
      "Epoch 10/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5103.1113 - val_loss: 3704.6016\n",
      "Epoch 11/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5030.4551 - val_loss: 3654.2124\n",
      "Epoch 12/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 4958.7358 - val_loss: 3604.5693\n",
      "Epoch 13/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 4887.9170 - val_loss: 3555.6467\n",
      "Epoch 14/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 4817.9668 - val_loss: 3507.4231\n",
      "Epoch 15/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 4748.8618 - val_loss: 3459.8813\n",
      "Epoch 16/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4680.5815 - val_loss: 3413.0066\n",
      "Epoch 17/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4613.1099 - val_loss: 3366.7866\n",
      "Epoch 18/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4546.4297 - val_loss: 3321.2100\n",
      "Epoch 19/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4480.5303 - val_loss: 3276.2661\n",
      "Epoch 20/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4415.3979 - val_loss: 3231.9458\n",
      "Epoch 21/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4351.0210 - val_loss: 3188.2405\n",
      "Epoch 22/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4287.3906 - val_loss: 3145.1423\n",
      "Epoch 23/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4224.4976 - val_loss: 3102.6438\n",
      "Epoch 24/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 4162.3320 - val_loss: 3060.7371\n",
      "Epoch 25/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 4100.8862 - val_loss: 3019.4158\n",
      "Epoch 26/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 4040.1514 - val_loss: 2978.6741\n",
      "Epoch 27/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 3980.1223 - val_loss: 2938.5051\n",
      "Epoch 28/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 3920.7893 - val_loss: 2898.9028\n",
      "Epoch 29/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 3862.1460 - val_loss: 2859.8623\n",
      "Epoch 30/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 3804.1853 - val_loss: 2821.3770\n",
      "Epoch 31/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 3746.9009 - val_loss: 2783.4434\n",
      "Epoch 32/500\n",
      "18/18 [==============================] - 0s 18ms/step - loss: 3690.2866 - val_loss: 2746.0557\n",
      "Epoch 33/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 3634.3357 - val_loss: 2709.2131\n",
      "Epoch 34/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 3579.0422 - val_loss: 2672.9207\n",
      "Epoch 35/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 3524.3997 - val_loss: 2637.2693\n",
      "Epoch 36/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 3470.4028 - val_loss: 2601.7007\n",
      "Epoch 37/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 3417.0452 - val_loss: 2566.8621\n",
      "Epoch 38/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3364.3206 - val_loss: 2532.6060\n",
      "Epoch 39/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 3312.2258 - val_loss: 2498.8650\n",
      "Epoch 40/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3260.7532 - val_loss: 2465.6282\n",
      "Epoch 41/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3209.8975 - val_loss: 2432.8896\n",
      "Epoch 42/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3159.6545 - val_loss: 2400.6453\n",
      "Epoch 43/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 3110.0173 - val_loss: 2368.8899\n",
      "Epoch 44/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 3060.9819 - val_loss: 2337.6189\n",
      "Epoch 45/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 3012.5430 - val_loss: 2306.8279\n",
      "Epoch 46/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 2964.6946 - val_loss: 2276.5122\n",
      "Epoch 47/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2917.4329 - val_loss: 2246.6675\n",
      "Epoch 48/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2870.7515 - val_loss: 2217.2900\n",
      "Epoch 49/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2824.6465 - val_loss: 2188.3750\n",
      "Epoch 50/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 2779.1128 - val_loss: 2159.9177\n",
      "Epoch 51/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 2734.1453 - val_loss: 2131.9141\n",
      "Epoch 52/500\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 2689.7393 - val_loss: 2104.3606\n",
      "Epoch 53/500\n",
      "18/18 [==============================] - 0s 16ms/step - loss: 2645.8899 - val_loss: 2077.2517\n",
      "Epoch 54/500\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 2602.5925 - val_loss: 2050.5847\n",
      "Epoch 55/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 2559.8425 - val_loss: 2024.3551\n",
      "Epoch 56/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 2517.6355 - val_loss: 1998.5579\n",
      "Epoch 57/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 2475.9658 - val_loss: 1973.1899\n",
      "Epoch 58/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 2434.8298 - val_loss: 1948.2468\n",
      "Epoch 59/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2394.2227 - val_loss: 1923.7241\n",
      "Epoch 60/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2354.1404 - val_loss: 1899.6190\n",
      "Epoch 61/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 2314.5776 - val_loss: 1875.9265\n",
      "Epoch 62/500\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 2275.5305 - val_loss: 1852.6428\n",
      "Epoch 63/500\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 2236.9946 - val_loss: 1829.7645\n",
      "Epoch 64/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 2198.9653 - val_loss: 1807.2876\n",
      "Epoch 65/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 2161.4385 - val_loss: 1785.2075\n",
      "Epoch 66/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 2124.4099 - val_loss: 1763.5211\n",
      "Epoch 67/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2087.8748 - val_loss: 1742.2244\n",
      "Epoch 68/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 2051.8291 - val_loss: 1721.3138\n",
      "Epoch 69/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 2016.2686 - val_loss: 1700.7847\n",
      "Epoch 70/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 1981.1895 - val_loss: 1680.6343\n",
      "Epoch 71/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1946.5873 - val_loss: 1660.8585\n",
      "Epoch 72/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1912.4576 - val_loss: 1641.4537\n",
      "Epoch 73/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1878.7964 - val_loss: 1622.4160\n",
      "Epoch 74/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1845.6002 - val_loss: 1603.7416\n",
      "Epoch 75/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1812.8641 - val_loss: 1585.4268\n",
      "Epoch 76/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1780.5840 - val_loss: 1567.4685\n",
      "Epoch 77/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1748.7566 - val_loss: 1549.8629\n",
      "Epoch 78/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1717.3773 - val_loss: 1532.6060\n",
      "Epoch 79/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1686.4424 - val_loss: 1515.6943\n",
      "Epoch 80/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1655.9482 - val_loss: 1499.1245\n",
      "Epoch 81/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1625.8894 - val_loss: 1482.8928\n",
      "Epoch 82/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1596.2638 - val_loss: 1466.9958\n",
      "Epoch 83/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1567.0662 - val_loss: 1451.4299\n",
      "Epoch 84/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1538.2936 - val_loss: 1436.1918\n",
      "Epoch 85/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1509.9414 - val_loss: 1421.2772\n",
      "Epoch 86/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1482.0061 - val_loss: 1406.6835\n",
      "Epoch 87/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1454.4838 - val_loss: 1392.4067\n",
      "Epoch 88/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1427.3707 - val_loss: 1378.4438\n",
      "Epoch 89/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1400.6636 - val_loss: 1364.7910\n",
      "Epoch 90/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1374.3579 - val_loss: 1351.4451\n",
      "Epoch 91/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1348.4497 - val_loss: 1338.4020\n",
      "Epoch 92/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1322.9355 - val_loss: 1325.6591\n",
      "Epoch 93/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1297.8120 - val_loss: 1313.2129\n",
      "Epoch 94/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1273.0752 - val_loss: 1301.0598\n",
      "Epoch 95/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1248.7216 - val_loss: 1289.1960\n",
      "Epoch 96/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1224.7468 - val_loss: 1277.6188\n",
      "Epoch 97/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1201.1478 - val_loss: 1266.3247\n",
      "Epoch 98/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1177.9204 - val_loss: 1255.3103\n",
      "Epoch 99/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1155.0614 - val_loss: 1244.5720\n",
      "Epoch 100/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1132.5675 - val_loss: 1234.1069\n",
      "Epoch 101/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1110.4344 - val_loss: 1223.9111\n",
      "Epoch 102/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1088.6592 - val_loss: 1213.9824\n",
      "Epoch 103/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1067.2372 - val_loss: 1204.3167\n",
      "Epoch 104/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1046.1656 - val_loss: 1194.9104\n",
      "Epoch 105/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 1025.4409 - val_loss: 1185.7610\n",
      "Epoch 106/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1005.0594 - val_loss: 1176.8647\n",
      "Epoch 107/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 985.0176 - val_loss: 1168.2185\n",
      "Epoch 108/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 965.3118 - val_loss: 1159.8195\n",
      "Epoch 109/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 945.9388 - val_loss: 1151.6639\n",
      "Epoch 110/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 926.8951 - val_loss: 1143.7490\n",
      "Epoch 111/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 908.1773 - val_loss: 1136.0714\n",
      "Epoch 112/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 889.7816 - val_loss: 1128.6279\n",
      "Epoch 113/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 871.7048 - val_loss: 1121.4160\n",
      "Epoch 114/500\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 853.9433 - val_loss: 1114.4325\n",
      "Epoch 115/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 836.4937 - val_loss: 1107.6752\n",
      "Epoch 116/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 819.3525 - val_loss: 1101.1434\n",
      "Epoch 117/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 802.5166 - val_loss: 1094.8326\n",
      "Epoch 118/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 785.9822 - val_loss: 1088.6578\n",
      "Epoch 119/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 770.0643 - val_loss: 1082.9453\n",
      "Epoch 120/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 753.8026 - val_loss: 1077.2633\n",
      "Epoch 121/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 738.1532 - val_loss: 1071.7869\n",
      "Epoch 122/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 722.7921 - val_loss: 1066.5134\n",
      "Epoch 123/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 707.7163 - val_loss: 1061.4388\n",
      "Epoch 124/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 692.9218 - val_loss: 1056.5608\n",
      "Epoch 125/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 678.4059 - val_loss: 1051.8761\n",
      "Epoch 126/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 664.1647 - val_loss: 1047.3815\n",
      "Epoch 127/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 650.1955 - val_loss: 1043.0740\n",
      "Epoch 128/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 636.4947 - val_loss: 1038.9507\n",
      "Epoch 129/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 623.0589 - val_loss: 1035.0083\n",
      "Epoch 130/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 609.8851 - val_loss: 1031.2440\n",
      "Epoch 131/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 596.9702 - val_loss: 1027.6545\n",
      "Epoch 132/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 584.3107 - val_loss: 1024.2372\n",
      "Epoch 133/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 571.9031 - val_loss: 1020.9888\n",
      "Epoch 134/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 559.7451 - val_loss: 1017.9066\n",
      "Epoch 135/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 547.8323 - val_loss: 1014.9872\n",
      "Epoch 136/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 536.1625 - val_loss: 1012.2279\n",
      "Epoch 137/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 524.7324 - val_loss: 1009.6257\n",
      "Epoch 138/500\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 513.5384 - val_loss: 1007.1777\n",
      "Epoch 139/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 502.5777 - val_loss: 1004.8809\n",
      "Epoch 140/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 491.8470 - val_loss: 1002.7325\n",
      "Epoch 141/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 481.3434 - val_loss: 1000.7295\n",
      "Epoch 142/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 471.0637 - val_loss: 998.8688\n",
      "Epoch 143/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 461.0045 - val_loss: 997.1480\n",
      "Epoch 144/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 451.1631 - val_loss: 995.5636\n",
      "Epoch 145/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 441.5362 - val_loss: 994.1134\n",
      "Epoch 146/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 432.1210 - val_loss: 992.7942\n",
      "Epoch 147/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 422.9141 - val_loss: 991.6030\n",
      "Epoch 148/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 413.9128 - val_loss: 990.5375\n",
      "Epoch 149/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 405.1141 - val_loss: 989.5944\n",
      "Epoch 150/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 396.5146 - val_loss: 988.7710\n",
      "Epoch 151/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 388.1117 - val_loss: 988.0648\n",
      "Epoch 152/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 379.9023 - val_loss: 987.4728\n",
      "Epoch 153/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 371.8834 - val_loss: 986.9922\n",
      "Epoch 154/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 364.0522 - val_loss: 986.6205\n",
      "Epoch 155/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 356.4054 - val_loss: 986.3547\n",
      "Epoch 156/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 348.9403 - val_loss: 986.1923\n",
      "Epoch 157/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 341.6540 - val_loss: 986.1306\n",
      "Epoch 158/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 334.5437 - val_loss: 986.1670\n",
      "Epoch 159/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 327.6064 - val_loss: 986.2986\n",
      "Epoch 160/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 320.8396 - val_loss: 986.5229\n",
      "Epoch 161/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 314.2399 - val_loss: 986.8375\n",
      "Epoch 162/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 307.8048 - val_loss: 987.2395\n",
      "Epoch 163/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 301.5316 - val_loss: 987.7263\n",
      "Epoch 164/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 295.4172 - val_loss: 988.2957\n",
      "Epoch 165/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 289.4592 - val_loss: 988.9451\n",
      "Epoch 166/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 283.6546 - val_loss: 989.6719\n",
      "Epoch 167/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 278.0004 - val_loss: 990.4740\n",
      "Epoch 168/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 272.4944 - val_loss: 991.3505\n",
      "Epoch 169/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 267.1338 - val_loss: 992.3093\n",
      "Epoch 170/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 261.9157 - val_loss: 993.3401\n",
      "Epoch 171/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 256.8377 - val_loss: 994.2283\n",
      "Epoch 172/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 251.8970 - val_loss: 995.3708\n",
      "Epoch 173/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 247.0910 - val_loss: 996.5735\n",
      "Epoch 174/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 242.4171 - val_loss: 997.8344\n",
      "Epoch 175/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 237.8727 - val_loss: 999.1507\n",
      "Epoch 176/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 233.4553 - val_loss: 1000.5206\n",
      "Epoch 177/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 229.1624 - val_loss: 1001.9415\n",
      "Epoch 178/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 224.9915 - val_loss: 1003.4114\n",
      "Epoch 179/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 220.9402 - val_loss: 1004.9277\n",
      "Epoch 180/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 217.0059 - val_loss: 1006.4885\n",
      "Epoch 181/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 213.1860 - val_loss: 1008.0917\n",
      "Epoch 182/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 209.4782 - val_loss: 1009.7353\n",
      "Epoch 183/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 205.8803 - val_loss: 1011.4167\n",
      "Epoch 184/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 202.3897 - val_loss: 1013.1344\n",
      "Epoch 185/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 199.0041 - val_loss: 1014.8859\n",
      "Epoch 186/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 195.7212 - val_loss: 1016.6696\n",
      "Epoch 187/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 192.5386 - val_loss: 1018.4830\n",
      "Epoch 188/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 189.4541 - val_loss: 1020.3245\n",
      "Epoch 189/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 186.4656 - val_loss: 1022.1921\n",
      "Epoch 190/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 183.5708 - val_loss: 1024.0841\n",
      "Epoch 191/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 180.7675 - val_loss: 1025.9984\n",
      "Epoch 192/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 178.0533 - val_loss: 1027.9332\n",
      "Epoch 193/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 175.4263 - val_loss: 1029.8868\n",
      "Epoch 194/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 172.8844 - val_loss: 1031.8574\n",
      "Epoch 195/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 170.4254 - val_loss: 1033.8433\n",
      "Epoch 196/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 168.0473 - val_loss: 1035.8430\n",
      "Epoch 197/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 165.7481 - val_loss: 1037.8542\n",
      "Epoch 198/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 163.5258 - val_loss: 1039.8757\n",
      "Epoch 199/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 161.3783 - val_loss: 1041.9062\n",
      "Epoch 200/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 159.3038 - val_loss: 1043.9438\n",
      "Epoch 201/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 157.3001 - val_loss: 1045.9871\n",
      "Epoch 202/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 155.3655 - val_loss: 1048.0345\n",
      "Epoch 203/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 153.4984 - val_loss: 1050.0846\n",
      "Epoch 204/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 151.6967 - val_loss: 1052.1359\n",
      "Epoch 205/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 149.9586 - val_loss: 1054.1870\n",
      "Epoch 206/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 148.2824 - val_loss: 1056.2368\n",
      "Epoch 207/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 146.6663 - val_loss: 1058.2841\n",
      "Epoch 208/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 145.1086 - val_loss: 1060.3273\n",
      "Epoch 209/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 143.6077 - val_loss: 1062.3650\n",
      "Epoch 210/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 142.1618 - val_loss: 1064.3965\n",
      "Epoch 211/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 140.7694 - val_loss: 1066.4203\n",
      "Epoch 212/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 139.4288 - val_loss: 1068.4355\n",
      "Epoch 213/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 138.1385 - val_loss: 1070.4408\n",
      "Epoch 214/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 136.8971 - val_loss: 1072.4352\n",
      "Epoch 215/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 135.7029 - val_loss: 1074.4180\n",
      "Epoch 216/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 134.5546 - val_loss: 1076.3879\n",
      "Epoch 217/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 133.4505 - val_loss: 1078.3444\n",
      "Epoch 218/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 132.3894 - val_loss: 1080.2855\n",
      "Epoch 219/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 131.3700 - val_loss: 1082.2115\n",
      "Epoch 220/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 130.3909 - val_loss: 1084.1211\n",
      "Epoch 221/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 129.4505 - val_loss: 1086.0134\n",
      "Epoch 222/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 128.5478 - val_loss: 1087.8882\n",
      "Epoch 223/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 127.6814 - val_loss: 1089.7441\n",
      "Epoch 224/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 126.8502 - val_loss: 1091.5809\n",
      "Epoch 225/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 126.0530 - val_loss: 1093.3981\n",
      "Epoch 226/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 125.2884 - val_loss: 1095.1945\n",
      "Epoch 227/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 124.5554 - val_loss: 1096.9694\n",
      "Epoch 228/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 123.8531 - val_loss: 1098.7229\n",
      "Epoch 229/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 123.1801 - val_loss: 1100.4537\n",
      "Epoch 230/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 122.5355 - val_loss: 1102.1625\n",
      "Epoch 231/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 121.9183 - val_loss: 1103.8483\n",
      "Epoch 232/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 121.3273 - val_loss: 1105.5099\n",
      "Epoch 233/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 120.7618 - val_loss: 1107.1479\n",
      "Epoch 234/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 120.2207 - val_loss: 1108.7618\n",
      "Epoch 235/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 119.7030 - val_loss: 1110.3512\n",
      "Epoch 236/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 119.2079 - val_loss: 1111.9158\n",
      "Epoch 237/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 118.7345 - val_loss: 1113.4551\n",
      "Epoch 238/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 118.2820 - val_loss: 1114.9694\n",
      "Epoch 239/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 117.8495 - val_loss: 1116.4574\n",
      "Epoch 240/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 117.4366 - val_loss: 1117.9200\n",
      "Epoch 241/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 117.0420 - val_loss: 1119.3572\n",
      "Epoch 242/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 116.6651 - val_loss: 1120.7681\n",
      "Epoch 243/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 116.3054 - val_loss: 1122.1531\n",
      "Epoch 244/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 115.9619 - val_loss: 1123.5123\n",
      "Epoch 245/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 115.6341 - val_loss: 1124.8447\n",
      "Epoch 246/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 115.3215 - val_loss: 1126.1514\n",
      "Epoch 247/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 115.0232 - val_loss: 1127.4316\n",
      "Epoch 248/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 114.7387 - val_loss: 1128.6859\n",
      "Epoch 249/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 114.4675 - val_loss: 1129.9146\n",
      "Epoch 250/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 114.2088 - val_loss: 1131.1161\n",
      "Epoch 251/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 113.9623 - val_loss: 1132.2926\n",
      "Epoch 252/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 113.7275 - val_loss: 1133.4434\n",
      "Epoch 253/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 113.5036 - val_loss: 1134.5681\n",
      "Epoch 254/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 113.2903 - val_loss: 1135.6674\n",
      "Epoch 255/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 113.0872 - val_loss: 1136.7412\n",
      "Epoch 256/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 112.8937 - val_loss: 1137.7899\n",
      "Epoch 257/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 112.7095 - val_loss: 1138.8141\n",
      "Epoch 258/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 112.5341 - val_loss: 1139.8131\n",
      "Epoch 259/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 112.3672 - val_loss: 1140.7877\n",
      "Epoch 260/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 112.2083 - val_loss: 1141.7380\n",
      "Epoch 261/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 112.0570 - val_loss: 1142.6641\n",
      "Epoch 262/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 111.9131 - val_loss: 1143.5670\n",
      "Epoch 263/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 111.7761 - val_loss: 1144.4459\n",
      "Epoch 264/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 111.6458 - val_loss: 1145.3020\n",
      "Epoch 265/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 111.5219 - val_loss: 1146.1348\n",
      "Epoch 266/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 111.4039 - val_loss: 1146.9456\n",
      "Epoch 267/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 111.2918 - val_loss: 1147.7340\n",
      "Epoch 268/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 111.1851 - val_loss: 1148.5002\n",
      "Epoch 269/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 111.0836 - val_loss: 1149.2452\n",
      "Epoch 270/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 110.9872 - val_loss: 1149.9690\n",
      "Epoch 271/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 110.8954 - val_loss: 1150.6714\n",
      "Epoch 272/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 110.8082 - val_loss: 1151.3535\n",
      "Epoch 273/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 110.7253 - val_loss: 1152.0156\n",
      "Epoch 274/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 110.6464 - val_loss: 1152.6576\n",
      "Epoch 275/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 110.5714 - val_loss: 1153.2806\n",
      "Epoch 276/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 110.5002 - val_loss: 1153.8845\n",
      "Epoch 277/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 110.4324 - val_loss: 1154.4698\n",
      "Epoch 278/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 110.3680 - val_loss: 1155.0366\n",
      "Epoch 279/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 110.3067 - val_loss: 1155.5854\n",
      "Epoch 280/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 110.2485 - val_loss: 1156.1169\n",
      "Epoch 281/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 110.1932 - val_loss: 1156.6312\n",
      "Epoch 282/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 110.1406 - val_loss: 1157.1288\n",
      "Epoch 283/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 110.0906 - val_loss: 1157.6100\n",
      "Epoch 284/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 110.0430 - val_loss: 1158.0750\n",
      "Epoch 285/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.9978 - val_loss: 1158.5248\n",
      "Epoch 286/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.9548 - val_loss: 1158.9590\n",
      "Epoch 287/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 109.9140 - val_loss: 1159.3788\n",
      "Epoch 288/500\n",
      "18/18 [==============================] - 0s 17ms/step - loss: 109.8751 - val_loss: 1159.7831\n",
      "Epoch 289/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 109.8382 - val_loss: 1160.1746\n",
      "Epoch 290/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 109.8030 - val_loss: 1160.5509\n",
      "Epoch 291/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 109.7696 - val_loss: 1160.9149\n",
      "Epoch 292/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 109.7379 - val_loss: 1161.2653\n",
      "Epoch 293/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.7078 - val_loss: 1161.6027\n",
      "Epoch 294/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.6791 - val_loss: 1161.9285\n",
      "Epoch 295/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 109.6518 - val_loss: 1162.2417\n",
      "Epoch 296/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 109.6259 - val_loss: 1162.5430\n",
      "Epoch 297/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 109.6014 - val_loss: 1162.8335\n",
      "Epoch 298/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 109.5779 - val_loss: 1163.1129\n",
      "Epoch 299/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 109.5556 - val_loss: 1163.3815\n",
      "Epoch 300/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.5345 - val_loss: 1163.6394\n",
      "Epoch 301/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.5144 - val_loss: 1163.8876\n",
      "Epoch 302/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.4953 - val_loss: 1164.1261\n",
      "Epoch 303/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 109.4771 - val_loss: 1164.3551\n",
      "Epoch 304/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 109.4598 - val_loss: 1164.5752\n",
      "Epoch 305/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.4434 - val_loss: 1164.7861\n",
      "Epoch 306/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.4278 - val_loss: 1164.9889\n",
      "Epoch 307/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.4130 - val_loss: 1165.1831\n",
      "Epoch 308/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.3989 - val_loss: 1165.3701\n",
      "Epoch 309/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 109.3854 - val_loss: 1165.5482\n",
      "Epoch 310/500\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 109.3727 - val_loss: 1165.7194\n",
      "Epoch 311/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.3607 - val_loss: 1165.8839\n",
      "Epoch 312/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.3492 - val_loss: 1166.0413\n",
      "Epoch 313/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.3383 - val_loss: 1166.1919\n",
      "Epoch 314/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.3279 - val_loss: 1166.3362\n",
      "Epoch 315/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.3180 - val_loss: 1166.4741\n",
      "Epoch 316/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.3086 - val_loss: 1166.6061\n",
      "Epoch 317/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.2998 - val_loss: 1166.7323\n",
      "Epoch 318/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.2913 - val_loss: 1166.8527\n",
      "Epoch 319/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.2833 - val_loss: 1166.9674\n",
      "Epoch 320/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.2758 - val_loss: 1167.0776\n",
      "Epoch 321/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.2686 - val_loss: 1167.1830\n",
      "Epoch 322/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.2617 - val_loss: 1167.2839\n",
      "Epoch 323/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.2552 - val_loss: 1167.3793\n",
      "Epoch 324/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.2491 - val_loss: 1167.4714\n",
      "Epoch 325/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.2432 - val_loss: 1167.5590\n",
      "Epoch 326/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.2377 - val_loss: 1167.6422\n",
      "Epoch 327/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.2324 - val_loss: 1167.7208\n",
      "Epoch 328/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.2275 - val_loss: 1167.7970\n",
      "Epoch 329/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.2227 - val_loss: 1167.8688\n",
      "Epoch 330/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.2183 - val_loss: 1167.9376\n",
      "Epoch 331/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.2141 - val_loss: 1168.0029\n",
      "Epoch 332/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.2101 - val_loss: 1168.0653\n",
      "Epoch 333/500\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 109.2063 - val_loss: 1168.1244\n",
      "Epoch 334/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.2028 - val_loss: 1168.1804\n",
      "Epoch 335/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 109.1994 - val_loss: 1168.2339\n",
      "Epoch 336/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1962 - val_loss: 1168.2849\n",
      "Epoch 337/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1932 - val_loss: 1168.3339\n",
      "Epoch 338/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1903 - val_loss: 1168.3795\n",
      "Epoch 339/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1876 - val_loss: 1168.4237\n",
      "Epoch 340/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1851 - val_loss: 1168.4655\n",
      "Epoch 341/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1827 - val_loss: 1168.5051\n",
      "Epoch 342/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1804 - val_loss: 1168.5427\n",
      "Epoch 343/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1783 - val_loss: 1168.5784\n",
      "Epoch 344/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1763 - val_loss: 1168.6123\n",
      "Epoch 345/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1744 - val_loss: 1168.6445\n",
      "Epoch 346/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1726 - val_loss: 1168.6749\n",
      "Epoch 347/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.1710 - val_loss: 1168.7036\n",
      "Epoch 348/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1693 - val_loss: 1168.7311\n",
      "Epoch 349/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1679 - val_loss: 1168.7576\n",
      "Epoch 350/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1665 - val_loss: 1168.7819\n",
      "Epoch 351/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1652 - val_loss: 1168.8055\n",
      "Epoch 352/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1639 - val_loss: 1168.8275\n",
      "Epoch 353/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1628 - val_loss: 1168.8484\n",
      "Epoch 354/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1617 - val_loss: 1168.8684\n",
      "Epoch 355/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1607 - val_loss: 1168.8870\n",
      "Epoch 356/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1598 - val_loss: 1168.9053\n",
      "Epoch 357/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1589 - val_loss: 1168.9220\n",
      "Epoch 358/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 109.1581 - val_loss: 1168.9380\n",
      "Epoch 359/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1573 - val_loss: 1168.9529\n",
      "Epoch 360/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1566 - val_loss: 1168.9672\n",
      "Epoch 361/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1559 - val_loss: 1168.9808\n",
      "Epoch 362/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1554 - val_loss: 1168.9933\n",
      "Epoch 363/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1548 - val_loss: 1169.0059\n",
      "Epoch 364/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1542 - val_loss: 1169.0170\n",
      "Epoch 365/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1537 - val_loss: 1169.0276\n",
      "Epoch 366/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1533 - val_loss: 1169.0377\n",
      "Epoch 367/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1529 - val_loss: 1169.0476\n",
      "Epoch 368/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1525 - val_loss: 1169.0560\n",
      "Epoch 369/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1521 - val_loss: 1169.0649\n",
      "Epoch 370/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1518 - val_loss: 1169.0728\n",
      "Epoch 371/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1515 - val_loss: 1169.0804\n",
      "Epoch 372/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1513 - val_loss: 1169.0878\n",
      "Epoch 373/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1511 - val_loss: 1169.0941\n",
      "Epoch 374/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1508 - val_loss: 1169.1006\n",
      "Epoch 375/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1506 - val_loss: 1169.1067\n",
      "Epoch 376/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1505 - val_loss: 1169.1123\n",
      "Epoch 377/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1503 - val_loss: 1169.1177\n",
      "Epoch 378/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1502 - val_loss: 1169.1228\n",
      "Epoch 379/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1501 - val_loss: 1169.1273\n",
      "Epoch 380/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1500 - val_loss: 1169.1320\n",
      "Epoch 381/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1499 - val_loss: 1169.1366\n",
      "Epoch 382/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 109.1498 - val_loss: 1169.1406\n",
      "Epoch 383/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1497 - val_loss: 1169.1444\n",
      "Epoch 384/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 109.1496 - val_loss: 1169.1477\n",
      "Epoch 385/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1496 - val_loss: 1169.1515\n",
      "Epoch 386/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1496 - val_loss: 1169.1549\n",
      "Epoch 387/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1496 - val_loss: 1169.1576\n",
      "Epoch 388/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1496 - val_loss: 1169.1605\n",
      "Epoch 389/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1496 - val_loss: 1169.1633\n",
      "Epoch 390/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1496 - val_loss: 1169.1660\n",
      "Epoch 391/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1495 - val_loss: 1169.1677\n",
      "Epoch 392/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1496 - val_loss: 1169.1699\n",
      "Epoch 393/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1497 - val_loss: 1169.1724\n",
      "Epoch 394/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1497 - val_loss: 1169.1742\n",
      "Epoch 395/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1497 - val_loss: 1169.1757\n",
      "Epoch 396/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1498 - val_loss: 1169.1775\n",
      "Epoch 397/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1498 - val_loss: 1169.1793\n",
      "Epoch 398/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1499 - val_loss: 1169.1810\n",
      "Epoch 399/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1499 - val_loss: 1169.1826\n",
      "Epoch 400/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1500 - val_loss: 1169.1837\n",
      "Epoch 401/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1501 - val_loss: 1169.1846\n",
      "Epoch 402/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1501 - val_loss: 1169.1863\n",
      "Epoch 403/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 109.1503 - val_loss: 1169.1875\n",
      "Epoch 404/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1503 - val_loss: 1169.1887\n",
      "Epoch 405/500\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 109.1504 - val_loss: 1169.1899\n",
      "Epoch 406/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1504 - val_loss: 1169.1904\n",
      "Epoch 407/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1504 - val_loss: 1169.1910\n",
      "Epoch 408/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1506 - val_loss: 1169.1920\n",
      "Epoch 409/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1507 - val_loss: 1169.1934\n",
      "Epoch 410/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1508 - val_loss: 1169.1946\n",
      "Epoch 411/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1508 - val_loss: 1169.1951\n",
      "Epoch 412/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1509 - val_loss: 1169.1957\n",
      "Epoch 413/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1510 - val_loss: 1169.1967\n",
      "Epoch 414/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1511 - val_loss: 1169.1969\n",
      "Epoch 415/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1512 - val_loss: 1169.1978\n",
      "Epoch 416/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1512 - val_loss: 1169.1981\n",
      "Epoch 417/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1514 - val_loss: 1169.1986\n",
      "Epoch 418/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1515 - val_loss: 1169.1995\n",
      "Epoch 419/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1515 - val_loss: 1169.2000\n",
      "Epoch 420/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1516 - val_loss: 1169.2003\n",
      "Epoch 421/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1517 - val_loss: 1169.2007\n",
      "Epoch 422/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1518 - val_loss: 1169.2009\n",
      "Epoch 423/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1519 - val_loss: 1169.2015\n",
      "Epoch 424/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1520 - val_loss: 1169.2020\n",
      "Epoch 425/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1521 - val_loss: 1169.2025\n",
      "Epoch 426/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 109.1522 - val_loss: 1169.2025\n",
      "Epoch 427/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1522 - val_loss: 1169.2026\n",
      "Epoch 428/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1523 - val_loss: 1169.2031\n",
      "Epoch 429/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1524 - val_loss: 1169.2034\n",
      "Epoch 430/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1525 - val_loss: 1169.2036\n",
      "Epoch 431/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1526 - val_loss: 1169.2040\n",
      "Epoch 432/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1526 - val_loss: 1169.2047\n",
      "Epoch 433/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1527 - val_loss: 1169.2047\n",
      "Epoch 434/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1528 - val_loss: 1169.2048\n",
      "Epoch 435/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1529 - val_loss: 1169.2048\n",
      "Epoch 436/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1530 - val_loss: 1169.2053\n",
      "Epoch 437/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1531 - val_loss: 1169.2056\n",
      "Epoch 438/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1532 - val_loss: 1169.2057\n",
      "Epoch 439/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1532 - val_loss: 1169.2058\n",
      "Epoch 440/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1533 - val_loss: 1169.2058\n",
      "Epoch 441/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1534 - val_loss: 1169.2061\n",
      "Epoch 442/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1534 - val_loss: 1169.2062\n",
      "Epoch 443/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1535 - val_loss: 1169.2062\n",
      "Epoch 444/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1536 - val_loss: 1169.2064\n",
      "Epoch 445/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1537 - val_loss: 1169.2068\n",
      "Epoch 446/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1538 - val_loss: 1169.2072\n",
      "Epoch 447/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1538 - val_loss: 1169.2073\n",
      "Epoch 448/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1538 - val_loss: 1169.2073\n",
      "Epoch 449/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.1539 - val_loss: 1169.2072\n",
      "Epoch 450/500\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 109.1540 - val_loss: 1169.2074\n",
      "Epoch 451/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1541 - val_loss: 1169.2074\n",
      "Epoch 452/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1541 - val_loss: 1169.2074\n",
      "Epoch 453/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1542 - val_loss: 1169.2076\n",
      "Epoch 454/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1543 - val_loss: 1169.2083\n",
      "Epoch 455/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1543 - val_loss: 1169.2084\n",
      "Epoch 456/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1544 - val_loss: 1169.2083\n",
      "Epoch 457/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1545 - val_loss: 1169.2085\n",
      "Epoch 458/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1545 - val_loss: 1169.2085\n",
      "Epoch 459/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1545 - val_loss: 1169.2085\n",
      "Epoch 460/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1546 - val_loss: 1169.2084\n",
      "Epoch 461/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1547 - val_loss: 1169.2084\n",
      "Epoch 462/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1547 - val_loss: 1169.2085\n",
      "Epoch 463/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1548 - val_loss: 1169.2085\n",
      "Epoch 464/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1548 - val_loss: 1169.2087\n",
      "Epoch 465/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1549 - val_loss: 1169.2085\n",
      "Epoch 466/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1550 - val_loss: 1169.2089\n",
      "Epoch 467/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1550 - val_loss: 1169.2090\n",
      "Epoch 468/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1551 - val_loss: 1169.2091\n",
      "Epoch 469/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1551 - val_loss: 1169.2096\n",
      "Epoch 470/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1551 - val_loss: 1169.2096\n",
      "Epoch 471/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 109.1552 - val_loss: 1169.2101\n",
      "Epoch 472/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1552 - val_loss: 1169.2096\n",
      "Epoch 473/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1553 - val_loss: 1169.2098\n",
      "Epoch 474/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1554 - val_loss: 1169.2100\n",
      "Epoch 475/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1554 - val_loss: 1169.2094\n",
      "Epoch 476/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1555 - val_loss: 1169.2092\n",
      "Epoch 477/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1556 - val_loss: 1169.2096\n",
      "Epoch 478/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1555 - val_loss: 1169.2097\n",
      "Epoch 479/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1556 - val_loss: 1169.2098\n",
      "Epoch 480/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1556 - val_loss: 1169.2101\n",
      "Epoch 481/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1556 - val_loss: 1169.2101\n",
      "Epoch 482/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1557 - val_loss: 1169.2101\n",
      "Epoch 483/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1558 - val_loss: 1169.2103\n",
      "Epoch 484/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1558 - val_loss: 1169.2104\n",
      "Epoch 485/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1558 - val_loss: 1169.2103\n",
      "Epoch 486/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1558 - val_loss: 1169.2101\n",
      "Epoch 487/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1559 - val_loss: 1169.2103\n",
      "Epoch 488/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1560 - val_loss: 1169.2107\n",
      "Epoch 489/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1560 - val_loss: 1169.2107\n",
      "Epoch 490/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1560 - val_loss: 1169.2107\n",
      "Epoch 491/500\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 109.1561 - val_loss: 1169.2109\n",
      "Epoch 492/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1561 - val_loss: 1169.2109\n",
      "Epoch 493/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 109.1561 - val_loss: 1169.2109\n",
      "Epoch 494/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1562 - val_loss: 1169.2109\n",
      "Epoch 495/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1562 - val_loss: 1169.2107\n",
      "Epoch 496/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1562 - val_loss: 1169.2104\n",
      "Epoch 497/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1563 - val_loss: 1169.2104\n",
      "Epoch 498/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1563 - val_loss: 1169.2104\n",
      "Epoch 499/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.1564 - val_loss: 1169.2104\n",
      "Epoch 500/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.1564 - val_loss: 1169.2104\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.0403, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0012, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0120, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.0334, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 412ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.52197992e+01, 7.51408077e+01, 7.50618161e+01, 7.49828245e+01,\n",
       "        7.49038329e+01, 7.48248413e+01, 7.47458497e+01, 2.17284888e-01,\n",
       "        0.00000000e+00, 3.00090790e-01, 0.00000000e+00, 1.82285339e-01,\n",
       "        0.00000000e+00, 7.58815126e+01, 7.58142857e+01, 7.57470588e+01,\n",
       "        7.56798319e+01, 7.56126050e+01, 7.55453782e+01, 7.54743277e+01,\n",
       "        7.53953361e+01, 7.53163445e+01, 7.52373529e+01, 7.51583613e+01,\n",
       "        7.50793697e+01, 7.50003782e+01, 7.49213865e+01, 7.48423950e+01,\n",
       "        7.47634034e+01, 7.46844118e+01, 7.46054202e+01, 7.45507143e+01,\n",
       "        7.45288656e+01, 7.45070168e+01, 7.44851681e+01, 7.44633193e+01,\n",
       "        7.44414706e+01, 7.44196218e+01, 7.43977731e+01, 7.43759244e+01,\n",
       "        4.13527070e-01, 5.16547800e-01, 0.00000000e+00, 3.00692470e-01,\n",
       "        0.00000000e+00, 2.42449280e-01, 0.00000000e+00, 7.50179318e+01,\n",
       "        7.49389402e+01, 7.48599487e+01, 7.47809570e+01, 7.47019655e+01,\n",
       "        7.46229739e+01, 7.45555696e+01, 7.45337208e+01, 7.45118721e+01,\n",
       "        7.44900233e+01, 7.44681746e+01, 7.44463259e+01, 7.44244771e+01,\n",
       "        7.44026284e+01, 7.43807796e+01, 7.81594538e+01, 7.79325630e+01,\n",
       "        7.75545285e+01, 7.71511671e+01, 7.67478058e+01, 7.63444444e+01,\n",
       "        7.61205415e+01, 7.59188609e+01, 7.57171802e+01, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.10170059e+01, 6.24775290e-01, 4.78058785e-01,\n",
       "        5.81867754e-01, 5.50841987e-01, 0.00000000e+00, 9.38433111e-01,\n",
       "        7.10341339e+01, 4.87217605e-01, 4.82364625e-01, 0.00000000e+00,\n",
       "        5.77322319e-02, 3.95282090e-01, 4.21792805e-01, 0.00000000e+00,\n",
       "        3.34464639e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.34997177e-01, 7.82079101e-01, 0.00000000e+00, 9.17057395e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.05774283e-01, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([70.91447246, 70.9070028 , 70.89953315, 70.89206349, 70.88459384,\n",
       "       70.87712418, 70.86965453, 70.86218487, 70.85471522, 70.84724556,\n",
       "       70.83977591, 70.83230626, 70.8248366 , 70.81736695, 70.80989729,\n",
       "       70.80242764, 70.79495798, 70.78748833, 70.78001867, 70.77254902,\n",
       "       70.76507937, 70.75760971, 70.75014006, 70.7426704 , 70.73520075,\n",
       "       70.72773109, 70.72026144, 70.71279178, 70.70532213, 70.69785247,\n",
       "       70.69038282, 70.68291317, 70.67544351, 70.66797386, 70.6605042 ,\n",
       "       70.65303455, 70.64556489, 70.63809524, 70.63062558, 70.62315593,\n",
       "       70.61568627, 70.60821662, 70.60074697, 70.59327731, 70.58580766,\n",
       "       70.578338  , 70.57086835, 70.56339869, 70.55592904, 70.54845938,\n",
       "       70.54098973, 70.53352007, 70.52605042, 70.51858077, 70.51111111,\n",
       "       70.50364146, 70.4961718 , 70.48870215, 70.48123249, 70.47376284,\n",
       "       70.46629318, 70.45882353, 70.45135387, 70.44388422, 70.43641457,\n",
       "       70.42894491, 70.42147526, 70.4140056 , 70.40653595, 70.39935808,\n",
       "       70.39422269, 70.3890873 , 70.38395191, 70.37881653, 70.37368114,\n",
       "       70.36854575, 70.36341036, 70.35827498, 70.35313959, 70.3480042 ,\n",
       "       70.34286881, 70.33773343, 70.33259804, 70.32746265, 70.32232726,\n",
       "       70.31719188, 70.31205649, 70.3069211 , 70.30178571, 70.29665033,\n",
       "       70.29151494, 70.28637955, 70.28124416, 70.27610878, 70.27097339,\n",
       "       70.265838  , 70.26070261, 70.25556723, 70.25043184, 70.24529645])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.5447472635181\n",
      "30.731173479003942\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
