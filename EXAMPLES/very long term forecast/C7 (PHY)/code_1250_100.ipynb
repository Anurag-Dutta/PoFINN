{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       92.600000\n",
       "1       92.387115\n",
       "2       92.174230\n",
       "3       91.961345\n",
       "4       91.748459\n",
       "          ...    \n",
       "1345    71.150817\n",
       "1346    71.144281\n",
       "1347    71.137745\n",
       "1348    71.131209\n",
       "1349    71.124673\n",
       "Name: C7, Length: 1350, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c7_interpolated_1250_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       92.600000\n",
       "1       92.387115\n",
       "2       92.174230\n",
       "3       91.961345\n",
       "4       91.748459\n",
       "          ...    \n",
       "1245    71.713887\n",
       "1246    71.710245\n",
       "1247    71.706604\n",
       "1248    71.702962\n",
       "1249    71.699321\n",
       "Name: C7, Length: 1250, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1250)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92.387115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92.174230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.961345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91.748459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     92.600000  0.000298\n",
       "1     92.387115  0.000298\n",
       "2     92.174230  0.000297\n",
       "3     91.961345  0.000297\n",
       "4     91.748459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAAsTAAALEwEAmpwYAAAoBklEQVR4nO3deXxU9b3/8dcnCUkgkAAhbAlLWFQQWSMgFsS1aBVqqxa1Lq3V1qXa1t5Wb9tba2/vrb2tK/anVK1Uq6i0ViruQl1QkKCAsknYwxrCvoYkn98fc6BjDLJkkjOTeT8fjzw453u+k/mcGZg35/s9c465OyIikrxSwi5ARETCpSAQEUlyCgIRkSSnIBARSXIKAhGRJJcWdgHHok2bNt61a9ewyxARSSizZ8/e5O55NdsTMgi6du1KcXFx2GWIiCQUM1tZW3tMhobMbJSZLTazEjO7rZbtI8zsQzOrNLOLatmebWalZjYuFvWIiMiRq3MQmFkq8CBwLtAbuNTMetfotgq4GnjqEL/m18Dbda1FRESOXiyOCAYDJe6+zN0rgInAmOgO7r7C3ecB1TUfbGaDgHbAazGoRUREjlIsgiAfWB21Xhq0HZaZpQB/AH58BH2vM7NiMysuKys7pkJFROTzwj599AbgJXcvPVxHdx/v7kXuXpSX97lJbxEROUaxOGtoDdApar0gaDsSpwDDzewGoDmQbmY73f1zE84iIlI/YhEEs4CeZlZIJADGApcdyQPd/fIDy2Z2NVCkEBARaVh1Hhpy90rgJuBVYCHwrLvPN7M7zWw0gJmdbGalwMXAw2Y2v67PeywmvLeCyXPXhvHUIiJxKyZfKHP3l4CXarT9V9TyLCJDRl/0Ox4HHo9FPYcycdZqOuZkMrpfx/p8GhGRhBL2ZHGD6piTybpte8MuQ0QkriRVELTPyWTdtj1hlyEiEleSKgg65GSyZfd+9u6vCrsUEZG4kVRB0D6nKQDrNTwkInJQUgVBx5xMAM0TiIhESaogaH8wCDRPICJyQJIGgY4IREQOSKogaJaeRk7TJpojEBGJklRBAJEzh3REICLyb0kZBOu3a45AROSApAuC9jlNWbdVRwQiIgckXRB0yMmkfFeFvlQmIhJIyiAA2Lh9X8iViIjEhyQMgsi3i/VdAhGRiKQLggPfJVi/XfMEIiKQhEFwYGhorSaMRUSAGAWBmY0ys8VmVmJmn7vVpJmNMLMPzazSzC6Kau9vZu+b2Xwzm2dm34hFPV8kKyON7Mw01mtoSEQEiEEQmFkq8CBwLtAbuNTMetfotgq4GniqRvtu4Ep3PxEYBdxrZi3rWtPhdMhpqi+ViYgEYnGrysFAibsvAzCzicAYYMGBDu6+IthWHf1Ad/80anmtmW0E8oCtMajrkNrnZGqOQEQkEIuhoXxgddR6adB2VMxsMJAOLI1BTV+oa24zlm7cSVW11/dTiYjEvbiYLDazDsATwLfcvfoQfa4zs2IzKy4rK6vT8/Xr1JJdFVWUbNxZp98jItIYxCII1gCdotYLgrYjYmbZwBTgZ+4+41D93H28uxe5e1FeXt4xFwvQv1NLAOas3lKn3yMi0hjEIghmAT3NrNDM0oGxwOQjeWDQ/3ngL+4+KQa1HJHCNllkZ6YxZ/XWhnpKEZG4VecgcPdK4CbgVWAh8Ky7zzezO81sNICZnWxmpcDFwMNmNj94+CXACOBqM5sT/PSva02HY2b069SSj1Ztre+nEhGJe7E4awh3fwl4qUbbf0UtzyIyZFTzcU8CT8aihqM1oFNLxk0rYde+SrIyYvIyiIgkpLiYLA5D/84tqXb4eM22sEsREQlV0gZBv4KWAMzVPIGIJLmkDYLc5hl0bt1ME8YikvSSNgggchqpgkBEkl3SB8G6bXvZoMtNiEgSS+4g6NwSQKeRikhSS+og6N0hmyappuEhEUlqSR0EmU1S6dUhW5eaEJGkltRBAJF5go9Lt+lKpCKStBQEwZVIl2zcEXYpIiKhUBAcuBKpJoxFJEklfRAUtskip2kTTRiLSNJK+iA4cCVSBYGIJKukDwKIDA99umEHu/ZVhl2KiEiDUxAAA4MrkU6Zty7sUkREGpyCABjeM4/BXVvzq3/OZ8WmXWGXIyLSoBQEQGqKcc/Y/qSlpnDzxI+oqKwOuyQRkQYTkyAws1FmttjMSszstlq2jzCzD82s0swuqrHtKjNbEvxcFYt6jkV+y6bc9fWTmFe6jT+8tjisMkREGlydg8DMUoEHgXOB3sClZta7RrdVwNXAUzUe2xr4JTAEGAz80sxa1bWmYzWqTwcuG9KZh99extufloVVhohIg4rFEcFgoMTdl7l7BTARGBPdwd1XuPs8oOaYy5eB1919s7tvAV4HRsWgpmP2i6/0pmfb5vzo2bls2rkvzFJERBpELIIgH1gdtV4atMX0sWZ2nZkVm1lxWVn9/W+9aXoqD1w2gO1793Prs3Op1jWIRKSRS5jJYncf7+5F7l6Ul5dXr891Qvtsfv6VXrz1aRmPTV9er88lIhK2WATBGqBT1HpB0Fbfj61XVwztwlm92nHXK4v4ZM22sMsREak3sQiCWUBPMys0s3RgLDD5CB/7KnCOmbUKJonPCdpCZ2b830V9yc3K4OanP9K3jkWk0apzELh7JXATkQ/whcCz7j7fzO40s9EAZnaymZUCFwMPm9n84LGbgV8TCZNZwJ1BW1xolZXOPd/oz/LyXdwxeX7Y5YiI1AtzT7zJ0KKiIi8uLm6w5/v9q4sZN62E+y8dwOh+HRvseUVEYsnMZrt7Uc32hJksDtMtZ/VkYOeW/GTSXMZNXcLe/VVhlyQiEjMKgiPQJDWFh745iJHHteX3r33KmX94iynz1pGIR1MiIjUpCI5Q2+xMHrpiEE9dO4QWmWnc+NSHfGP8DJ1RJCIJT0FwlIZ1b8OUm4fzmwv7ULJxJxeMe5fb/jaPsh36FrKIJCYFwTFITTEuH9KFaT8eyTWnFjJpdimn//5fPPzWUl25VEQSjoKgDnKaNuHn5/fm1R+OYHBha/735UX8+Lm5YZclInJUFAQx0D2vOY9dfTI/OKsnk+eu5Z9z14ZdkojIEVMQxNBNp/egX6eW/OKFT9i4Y2/Y5YiIHBEFQQylpabwh4v7saeiiv/8+8c6vVREEoKCIMZ6tG3Of3z5eN5YuJG/fRgX188TEflCCoJ68O1TCxlc2JpfTZ7P2q17wi5HROQLKQjqQUqK8fuL+lHlzk//Nk9DRCIS1xQE9aRzbjNuP68X7yzZxFMfrAq7HBGRQ1IQ1KNvDunM8J5t+M2Uhawq3x12OSIitVIQ1CMz466v9yXVjB9P0v2PRSQ+KQjqWceWTfmvC3rzwfLN/Pm9FWGXIyLyOQqCBnDRoALOPKEtv3tlEUvLdoZdjojIZ8QkCMxslJktNrMSM7utlu0ZZvZMsH2mmXUN2puY2QQz+9jMFprZ7bGoJ96YGf/7tZPIbJLKrc/OpbJKF6YTkfhR5yAws1TgQeBcoDdwqZn1rtHtGmCLu/cA7gHuCtovBjLc/SRgEPDdAyHR2LTNzuTXX+3DnNVbufRPM1ixaVfYJYmIALE5IhgMlLj7MnevACYCY2r0GQNMCJYnAWeamQEOZJlZGtAUqAC2x6CmuDS6X0fuvqQfi9bvYNR9b/P49OWaQBaR0MUiCPKB1VHrpUFbrX3cvRLYBuQSCYVdwDpgFfB7d99c25OY2XVmVmxmxWVlZTEoOxxfG1jA6z88jaHdcrnjnwu49E8zdGqpiIQq7MniwUAV0BEoBG41s261dXT38e5e5O5FeXl5DVljzLXPyeTPV5/M777elwVrtzPqvrd5YsZKHR2ISChiEQRrgE5R6wVBW619gmGgHKAcuAx4xd33u/tGYDpQFIOa4p6ZccnJnXjlhyMY1KUVv/jHJ1zx2ExKt+joQEQaViyCYBbQ08wKzSwdGAtMrtFnMnBVsHwRMNUjF+BZBZwBYGZZwFBgUQxqShj5LZvyl28P5n8uPIk5q7Yy6t53ePqDVbo+kYg0mDoHQTDmfxPwKrAQeNbd55vZnWY2Ouj2KJBrZiXAj4ADp5g+CDQ3s/lEAuXP7j6vrjUlGjPjsiGdeeUHI+hbkMPtf/+Yq/48S1cuFZEGYYn4P8+ioiIvLi4Ou4x6UV3t/HXmSv7npUWkpRi/uKA3Fw8qIHKSlYjIsTOz2e7+ueH3sCeLpYaUFOOKU7ry6g9G0KtjNj+ZNI9vPz6LDdt160sRqR8KgjjVObcZE68dyi8v6M37y8o5++63+PuHpZo7EJGYUxDEsZQU41unFvLyLSM4rl0LfvTsXK79y2w27tDRgYjEjoIgARS2yeKZ757Cz7/Si3eWlHHOPW/zwpw1OjoQkZhQECSI1BTjO8O7MeXm4XTNzeKWiXP49uOzdM0iEakzBUGC6dG2OX+7fhg//0ovZq3Ywjn3vM3dry1mT0VV2KWJSIJSECSgA0cHU289jfNOas/9U0s46+63eG3+eg0XichRUxAksLbZmdw7dgATrxtKVkYq1z0xm28/PouV5RouEpEjpyBoBIZ2y2XKzcMPDhedfc/b3P36p+zdr+EiETk8BUEj0SQ1he8M78abt57GuX3ac/+bSzjr7rd4fcEGDReJyBdSEDQy7bIzuW/sAJ6+dihNm6Ry7V+KuWZCsYaLROSQFASN1Cndc3nplshw0cxl5RouEpFDUhA0YgeGi6b+eCSjTowMF519z1u8sWBD2KWJSBxRECSBdtmZ3H/pAJ66dggZaal85y/FXPP4LN0iU0QABUFSGda9DS/fMpyfndeLGcvKOeuet7hHw0UiSU9BkGSapKZw7YhuvHnrSL58YnvuC4aL3lyo4SKRZKUgSFLtczJ5IGq46JoJxXxngoaLRJJRTILAzEaZ2WIzKzGz22rZnmFmzwTbZ5pZ16htfc3sfTObb2Yfm1lmLGqSIzOsexteunk4/3neCby3NDJcdO8bGi4SSSZ1DgIzSyVy7+Fzgd7ApWbWu0a3a4At7t4DuAe4K3hsGvAk8D13PxEYCeyva01ydNLTUrhuRHfevPU0zundjnvfWMI597yt4SKRJBGLI4LBQIm7L3P3CmAiMKZGnzHAhGB5EnCmRW7Cew4wz93nArh7ubvrv6Ih6ZDTlHGXDeSv3xlCk1Q7OFxUukXDRSKNWSyCIB9YHbVeGrTV2sfdK4FtQC5wHOBm9qqZfWhmPznUk5jZdWZWbGbFZWVlMShbDuXUHm14+ZYR3H5uZLho9LjpzF65JeyyRKSehD1ZnAZ8Cbg8+PNCMzuzto7uPt7di9y9KC8vryFrTErpaSl897TuTLl5ONmZaVz2pxm8/PG6sMsSkXoQiyBYA3SKWi8I2mrtE8wL5ADlRI4e3nb3Te6+G3gJGBiDmiRGCttk8bfrh3Fix2xueOpDHnlnmS5iJ9LIxCIIZgE9zazQzNKBscDkGn0mA1cFyxcBUz3yafIqcJKZNQsC4jRgQQxqkhjKbZ7BU9cO5dw+7fnvKQu5Y/J8qqoVBiKNRZ2DIBjzv4nIh/pC4Fl3n29md5rZ6KDbo0CumZUAPwJuCx67BbibSJjMAT509yl1rUliL7NJKuMuHch1I7ox4f2VfPeJYnZXVIZdlojEgCXiYX5RUZEXFxeHXUbSeuL9Ffxy8nz65OfwyFVFtG2hr36IJAIzm+3uRTXbw54slgR0xSldGX9FEUs27OTCB9+jZOOOsEsSkTpQEMgxOat3O5797ilUVFXztT++x/tLy8MuSUSOkYJAjtlJBTk8f8Mw2mVncuVjM3n+o9KwSxKRY6AgkDopaNWMSdcPo6hLa374zFzuf3OJTi8VSTAKAqmznKZNmPDtwXxtQD53v/4pP/3bPPZXVYddlogcobSwC5DGIT0thT9c0o+C1s24/80lrNu2lz9ePpAWmU3CLk1EDkNHBBIzZsaPzj6O313Ul/eXlnPxQ++zduuesMsSkcNQEEjMXVLUice/NZg1W/Zw4R+nM3/ttrBLEpEvoCCQevGlnm147vpTSDXjkofe51+LN4ZdkogcgoJA6s0J7bN5/sZT6ZKbxTUTinlq5qqwSxKRWigIpF61y87k2e+dwvCebfjP5z/mf19aSLUuWCcSVxQEUu+aZ6TxyJVFXHlKFx5+exnfe3K2LlgnEkcUBNIg0lJTuHNMH+64oDdvLNzAJQ+/z4bte8MuS0RQEEgDu/rUQh65qojlZbsYM05nFInEAwWBNLgzTmjHpOuHkWJw8UPv88aCDWGXJJLUFAQSil4dsvnHjafSs21zrn2imAenlbBj7/6wyxJJSroxjYRqT0UVtz43h5c+Xk9GWgpn9WrH6P4dGXl8HhlpqWGXJ9KoHOrGNDG51pCZjQLuA1KBR9z9tzW2ZwB/AQYRuWn9N9x9RdT2zkTuVXyHu/8+FjVJYmiansqDlw3kw1VbmTxnDS/OW8eUj9eRnZnGuX06MGZAR4YU5pKaYmGXKtJo1fmIwMxSgU+Bs4FSIvcfvtTdF0T1uQHo6+7fM7OxwIXu/o2o7ZMAB2YeSRDoiKDxqqyqZvrScl6Ys4ZXP1nProoq2mVncEHfjozpn0+f/GzMFAoix6I+jwgGAyXuvix4oonAGCL/wz9gDHBHsDwJGGdm5u5uZl8FlgO7YlCLJLi01BROOy6P047LY89Xq3hz0QZemLOWCe+v4JF3l9MtL4sx/fIZ3b8jhW2ywi5XpFGIRRDkA6uj1kuBIYfq4+6VZrYNyDWzvcBPiRxN/PiLnsTMrgOuA+jcuXMMypZ41zQ9lfP7duT8vh3Ztns/L3+yjhfmrOXeNz/lnjc+pV9BDqP753NB3w60zc4Mu1yRhBX2/QjuAO5x952HO9x39/HAeIgMDdV/aRJPcpo1Yezgzowd3Jl12/bw4tx1vDB3Db9+cQG/mbKAYd3bMLp/R0b1aU+27oEgclRiEQRrgE5R6wVBW219Ss0sDcghMmk8BLjIzH4HtASqzWyvu4+LQV3SSHXIacq1I7px7YhulGzcyeS5a3lhzhp+MmkeP//HJ5xxfFvG9O/I6Se0JbOJzjwSOZxYTBanEZksPpPIB/4s4DJ3nx/V50bgpKjJ4q+5+yU1fs8dwE5NFsuxcHfmlm7jhTlr+OfcdWzauY8WGWmM6tOeMf3zOaW7zjwSqbfJ4mDM/ybgVSKnjz7m7vPN7E6g2N0nA48CT5hZCbAZGFvX5xWJZmb079SS/p1a8rPzejFj2Wb+MWcNL3+ynudml5LXIoPz+3ZgdL+OnJSfQ1qqvkspcoC+UCaN2t79VUxbtJEX5qxl6qKNVFRVk56WwnHtmnNC+2xOaN+CXh0if+Y2zwi7XJF6dagjAgWBJI1te/bzr8Ubmb92OwvXbWfR+h2U7dh3cHtei4zPBMMJ7bPp3jZL33CWRkNBIFKLTTv3sXj9joPBsGj9dj7dsJOKymoA0lKM7nnNOaFDJBh6dYgERdsWGfpimyScer3EhEiiatM8gzY9Mji1R5uDbZVV1awo38XCdZFgWLRuB8UrtvDCnLUH+7Rq1iQytNShBb2CP3u2bUHTdB09SOJREIjUkJaaQo+2LejRtgUX9Ot4sH3b7v2RYAiOHBau28HED1azZ38VACkGXdtk0Ss4crigX0e65OrbzxL/NDQkUgfV1c6qzbsPBsOBoFhZvpvUFGNMv47ccHp3erRtEXapIhoaEqkPKSlG1zZZdG2Txag+HQ62b9y+lz+9s4wnZ6zi+TlrOK9PB246owe9OmSHWK1I7XREIFKPynfu47Hpy5nw3kp27qvkrF7t+P4ZPejXqWXYpUkS0llDIiHatns/j7+3gsemL2fbnv2MOC6P75/Rg5O7tg67NEkiCgKROLBzXyVPvL+SR95ZRvmuCoZ2a833z+jJsO65Oh1V6p2CQCSO7Kmo4qkPVvHwW0vZuGMfAzu35Ptn9GTk8XkKBKk3CgKROLR3fxXPzS7loX8tZc3WPfTJz+am03tyTu92pOgieRJjCgKROLa/qprnP1rDH6eVsKJ8N8e1a86Np/fg/L4dddVUiRkFgUgCqKyqZsrH6xg3tYQlG3dS2CaLG0Z256sD8mmiK6ZKHSkIRBJIdbXz6vz1PDC1hAXrtlPQqinXj+zORYMKdBE8OWYKApEE5O5MW7yR+98sYc7qrbTPzuS7p3Vj7MmddV0jOWoKApEE5u5MLynn/qlL+GD5Zto0T+fa4d24fGgXmmfoAgFyZBQEIo3EzGXljJtWwjtLNpGdmcapPdowpLA1Q7rlcny7FjrbSA6pXq81ZGajgPuI3KryEXf/bY3tGcBfgEFEblr/DXdfYWZnA78F0oEK4D/cfWosahJprIZ0y2VIt1w+WrWFJ2esYsaycl7+ZD0ALZs14eSurSPBUJhL747ZOutIDqvOQWBmqcCDwNlAKTDLzCa7+4KobtcAW9y9R3Dz+ruAbwCbgAvcfa2Z9SFy3+P8utYkkgwGdG7FgM6tACjdspuZyzYzc3k5M5dv5vUFGwBokZFGUddWDC7MZUi31pyUn6Ozj+RzYnFEMBgocfdlAGY2ERgDRAfBGOCOYHkSMM7MzN0/iuozH2hqZhnuvg8ROWIFrZpRMKgZXx9UAMD6bXsPhsLMZeVMW1wGQNMmqQzq0urgUFK/Tjk6C0liEgT5wOqo9VJgyKH6uHulmW0DcokcERzwdeDDQ4WAmV0HXAfQuXPnGJQt0ni1z8lkTP98xvSPHGBv2rmPD4JQmLl8M394/VMAMtJSGNC5JYMLcxla2JoBnVvpbKQkFBenG5jZiUSGi845VB93Hw+Mh8hkcQOVJtIotGmewXkndeC8kyL3TNiyq4JZKzZHjhiWlzNu6hLud2iSavQtaHnwiGFQl1Y6KykJxOIdXgN0ilovCNpq61NqZmlADpFJY8ysAHgeuNLdl8agHhE5jFZZ6ZxzYnvOObE9ANv37mf2ii3MWF7OzGWbefjtZfzxX0tJTTH6dMxmSLdcTu3Rhi/1aKPJ50YoFkEwC+hpZoVEPvDHApfV6DMZuAp4H7gImOrubmYtgSnAbe4+PQa1iMgxyM5swukntOX0E9oCsGtfJR+u2nJwAvrx6SsY//Yy8ls25bIhnbmkqBN5LTJCrlpiJSbfIzCz84B7iZw++pi7/8bM7gSK3X2ymWUCTwADgM3AWHdfZmY/B24HlkT9unPcfeMXPZ++RyDSsPbur2Lqoo08OWMl7y0tp0mqcW6fDnxzaBdO7tpKl85OEPpCmYjERMnGnfx15komzS5lx95Kjm/Xgm+e0oULB+RrPiHOKQhEJKZ2V1Qyec5anpixkvlrt5OVnsrXBhbwzaFdOL59i7DLk1ooCESkXrg7c1Zv5YkZK3lx3joqKqsZ3LU13zylC6NObE96mr7AFi8UBCJS77bsquC52at5csYqVm3eTZvm6Xzj5E5cNqQL+S2bhl1e0lMQiEiDqa523l5SxpMzVjF1UeRyF2ec0I5vDu3MiJ55ujBeSOr1onMiItFSUoyRx7dl5PFtKd2ym6c/WMUzs1bzxsINdMltxuVDOnPxoE60ykoPu1RBRwQi0kAqKqt5Zf56nnx/JR+s2Ex6Wgrn9+3AFUO70L9TS52C2gA0NCQicWPR+u08OWMlz3+4hl0VVfTJz+aKoV0Y3S9f1zqqRwoCEYk7O/dV8vxHa3jy/ZUs3rCDFplpXDG0C7ec1VNXRa0HCgIRiVvuTvHKLTz+3gqmzFvHiR2zeeDSAXTLax52aY3KoYJAJ/iKSOjMjJO7tubBywbypyuLWLN1D+c/8C5/m10admlJQUEgInHl7N7tePmW4fTJz+HW5+byw2fmsHNfZdhlNWoKAhGJOx1ymvL0tUP5wVk9eWHOGs6//x0+WbMt7LIaLQWBiMSl1BTjB2cdx9PXDmXv/mou/ON0Hn13OYk4rxnvFAQiEteGdMvl5VuGc9pxbfn1iwu4ZkIx5Tt1W/NYUhCISNxrlZXOn64cxK9Gn8i7SzZx7n3v8N7STYd/oBwRBYGIJAQz46phXXn+xmE0z0zj8kdm8ofXFlNZVR12aQkvJkFgZqPMbLGZlZjZbbVszzCzZ4LtM82sa9S224P2xWb25VjUIyKN14kdc/jnTV/i6wMLeGBqCWPHz2DN1j1hl5XQ6hwEZpYKPAicC/QGLjWz3jW6XQNscfcewD3AXcFjexO5x/GJwCjgj8HvExE5pKyMNH5/cT/uG9ufRet3cO69b/PKJ+vDLithxeKIYDBQ4u7L3L0CmAiMqdFnDDAhWJ4EnGmRK0yNASa6+z53Xw6UBL9PROSwxvTPZ8rNX6Jrmyy+9+RsfvGPT9i7vyrsshJOLIIgH1gdtV4atNXax90rgW1A7hE+FgAzu87Mis2suKysLAZli0hj0CU3i0nfG8a1wwt5YsZKvvrgdEo27gi7rISSMJPF7j7e3YvcvSgvLy/sckQkjqSnpfCzr/Tmz986mbId+zj/gXeZ+MEqfefgCMUiCNYAnaLWC4K2WvuYWRqQA5Qf4WNFRI7I6ce35eVbhjOoSytu+/vH3PjUh2zdXRF2WXEvFkEwC+hpZoVmlk5k8ndyjT6TgauC5YuAqR6J6snA2OCsokKgJ/BBDGoSkSTVNjuTv3x7CD8ddQKvzd/AqHvfYXqJvnPwReocBMGY/03Aq8BC4Fl3n29md5rZ6KDbo0CumZUAPwJuCx47H3gWWAC8Atzo7prpEZE6SU0xrh/ZnedvOJVmGalc/shMfjNlAfsq9fFSG92PQEQatT0VVfz3lAX8deYqenXI5r6x/TmuXYuwywqF7kcgIkmpaXoqv7nwJB69qoiN2/dywQPv8vh0XbwumoJARJLCmb3a8coPRjCsey53/HMB33x0Jh+t2hJ2WXFBQSAiSSOvRQaPXX0yv/5qH+av3c6Ff3yPKx6dyQfLN4ddWqg0RyAiSWnnvkqenLGSR95ZxqadFQwpbM3NZ/ZkWPdcIhc+aHx083oRkVrsqajiqQ9W8fBbS9m4Yx8DO7fk+2f0ZOTxeY0uEBQEIiJfYO/+Kp6bXcpD/1rKmq17OCk/h5vO6MHZvdqRktI4AkFBICJyBCoqq3n+o1IenLaUVZt3c0L7Ftx0Rg/O7dOB1AQPBAWBiMhRqKyqZvLctYybVsKysl10z8vipjN6cEHfjqSlJuZ5NgoCEZFjUFXtvPTxOsZNLWHxhh10yW3GjSN7cOHAfJokWCAoCERE6qC62nltwQYemLqE+Wu3k9+yKdeP7M7FRQVkpCXG/bQUBCIiMeDuTFu8kfvfLGHO6q20z87ku6d149LBnclsEt+BoCAQEYkhd2d6STn3T13CB8s306Z5BteNKOTyIV3IykgLu7xaKQhEROrJzGXlPDC1hHdLNtGqWROu+VIhVw7rSnZmk7BL+wwFgYhIPZu9cgvjpi5h2uIysjPTuGpYV0b1aU+n1s3iIhQUBCIiDeTj0m08MHUJry3YcLAtOzONglbNKGjVNOrPYLl10wYJikMFQXwOZImIJLCTCnIYf2URpVt2M690G6VbdlO6ZQ+lW/awonwX75ZsYnfFZ2+ScyAo8qMDImo5p2n9BYWCQESknkQ+zJt9rt3d2bJ7f1RA/DsoVpXvZnotQdEiCIpnvzuUFjE+eqhTEJhZa+AZoCuwArjE3T93gW8zuwr4ebD63+4+wcyaAc8B3YEq4J/ufltd6hERSQRmRuusdFpnpdO3oOXntrs7W3fvrxESu1m3bS/N6+GMpDrNEZjZ74DN7v5bM7sNaOXuP63RpzVQDBQBDswGBgH7gCHuPi246f2bwP+4+8uHe17NEYiIHL36ulXlGGBCsDwB+Gotfb4MvO7um4OjhdeBUe6+292nAbh7BfAhUFDHekRE5CjVNQjaufu6YHk90K6WPvnA6qj10qDtIDNrCVxA5KigVmZ2nZkVm1lxWVlZnYoWEZF/O+xgk5m9AbSvZdPPolfc3c3sqMeZzCwNeBq4392XHaqfu48HxkNkaOhon0dERGp32CBw97MOtc3MNphZB3dfZ2YdgI21dFsDjIxaLwD+FbU+Hlji7vceScEiIhJbdR0amgxcFSxfBbxQS59XgXPMrJWZtQLOCdows/8GcoAf1LEOERE5RnUNgt8CZ5vZEuCsYB0zKzKzRwDcfTPwa2BW8HOnu282swIiw0u9gQ/NbI6ZfaeO9YiIyFHSJSZERJJEfZ0+KiIiCS4hjwjMrAxYeYwPbwNsimE5YUj0fUj0+kH7EC8SfR8auv4u7p5XszEhg6AuzKy4tkOjRJLo+5Do9YP2IV4k+j7ES/0aGhIRSXIKAhGRJJeMQTA+7AJiINH3IdHrB+1DvEj0fYiL+pNujkBERD4rGY8IREQkioJARCTJJU0QmNkoM1tsZiXBTXTikpl1MrNpZrbAzOab2S1Be2sze93MlgR/tgrazczuD/ZrnpkNDHcPIsws1cw+MrMXg/VCM5sZ1PlMcDMizCwjWC8JtncNtfCAmbU0s0lmtsjMFprZKQn4Hvww+Dv0iZk9bWaZ8f4+mNljZrbRzD6Jajvq193Mrgr6LwnukBj2Pvxf8Hdpnpk9H1x6/8C224N9WGxmX45qb7jPLHdv9D9AKrAU6AakA3OB3mHXdYhaOwADg+UWwKdErsf0O+C2oP024K5g+TzgZcCAocDMsPchqOtHwFPAi8H6s8DYYPkh4Ppg+QbgoWB5LPBM2LUHtUwAvhMspwMtE+k9IHLPj+VA06jX/+p4fx+AEcBA4JOotqN63YHWwLLgz1bBcquQ9+EcIC1YvitqH3oHn0cZQGHwOZXa0J9Zof5lbcA35hTg1aj124Hbw67rCGt/ATgbWAx0CNo6AIuD5YeBS6P6H+wXYs0FRG4ydAbwYvAPdVPUP4SD7weRK9GeEiynBf0s5Ppzgg9Rq9GeSO/BgRtCtQ5e1xeJ3C0w7t8HIvdAj/4QParXHbgUeDiq/TP9wtiHGtsuBP4aLH/ms+jA+9DQn1nJMjR02LukxaPg8HwAMJND3w0uHvftXuAnQHWwngtsdffKYD26xoP1B9u3Bf3DVAiUAX8OhrceMbMsEug9cPc1wO+BVcA6Iq/rbBLrfTjgaF/3uHs/avg2kSMZiJN9SJYgSDhm1hz4G/ADd98evc0j/0WIy/N+zex8YKO7zw67ljpII3Jo///cfQCwi8iQxEHx/B4ABOPoY4iEWkcgCxgValExEO+v++GY2c+ASuCvYdcSLVmCYA3QKWq9IGiLS2bWhEgI/NXd/x40b7DIXeCwz94NLt727VRgtJmtACYSGR66D2hpkduSwmdrPFh/sD0HKG/IgmtRCpS6+8xgfRKRYEiU9wAi9wdZ7u5l7r4f+DuR9yaR3ocDjvZ1j8f3AzO7GjgfuDwINIiTfUiWIJgF9AzOmEgnMhk2OeSaamVmBjwKLHT3u6M2HepucJOBK4MzKIYC26IOoxucu9/u7gXu3pXI6zzV3S8HpgEXBd1q1n9gvy4K+of6Pz53Xw+sNrPjg6YzgQUkyHsQWAUMNbNmwd+pA/uQMO9DlKN93Q95V8SwmNkoIsOlo919d9SmycDY4KytQqAn8AEN/ZnVkBMoYf4QOcPgUyIz8T8Lu54vqPNLRA595wFzgp/ziIzXvgksAd4AWgf9DXgw2K+PgaKw9yFqX0by77OGugV/wUuA54CMoD0zWC8JtncLu+6grv5AcfA+/IPI2ScJ9R4AvwIWAZ8ATxA5MyWu3wfgaSJzGvuJHJldcyyvO5Fx+JLg51txsA8lRMb8D/ybfiiq/8+CfVgMnBvV3mCfWbrEhIhIkkuWoSERETkEBYGISJJTEIiIJDkFgYhIklMQiIgkOQWBiEiSUxCIiCS5/w/RvHeik4V+wwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 89.3180  # Value for C0\n",
    "K0 = -0.0010  # Value for K0\n",
    "K1 = 0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = -0.0204    # Value for b\n",
    "c = 2.2194    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    92.600000    92.387115    92.174230    91.961345    91.748459   \n",
      "351    92.387115    92.174230    91.961345    91.748459    91.535574   \n",
      "352    92.174230    91.961345    91.748459    91.535574    91.322689   \n",
      "353    91.961345    91.748459    91.535574    91.322689    91.109804   \n",
      "354    91.748459    91.535574    91.322689    91.109804    90.896919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    91.535574    91.322689    91.109804    90.896919    90.691597  ...   \n",
      "351    91.322689    91.109804    90.896919    90.691597    90.579552  ...   \n",
      "352    91.109804    90.896919    90.691597    90.579552    90.467507  ...   \n",
      "353    90.896919    90.691597    90.579552    90.467507    90.355462  ...   \n",
      "354    90.691597    90.579552    90.467507    90.355462    90.243417  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   80.261671    0.000263   80.252334    0.000263   80.242997    0.000263   \n",
      "351   80.252334    0.000263   80.242997    0.000263   80.233660    0.000262   \n",
      "352   80.242997    0.000263   80.233660    0.000262   80.224323    0.000262   \n",
      "353   80.233660    0.000262   80.224323    0.000262   80.214986    0.000262   \n",
      "354   80.224323    0.000262   80.214986    0.000262   80.205649    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   80.233660    0.000262   80.224323    0.000262  \n",
      "351   80.224323    0.000262   80.214986    0.000262  \n",
      "352   80.214986    0.000262   80.205649    0.000262  \n",
      "353   80.205649    0.000262   80.196312    0.000262  \n",
      "354   80.196312    0.000262   80.186975    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 1, 251) (800, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "10/10 [==============================] - 3s 72ms/step - loss: 6136.9326 - val_loss: 5450.3755\n",
      "Epoch 2/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 6084.1670 - val_loss: 5415.5439\n",
      "Epoch 3/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 6047.2651 - val_loss: 5380.6494\n",
      "Epoch 4/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 6010.3857 - val_loss: 5345.8535\n",
      "Epoch 5/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 5973.6289 - val_loss: 5311.2002\n",
      "Epoch 6/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 5935.1387 - val_loss: 5260.6221\n",
      "Epoch 7/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 5882.8721 - val_loss: 5224.7017\n",
      "Epoch 8/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5844.7461 - val_loss: 5188.7222\n",
      "Epoch 9/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 5801.5493 - val_loss: 5145.9756\n",
      "Epoch 10/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5760.6313 - val_loss: 5108.4507\n",
      "Epoch 11/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5721.0264 - val_loss: 5071.2856\n",
      "Epoch 12/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5681.8359 - val_loss: 5034.5327\n",
      "Epoch 13/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5643.0601 - val_loss: 4998.1587\n",
      "Epoch 14/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5604.6577 - val_loss: 4962.1196\n",
      "Epoch 15/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 5566.5854 - val_loss: 4926.3804\n",
      "Epoch 16/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5528.8125 - val_loss: 4890.9180\n",
      "Epoch 17/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5491.3135 - val_loss: 4855.7124\n",
      "Epoch 18/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 5454.0732 - val_loss: 4820.7480\n",
      "Epoch 19/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5417.0757 - val_loss: 4786.0156\n",
      "Epoch 20/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5380.3125 - val_loss: 4751.5054\n",
      "Epoch 21/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 5343.7744 - val_loss: 4717.2109\n",
      "Epoch 22/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5307.4546 - val_loss: 4683.1270\n",
      "Epoch 23/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5271.3477 - val_loss: 4649.2476\n",
      "Epoch 24/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 5235.4482 - val_loss: 4615.5688\n",
      "Epoch 25/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5199.7539 - val_loss: 4582.0874\n",
      "Epoch 26/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 5164.2593 - val_loss: 4532.5679\n",
      "Epoch 27/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5110.6045 - val_loss: 4496.8345\n",
      "Epoch 28/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 5072.3208 - val_loss: 4460.8354\n",
      "Epoch 29/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5034.2363 - val_loss: 4425.3047\n",
      "Epoch 30/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4996.6733 - val_loss: 4390.2749\n",
      "Epoch 31/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4959.6089 - val_loss: 4355.6904\n",
      "Epoch 32/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 4922.9814 - val_loss: 4321.4932\n",
      "Epoch 33/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 4886.7373 - val_loss: 4287.6401\n",
      "Epoch 34/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4850.8330 - val_loss: 4254.0981\n",
      "Epoch 35/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 4815.2407 - val_loss: 4220.8433\n",
      "Epoch 36/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4779.9360 - val_loss: 4187.8579\n",
      "Epoch 37/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 4744.9028 - val_loss: 4155.1274\n",
      "Epoch 38/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 4710.1265 - val_loss: 4122.6387\n",
      "Epoch 39/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4672.9951 - val_loss: 4077.4011\n",
      "Epoch 40/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 4625.9766 - val_loss: 4042.0730\n",
      "Epoch 41/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 4588.2236 - val_loss: 4006.8367\n",
      "Epoch 42/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 4550.8843 - val_loss: 3972.1790\n",
      "Epoch 43/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4514.1616 - val_loss: 3938.0906\n",
      "Epoch 44/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4478.0039 - val_loss: 3904.5005\n",
      "Epoch 45/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4442.3359 - val_loss: 3871.3425\n",
      "Epoch 46/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4407.0962 - val_loss: 3838.5679\n",
      "Epoch 47/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 4372.2373 - val_loss: 3806.1399\n",
      "Epoch 48/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 4337.7251 - val_loss: 3774.0298\n",
      "Epoch 49/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4303.5337 - val_loss: 3742.2173\n",
      "Epoch 50/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4269.6421 - val_loss: 3710.6851\n",
      "Epoch 51/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4236.0352 - val_loss: 3679.4194\n",
      "Epoch 52/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4202.6982 - val_loss: 3648.4102\n",
      "Epoch 53/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4169.6235 - val_loss: 3617.6479\n",
      "Epoch 54/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 4136.7983 - val_loss: 3587.1250\n",
      "Epoch 55/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4104.2173 - val_loss: 3556.8333\n",
      "Epoch 56/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4071.8723 - val_loss: 3526.7688\n",
      "Epoch 57/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4039.7593 - val_loss: 3496.9243\n",
      "Epoch 58/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 4007.8706 - val_loss: 3467.2974\n",
      "Epoch 59/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 3976.2039 - val_loss: 3437.8821\n",
      "Epoch 60/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 3944.7534 - val_loss: 3408.6750\n",
      "Epoch 61/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3913.5164 - val_loss: 3379.6738\n",
      "Epoch 62/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 3882.4888 - val_loss: 3350.8750\n",
      "Epoch 63/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3851.6667 - val_loss: 3322.2742\n",
      "Epoch 64/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3821.0488 - val_loss: 3293.8711\n",
      "Epoch 65/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3790.6316 - val_loss: 3265.6609\n",
      "Epoch 66/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3760.4133 - val_loss: 3237.6431\n",
      "Epoch 67/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 3730.3901 - val_loss: 3209.8140\n",
      "Epoch 68/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3700.5605 - val_loss: 3182.1731\n",
      "Epoch 69/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3670.9233 - val_loss: 3154.7180\n",
      "Epoch 70/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3641.4758 - val_loss: 3127.4465\n",
      "Epoch 71/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3612.2163 - val_loss: 3100.3569\n",
      "Epoch 72/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3583.1431 - val_loss: 3073.4478\n",
      "Epoch 73/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3554.2534 - val_loss: 3046.7170\n",
      "Epoch 74/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3525.5474 - val_loss: 3020.1641\n",
      "Epoch 75/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3497.0227 - val_loss: 2993.7866\n",
      "Epoch 76/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 3468.6772 - val_loss: 2967.5835\n",
      "Epoch 77/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3440.5098 - val_loss: 2941.5542\n",
      "Epoch 78/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3412.5203 - val_loss: 2915.6958\n",
      "Epoch 79/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 3384.7063 - val_loss: 2890.0085\n",
      "Epoch 80/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3357.0669 - val_loss: 2864.4902\n",
      "Epoch 81/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 3329.6001 - val_loss: 2839.1399\n",
      "Epoch 82/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3302.3059 - val_loss: 2813.9570\n",
      "Epoch 83/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 3275.1824 - val_loss: 2788.9407\n",
      "Epoch 84/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3248.2290 - val_loss: 2764.0886\n",
      "Epoch 85/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 3221.4438 - val_loss: 2739.3999\n",
      "Epoch 86/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3194.8262 - val_loss: 2714.8745\n",
      "Epoch 87/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3168.3760 - val_loss: 2690.5105\n",
      "Epoch 88/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 3142.0903 - val_loss: 2666.3079\n",
      "Epoch 89/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3115.9695 - val_loss: 2639.2017\n",
      "Epoch 90/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 3085.3091 - val_loss: 2612.0444\n",
      "Epoch 91/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3055.5232 - val_loss: 2584.5308\n",
      "Epoch 92/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3025.9507 - val_loss: 2557.5608\n",
      "Epoch 93/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2996.9910 - val_loss: 2531.1655\n",
      "Epoch 94/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2968.6106 - val_loss: 2505.2720\n",
      "Epoch 95/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2940.7280 - val_loss: 2479.8083\n",
      "Epoch 96/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2913.2725 - val_loss: 2454.7192\n",
      "Epoch 97/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2886.1912 - val_loss: 2429.9629\n",
      "Epoch 98/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2859.4448 - val_loss: 2405.5088\n",
      "Epoch 99/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2833.0054 - val_loss: 2381.3335\n",
      "Epoch 100/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2806.8496 - val_loss: 2357.4194\n",
      "Epoch 101/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2780.9595 - val_loss: 2333.7532\n",
      "Epoch 102/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2755.3223 - val_loss: 2310.3210\n",
      "Epoch 103/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2729.9250 - val_loss: 2287.1147\n",
      "Epoch 104/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 2704.7593 - val_loss: 2264.1260\n",
      "Epoch 105/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2679.8164 - val_loss: 2241.3472\n",
      "Epoch 106/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 2655.0901 - val_loss: 2218.7737\n",
      "Epoch 107/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2630.5732 - val_loss: 2196.3987\n",
      "Epoch 108/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2606.2615 - val_loss: 2174.2180\n",
      "Epoch 109/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2582.1489 - val_loss: 2152.2288\n",
      "Epoch 110/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2558.2324 - val_loss: 2130.4243\n",
      "Epoch 111/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2534.5078 - val_loss: 2108.8042\n",
      "Epoch 112/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2510.9712 - val_loss: 2087.3650\n",
      "Epoch 113/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 2487.6204 - val_loss: 2066.1018\n",
      "Epoch 114/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2464.4509 - val_loss: 2045.0123\n",
      "Epoch 115/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2441.4617 - val_loss: 2024.0959\n",
      "Epoch 116/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2418.6492 - val_loss: 2003.3492\n",
      "Epoch 117/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2396.0112 - val_loss: 1982.7699\n",
      "Epoch 118/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2373.5457 - val_loss: 1962.3567\n",
      "Epoch 119/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2351.2507 - val_loss: 1942.1062\n",
      "Epoch 120/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2329.1245 - val_loss: 1922.0186\n",
      "Epoch 121/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2307.1648 - val_loss: 1902.0908\n",
      "Epoch 122/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2285.3699 - val_loss: 1882.3225\n",
      "Epoch 123/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2263.7385 - val_loss: 1862.7097\n",
      "Epoch 124/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 2242.2681 - val_loss: 1843.2535\n",
      "Epoch 125/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 2220.9585 - val_loss: 1823.9508\n",
      "Epoch 126/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2199.8071 - val_loss: 1804.8011\n",
      "Epoch 127/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2178.8130 - val_loss: 1785.8024\n",
      "Epoch 128/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2157.9741 - val_loss: 1766.9539\n",
      "Epoch 129/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2137.2903 - val_loss: 1748.2543\n",
      "Epoch 130/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2116.7603 - val_loss: 1729.7025\n",
      "Epoch 131/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2096.3816 - val_loss: 1711.2979\n",
      "Epoch 132/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2076.1543 - val_loss: 1693.0383\n",
      "Epoch 133/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 2056.0767 - val_loss: 1674.9226\n",
      "Epoch 134/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2036.1477 - val_loss: 1656.9504\n",
      "Epoch 135/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2016.3666 - val_loss: 1639.1207\n",
      "Epoch 136/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1996.7317 - val_loss: 1621.4319\n",
      "Epoch 137/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1977.2422 - val_loss: 1603.8834\n",
      "Epoch 138/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1957.8969 - val_loss: 1586.4742\n",
      "Epoch 139/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 1938.6959 - val_loss: 1569.2028\n",
      "Epoch 140/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 1919.6367 - val_loss: 1552.0697\n",
      "Epoch 141/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1900.7195 - val_loss: 1535.0726\n",
      "Epoch 142/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1881.9431 - val_loss: 1518.2113\n",
      "Epoch 143/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1863.3060 - val_loss: 1501.4841\n",
      "Epoch 144/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1844.8079 - val_loss: 1484.8909\n",
      "Epoch 145/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1826.4475 - val_loss: 1468.4307\n",
      "Epoch 146/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1808.2246 - val_loss: 1452.1028\n",
      "Epoch 147/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1790.1381 - val_loss: 1435.9070\n",
      "Epoch 148/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1772.1871 - val_loss: 1419.8412\n",
      "Epoch 149/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1754.3708 - val_loss: 1403.9047\n",
      "Epoch 150/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1736.6879 - val_loss: 1388.0974\n",
      "Epoch 151/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1719.1383 - val_loss: 1372.4186\n",
      "Epoch 152/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1701.7214 - val_loss: 1356.8674\n",
      "Epoch 153/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1684.4362 - val_loss: 1341.4420\n",
      "Epoch 154/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1667.2810 - val_loss: 1326.1438\n",
      "Epoch 155/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1650.2562 - val_loss: 1310.9697\n",
      "Epoch 156/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1633.3604 - val_loss: 1295.9194\n",
      "Epoch 157/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1616.5928 - val_loss: 1280.9944\n",
      "Epoch 158/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1599.9535 - val_loss: 1266.1920\n",
      "Epoch 159/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1583.4412 - val_loss: 1251.5112\n",
      "Epoch 160/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1567.0549 - val_loss: 1236.9524\n",
      "Epoch 161/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1550.7946 - val_loss: 1222.5137\n",
      "Epoch 162/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1534.6588 - val_loss: 1208.1965\n",
      "Epoch 163/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1518.6473 - val_loss: 1193.9984\n",
      "Epoch 164/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1502.7599 - val_loss: 1179.9193\n",
      "Epoch 165/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 1486.9951 - val_loss: 1165.9578\n",
      "Epoch 166/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1471.3522 - val_loss: 1152.1144\n",
      "Epoch 167/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1455.8312 - val_loss: 1138.3877\n",
      "Epoch 168/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1440.4309 - val_loss: 1124.7776\n",
      "Epoch 169/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1425.1511 - val_loss: 1111.2821\n",
      "Epoch 170/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1409.9902 - val_loss: 1097.9026\n",
      "Epoch 171/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1394.9495 - val_loss: 1084.6361\n",
      "Epoch 172/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1380.0259 - val_loss: 1071.4850\n",
      "Epoch 173/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1365.2209 - val_loss: 1058.4460\n",
      "Epoch 174/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1350.5326 - val_loss: 1045.5197\n",
      "Epoch 175/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1335.9604 - val_loss: 1032.7050\n",
      "Epoch 176/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1321.5046 - val_loss: 1020.0013\n",
      "Epoch 177/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1307.1635 - val_loss: 1007.4086\n",
      "Epoch 178/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1292.9374 - val_loss: 994.9258\n",
      "Epoch 179/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1278.8250 - val_loss: 982.5519\n",
      "Epoch 180/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1264.8259 - val_loss: 970.2863\n",
      "Epoch 181/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1250.9395 - val_loss: 958.1301\n",
      "Epoch 182/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 1237.1652 - val_loss: 946.0811\n",
      "Epoch 183/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1223.5026 - val_loss: 934.1383\n",
      "Epoch 184/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 1209.9507 - val_loss: 922.3022\n",
      "Epoch 185/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 1196.5094 - val_loss: 910.5724\n",
      "Epoch 186/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 1183.1779 - val_loss: 898.9474\n",
      "Epoch 187/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1169.9553 - val_loss: 887.4271\n",
      "Epoch 188/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1156.8417 - val_loss: 876.0101\n",
      "Epoch 189/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1143.8362 - val_loss: 864.6971\n",
      "Epoch 190/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1130.9379 - val_loss: 853.4878\n",
      "Epoch 191/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 1118.1466 - val_loss: 842.3806\n",
      "Epoch 192/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1105.4618 - val_loss: 831.3743\n",
      "Epoch 193/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1092.8827 - val_loss: 820.4698\n",
      "Epoch 194/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1080.4088 - val_loss: 809.6658\n",
      "Epoch 195/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 1068.0398 - val_loss: 798.9615\n",
      "Epoch 196/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1055.7745 - val_loss: 788.3574\n",
      "Epoch 197/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1043.6130 - val_loss: 777.8517\n",
      "Epoch 198/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1031.5546 - val_loss: 767.4451\n",
      "Epoch 199/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1019.5988 - val_loss: 757.1362\n",
      "Epoch 200/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1007.7448 - val_loss: 746.9249\n",
      "Epoch 201/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 995.9922 - val_loss: 736.8093\n",
      "Epoch 202/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 984.3400 - val_loss: 726.7903\n",
      "Epoch 203/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 972.7879 - val_loss: 716.8671\n",
      "Epoch 204/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 961.3358 - val_loss: 707.0389\n",
      "Epoch 205/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 949.9831 - val_loss: 697.3055\n",
      "Epoch 206/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 938.7291 - val_loss: 687.6662\n",
      "Epoch 207/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 927.5729 - val_loss: 678.1204\n",
      "Epoch 208/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 916.5145 - val_loss: 668.6674\n",
      "Epoch 209/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 905.5530 - val_loss: 659.3066\n",
      "Epoch 210/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 894.6881 - val_loss: 650.0386\n",
      "Epoch 211/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 883.9193 - val_loss: 640.8613\n",
      "Epoch 212/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 873.2459 - val_loss: 631.7748\n",
      "Epoch 213/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 862.6674 - val_loss: 622.7787\n",
      "Epoch 214/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 852.1834 - val_loss: 613.8729\n",
      "Epoch 215/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 841.7935 - val_loss: 605.0558\n",
      "Epoch 216/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 831.4969 - val_loss: 596.3279\n",
      "Epoch 217/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 821.2933 - val_loss: 587.6886\n",
      "Epoch 218/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 811.1818 - val_loss: 579.1362\n",
      "Epoch 219/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 801.1622 - val_loss: 570.6708\n",
      "Epoch 220/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 791.2341 - val_loss: 562.2931\n",
      "Epoch 221/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 781.3970 - val_loss: 554.0011\n",
      "Epoch 222/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 771.6500 - val_loss: 545.7955\n",
      "Epoch 223/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 761.9930 - val_loss: 537.6738\n",
      "Epoch 224/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 752.4250 - val_loss: 529.6370\n",
      "Epoch 225/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 742.9457 - val_loss: 521.6845\n",
      "Epoch 226/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 733.5547 - val_loss: 513.8156\n",
      "Epoch 227/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 724.2518 - val_loss: 506.0302\n",
      "Epoch 228/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 715.0363 - val_loss: 498.3273\n",
      "Epoch 229/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 705.9075 - val_loss: 490.7068\n",
      "Epoch 230/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 696.8651 - val_loss: 483.1673\n",
      "Epoch 231/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 687.9081 - val_loss: 475.7093\n",
      "Epoch 232/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 679.0368 - val_loss: 468.3315\n",
      "Epoch 233/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 670.2502 - val_loss: 461.0340\n",
      "Epoch 234/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 661.5479 - val_loss: 453.8163\n",
      "Epoch 235/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 652.9293 - val_loss: 446.6774\n",
      "Epoch 236/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 644.3941 - val_loss: 439.6173\n",
      "Epoch 237/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 635.9418 - val_loss: 432.6353\n",
      "Epoch 238/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 627.5719 - val_loss: 425.7308\n",
      "Epoch 239/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 619.2838 - val_loss: 418.9033\n",
      "Epoch 240/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 611.0769 - val_loss: 412.1529\n",
      "Epoch 241/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 602.9510 - val_loss: 405.4781\n",
      "Epoch 242/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 594.9055 - val_loss: 398.8791\n",
      "Epoch 243/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 586.9399 - val_loss: 392.3551\n",
      "Epoch 244/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 579.0537 - val_loss: 385.9062\n",
      "Epoch 245/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 571.2464 - val_loss: 379.5310\n",
      "Epoch 246/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 563.5176 - val_loss: 373.2297\n",
      "Epoch 247/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 555.8668 - val_loss: 367.0016\n",
      "Epoch 248/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 548.2938 - val_loss: 360.8463\n",
      "Epoch 249/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 540.7977 - val_loss: 354.7634\n",
      "Epoch 250/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 533.3782 - val_loss: 348.7519\n",
      "Epoch 251/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 526.0347 - val_loss: 342.8118\n",
      "Epoch 252/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 518.7666 - val_loss: 336.9428\n",
      "Epoch 253/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 511.5740 - val_loss: 331.1436\n",
      "Epoch 254/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 504.4559 - val_loss: 325.4144\n",
      "Epoch 255/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 497.4120 - val_loss: 319.7548\n",
      "Epoch 256/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 490.4418 - val_loss: 314.1637\n",
      "Epoch 257/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 483.5448 - val_loss: 308.6412\n",
      "Epoch 258/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 476.7206 - val_loss: 303.1861\n",
      "Epoch 259/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 469.9683 - val_loss: 297.7987\n",
      "Epoch 260/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 463.2883 - val_loss: 292.4782\n",
      "Epoch 261/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 456.6791 - val_loss: 287.2241\n",
      "Epoch 262/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 450.1410 - val_loss: 282.0359\n",
      "Epoch 263/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 443.6734 - val_loss: 276.9136\n",
      "Epoch 264/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 437.2759 - val_loss: 271.8560\n",
      "Epoch 265/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 430.9480 - val_loss: 266.8634\n",
      "Epoch 266/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 424.6889 - val_loss: 261.9348\n",
      "Epoch 267/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 418.4982 - val_loss: 257.0696\n",
      "Epoch 268/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 412.3759 - val_loss: 252.2675\n",
      "Epoch 269/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 406.3210 - val_loss: 247.5285\n",
      "Epoch 270/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 400.3336 - val_loss: 242.8517\n",
      "Epoch 271/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 394.4128 - val_loss: 238.2360\n",
      "Epoch 272/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 388.5580 - val_loss: 233.6825\n",
      "Epoch 273/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 382.7693 - val_loss: 229.1890\n",
      "Epoch 274/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 377.0458 - val_loss: 224.7564\n",
      "Epoch 275/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 371.3874 - val_loss: 220.3835\n",
      "Epoch 276/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 365.7931 - val_loss: 216.0703\n",
      "Epoch 277/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 360.2632 - val_loss: 211.8158\n",
      "Epoch 278/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 354.7966 - val_loss: 207.6199\n",
      "Epoch 279/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 349.3931 - val_loss: 203.4824\n",
      "Epoch 280/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 344.0521 - val_loss: 199.4018\n",
      "Epoch 281/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 338.7733 - val_loss: 195.3788\n",
      "Epoch 282/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 333.5565 - val_loss: 191.4126\n",
      "Epoch 283/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 328.4007 - val_loss: 187.5021\n",
      "Epoch 284/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 323.3058 - val_loss: 183.6480\n",
      "Epoch 285/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 318.2715 - val_loss: 179.8492\n",
      "Epoch 286/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 313.2970 - val_loss: 176.1046\n",
      "Epoch 287/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 308.3818 - val_loss: 172.4151\n",
      "Epoch 288/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 303.5258 - val_loss: 168.7791\n",
      "Epoch 289/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 298.7285 - val_loss: 165.1972\n",
      "Epoch 290/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 293.9894 - val_loss: 161.6679\n",
      "Epoch 291/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 289.3079 - val_loss: 158.1910\n",
      "Epoch 292/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 284.6837 - val_loss: 154.7670\n",
      "Epoch 293/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 280.1164 - val_loss: 151.3940\n",
      "Epoch 294/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 275.6054 - val_loss: 148.0725\n",
      "Epoch 295/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 271.1505 - val_loss: 144.8020\n",
      "Epoch 296/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 266.7510 - val_loss: 141.5818\n",
      "Epoch 297/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 262.4068 - val_loss: 138.4114\n",
      "Epoch 298/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 258.1170 - val_loss: 135.2908\n",
      "Epoch 299/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 253.8815 - val_loss: 132.2189\n",
      "Epoch 300/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 249.6998 - val_loss: 129.1957\n",
      "Epoch 301/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 245.5712 - val_loss: 126.2206\n",
      "Epoch 302/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 241.4960 - val_loss: 123.2936\n",
      "Epoch 303/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 237.4730 - val_loss: 120.4132\n",
      "Epoch 304/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 233.5019 - val_loss: 117.5801\n",
      "Epoch 305/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 229.5826 - val_loss: 114.7933\n",
      "Epoch 306/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 225.7144 - val_loss: 112.0527\n",
      "Epoch 307/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 221.8973 - val_loss: 109.3575\n",
      "Epoch 308/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 218.1301 - val_loss: 106.7069\n",
      "Epoch 309/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 214.4125 - val_loss: 104.1011\n",
      "Epoch 310/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 210.7447 - val_loss: 101.5398\n",
      "Epoch 311/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 207.1259 - val_loss: 99.0221\n",
      "Epoch 312/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 203.5558 - val_loss: 96.5478\n",
      "Epoch 313/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 200.0337 - val_loss: 94.1168\n",
      "Epoch 314/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 196.5598 - val_loss: 91.7281\n",
      "Epoch 315/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 193.1332 - val_loss: 89.3815\n",
      "Epoch 316/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 189.7534 - val_loss: 87.0764\n",
      "Epoch 317/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 186.4204 - val_loss: 84.8130\n",
      "Epoch 318/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 183.1335 - val_loss: 82.5901\n",
      "Epoch 319/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 179.8921 - val_loss: 80.4076\n",
      "Epoch 320/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 176.6961 - val_loss: 78.2652\n",
      "Epoch 321/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 173.5450 - val_loss: 76.1623\n",
      "Epoch 322/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 170.4383 - val_loss: 74.0987\n",
      "Epoch 323/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 167.3757 - val_loss: 72.0736\n",
      "Epoch 324/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 164.3568 - val_loss: 70.0869\n",
      "Epoch 325/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 161.3811 - val_loss: 68.1380\n",
      "Epoch 326/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 158.4482 - val_loss: 66.2266\n",
      "Epoch 327/500\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 155.5576 - val_loss: 64.3522\n",
      "Epoch 328/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 152.7090 - val_loss: 62.5148\n",
      "Epoch 329/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 149.9022 - val_loss: 60.7133\n",
      "Epoch 330/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 147.1367 - val_loss: 58.9479\n",
      "Epoch 331/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 144.4119 - val_loss: 57.2177\n",
      "Epoch 332/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 141.7274 - val_loss: 55.5227\n",
      "Epoch 333/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 139.0830 - val_loss: 53.8622\n",
      "Epoch 334/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 136.4781 - val_loss: 52.2361\n",
      "Epoch 335/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 133.9126 - val_loss: 50.6437\n",
      "Epoch 336/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 131.3858 - val_loss: 49.0849\n",
      "Epoch 337/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 128.8976 - val_loss: 47.5590\n",
      "Epoch 338/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 126.4473 - val_loss: 46.0655\n",
      "Epoch 339/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 124.0348 - val_loss: 44.6046\n",
      "Epoch 340/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 121.6594 - val_loss: 43.1756\n",
      "Epoch 341/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 119.3211 - val_loss: 41.7780\n",
      "Epoch 342/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 117.0193 - val_loss: 40.4113\n",
      "Epoch 343/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 114.7535 - val_loss: 39.0756\n",
      "Epoch 344/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 112.5235 - val_loss: 37.7698\n",
      "Epoch 345/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 110.3286 - val_loss: 36.4940\n",
      "Epoch 346/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 108.1687 - val_loss: 35.2476\n",
      "Epoch 347/500\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 106.0432 - val_loss: 34.0307\n",
      "Epoch 348/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 103.9522 - val_loss: 32.8424\n",
      "Epoch 349/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 101.8952 - val_loss: 31.6824\n",
      "Epoch 350/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 99.8714 - val_loss: 30.5505\n",
      "Epoch 351/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 97.8808 - val_loss: 29.4464\n",
      "Epoch 352/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 95.9229 - val_loss: 28.3694\n",
      "Epoch 353/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 93.9973 - val_loss: 27.3192\n",
      "Epoch 354/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 92.1036 - val_loss: 26.2956\n",
      "Epoch 355/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 90.2417 - val_loss: 25.2982\n",
      "Epoch 356/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 88.4110 - val_loss: 24.3266\n",
      "Epoch 357/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 86.6113 - val_loss: 23.3806\n",
      "Epoch 358/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 84.8421 - val_loss: 22.4596\n",
      "Epoch 359/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 83.1030 - val_loss: 21.5629\n",
      "Epoch 360/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 81.3936 - val_loss: 20.6909\n",
      "Epoch 361/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 79.7138 - val_loss: 19.8427\n",
      "Epoch 362/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 78.0631 - val_loss: 19.0185\n",
      "Epoch 363/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 76.4412 - val_loss: 18.2172\n",
      "Epoch 364/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 74.8475 - val_loss: 17.4390\n",
      "Epoch 365/500\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 73.2822 - val_loss: 16.6834\n",
      "Epoch 366/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 71.7443 - val_loss: 15.9500\n",
      "Epoch 367/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 70.2338 - val_loss: 15.2385\n",
      "Epoch 368/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 68.7504 - val_loss: 14.5485\n",
      "Epoch 369/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 67.2936 - val_loss: 13.8798\n",
      "Epoch 370/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 65.8632 - val_loss: 13.2319\n",
      "Epoch 371/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 64.4588 - val_loss: 12.6046\n",
      "Epoch 372/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 63.0800 - val_loss: 11.9974\n",
      "Epoch 373/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 61.7266 - val_loss: 11.4103\n",
      "Epoch 374/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 60.3981 - val_loss: 10.8426\n",
      "Epoch 375/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 59.0943 - val_loss: 10.2940\n",
      "Epoch 376/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 57.8149 - val_loss: 9.7644\n",
      "Epoch 377/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 56.5595 - val_loss: 9.2535\n",
      "Epoch 378/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 55.3279 - val_loss: 8.7607\n",
      "Epoch 379/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 54.1196 - val_loss: 8.2858\n",
      "Epoch 380/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 52.9343 - val_loss: 7.8288\n",
      "Epoch 381/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 51.7720 - val_loss: 7.3889\n",
      "Epoch 382/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 50.6320 - val_loss: 6.9660\n",
      "Epoch 383/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 49.5141 - val_loss: 6.5598\n",
      "Epoch 384/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 48.4178 - val_loss: 6.1700\n",
      "Epoch 385/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 47.3432 - val_loss: 5.7962\n",
      "Epoch 386/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 46.2896 - val_loss: 5.4382\n",
      "Epoch 387/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 45.2569 - val_loss: 5.0958\n",
      "Epoch 388/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 44.2449 - val_loss: 4.7684\n",
      "Epoch 389/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 43.2531 - val_loss: 4.4560\n",
      "Epoch 390/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 42.2814 - val_loss: 4.1583\n",
      "Epoch 391/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 41.3294 - val_loss: 3.8748\n",
      "Epoch 392/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 40.3967 - val_loss: 3.6052\n",
      "Epoch 393/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 39.4831 - val_loss: 3.3495\n",
      "Epoch 394/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 38.5884 - val_loss: 3.1072\n",
      "Epoch 395/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 37.7121 - val_loss: 2.8781\n",
      "Epoch 396/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 36.8541 - val_loss: 2.6619\n",
      "Epoch 397/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 36.0142 - val_loss: 2.4584\n",
      "Epoch 398/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 35.1920 - val_loss: 2.2672\n",
      "Epoch 399/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 34.3871 - val_loss: 2.0881\n",
      "Epoch 400/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 33.5994 - val_loss: 1.9208\n",
      "Epoch 401/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 32.8286 - val_loss: 1.7651\n",
      "Epoch 402/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 32.0744 - val_loss: 1.6207\n",
      "Epoch 403/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 31.3365 - val_loss: 1.4873\n",
      "Epoch 404/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 30.6145 - val_loss: 1.3647\n",
      "Epoch 405/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 29.9084 - val_loss: 1.2527\n",
      "Epoch 406/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 29.2180 - val_loss: 1.1510\n",
      "Epoch 407/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 28.5428 - val_loss: 1.0593\n",
      "Epoch 408/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 27.8827 - val_loss: 0.9774\n",
      "Epoch 409/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 27.2373 - val_loss: 0.9051\n",
      "Epoch 410/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 26.6064 - val_loss: 0.8421\n",
      "Epoch 411/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 25.9898 - val_loss: 0.7882\n",
      "Epoch 412/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 25.3872 - val_loss: 0.7431\n",
      "Epoch 413/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 24.7984 - val_loss: 0.7066\n",
      "Epoch 414/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 24.2231 - val_loss: 0.6786\n",
      "Epoch 415/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 23.6612 - val_loss: 0.6587\n",
      "Epoch 416/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 23.1124 - val_loss: 0.6468\n",
      "Epoch 417/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 22.5764 - val_loss: 0.6426\n",
      "Epoch 418/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 22.0530 - val_loss: 0.6459\n",
      "Epoch 419/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 21.5420 - val_loss: 0.6565\n",
      "Epoch 420/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 21.0432 - val_loss: 0.6742\n",
      "Epoch 421/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 20.5564 - val_loss: 0.6988\n",
      "Epoch 422/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 20.0812 - val_loss: 0.7300\n",
      "Epoch 423/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 19.6176 - val_loss: 0.7678\n",
      "Epoch 424/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 19.1652 - val_loss: 0.8118\n",
      "Epoch 425/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 18.7239 - val_loss: 0.8619\n",
      "Epoch 426/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 18.2934 - val_loss: 0.9178\n",
      "Epoch 427/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 17.8737 - val_loss: 0.9795\n",
      "Epoch 428/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 17.4643 - val_loss: 1.0466\n",
      "Epoch 429/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 17.0653 - val_loss: 1.1191\n",
      "Epoch 430/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 16.6763 - val_loss: 1.1968\n",
      "Epoch 431/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 16.2970 - val_loss: 1.2794\n",
      "Epoch 432/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 15.9274 - val_loss: 1.3667\n",
      "Epoch 433/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 15.5674 - val_loss: 1.4586\n",
      "Epoch 434/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 15.2167 - val_loss: 1.5550\n",
      "Epoch 435/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 14.8749 - val_loss: 1.6557\n",
      "Epoch 436/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 14.5421 - val_loss: 1.7605\n",
      "Epoch 437/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 14.2180 - val_loss: 1.8692\n",
      "Epoch 438/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 13.9023 - val_loss: 1.9817\n",
      "Epoch 439/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 13.5951 - val_loss: 2.0978\n",
      "Epoch 440/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 13.2961 - val_loss: 2.2174\n",
      "Epoch 441/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 13.0050 - val_loss: 2.3402\n",
      "Epoch 442/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 12.7219 - val_loss: 2.4663\n",
      "Epoch 443/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 12.4463 - val_loss: 2.5954\n",
      "Epoch 444/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 12.1783 - val_loss: 2.7273\n",
      "Epoch 445/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 11.9176 - val_loss: 2.8620\n",
      "Epoch 446/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 11.6641 - val_loss: 2.9993\n",
      "Epoch 447/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 11.4175 - val_loss: 3.1390\n",
      "Epoch 448/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 11.1779 - val_loss: 3.2810\n",
      "Epoch 449/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 10.9450 - val_loss: 3.4253\n",
      "Epoch 450/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 10.7187 - val_loss: 3.5715\n",
      "Epoch 451/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 10.4987 - val_loss: 3.7197\n",
      "Epoch 452/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 10.2851 - val_loss: 3.8698\n",
      "Epoch 453/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 10.0775 - val_loss: 4.0216\n",
      "Epoch 454/500\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 9.8759 - val_loss: 4.1749\n",
      "Epoch 455/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 9.6801 - val_loss: 4.3298\n",
      "Epoch 456/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 9.4900 - val_loss: 4.4860\n",
      "Epoch 457/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 9.3055 - val_loss: 4.6434\n",
      "Epoch 458/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 9.1264 - val_loss: 4.8019\n",
      "Epoch 459/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.9527 - val_loss: 4.9615\n",
      "Epoch 460/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.7841 - val_loss: 5.1220\n",
      "Epoch 461/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.6206 - val_loss: 5.2834\n",
      "Epoch 462/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 8.4620 - val_loss: 5.4455\n",
      "Epoch 463/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 8.3082 - val_loss: 5.6082\n",
      "Epoch 464/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.1590 - val_loss: 5.7714\n",
      "Epoch 465/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.0144 - val_loss: 5.9352\n",
      "Epoch 466/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 7.8743 - val_loss: 6.0991\n",
      "Epoch 467/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 7.7386 - val_loss: 6.2636\n",
      "Epoch 468/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 7.6070 - val_loss: 6.4282\n",
      "Epoch 469/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 7.4796 - val_loss: 6.5928\n",
      "Epoch 470/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 7.3562 - val_loss: 6.7576\n",
      "Epoch 471/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 7.2367 - val_loss: 6.9223\n",
      "Epoch 472/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 7.1210 - val_loss: 7.0868\n",
      "Epoch 473/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 7.0090 - val_loss: 7.2511\n",
      "Epoch 474/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 6.9006 - val_loss: 7.4153\n",
      "Epoch 475/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 6.7957 - val_loss: 7.5792\n",
      "Epoch 476/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 6.6942 - val_loss: 7.7425\n",
      "Epoch 477/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 6.5961 - val_loss: 7.9055\n",
      "Epoch 478/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 6.5011 - val_loss: 8.0679\n",
      "Epoch 479/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 6.4094 - val_loss: 8.2298\n",
      "Epoch 480/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 6.3206 - val_loss: 8.3911\n",
      "Epoch 481/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 6.2348 - val_loss: 8.5517\n",
      "Epoch 482/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 6.1519 - val_loss: 8.7114\n",
      "Epoch 483/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 6.0719 - val_loss: 8.8704\n",
      "Epoch 484/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5.9946 - val_loss: 9.0286\n",
      "Epoch 485/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5.9199 - val_loss: 9.1859\n",
      "Epoch 486/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5.8477 - val_loss: 9.3423\n",
      "Epoch 487/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5.7781 - val_loss: 9.4977\n",
      "Epoch 488/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 5.7109 - val_loss: 9.6520\n",
      "Epoch 489/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5.6460 - val_loss: 9.8053\n",
      "Epoch 490/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5.5834 - val_loss: 9.9575\n",
      "Epoch 491/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5.5230 - val_loss: 10.1085\n",
      "Epoch 492/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5.4648 - val_loss: 10.2584\n",
      "Epoch 493/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 5.4087 - val_loss: 10.4071\n",
      "Epoch 494/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5.3545 - val_loss: 10.5546\n",
      "Epoch 495/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5.3023 - val_loss: 10.7007\n",
      "Epoch 496/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5.2521 - val_loss: 10.8456\n",
      "Epoch 497/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5.2036 - val_loss: 10.9891\n",
      "Epoch 498/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5.1569 - val_loss: 11.1313\n",
      "Epoch 499/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5.1120 - val_loss: 11.2720\n",
      "Epoch 500/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5.0687 - val_loss: 11.4115\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(89.3180, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0010, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(-0.0204, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.2194, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 350ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.71410131e+01, 7.71214052e+01, 7.71017974e+01, 7.70821895e+01,\n",
       "        7.70625817e+01, 7.70429739e+01, 7.70233660e+01, 7.70037582e+01,\n",
       "        7.69841503e+01, 7.69645425e+01, 7.69449346e+01, 7.69253268e+01,\n",
       "        7.69057189e+01, 7.68761905e+01, 7.68425770e+01, 7.68089636e+01,\n",
       "        7.67753501e+01, 7.67417367e+01, 7.67081232e+01, 7.66745098e+01,\n",
       "        7.66408964e+01, 7.66072829e+01, 7.65736695e+01, 7.65400560e+01,\n",
       "        7.65064426e+01, 7.64728291e+01, 7.64392157e+01, 7.64056022e+01,\n",
       "        7.63719888e+01, 7.63383754e+01, 7.63047619e+01, 7.62711485e+01,\n",
       "        7.62375350e+01, 7.62039216e+01, 7.61703081e+01, 7.61366947e+01,\n",
       "        7.61030812e+01, 7.60694678e+01, 7.60358543e+01, 7.60022409e+01,\n",
       "        7.59686274e+01, 7.59350140e+01, 7.59014006e+01, 7.58677871e+01,\n",
       "        7.58341737e+01, 7.58005602e+01, 7.57669468e+01, 7.57333333e+01,\n",
       "        7.56994865e+01, 7.56378618e+01, 7.55762372e+01, 7.55146125e+01,\n",
       "        7.54529879e+01, 7.53913632e+01, 7.53297386e+01, 7.52681139e+01,\n",
       "        7.52064893e+01, 7.51448646e+01, 7.50832400e+01, 7.50216153e+01,\n",
       "        7.49599907e+01, 7.48983660e+01, 7.48367414e+01, 7.47751167e+01,\n",
       "        7.47134921e+01, 7.46518674e+01, 7.45902428e+01, 7.45286181e+01,\n",
       "        7.44669935e+01, 7.44053688e+01, 7.43437442e+01, 7.42821195e+01,\n",
       "        7.42204949e+01, 7.41588702e+01, 7.40972456e+01, 7.40356209e+01,\n",
       "        7.39739963e+01, 7.39123716e+01, 7.38507470e+01, 7.37891223e+01,\n",
       "        7.85414047e+01, 5.74235499e-01, 6.43230319e-01, 8.65812778e-01,\n",
       "        0.00000000e+00, 5.04692912e-01, 4.47440207e-01, 2.71232128e-01,\n",
       "        2.72157162e-01, 3.89988646e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 5.77498794e-01, 1.43160075e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.28929925e-01, 0.00000000e+00, 4.09462452e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([71.69567927, 71.69203782, 71.68839636, 71.6847549 , 71.68111345,\n",
       "       71.67747199, 71.67383053, 71.67018908, 71.66654762, 71.66290616,\n",
       "       71.65926471, 71.65562325, 71.65198179, 71.64834034, 71.64469888,\n",
       "       71.64105742, 71.63741597, 71.63377451, 71.63013305, 71.6264916 ,\n",
       "       71.62285014, 71.61920868, 71.61556723, 71.61192577, 71.60828431,\n",
       "       71.60464286, 71.6010014 , 71.59526144, 71.58872549, 71.58218954,\n",
       "       71.57565359, 71.56911765, 71.5625817 , 71.55604575, 71.5495098 ,\n",
       "       71.54297386, 71.53643791, 71.52990196, 71.52336601, 71.51683007,\n",
       "       71.51029412, 71.50375817, 71.49722222, 71.49068627, 71.48415033,\n",
       "       71.47761438, 71.47107843, 71.46454248, 71.45800654, 71.45147059,\n",
       "       71.44493464, 71.43839869, 71.43186275, 71.4253268 , 71.41879085,\n",
       "       71.4122549 , 71.40571895, 71.39918301, 71.39264706, 71.38611111,\n",
       "       71.37957516, 71.37303922, 71.36650327, 71.35996732, 71.35343137,\n",
       "       71.34689542, 71.34035948, 71.33382353, 71.32728758, 71.32075163,\n",
       "       71.31421569, 71.30767974, 71.30114379, 71.29460784, 71.2880719 ,\n",
       "       71.28153595, 71.275     , 71.26846405, 71.2619281 , 71.25539216,\n",
       "       71.24885621, 71.24232026, 71.23578431, 71.22924837, 71.22271242,\n",
       "       71.21617647, 71.20964052, 71.20310458, 71.19656863, 71.19003268,\n",
       "       71.18349673, 71.17696078, 71.17042484, 71.16388889, 71.15735294,\n",
       "       71.15081699, 71.14428105, 71.1377451 , 71.13120915, 71.1246732 ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.16851056054012\n",
      "16.984716352254544\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
