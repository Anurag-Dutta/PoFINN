{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       92.600000\n",
       "1       92.387115\n",
       "2       92.174230\n",
       "3       91.961345\n",
       "4       91.748459\n",
       "          ...    \n",
       "1945    67.043873\n",
       "1946    67.034069\n",
       "1947    67.024265\n",
       "1948    67.014461\n",
       "1949    67.004657\n",
       "Name: C7, Length: 1950, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c7_interpolated_1850_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       92.600000\n",
       "1       92.387115\n",
       "2       92.174230\n",
       "3       91.961345\n",
       "4       91.748459\n",
       "          ...    \n",
       "1845     0.000000\n",
       "1846     0.191040\n",
       "1847     0.000000\n",
       "1848     0.000000\n",
       "1849     0.043149\n",
       "Name: C7, Length: 1850, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1850)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92.387115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92.174230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.961345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91.748459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     92.600000  0.000298\n",
       "1     92.387115  0.000298\n",
       "2     92.174230  0.000297\n",
       "3     91.961345  0.000297\n",
       "4     91.748459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 89.3180  # Value for C0\n",
    "K0 = -0.0010  # Value for K0\n",
    "K1 = 0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = -0.0204    # Value for b\n",
    "c = 2.2194    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    92.600000    92.387115    92.174230    91.961345    91.748459   \n",
      "351    92.387115    92.174230    91.961345    91.748459    91.535574   \n",
      "352    92.174230    91.961345    91.748459    91.535574    91.322689   \n",
      "353    91.961345    91.748459    91.535574    91.322689    91.109804   \n",
      "354    91.748459    91.535574    91.322689    91.109804    90.896919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    91.535574    91.322689    91.109804    90.896919    90.691597  ...   \n",
      "351    91.322689    91.109804    90.896919    90.691597    90.579552  ...   \n",
      "352    91.109804    90.896919    90.691597    90.579552    90.467507  ...   \n",
      "353    90.896919    90.691597    90.579552    90.467507    90.355462  ...   \n",
      "354    90.691597    90.579552    90.467507    90.355462    90.243417  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   80.261671    0.000263   80.252334    0.000263   80.242997    0.000263   \n",
      "351   80.252334    0.000263   80.242997    0.000263   80.233660    0.000262   \n",
      "352   80.242997    0.000263   80.233660    0.000262   80.224323    0.000262   \n",
      "353   80.233660    0.000262   80.224323    0.000262   80.214986    0.000262   \n",
      "354   80.224323    0.000262   80.214986    0.000262   80.205649    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   80.233660    0.000262   80.224323    0.000262  \n",
      "351   80.224323    0.000262   80.214986    0.000262  \n",
      "352   80.214986    0.000262   80.205649    0.000262  \n",
      "353   80.205649    0.000262   80.196312    0.000262  \n",
      "354   80.196312    0.000262   80.186975    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 1, 251) (1400, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "18/18 [==============================] - 2s 27ms/step - loss: 5678.7554 - val_loss: 3998.1201\n",
      "Epoch 2/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5501.3174 - val_loss: 3877.9817\n",
      "Epoch 3/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5404.4956 - val_loss: 3815.8459\n",
      "Epoch 4/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5326.7271 - val_loss: 3763.5273\n",
      "Epoch 5/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5250.3745 - val_loss: 3712.4180\n",
      "Epoch 6/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5175.3110 - val_loss: 3662.2375\n",
      "Epoch 7/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5101.3682 - val_loss: 3612.8716\n",
      "Epoch 8/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5028.4321 - val_loss: 3564.2549\n",
      "Epoch 9/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4956.4355 - val_loss: 3516.3455\n",
      "Epoch 10/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4885.3325 - val_loss: 3469.1157\n",
      "Epoch 11/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4815.0898 - val_loss: 3422.5430\n",
      "Epoch 12/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4745.6855 - val_loss: 3376.6116\n",
      "Epoch 13/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4677.0977 - val_loss: 3331.3079\n",
      "Epoch 14/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4609.3105 - val_loss: 3286.6204\n",
      "Epoch 15/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4542.3125 - val_loss: 3242.5391\n",
      "Epoch 16/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4476.0889 - val_loss: 3199.0552\n",
      "Epoch 17/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4410.6309 - val_loss: 3156.1599\n",
      "Epoch 18/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4345.9272 - val_loss: 3113.8469\n",
      "Epoch 19/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4281.9712 - val_loss: 3072.1086\n",
      "Epoch 20/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4218.7505 - val_loss: 3030.9382\n",
      "Epoch 21/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4156.2607 - val_loss: 2990.3296\n",
      "Epoch 22/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4094.4929 - val_loss: 2950.2773\n",
      "Epoch 23/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4033.4397 - val_loss: 2910.7749\n",
      "Epoch 24/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3973.0938 - val_loss: 2871.8169\n",
      "Epoch 25/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3913.4495 - val_loss: 2833.3984\n",
      "Epoch 26/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3854.4988 - val_loss: 2795.5134\n",
      "Epoch 27/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3796.2356 - val_loss: 2758.1570\n",
      "Epoch 28/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3738.6545 - val_loss: 2721.3242\n",
      "Epoch 29/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3681.7488 - val_loss: 2685.0100\n",
      "Epoch 30/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3625.5122 - val_loss: 2649.2092\n",
      "Epoch 31/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3569.9387 - val_loss: 2613.9172\n",
      "Epoch 32/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3515.0234 - val_loss: 2579.1296\n",
      "Epoch 33/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3460.7593 - val_loss: 2544.8416\n",
      "Epoch 34/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3407.1418 - val_loss: 2511.0479\n",
      "Epoch 35/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3354.1650 - val_loss: 2477.7446\n",
      "Epoch 36/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3301.8230 - val_loss: 2444.9272\n",
      "Epoch 37/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3250.1106 - val_loss: 2412.5911\n",
      "Epoch 38/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3199.0227 - val_loss: 2380.7317\n",
      "Epoch 39/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3148.5537 - val_loss: 2349.3452\n",
      "Epoch 40/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3098.6978 - val_loss: 2318.4265\n",
      "Epoch 41/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3049.4509 - val_loss: 2287.9719\n",
      "Epoch 42/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3000.8071 - val_loss: 2257.9768\n",
      "Epoch 43/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2952.7612 - val_loss: 2228.4375\n",
      "Epoch 44/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2905.3091 - val_loss: 2199.3491\n",
      "Epoch 45/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2858.4448 - val_loss: 2170.7085\n",
      "Epoch 46/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 2812.1638 - val_loss: 2142.5105\n",
      "Epoch 47/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2766.4607 - val_loss: 2114.7517\n",
      "Epoch 48/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2721.3308 - val_loss: 2087.4277\n",
      "Epoch 49/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2676.7695 - val_loss: 2060.5349\n",
      "Epoch 50/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2632.7720 - val_loss: 2034.0692\n",
      "Epoch 51/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2589.3337 - val_loss: 2008.0259\n",
      "Epoch 52/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2546.4492 - val_loss: 1982.4017\n",
      "Epoch 53/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2504.1140 - val_loss: 1957.1931\n",
      "Epoch 54/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2462.3240 - val_loss: 1932.3958\n",
      "Epoch 55/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2421.0732 - val_loss: 1908.0051\n",
      "Epoch 56/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2380.3586 - val_loss: 1884.0194\n",
      "Epoch 57/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2340.1750 - val_loss: 1860.4323\n",
      "Epoch 58/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2300.5176 - val_loss: 1837.2416\n",
      "Epoch 59/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2261.3818 - val_loss: 1814.4431\n",
      "Epoch 60/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2222.7637 - val_loss: 1792.0331\n",
      "Epoch 61/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2184.6582 - val_loss: 1770.0083\n",
      "Epoch 62/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2147.0615 - val_loss: 1748.3639\n",
      "Epoch 63/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2109.9692 - val_loss: 1727.0973\n",
      "Epoch 64/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2073.3760 - val_loss: 1706.2041\n",
      "Epoch 65/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2037.2781 - val_loss: 1685.6816\n",
      "Epoch 66/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2001.6719 - val_loss: 1665.5254\n",
      "Epoch 67/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1966.5525 - val_loss: 1645.7323\n",
      "Epoch 68/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1931.9154 - val_loss: 1626.2977\n",
      "Epoch 69/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1897.7565 - val_loss: 1607.2195\n",
      "Epoch 70/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1864.0717 - val_loss: 1588.4934\n",
      "Epoch 71/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1830.8563 - val_loss: 1570.1165\n",
      "Epoch 72/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1798.1072 - val_loss: 1552.0840\n",
      "Epoch 73/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1765.8197 - val_loss: 1534.3936\n",
      "Epoch 74/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1733.9890 - val_loss: 1517.0409\n",
      "Epoch 75/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1702.6116 - val_loss: 1500.0234\n",
      "Epoch 76/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1671.6835 - val_loss: 1483.3368\n",
      "Epoch 77/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1641.2004 - val_loss: 1466.9781\n",
      "Epoch 78/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1611.1584 - val_loss: 1450.9438\n",
      "Epoch 79/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1581.5533 - val_loss: 1435.2308\n",
      "Epoch 80/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1552.3820 - val_loss: 1419.8350\n",
      "Epoch 81/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1523.6389 - val_loss: 1404.7538\n",
      "Epoch 82/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1495.3208 - val_loss: 1389.9833\n",
      "Epoch 83/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1467.4238 - val_loss: 1375.5203\n",
      "Epoch 84/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1439.9447 - val_loss: 1361.3615\n",
      "Epoch 85/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 1412.8783 - val_loss: 1347.5038\n",
      "Epoch 86/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1386.2213 - val_loss: 1333.9434\n",
      "Epoch 87/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1359.9701 - val_loss: 1320.6776\n",
      "Epoch 88/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1334.1205 - val_loss: 1307.7026\n",
      "Epoch 89/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1308.6686 - val_loss: 1295.0154\n",
      "Epoch 90/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1283.6110 - val_loss: 1282.6127\n",
      "Epoch 91/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1258.9432 - val_loss: 1270.4911\n",
      "Epoch 92/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1234.6619 - val_loss: 1258.6473\n",
      "Epoch 93/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1210.7632 - val_loss: 1247.0786\n",
      "Epoch 94/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1187.2433 - val_loss: 1235.7814\n",
      "Epoch 95/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1164.0986 - val_loss: 1224.7526\n",
      "Epoch 96/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1141.3257 - val_loss: 1213.9886\n",
      "Epoch 97/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1118.9199 - val_loss: 1203.4867\n",
      "Epoch 98/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1096.8783 - val_loss: 1193.2440\n",
      "Epoch 99/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1075.1970 - val_loss: 1183.2566\n",
      "Epoch 100/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1053.8722 - val_loss: 1173.5217\n",
      "Epoch 101/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1032.9005 - val_loss: 1164.0363\n",
      "Epoch 102/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1012.2783 - val_loss: 1154.7972\n",
      "Epoch 103/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 992.0015 - val_loss: 1145.8009\n",
      "Epoch 104/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 972.0667 - val_loss: 1137.0450\n",
      "Epoch 105/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 952.4708 - val_loss: 1128.5259\n",
      "Epoch 106/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 933.2095 - val_loss: 1120.2406\n",
      "Epoch 107/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 914.2797 - val_loss: 1112.1860\n",
      "Epoch 108/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 895.6776 - val_loss: 1104.3589\n",
      "Epoch 109/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 877.3999 - val_loss: 1096.7571\n",
      "Epoch 110/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 859.4431 - val_loss: 1089.3763\n",
      "Epoch 111/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 841.8032 - val_loss: 1082.2145\n",
      "Epoch 112/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 824.4772 - val_loss: 1075.2677\n",
      "Epoch 113/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 807.4611 - val_loss: 1068.5337\n",
      "Epoch 114/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 790.7517 - val_loss: 1062.0089\n",
      "Epoch 115/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 774.3458 - val_loss: 1055.6910\n",
      "Epoch 116/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 758.2394 - val_loss: 1049.5763\n",
      "Epoch 117/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 742.4294 - val_loss: 1043.6622\n",
      "Epoch 118/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 726.9124 - val_loss: 1037.9457\n",
      "Epoch 119/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 711.6848 - val_loss: 1032.4237\n",
      "Epoch 120/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 696.7433 - val_loss: 1027.0931\n",
      "Epoch 121/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 682.0847 - val_loss: 1021.9518\n",
      "Epoch 122/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 667.7055 - val_loss: 1016.9958\n",
      "Epoch 123/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 653.6019 - val_loss: 1012.2228\n",
      "Epoch 124/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 639.7709 - val_loss: 1007.6295\n",
      "Epoch 125/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 626.2093 - val_loss: 1003.2135\n",
      "Epoch 126/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 612.9136 - val_loss: 998.9713\n",
      "Epoch 127/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 599.8804 - val_loss: 994.9005\n",
      "Epoch 128/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 587.1064 - val_loss: 990.9979\n",
      "Epoch 129/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 574.5888 - val_loss: 987.2610\n",
      "Epoch 130/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 562.3237 - val_loss: 983.6866\n",
      "Epoch 131/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 550.3082 - val_loss: 980.2722\n",
      "Epoch 132/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 538.5388 - val_loss: 977.0147\n",
      "Epoch 133/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 527.0122 - val_loss: 973.9113\n",
      "Epoch 134/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 515.7253 - val_loss: 970.9593\n",
      "Epoch 135/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 504.6751 - val_loss: 968.1557\n",
      "Epoch 136/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 493.8583 - val_loss: 965.4979\n",
      "Epoch 137/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 483.2711 - val_loss: 962.9830\n",
      "Epoch 138/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 472.9112 - val_loss: 960.6085\n",
      "Epoch 139/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 462.7746 - val_loss: 958.3713\n",
      "Epoch 140/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 452.8590 - val_loss: 956.2689\n",
      "Epoch 141/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 443.1606 - val_loss: 954.2985\n",
      "Epoch 142/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 433.6767 - val_loss: 952.4573\n",
      "Epoch 143/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 424.4040 - val_loss: 950.7430\n",
      "Epoch 144/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 415.3395 - val_loss: 949.1522\n",
      "Epoch 145/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 406.4802 - val_loss: 947.6828\n",
      "Epoch 146/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 397.8224 - val_loss: 946.3319\n",
      "Epoch 147/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 389.3638 - val_loss: 945.0970\n",
      "Epoch 148/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 381.1010 - val_loss: 943.9753\n",
      "Epoch 149/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 373.0312 - val_loss: 942.9644\n",
      "Epoch 150/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 365.1510 - val_loss: 942.0615\n",
      "Epoch 151/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 357.4576 - val_loss: 941.2640\n",
      "Epoch 152/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 349.9483 - val_loss: 940.5694\n",
      "Epoch 153/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 342.6196 - val_loss: 939.9752\n",
      "Epoch 154/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 335.4691 - val_loss: 939.4788\n",
      "Epoch 155/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 328.4936 - val_loss: 939.0777\n",
      "Epoch 156/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 321.6901 - val_loss: 938.7694\n",
      "Epoch 157/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 315.0560 - val_loss: 938.5515\n",
      "Epoch 158/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 308.5883 - val_loss: 938.4213\n",
      "Epoch 159/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 302.2843 - val_loss: 938.3765\n",
      "Epoch 160/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 296.1405 - val_loss: 938.4146\n",
      "Epoch 161/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 290.1549 - val_loss: 938.5334\n",
      "Epoch 162/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 284.3243 - val_loss: 938.7302\n",
      "Epoch 163/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 278.6459 - val_loss: 939.0029\n",
      "Epoch 164/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 273.1171 - val_loss: 939.3488\n",
      "Epoch 165/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 267.7350 - val_loss: 939.7659\n",
      "Epoch 166/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 262.4970 - val_loss: 940.2518\n",
      "Epoch 167/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 257.4003 - val_loss: 940.8040\n",
      "Epoch 168/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 252.4424 - val_loss: 941.4207\n",
      "Epoch 169/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 247.6202 - val_loss: 942.0991\n",
      "Epoch 170/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 242.9313 - val_loss: 942.8374\n",
      "Epoch 171/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 238.3733 - val_loss: 943.6332\n",
      "Epoch 172/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 233.9432 - val_loss: 944.4844\n",
      "Epoch 173/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 229.6389 - val_loss: 945.3889\n",
      "Epoch 174/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 225.4575 - val_loss: 946.3443\n",
      "Epoch 175/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 221.3965 - val_loss: 947.3486\n",
      "Epoch 176/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 217.4536 - val_loss: 948.4001\n",
      "Epoch 177/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 213.6259 - val_loss: 949.4964\n",
      "Epoch 178/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 209.9111 - val_loss: 950.6355\n",
      "Epoch 179/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 206.3071 - val_loss: 951.8152\n",
      "Epoch 180/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 202.8113 - val_loss: 953.0342\n",
      "Epoch 181/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 199.4214 - val_loss: 954.2899\n",
      "Epoch 182/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 196.1348 - val_loss: 955.5804\n",
      "Epoch 183/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 192.9492 - val_loss: 956.9044\n",
      "Epoch 184/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 189.8624 - val_loss: 958.2596\n",
      "Epoch 185/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 186.8721 - val_loss: 959.6441\n",
      "Epoch 186/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 183.9760 - val_loss: 961.0562\n",
      "Epoch 187/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 181.1720 - val_loss: 962.4944\n",
      "Epoch 188/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 178.4578 - val_loss: 963.9565\n",
      "Epoch 189/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 175.8313 - val_loss: 965.4411\n",
      "Epoch 190/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 173.2904 - val_loss: 966.9461\n",
      "Epoch 191/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 170.8330 - val_loss: 968.4705\n",
      "Epoch 192/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 168.4568 - val_loss: 970.0125\n",
      "Epoch 193/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 166.1597 - val_loss: 971.5704\n",
      "Epoch 194/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 163.9399 - val_loss: 973.1426\n",
      "Epoch 195/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 161.7953 - val_loss: 974.7275\n",
      "Epoch 196/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 159.7239 - val_loss: 976.3237\n",
      "Epoch 197/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 157.7240 - val_loss: 977.9300\n",
      "Epoch 198/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 155.7933 - val_loss: 979.5444\n",
      "Epoch 199/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 153.9303 - val_loss: 981.1663\n",
      "Epoch 200/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 152.1329 - val_loss: 982.7936\n",
      "Epoch 201/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 150.3994 - val_loss: 984.4258\n",
      "Epoch 202/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 148.7279 - val_loss: 986.0606\n",
      "Epoch 203/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 147.1166 - val_loss: 987.6976\n",
      "Epoch 204/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 145.5639 - val_loss: 989.3353\n",
      "Epoch 205/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 144.0680 - val_loss: 990.9724\n",
      "Epoch 206/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 142.6275 - val_loss: 992.6077\n",
      "Epoch 207/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 141.2404 - val_loss: 994.2404\n",
      "Epoch 208/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 139.9053 - val_loss: 995.8693\n",
      "Epoch 209/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 138.6205 - val_loss: 997.4932\n",
      "Epoch 210/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 137.3846 - val_loss: 999.1110\n",
      "Epoch 211/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 136.1962 - val_loss: 1000.7221\n",
      "Epoch 212/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 135.0535 - val_loss: 1002.3257\n",
      "Epoch 213/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 133.9550 - val_loss: 1003.9203\n",
      "Epoch 214/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 132.8997 - val_loss: 1005.5049\n",
      "Epoch 215/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 131.8860 - val_loss: 1007.0796\n",
      "Epoch 216/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 130.9124 - val_loss: 1008.6429\n",
      "Epoch 217/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 129.9778 - val_loss: 1010.1940\n",
      "Epoch 218/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 129.0808 - val_loss: 1011.7325\n",
      "Epoch 219/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 128.2200 - val_loss: 1013.2573\n",
      "Epoch 220/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 127.3945 - val_loss: 1014.7681\n",
      "Epoch 221/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 126.6028 - val_loss: 1016.2639\n",
      "Epoch 222/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 125.8438 - val_loss: 1017.7444\n",
      "Epoch 223/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 125.1164 - val_loss: 1019.2090\n",
      "Epoch 224/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 124.4194 - val_loss: 1020.6572\n",
      "Epoch 225/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 123.7517 - val_loss: 1022.0885\n",
      "Epoch 226/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 123.1124 - val_loss: 1023.5021\n",
      "Epoch 227/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 122.5002 - val_loss: 1024.8981\n",
      "Epoch 228/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 121.9143 - val_loss: 1026.2754\n",
      "Epoch 229/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 121.3536 - val_loss: 1027.6339\n",
      "Epoch 230/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 120.8173 - val_loss: 1028.9734\n",
      "Epoch 231/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 120.3044 - val_loss: 1030.2936\n",
      "Epoch 232/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 119.8139 - val_loss: 1031.5939\n",
      "Epoch 233/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 119.3451 - val_loss: 1032.8743\n",
      "Epoch 234/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 118.8971 - val_loss: 1034.1344\n",
      "Epoch 235/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 118.4691 - val_loss: 1035.3735\n",
      "Epoch 236/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 118.0603 - val_loss: 1036.5924\n",
      "Epoch 237/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 117.6699 - val_loss: 1037.7902\n",
      "Epoch 238/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 117.2972 - val_loss: 1038.9668\n",
      "Epoch 239/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 116.9414 - val_loss: 1040.1223\n",
      "Epoch 240/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 116.6019 - val_loss: 1041.2567\n",
      "Epoch 241/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 116.2780 - val_loss: 1042.3696\n",
      "Epoch 242/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 115.9689 - val_loss: 1043.4614\n",
      "Epoch 243/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 115.6742 - val_loss: 1044.5315\n",
      "Epoch 244/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 115.3933 - val_loss: 1045.5804\n",
      "Epoch 245/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 115.1254 - val_loss: 1046.6075\n",
      "Epoch 246/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 114.8700 - val_loss: 1047.6141\n",
      "Epoch 247/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 114.6267 - val_loss: 1048.5991\n",
      "Epoch 248/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 114.3948 - val_loss: 1049.5626\n",
      "Epoch 249/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 114.1739 - val_loss: 1050.5049\n",
      "Epoch 250/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 113.9635 - val_loss: 1051.4258\n",
      "Epoch 251/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 113.7633 - val_loss: 1052.3257\n",
      "Epoch 252/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 113.5725 - val_loss: 1053.2053\n",
      "Epoch 253/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 113.3909 - val_loss: 1054.0636\n",
      "Epoch 254/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 113.2181 - val_loss: 1054.9019\n",
      "Epoch 255/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 113.0535 - val_loss: 1055.7196\n",
      "Epoch 256/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 112.8970 - val_loss: 1056.5167\n",
      "Epoch 257/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 112.7480 - val_loss: 1057.2942\n",
      "Epoch 258/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 112.6063 - val_loss: 1058.0518\n",
      "Epoch 259/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 112.4715 - val_loss: 1058.7897\n",
      "Epoch 260/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 112.3433 - val_loss: 1059.5084\n",
      "Epoch 261/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 112.2213 - val_loss: 1060.2080\n",
      "Epoch 262/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 112.1053 - val_loss: 1060.8884\n",
      "Epoch 263/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 111.9950 - val_loss: 1061.5508\n",
      "Epoch 264/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 111.8901 - val_loss: 1062.1946\n",
      "Epoch 265/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 111.7905 - val_loss: 1062.8204\n",
      "Epoch 266/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 111.6955 - val_loss: 1063.4281\n",
      "Epoch 267/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 111.6055 - val_loss: 1064.0187\n",
      "Epoch 268/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 111.5198 - val_loss: 1064.5927\n",
      "Epoch 269/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 111.4383 - val_loss: 1065.1488\n",
      "Epoch 270/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 111.3608 - val_loss: 1065.6888\n",
      "Epoch 271/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 111.2873 - val_loss: 1066.2128\n",
      "Epoch 272/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 111.2172 - val_loss: 1066.7206\n",
      "Epoch 273/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 111.1507 - val_loss: 1067.2129\n",
      "Epoch 274/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 111.0874 - val_loss: 1067.6895\n",
      "Epoch 275/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 111.0274 - val_loss: 1068.1516\n",
      "Epoch 276/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.9702 - val_loss: 1068.5986\n",
      "Epoch 277/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.9159 - val_loss: 1069.0315\n",
      "Epoch 278/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.8643 - val_loss: 1069.4498\n",
      "Epoch 279/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 110.8152 - val_loss: 1069.8547\n",
      "Epoch 280/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.7686 - val_loss: 1070.2460\n",
      "Epoch 281/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.7243 - val_loss: 1070.6239\n",
      "Epoch 282/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.6822 - val_loss: 1070.9893\n",
      "Epoch 283/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 110.6422 - val_loss: 1071.3419\n",
      "Epoch 284/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.6041 - val_loss: 1071.6830\n",
      "Epoch 285/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.5679 - val_loss: 1072.0115\n",
      "Epoch 286/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.5335 - val_loss: 1072.3287\n",
      "Epoch 287/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.5009 - val_loss: 1072.6346\n",
      "Epoch 288/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.4698 - val_loss: 1072.9298\n",
      "Epoch 289/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.4402 - val_loss: 1073.2141\n",
      "Epoch 290/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.4122 - val_loss: 1073.4875\n",
      "Epoch 291/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 110.3855 - val_loss: 1073.7511\n",
      "Epoch 292/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.3602 - val_loss: 1074.0054\n",
      "Epoch 293/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.3361 - val_loss: 1074.2498\n",
      "Epoch 294/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.3131 - val_loss: 1074.4850\n",
      "Epoch 295/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.2914 - val_loss: 1074.7115\n",
      "Epoch 296/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.2706 - val_loss: 1074.9286\n",
      "Epoch 297/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 110.2510 - val_loss: 1075.1377\n",
      "Epoch 298/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 110.2323 - val_loss: 1075.3385\n",
      "Epoch 299/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.2145 - val_loss: 1075.5311\n",
      "Epoch 300/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.1977 - val_loss: 1075.7164\n",
      "Epoch 301/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.1816 - val_loss: 1075.8942\n",
      "Epoch 302/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.1663 - val_loss: 1076.0647\n",
      "Epoch 303/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.1518 - val_loss: 1076.2285\n",
      "Epoch 304/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.1381 - val_loss: 1076.3853\n",
      "Epoch 305/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.1250 - val_loss: 1076.5358\n",
      "Epoch 306/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.1126 - val_loss: 1076.6803\n",
      "Epoch 307/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.1008 - val_loss: 1076.8186\n",
      "Epoch 308/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.0895 - val_loss: 1076.9512\n",
      "Epoch 309/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.0789 - val_loss: 1077.0775\n",
      "Epoch 310/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 110.0688 - val_loss: 1077.1992\n",
      "Epoch 311/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 110.0592 - val_loss: 1077.3156\n",
      "Epoch 312/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 110.0500 - val_loss: 1077.4264\n",
      "Epoch 313/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 110.0414 - val_loss: 1077.5330\n",
      "Epoch 314/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.0331 - val_loss: 1077.6345\n",
      "Epoch 315/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.0254 - val_loss: 1077.7319\n",
      "Epoch 316/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.0179 - val_loss: 1077.8250\n",
      "Epoch 317/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.0109 - val_loss: 1077.9137\n",
      "Epoch 318/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.0042 - val_loss: 1077.9978\n",
      "Epoch 319/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9979 - val_loss: 1078.0787\n",
      "Epoch 320/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9920 - val_loss: 1078.1555\n",
      "Epoch 321/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 109.9863 - val_loss: 1078.2295\n",
      "Epoch 322/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9808 - val_loss: 1078.2994\n",
      "Epoch 323/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9758 - val_loss: 1078.3660\n",
      "Epoch 324/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9709 - val_loss: 1078.4297\n",
      "Epoch 325/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9664 - val_loss: 1078.4902\n",
      "Epoch 326/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9621 - val_loss: 1078.5483\n",
      "Epoch 327/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9580 - val_loss: 1078.6034\n",
      "Epoch 328/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9541 - val_loss: 1078.6561\n",
      "Epoch 329/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9504 - val_loss: 1078.7062\n",
      "Epoch 330/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9470 - val_loss: 1078.7537\n",
      "Epoch 331/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9437 - val_loss: 1078.7987\n",
      "Epoch 332/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9406 - val_loss: 1078.8417\n",
      "Epoch 333/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9377 - val_loss: 1078.8823\n",
      "Epoch 334/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9349 - val_loss: 1078.9213\n",
      "Epoch 335/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9324 - val_loss: 1078.9584\n",
      "Epoch 336/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9299 - val_loss: 1078.9935\n",
      "Epoch 337/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9276 - val_loss: 1079.0272\n",
      "Epoch 338/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9254 - val_loss: 1079.0590\n",
      "Epoch 339/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9233 - val_loss: 1079.0886\n",
      "Epoch 340/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9214 - val_loss: 1079.1169\n",
      "Epoch 341/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9196 - val_loss: 1079.1445\n",
      "Epoch 342/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9178 - val_loss: 1079.1699\n",
      "Epoch 343/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 109.9163 - val_loss: 1079.1948\n",
      "Epoch 344/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9147 - val_loss: 1079.2177\n",
      "Epoch 345/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9133 - val_loss: 1079.2397\n",
      "Epoch 346/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9120 - val_loss: 1079.2607\n",
      "Epoch 347/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9107 - val_loss: 1079.2804\n",
      "Epoch 348/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9096 - val_loss: 1079.2991\n",
      "Epoch 349/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9085 - val_loss: 1079.3165\n",
      "Epoch 350/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9074 - val_loss: 1079.3333\n",
      "Epoch 351/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9065 - val_loss: 1079.3494\n",
      "Epoch 352/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9056 - val_loss: 1079.3641\n",
      "Epoch 353/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9048 - val_loss: 1079.3788\n",
      "Epoch 354/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9040 - val_loss: 1079.3918\n",
      "Epoch 355/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9033 - val_loss: 1079.4043\n",
      "Epoch 356/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9026 - val_loss: 1079.4164\n",
      "Epoch 357/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9021 - val_loss: 1079.4276\n",
      "Epoch 358/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9015 - val_loss: 1079.4385\n",
      "Epoch 359/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 109.9009 - val_loss: 1079.4486\n",
      "Epoch 360/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9005 - val_loss: 1079.4580\n",
      "Epoch 361/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9000 - val_loss: 1079.4674\n",
      "Epoch 362/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8996 - val_loss: 1079.4757\n",
      "Epoch 363/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8993 - val_loss: 1079.4839\n",
      "Epoch 364/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8989 - val_loss: 1079.4916\n",
      "Epoch 365/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8986 - val_loss: 1079.4987\n",
      "Epoch 366/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8983 - val_loss: 1079.5056\n",
      "Epoch 367/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8980 - val_loss: 1079.5121\n",
      "Epoch 368/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8977 - val_loss: 1079.5176\n",
      "Epoch 369/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 109.8976 - val_loss: 1079.5231\n",
      "Epoch 370/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8974 - val_loss: 1079.5289\n",
      "Epoch 371/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8972 - val_loss: 1079.5342\n",
      "Epoch 372/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8971 - val_loss: 1079.5386\n",
      "Epoch 373/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8970 - val_loss: 1079.5431\n",
      "Epoch 374/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8969 - val_loss: 1079.5476\n",
      "Epoch 375/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8968 - val_loss: 1079.5514\n",
      "Epoch 376/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8967 - val_loss: 1079.5549\n",
      "Epoch 377/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8967 - val_loss: 1079.5586\n",
      "Epoch 378/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8966 - val_loss: 1079.5620\n",
      "Epoch 379/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8966 - val_loss: 1079.5647\n",
      "Epoch 380/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 109.8966 - val_loss: 1079.5676\n",
      "Epoch 381/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8966 - val_loss: 1079.5704\n",
      "Epoch 382/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8966 - val_loss: 1079.5732\n",
      "Epoch 383/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8966 - val_loss: 1079.5759\n",
      "Epoch 384/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8966 - val_loss: 1079.5784\n",
      "Epoch 385/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8966 - val_loss: 1079.5802\n",
      "Epoch 386/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8967 - val_loss: 1079.5826\n",
      "Epoch 387/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8967 - val_loss: 1079.5847\n",
      "Epoch 388/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8968 - val_loss: 1079.5864\n",
      "Epoch 389/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8968 - val_loss: 1079.5887\n",
      "Epoch 390/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8969 - val_loss: 1079.5901\n",
      "Epoch 391/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8969 - val_loss: 1079.5920\n",
      "Epoch 392/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 109.8970 - val_loss: 1079.5936\n",
      "Epoch 393/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8970 - val_loss: 1079.5951\n",
      "Epoch 394/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8971 - val_loss: 1079.5962\n",
      "Epoch 395/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8971 - val_loss: 1079.5974\n",
      "Epoch 396/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8973 - val_loss: 1079.5988\n",
      "Epoch 397/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8973 - val_loss: 1079.5996\n",
      "Epoch 398/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8975 - val_loss: 1079.6002\n",
      "Epoch 399/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8975 - val_loss: 1079.6013\n",
      "Epoch 400/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8976 - val_loss: 1079.6022\n",
      "Epoch 401/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8978 - val_loss: 1079.6027\n",
      "Epoch 402/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8979 - val_loss: 1079.6034\n",
      "Epoch 403/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8980 - val_loss: 1079.6041\n",
      "Epoch 404/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8981 - val_loss: 1079.6050\n",
      "Epoch 405/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8982 - val_loss: 1079.6056\n",
      "Epoch 406/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8983 - val_loss: 1079.6061\n",
      "Epoch 407/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8984 - val_loss: 1079.6068\n",
      "Epoch 408/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8985 - val_loss: 1079.6080\n",
      "Epoch 409/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8986 - val_loss: 1079.6088\n",
      "Epoch 410/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8987 - val_loss: 1079.6091\n",
      "Epoch 411/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8987 - val_loss: 1079.6094\n",
      "Epoch 412/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8988 - val_loss: 1079.6096\n",
      "Epoch 413/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.8990 - val_loss: 1079.6101\n",
      "Epoch 414/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8991 - val_loss: 1079.6108\n",
      "Epoch 415/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8992 - val_loss: 1079.6113\n",
      "Epoch 416/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8993 - val_loss: 1079.6116\n",
      "Epoch 417/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.8994 - val_loss: 1079.6121\n",
      "Epoch 418/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8995 - val_loss: 1079.6121\n",
      "Epoch 419/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.8996 - val_loss: 1079.6123\n",
      "Epoch 420/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.8997 - val_loss: 1079.6123\n",
      "Epoch 421/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 109.8998 - val_loss: 1079.6123\n",
      "Epoch 422/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 109.9000 - val_loss: 1079.6125\n",
      "Epoch 423/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9001 - val_loss: 1079.6128\n",
      "Epoch 424/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9002 - val_loss: 1079.6130\n",
      "Epoch 425/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9003 - val_loss: 1079.6129\n",
      "Epoch 426/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9004 - val_loss: 1079.6134\n",
      "Epoch 427/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9005 - val_loss: 1079.6140\n",
      "Epoch 428/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9005 - val_loss: 1079.6146\n",
      "Epoch 429/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9007 - val_loss: 1079.6150\n",
      "Epoch 430/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9007 - val_loss: 1079.6152\n",
      "Epoch 431/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.9008 - val_loss: 1079.6154\n",
      "Epoch 432/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.9008 - val_loss: 1079.6154\n",
      "Epoch 433/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9010 - val_loss: 1079.6156\n",
      "Epoch 434/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9011 - val_loss: 1079.6158\n",
      "Epoch 435/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9011 - val_loss: 1079.6158\n",
      "Epoch 436/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9013 - val_loss: 1079.6160\n",
      "Epoch 437/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9013 - val_loss: 1079.6161\n",
      "Epoch 438/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9014 - val_loss: 1079.6160\n",
      "Epoch 439/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9015 - val_loss: 1079.6163\n",
      "Epoch 440/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9016 - val_loss: 1079.6163\n",
      "Epoch 441/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.9017 - val_loss: 1079.6166\n",
      "Epoch 442/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9017 - val_loss: 1079.6167\n",
      "Epoch 443/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9019 - val_loss: 1079.6173\n",
      "Epoch 444/500\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 109.9019 - val_loss: 1079.6173\n",
      "Epoch 445/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.9020 - val_loss: 1079.6173\n",
      "Epoch 446/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9021 - val_loss: 1079.6174\n",
      "Epoch 447/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9021 - val_loss: 1079.6173\n",
      "Epoch 448/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.9022 - val_loss: 1079.6174\n",
      "Epoch 449/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9023 - val_loss: 1079.6178\n",
      "Epoch 450/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9024 - val_loss: 1079.6178\n",
      "Epoch 451/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9024 - val_loss: 1079.6177\n",
      "Epoch 452/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9025 - val_loss: 1079.6180\n",
      "Epoch 453/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9026 - val_loss: 1079.6180\n",
      "Epoch 454/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9027 - val_loss: 1079.6180\n",
      "Epoch 455/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9028 - val_loss: 1079.6180\n",
      "Epoch 456/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.9028 - val_loss: 1079.6183\n",
      "Epoch 457/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 109.9029 - val_loss: 1079.6185\n",
      "Epoch 458/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9029 - val_loss: 1079.6187\n",
      "Epoch 459/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.9030 - val_loss: 1079.6187\n",
      "Epoch 460/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9031 - val_loss: 1079.6188\n",
      "Epoch 461/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9031 - val_loss: 1079.6187\n",
      "Epoch 462/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9032 - val_loss: 1079.6189\n",
      "Epoch 463/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9032 - val_loss: 1079.6188\n",
      "Epoch 464/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9033 - val_loss: 1079.6189\n",
      "Epoch 465/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 109.9034 - val_loss: 1079.6189\n",
      "Epoch 466/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.9034 - val_loss: 1079.6190\n",
      "Epoch 467/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.9034 - val_loss: 1079.6189\n",
      "Epoch 468/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9035 - val_loss: 1079.6188\n",
      "Epoch 469/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9036 - val_loss: 1079.6188\n",
      "Epoch 470/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9037 - val_loss: 1079.6187\n",
      "Epoch 471/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9037 - val_loss: 1079.6187\n",
      "Epoch 472/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9038 - val_loss: 1079.6185\n",
      "Epoch 473/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9038 - val_loss: 1079.6185\n",
      "Epoch 474/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9039 - val_loss: 1079.6185\n",
      "Epoch 475/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9040 - val_loss: 1079.6188\n",
      "Epoch 476/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9039 - val_loss: 1079.6187\n",
      "Epoch 477/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9040 - val_loss: 1079.6183\n",
      "Epoch 478/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9042 - val_loss: 1079.6185\n",
      "Epoch 479/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9041 - val_loss: 1079.6189\n",
      "Epoch 480/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9041 - val_loss: 1079.6188\n",
      "Epoch 481/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9043 - val_loss: 1079.6188\n",
      "Epoch 482/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9042 - val_loss: 1079.6187\n",
      "Epoch 483/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9043 - val_loss: 1079.6187\n",
      "Epoch 484/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9044 - val_loss: 1079.6188\n",
      "Epoch 485/500\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 109.9044 - val_loss: 1079.6190\n",
      "Epoch 486/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.9044 - val_loss: 1079.6190\n",
      "Epoch 487/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.9045 - val_loss: 1079.6191\n",
      "Epoch 488/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9046 - val_loss: 1079.6193\n",
      "Epoch 489/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9046 - val_loss: 1079.6195\n",
      "Epoch 490/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9046 - val_loss: 1079.6195\n",
      "Epoch 491/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.9047 - val_loss: 1079.6196\n",
      "Epoch 492/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9047 - val_loss: 1079.6198\n",
      "Epoch 493/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9048 - val_loss: 1079.6199\n",
      "Epoch 494/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9047 - val_loss: 1079.6198\n",
      "Epoch 495/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9048 - val_loss: 1079.6198\n",
      "Epoch 496/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9049 - val_loss: 1079.6199\n",
      "Epoch 497/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9049 - val_loss: 1079.6198\n",
      "Epoch 498/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9049 - val_loss: 1079.6200\n",
      "Epoch 499/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 109.9049 - val_loss: 1079.6201\n",
      "Epoch 500/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 109.9050 - val_loss: 1079.6204\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(89.3180, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0010, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(-0.0204, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.2194, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 481ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.30498903e+01, 7.29229995e+01, 7.27961088e+01, 7.26692180e+01,\n",
       "        7.25423273e+01, 7.24154365e+01, 7.22885457e+01, 1.89062133e-01,\n",
       "        8.33228588e-01, 5.38449287e-01, 0.00000000e+00, 6.77165926e-01,\n",
       "        0.00000000e+00, 7.45491597e+01, 7.43642857e+01, 7.41794118e+01,\n",
       "        7.39945378e+01, 7.38096639e+01, 7.36247899e+01, 7.34587605e+01,\n",
       "        7.33318697e+01, 7.32049790e+01, 7.30780882e+01, 7.29511975e+01,\n",
       "        7.28243067e+01, 7.26974160e+01, 7.25705252e+01, 7.24436345e+01,\n",
       "        7.23167437e+01, 7.21898529e+01, 7.20629622e+01, 7.19760714e+01,\n",
       "        7.19432983e+01, 7.19105252e+01, 7.18777521e+01, 7.18449790e+01,\n",
       "        7.18122059e+01, 7.17794328e+01, 7.17466597e+01, 7.17138865e+01,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.48647080e-02, 2.48702437e-01,\n",
       "        0.00000000e+00, 7.30836868e-01, 1.36577690e+00, 7.27256139e+01,\n",
       "        7.25987232e+01, 7.24718324e+01, 7.23449416e+01, 7.22180509e+01,\n",
       "        7.20911601e+01, 7.19833543e+01, 7.19505812e+01, 7.19178081e+01,\n",
       "        7.18850350e+01, 7.18522619e+01, 7.18194888e+01, 7.17867157e+01,\n",
       "        7.17539426e+01, 7.17211695e+01, 7.71017974e+01, 7.69253268e+01,\n",
       "        7.66408964e+01, 7.63383754e+01, 7.60358543e+01, 7.57333333e+01,\n",
       "        7.52064893e+01, 7.46518674e+01, 7.40972456e+01, 8.65812778e-01,\n",
       "        0.00000000e+00, 7.35769043e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "        9.62374210e-01, 0.00000000e+00, 0.00000000e+00, 7.65121341e-01,\n",
       "        7.12379684e+01, 0.00000000e+00, 8.44314992e-02, 0.00000000e+00,\n",
       "        2.21041411e-01, 0.00000000e+00, 4.45122272e-01, 0.00000000e+00,\n",
       "        2.25800082e-01, 2.75238454e-01, 2.69994140e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.86738712e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([68.07170868, 68.0605042 , 68.04929972, 68.03809524, 68.02689076,\n",
       "       68.01568627, 68.00448179, 67.99327731, 67.98207283, 67.97086835,\n",
       "       67.95966387, 67.94845938, 67.9372549 , 67.92605042, 67.91484594,\n",
       "       67.90364146, 67.89243697, 67.88123249, 67.87002801, 67.85882353,\n",
       "       67.84761905, 67.83641457, 67.82521008, 67.8140056 , 67.80280112,\n",
       "       67.79159664, 67.78039216, 67.76918768, 67.75798319, 67.74677871,\n",
       "       67.73557423, 67.72436975, 67.71316527, 67.70196078, 67.6907563 ,\n",
       "       67.67955182, 67.66834734, 67.65714286, 67.64593838, 67.63473389,\n",
       "       67.62352941, 67.61232493, 67.60112045, 67.58991597, 67.57871148,\n",
       "       67.567507  , 67.55630252, 67.54509804, 67.53389356, 67.52268908,\n",
       "       67.51148459, 67.50028011, 67.48907563, 67.47787115, 67.46666667,\n",
       "       67.45546218, 67.4442577 , 67.43305322, 67.42184874, 67.41064426,\n",
       "       67.39943978, 67.38823529, 67.37703081, 67.36582633, 67.35462185,\n",
       "       67.34341737, 67.33221289, 67.3210084 , 67.30980392, 67.29877451,\n",
       "       67.28897059, 67.27916667, 67.26936275, 67.25955882, 67.2497549 ,\n",
       "       67.23995098, 67.23014706, 67.22034314, 67.21053922, 67.20073529,\n",
       "       67.19093137, 67.18112745, 67.17132353, 67.16151961, 67.15171569,\n",
       "       67.14191176, 67.13210784, 67.12230392, 67.1125    , 67.10269608,\n",
       "       67.09289216, 67.08308824, 67.07328431, 67.06348039, 67.05367647,\n",
       "       67.04387255, 67.03406863, 67.02426471, 67.01446078, 67.00465686])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.668254406544605\n",
      "30.074423398272224\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
