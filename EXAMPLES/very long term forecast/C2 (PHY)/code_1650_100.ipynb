{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "1745    66.498786\n",
       "1746    66.491317\n",
       "1747    66.483847\n",
       "1748    66.476377\n",
       "1749    66.468908\n",
       "Name: C2, Length: 1750, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c2_interpolated_1650_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "1645     0.000000\n",
       "1646     0.353423\n",
       "1647     0.461028\n",
       "1648     0.000000\n",
       "1649     0.116272\n",
       "Name: C2, Length: 1650, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1650)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.500000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.275910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.051821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.827731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.603641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     90.500000  0.000298\n",
       "1     90.275910  0.000298\n",
       "2     90.051821  0.000297\n",
       "3     89.827731  0.000297\n",
       "4     89.603641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.1552  # Value for C0\n",
    "K0 = -0.0026  # Value for K0\n",
    "K1 = -0.0004  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0102    # Value for b\n",
    "c = 2.8734    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    90.500000    90.275910    90.051821    89.827731    89.603641   \n",
      "351    90.275910    90.051821    89.827731    89.603641    89.379552   \n",
      "352    90.051821    89.827731    89.603641    89.379552    89.155462   \n",
      "353    89.827731    89.603641    89.379552    89.155462    88.931373   \n",
      "354    89.603641    89.379552    89.155462    88.931373    88.707283   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    89.379552    89.155462    88.931373    88.707283    88.494958  ...   \n",
      "351    89.155462    88.931373    88.707283    88.494958    88.427731  ...   \n",
      "352    88.931373    88.707283    88.494958    88.427731    88.360504  ...   \n",
      "353    88.707283    88.494958    88.427731    88.360504    88.293277  ...   \n",
      "354    88.494958    88.427731    88.360504    88.293277    88.226050  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   79.071008    0.000263   79.054202    0.000263   79.037395    0.000263   \n",
      "351   79.054202    0.000263   79.037395    0.000263   79.020588    0.000262   \n",
      "352   79.037395    0.000263   79.020588    0.000262   79.003782    0.000262   \n",
      "353   79.020588    0.000262   79.003782    0.000262   78.986975    0.000262   \n",
      "354   79.003782    0.000262   78.986975    0.000262   78.970168    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   79.020588    0.000262   79.003782    0.000262  \n",
      "351   79.003782    0.000262   78.986975    0.000262  \n",
      "352   78.986975    0.000262   78.970168    0.000262  \n",
      "353   78.970168    0.000262   78.953361    0.000262  \n",
      "354   78.953361    0.000262   78.936555    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 1, 251) (1200, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "15/15 [==============================] - 2s 37ms/step - loss: 5563.2998 - val_loss: 4207.3970\n",
      "Epoch 2/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5486.8140 - val_loss: 4162.5176\n",
      "Epoch 3/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5429.1895 - val_loss: 4118.0630\n",
      "Epoch 4/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5371.9360 - val_loss: 4074.0156\n",
      "Epoch 5/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5315.1265 - val_loss: 4030.3672\n",
      "Epoch 6/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5258.7729 - val_loss: 3987.0920\n",
      "Epoch 7/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5200.5103 - val_loss: 3940.5654\n",
      "Epoch 8/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5140.0249 - val_loss: 3895.8345\n",
      "Epoch 9/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 5081.7578 - val_loss: 3851.7266\n",
      "Epoch 10/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5024.3354 - val_loss: 3808.2583\n",
      "Epoch 11/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4967.6890 - val_loss: 3765.3550\n",
      "Epoch 12/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4911.7168 - val_loss: 3722.9595\n",
      "Epoch 13/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 4856.3525 - val_loss: 3681.0352\n",
      "Epoch 14/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4801.5537 - val_loss: 3639.5540\n",
      "Epoch 15/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4747.2915 - val_loss: 3598.4795\n",
      "Epoch 16/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4692.5405 - val_loss: 3545.3992\n",
      "Epoch 17/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4622.1299 - val_loss: 3501.8845\n",
      "Epoch 18/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4564.9956 - val_loss: 3458.8521\n",
      "Epoch 19/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4508.8647 - val_loss: 3416.7495\n",
      "Epoch 20/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4453.7969 - val_loss: 3375.4370\n",
      "Epoch 21/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4399.6206 - val_loss: 3334.7954\n",
      "Epoch 22/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4346.2080 - val_loss: 3294.7463\n",
      "Epoch 23/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4293.4795 - val_loss: 3255.2397\n",
      "Epoch 24/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4241.3784 - val_loss: 3216.2407\n",
      "Epoch 25/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4189.8691 - val_loss: 3177.7241\n",
      "Epoch 26/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 4138.9214 - val_loss: 3139.6697\n",
      "Epoch 27/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4088.5151 - val_loss: 3102.0632\n",
      "Epoch 28/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4038.6313 - val_loss: 3064.8916\n",
      "Epoch 29/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3989.2568 - val_loss: 3028.1443\n",
      "Epoch 30/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3940.3801 - val_loss: 2991.8137\n",
      "Epoch 31/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3891.9897 - val_loss: 2955.8906\n",
      "Epoch 32/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3844.0779 - val_loss: 2920.3696\n",
      "Epoch 33/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3796.6362 - val_loss: 2885.2434\n",
      "Epoch 34/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3749.6567 - val_loss: 2850.5068\n",
      "Epoch 35/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3703.1343 - val_loss: 2816.1548\n",
      "Epoch 36/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3657.0615 - val_loss: 2782.1824\n",
      "Epoch 37/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3611.4348 - val_loss: 2748.5854\n",
      "Epoch 38/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3566.2471 - val_loss: 2715.3596\n",
      "Epoch 39/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3521.4946 - val_loss: 2682.5007\n",
      "Epoch 40/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3477.1726 - val_loss: 2650.0056\n",
      "Epoch 41/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3433.2769 - val_loss: 2617.8701\n",
      "Epoch 42/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3389.8025 - val_loss: 2586.0916\n",
      "Epoch 43/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3346.7466 - val_loss: 2554.6653\n",
      "Epoch 44/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3304.1055 - val_loss: 2523.5898\n",
      "Epoch 45/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3261.8740 - val_loss: 2492.8608\n",
      "Epoch 46/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3220.0500 - val_loss: 2462.4756\n",
      "Epoch 47/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3178.6296 - val_loss: 2432.4307\n",
      "Epoch 48/500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 3137.6091 - val_loss: 2402.7249\n",
      "Epoch 49/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3096.9858 - val_loss: 2373.3538\n",
      "Epoch 50/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3056.7568 - val_loss: 2344.3149\n",
      "Epoch 51/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3016.9180 - val_loss: 2315.6062\n",
      "Epoch 52/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2977.4666 - val_loss: 2287.2251\n",
      "Epoch 53/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2938.4009 - val_loss: 2259.1677\n",
      "Epoch 54/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2899.7158 - val_loss: 2231.4329\n",
      "Epoch 55/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2861.4104 - val_loss: 2204.0173\n",
      "Epoch 56/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2823.4810 - val_loss: 2176.9192\n",
      "Epoch 57/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2785.9250 - val_loss: 2150.1357\n",
      "Epoch 58/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2748.7395 - val_loss: 2123.6643\n",
      "Epoch 59/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2711.9219 - val_loss: 2097.5029\n",
      "Epoch 60/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2675.4692 - val_loss: 2071.6492\n",
      "Epoch 61/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2639.3796 - val_loss: 2046.1000\n",
      "Epoch 62/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2603.6499 - val_loss: 2020.8544\n",
      "Epoch 63/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2568.2776 - val_loss: 1995.9097\n",
      "Epoch 64/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2533.2605 - val_loss: 1971.2626\n",
      "Epoch 65/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2498.5952 - val_loss: 1946.9120\n",
      "Epoch 66/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2464.2803 - val_loss: 1922.8557\n",
      "Epoch 67/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2430.3130 - val_loss: 1899.0914\n",
      "Epoch 68/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 2396.6909 - val_loss: 1875.6165\n",
      "Epoch 69/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2363.4109 - val_loss: 1852.4286\n",
      "Epoch 70/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2330.4714 - val_loss: 1829.5267\n",
      "Epoch 71/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2297.8699 - val_loss: 1806.9078\n",
      "Epoch 72/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2265.6038 - val_loss: 1784.5701\n",
      "Epoch 73/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2233.6709 - val_loss: 1762.5112\n",
      "Epoch 74/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2202.0691 - val_loss: 1740.7297\n",
      "Epoch 75/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2170.7959 - val_loss: 1719.2229\n",
      "Epoch 76/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2139.8486 - val_loss: 1697.9891\n",
      "Epoch 77/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2109.2258 - val_loss: 1677.0259\n",
      "Epoch 78/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2078.9248 - val_loss: 1656.3319\n",
      "Epoch 79/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 2048.9436 - val_loss: 1635.9049\n",
      "Epoch 80/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2019.2798 - val_loss: 1615.7427\n",
      "Epoch 81/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1989.9310 - val_loss: 1595.8427\n",
      "Epoch 82/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1960.8950 - val_loss: 1576.2045\n",
      "Epoch 83/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1932.1704 - val_loss: 1556.8245\n",
      "Epoch 84/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 1903.7544 - val_loss: 1537.7018\n",
      "Epoch 85/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1875.6442 - val_loss: 1518.8341\n",
      "Epoch 86/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1847.8392 - val_loss: 1500.2198\n",
      "Epoch 87/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1820.3364 - val_loss: 1481.8567\n",
      "Epoch 88/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1793.1339 - val_loss: 1463.7429\n",
      "Epoch 89/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1766.2297 - val_loss: 1445.8768\n",
      "Epoch 90/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1739.6215 - val_loss: 1428.2557\n",
      "Epoch 91/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1713.3073 - val_loss: 1410.8783\n",
      "Epoch 92/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1687.2849 - val_loss: 1393.7437\n",
      "Epoch 93/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1661.5526 - val_loss: 1376.8480\n",
      "Epoch 94/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1636.1080 - val_loss: 1360.1909\n",
      "Epoch 95/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1610.9492 - val_loss: 1343.7698\n",
      "Epoch 96/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1586.0743 - val_loss: 1327.5834\n",
      "Epoch 97/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1561.4812 - val_loss: 1311.6288\n",
      "Epoch 98/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1537.1680 - val_loss: 1295.9055\n",
      "Epoch 99/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1513.1327 - val_loss: 1280.4114\n",
      "Epoch 100/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1489.3728 - val_loss: 1265.1442\n",
      "Epoch 101/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1465.8867 - val_loss: 1250.1021\n",
      "Epoch 102/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1442.6730 - val_loss: 1235.2833\n",
      "Epoch 103/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1419.7290 - val_loss: 1220.6868\n",
      "Epoch 104/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1397.0532 - val_loss: 1206.3099\n",
      "Epoch 105/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1374.6432 - val_loss: 1192.1508\n",
      "Epoch 106/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1352.4976 - val_loss: 1178.2091\n",
      "Epoch 107/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1330.6141 - val_loss: 1164.4817\n",
      "Epoch 108/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1308.9910 - val_loss: 1150.9666\n",
      "Epoch 109/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1287.6266 - val_loss: 1137.6641\n",
      "Epoch 110/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1266.5190 - val_loss: 1124.5702\n",
      "Epoch 111/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1245.6660 - val_loss: 1111.6838\n",
      "Epoch 112/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1225.0659 - val_loss: 1099.0040\n",
      "Epoch 113/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1204.7167 - val_loss: 1086.5282\n",
      "Epoch 114/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1184.6165 - val_loss: 1074.2545\n",
      "Epoch 115/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1164.7633 - val_loss: 1062.1819\n",
      "Epoch 116/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 1145.1554 - val_loss: 1050.3083\n",
      "Epoch 117/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1125.7910 - val_loss: 1038.6323\n",
      "Epoch 118/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1106.6683 - val_loss: 1027.1517\n",
      "Epoch 119/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1087.7855 - val_loss: 1015.8653\n",
      "Epoch 120/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1069.1407 - val_loss: 1004.7708\n",
      "Epoch 121/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1050.7319 - val_loss: 993.8673\n",
      "Epoch 122/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1032.5577 - val_loss: 983.1526\n",
      "Epoch 123/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1014.6158 - val_loss: 972.6252\n",
      "Epoch 124/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 996.9048 - val_loss: 962.2831\n",
      "Epoch 125/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 979.4229 - val_loss: 952.1254\n",
      "Epoch 126/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 962.1680 - val_loss: 942.1494\n",
      "Epoch 127/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 945.1384 - val_loss: 932.3542\n",
      "Epoch 128/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 928.3328 - val_loss: 922.7383\n",
      "Epoch 129/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 911.7487 - val_loss: 913.2994\n",
      "Epoch 130/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 895.3846 - val_loss: 904.0359\n",
      "Epoch 131/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 879.2391 - val_loss: 894.9467\n",
      "Epoch 132/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 863.3099 - val_loss: 886.0297\n",
      "Epoch 133/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 847.5956 - val_loss: 877.2833\n",
      "Epoch 134/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 832.0941 - val_loss: 868.7065\n",
      "Epoch 135/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 816.8044 - val_loss: 860.2966\n",
      "Epoch 136/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 801.7239 - val_loss: 852.0529\n",
      "Epoch 137/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 786.8514 - val_loss: 843.9733\n",
      "Epoch 138/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 772.1852 - val_loss: 836.0563\n",
      "Epoch 139/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 757.7231 - val_loss: 828.3001\n",
      "Epoch 140/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 743.4637 - val_loss: 820.7036\n",
      "Epoch 141/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 729.4054 - val_loss: 813.2645\n",
      "Epoch 142/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 715.5460 - val_loss: 805.9819\n",
      "Epoch 143/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 701.8842 - val_loss: 798.8536\n",
      "Epoch 144/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 688.4184 - val_loss: 791.8785\n",
      "Epoch 145/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 675.1467 - val_loss: 785.0545\n",
      "Epoch 146/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 662.0673 - val_loss: 778.3807\n",
      "Epoch 147/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 649.1787 - val_loss: 771.8547\n",
      "Epoch 148/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 636.4791 - val_loss: 765.4755\n",
      "Epoch 149/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 623.9670 - val_loss: 759.2415\n",
      "Epoch 150/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 611.6405 - val_loss: 753.1505\n",
      "Epoch 151/500\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 599.4982 - val_loss: 747.2018\n",
      "Epoch 152/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 587.5384 - val_loss: 741.3933\n",
      "Epoch 153/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 575.7590 - val_loss: 735.7235\n",
      "Epoch 154/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 564.1586 - val_loss: 730.1909\n",
      "Epoch 155/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 552.7357 - val_loss: 724.7940\n",
      "Epoch 156/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 541.4886 - val_loss: 719.5310\n",
      "Epoch 157/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 530.4155 - val_loss: 714.4006\n",
      "Epoch 158/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 519.5149 - val_loss: 709.4014\n",
      "Epoch 159/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 508.7853 - val_loss: 704.5317\n",
      "Epoch 160/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 498.2248 - val_loss: 699.7896\n",
      "Epoch 161/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 487.8319 - val_loss: 695.1739\n",
      "Epoch 162/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 477.6047 - val_loss: 690.6830\n",
      "Epoch 163/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 467.5419 - val_loss: 686.3156\n",
      "Epoch 164/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 457.6419 - val_loss: 682.0696\n",
      "Epoch 165/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 447.9030 - val_loss: 677.9442\n",
      "Epoch 166/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 438.3235 - val_loss: 673.9373\n",
      "Epoch 167/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 428.9021 - val_loss: 670.0475\n",
      "Epoch 168/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 419.6367 - val_loss: 666.2734\n",
      "Epoch 169/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 410.5259 - val_loss: 662.6135\n",
      "Epoch 170/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 401.5682 - val_loss: 659.0663\n",
      "Epoch 171/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 392.7620 - val_loss: 655.6300\n",
      "Epoch 172/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 384.1058 - val_loss: 652.3035\n",
      "Epoch 173/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 375.5977 - val_loss: 649.0851\n",
      "Epoch 174/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 367.2364 - val_loss: 645.9733\n",
      "Epoch 175/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 359.0201 - val_loss: 642.9666\n",
      "Epoch 176/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 350.9474 - val_loss: 640.0636\n",
      "Epoch 177/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 343.0167 - val_loss: 637.2626\n",
      "Epoch 178/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 335.2265 - val_loss: 634.5624\n",
      "Epoch 179/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 327.5752 - val_loss: 631.9615\n",
      "Epoch 180/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 320.0611 - val_loss: 629.4583\n",
      "Epoch 181/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 312.6830 - val_loss: 627.0513\n",
      "Epoch 182/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 305.4390 - val_loss: 624.7393\n",
      "Epoch 183/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 298.3279 - val_loss: 622.5204\n",
      "Epoch 184/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 291.3477 - val_loss: 620.3935\n",
      "Epoch 185/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 284.4973 - val_loss: 618.3571\n",
      "Epoch 186/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 277.7748 - val_loss: 616.4095\n",
      "Epoch 187/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 271.1790 - val_loss: 614.5495\n",
      "Epoch 188/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 264.7080 - val_loss: 612.7755\n",
      "Epoch 189/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 258.3607 - val_loss: 611.0865\n",
      "Epoch 190/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 252.1355 - val_loss: 609.4805\n",
      "Epoch 191/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 246.0308 - val_loss: 607.9565\n",
      "Epoch 192/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 240.0451 - val_loss: 606.5128\n",
      "Epoch 193/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 234.1770 - val_loss: 605.1481\n",
      "Epoch 194/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 228.4250 - val_loss: 603.8611\n",
      "Epoch 195/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 222.7877 - val_loss: 602.6503\n",
      "Epoch 196/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 217.2632 - val_loss: 601.5142\n",
      "Epoch 197/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 211.8505 - val_loss: 600.4515\n",
      "Epoch 198/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 206.5480 - val_loss: 599.4609\n",
      "Epoch 199/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 201.3540 - val_loss: 598.5409\n",
      "Epoch 200/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 196.2675 - val_loss: 597.6904\n",
      "Epoch 201/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 191.2866 - val_loss: 596.9075\n",
      "Epoch 202/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 186.4100 - val_loss: 596.1913\n",
      "Epoch 203/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 181.6365 - val_loss: 595.5403\n",
      "Epoch 204/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 176.9646 - val_loss: 594.9532\n",
      "Epoch 205/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 172.3928 - val_loss: 594.4286\n",
      "Epoch 206/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 167.9193 - val_loss: 593.9651\n",
      "Epoch 207/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 163.5431 - val_loss: 593.5616\n",
      "Epoch 208/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 159.2628 - val_loss: 593.2166\n",
      "Epoch 209/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 155.0771 - val_loss: 592.9288\n",
      "Epoch 210/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 150.9844 - val_loss: 592.6970\n",
      "Epoch 211/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 146.9834 - val_loss: 592.5198\n",
      "Epoch 212/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 143.0727 - val_loss: 592.3961\n",
      "Epoch 213/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 139.2509 - val_loss: 592.3242\n",
      "Epoch 214/500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 135.5166 - val_loss: 592.3033\n",
      "Epoch 215/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 131.8685 - val_loss: 592.3318\n",
      "Epoch 216/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 128.3051 - val_loss: 592.4086\n",
      "Epoch 217/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 124.8253 - val_loss: 592.5325\n",
      "Epoch 218/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 121.4277 - val_loss: 592.7021\n",
      "Epoch 219/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 118.1110 - val_loss: 592.9164\n",
      "Epoch 220/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 114.8739 - val_loss: 593.1740\n",
      "Epoch 221/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 111.7149 - val_loss: 593.4738\n",
      "Epoch 222/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 108.6329 - val_loss: 593.8145\n",
      "Epoch 223/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 105.6265 - val_loss: 594.1949\n",
      "Epoch 224/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 102.6944 - val_loss: 594.6140\n",
      "Epoch 225/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 99.8355 - val_loss: 595.0703\n",
      "Epoch 226/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 97.0483 - val_loss: 595.5630\n",
      "Epoch 227/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 94.3316 - val_loss: 596.0909\n",
      "Epoch 228/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 91.6841 - val_loss: 596.6526\n",
      "Epoch 229/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 89.1049 - val_loss: 597.2471\n",
      "Epoch 230/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 86.5925 - val_loss: 597.8734\n",
      "Epoch 231/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 84.1456 - val_loss: 598.5302\n",
      "Epoch 232/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 81.7632 - val_loss: 599.2166\n",
      "Epoch 233/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 79.4441 - val_loss: 599.9315\n",
      "Epoch 234/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 77.1867 - val_loss: 600.6736\n",
      "Epoch 235/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 74.9904 - val_loss: 601.4421\n",
      "Epoch 236/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 72.8538 - val_loss: 602.2358\n",
      "Epoch 237/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 70.7757 - val_loss: 603.0536\n",
      "Epoch 238/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 68.7551 - val_loss: 603.8948\n",
      "Epoch 239/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 66.7906 - val_loss: 604.7581\n",
      "Epoch 240/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 64.8811 - val_loss: 605.6426\n",
      "Epoch 241/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 63.0256 - val_loss: 606.5472\n",
      "Epoch 242/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 61.2230 - val_loss: 607.4710\n",
      "Epoch 243/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 59.4723 - val_loss: 608.4131\n",
      "Epoch 244/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 57.7721 - val_loss: 609.3727\n",
      "Epoch 245/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 56.1218 - val_loss: 610.3483\n",
      "Epoch 246/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 54.5200 - val_loss: 611.3395\n",
      "Epoch 247/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 52.9657 - val_loss: 612.3453\n",
      "Epoch 248/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 51.4579 - val_loss: 613.3646\n",
      "Epoch 249/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 49.9956 - val_loss: 614.3967\n",
      "Epoch 250/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 48.5776 - val_loss: 615.4412\n",
      "Epoch 251/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 47.2030 - val_loss: 616.4961\n",
      "Epoch 252/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 45.8711 - val_loss: 617.5615\n",
      "Epoch 253/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 44.5808 - val_loss: 618.6360\n",
      "Epoch 254/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 43.3309 - val_loss: 619.7193\n",
      "Epoch 255/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 42.1205 - val_loss: 620.8104\n",
      "Epoch 256/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 40.9488 - val_loss: 621.9084\n",
      "Epoch 257/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 39.8147 - val_loss: 623.0129\n",
      "Epoch 258/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 38.7172 - val_loss: 624.1229\n",
      "Epoch 259/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 37.6558 - val_loss: 625.2375\n",
      "Epoch 260/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 36.6293 - val_loss: 626.3564\n",
      "Epoch 261/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 35.6370 - val_loss: 627.4783\n",
      "Epoch 262/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 34.6780 - val_loss: 628.6028\n",
      "Epoch 263/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 33.7514 - val_loss: 629.7297\n",
      "Epoch 264/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 32.8562 - val_loss: 630.8575\n",
      "Epoch 265/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 31.9918 - val_loss: 631.9863\n",
      "Epoch 266/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 31.1574 - val_loss: 633.1151\n",
      "Epoch 267/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 30.3520 - val_loss: 634.2430\n",
      "Epoch 268/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 29.5751 - val_loss: 635.3699\n",
      "Epoch 269/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 28.8258 - val_loss: 636.4951\n",
      "Epoch 270/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 28.1032 - val_loss: 637.6182\n",
      "Epoch 271/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 27.4066 - val_loss: 638.7381\n",
      "Epoch 272/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 26.7354 - val_loss: 639.8549\n",
      "Epoch 273/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 26.0888 - val_loss: 640.9675\n",
      "Epoch 274/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.4661 - val_loss: 642.0758\n",
      "Epoch 275/500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 24.8666 - val_loss: 643.1794\n",
      "Epoch 276/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 24.2896 - val_loss: 644.2773\n",
      "Epoch 277/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 23.7345 - val_loss: 645.3694\n",
      "Epoch 278/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 23.2005 - val_loss: 646.4554\n",
      "Epoch 279/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 22.6871 - val_loss: 647.5344\n",
      "Epoch 280/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 22.1937 - val_loss: 648.6064\n",
      "Epoch 281/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 21.7196 - val_loss: 649.6706\n",
      "Epoch 282/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 21.2642 - val_loss: 650.7272\n",
      "Epoch 283/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 20.8268 - val_loss: 651.7757\n",
      "Epoch 284/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 20.4070 - val_loss: 652.8152\n",
      "Epoch 285/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 20.0042 - val_loss: 653.8460\n",
      "Epoch 286/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 19.6177 - val_loss: 654.8674\n",
      "Epoch 287/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 19.2472 - val_loss: 655.8793\n",
      "Epoch 288/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 18.8920 - val_loss: 656.8814\n",
      "Epoch 289/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.5516 - val_loss: 657.8730\n",
      "Epoch 290/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 18.2256 - val_loss: 658.8541\n",
      "Epoch 291/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 17.9134 - val_loss: 659.8251\n",
      "Epoch 292/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 17.6145 - val_loss: 660.7844\n",
      "Epoch 293/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 17.3286 - val_loss: 661.7327\n",
      "Epoch 294/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 17.0550 - val_loss: 662.6699\n",
      "Epoch 295/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.7935 - val_loss: 663.5955\n",
      "Epoch 296/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.5435 - val_loss: 664.5092\n",
      "Epoch 297/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.3046 - val_loss: 665.4111\n",
      "Epoch 298/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.0765 - val_loss: 666.3011\n",
      "Epoch 299/500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 15.8586 - val_loss: 667.1789\n",
      "Epoch 300/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.6506 - val_loss: 668.0443\n",
      "Epoch 301/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.4522 - val_loss: 668.8973\n",
      "Epoch 302/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.2630 - val_loss: 669.7375\n",
      "Epoch 303/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.0827 - val_loss: 670.5652\n",
      "Epoch 304/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.9108 - val_loss: 671.3799\n",
      "Epoch 305/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.7471 - val_loss: 672.1816\n",
      "Epoch 306/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.5912 - val_loss: 672.9705\n",
      "Epoch 307/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4428 - val_loss: 673.7465\n",
      "Epoch 308/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.3016 - val_loss: 674.5096\n",
      "Epoch 309/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 14.1674 - val_loss: 675.2598\n",
      "Epoch 310/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.0397 - val_loss: 675.9968\n",
      "Epoch 311/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.9184 - val_loss: 676.7205\n",
      "Epoch 312/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.8032 - val_loss: 677.4314\n",
      "Epoch 313/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.6937 - val_loss: 678.1288\n",
      "Epoch 314/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.5899 - val_loss: 678.8133\n",
      "Epoch 315/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.4914 - val_loss: 679.4852\n",
      "Epoch 316/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 13.3979 - val_loss: 680.1434\n",
      "Epoch 317/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.3093 - val_loss: 680.7891\n",
      "Epoch 318/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.2254 - val_loss: 681.4216\n",
      "Epoch 319/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 13.1459 - val_loss: 682.0411\n",
      "Epoch 320/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 13.0707 - val_loss: 682.6484\n",
      "Epoch 321/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.9994 - val_loss: 683.2425\n",
      "Epoch 322/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.9321 - val_loss: 683.8242\n",
      "Epoch 323/500\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 12.8683 - val_loss: 684.3929\n",
      "Epoch 324/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.8081 - val_loss: 684.9495\n",
      "Epoch 325/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.7512 - val_loss: 685.4939\n",
      "Epoch 326/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.6975 - val_loss: 686.0256\n",
      "Epoch 327/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.6468 - val_loss: 686.5455\n",
      "Epoch 328/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.5989 - val_loss: 687.0535\n",
      "Epoch 329/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.5537 - val_loss: 687.5494\n",
      "Epoch 330/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 12.5112 - val_loss: 688.0332\n",
      "Epoch 331/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.4711 - val_loss: 688.5055\n",
      "Epoch 332/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.4333 - val_loss: 688.9662\n",
      "Epoch 333/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.3978 - val_loss: 689.4156\n",
      "Epoch 334/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.3643 - val_loss: 689.8538\n",
      "Epoch 335/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.3329 - val_loss: 690.2808\n",
      "Epoch 336/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.3032 - val_loss: 690.6968\n",
      "Epoch 337/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 12.2754 - val_loss: 691.1023\n",
      "Epoch 338/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.2493 - val_loss: 691.4965\n",
      "Epoch 339/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.2248 - val_loss: 691.8809\n",
      "Epoch 340/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.2017 - val_loss: 692.2543\n",
      "Epoch 341/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.1801 - val_loss: 692.6179\n",
      "Epoch 342/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.1598 - val_loss: 692.9713\n",
      "Epoch 343/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.1408 - val_loss: 693.3146\n",
      "Epoch 344/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.1231 - val_loss: 693.6483\n",
      "Epoch 345/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.1064 - val_loss: 693.9727\n",
      "Epoch 346/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.0908 - val_loss: 694.2874\n",
      "Epoch 347/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 12.0762 - val_loss: 694.5931\n",
      "Epoch 348/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 12.0626 - val_loss: 694.8901\n",
      "Epoch 349/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.0498 - val_loss: 695.1780\n",
      "Epoch 350/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.0379 - val_loss: 695.4572\n",
      "Epoch 351/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.0268 - val_loss: 695.7280\n",
      "Epoch 352/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.0164 - val_loss: 695.9904\n",
      "Epoch 353/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.0067 - val_loss: 696.2445\n",
      "Epoch 354/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.9976 - val_loss: 696.4905\n",
      "Epoch 355/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.9892 - val_loss: 696.7291\n",
      "Epoch 356/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11.9813 - val_loss: 696.9598\n",
      "Epoch 357/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.9740 - val_loss: 697.1832\n",
      "Epoch 358/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.9672 - val_loss: 697.3995\n",
      "Epoch 359/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.9608 - val_loss: 697.6082\n",
      "Epoch 360/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.9549 - val_loss: 697.8102\n",
      "Epoch 361/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.9495 - val_loss: 698.0054\n",
      "Epoch 362/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.9443 - val_loss: 698.1940\n",
      "Epoch 363/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.9396 - val_loss: 698.3761\n",
      "Epoch 364/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.9352 - val_loss: 698.5520\n",
      "Epoch 365/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.9311 - val_loss: 698.7213\n",
      "Epoch 366/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.9273 - val_loss: 698.8846\n",
      "Epoch 367/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.9238 - val_loss: 699.0424\n",
      "Epoch 368/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.9206 - val_loss: 699.1950\n",
      "Epoch 369/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.9175 - val_loss: 699.3415\n",
      "Epoch 370/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 11.9148 - val_loss: 699.4831\n",
      "Epoch 371/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.9121 - val_loss: 699.6191\n",
      "Epoch 372/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.9098 - val_loss: 699.7504\n",
      "Epoch 373/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 11.9075 - val_loss: 699.8763\n",
      "Epoch 374/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.9055 - val_loss: 699.9973\n",
      "Epoch 375/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.9037 - val_loss: 700.1141\n",
      "Epoch 376/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.9019 - val_loss: 700.2257\n",
      "Epoch 377/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.9003 - val_loss: 700.3341\n",
      "Epoch 378/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.8988 - val_loss: 700.4370\n",
      "Epoch 379/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 11.8975 - val_loss: 700.5363\n",
      "Epoch 380/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8962 - val_loss: 700.6318\n",
      "Epoch 381/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8951 - val_loss: 700.7229\n",
      "Epoch 382/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8940 - val_loss: 700.8108\n",
      "Epoch 383/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8931 - val_loss: 700.8952\n",
      "Epoch 384/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8922 - val_loss: 700.9758\n",
      "Epoch 385/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.8914 - val_loss: 701.0530\n",
      "Epoch 386/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8906 - val_loss: 701.1270\n",
      "Epoch 387/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8900 - val_loss: 701.1981\n",
      "Epoch 388/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.8894 - val_loss: 701.2657\n",
      "Epoch 389/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8888 - val_loss: 701.3309\n",
      "Epoch 390/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8882 - val_loss: 701.3926\n",
      "Epoch 391/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8878 - val_loss: 701.4522\n",
      "Epoch 392/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8875 - val_loss: 701.5089\n",
      "Epoch 393/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8871 - val_loss: 701.5633\n",
      "Epoch 394/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8868 - val_loss: 701.6154\n",
      "Epoch 395/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8864 - val_loss: 701.6648\n",
      "Epoch 396/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8862 - val_loss: 701.7118\n",
      "Epoch 397/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8859 - val_loss: 701.7567\n",
      "Epoch 398/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8857 - val_loss: 701.7996\n",
      "Epoch 399/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 11.8855 - val_loss: 701.8398\n",
      "Epoch 400/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.8854 - val_loss: 701.8788\n",
      "Epoch 401/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8853 - val_loss: 701.9161\n",
      "Epoch 402/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8852 - val_loss: 701.9512\n",
      "Epoch 403/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.8851 - val_loss: 701.9845\n",
      "Epoch 404/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8850 - val_loss: 702.0162\n",
      "Epoch 405/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8850 - val_loss: 702.0468\n",
      "Epoch 406/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8850 - val_loss: 702.0755\n",
      "Epoch 407/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8850 - val_loss: 702.1031\n",
      "Epoch 408/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8849 - val_loss: 702.1290\n",
      "Epoch 409/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8849 - val_loss: 702.1535\n",
      "Epoch 410/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8850 - val_loss: 702.1771\n",
      "Epoch 411/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8850 - val_loss: 702.1994\n",
      "Epoch 412/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8851 - val_loss: 702.2208\n",
      "Epoch 413/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8851 - val_loss: 702.2408\n",
      "Epoch 414/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8852 - val_loss: 702.2595\n",
      "Epoch 415/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8853 - val_loss: 702.2775\n",
      "Epoch 416/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8853 - val_loss: 702.2941\n",
      "Epoch 417/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8854 - val_loss: 702.3100\n",
      "Epoch 418/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8855 - val_loss: 702.3248\n",
      "Epoch 419/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8856 - val_loss: 702.3388\n",
      "Epoch 420/500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 11.8858 - val_loss: 702.3522\n",
      "Epoch 421/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8859 - val_loss: 702.3647\n",
      "Epoch 422/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8860 - val_loss: 702.3764\n",
      "Epoch 423/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8861 - val_loss: 702.3877\n",
      "Epoch 424/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8862 - val_loss: 702.3983\n",
      "Epoch 425/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.8864 - val_loss: 702.4084\n",
      "Epoch 426/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8865 - val_loss: 702.4179\n",
      "Epoch 427/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8867 - val_loss: 702.4262\n",
      "Epoch 428/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.8868 - val_loss: 702.4348\n",
      "Epoch 429/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8870 - val_loss: 702.4424\n",
      "Epoch 430/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8871 - val_loss: 702.4494\n",
      "Epoch 431/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.8873 - val_loss: 702.4561\n",
      "Epoch 432/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.8874 - val_loss: 702.4625\n",
      "Epoch 433/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8876 - val_loss: 702.4681\n",
      "Epoch 434/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 11.8878 - val_loss: 702.4738\n",
      "Epoch 435/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.8880 - val_loss: 702.4792\n",
      "Epoch 436/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8881 - val_loss: 702.4841\n",
      "Epoch 437/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8883 - val_loss: 702.4882\n",
      "Epoch 438/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8885 - val_loss: 702.4925\n",
      "Epoch 439/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 11.8887 - val_loss: 702.4966\n",
      "Epoch 440/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8889 - val_loss: 702.5001\n",
      "Epoch 441/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8891 - val_loss: 702.5035\n",
      "Epoch 442/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8892 - val_loss: 702.5064\n",
      "Epoch 443/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.8894 - val_loss: 702.5091\n",
      "Epoch 444/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8897 - val_loss: 702.5120\n",
      "Epoch 445/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8898 - val_loss: 702.5143\n",
      "Epoch 446/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.8900 - val_loss: 702.5164\n",
      "Epoch 447/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8902 - val_loss: 702.5182\n",
      "Epoch 448/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8904 - val_loss: 702.5202\n",
      "Epoch 449/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8906 - val_loss: 702.5215\n",
      "Epoch 450/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8908 - val_loss: 702.5231\n",
      "Epoch 451/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.8910 - val_loss: 702.5245\n",
      "Epoch 452/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8912 - val_loss: 702.5256\n",
      "Epoch 453/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8914 - val_loss: 702.5267\n",
      "Epoch 454/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8917 - val_loss: 702.5284\n",
      "Epoch 455/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8918 - val_loss: 702.5289\n",
      "Epoch 456/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8920 - val_loss: 702.5294\n",
      "Epoch 457/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8922 - val_loss: 702.5299\n",
      "Epoch 458/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8924 - val_loss: 702.5307\n",
      "Epoch 459/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 11.8926 - val_loss: 702.5311\n",
      "Epoch 460/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8928 - val_loss: 702.5313\n",
      "Epoch 461/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8930 - val_loss: 702.5318\n",
      "Epoch 462/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8933 - val_loss: 702.5319\n",
      "Epoch 463/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8934 - val_loss: 702.5319\n",
      "Epoch 464/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8937 - val_loss: 702.5319\n",
      "Epoch 465/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8939 - val_loss: 702.5318\n",
      "Epoch 466/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.8941 - val_loss: 702.5318\n",
      "Epoch 467/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8943 - val_loss: 702.5318\n",
      "Epoch 468/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 11.8945 - val_loss: 702.5318\n",
      "Epoch 469/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8947 - val_loss: 702.5316\n",
      "Epoch 470/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8949 - val_loss: 702.5316\n",
      "Epoch 471/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8952 - val_loss: 702.5316\n",
      "Epoch 472/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8953 - val_loss: 702.5314\n",
      "Epoch 473/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8956 - val_loss: 702.5312\n",
      "Epoch 474/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8958 - val_loss: 702.5312\n",
      "Epoch 475/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8959 - val_loss: 702.5310\n",
      "Epoch 476/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8961 - val_loss: 702.5306\n",
      "Epoch 477/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8964 - val_loss: 702.5302\n",
      "Epoch 478/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8966 - val_loss: 702.5295\n",
      "Epoch 479/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8968 - val_loss: 702.5293\n",
      "Epoch 480/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.8969 - val_loss: 702.5287\n",
      "Epoch 481/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8972 - val_loss: 702.5285\n",
      "Epoch 482/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8974 - val_loss: 702.5280\n",
      "Epoch 483/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.8976 - val_loss: 702.5269\n",
      "Epoch 484/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8978 - val_loss: 702.5266\n",
      "Epoch 485/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8980 - val_loss: 702.5263\n",
      "Epoch 486/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 11.8982 - val_loss: 702.5259\n",
      "Epoch 487/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8984 - val_loss: 702.5254\n",
      "Epoch 488/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8986 - val_loss: 702.5248\n",
      "Epoch 489/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8988 - val_loss: 702.5242\n",
      "Epoch 490/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8990 - val_loss: 702.5240\n",
      "Epoch 491/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.8992 - val_loss: 702.5233\n",
      "Epoch 492/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8994 - val_loss: 702.5228\n",
      "Epoch 493/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8996 - val_loss: 702.5223\n",
      "Epoch 494/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.8998 - val_loss: 702.5215\n",
      "Epoch 495/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 11.9000 - val_loss: 702.5208\n",
      "Epoch 496/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.9002 - val_loss: 702.5203\n",
      "Epoch 497/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.9004 - val_loss: 702.5196\n",
      "Epoch 498/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.9006 - val_loss: 702.5185\n",
      "Epoch 499/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.9008 - val_loss: 702.5179\n",
      "Epoch 500/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.9009 - val_loss: 702.5173\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.1552, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0026, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0004, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0102, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.8734, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 454ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[70.78284314, 70.77443978, 70.76603641, 70.75763305, 70.74922969,\n",
       "        70.74082633, 70.73242297, 70.72401961, 70.71561625, 70.70721289,\n",
       "        70.69880952, 70.69040616, 70.6820028 , 74.6131653 , 74.5123249 ,\n",
       "        74.4114846 , 74.3106443 , 74.2098039 , 74.1013772 , 73.9921335 ,\n",
       "        73.8828898 , 73.7736461 , 73.6644024 , 73.5551587 , 73.445915  ,\n",
       "        73.3366713 , 73.2274276 , 73.1181839 , 73.0089402 , 72.8996078 ,\n",
       "        72.7584314 , 72.6172549 , 72.4760784 , 72.334902  , 72.1937255 ,\n",
       "        72.052549  , 71.9113725 , 71.7701961 , 71.6290196 , 71.4878431 ,\n",
       "         0.        ,  0.37365294,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.09959716, 73.3609477 , 73.251704  , 73.1424603 ,\n",
       "        73.0332166 , 72.9239729 , 72.7898039 , 72.6486275 , 72.507451  ,\n",
       "        72.3662745 , 72.225098  , 72.0839216 , 71.9427451 , 71.8015686 ,\n",
       "        71.6603922 , 71.5192157 , 71.3780392 , 71.2368628 , 71.1963212 ,\n",
       "        71.1694304 , 71.1425397 , 71.1156489 , 71.0887582 , 71.0618674 ,\n",
       "        71.0349767 , 71.0080859 , 70.9811951 , 70.6933365 ,  0.        ,\n",
       "         0.16530041,  0.21967015,  0.        ,  0.        ,  0.86611646,\n",
       "        50.74946976,  0.        ,  0.        ,  0.42248181,  0.53640378,\n",
       "         0.        ,  0.        ,  0.99430245,  0.46666065,  0.        ,\n",
       "         0.52531165,  0.49133089,  0.        ,  0.19136885,  0.        ,\n",
       "         0.        ,  0.        ,  0.66350639,  0.7508601 ,  0.37061971]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([67.3105042 , 67.30116713, 67.29183007, 67.282493  , 67.27315593,\n",
       "       67.26381886, 67.25448179, 67.24514472, 67.23580766, 67.22647059,\n",
       "       67.21713352, 67.20779645, 67.19845938, 67.18912232, 67.17978525,\n",
       "       67.17044818, 67.16111111, 67.15177404, 67.14243697, 67.13309991,\n",
       "       67.12376284, 67.11442577, 67.1050887 , 67.09575163, 67.08641457,\n",
       "       67.0770775 , 67.06774043, 67.05840336, 67.04906629, 67.03972923,\n",
       "       67.03039216, 67.02105509, 67.01171802, 67.00238095, 66.99304388,\n",
       "       66.98370682, 66.97436975, 66.96503268, 66.95569561, 66.94635854,\n",
       "       66.93702148, 66.92768441, 66.91834734, 66.90901027, 66.8996732 ,\n",
       "       66.89033613, 66.88099907, 66.871662  , 66.86232493, 66.85298786,\n",
       "       66.84365079, 66.83431373, 66.82497666, 66.81563959, 66.80630252,\n",
       "       66.79757236, 66.79010271, 66.78263305, 66.7751634 , 66.76769374,\n",
       "       66.76022409, 66.75275444, 66.74528478, 66.73781513, 66.73034547,\n",
       "       66.72287582, 66.71540616, 66.70793651, 66.70046685, 66.6929972 ,\n",
       "       66.68552754, 66.67805789, 66.67058824, 66.66311858, 66.65564893,\n",
       "       66.64817927, 66.64070962, 66.63323996, 66.62577031, 66.61830065,\n",
       "       66.610831  , 66.60336134, 66.59589169, 66.58842204, 66.58095238,\n",
       "       66.57348273, 66.56601307, 66.55854342, 66.55107376, 66.54360411,\n",
       "       66.53613445, 66.5286648 , 66.52119514, 66.51372549, 66.50625584,\n",
       "       66.49878618, 66.49131653, 66.48384687, 66.47637722, 66.46890756])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.85473462892183\n",
      "24.885510853796962\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
