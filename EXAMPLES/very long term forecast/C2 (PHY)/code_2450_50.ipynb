{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "2495    52.162308\n",
       "2496    52.146731\n",
       "2497    52.131154\n",
       "2498    52.115577\n",
       "2499    52.100000\n",
       "Name: C2, Length: 2500, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c2_interpolated_2450_50.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "2445     0.000000\n",
       "2446     0.000000\n",
       "2447     0.000000\n",
       "2448     0.116940\n",
       "2449     0.000000\n",
       "Name: C2, Length: 2450, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(50)\n",
    "test\n",
    "training_set = training_set.head(2450)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.500000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.275910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.051821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.827731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.603641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     90.500000  0.000298\n",
       "1     90.275910  0.000298\n",
       "2     90.051821  0.000297\n",
       "3     89.827731  0.000297\n",
       "4     89.603641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.1552  # Value for C0\n",
    "K0 = -0.0026  # Value for K0\n",
    "K1 = -0.0004  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0102    # Value for b\n",
    "c = 2.8734    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-175)  var1(t-174)  var1(t-173)  var1(t-172)  var1(t-171)  \\\n",
      "175    90.500000    90.275910    90.051821    89.827731    89.603641   \n",
      "176    90.275910    90.051821    89.827731    89.603641    89.379552   \n",
      "177    90.051821    89.827731    89.603641    89.379552    89.155462   \n",
      "178    89.827731    89.603641    89.379552    89.155462    88.931373   \n",
      "179    89.603641    89.379552    89.155462    88.931373    88.707283   \n",
      "\n",
      "     var1(t-170)  var1(t-169)  var1(t-168)  var1(t-167)  var1(t-166)  ...  \\\n",
      "175    89.379552    89.155462    88.931373    88.707283    88.494958  ...   \n",
      "176    89.155462    88.931373    88.707283    88.494958    88.427731  ...   \n",
      "177    88.931373    88.707283    88.494958    88.427731    88.360504  ...   \n",
      "178    88.707283    88.494958    88.427731    88.360504    88.293277  ...   \n",
      "179    88.494958    88.427731    88.360504    88.293277    88.226050  ...   \n",
      "\n",
      "     var1(t+45)  var2(t+45)  var1(t+46)  var2(t+46)  var1(t+47)  var2(t+47)  \\\n",
      "175   83.025023    0.000280   83.006349    0.000280   82.987675    0.000280   \n",
      "176   83.006349    0.000280   82.987675    0.000280   82.969001    0.000280   \n",
      "177   82.987675    0.000280   82.969001    0.000280   82.950327    0.000279   \n",
      "178   82.969001    0.000280   82.950327    0.000279   82.931653    0.000279   \n",
      "179   82.950327    0.000279   82.931653    0.000279   82.912979    0.000279   \n",
      "\n",
      "     var1(t+48)  var2(t+48)  var1(t+49)  var2(t+49)  \n",
      "175   82.969001    0.000280   82.950327    0.000279  \n",
      "176   82.950327    0.000279   82.931653    0.000279  \n",
      "177   82.931653    0.000279   82.912979    0.000279  \n",
      "178   82.912979    0.000279   82.894304    0.000279  \n",
      "179   82.894304    0.000279   82.875630    0.000279  \n",
      "\n",
      "[5 rows x 276 columns]\n",
      "Index(['var1(t-175)', 'var1(t-174)', 'var1(t-173)', 'var1(t-172)',\n",
      "       'var1(t-171)', 'var1(t-170)', 'var1(t-169)', 'var1(t-168)',\n",
      "       'var1(t-167)', 'var1(t-166)',\n",
      "       ...\n",
      "       'var1(t+45)', 'var2(t+45)', 'var1(t+46)', 'var2(t+46)', 'var1(t+47)',\n",
      "       'var2(t+47)', 'var1(t+48)', 'var2(t+48)', 'var1(t+49)', 'var2(t+49)'],\n",
      "      dtype='object', length=276)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 175, n_out = 50)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 176):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-150:]\n",
    "trainX = train[:,:-150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-150:]\n",
    "forecastX = forecast[:,:-150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225, 1, 126) (2225, 150) (1, 1, 126)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "28/28 [==============================] - 2s 19ms/step - loss: 5053.1909 - val_loss: 2404.9949\n",
      "Epoch 2/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4767.2900 - val_loss: 2285.0911\n",
      "Epoch 3/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4539.4556 - val_loss: 2190.6323\n",
      "Epoch 4/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4384.4756 - val_loss: 2105.1890\n",
      "Epoch 5/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 4208.5649 - val_loss: 2026.8038\n",
      "Epoch 6/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 4044.4783 - val_loss: 1958.0691\n",
      "Epoch 7/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3904.3313 - val_loss: 1900.7426\n",
      "Epoch 8/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3759.8806 - val_loss: 1830.5569\n",
      "Epoch 9/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3610.5339 - val_loss: 1769.6235\n",
      "Epoch 10/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3471.0486 - val_loss: 1717.8822\n",
      "Epoch 11/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3341.6443 - val_loss: 1665.4010\n",
      "Epoch 12/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 3218.9890 - val_loss: 1616.6985\n",
      "Epoch 13/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 3101.2180 - val_loss: 1566.6370\n",
      "Epoch 14/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2991.9548 - val_loss: 1529.2236\n",
      "Epoch 15/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2872.6248 - val_loss: 1489.4078\n",
      "Epoch 16/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2766.0845 - val_loss: 1453.1595\n",
      "Epoch 17/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2662.8906 - val_loss: 1418.5719\n",
      "Epoch 18/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2565.1965 - val_loss: 1381.5525\n",
      "Epoch 19/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2469.7444 - val_loss: 1348.7926\n",
      "Epoch 20/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2382.6602 - val_loss: 1324.7220\n",
      "Epoch 21/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2291.5615 - val_loss: 1299.1104\n",
      "Epoch 22/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2205.7227 - val_loss: 1266.3411\n",
      "Epoch 23/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 2123.7236 - val_loss: 1236.9646\n",
      "Epoch 24/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2044.8582 - val_loss: 1217.8824\n",
      "Epoch 25/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1968.6290 - val_loss: 1192.6520\n",
      "Epoch 26/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1895.3734 - val_loss: 1165.6443\n",
      "Epoch 27/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1822.3263 - val_loss: 1147.3059\n",
      "Epoch 28/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1755.0967 - val_loss: 1133.0157\n",
      "Epoch 29/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1683.9667 - val_loss: 1120.3726\n",
      "Epoch 30/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1609.4198 - val_loss: 1103.0834\n",
      "Epoch 31/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1547.6007 - val_loss: 1095.5837\n",
      "Epoch 32/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1483.3806 - val_loss: 1075.1489\n",
      "Epoch 33/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1422.9978 - val_loss: 1055.8142\n",
      "Epoch 34/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1365.6876 - val_loss: 1052.7013\n",
      "Epoch 35/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1311.0941 - val_loss: 1034.4220\n",
      "Epoch 36/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1273.1272 - val_loss: 1047.2190\n",
      "Epoch 37/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1218.2632 - val_loss: 1050.2205\n",
      "Epoch 38/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1195.0675 - val_loss: 1034.0745\n",
      "Epoch 39/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1118.5026 - val_loss: 1012.5132\n",
      "Epoch 40/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1073.5438 - val_loss: 995.8425\n",
      "Epoch 41/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1074.4958 - val_loss: 996.9747\n",
      "Epoch 42/500\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 988.7115 - val_loss: 977.5192\n",
      "Epoch 43/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1009.5109 - val_loss: 966.2050\n",
      "Epoch 44/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 914.1886 - val_loss: 979.5942\n",
      "Epoch 45/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1007.0794 - val_loss: 1063.3859\n",
      "Epoch 46/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 924.2975 - val_loss: 1046.8538\n",
      "Epoch 47/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 866.0081 - val_loss: 1070.0656\n",
      "Epoch 48/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 784.0393 - val_loss: 1064.4961\n",
      "Epoch 49/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 760.7663 - val_loss: 1085.4637\n",
      "Epoch 50/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 724.0961 - val_loss: 1061.4583\n",
      "Epoch 51/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 752.6688 - val_loss: 1058.4707\n",
      "Epoch 52/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 737.0612 - val_loss: 1152.3995\n",
      "Epoch 53/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 697.4827 - val_loss: 1153.2968\n",
      "Epoch 54/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 674.7324 - val_loss: 1154.0956\n",
      "Epoch 55/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 673.9948 - val_loss: 1154.4957\n",
      "Epoch 56/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 631.2090 - val_loss: 1161.3011\n",
      "Epoch 57/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 622.5465 - val_loss: 1176.4120\n",
      "Epoch 58/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 593.4677 - val_loss: 1183.2128\n",
      "Epoch 59/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 575.8445 - val_loss: 1188.2767\n",
      "Epoch 60/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 555.3420 - val_loss: 1187.7656\n",
      "Epoch 61/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 547.6922 - val_loss: 1220.0385\n",
      "Epoch 62/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 560.9748 - val_loss: 1230.8574\n",
      "Epoch 63/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 521.3766 - val_loss: 1253.2422\n",
      "Epoch 64/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 504.0040 - val_loss: 1249.1865\n",
      "Epoch 65/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 517.1740 - val_loss: 1254.6700\n",
      "Epoch 66/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 475.7967 - val_loss: 1238.7524\n",
      "Epoch 67/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 465.9165 - val_loss: 1259.6583\n",
      "Epoch 68/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 446.8715 - val_loss: 1226.5072\n",
      "Epoch 69/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 457.5194 - val_loss: 1230.9403\n",
      "Epoch 70/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 428.3946 - val_loss: 1243.0735\n",
      "Epoch 71/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 448.4538 - val_loss: 1233.1398\n",
      "Epoch 72/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 422.5633 - val_loss: 1253.4315\n",
      "Epoch 73/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 391.5338 - val_loss: 1242.6062\n",
      "Epoch 74/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 387.7377 - val_loss: 1256.2341\n",
      "Epoch 75/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 408.7614 - val_loss: 1262.5691\n",
      "Epoch 76/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 417.6630 - val_loss: 1371.8782\n",
      "Epoch 77/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 388.3155 - val_loss: 1382.0981\n",
      "Epoch 78/500\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 382.0787 - val_loss: 1393.5708\n",
      "Epoch 79/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 374.5385 - val_loss: 1391.1554\n",
      "Epoch 80/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 368.5439 - val_loss: 1386.8331\n",
      "Epoch 81/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 361.6078 - val_loss: 1349.9365\n",
      "Epoch 82/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 364.3403 - val_loss: 1311.8479\n",
      "Epoch 83/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 355.0064 - val_loss: 1298.5820\n",
      "Epoch 84/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 348.6786 - val_loss: 1311.1946\n",
      "Epoch 85/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 324.1777 - val_loss: 1307.2522\n",
      "Epoch 86/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 320.2805 - val_loss: 1307.4941\n",
      "Epoch 87/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 310.5937 - val_loss: 1297.7444\n",
      "Epoch 88/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 323.9831 - val_loss: 1365.0205\n",
      "Epoch 89/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 321.4799 - val_loss: 1378.5544\n",
      "Epoch 90/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 331.1875 - val_loss: 1349.9625\n",
      "Epoch 91/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 331.3202 - val_loss: 1394.6368\n",
      "Epoch 92/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 310.3147 - val_loss: 1373.6226\n",
      "Epoch 93/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 305.1276 - val_loss: 1368.8687\n",
      "Epoch 94/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 319.1153 - val_loss: 1368.4747\n",
      "Epoch 95/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 314.2511 - val_loss: 1370.3878\n",
      "Epoch 96/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 293.2227 - val_loss: 1353.7126\n",
      "Epoch 97/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 291.2922 - val_loss: 1297.4751\n",
      "Epoch 98/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 286.8889 - val_loss: 1279.1682\n",
      "Epoch 99/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 296.7375 - val_loss: 1257.4989\n",
      "Epoch 100/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 298.6948 - val_loss: 1282.1448\n",
      "Epoch 101/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 280.0440 - val_loss: 1293.8673\n",
      "Epoch 102/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 277.7228 - val_loss: 1292.8781\n",
      "Epoch 103/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 275.6196 - val_loss: 1297.6213\n",
      "Epoch 104/500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 273.9198 - val_loss: 1293.8517\n",
      "Epoch 105/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 272.1257 - val_loss: 1301.6871\n",
      "Epoch 106/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 270.4418 - val_loss: 1300.5668\n",
      "Epoch 107/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 269.4126 - val_loss: 1308.9191\n",
      "Epoch 108/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 267.4337 - val_loss: 1310.1345\n",
      "Epoch 109/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 265.5502 - val_loss: 1310.2078\n",
      "Epoch 110/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 264.3841 - val_loss: 1309.8301\n",
      "Epoch 111/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 262.8194 - val_loss: 1309.1520\n",
      "Epoch 112/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 260.6560 - val_loss: 1300.7017\n",
      "Epoch 113/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 267.1547 - val_loss: 1320.2745\n",
      "Epoch 114/500\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 258.5361 - val_loss: 1315.6425\n",
      "Epoch 115/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 256.7191 - val_loss: 1312.2944\n",
      "Epoch 116/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 255.3486 - val_loss: 1309.2622\n",
      "Epoch 117/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 254.4279 - val_loss: 1310.4260\n",
      "Epoch 118/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 253.4593 - val_loss: 1309.5015\n",
      "Epoch 119/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 252.2150 - val_loss: 1306.4128\n",
      "Epoch 120/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 251.1846 - val_loss: 1307.7471\n",
      "Epoch 121/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 255.1809 - val_loss: 1328.8311\n",
      "Epoch 122/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 251.4505 - val_loss: 1315.3323\n",
      "Epoch 123/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 249.9761 - val_loss: 1311.1648\n",
      "Epoch 124/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 249.0227 - val_loss: 1308.9918\n",
      "Epoch 125/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 247.7209 - val_loss: 1304.4978\n",
      "Epoch 126/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 253.0361 - val_loss: 1323.3582\n",
      "Epoch 127/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 250.1446 - val_loss: 1316.2584\n",
      "Epoch 128/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 248.5438 - val_loss: 1314.2531\n",
      "Epoch 129/500\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 246.8698 - val_loss: 1306.6403\n",
      "Epoch 130/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 245.5322 - val_loss: 1301.4686\n",
      "Epoch 131/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 242.7498 - val_loss: 1295.1652\n",
      "Epoch 132/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 241.3519 - val_loss: 1293.4978\n",
      "Epoch 133/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 240.4582 - val_loss: 1289.7136\n",
      "Epoch 134/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 239.5760 - val_loss: 1289.6550\n",
      "Epoch 135/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 238.7276 - val_loss: 1278.2158\n",
      "Epoch 136/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 238.0932 - val_loss: 1289.7125\n",
      "Epoch 137/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 236.0524 - val_loss: 1283.7128\n",
      "Epoch 138/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 234.4185 - val_loss: 1281.1453\n",
      "Epoch 139/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 233.4043 - val_loss: 1275.8109\n",
      "Epoch 140/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 232.5031 - val_loss: 1273.2761\n",
      "Epoch 141/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 231.6052 - val_loss: 1270.3298\n",
      "Epoch 142/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 230.7066 - val_loss: 1265.7719\n",
      "Epoch 143/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 229.5164 - val_loss: 1259.4730\n",
      "Epoch 144/500\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 226.6358 - val_loss: 1248.8203\n",
      "Epoch 145/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 226.7308 - val_loss: 1245.6843\n",
      "Epoch 146/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 237.8668 - val_loss: 1305.2675\n",
      "Epoch 147/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 236.1758 - val_loss: 1298.9493\n",
      "Epoch 148/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 235.2415 - val_loss: 1295.7034\n",
      "Epoch 149/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 234.2856 - val_loss: 1291.9563\n",
      "Epoch 150/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 233.2808 - val_loss: 1287.0662\n",
      "Epoch 151/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 233.6575 - val_loss: 1287.2595\n",
      "Epoch 152/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 231.4200 - val_loss: 1279.7893\n",
      "Epoch 153/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 230.4819 - val_loss: 1276.3610\n",
      "Epoch 154/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 229.3866 - val_loss: 1271.2452\n",
      "Epoch 155/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 228.4534 - val_loss: 1267.3311\n",
      "Epoch 156/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 227.5165 - val_loss: 1263.4124\n",
      "Epoch 157/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 226.5792 - val_loss: 1259.4692\n",
      "Epoch 158/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 225.6415 - val_loss: 1255.5021\n",
      "Epoch 159/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 224.7032 - val_loss: 1251.5060\n",
      "Epoch 160/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 223.7640 - val_loss: 1247.4608\n",
      "Epoch 161/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 222.8232 - val_loss: 1243.2848\n",
      "Epoch 162/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 221.8676 - val_loss: 1238.0280\n",
      "Epoch 163/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 220.8417 - val_loss: 1237.3984\n",
      "Epoch 164/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 219.9046 - val_loss: 1227.5049\n",
      "Epoch 165/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 231.3177 - val_loss: 1276.9457\n",
      "Epoch 166/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 226.2334 - val_loss: 1227.1681\n",
      "Epoch 167/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 221.8836 - val_loss: 1236.2363\n",
      "Epoch 168/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 222.2947 - val_loss: 1226.7659\n",
      "Epoch 169/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 219.3165 - val_loss: 1222.4061\n",
      "Epoch 170/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 229.8529 - val_loss: 1183.9675\n",
      "Epoch 171/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 232.1366 - val_loss: 1209.6576\n",
      "Epoch 172/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 232.1669 - val_loss: 1219.1935\n",
      "Epoch 173/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 233.2749 - val_loss: 1239.9276\n",
      "Epoch 174/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 228.9209 - val_loss: 1215.2609\n",
      "Epoch 175/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 224.7073 - val_loss: 1204.3312\n",
      "Epoch 176/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 222.3873 - val_loss: 1197.7677\n",
      "Epoch 177/500\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 221.0294 - val_loss: 1192.4795\n",
      "Epoch 178/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 223.7257 - val_loss: 1205.5115\n",
      "Epoch 179/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 233.0049 - val_loss: 1196.9634\n",
      "Epoch 180/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 231.1032 - val_loss: 1195.4850\n",
      "Epoch 181/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 228.2971 - val_loss: 1185.3528\n",
      "Epoch 182/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 225.4146 - val_loss: 1181.1686\n",
      "Epoch 183/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 223.1041 - val_loss: 1178.6641\n",
      "Epoch 184/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 221.1974 - val_loss: 1174.8184\n",
      "Epoch 185/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 218.6535 - val_loss: 1172.9983\n",
      "Epoch 186/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 216.8324 - val_loss: 1168.8755\n",
      "Epoch 187/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 213.9733 - val_loss: 1165.5286\n",
      "Epoch 188/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 213.1799 - val_loss: 1158.8322\n",
      "Epoch 189/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 215.7668 - val_loss: 1179.7765\n",
      "Epoch 190/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 237.1031 - val_loss: 1252.5269\n",
      "Epoch 191/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 228.9914 - val_loss: 1238.5208\n",
      "Epoch 192/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 225.1208 - val_loss: 1218.8402\n",
      "Epoch 193/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 218.1632 - val_loss: 1196.8276\n",
      "Epoch 194/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 212.9473 - val_loss: 1181.0088\n",
      "Epoch 195/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 209.9280 - val_loss: 1169.7844\n",
      "Epoch 196/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 207.5182 - val_loss: 1136.3544\n",
      "Epoch 197/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 211.9567 - val_loss: 1136.6844\n",
      "Epoch 198/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 209.5729 - val_loss: 1130.8397\n",
      "Epoch 199/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 205.8330 - val_loss: 1115.8916\n",
      "Epoch 200/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 219.4595 - val_loss: 1104.2428\n",
      "Epoch 201/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 217.6565 - val_loss: 1111.4559\n",
      "Epoch 202/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 213.5100 - val_loss: 1107.1938\n",
      "Epoch 203/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 211.5795 - val_loss: 1113.8970\n",
      "Epoch 204/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 209.5495 - val_loss: 1103.6387\n",
      "Epoch 205/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 207.5711 - val_loss: 1114.5000\n",
      "Epoch 206/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 205.3392 - val_loss: 1114.2072\n",
      "Epoch 207/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 204.1747 - val_loss: 1114.2173\n",
      "Epoch 208/500\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 214.9922 - val_loss: 1161.8400\n",
      "Epoch 209/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 208.7754 - val_loss: 1146.9803\n",
      "Epoch 210/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 206.6178 - val_loss: 1133.1672\n",
      "Epoch 211/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 204.9025 - val_loss: 1126.2683\n",
      "Epoch 212/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 201.0063 - val_loss: 1109.2947\n",
      "Epoch 213/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 199.9035 - val_loss: 1106.1476\n",
      "Epoch 214/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 198.2830 - val_loss: 1101.7924\n",
      "Epoch 215/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 196.1471 - val_loss: 1096.5505\n",
      "Epoch 216/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 194.2464 - val_loss: 1088.9463\n",
      "Epoch 217/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 192.4626 - val_loss: 1082.1313\n",
      "Epoch 218/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 191.3742 - val_loss: 1081.8665\n",
      "Epoch 219/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 187.0846 - val_loss: 1032.0624\n",
      "Epoch 220/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 214.2580 - val_loss: 1047.6121\n",
      "Epoch 221/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 189.2212 - val_loss: 1038.5267\n",
      "Epoch 222/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 209.4062 - val_loss: 1003.3268\n",
      "Epoch 223/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 300.3209 - val_loss: 999.5261\n",
      "Epoch 224/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 284.4279 - val_loss: 997.8395\n",
      "Epoch 225/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 283.0872 - val_loss: 1005.4737\n",
      "Epoch 226/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 247.9509 - val_loss: 1028.4449\n",
      "Epoch 227/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 263.0157 - val_loss: 1043.4519\n",
      "Epoch 228/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 233.7774 - val_loss: 1060.6965\n",
      "Epoch 229/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 228.5455 - val_loss: 1071.2394\n",
      "Epoch 230/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 224.6794 - val_loss: 1080.3418\n",
      "Epoch 231/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 231.7210 - val_loss: 1129.5349\n",
      "Epoch 232/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 229.1769 - val_loss: 1142.0697\n",
      "Epoch 233/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 225.9729 - val_loss: 1144.6631\n",
      "Epoch 234/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 217.4883 - val_loss: 1154.5372\n",
      "Epoch 235/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 210.1456 - val_loss: 1121.8190\n",
      "Epoch 236/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 213.8931 - val_loss: 1112.3301\n",
      "Epoch 237/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 216.3523 - val_loss: 1113.5836\n",
      "Epoch 238/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 275.7542 - val_loss: 1110.5436\n",
      "Epoch 239/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 258.2029 - val_loss: 1198.0548\n",
      "Epoch 240/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 207.2073 - val_loss: 1093.9105\n",
      "Epoch 241/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 332.9972 - val_loss: 1099.5482\n",
      "Epoch 242/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 298.2857 - val_loss: 1228.6843\n",
      "Epoch 243/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 287.0312 - val_loss: 1252.9819\n",
      "Epoch 244/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 282.8304 - val_loss: 1272.6732\n",
      "Epoch 245/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 279.0303 - val_loss: 1310.6160\n",
      "Epoch 246/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 223.3041 - val_loss: 1289.2235\n",
      "Epoch 247/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 221.0090 - val_loss: 1279.9120\n",
      "Epoch 248/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 219.8101 - val_loss: 1233.2079\n",
      "Epoch 249/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 274.4490 - val_loss: 1206.4581\n",
      "Epoch 250/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 293.1887 - val_loss: 1298.3115\n",
      "Epoch 251/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 282.0059 - val_loss: 1297.6530\n",
      "Epoch 252/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 277.1313 - val_loss: 1283.1871\n",
      "Epoch 253/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 278.6328 - val_loss: 1291.7417\n",
      "Epoch 254/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 267.1158 - val_loss: 1267.7657\n",
      "Epoch 255/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 275.1468 - val_loss: 1306.4537\n",
      "Epoch 256/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 267.4734 - val_loss: 1295.4989\n",
      "Epoch 257/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 270.4175 - val_loss: 1337.1299\n",
      "Epoch 258/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 266.7558 - val_loss: 1338.5364\n",
      "Epoch 259/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 263.0948 - val_loss: 1326.2593\n",
      "Epoch 260/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 260.6665 - val_loss: 1324.8898\n",
      "Epoch 261/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 258.1715 - val_loss: 1312.8759\n",
      "Epoch 262/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 254.0081 - val_loss: 1304.4056\n",
      "Epoch 263/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 251.1975 - val_loss: 1305.6331\n",
      "Epoch 264/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 250.2327 - val_loss: 1306.0859\n",
      "Epoch 265/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 249.3233 - val_loss: 1298.3823\n",
      "Epoch 266/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 273.3523 - val_loss: 1408.9819\n",
      "Epoch 267/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 270.5319 - val_loss: 1403.7468\n",
      "Epoch 268/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 269.1766 - val_loss: 1387.5491\n",
      "Epoch 269/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 292.9141 - val_loss: 1378.8907\n",
      "Epoch 270/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 282.8850 - val_loss: 1358.7081\n",
      "Epoch 271/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 276.1258 - val_loss: 1350.2987\n",
      "Epoch 272/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 274.0561 - val_loss: 1344.2985\n",
      "Epoch 273/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 271.8841 - val_loss: 1348.1139\n",
      "Epoch 274/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 270.5503 - val_loss: 1352.7319\n",
      "Epoch 275/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 269.1250 - val_loss: 1358.7854\n",
      "Epoch 276/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 266.4612 - val_loss: 1356.5385\n",
      "Epoch 277/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 267.6945 - val_loss: 1348.3080\n",
      "Epoch 278/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 274.9909 - val_loss: 1392.6323\n",
      "Epoch 279/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 268.8970 - val_loss: 1377.6129\n",
      "Epoch 280/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 270.5887 - val_loss: 1371.5015\n",
      "Epoch 281/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 267.8427 - val_loss: 1361.5367\n",
      "Epoch 282/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 263.2723 - val_loss: 1351.1787\n",
      "Epoch 283/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 261.3004 - val_loss: 1351.5973\n",
      "Epoch 284/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 259.8897 - val_loss: 1349.8740\n",
      "Epoch 285/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 257.3800 - val_loss: 1338.3376\n",
      "Epoch 286/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 255.0861 - val_loss: 1335.6134\n",
      "Epoch 287/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 253.0126 - val_loss: 1336.0171\n",
      "Epoch 288/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 251.7626 - val_loss: 1333.4208\n",
      "Epoch 289/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 251.1924 - val_loss: 1329.1660\n",
      "Epoch 290/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 253.6221 - val_loss: 1347.3435\n",
      "Epoch 291/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 253.4681 - val_loss: 1341.8071\n",
      "Epoch 292/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 248.9635 - val_loss: 1301.8314\n",
      "Epoch 293/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 290.5429 - val_loss: 1339.0393\n",
      "Epoch 294/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 284.6259 - val_loss: 1332.8364\n",
      "Epoch 295/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 281.8233 - val_loss: 1339.2137\n",
      "Epoch 296/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 267.3896 - val_loss: 1386.2656\n",
      "Epoch 297/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 264.9300 - val_loss: 1377.8563\n",
      "Epoch 298/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 256.4796 - val_loss: 1342.5750\n",
      "Epoch 299/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 252.8569 - val_loss: 1325.1315\n",
      "Epoch 300/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 245.9426 - val_loss: 1294.4487\n",
      "Epoch 301/500\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 239.7123 - val_loss: 1233.6351\n",
      "Epoch 302/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 240.8191 - val_loss: 1237.1486\n",
      "Epoch 303/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 239.7550 - val_loss: 1246.8082\n",
      "Epoch 304/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 238.5490 - val_loss: 1243.9937\n",
      "Epoch 305/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 237.3035 - val_loss: 1247.3208\n",
      "Epoch 306/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 239.0840 - val_loss: 1218.2357\n",
      "Epoch 307/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 348.2487 - val_loss: 1299.4789\n",
      "Epoch 308/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 241.8575 - val_loss: 1263.2740\n",
      "Epoch 309/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 293.4407 - val_loss: 1522.1047\n",
      "Epoch 310/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 318.2960 - val_loss: 1527.2817\n",
      "Epoch 311/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 303.6348 - val_loss: 1550.2416\n",
      "Epoch 312/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 302.2434 - val_loss: 1500.9517\n",
      "Epoch 313/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 277.2671 - val_loss: 1460.5088\n",
      "Epoch 314/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 269.3900 - val_loss: 1414.5250\n",
      "Epoch 315/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 257.0272 - val_loss: 1386.1835\n",
      "Epoch 316/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 255.0670 - val_loss: 1378.4027\n",
      "Epoch 317/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 253.6117 - val_loss: 1366.1606\n",
      "Epoch 318/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 252.6988 - val_loss: 1349.0277\n",
      "Epoch 319/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 265.9789 - val_loss: 1354.0026\n",
      "Epoch 320/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 295.8875 - val_loss: 1397.5858\n",
      "Epoch 321/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 282.9336 - val_loss: 1379.2317\n",
      "Epoch 322/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 245.8545 - val_loss: 1305.6089\n",
      "Epoch 323/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 234.7731 - val_loss: 1273.7255\n",
      "Epoch 324/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 233.1254 - val_loss: 1265.8392\n",
      "Epoch 325/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 231.9093 - val_loss: 1261.1970\n",
      "Epoch 326/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 229.4086 - val_loss: 1246.5171\n",
      "Epoch 327/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 229.0976 - val_loss: 1243.5778\n",
      "Epoch 328/500\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 228.1630 - val_loss: 1243.4961\n",
      "Epoch 329/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 227.6436 - val_loss: 1234.7930\n",
      "Epoch 330/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 226.8310 - val_loss: 1244.9004\n",
      "Epoch 331/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 227.6971 - val_loss: 1249.4418\n",
      "Epoch 332/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 226.4366 - val_loss: 1222.8815\n",
      "Epoch 333/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 238.8743 - val_loss: 1295.4237\n",
      "Epoch 334/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 236.9035 - val_loss: 1294.6321\n",
      "Epoch 335/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 236.1910 - val_loss: 1292.5911\n",
      "Epoch 336/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 235.5780 - val_loss: 1288.7493\n",
      "Epoch 337/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 234.9669 - val_loss: 1285.2867\n",
      "Epoch 338/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 234.3331 - val_loss: 1281.6343\n",
      "Epoch 339/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 232.5708 - val_loss: 1271.6646\n",
      "Epoch 340/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 227.3221 - val_loss: 1237.2781\n",
      "Epoch 341/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 225.2910 - val_loss: 1228.3710\n",
      "Epoch 342/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 224.2452 - val_loss: 1220.3926\n",
      "Epoch 343/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 218.2258 - val_loss: 1209.9351\n",
      "Epoch 344/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 216.7471 - val_loss: 1202.7350\n",
      "Epoch 345/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 216.2713 - val_loss: 1200.7007\n",
      "Epoch 346/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 215.4863 - val_loss: 1200.7845\n",
      "Epoch 347/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 214.8515 - val_loss: 1200.0250\n",
      "Epoch 348/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 214.2274 - val_loss: 1196.3641\n",
      "Epoch 349/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 213.7608 - val_loss: 1199.3179\n",
      "Epoch 350/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 213.1651 - val_loss: 1187.4218\n",
      "Epoch 351/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 212.8823 - val_loss: 1182.1716\n",
      "Epoch 352/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 211.7062 - val_loss: 1187.0963\n",
      "Epoch 353/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 212.1612 - val_loss: 1194.2716\n",
      "Epoch 354/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 211.3107 - val_loss: 1161.4218\n",
      "Epoch 355/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 249.4926 - val_loss: 1345.6239\n",
      "Epoch 356/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 243.3697 - val_loss: 1341.7836\n",
      "Epoch 357/500\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 242.4363 - val_loss: 1340.7957\n",
      "Epoch 358/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 241.7675 - val_loss: 1338.3571\n",
      "Epoch 359/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 239.1899 - val_loss: 1324.5990\n",
      "Epoch 360/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 232.8350 - val_loss: 1309.8713\n",
      "Epoch 361/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 232.7457 - val_loss: 1303.9275\n",
      "Epoch 362/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 231.7574 - val_loss: 1299.1396\n",
      "Epoch 363/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 231.2414 - val_loss: 1295.7526\n",
      "Epoch 364/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 230.8585 - val_loss: 1286.5691\n",
      "Epoch 365/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 229.9826 - val_loss: 1292.6362\n",
      "Epoch 366/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 229.4211 - val_loss: 1291.6300\n",
      "Epoch 367/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 228.8064 - val_loss: 1294.0590\n",
      "Epoch 368/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 228.1948 - val_loss: 1292.7629\n",
      "Epoch 369/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 227.5748 - val_loss: 1289.0959\n",
      "Epoch 370/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 227.0515 - val_loss: 1284.5339\n",
      "Epoch 371/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 226.9997 - val_loss: 1273.2638\n",
      "Epoch 372/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 225.9156 - val_loss: 1280.4923\n",
      "Epoch 373/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 225.3588 - val_loss: 1278.2924\n",
      "Epoch 374/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 224.8098 - val_loss: 1275.8413\n",
      "Epoch 375/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 224.2629 - val_loss: 1273.4827\n",
      "Epoch 376/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 223.7168 - val_loss: 1271.1632\n",
      "Epoch 377/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 223.1716 - val_loss: 1268.8662\n",
      "Epoch 378/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 222.6273 - val_loss: 1266.5887\n",
      "Epoch 379/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 222.0840 - val_loss: 1264.3271\n",
      "Epoch 380/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 221.5416 - val_loss: 1262.0824\n",
      "Epoch 381/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 221.0003 - val_loss: 1259.8511\n",
      "Epoch 382/500\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 220.4600 - val_loss: 1257.6326\n",
      "Epoch 383/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 219.9207 - val_loss: 1255.4238\n",
      "Epoch 384/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 219.3825 - val_loss: 1253.2257\n",
      "Epoch 385/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 218.8454 - val_loss: 1251.0376\n",
      "Epoch 386/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 218.3094 - val_loss: 1248.8612\n",
      "Epoch 387/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 217.7744 - val_loss: 1246.6881\n",
      "Epoch 388/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 217.2407 - val_loss: 1244.5249\n",
      "Epoch 389/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 216.7091 - val_loss: 1242.5090\n",
      "Epoch 390/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 216.1766 - val_loss: 1240.2740\n",
      "Epoch 391/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 215.6462 - val_loss: 1238.1298\n",
      "Epoch 392/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 215.1171 - val_loss: 1235.9954\n",
      "Epoch 393/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 214.5891 - val_loss: 1233.8640\n",
      "Epoch 394/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 214.0623 - val_loss: 1231.7473\n",
      "Epoch 395/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 213.5367 - val_loss: 1229.6246\n",
      "Epoch 396/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 213.0124 - val_loss: 1227.5243\n",
      "Epoch 397/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 212.4891 - val_loss: 1225.4054\n",
      "Epoch 398/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 211.9671 - val_loss: 1223.3264\n",
      "Epoch 399/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 211.4463 - val_loss: 1221.2018\n",
      "Epoch 400/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 210.9268 - val_loss: 1219.1418\n",
      "Epoch 401/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 210.4084 - val_loss: 1217.0288\n",
      "Epoch 402/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 209.8912 - val_loss: 1214.9803\n",
      "Epoch 403/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 209.3752 - val_loss: 1212.8766\n",
      "Epoch 404/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 208.8606 - val_loss: 1210.8383\n",
      "Epoch 405/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 208.3470 - val_loss: 1208.7512\n",
      "Epoch 406/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 207.8349 - val_loss: 1206.7253\n",
      "Epoch 407/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 207.3238 - val_loss: 1204.6555\n",
      "Epoch 408/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 206.8139 - val_loss: 1202.6582\n",
      "Epoch 409/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 206.3049 - val_loss: 1200.6509\n",
      "Epoch 410/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 205.7961 - val_loss: 1198.8221\n",
      "Epoch 411/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 205.2979 - val_loss: 1196.5088\n",
      "Epoch 412/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 204.7929 - val_loss: 1193.2068\n",
      "Epoch 413/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 204.2654 - val_loss: 1194.2306\n",
      "Epoch 414/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 204.2353 - val_loss: 1192.3334\n",
      "Epoch 415/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 203.7172 - val_loss: 1190.1799\n",
      "Epoch 416/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 203.2168 - val_loss: 1188.1456\n",
      "Epoch 417/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 202.7173 - val_loss: 1186.2117\n",
      "Epoch 418/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 202.2111 - val_loss: 1184.6976\n",
      "Epoch 419/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 201.9187 - val_loss: 1182.4094\n",
      "Epoch 420/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 200.7795 - val_loss: 1183.0861\n",
      "Epoch 421/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 201.2887 - val_loss: 1185.7175\n",
      "Epoch 422/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 199.7702 - val_loss: 1170.5767\n",
      "Epoch 423/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 217.4436 - val_loss: 1204.0824\n",
      "Epoch 424/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 244.9849 - val_loss: 1108.8234\n",
      "Epoch 425/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 276.5358 - val_loss: 1262.0304\n",
      "Epoch 426/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 244.4143 - val_loss: 1202.8658\n",
      "Epoch 427/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 257.9815 - val_loss: 1193.7848\n",
      "Epoch 428/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 246.2509 - val_loss: 1174.3856\n",
      "Epoch 429/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 295.1755 - val_loss: 1192.0009\n",
      "Epoch 430/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 288.5685 - val_loss: 1215.4252\n",
      "Epoch 431/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 281.7389 - val_loss: 1237.2377\n",
      "Epoch 432/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 243.5163 - val_loss: 1257.1051\n",
      "Epoch 433/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 230.6388 - val_loss: 1238.6034\n",
      "Epoch 434/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 230.0742 - val_loss: 1239.5459\n",
      "Epoch 435/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 228.9295 - val_loss: 1237.4291\n",
      "Epoch 436/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 228.2361 - val_loss: 1240.0209\n",
      "Epoch 437/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 227.6530 - val_loss: 1240.7839\n",
      "Epoch 438/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 227.0083 - val_loss: 1244.2377\n",
      "Epoch 439/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 226.1955 - val_loss: 1246.9199\n",
      "Epoch 440/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 225.6957 - val_loss: 1244.7974\n",
      "Epoch 441/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 218.4557 - val_loss: 1211.9775\n",
      "Epoch 442/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 228.5818 - val_loss: 1236.2994\n",
      "Epoch 443/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 223.3215 - val_loss: 1225.5320\n",
      "Epoch 444/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 223.6199 - val_loss: 1193.8694\n",
      "Epoch 445/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 224.2171 - val_loss: 1195.2538\n",
      "Epoch 446/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 222.9753 - val_loss: 1201.7494\n",
      "Epoch 447/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 222.1814 - val_loss: 1201.4989\n",
      "Epoch 448/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 221.1255 - val_loss: 1200.4941\n",
      "Epoch 449/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 220.5981 - val_loss: 1199.2493\n",
      "Epoch 450/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 219.3566 - val_loss: 1205.0753\n",
      "Epoch 451/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 218.7974 - val_loss: 1205.9443\n",
      "Epoch 452/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 217.8712 - val_loss: 1208.5137\n",
      "Epoch 453/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 217.3643 - val_loss: 1207.7450\n",
      "Epoch 454/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 217.0332 - val_loss: 1209.4205\n",
      "Epoch 455/500\n",
      "28/28 [==============================] - 0s 10ms/step - loss: 216.2481 - val_loss: 1210.8844\n",
      "Epoch 456/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 215.7353 - val_loss: 1210.9617\n",
      "Epoch 457/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 215.2428 - val_loss: 1212.5093\n",
      "Epoch 458/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 214.9195 - val_loss: 1208.9290\n",
      "Epoch 459/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 214.7279 - val_loss: 1207.4658\n",
      "Epoch 460/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 214.1972 - val_loss: 1208.4496\n",
      "Epoch 461/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 213.4374 - val_loss: 1208.9601\n",
      "Epoch 462/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 212.9281 - val_loss: 1211.9604\n",
      "Epoch 463/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 212.6587 - val_loss: 1205.8076\n",
      "Epoch 464/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 212.3020 - val_loss: 1207.0665\n",
      "Epoch 465/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 211.9018 - val_loss: 1202.2964\n",
      "Epoch 466/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 211.7795 - val_loss: 1201.2388\n",
      "Epoch 467/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 210.7883 - val_loss: 1202.3004\n",
      "Epoch 468/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 210.4684 - val_loss: 1200.3994\n",
      "Epoch 469/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 209.8689 - val_loss: 1197.9357\n",
      "Epoch 470/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 209.0456 - val_loss: 1186.0291\n",
      "Epoch 471/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 209.5755 - val_loss: 1184.4674\n",
      "Epoch 472/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 208.8757 - val_loss: 1185.8292\n",
      "Epoch 473/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 208.5166 - val_loss: 1182.8635\n",
      "Epoch 474/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 208.2837 - val_loss: 1180.5563\n",
      "Epoch 475/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 207.6546 - val_loss: 1182.1606\n",
      "Epoch 476/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 206.8740 - val_loss: 1186.6980\n",
      "Epoch 477/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 206.8251 - val_loss: 1179.0184\n",
      "Epoch 478/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 206.2774 - val_loss: 1179.5386\n",
      "Epoch 479/500\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 205.9305 - val_loss: 1175.5776\n",
      "Epoch 480/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 205.6559 - val_loss: 1172.6627\n",
      "Epoch 481/500\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 205.0797 - val_loss: 1168.6520\n",
      "Epoch 482/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 204.4464 - val_loss: 1171.0211\n",
      "Epoch 483/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 204.5643 - val_loss: 1162.7701\n",
      "Epoch 484/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 211.3134 - val_loss: 1211.3822\n",
      "Epoch 485/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 212.4570 - val_loss: 1205.0233\n",
      "Epoch 486/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 206.5214 - val_loss: 1176.8423\n",
      "Epoch 487/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 202.6040 - val_loss: 1161.9069\n",
      "Epoch 488/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 201.3084 - val_loss: 1155.2505\n",
      "Epoch 489/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 200.8336 - val_loss: 1152.8115\n",
      "Epoch 490/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 197.3937 - val_loss: 1146.9838\n",
      "Epoch 491/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 196.1933 - val_loss: 1119.9397\n",
      "Epoch 492/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 208.5691 - val_loss: 1164.3451\n",
      "Epoch 493/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 203.7024 - val_loss: 1139.5662\n",
      "Epoch 494/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 268.9371 - val_loss: 1157.4979\n",
      "Epoch 495/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 201.3240 - val_loss: 1154.6244\n",
      "Epoch 496/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 266.7549 - val_loss: 1160.1327\n",
      "Epoch 497/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 260.1306 - val_loss: 1186.1433\n",
      "Epoch 498/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 256.9810 - val_loss: 1201.1807\n",
      "Epoch 499/500\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 253.9826 - val_loss: 1216.6121\n",
      "Epoch 500/500\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 252.4589 - val_loss: 1227.8674\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.1552, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0026, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0004, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0102, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.8734, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 553ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 126)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 126)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 186)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:150:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 7.18643137e+01, 0.00000000e+00, 7.24603922e+01,\n",
       "        7.12142484e+01, 7.09722316e+01, 7.17858823e+01, 7.10857703e+01,\n",
       "        7.08472689e+01, 1.45036131e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.07800420e+01, 3.02265940e-02, 7.08864846e+01, 7.44450980e+01,\n",
       "        8.38599100e-02, 7.07660364e+01, 7.28996078e+01, 7.22250980e+01,\n",
       "        4.22481810e-01, 1.26242206e-01, 0.00000000e+00, 6.96443939e+01,\n",
       "        3.49589199e-01, 0.00000000e+00, 6.50203168e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.43063812e+01, 3.05565059e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.97118598e-01,\n",
       "        0.00000000e+00, 1.31422058e-02, 0.00000000e+00, 1.40174520e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.03492785e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.02149177e-01, 8.35218668e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([52.86327116, 52.8476942 , 52.83211724, 52.81654027, 52.80096331,\n",
       "       52.78538635, 52.76980939, 52.75423242, 52.73865546, 52.7230785 ,\n",
       "       52.70750154, 52.69192457, 52.67634761, 52.66077065, 52.64519369,\n",
       "       52.62961672, 52.61403976, 52.5984628 , 52.58288584, 52.56730887,\n",
       "       52.55173191, 52.53615495, 52.52057799, 52.50500102, 52.48942406,\n",
       "       52.4738471 , 52.45827014, 52.44269317, 52.42711621, 52.41153925,\n",
       "       52.39596229, 52.38038532, 52.36480836, 52.3492314 , 52.33365444,\n",
       "       52.31807747, 52.30250051, 52.28692355, 52.27134659, 52.25576962,\n",
       "       52.24019266, 52.2246157 , 52.20903874, 52.19346177, 52.17788481,\n",
       "       52.16230785, 52.14673089, 52.13115392, 52.11557696, 52.1       ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.94984806347921\n",
      "42.293416742703286\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
