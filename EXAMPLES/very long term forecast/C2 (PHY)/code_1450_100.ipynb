{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "1545    68.045448\n",
       "1546    68.040780\n",
       "1547    68.036111\n",
       "1548    68.031443\n",
       "1549    68.026774\n",
       "Name: C2, Length: 1550, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c2_interpolated_1450_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "1445     0.000000\n",
       "1446     0.081851\n",
       "1447     0.000000\n",
       "1448     0.866116\n",
       "1449     0.172430\n",
       "Name: C2, Length: 1450, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1450)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.500000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.275910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.051821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.827731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.603641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     90.500000  0.000298\n",
       "1     90.275910  0.000298\n",
       "2     90.051821  0.000297\n",
       "3     89.827731  0.000297\n",
       "4     89.603641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.1552  # Value for C0\n",
    "K0 = -0.0026  # Value for K0\n",
    "K1 = -0.0004  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0102    # Value for b\n",
    "c = 2.8734    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    90.500000    90.275910    90.051821    89.827731    89.603641   \n",
      "351    90.275910    90.051821    89.827731    89.603641    89.379552   \n",
      "352    90.051821    89.827731    89.603641    89.379552    89.155462   \n",
      "353    89.827731    89.603641    89.379552    89.155462    88.931373   \n",
      "354    89.603641    89.379552    89.155462    88.931373    88.707283   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    89.379552    89.155462    88.931373    88.707283    88.494958  ...   \n",
      "351    89.155462    88.931373    88.707283    88.494958    88.427731  ...   \n",
      "352    88.931373    88.707283    88.494958    88.427731    88.360504  ...   \n",
      "353    88.707283    88.494958    88.427731    88.360504    88.293277  ...   \n",
      "354    88.494958    88.427731    88.360504    88.293277    88.226050  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   79.071008    0.000263   79.054202    0.000263   79.037395    0.000263   \n",
      "351   79.054202    0.000263   79.037395    0.000263   79.020588    0.000262   \n",
      "352   79.037395    0.000263   79.020588    0.000262   79.003782    0.000262   \n",
      "353   79.020588    0.000262   79.003782    0.000262   78.986975    0.000262   \n",
      "354   79.003782    0.000262   78.986975    0.000262   78.970168    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   79.020588    0.000262   79.003782    0.000262  \n",
      "351   79.003782    0.000262   78.986975    0.000262  \n",
      "352   78.986975    0.000262   78.970168    0.000262  \n",
      "353   78.970168    0.000262   78.953361    0.000262  \n",
      "354   78.953361    0.000262   78.936555    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1, 251) (1000, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "13/13 [==============================] - 3s 41ms/step - loss: 5764.7637 - val_loss: 4892.4639\n",
      "Epoch 2/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5706.8208 - val_loss: 4850.7583\n",
      "Epoch 3/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 5661.7808 - val_loss: 4809.2051\n",
      "Epoch 4/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5616.9414 - val_loss: 4767.8892\n",
      "Epoch 5/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5572.3521 - val_loss: 4726.8223\n",
      "Epoch 6/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5528.0205 - val_loss: 4686.0117\n",
      "Epoch 7/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5483.9526 - val_loss: 4645.4565\n",
      "Epoch 8/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5440.1475 - val_loss: 4605.1572\n",
      "Epoch 9/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5396.6050 - val_loss: 4565.1123\n",
      "Epoch 10/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5353.3223 - val_loss: 4525.3218\n",
      "Epoch 11/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 5310.2988 - val_loss: 4485.7827\n",
      "Epoch 12/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 5267.5342 - val_loss: 4446.4951\n",
      "Epoch 13/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 5225.0269 - val_loss: 4407.4561\n",
      "Epoch 14/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 5182.7754 - val_loss: 4368.6670\n",
      "Epoch 15/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 5140.7788 - val_loss: 4330.1245\n",
      "Epoch 16/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 5099.0352 - val_loss: 4291.8296\n",
      "Epoch 17/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 5057.5439 - val_loss: 4253.7783\n",
      "Epoch 18/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5016.3032 - val_loss: 4215.9722\n",
      "Epoch 19/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4975.3140 - val_loss: 4178.4077\n",
      "Epoch 20/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4934.5723 - val_loss: 4141.0859\n",
      "Epoch 21/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4894.0786 - val_loss: 4104.0034\n",
      "Epoch 22/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4853.8320 - val_loss: 4067.1616\n",
      "Epoch 23/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4813.8301 - val_loss: 4030.5579\n",
      "Epoch 24/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4774.0723 - val_loss: 3994.1912\n",
      "Epoch 25/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4734.5586 - val_loss: 3958.0601\n",
      "Epoch 26/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4695.2852 - val_loss: 3922.1641\n",
      "Epoch 27/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4656.2539 - val_loss: 3886.5022\n",
      "Epoch 28/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4617.4619 - val_loss: 3851.0732\n",
      "Epoch 29/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4578.9082 - val_loss: 3815.8752\n",
      "Epoch 30/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4540.5928 - val_loss: 3780.9082\n",
      "Epoch 31/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4502.5137 - val_loss: 3746.1707\n",
      "Epoch 32/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4464.6694 - val_loss: 3711.6606\n",
      "Epoch 33/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4427.0596 - val_loss: 3677.3792\n",
      "Epoch 34/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4389.6831 - val_loss: 3643.3237\n",
      "Epoch 35/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4352.5386 - val_loss: 3609.4929\n",
      "Epoch 36/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4315.6245 - val_loss: 3575.8865\n",
      "Epoch 37/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4278.9414 - val_loss: 3542.5034\n",
      "Epoch 38/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4242.4863 - val_loss: 3509.3418\n",
      "Epoch 39/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4206.2598 - val_loss: 3476.4021\n",
      "Epoch 40/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4170.2603 - val_loss: 3443.6816\n",
      "Epoch 41/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4134.4858 - val_loss: 3411.1809\n",
      "Epoch 42/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4098.9365 - val_loss: 3378.8972\n",
      "Epoch 43/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4063.6113 - val_loss: 3346.8311\n",
      "Epoch 44/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4028.5078 - val_loss: 3314.9807\n",
      "Epoch 45/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3993.6262 - val_loss: 3283.3450\n",
      "Epoch 46/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3958.9656 - val_loss: 3251.9233\n",
      "Epoch 47/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3924.5242 - val_loss: 3220.7146\n",
      "Epoch 48/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3890.3025 - val_loss: 3189.7178\n",
      "Epoch 49/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3856.2979 - val_loss: 3158.9316\n",
      "Epoch 50/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 3822.5093 - val_loss: 3128.3562\n",
      "Epoch 51/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3788.9375 - val_loss: 3097.9890\n",
      "Epoch 52/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3755.5793 - val_loss: 3067.8303\n",
      "Epoch 53/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3722.4355 - val_loss: 3037.8782\n",
      "Epoch 54/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3689.5042 - val_loss: 3008.1321\n",
      "Epoch 55/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3656.7849 - val_loss: 2978.5928\n",
      "Epoch 56/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3624.2766 - val_loss: 2949.2566\n",
      "Epoch 57/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3591.9780 - val_loss: 2920.1230\n",
      "Epoch 58/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3559.8887 - val_loss: 2891.1929\n",
      "Epoch 59/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3528.0078 - val_loss: 2862.4634\n",
      "Epoch 60/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3496.3325 - val_loss: 2833.9343\n",
      "Epoch 61/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3464.8638 - val_loss: 2805.6052\n",
      "Epoch 62/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3433.6008 - val_loss: 2777.4746\n",
      "Epoch 63/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3402.5413 - val_loss: 2749.5420\n",
      "Epoch 64/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3371.6860 - val_loss: 2721.8049\n",
      "Epoch 65/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3341.0325 - val_loss: 2694.2646\n",
      "Epoch 66/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3310.5806 - val_loss: 2666.9194\n",
      "Epoch 67/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3280.3291 - val_loss: 2639.7681\n",
      "Epoch 68/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3250.2776 - val_loss: 2612.8093\n",
      "Epoch 69/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3220.4243 - val_loss: 2586.0432\n",
      "Epoch 70/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3190.7693 - val_loss: 2559.4695\n",
      "Epoch 71/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3161.3110 - val_loss: 2533.0840\n",
      "Epoch 72/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3132.0491 - val_loss: 2506.8896\n",
      "Epoch 73/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3102.9819 - val_loss: 2480.8838\n",
      "Epoch 74/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 3074.1096 - val_loss: 2455.0659\n",
      "Epoch 75/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3045.4307 - val_loss: 2429.4346\n",
      "Epoch 76/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3016.9443 - val_loss: 2403.9897\n",
      "Epoch 77/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2988.6494 - val_loss: 2378.7302\n",
      "Epoch 78/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2960.5454 - val_loss: 2353.6553\n",
      "Epoch 79/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 2932.6309 - val_loss: 2328.7629\n",
      "Epoch 80/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2904.9070 - val_loss: 2304.0542\n",
      "Epoch 81/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2877.3706 - val_loss: 2279.5271\n",
      "Epoch 82/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2850.0215 - val_loss: 2255.1821\n",
      "Epoch 83/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 2822.8594 - val_loss: 2231.0164\n",
      "Epoch 84/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2795.8826 - val_loss: 2207.0298\n",
      "Epoch 85/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2769.0913 - val_loss: 2183.2217\n",
      "Epoch 86/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2742.4834 - val_loss: 2159.5920\n",
      "Epoch 87/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2716.0593 - val_loss: 2136.1387\n",
      "Epoch 88/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2689.8179 - val_loss: 2112.8613\n",
      "Epoch 89/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2663.7576 - val_loss: 2089.7593\n",
      "Epoch 90/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2637.8774 - val_loss: 2066.8318\n",
      "Epoch 91/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2612.1785 - val_loss: 2044.0775\n",
      "Epoch 92/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2586.6582 - val_loss: 2021.4968\n",
      "Epoch 93/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2561.3167 - val_loss: 1999.0876\n",
      "Epoch 94/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2536.1523 - val_loss: 1976.8500\n",
      "Epoch 95/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2511.1648 - val_loss: 1954.7823\n",
      "Epoch 96/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 2486.3530 - val_loss: 1932.8849\n",
      "Epoch 97/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2461.7173 - val_loss: 1911.1561\n",
      "Epoch 98/500\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 2437.2551 - val_loss: 1889.5952\n",
      "Epoch 99/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2412.9673 - val_loss: 1868.2013\n",
      "Epoch 100/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2388.8513 - val_loss: 1846.9742\n",
      "Epoch 101/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2364.9075 - val_loss: 1825.9128\n",
      "Epoch 102/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2341.1353 - val_loss: 1805.0167\n",
      "Epoch 103/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2317.5337 - val_loss: 1784.2847\n",
      "Epoch 104/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 2294.1023 - val_loss: 1763.7161\n",
      "Epoch 105/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2270.8396 - val_loss: 1743.3101\n",
      "Epoch 106/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2247.7444 - val_loss: 1723.0657\n",
      "Epoch 107/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2224.8176 - val_loss: 1702.9825\n",
      "Epoch 108/500\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 2202.0571 - val_loss: 1683.0598\n",
      "Epoch 109/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2179.4624 - val_loss: 1663.2968\n",
      "Epoch 110/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2157.0327 - val_loss: 1643.6930\n",
      "Epoch 111/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2134.7683 - val_loss: 1624.2471\n",
      "Epoch 112/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2112.6672 - val_loss: 1604.9586\n",
      "Epoch 113/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2090.7290 - val_loss: 1585.8269\n",
      "Epoch 114/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2068.9531 - val_loss: 1566.8510\n",
      "Epoch 115/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2047.3389 - val_loss: 1548.0300\n",
      "Epoch 116/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 2025.8846 - val_loss: 1529.3634\n",
      "Epoch 117/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 2004.5913 - val_loss: 1510.8510\n",
      "Epoch 118/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1983.4565 - val_loss: 1492.4912\n",
      "Epoch 119/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1962.4808 - val_loss: 1474.2841\n",
      "Epoch 120/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1941.6626 - val_loss: 1456.2281\n",
      "Epoch 121/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1921.0018 - val_loss: 1438.3231\n",
      "Epoch 122/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1900.4972 - val_loss: 1420.5681\n",
      "Epoch 123/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1880.1483 - val_loss: 1402.9624\n",
      "Epoch 124/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1859.9547 - val_loss: 1385.5050\n",
      "Epoch 125/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1839.9150 - val_loss: 1368.1959\n",
      "Epoch 126/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1820.0288 - val_loss: 1351.0341\n",
      "Epoch 127/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1800.2957 - val_loss: 1334.0190\n",
      "Epoch 128/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1780.7147 - val_loss: 1317.1499\n",
      "Epoch 129/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 1761.2852 - val_loss: 1300.4254\n",
      "Epoch 130/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 1742.0062 - val_loss: 1283.8456\n",
      "Epoch 131/500\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 1722.8773 - val_loss: 1267.4092\n",
      "Epoch 132/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 1703.8979 - val_loss: 1251.1156\n",
      "Epoch 133/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 1685.0671 - val_loss: 1234.9652\n",
      "Epoch 134/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1666.3843 - val_loss: 1218.9556\n",
      "Epoch 135/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1647.8488 - val_loss: 1203.0872\n",
      "Epoch 136/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 1629.4600 - val_loss: 1187.3594\n",
      "Epoch 137/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1611.2172 - val_loss: 1171.7715\n",
      "Epoch 138/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1593.1193 - val_loss: 1156.3219\n",
      "Epoch 139/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1575.1663 - val_loss: 1141.0107\n",
      "Epoch 140/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1557.3571 - val_loss: 1125.8368\n",
      "Epoch 141/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1539.6908 - val_loss: 1110.7998\n",
      "Epoch 142/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1522.1672 - val_loss: 1095.8988\n",
      "Epoch 143/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1504.7855 - val_loss: 1081.1337\n",
      "Epoch 144/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1487.5450 - val_loss: 1066.5032\n",
      "Epoch 145/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1470.4449 - val_loss: 1052.0066\n",
      "Epoch 146/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1453.4850 - val_loss: 1037.6436\n",
      "Epoch 147/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1436.6641 - val_loss: 1023.4135\n",
      "Epoch 148/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1419.9821 - val_loss: 1009.3156\n",
      "Epoch 149/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1403.4375 - val_loss: 995.3492\n",
      "Epoch 150/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1387.0302 - val_loss: 981.5134\n",
      "Epoch 151/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1370.7596 - val_loss: 967.8082\n",
      "Epoch 152/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1354.6251 - val_loss: 954.2316\n",
      "Epoch 153/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1338.6251 - val_loss: 940.7843\n",
      "Epoch 154/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1322.7603 - val_loss: 927.4651\n",
      "Epoch 155/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1307.0294 - val_loss: 914.2736\n",
      "Epoch 156/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1291.4316 - val_loss: 901.2087\n",
      "Epoch 157/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1275.9667 - val_loss: 888.2698\n",
      "Epoch 158/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1260.6338 - val_loss: 875.4571\n",
      "Epoch 159/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1245.4323 - val_loss: 862.7684\n",
      "Epoch 160/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1230.3612 - val_loss: 850.2047\n",
      "Epoch 161/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1215.4200 - val_loss: 837.7639\n",
      "Epoch 162/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1200.6088 - val_loss: 825.4464\n",
      "Epoch 163/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1185.9260 - val_loss: 813.2510\n",
      "Epoch 164/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 1171.3719 - val_loss: 801.1780\n",
      "Epoch 165/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1156.9449 - val_loss: 789.2254\n",
      "Epoch 166/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1142.6450 - val_loss: 777.3931\n",
      "Epoch 167/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1128.4713 - val_loss: 765.6806\n",
      "Epoch 168/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1114.4233 - val_loss: 754.0870\n",
      "Epoch 169/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 1100.5005 - val_loss: 742.6118\n",
      "Epoch 170/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1086.7019 - val_loss: 731.2544\n",
      "Epoch 171/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1073.0270 - val_loss: 720.0148\n",
      "Epoch 172/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1059.4753 - val_loss: 708.8912\n",
      "Epoch 173/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1046.0461 - val_loss: 697.8834\n",
      "Epoch 174/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1032.7388 - val_loss: 686.9916\n",
      "Epoch 175/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1019.5527 - val_loss: 676.2136\n",
      "Epoch 176/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1006.4874 - val_loss: 665.5491\n",
      "Epoch 177/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 993.5418 - val_loss: 654.9987\n",
      "Epoch 178/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 980.7156 - val_loss: 644.5607\n",
      "Epoch 179/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 968.0083 - val_loss: 634.2352\n",
      "Epoch 180/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 955.4191 - val_loss: 624.0211\n",
      "Epoch 181/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 942.9473 - val_loss: 613.9172\n",
      "Epoch 182/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 930.5927 - val_loss: 603.9241\n",
      "Epoch 183/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 918.3544 - val_loss: 594.0401\n",
      "Epoch 184/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 906.2317 - val_loss: 584.2657\n",
      "Epoch 185/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 894.2244 - val_loss: 574.5984\n",
      "Epoch 186/500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 882.3308 - val_loss: 565.0404\n",
      "Epoch 187/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 870.5519 - val_loss: 555.5884\n",
      "Epoch 188/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 858.8861 - val_loss: 546.2432\n",
      "Epoch 189/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 847.3325 - val_loss: 537.0041\n",
      "Epoch 190/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 835.8912 - val_loss: 527.8693\n",
      "Epoch 191/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 824.5614 - val_loss: 518.8395\n",
      "Epoch 192/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 813.3425 - val_loss: 509.9143\n",
      "Epoch 193/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 802.2340 - val_loss: 501.0923\n",
      "Epoch 194/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 791.2352 - val_loss: 492.3724\n",
      "Epoch 195/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 780.3448 - val_loss: 483.7546\n",
      "Epoch 196/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 769.5630 - val_loss: 475.2389\n",
      "Epoch 197/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 758.8895 - val_loss: 466.8240\n",
      "Epoch 198/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 748.3230 - val_loss: 458.5084\n",
      "Epoch 199/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 737.8627 - val_loss: 450.2934\n",
      "Epoch 200/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 727.5087 - val_loss: 442.1767\n",
      "Epoch 201/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 717.2598 - val_loss: 434.1589\n",
      "Epoch 202/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 707.1155 - val_loss: 426.2385\n",
      "Epoch 203/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 697.0754 - val_loss: 418.4152\n",
      "Epoch 204/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 687.1396 - val_loss: 410.6888\n",
      "Epoch 205/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 677.3067 - val_loss: 403.0583\n",
      "Epoch 206/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 667.5756 - val_loss: 395.5229\n",
      "Epoch 207/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 657.9465 - val_loss: 388.0818\n",
      "Epoch 208/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 648.4187 - val_loss: 380.7357\n",
      "Epoch 209/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 638.9916 - val_loss: 373.4821\n",
      "Epoch 210/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 629.6644 - val_loss: 366.3219\n",
      "Epoch 211/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 620.4368 - val_loss: 359.2540\n",
      "Epoch 212/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 611.3080 - val_loss: 352.2776\n",
      "Epoch 213/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 602.2770 - val_loss: 345.3921\n",
      "Epoch 214/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 593.3439 - val_loss: 338.5969\n",
      "Epoch 215/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 584.5080 - val_loss: 331.8916\n",
      "Epoch 216/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 575.2678 - val_loss: 323.3252\n",
      "Epoch 217/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 563.4103 - val_loss: 314.6533\n",
      "Epoch 218/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 552.0906 - val_loss: 306.2893\n",
      "Epoch 219/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 540.6152 - val_loss: 293.0985\n",
      "Epoch 220/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 522.9445 - val_loss: 283.6249\n",
      "Epoch 221/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 510.4411 - val_loss: 274.5552\n",
      "Epoch 222/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 498.5988 - val_loss: 266.0551\n",
      "Epoch 223/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 487.4235 - val_loss: 258.0247\n",
      "Epoch 224/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 476.7852 - val_loss: 250.3743\n",
      "Epoch 225/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 466.5826 - val_loss: 243.0409\n",
      "Epoch 226/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 456.7456 - val_loss: 235.9804\n",
      "Epoch 227/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 447.2246 - val_loss: 229.1620\n",
      "Epoch 228/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 437.9841 - val_loss: 222.5619\n",
      "Epoch 229/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 428.9966 - val_loss: 216.1616\n",
      "Epoch 230/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 420.2408 - val_loss: 209.9465\n",
      "Epoch 231/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 411.6999 - val_loss: 203.9050\n",
      "Epoch 232/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 403.3604 - val_loss: 198.0272\n",
      "Epoch 233/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 395.2097 - val_loss: 192.3044\n",
      "Epoch 234/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 387.2384 - val_loss: 186.7290\n",
      "Epoch 235/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 379.4378 - val_loss: 181.2949\n",
      "Epoch 236/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 371.8004 - val_loss: 175.9963\n",
      "Epoch 237/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 364.3193 - val_loss: 170.8279\n",
      "Epoch 238/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 356.9884 - val_loss: 165.7853\n",
      "Epoch 239/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 349.8031 - val_loss: 160.8647\n",
      "Epoch 240/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 342.7580 - val_loss: 156.0618\n",
      "Epoch 241/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 335.8489 - val_loss: 151.3737\n",
      "Epoch 242/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 329.0718 - val_loss: 146.7965\n",
      "Epoch 243/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 322.4227 - val_loss: 142.3272\n",
      "Epoch 244/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 315.8982 - val_loss: 137.9632\n",
      "Epoch 245/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 309.4952 - val_loss: 133.7025\n",
      "Epoch 246/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 303.2110 - val_loss: 129.5416\n",
      "Epoch 247/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 297.0425 - val_loss: 125.4790\n",
      "Epoch 248/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 290.9872 - val_loss: 121.5119\n",
      "Epoch 249/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 285.0425 - val_loss: 117.6383\n",
      "Epoch 250/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 279.2059 - val_loss: 113.8567\n",
      "Epoch 251/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 273.4753 - val_loss: 110.1645\n",
      "Epoch 252/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 267.8489 - val_loss: 106.5604\n",
      "Epoch 253/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 262.3242 - val_loss: 103.0423\n",
      "Epoch 254/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 256.8994 - val_loss: 99.6088\n",
      "Epoch 255/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 251.5727 - val_loss: 96.2581\n",
      "Epoch 256/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 246.3421 - val_loss: 92.9884\n",
      "Epoch 257/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 241.2060 - val_loss: 89.7985\n",
      "Epoch 258/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 236.1627 - val_loss: 86.6869\n",
      "Epoch 259/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 231.2105 - val_loss: 83.6517\n",
      "Epoch 260/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 226.3481 - val_loss: 80.6921\n",
      "Epoch 261/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 221.5738 - val_loss: 77.8065\n",
      "Epoch 262/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 216.8862 - val_loss: 74.9937\n",
      "Epoch 263/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 212.2838 - val_loss: 72.2522\n",
      "Epoch 264/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 207.7652 - val_loss: 69.5809\n",
      "Epoch 265/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 203.3292 - val_loss: 66.9786\n",
      "Epoch 266/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 198.9745 - val_loss: 64.4439\n",
      "Epoch 267/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 194.6994 - val_loss: 61.9755\n",
      "Epoch 268/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 190.5031 - val_loss: 59.5726\n",
      "Epoch 269/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 186.3841 - val_loss: 57.2341\n",
      "Epoch 270/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 182.3417 - val_loss: 54.9585\n",
      "Epoch 271/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 178.3739 - val_loss: 52.7452\n",
      "Epoch 272/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 174.4803 - val_loss: 50.5928\n",
      "Epoch 273/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 170.6594 - val_loss: 48.5005\n",
      "Epoch 274/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 166.9104 - val_loss: 46.4668\n",
      "Epoch 275/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 163.2319 - val_loss: 44.4913\n",
      "Epoch 276/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 159.6228 - val_loss: 42.5725\n",
      "Epoch 277/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 156.0822 - val_loss: 40.7095\n",
      "Epoch 278/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 152.6090 - val_loss: 38.9014\n",
      "Epoch 279/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 149.2021 - val_loss: 37.1473\n",
      "Epoch 280/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 145.8608 - val_loss: 35.4462\n",
      "Epoch 281/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 142.5841 - val_loss: 33.7973\n",
      "Epoch 282/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 139.3707 - val_loss: 32.1995\n",
      "Epoch 283/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 136.2200 - val_loss: 30.6520\n",
      "Epoch 284/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 133.1308 - val_loss: 29.1538\n",
      "Epoch 285/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 130.1024 - val_loss: 27.7042\n",
      "Epoch 286/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 127.1337 - val_loss: 26.3022\n",
      "Epoch 287/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 124.2240 - val_loss: 24.9470\n",
      "Epoch 288/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 121.3723 - val_loss: 23.6376\n",
      "Epoch 289/500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 118.5776 - val_loss: 22.3733\n",
      "Epoch 290/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 115.8392 - val_loss: 21.1533\n",
      "Epoch 291/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 113.1563 - val_loss: 19.9767\n",
      "Epoch 292/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 110.5279 - val_loss: 18.8426\n",
      "Epoch 293/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 107.9534 - val_loss: 17.7504\n",
      "Epoch 294/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 105.4315 - val_loss: 16.6992\n",
      "Epoch 295/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 102.9620 - val_loss: 15.6882\n",
      "Epoch 296/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 100.5435 - val_loss: 14.7165\n",
      "Epoch 297/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 98.1755 - val_loss: 13.7838\n",
      "Epoch 298/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 95.8575 - val_loss: 12.8888\n",
      "Epoch 299/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 93.5884 - val_loss: 12.0311\n",
      "Epoch 300/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 91.3674 - val_loss: 11.2097\n",
      "Epoch 301/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 89.1936 - val_loss: 10.4240\n",
      "Epoch 302/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 87.0665 - val_loss: 9.6732\n",
      "Epoch 303/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 84.9854 - val_loss: 8.9568\n",
      "Epoch 304/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 82.9497 - val_loss: 8.2739\n",
      "Epoch 305/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 80.9583 - val_loss: 7.6238\n",
      "Epoch 306/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 79.0106 - val_loss: 7.0057\n",
      "Epoch 307/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 77.1060 - val_loss: 6.4192\n",
      "Epoch 308/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 75.2437 - val_loss: 5.8634\n",
      "Epoch 309/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 73.4230 - val_loss: 5.3376\n",
      "Epoch 310/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 71.6431 - val_loss: 4.8413\n",
      "Epoch 311/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 69.9035 - val_loss: 4.3738\n",
      "Epoch 312/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 68.2037 - val_loss: 3.9342\n",
      "Epoch 313/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 66.5424 - val_loss: 3.5221\n",
      "Epoch 314/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 64.9195 - val_loss: 3.1369\n",
      "Epoch 315/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 63.3341 - val_loss: 2.7777\n",
      "Epoch 316/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 61.7858 - val_loss: 2.4442\n",
      "Epoch 317/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 60.2737 - val_loss: 2.1355\n",
      "Epoch 318/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 58.7973 - val_loss: 1.8513\n",
      "Epoch 319/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 57.3561 - val_loss: 1.5907\n",
      "Epoch 320/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 55.9492 - val_loss: 1.3532\n",
      "Epoch 321/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 54.5760 - val_loss: 1.1382\n",
      "Epoch 322/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 53.2360 - val_loss: 0.9452\n",
      "Epoch 323/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 51.9286 - val_loss: 0.7735\n",
      "Epoch 324/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 50.6530 - val_loss: 0.6225\n",
      "Epoch 325/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 49.4090 - val_loss: 0.4919\n",
      "Epoch 326/500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 48.1957 - val_loss: 0.3809\n",
      "Epoch 327/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 47.0128 - val_loss: 0.2890\n",
      "Epoch 328/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 45.8596 - val_loss: 0.2157\n",
      "Epoch 329/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 44.7354 - val_loss: 0.1604\n",
      "Epoch 330/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 43.6398 - val_loss: 0.1226\n",
      "Epoch 331/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 42.5722 - val_loss: 0.1018\n",
      "Epoch 332/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 41.5320 - val_loss: 0.0975\n",
      "Epoch 333/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 40.5189 - val_loss: 0.1091\n",
      "Epoch 334/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 39.5320 - val_loss: 0.1362\n",
      "Epoch 335/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 38.5713 - val_loss: 0.1783\n",
      "Epoch 336/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 37.6358 - val_loss: 0.2349\n",
      "Epoch 337/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 36.7250 - val_loss: 0.3054\n",
      "Epoch 338/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 35.8387 - val_loss: 0.3895\n",
      "Epoch 339/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 34.9762 - val_loss: 0.4866\n",
      "Epoch 340/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 34.1371 - val_loss: 0.5963\n",
      "Epoch 341/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 33.3209 - val_loss: 0.7182\n",
      "Epoch 342/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 32.5269 - val_loss: 0.8518\n",
      "Epoch 343/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 31.7549 - val_loss: 0.9966\n",
      "Epoch 344/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 31.0043 - val_loss: 1.1523\n",
      "Epoch 345/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 30.2747 - val_loss: 1.3183\n",
      "Epoch 346/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 29.5657 - val_loss: 1.4943\n",
      "Epoch 347/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 28.8768 - val_loss: 1.6800\n",
      "Epoch 348/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 28.2074 - val_loss: 1.8748\n",
      "Epoch 349/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 27.5573 - val_loss: 2.0782\n",
      "Epoch 350/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 26.9260 - val_loss: 2.2902\n",
      "Epoch 351/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 26.3128 - val_loss: 2.5101\n",
      "Epoch 352/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 25.7177 - val_loss: 2.7377\n",
      "Epoch 353/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 25.1401 - val_loss: 2.9725\n",
      "Epoch 354/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 24.5796 - val_loss: 3.2141\n",
      "Epoch 355/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 24.0358 - val_loss: 3.4623\n",
      "Epoch 356/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 23.5084 - val_loss: 3.7168\n",
      "Epoch 357/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 22.9968 - val_loss: 3.9770\n",
      "Epoch 358/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 22.5008 - val_loss: 4.2426\n",
      "Epoch 359/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 22.0200 - val_loss: 4.5136\n",
      "Epoch 360/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 21.5538 - val_loss: 4.7893\n",
      "Epoch 361/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 21.1022 - val_loss: 5.0697\n",
      "Epoch 362/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.6646 - val_loss: 5.3543\n",
      "Epoch 363/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.2407 - val_loss: 5.6427\n",
      "Epoch 364/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 19.8302 - val_loss: 5.9348\n",
      "Epoch 365/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 19.4328 - val_loss: 6.2302\n",
      "Epoch 366/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 19.0480 - val_loss: 6.5287\n",
      "Epoch 367/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 18.6757 - val_loss: 6.8301\n",
      "Epoch 368/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 18.3154 - val_loss: 7.1339\n",
      "Epoch 369/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 17.9668 - val_loss: 7.4399\n",
      "Epoch 370/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 17.6297 - val_loss: 7.7482\n",
      "Epoch 371/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 17.3036 - val_loss: 8.0580\n",
      "Epoch 372/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 16.9883 - val_loss: 8.3694\n",
      "Epoch 373/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 16.6836 - val_loss: 8.6821\n",
      "Epoch 374/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 16.3891 - val_loss: 8.9959\n",
      "Epoch 375/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.1046 - val_loss: 9.3104\n",
      "Epoch 376/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.8297 - val_loss: 9.6256\n",
      "Epoch 377/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.5643 - val_loss: 9.9413\n",
      "Epoch 378/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.3079 - val_loss: 10.2571\n",
      "Epoch 379/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.0605 - val_loss: 10.5730\n",
      "Epoch 380/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 14.8217 - val_loss: 10.8886\n",
      "Epoch 381/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 14.5912 - val_loss: 11.2040\n",
      "Epoch 382/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 14.3688 - val_loss: 11.5188\n",
      "Epoch 383/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 14.1544 - val_loss: 11.8328\n",
      "Epoch 384/500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 13.9476 - val_loss: 12.1460\n",
      "Epoch 385/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 13.7482 - val_loss: 12.4581\n",
      "Epoch 386/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 13.5561 - val_loss: 12.7690\n",
      "Epoch 387/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 13.3709 - val_loss: 13.0786\n",
      "Epoch 388/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 13.1924 - val_loss: 13.3866\n",
      "Epoch 389/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 13.0206 - val_loss: 13.6930\n",
      "Epoch 390/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 12.8550 - val_loss: 13.9977\n",
      "Epoch 391/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 12.6957 - val_loss: 14.3005\n",
      "Epoch 392/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 12.5423 - val_loss: 14.6012\n",
      "Epoch 393/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 12.3946 - val_loss: 14.8998\n",
      "Epoch 394/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 12.2525 - val_loss: 15.1961\n",
      "Epoch 395/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 12.1159 - val_loss: 15.4900\n",
      "Epoch 396/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.9844 - val_loss: 15.7814\n",
      "Epoch 397/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 11.8580 - val_loss: 16.0705\n",
      "Epoch 398/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 11.7365 - val_loss: 16.3567\n",
      "Epoch 399/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.6198 - val_loss: 16.6406\n",
      "Epoch 400/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.5075 - val_loss: 16.9211\n",
      "Epoch 401/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 11.3997 - val_loss: 17.1991\n",
      "Epoch 402/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 11.2961 - val_loss: 17.4741\n",
      "Epoch 403/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.1966 - val_loss: 17.7460\n",
      "Epoch 404/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 11.1011 - val_loss: 18.0147\n",
      "Epoch 405/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 11.0095 - val_loss: 18.2803\n",
      "Epoch 406/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 10.9214 - val_loss: 18.5427\n",
      "Epoch 407/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 10.8370 - val_loss: 18.8017\n",
      "Epoch 408/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 10.7560 - val_loss: 19.0575\n",
      "Epoch 409/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 10.6783 - val_loss: 19.3102\n",
      "Epoch 410/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 10.6038 - val_loss: 19.5592\n",
      "Epoch 411/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 10.5324 - val_loss: 19.8049\n",
      "Epoch 412/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 10.4639 - val_loss: 20.0471\n",
      "Epoch 413/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 10.3983 - val_loss: 20.2857\n",
      "Epoch 414/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 10.3355 - val_loss: 20.5208\n",
      "Epoch 415/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 10.2752 - val_loss: 20.7523\n",
      "Epoch 416/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 10.2176 - val_loss: 20.9803\n",
      "Epoch 417/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 10.1623 - val_loss: 21.2047\n",
      "Epoch 418/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 10.1094 - val_loss: 21.4254\n",
      "Epoch 419/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 10.0588 - val_loss: 21.6426\n",
      "Epoch 420/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 10.0104 - val_loss: 21.8562\n",
      "Epoch 421/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.9640 - val_loss: 22.0661\n",
      "Epoch 422/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.9197 - val_loss: 22.2725\n",
      "Epoch 423/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.8773 - val_loss: 22.4753\n",
      "Epoch 424/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 9.8366 - val_loss: 22.6745\n",
      "Epoch 425/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.7978 - val_loss: 22.8697\n",
      "Epoch 426/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.7608 - val_loss: 23.0616\n",
      "Epoch 427/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.7253 - val_loss: 23.2500\n",
      "Epoch 428/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.6914 - val_loss: 23.4346\n",
      "Epoch 429/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.6590 - val_loss: 23.6159\n",
      "Epoch 430/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.6281 - val_loss: 23.7935\n",
      "Epoch 431/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.5985 - val_loss: 23.9674\n",
      "Epoch 432/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.5703 - val_loss: 24.1379\n",
      "Epoch 433/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.5433 - val_loss: 24.3050\n",
      "Epoch 434/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.5176 - val_loss: 24.4688\n",
      "Epoch 435/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.4929 - val_loss: 24.6288\n",
      "Epoch 436/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.4695 - val_loss: 24.7858\n",
      "Epoch 437/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 9.4471 - val_loss: 24.9390\n",
      "Epoch 438/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 9.4257 - val_loss: 25.0891\n",
      "Epoch 439/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 9.4053 - val_loss: 25.2360\n",
      "Epoch 440/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 9.3858 - val_loss: 25.3794\n",
      "Epoch 441/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 9.3673 - val_loss: 25.5195\n",
      "Epoch 442/500\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 9.3495 - val_loss: 25.6567\n",
      "Epoch 443/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.3326 - val_loss: 25.7904\n",
      "Epoch 444/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.3165 - val_loss: 25.9213\n",
      "Epoch 445/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.3011 - val_loss: 26.0489\n",
      "Epoch 446/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.2865 - val_loss: 26.1733\n",
      "Epoch 447/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.2725 - val_loss: 26.2949\n",
      "Epoch 448/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.2591 - val_loss: 26.4133\n",
      "Epoch 449/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.2464 - val_loss: 26.5289\n",
      "Epoch 450/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.2343 - val_loss: 26.6415\n",
      "Epoch 451/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.2228 - val_loss: 26.7514\n",
      "Epoch 452/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 9.2118 - val_loss: 26.8582\n",
      "Epoch 453/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.2013 - val_loss: 26.9625\n",
      "Epoch 454/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.1913 - val_loss: 27.0641\n",
      "Epoch 455/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 9.1817 - val_loss: 27.1628\n",
      "Epoch 456/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.1727 - val_loss: 27.2591\n",
      "Epoch 457/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.1640 - val_loss: 27.3527\n",
      "Epoch 458/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.1558 - val_loss: 27.4437\n",
      "Epoch 459/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.1479 - val_loss: 27.5324\n",
      "Epoch 460/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.1404 - val_loss: 27.6184\n",
      "Epoch 461/500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 9.1333 - val_loss: 27.7020\n",
      "Epoch 462/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.1265 - val_loss: 27.7833\n",
      "Epoch 463/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.1200 - val_loss: 27.8622\n",
      "Epoch 464/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.1139 - val_loss: 27.9392\n",
      "Epoch 465/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.1080 - val_loss: 28.0137\n",
      "Epoch 466/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.1024 - val_loss: 28.0860\n",
      "Epoch 467/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.0971 - val_loss: 28.1564\n",
      "Epoch 468/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.0919 - val_loss: 28.2245\n",
      "Epoch 469/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.0871 - val_loss: 28.2906\n",
      "Epoch 470/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 9.0825 - val_loss: 28.3547\n",
      "Epoch 471/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.0781 - val_loss: 28.4169\n",
      "Epoch 472/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.0739 - val_loss: 28.4771\n",
      "Epoch 473/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.0699 - val_loss: 28.5355\n",
      "Epoch 474/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 9.0661 - val_loss: 28.5919\n",
      "Epoch 475/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.0624 - val_loss: 28.6467\n",
      "Epoch 476/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.0590 - val_loss: 28.6994\n",
      "Epoch 477/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.0557 - val_loss: 28.7509\n",
      "Epoch 478/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.0526 - val_loss: 28.8004\n",
      "Epoch 479/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 9.0496 - val_loss: 28.8484\n",
      "Epoch 480/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 9.0467 - val_loss: 28.8946\n",
      "Epoch 481/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.0441 - val_loss: 28.9397\n",
      "Epoch 482/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.0415 - val_loss: 28.9829\n",
      "Epoch 483/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.0390 - val_loss: 29.0250\n",
      "Epoch 484/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.0367 - val_loss: 29.0654\n",
      "Epoch 485/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 9.0344 - val_loss: 29.1045\n",
      "Epoch 486/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.0323 - val_loss: 29.1422\n",
      "Epoch 487/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.0303 - val_loss: 29.1786\n",
      "Epoch 488/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.0284 - val_loss: 29.2139\n",
      "Epoch 489/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.0265 - val_loss: 29.2476\n",
      "Epoch 490/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.0248 - val_loss: 29.2806\n",
      "Epoch 491/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.0231 - val_loss: 29.3122\n",
      "Epoch 492/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.0215 - val_loss: 29.3424\n",
      "Epoch 493/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.0200 - val_loss: 29.3716\n",
      "Epoch 494/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 9.0186 - val_loss: 29.3997\n",
      "Epoch 495/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.0173 - val_loss: 29.4271\n",
      "Epoch 496/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 9.0159 - val_loss: 29.4533\n",
      "Epoch 497/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.0147 - val_loss: 29.4786\n",
      "Epoch 498/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 9.0135 - val_loss: 29.5027\n",
      "Epoch 499/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 9.0124 - val_loss: 29.5260\n",
      "Epoch 500/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 9.0113 - val_loss: 29.5484\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.1552, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0026, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0004, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0102, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.8734, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 432ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.20211765e+01, 7.19741176e+01, 7.19270588e+01, 7.18800000e+01,\n",
       "        7.18329412e+01, 7.17858823e+01, 7.17388235e+01, 7.16917647e+01,\n",
       "        7.16447059e+01, 7.15976471e+01, 7.15505882e+01, 7.15035294e+01,\n",
       "        7.14564706e+01, 7.14094118e+01, 7.13623529e+01, 7.13152941e+01,\n",
       "        7.12682353e+01, 7.12211765e+01, 7.12112605e+01, 7.12022969e+01,\n",
       "        7.11933333e+01, 7.11843698e+01, 7.11754062e+01, 7.11664426e+01,\n",
       "        7.11574790e+01, 7.11485154e+01, 7.11395518e+01, 7.11305882e+01,\n",
       "        7.11216247e+01, 7.11126611e+01, 7.11036975e+01, 7.10947339e+01,\n",
       "        7.10857703e+01, 7.10768067e+01, 7.10678431e+01, 7.10588796e+01,\n",
       "        7.10499160e+01, 7.10409524e+01, 7.10319888e+01, 7.10230252e+01,\n",
       "        7.10140616e+01, 7.10050980e+01, 7.09961344e+01, 7.09871709e+01,\n",
       "        7.09782073e+01, 7.09692437e+01, 7.09602801e+01, 7.09513165e+01,\n",
       "        7.09423529e+01, 7.09333894e+01, 7.09244258e+01, 7.09154622e+01,\n",
       "        7.09064986e+01, 7.08976891e+01, 7.08892857e+01, 7.08808823e+01,\n",
       "        7.08724790e+01, 7.08640756e+01, 7.08556723e+01, 7.08472689e+01,\n",
       "        7.08388655e+01, 7.08304622e+01, 7.08220588e+01, 7.08136555e+01,\n",
       "        7.08052521e+01, 7.07968487e+01, 7.07884454e+01, 7.07800420e+01,\n",
       "        7.07716387e+01, 7.07632353e+01, 7.07548319e+01, 7.07464286e+01,\n",
       "        7.07380252e+01, 7.07296219e+01, 7.07212185e+01, 7.07128151e+01,\n",
       "        7.07044118e+01, 7.06960084e+01, 7.06876050e+01, 7.06792017e+01,\n",
       "        7.64097366e+01, 3.94928992e-01, 1.01987824e-01, 2.27639116e-02,\n",
       "        3.73155981e-01, 0.00000000e+00, 1.45036131e-01, 3.30339521e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.04044646e-01,\n",
       "        0.00000000e+00, 7.92246580e-01, 3.02265938e-02, 5.25366127e-01,\n",
       "        6.44484609e-02, 2.92133838e-01, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([68.90466853, 68.88972923, 68.87478992, 68.85985061, 68.8449113 ,\n",
       "       68.82997199, 68.81503268, 68.80009337, 68.78515406, 68.77021475,\n",
       "       68.75527544, 68.74033613, 68.72539683, 68.71045752, 68.69551821,\n",
       "       68.6805789 , 68.66563959, 68.65070028, 68.63576097, 68.62082166,\n",
       "       68.60588235, 68.59094304, 68.57600373, 68.56106443, 68.54612512,\n",
       "       68.53118581, 68.5162465 , 68.50130719, 68.48636788, 68.47142857,\n",
       "       68.45648926, 68.44154995, 68.42661064, 68.41167134, 68.39673203,\n",
       "       68.38179272, 68.36685341, 68.3519141 , 68.33697479, 68.32203548,\n",
       "       68.30709617, 68.29754902, 68.29288049, 68.28821195, 68.28354342,\n",
       "       68.27887488, 68.27420635, 68.26953782, 68.26486928, 68.26020075,\n",
       "       68.25553221, 68.25086368, 68.24619514, 68.24152661, 68.23685808,\n",
       "       68.23218954, 68.22752101, 68.22285247, 68.21818394, 68.21351541,\n",
       "       68.20884687, 68.20417834, 68.1995098 , 68.19484127, 68.19017274,\n",
       "       68.1855042 , 68.18083567, 68.17616713, 68.1714986 , 68.16683007,\n",
       "       68.16216153, 68.157493  , 68.15282446, 68.14815593, 68.14348739,\n",
       "       68.13881886, 68.13415033, 68.12948179, 68.12481326, 68.12014472,\n",
       "       68.11547619, 68.11080766, 68.10613912, 68.10147059, 68.09680205,\n",
       "       68.09213352, 68.08746499, 68.08279645, 68.07812792, 68.07345938,\n",
       "       68.06879085, 68.06412232, 68.05945378, 68.05478525, 68.05011671,\n",
       "       68.04544818, 68.04077965, 68.03611111, 68.03144258, 68.02677404])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.706599209722\n",
      "15.129683894760603\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
