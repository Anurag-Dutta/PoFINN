{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "2245    56.056548\n",
       "2246    56.040972\n",
       "2247    56.025395\n",
       "2248    56.009818\n",
       "2249    55.994241\n",
       "Name: C2, Length: 2250, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c2_interpolated_2150_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "2145     0.300421\n",
       "2146     0.000000\n",
       "2147     0.000000\n",
       "2148     0.000000\n",
       "2149     0.000000\n",
       "Name: C2, Length: 2150, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2150)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.500000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.275910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.051821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.827731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.603641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     90.500000  0.000298\n",
       "1     90.275910  0.000298\n",
       "2     90.051821  0.000297\n",
       "3     89.827731  0.000297\n",
       "4     89.603641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.1552  # Value for C0\n",
    "K0 = -0.0026  # Value for K0\n",
    "K1 = -0.0004  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0102    # Value for b\n",
    "c = 2.8734    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    90.500000    90.275910    90.051821    89.827731    89.603641   \n",
      "351    90.275910    90.051821    89.827731    89.603641    89.379552   \n",
      "352    90.051821    89.827731    89.603641    89.379552    89.155462   \n",
      "353    89.827731    89.603641    89.379552    89.155462    88.931373   \n",
      "354    89.603641    89.379552    89.155462    88.931373    88.707283   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    89.379552    89.155462    88.931373    88.707283    88.494958  ...   \n",
      "351    89.155462    88.931373    88.707283    88.494958    88.427731  ...   \n",
      "352    88.931373    88.707283    88.494958    88.427731    88.360504  ...   \n",
      "353    88.707283    88.494958    88.427731    88.360504    88.293277  ...   \n",
      "354    88.494958    88.427731    88.360504    88.293277    88.226050  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   79.071008    0.000263   79.054202    0.000263   79.037395    0.000263   \n",
      "351   79.054202    0.000263   79.037395    0.000263   79.020588    0.000262   \n",
      "352   79.037395    0.000263   79.020588    0.000262   79.003782    0.000262   \n",
      "353   79.020588    0.000262   79.003782    0.000262   78.986975    0.000262   \n",
      "354   79.003782    0.000262   78.986975    0.000262   78.970168    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   79.020588    0.000262   79.003782    0.000262  \n",
      "351   79.003782    0.000262   78.986975    0.000262  \n",
      "352   78.986975    0.000262   78.970168    0.000262  \n",
      "353   78.970168    0.000262   78.953361    0.000262  \n",
      "354   78.953361    0.000262   78.936555    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1700, 1, 251) (1700, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 3s 29ms/step - loss: 5199.3735 - val_loss: 2939.3816\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4923.0415 - val_loss: 2787.1741\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4721.7896 - val_loss: 2700.3245\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4600.4136 - val_loss: 2630.5376\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4479.4180 - val_loss: 2567.2222\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4376.1987 - val_loss: 2510.5083\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4277.1304 - val_loss: 2456.3691\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4181.3281 - val_loss: 2404.3291\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 4088.1978 - val_loss: 2354.1165\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 3997.3889 - val_loss: 2305.5664\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 3908.6802 - val_loss: 2258.5667\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 3821.9170 - val_loss: 2213.0339\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 3736.9871 - val_loss: 2168.9050\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3653.8040 - val_loss: 2126.1270\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 3572.2974 - val_loss: 2084.6567\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 3492.4099 - val_loss: 2044.4561\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3414.0930 - val_loss: 2005.4919\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3337.3037 - val_loss: 1967.7346\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3262.0051 - val_loss: 1931.1559\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 3188.1636 - val_loss: 1895.7303\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 3115.7471 - val_loss: 1861.4346\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 3044.7285 - val_loss: 1828.2456\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2975.0801 - val_loss: 1796.1432\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2906.7791 - val_loss: 1765.1053\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2839.8003 - val_loss: 1735.1129\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2774.1211 - val_loss: 1706.1465\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2709.7219 - val_loss: 1678.1882\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2646.5806 - val_loss: 1651.2195\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2584.6775 - val_loss: 1625.2231\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2523.9939 - val_loss: 1600.1814\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2464.5107 - val_loss: 1576.0785\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2406.2109 - val_loss: 1552.8971\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2349.0757 - val_loss: 1530.6208\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2293.0881 - val_loss: 1509.2346\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2238.2317 - val_loss: 1488.7219\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2184.4893 - val_loss: 1469.0677\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 2131.8445 - val_loss: 1450.2563\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2080.2817 - val_loss: 1432.2729\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2029.7852 - val_loss: 1415.1028\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1980.3398 - val_loss: 1398.7307\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1931.9294 - val_loss: 1383.1423\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1884.5403 - val_loss: 1368.3230\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1838.1569 - val_loss: 1354.2592\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1792.7638 - val_loss: 1340.9357\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1748.3484 - val_loss: 1328.3390\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1704.8945 - val_loss: 1316.4551\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1662.3888 - val_loss: 1305.2705\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1620.8180 - val_loss: 1294.7711\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1580.1671 - val_loss: 1284.9435\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1540.4231 - val_loss: 1275.7745\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1501.5725 - val_loss: 1267.2504\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1463.6016 - val_loss: 1259.3580\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1426.4974 - val_loss: 1252.0842\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1390.2465 - val_loss: 1245.4160\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1354.8357 - val_loss: 1239.3405\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1320.2528 - val_loss: 1233.8445\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1286.4843 - val_loss: 1228.9154\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1253.5178 - val_loss: 1224.5408\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1221.3406 - val_loss: 1220.7078\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1189.9401 - val_loss: 1217.4038\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1159.3042 - val_loss: 1214.6162\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1129.4205 - val_loss: 1212.3331\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1100.2770 - val_loss: 1210.5419\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1071.8612 - val_loss: 1209.2308\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1044.1614 - val_loss: 1208.3871\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1017.1653 - val_loss: 1207.9990\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 990.8616 - val_loss: 1208.0548\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 965.2384 - val_loss: 1208.5422\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 940.2838 - val_loss: 1209.4498\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 915.9863 - val_loss: 1210.7657\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 892.3347 - val_loss: 1212.4783\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 869.3176 - val_loss: 1214.5758\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 846.9235 - val_loss: 1217.0471\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 825.1414 - val_loss: 1219.8807\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 803.9601 - val_loss: 1223.0651\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 783.3687 - val_loss: 1226.5894\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 763.3561 - val_loss: 1230.4421\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 743.9110 - val_loss: 1234.6122\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 725.0236 - val_loss: 1239.0890\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 706.6823 - val_loss: 1243.8613\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 688.8770 - val_loss: 1248.9186\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 671.5969 - val_loss: 1254.2499\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 654.8319 - val_loss: 1259.8447\n",
      "Epoch 84/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 638.5714 - val_loss: 1265.6926\n",
      "Epoch 85/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 622.8049 - val_loss: 1271.7831\n",
      "Epoch 86/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 607.5226 - val_loss: 1278.1060\n",
      "Epoch 87/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 592.7142 - val_loss: 1284.6504\n",
      "Epoch 88/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 578.3698 - val_loss: 1291.4070\n",
      "Epoch 89/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 564.4792 - val_loss: 1298.3655\n",
      "Epoch 90/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 551.0329 - val_loss: 1305.5156\n",
      "Epoch 91/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 538.0209 - val_loss: 1312.8478\n",
      "Epoch 92/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 525.4339 - val_loss: 1320.3523\n",
      "Epoch 93/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 513.2618 - val_loss: 1328.0199\n",
      "Epoch 94/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 501.4954 - val_loss: 1335.8408\n",
      "Epoch 95/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 490.1255 - val_loss: 1343.8052\n",
      "Epoch 96/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 479.1424 - val_loss: 1351.9039\n",
      "Epoch 97/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 468.5373 - val_loss: 1360.1288\n",
      "Epoch 98/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 458.3008 - val_loss: 1368.4698\n",
      "Epoch 99/500\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 448.4241 - val_loss: 1376.9183\n",
      "Epoch 100/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 438.8982 - val_loss: 1385.4656\n",
      "Epoch 101/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 429.7144 - val_loss: 1394.1028\n",
      "Epoch 102/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 420.8637 - val_loss: 1402.8220\n",
      "Epoch 103/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 412.3378 - val_loss: 1411.6144\n",
      "Epoch 104/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 404.1281 - val_loss: 1420.4716\n",
      "Epoch 105/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 396.2263 - val_loss: 1429.3854\n",
      "Epoch 106/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 388.6239 - val_loss: 1438.3488\n",
      "Epoch 107/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 381.3129 - val_loss: 1447.3530\n",
      "Epoch 108/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 374.2852 - val_loss: 1456.3911\n",
      "Epoch 109/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 367.5328 - val_loss: 1465.4547\n",
      "Epoch 110/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 361.0479 - val_loss: 1474.5376\n",
      "Epoch 111/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 354.8226 - val_loss: 1483.6313\n",
      "Epoch 112/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 348.8495 - val_loss: 1492.7294\n",
      "Epoch 113/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 343.1209 - val_loss: 1501.8253\n",
      "Epoch 114/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 337.6296 - val_loss: 1510.9120\n",
      "Epoch 115/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 332.3683 - val_loss: 1519.9830\n",
      "Epoch 116/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 327.3296 - val_loss: 1529.0321\n",
      "Epoch 117/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 322.5067 - val_loss: 1538.0525\n",
      "Epoch 118/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 317.8927 - val_loss: 1547.0389\n",
      "Epoch 119/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 313.4806 - val_loss: 1555.9855\n",
      "Epoch 120/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 309.2640 - val_loss: 1564.8860\n",
      "Epoch 121/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 305.2361 - val_loss: 1573.7350\n",
      "Epoch 122/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 301.3907 - val_loss: 1582.5272\n",
      "Epoch 123/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 297.7214 - val_loss: 1591.2583\n",
      "Epoch 124/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 294.2220 - val_loss: 1599.9219\n",
      "Epoch 125/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 290.8865 - val_loss: 1608.5145\n",
      "Epoch 126/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 287.7090 - val_loss: 1617.0305\n",
      "Epoch 127/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 284.6838 - val_loss: 1625.4656\n",
      "Epoch 128/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 281.8051 - val_loss: 1633.8168\n",
      "Epoch 129/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 279.0675 - val_loss: 1642.0796\n",
      "Epoch 130/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 276.4654 - val_loss: 1650.2485\n",
      "Epoch 131/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 273.9939 - val_loss: 1658.3220\n",
      "Epoch 132/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 271.6475 - val_loss: 1666.2958\n",
      "Epoch 133/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 269.4213 - val_loss: 1674.1664\n",
      "Epoch 134/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 267.3105 - val_loss: 1681.9316\n",
      "Epoch 135/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 265.3102 - val_loss: 1689.5869\n",
      "Epoch 136/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 263.4160 - val_loss: 1697.1316\n",
      "Epoch 137/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 261.6233 - val_loss: 1704.5614\n",
      "Epoch 138/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 259.9276 - val_loss: 1711.8750\n",
      "Epoch 139/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 258.3248 - val_loss: 1719.0698\n",
      "Epoch 140/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 256.8109 - val_loss: 1726.1449\n",
      "Epoch 141/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 255.3818 - val_loss: 1733.0963\n",
      "Epoch 142/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 254.0336 - val_loss: 1739.9247\n",
      "Epoch 143/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 252.7625 - val_loss: 1746.6271\n",
      "Epoch 144/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 251.5650 - val_loss: 1753.2028\n",
      "Epoch 145/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 250.4377 - val_loss: 1759.6515\n",
      "Epoch 146/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 249.3770 - val_loss: 1765.9712\n",
      "Epoch 147/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 248.3796 - val_loss: 1772.1624\n",
      "Epoch 148/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 247.4427 - val_loss: 1778.2234\n",
      "Epoch 149/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 246.5630 - val_loss: 1784.1555\n",
      "Epoch 150/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 245.7376 - val_loss: 1789.9579\n",
      "Epoch 151/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 244.9637 - val_loss: 1795.2489\n",
      "Epoch 152/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 244.2387 - val_loss: 1801.1378\n",
      "Epoch 153/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 243.5599 - val_loss: 1806.5425\n",
      "Epoch 154/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 242.9249 - val_loss: 1811.8162\n",
      "Epoch 155/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 242.3312 - val_loss: 1816.9600\n",
      "Epoch 156/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 241.7766 - val_loss: 1821.9742\n",
      "Epoch 157/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 241.2589 - val_loss: 1826.8590\n",
      "Epoch 158/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 240.7760 - val_loss: 1831.6166\n",
      "Epoch 159/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 240.3259 - val_loss: 1836.2463\n",
      "Epoch 160/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 239.9066 - val_loss: 1840.7496\n",
      "Epoch 161/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 239.5164 - val_loss: 1845.1293\n",
      "Epoch 162/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 239.1535 - val_loss: 1849.3838\n",
      "Epoch 163/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 238.8163 - val_loss: 1853.5166\n",
      "Epoch 164/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 238.5033 - val_loss: 1857.5262\n",
      "Epoch 165/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 238.2127 - val_loss: 1861.4191\n",
      "Epoch 166/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 237.9434 - val_loss: 1865.1926\n",
      "Epoch 167/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 237.6938 - val_loss: 1868.8499\n",
      "Epoch 168/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 237.4630 - val_loss: 1872.3917\n",
      "Epoch 169/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 237.2492 - val_loss: 1875.8219\n",
      "Epoch 170/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 237.0519 - val_loss: 1879.1410\n",
      "Epoch 171/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 236.8696 - val_loss: 1882.3500\n",
      "Epoch 172/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 236.7013 - val_loss: 1885.4535\n",
      "Epoch 173/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 236.5462 - val_loss: 1888.4509\n",
      "Epoch 174/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 236.4033 - val_loss: 1891.3448\n",
      "Epoch 175/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 236.2719 - val_loss: 1894.1389\n",
      "Epoch 176/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 236.1508 - val_loss: 1896.8339\n",
      "Epoch 177/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 236.0398 - val_loss: 1899.4321\n",
      "Epoch 178/500\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 235.9377 - val_loss: 1901.9368\n",
      "Epoch 179/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 235.8440 - val_loss: 1904.3477\n",
      "Epoch 180/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 235.7582 - val_loss: 1906.6683\n",
      "Epoch 181/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 235.6796 - val_loss: 1908.9016\n",
      "Epoch 182/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 235.6077 - val_loss: 1911.0494\n",
      "Epoch 183/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 235.5419 - val_loss: 1913.1138\n",
      "Epoch 184/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 235.4818 - val_loss: 1915.0974\n",
      "Epoch 185/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 235.4270 - val_loss: 1917.0017\n",
      "Epoch 186/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 235.3771 - val_loss: 1918.8291\n",
      "Epoch 187/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 235.3315 - val_loss: 1920.5825\n",
      "Epoch 188/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 235.2901 - val_loss: 1922.2628\n",
      "Epoch 189/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 235.2524 - val_loss: 1923.8733\n",
      "Epoch 190/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 235.2182 - val_loss: 1925.4150\n",
      "Epoch 191/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 235.1871 - val_loss: 1926.8916\n",
      "Epoch 192/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.1589 - val_loss: 1928.3052\n",
      "Epoch 193/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.1334 - val_loss: 1929.6545\n",
      "Epoch 194/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.1103 - val_loss: 1930.9452\n",
      "Epoch 195/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0895 - val_loss: 1932.1791\n",
      "Epoch 196/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0707 - val_loss: 1933.3569\n",
      "Epoch 197/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0537 - val_loss: 1934.4803\n",
      "Epoch 198/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0384 - val_loss: 1935.5522\n",
      "Epoch 199/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 235.0247 - val_loss: 1936.5735\n",
      "Epoch 200/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 235.0124 - val_loss: 1937.5483\n",
      "Epoch 201/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 235.0014 - val_loss: 1938.4757\n",
      "Epoch 202/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 234.9915 - val_loss: 1939.3597\n",
      "Epoch 203/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 234.9827 - val_loss: 1940.1998\n",
      "Epoch 204/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 234.9748 - val_loss: 1940.9985\n",
      "Epoch 205/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 234.9678 - val_loss: 1941.7583\n",
      "Epoch 206/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 234.9617 - val_loss: 1942.4800\n",
      "Epoch 207/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9562 - val_loss: 1943.1656\n",
      "Epoch 208/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9513 - val_loss: 1943.8164\n",
      "Epoch 209/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9470 - val_loss: 1944.4336\n",
      "Epoch 210/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9433 - val_loss: 1945.0189\n",
      "Epoch 211/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9401 - val_loss: 1945.5737\n",
      "Epoch 212/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9373 - val_loss: 1946.0989\n",
      "Epoch 213/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9349 - val_loss: 1946.5956\n",
      "Epoch 214/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9328 - val_loss: 1947.0669\n",
      "Epoch 215/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9311 - val_loss: 1947.5132\n",
      "Epoch 216/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9296 - val_loss: 1947.9342\n",
      "Epoch 217/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9284 - val_loss: 1948.3324\n",
      "Epoch 218/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9275 - val_loss: 1948.7086\n",
      "Epoch 219/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9267 - val_loss: 1949.0640\n",
      "Epoch 220/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9261 - val_loss: 1949.3994\n",
      "Epoch 221/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9257 - val_loss: 1949.7148\n",
      "Epoch 222/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9254 - val_loss: 1950.0125\n",
      "Epoch 223/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9253 - val_loss: 1950.2933\n",
      "Epoch 224/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9253 - val_loss: 1950.5582\n",
      "Epoch 225/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9254 - val_loss: 1950.8074\n",
      "Epoch 226/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9256 - val_loss: 1951.0408\n",
      "Epoch 227/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9259 - val_loss: 1951.2614\n",
      "Epoch 228/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9263 - val_loss: 1951.4688\n",
      "Epoch 229/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 234.9268 - val_loss: 1951.6643\n",
      "Epoch 230/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9273 - val_loss: 1951.8470\n",
      "Epoch 231/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9279 - val_loss: 1952.0188\n",
      "Epoch 232/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9285 - val_loss: 1952.1803\n",
      "Epoch 233/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9291 - val_loss: 1952.3313\n",
      "Epoch 234/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9298 - val_loss: 1952.4728\n",
      "Epoch 235/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 234.9306 - val_loss: 1952.6058\n",
      "Epoch 236/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 234.9313 - val_loss: 1952.7294\n",
      "Epoch 237/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 234.9321 - val_loss: 1952.8459\n",
      "Epoch 238/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9329 - val_loss: 1952.9553\n",
      "Epoch 239/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9338 - val_loss: 1953.0559\n",
      "Epoch 240/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9346 - val_loss: 1953.1522\n",
      "Epoch 241/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9355 - val_loss: 1953.2401\n",
      "Epoch 242/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9364 - val_loss: 1953.3230\n",
      "Epoch 243/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9373 - val_loss: 1953.4005\n",
      "Epoch 244/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9382 - val_loss: 1953.4735\n",
      "Epoch 245/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9390 - val_loss: 1953.5404\n",
      "Epoch 246/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9400 - val_loss: 1953.6033\n",
      "Epoch 247/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9410 - val_loss: 1953.6614\n",
      "Epoch 248/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9419 - val_loss: 1953.7166\n",
      "Epoch 249/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9427 - val_loss: 1953.7677\n",
      "Epoch 250/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9437 - val_loss: 1953.8147\n",
      "Epoch 251/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9445 - val_loss: 1953.8584\n",
      "Epoch 252/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9455 - val_loss: 1953.8983\n",
      "Epoch 253/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9465 - val_loss: 1953.9358\n",
      "Epoch 254/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9473 - val_loss: 1953.9709\n",
      "Epoch 255/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9483 - val_loss: 1954.0040\n",
      "Epoch 256/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 234.9491 - val_loss: 1954.0334\n",
      "Epoch 257/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9500 - val_loss: 1954.0614\n",
      "Epoch 258/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9509 - val_loss: 1954.0872\n",
      "Epoch 259/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9518 - val_loss: 1954.1104\n",
      "Epoch 260/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9528 - val_loss: 1954.1323\n",
      "Epoch 261/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9537 - val_loss: 1954.1526\n",
      "Epoch 262/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9545 - val_loss: 1954.1715\n",
      "Epoch 263/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 234.9554 - val_loss: 1954.1888\n",
      "Epoch 264/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9563 - val_loss: 1954.2046\n",
      "Epoch 265/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9571 - val_loss: 1954.2185\n",
      "Epoch 266/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9579 - val_loss: 1954.2329\n",
      "Epoch 267/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9587 - val_loss: 1954.2449\n",
      "Epoch 268/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9596 - val_loss: 1954.2555\n",
      "Epoch 269/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9603 - val_loss: 1954.2664\n",
      "Epoch 270/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9612 - val_loss: 1954.2750\n",
      "Epoch 271/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9619 - val_loss: 1954.2831\n",
      "Epoch 272/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9628 - val_loss: 1954.2910\n",
      "Epoch 273/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9636 - val_loss: 1954.2987\n",
      "Epoch 274/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9644 - val_loss: 1954.3048\n",
      "Epoch 275/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9651 - val_loss: 1954.3104\n",
      "Epoch 276/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 234.9658 - val_loss: 1954.3165\n",
      "Epoch 277/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 234.9666 - val_loss: 1954.3201\n",
      "Epoch 278/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 234.9674 - val_loss: 1954.3246\n",
      "Epoch 279/500\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 234.9681 - val_loss: 1954.3274\n",
      "Epoch 280/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 234.9688 - val_loss: 1954.3314\n",
      "Epoch 281/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 234.9695 - val_loss: 1954.3344\n",
      "Epoch 282/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 234.9702 - val_loss: 1954.3373\n",
      "Epoch 283/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9709 - val_loss: 1954.3397\n",
      "Epoch 284/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 234.9715 - val_loss: 1954.3417\n",
      "Epoch 285/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9722 - val_loss: 1954.3434\n",
      "Epoch 286/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 234.9728 - val_loss: 1954.3452\n",
      "Epoch 287/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 234.9734 - val_loss: 1954.3466\n",
      "Epoch 288/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 234.9741 - val_loss: 1954.3470\n",
      "Epoch 289/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9747 - val_loss: 1954.3474\n",
      "Epoch 290/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9754 - val_loss: 1954.3492\n",
      "Epoch 291/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9760 - val_loss: 1954.3499\n",
      "Epoch 292/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 234.9766 - val_loss: 1954.3499\n",
      "Epoch 293/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 234.9772 - val_loss: 1954.3502\n",
      "Epoch 294/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9777 - val_loss: 1954.3500\n",
      "Epoch 295/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9783 - val_loss: 1954.3503\n",
      "Epoch 296/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 234.9790 - val_loss: 1954.3503\n",
      "Epoch 297/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 234.9794 - val_loss: 1954.3496\n",
      "Epoch 298/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9800 - val_loss: 1954.3492\n",
      "Epoch 299/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 234.9804 - val_loss: 1954.3489\n",
      "Epoch 300/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9810 - val_loss: 1954.3485\n",
      "Epoch 301/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9816 - val_loss: 1954.3485\n",
      "Epoch 302/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9821 - val_loss: 1954.3481\n",
      "Epoch 303/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9826 - val_loss: 1954.3477\n",
      "Epoch 304/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9830 - val_loss: 1954.3459\n",
      "Epoch 305/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9836 - val_loss: 1954.3455\n",
      "Epoch 306/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9840 - val_loss: 1954.3445\n",
      "Epoch 307/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9845 - val_loss: 1954.3434\n",
      "Epoch 308/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9850 - val_loss: 1954.3423\n",
      "Epoch 309/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9854 - val_loss: 1954.3423\n",
      "Epoch 310/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9858 - val_loss: 1954.3412\n",
      "Epoch 311/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 234.9863 - val_loss: 1954.3406\n",
      "Epoch 312/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9867 - val_loss: 1954.3397\n",
      "Epoch 313/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9872 - val_loss: 1954.3391\n",
      "Epoch 314/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9876 - val_loss: 1954.3380\n",
      "Epoch 315/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9880 - val_loss: 1954.3372\n",
      "Epoch 316/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9884 - val_loss: 1954.3364\n",
      "Epoch 317/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9888 - val_loss: 1954.3357\n",
      "Epoch 318/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9892 - val_loss: 1954.3350\n",
      "Epoch 319/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9895 - val_loss: 1954.3336\n",
      "Epoch 320/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9898 - val_loss: 1954.3329\n",
      "Epoch 321/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9902 - val_loss: 1954.3318\n",
      "Epoch 322/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9905 - val_loss: 1954.3309\n",
      "Epoch 323/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9910 - val_loss: 1954.3298\n",
      "Epoch 324/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9913 - val_loss: 1954.3287\n",
      "Epoch 325/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9917 - val_loss: 1954.3278\n",
      "Epoch 326/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9920 - val_loss: 1954.3284\n",
      "Epoch 327/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9923 - val_loss: 1954.3274\n",
      "Epoch 328/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9926 - val_loss: 1954.3260\n",
      "Epoch 329/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9929 - val_loss: 1954.3250\n",
      "Epoch 330/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9932 - val_loss: 1954.3239\n",
      "Epoch 331/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9935 - val_loss: 1954.3228\n",
      "Epoch 332/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9939 - val_loss: 1954.3219\n",
      "Epoch 333/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9941 - val_loss: 1954.3215\n",
      "Epoch 334/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9944 - val_loss: 1954.3208\n",
      "Epoch 335/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 234.9947 - val_loss: 1954.3198\n",
      "Epoch 336/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 234.9950 - val_loss: 1954.3191\n",
      "Epoch 337/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9952 - val_loss: 1954.3184\n",
      "Epoch 338/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 234.9955 - val_loss: 1954.3173\n",
      "Epoch 339/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.9957 - val_loss: 1954.3162\n",
      "Epoch 340/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9960 - val_loss: 1954.3151\n",
      "Epoch 341/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9962 - val_loss: 1954.3143\n",
      "Epoch 342/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9964 - val_loss: 1954.3132\n",
      "Epoch 343/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9967 - val_loss: 1954.3125\n",
      "Epoch 344/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9970 - val_loss: 1954.3121\n",
      "Epoch 345/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9972 - val_loss: 1954.3118\n",
      "Epoch 346/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9974 - val_loss: 1954.3112\n",
      "Epoch 347/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9977 - val_loss: 1954.3110\n",
      "Epoch 348/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9979 - val_loss: 1954.3101\n",
      "Epoch 349/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9980 - val_loss: 1954.3092\n",
      "Epoch 350/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9982 - val_loss: 1954.3088\n",
      "Epoch 351/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9984 - val_loss: 1954.3077\n",
      "Epoch 352/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9986 - val_loss: 1954.3063\n",
      "Epoch 353/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9988 - val_loss: 1954.3052\n",
      "Epoch 354/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9990 - val_loss: 1954.3041\n",
      "Epoch 355/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9992 - val_loss: 1954.3033\n",
      "Epoch 356/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9994 - val_loss: 1954.3019\n",
      "Epoch 357/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9996 - val_loss: 1954.3015\n",
      "Epoch 358/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 234.9997 - val_loss: 1954.3004\n",
      "Epoch 359/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 234.9998 - val_loss: 1954.2991\n",
      "Epoch 360/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0000 - val_loss: 1954.2983\n",
      "Epoch 361/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0002 - val_loss: 1954.2979\n",
      "Epoch 362/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0004 - val_loss: 1954.2976\n",
      "Epoch 363/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0006 - val_loss: 1954.2966\n",
      "Epoch 364/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0008 - val_loss: 1954.2961\n",
      "Epoch 365/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0009 - val_loss: 1954.2950\n",
      "Epoch 366/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0011 - val_loss: 1954.2939\n",
      "Epoch 367/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0011 - val_loss: 1954.2930\n",
      "Epoch 368/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0013 - val_loss: 1954.2922\n",
      "Epoch 369/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0014 - val_loss: 1954.2908\n",
      "Epoch 370/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0015 - val_loss: 1954.2885\n",
      "Epoch 371/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0018 - val_loss: 1954.2882\n",
      "Epoch 372/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0019 - val_loss: 1954.2878\n",
      "Epoch 373/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0020 - val_loss: 1954.2866\n",
      "Epoch 374/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0021 - val_loss: 1954.2845\n",
      "Epoch 375/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0023 - val_loss: 1954.2833\n",
      "Epoch 376/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0024 - val_loss: 1954.2820\n",
      "Epoch 377/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0025 - val_loss: 1954.2805\n",
      "Epoch 378/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0026 - val_loss: 1954.2788\n",
      "Epoch 379/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0026 - val_loss: 1954.2766\n",
      "Epoch 380/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0027 - val_loss: 1954.2737\n",
      "Epoch 381/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0029 - val_loss: 1954.2722\n",
      "Epoch 382/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 235.0030 - val_loss: 1954.2695\n",
      "Epoch 383/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 235.0031 - val_loss: 1954.2664\n",
      "Epoch 384/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 235.0032 - val_loss: 1954.2632\n",
      "Epoch 385/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 235.0034 - val_loss: 1954.2594\n",
      "Epoch 386/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 235.0034 - val_loss: 1954.2540\n",
      "Epoch 387/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0035 - val_loss: 1954.2461\n",
      "Epoch 388/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0036 - val_loss: 1954.2327\n",
      "Epoch 389/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0037 - val_loss: 1954.1996\n",
      "Epoch 390/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9864 - val_loss: 1949.7366\n",
      "Epoch 391/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.1612 - val_loss: 1950.9890\n",
      "Epoch 392/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0765 - val_loss: 1951.4351\n",
      "Epoch 393/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0564 - val_loss: 1951.7379\n",
      "Epoch 394/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 235.0480 - val_loss: 1951.9932\n",
      "Epoch 395/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 235.0426 - val_loss: 1952.2163\n",
      "Epoch 396/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 235.0387 - val_loss: 1952.4156\n",
      "Epoch 397/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0358 - val_loss: 1952.5931\n",
      "Epoch 398/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0333 - val_loss: 1952.7537\n",
      "Epoch 399/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0312 - val_loss: 1952.8967\n",
      "Epoch 400/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 235.0296 - val_loss: 1953.0270\n",
      "Epoch 401/500\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 235.0282 - val_loss: 1953.1450\n",
      "Epoch 402/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0269 - val_loss: 1953.2500\n",
      "Epoch 403/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 235.0258 - val_loss: 1953.3459\n",
      "Epoch 404/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 235.0249 - val_loss: 1953.4342\n",
      "Epoch 405/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0241 - val_loss: 1953.5128\n",
      "Epoch 406/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0233 - val_loss: 1953.5850\n",
      "Epoch 407/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0226 - val_loss: 1953.6490\n",
      "Epoch 408/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0221 - val_loss: 1953.7100\n",
      "Epoch 409/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0215 - val_loss: 1953.7628\n",
      "Epoch 410/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0210 - val_loss: 1953.8118\n",
      "Epoch 411/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0206 - val_loss: 1953.8568\n",
      "Epoch 412/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0202 - val_loss: 1953.8976\n",
      "Epoch 413/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0198 - val_loss: 1953.9342\n",
      "Epoch 414/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0194 - val_loss: 1953.9673\n",
      "Epoch 415/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0192 - val_loss: 1953.9971\n",
      "Epoch 416/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0190 - val_loss: 1954.0250\n",
      "Epoch 417/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0188 - val_loss: 1954.0511\n",
      "Epoch 418/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0185 - val_loss: 1954.0741\n",
      "Epoch 419/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0183 - val_loss: 1954.0948\n",
      "Epoch 420/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0180 - val_loss: 1954.1134\n",
      "Epoch 421/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0179 - val_loss: 1954.1318\n",
      "Epoch 422/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0178 - val_loss: 1954.1471\n",
      "Epoch 423/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0175 - val_loss: 1954.1617\n",
      "Epoch 424/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 235.0174 - val_loss: 1954.1730\n",
      "Epoch 425/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0173 - val_loss: 1954.1842\n",
      "Epoch 426/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0172 - val_loss: 1954.1948\n",
      "Epoch 427/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0171 - val_loss: 1954.2045\n",
      "Epoch 428/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0170 - val_loss: 1954.2126\n",
      "Epoch 429/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0169 - val_loss: 1954.2216\n",
      "Epoch 430/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0168 - val_loss: 1954.2285\n",
      "Epoch 431/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0168 - val_loss: 1954.2341\n",
      "Epoch 432/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0168 - val_loss: 1954.2413\n",
      "Epoch 433/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 235.0167 - val_loss: 1954.2471\n",
      "Epoch 434/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0167 - val_loss: 1954.2524\n",
      "Epoch 435/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 235.0166 - val_loss: 1954.2568\n",
      "Epoch 436/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0165 - val_loss: 1954.2621\n",
      "Epoch 437/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0165 - val_loss: 1954.2671\n",
      "Epoch 438/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0164 - val_loss: 1954.2717\n",
      "Epoch 439/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0163 - val_loss: 1954.2737\n",
      "Epoch 440/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0163 - val_loss: 1954.2762\n",
      "Epoch 441/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0163 - val_loss: 1954.2791\n",
      "Epoch 442/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 235.0163 - val_loss: 1954.2831\n",
      "Epoch 443/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0162 - val_loss: 1954.2845\n",
      "Epoch 444/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0162 - val_loss: 1954.2859\n",
      "Epoch 445/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0162 - val_loss: 1954.2877\n",
      "Epoch 446/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 235.0162 - val_loss: 1954.2892\n",
      "Epoch 447/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0162 - val_loss: 1954.2914\n",
      "Epoch 448/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0161 - val_loss: 1954.2932\n",
      "Epoch 449/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0160 - val_loss: 1954.2939\n",
      "Epoch 450/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0160 - val_loss: 1954.2941\n",
      "Epoch 451/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0161 - val_loss: 1954.2950\n",
      "Epoch 452/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0161 - val_loss: 1954.2966\n",
      "Epoch 453/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0161 - val_loss: 1954.2983\n",
      "Epoch 454/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0160 - val_loss: 1954.2991\n",
      "Epoch 455/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0160 - val_loss: 1954.3000\n",
      "Epoch 456/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0160 - val_loss: 1954.3016\n",
      "Epoch 457/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0159 - val_loss: 1954.3024\n",
      "Epoch 458/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0160 - val_loss: 1954.3033\n",
      "Epoch 459/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0160 - val_loss: 1954.3037\n",
      "Epoch 460/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0159 - val_loss: 1954.3041\n",
      "Epoch 461/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0160 - val_loss: 1954.3042\n",
      "Epoch 462/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0159 - val_loss: 1954.3042\n",
      "Epoch 463/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0159 - val_loss: 1954.3046\n",
      "Epoch 464/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0159 - val_loss: 1954.3052\n",
      "Epoch 465/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0159 - val_loss: 1954.3053\n",
      "Epoch 466/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0159 - val_loss: 1954.3053\n",
      "Epoch 467/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0159 - val_loss: 1954.3049\n",
      "Epoch 468/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 235.0159 - val_loss: 1954.3052\n",
      "Epoch 469/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0159 - val_loss: 1954.3053\n",
      "Epoch 470/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0159 - val_loss: 1954.3053\n",
      "Epoch 471/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0159 - val_loss: 1954.3059\n",
      "Epoch 472/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0159 - val_loss: 1954.3059\n",
      "Epoch 473/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0159 - val_loss: 1954.3059\n",
      "Epoch 474/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0159 - val_loss: 1954.3068\n",
      "Epoch 475/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0159 - val_loss: 1954.3070\n",
      "Epoch 476/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0158 - val_loss: 1954.3074\n",
      "Epoch 477/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0158 - val_loss: 1954.3081\n",
      "Epoch 478/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0158 - val_loss: 1954.3085\n",
      "Epoch 479/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0158 - val_loss: 1954.3085\n",
      "Epoch 480/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0158 - val_loss: 1954.3085\n",
      "Epoch 481/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0158 - val_loss: 1954.3085\n",
      "Epoch 482/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0158 - val_loss: 1954.3074\n",
      "Epoch 483/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0158 - val_loss: 1954.3070\n",
      "Epoch 484/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0158 - val_loss: 1954.3066\n",
      "Epoch 485/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0158 - val_loss: 1954.3063\n",
      "Epoch 486/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0158 - val_loss: 1954.3066\n",
      "Epoch 487/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0159 - val_loss: 1954.3074\n",
      "Epoch 488/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 235.0159 - val_loss: 1954.3085\n",
      "Epoch 489/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0158 - val_loss: 1954.3090\n",
      "Epoch 490/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0158 - val_loss: 1954.3090\n",
      "Epoch 491/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0158 - val_loss: 1954.3099\n",
      "Epoch 492/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0158 - val_loss: 1954.3107\n",
      "Epoch 493/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0158 - val_loss: 1954.3110\n",
      "Epoch 494/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0158 - val_loss: 1954.3107\n",
      "Epoch 495/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0158 - val_loss: 1954.3101\n",
      "Epoch 496/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0158 - val_loss: 1954.3092\n",
      "Epoch 497/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0158 - val_loss: 1954.3086\n",
      "Epoch 498/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0158 - val_loss: 1954.3085\n",
      "Epoch 499/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.0158 - val_loss: 1954.3081\n",
      "Epoch 500/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 235.0158 - val_loss: 1954.3074\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.1552, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0026, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0004, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0102, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.8734, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 420ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.15192157e+01, 7.11963212e+01, 7.11156489e+01, 7.10349767e+01,\n",
       "        7.06933365e+01, 2.19670150e-01, 8.66116460e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.66660650e-01, 4.91330890e-01, 0.00000000e+00,\n",
       "        6.63506390e-01, 7.22878431e+01, 7.18643137e+01, 7.14407843e+01,\n",
       "        3.64094080e-02, 0.00000000e+00, 7.32152895e+01, 7.28839216e+01,\n",
       "        7.24603922e+01, 7.20368627e+01, 7.16133333e+01, 7.12142484e+01,\n",
       "        7.11335761e+01, 7.10529038e+01, 7.09722316e+01, 0.00000000e+00,\n",
       "        8.18514680e-02, 7.17858823e+01, 7.13623529e+01, 7.11664426e+01,\n",
       "        7.10857703e+01, 7.10050980e+01, 7.09244258e+01, 7.08472689e+01,\n",
       "        7.07716387e+01, 7.06960084e+01, 1.45036131e-01, 5.25366127e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.91179383e-01,\n",
       "        0.00000000e+00, 5.29398620e-01, 3.80662382e-01, 7.20054902e+01,\n",
       "        7.15819608e+01, 0.00000000e+00, 0.00000000e+00, 7.33245331e+01,\n",
       "        7.29968020e+01, 7.26015686e+01, 7.21780392e+01, 7.17545098e+01,\n",
       "        7.13309804e+01, 7.11604669e+01, 7.10797946e+01, 7.09991223e+01,\n",
       "        1.08551530e-02, 1.52217552e-01, 7.19270588e+01, 7.15035294e+01,\n",
       "        7.11933333e+01, 7.11126611e+01, 7.10319888e+01, 7.09513165e+01,\n",
       "        7.08724790e+01, 7.07968487e+01, 7.07212185e+01, 2.27639120e-02,\n",
       "        0.00000000e+00, 4.54919891e+01, 1.04988220e-02, 3.80814075e-01,\n",
       "        1.42019778e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.00496521e+01, 0.00000000e+00, 6.35787368e-01, 3.04394513e-01,\n",
       "        5.45300841e-01, 0.00000000e+00, 2.90130377e-01, 8.31156522e-02,\n",
       "        8.85007083e-01, 0.00000000e+00, 1.89214364e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.85397953e-01, 5.12452841e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([57.53635991, 57.52078295, 57.50520598, 57.48962902, 57.47405206,\n",
       "       57.4584751 , 57.44289813, 57.42732117, 57.41174421, 57.39616725,\n",
       "       57.38059028, 57.36501332, 57.34943636, 57.3338594 , 57.31828243,\n",
       "       57.30270547, 57.28712851, 57.27155155, 57.25597458, 57.24039762,\n",
       "       57.22482066, 57.2092437 , 57.19366673, 57.17808977, 57.16251281,\n",
       "       57.14693585, 57.13135889, 57.11578192, 57.10020496, 57.084628  ,\n",
       "       57.06905104, 57.05347407, 57.03789711, 57.02232015, 57.00674319,\n",
       "       56.99116622, 56.97558926, 56.9600123 , 56.94443534, 56.92885837,\n",
       "       56.91328141, 56.89770445, 56.88212749, 56.86655052, 56.85097356,\n",
       "       56.8353966 , 56.81981964, 56.80424267, 56.78866571, 56.77308875,\n",
       "       56.75751179, 56.74193482, 56.72635786, 56.7107809 , 56.69520394,\n",
       "       56.67962697, 56.66405001, 56.64847305, 56.63289609, 56.61731912,\n",
       "       56.60174216, 56.5861652 , 56.57058824, 56.55501127, 56.53943431,\n",
       "       56.52385735, 56.50828039, 56.49270342, 56.47712646, 56.4615495 ,\n",
       "       56.44597254, 56.43039557, 56.41481861, 56.39924165, 56.38366469,\n",
       "       56.36808772, 56.35251076, 56.3369338 , 56.32135684, 56.30577987,\n",
       "       56.29020291, 56.27462595, 56.25904899, 56.24347202, 56.22789506,\n",
       "       56.2123181 , 56.19674114, 56.18116417, 56.16558721, 56.15001025,\n",
       "       56.13443329, 56.11885632, 56.10327936, 56.0877024 , 56.07212544,\n",
       "       56.05654847, 56.04097151, 56.02539455, 56.00981759, 55.99424062])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.91975086872039\n",
      "36.284462471463584\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
