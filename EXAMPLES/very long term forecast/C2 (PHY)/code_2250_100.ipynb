{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "2345    54.498852\n",
       "2346    54.483275\n",
       "2347    54.467698\n",
       "2348    54.452121\n",
       "2349    54.436544\n",
       "Name: C2, Length: 2350, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c2_interpolated_2250_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90.500000\n",
       "1       90.275910\n",
       "2       90.051821\n",
       "3       89.827731\n",
       "4       89.603641\n",
       "          ...    \n",
       "2245     0.000000\n",
       "2246     0.000000\n",
       "2247     0.000000\n",
       "2248     0.000000\n",
       "2249     0.000000\n",
       "Name: C2, Length: 2250, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2250)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.500000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.275910</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.051821</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.827731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.603641</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     90.500000  0.000298\n",
       "1     90.275910  0.000298\n",
       "2     90.051821  0.000297\n",
       "3     89.827731  0.000297\n",
       "4     89.603641  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 88.1552  # Value for C0\n",
    "K0 = -0.0026  # Value for K0\n",
    "K1 = -0.0004  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0102    # Value for b\n",
    "c = 2.8734    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    90.500000    90.275910    90.051821    89.827731    89.603641   \n",
      "351    90.275910    90.051821    89.827731    89.603641    89.379552   \n",
      "352    90.051821    89.827731    89.603641    89.379552    89.155462   \n",
      "353    89.827731    89.603641    89.379552    89.155462    88.931373   \n",
      "354    89.603641    89.379552    89.155462    88.931373    88.707283   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    89.379552    89.155462    88.931373    88.707283    88.494958  ...   \n",
      "351    89.155462    88.931373    88.707283    88.494958    88.427731  ...   \n",
      "352    88.931373    88.707283    88.494958    88.427731    88.360504  ...   \n",
      "353    88.707283    88.494958    88.427731    88.360504    88.293277  ...   \n",
      "354    88.494958    88.427731    88.360504    88.293277    88.226050  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   79.071008    0.000263   79.054202    0.000263   79.037395    0.000263   \n",
      "351   79.054202    0.000263   79.037395    0.000263   79.020588    0.000262   \n",
      "352   79.037395    0.000263   79.020588    0.000262   79.003782    0.000262   \n",
      "353   79.020588    0.000262   79.003782    0.000262   78.986975    0.000262   \n",
      "354   79.003782    0.000262   78.986975    0.000262   78.970168    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   79.020588    0.000262   79.003782    0.000262  \n",
      "351   79.003782    0.000262   78.986975    0.000262  \n",
      "352   78.986975    0.000262   78.970168    0.000262  \n",
      "353   78.970168    0.000262   78.953361    0.000262  \n",
      "354   78.953361    0.000262   78.936555    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 1, 251) (1800, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "23/23 [==============================] - 3s 27ms/step - loss: 5043.2725 - val_loss: 2754.8230\n",
      "Epoch 2/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4699.0620 - val_loss: 2587.6301\n",
      "Epoch 3/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4538.2881 - val_loss: 2497.0061\n",
      "Epoch 4/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4375.3672 - val_loss: 2421.6885\n",
      "Epoch 5/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4250.2363 - val_loss: 2355.6240\n",
      "Epoch 6/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4129.3892 - val_loss: 2293.4456\n",
      "Epoch 7/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4013.4507 - val_loss: 2234.3384\n",
      "Epoch 8/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3901.5344 - val_loss: 2177.8792\n",
      "Epoch 9/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3793.0559 - val_loss: 2123.8091\n",
      "Epoch 10/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3687.6572 - val_loss: 2071.9915\n",
      "Epoch 11/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3585.1160 - val_loss: 2022.2479\n",
      "Epoch 12/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3485.2646 - val_loss: 1974.5100\n",
      "Epoch 13/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3387.9858 - val_loss: 1928.7222\n",
      "Epoch 14/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3293.2004 - val_loss: 1884.8499\n",
      "Epoch 15/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 3200.7222 - val_loss: 1842.6309\n",
      "Epoch 16/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 3110.6152 - val_loss: 1802.2434\n",
      "Epoch 17/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 3022.7617 - val_loss: 1763.5155\n",
      "Epoch 18/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2937.1274 - val_loss: 1727.0410\n",
      "Epoch 19/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2853.5874 - val_loss: 1691.7299\n",
      "Epoch 20/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2772.1318 - val_loss: 1657.8547\n",
      "Epoch 21/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2692.7009 - val_loss: 1625.5735\n",
      "Epoch 22/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2615.1584 - val_loss: 1594.6957\n",
      "Epoch 23/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2539.7810 - val_loss: 1564.9240\n",
      "Epoch 24/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2466.2480 - val_loss: 1537.0933\n",
      "Epoch 25/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2394.6177 - val_loss: 1510.5684\n",
      "Epoch 26/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2324.8152 - val_loss: 1485.6797\n",
      "Epoch 27/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2256.8452 - val_loss: 1461.7426\n",
      "Epoch 28/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2190.6060 - val_loss: 1439.1584\n",
      "Epoch 29/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2126.0886 - val_loss: 1418.4156\n",
      "Epoch 30/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2064.1006 - val_loss: 1403.1954\n",
      "Epoch 31/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2003.1011 - val_loss: 1384.8704\n",
      "Epoch 32/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1943.7645 - val_loss: 1367.7417\n",
      "Epoch 33/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1886.0448 - val_loss: 1351.7788\n",
      "Epoch 34/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1829.9097 - val_loss: 1336.9528\n",
      "Epoch 35/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1775.3285 - val_loss: 1323.2345\n",
      "Epoch 36/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1722.2712 - val_loss: 1310.5955\n",
      "Epoch 37/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1670.7085 - val_loss: 1299.0073\n",
      "Epoch 38/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1620.6113 - val_loss: 1288.4424\n",
      "Epoch 39/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1571.9506 - val_loss: 1278.8728\n",
      "Epoch 40/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1524.6987 - val_loss: 1270.2714\n",
      "Epoch 41/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1478.8273 - val_loss: 1262.6111\n",
      "Epoch 42/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1434.3087 - val_loss: 1255.8650\n",
      "Epoch 43/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1391.1154 - val_loss: 1250.0068\n",
      "Epoch 44/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1349.2207 - val_loss: 1245.0101\n",
      "Epoch 45/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1308.5979 - val_loss: 1240.8489\n",
      "Epoch 46/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1269.2200 - val_loss: 1237.4977\n",
      "Epoch 47/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1231.0615 - val_loss: 1234.9305\n",
      "Epoch 48/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1194.0963 - val_loss: 1233.1224\n",
      "Epoch 49/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1158.2991 - val_loss: 1232.0483\n",
      "Epoch 50/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1123.6442 - val_loss: 1231.6831\n",
      "Epoch 51/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1090.1066 - val_loss: 1231.9985\n",
      "Epoch 52/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1057.6620 - val_loss: 1232.6504\n",
      "Epoch 53/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1026.2856 - val_loss: 1234.7705\n",
      "Epoch 54/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 995.9528 - val_loss: 1237.0068\n",
      "Epoch 55/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 966.6400 - val_loss: 1239.8334\n",
      "Epoch 56/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 938.3234 - val_loss: 1243.2268\n",
      "Epoch 57/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 910.9798 - val_loss: 1247.1646\n",
      "Epoch 58/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 884.5855 - val_loss: 1251.6235\n",
      "Epoch 59/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 859.1179 - val_loss: 1256.5820\n",
      "Epoch 60/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 834.5543 - val_loss: 1262.0176\n",
      "Epoch 61/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 810.8721 - val_loss: 1267.9084\n",
      "Epoch 62/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 788.0492 - val_loss: 1274.2333\n",
      "Epoch 63/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 766.0639 - val_loss: 1280.9709\n",
      "Epoch 64/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 744.8943 - val_loss: 1288.1005\n",
      "Epoch 65/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 724.5193 - val_loss: 1295.6012\n",
      "Epoch 66/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 704.9173 - val_loss: 1303.4528\n",
      "Epoch 67/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 686.0678 - val_loss: 1311.6356\n",
      "Epoch 68/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 667.9501 - val_loss: 1320.1294\n",
      "Epoch 69/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 650.5443 - val_loss: 1328.9152\n",
      "Epoch 70/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 633.8298 - val_loss: 1337.9738\n",
      "Epoch 71/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 617.7872 - val_loss: 1347.2866\n",
      "Epoch 72/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 602.3969 - val_loss: 1356.8350\n",
      "Epoch 73/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 587.6394 - val_loss: 1366.6014\n",
      "Epoch 74/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 573.4963 - val_loss: 1376.5682\n",
      "Epoch 75/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 559.9489 - val_loss: 1386.7168\n",
      "Epoch 76/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 546.9785 - val_loss: 1397.0320\n",
      "Epoch 77/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 534.5674 - val_loss: 1407.4960\n",
      "Epoch 78/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 522.6975 - val_loss: 1418.0925\n",
      "Epoch 79/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 511.3517 - val_loss: 1428.8064\n",
      "Epoch 80/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 500.5129 - val_loss: 1439.6216\n",
      "Epoch 81/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 490.1639 - val_loss: 1450.5233\n",
      "Epoch 82/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 480.2883 - val_loss: 1461.4960\n",
      "Epoch 83/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 470.8698 - val_loss: 1472.5267\n",
      "Epoch 84/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 461.8927 - val_loss: 1483.6002\n",
      "Epoch 85/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 453.3409 - val_loss: 1494.7035\n",
      "Epoch 86/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 445.1998 - val_loss: 1505.8241\n",
      "Epoch 87/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 437.4538 - val_loss: 1516.9475\n",
      "Epoch 88/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 430.0884 - val_loss: 1528.0625\n",
      "Epoch 89/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 423.0895 - val_loss: 1539.1578\n",
      "Epoch 90/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 416.4427 - val_loss: 1550.2212\n",
      "Epoch 91/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 410.1345 - val_loss: 1561.2411\n",
      "Epoch 92/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 404.1515 - val_loss: 1572.2078\n",
      "Epoch 93/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 398.4808 - val_loss: 1583.1113\n",
      "Epoch 94/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 393.1093 - val_loss: 1593.9403\n",
      "Epoch 95/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 388.0247 - val_loss: 1604.6875\n",
      "Epoch 96/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 383.2152 - val_loss: 1615.3418\n",
      "Epoch 97/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 378.6689 - val_loss: 1625.8960\n",
      "Epoch 98/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 374.3743 - val_loss: 1636.3418\n",
      "Epoch 99/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 370.3201 - val_loss: 1646.6715\n",
      "Epoch 100/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 366.4959 - val_loss: 1656.8782\n",
      "Epoch 101/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 362.8911 - val_loss: 1666.9539\n",
      "Epoch 102/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 359.4953 - val_loss: 1676.8944\n",
      "Epoch 103/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 356.2991 - val_loss: 1686.6919\n",
      "Epoch 104/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 353.2928 - val_loss: 1696.3406\n",
      "Epoch 105/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 350.4671 - val_loss: 1705.8368\n",
      "Epoch 106/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 347.8134 - val_loss: 1715.1748\n",
      "Epoch 107/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 345.3228 - val_loss: 1724.3494\n",
      "Epoch 108/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 342.9872 - val_loss: 1733.3583\n",
      "Epoch 109/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 340.7985 - val_loss: 1742.1965\n",
      "Epoch 110/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 338.7490 - val_loss: 1750.8656\n",
      "Epoch 111/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 336.8388 - val_loss: 1759.4012\n",
      "Epoch 112/500\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 335.0394 - val_loss: 1767.6892\n",
      "Epoch 113/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 333.3651 - val_loss: 1775.8182\n",
      "Epoch 114/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 331.8020 - val_loss: 1783.7673\n",
      "Epoch 115/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 330.3441 - val_loss: 1791.5341\n",
      "Epoch 116/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 328.9854 - val_loss: 1799.1152\n",
      "Epoch 117/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 327.7203 - val_loss: 1806.5115\n",
      "Epoch 118/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 326.5432 - val_loss: 1813.7222\n",
      "Epoch 119/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 325.4489 - val_loss: 1820.7462\n",
      "Epoch 120/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 324.4323 - val_loss: 1827.5848\n",
      "Epoch 121/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 323.4885 - val_loss: 1834.2375\n",
      "Epoch 122/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 322.6134 - val_loss: 1840.7052\n",
      "Epoch 123/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 321.8025 - val_loss: 1846.9889\n",
      "Epoch 124/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 321.0515 - val_loss: 1853.0896\n",
      "Epoch 125/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 320.3568 - val_loss: 1859.0085\n",
      "Epoch 126/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 319.7146 - val_loss: 1864.7474\n",
      "Epoch 127/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 319.1215 - val_loss: 1870.3080\n",
      "Epoch 128/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 318.5740 - val_loss: 1875.6931\n",
      "Epoch 129/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 318.0692 - val_loss: 1880.9031\n",
      "Epoch 130/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 317.6042 - val_loss: 1885.9419\n",
      "Epoch 131/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 317.1759 - val_loss: 1890.8116\n",
      "Epoch 132/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 316.7821 - val_loss: 1895.5145\n",
      "Epoch 133/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 316.4200 - val_loss: 1900.0543\n",
      "Epoch 134/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 316.0876 - val_loss: 1904.4321\n",
      "Epoch 135/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 315.7825 - val_loss: 1908.6528\n",
      "Epoch 136/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 315.5028 - val_loss: 1912.7183\n",
      "Epoch 137/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 315.2466 - val_loss: 1916.6327\n",
      "Epoch 138/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 315.0120 - val_loss: 1920.3979\n",
      "Epoch 139/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 314.7975 - val_loss: 1924.0188\n",
      "Epoch 140/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 314.6015 - val_loss: 1927.4979\n",
      "Epoch 141/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 314.4224 - val_loss: 1930.8394\n",
      "Epoch 142/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 314.2590 - val_loss: 1934.0458\n",
      "Epoch 143/500\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 314.1100 - val_loss: 1937.1210\n",
      "Epoch 144/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 313.9743 - val_loss: 1940.0695\n",
      "Epoch 145/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 313.8507 - val_loss: 1942.8932\n",
      "Epoch 146/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 313.7384 - val_loss: 1945.5978\n",
      "Epoch 147/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 313.6362 - val_loss: 1948.1854\n",
      "Epoch 148/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 313.5434 - val_loss: 1950.6591\n",
      "Epoch 149/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 313.4592 - val_loss: 1953.0223\n",
      "Epoch 150/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 313.3828 - val_loss: 1955.2809\n",
      "Epoch 151/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 313.3135 - val_loss: 1957.4368\n",
      "Epoch 152/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 313.2509 - val_loss: 1959.4941\n",
      "Epoch 153/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 313.1942 - val_loss: 1961.4573\n",
      "Epoch 154/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 313.1430 - val_loss: 1963.3282\n",
      "Epoch 155/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 313.0967 - val_loss: 1965.0885\n",
      "Epoch 156/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 313.0549 - val_loss: 1966.7267\n",
      "Epoch 157/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 313.0172 - val_loss: 1968.3328\n",
      "Epoch 158/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.9832 - val_loss: 1969.8602\n",
      "Epoch 159/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.9526 - val_loss: 1971.3101\n",
      "Epoch 160/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.9251 - val_loss: 1972.6879\n",
      "Epoch 161/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.9003 - val_loss: 1973.9941\n",
      "Epoch 162/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8782 - val_loss: 1975.2318\n",
      "Epoch 163/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8582 - val_loss: 1976.4052\n",
      "Epoch 164/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8403 - val_loss: 1977.5161\n",
      "Epoch 165/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8245 - val_loss: 1978.5684\n",
      "Epoch 166/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8102 - val_loss: 1979.5636\n",
      "Epoch 167/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 312.7974 - val_loss: 1980.5042\n",
      "Epoch 168/500\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 312.7861 - val_loss: 1981.3927\n",
      "Epoch 169/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7760 - val_loss: 1982.2302\n",
      "Epoch 170/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7670 - val_loss: 1983.0226\n",
      "Epoch 171/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7590 - val_loss: 1983.7688\n",
      "Epoch 172/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 312.7521 - val_loss: 1984.4725\n",
      "Epoch 173/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 312.7458 - val_loss: 1985.1353\n",
      "Epoch 174/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7404 - val_loss: 1985.7590\n",
      "Epoch 175/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7357 - val_loss: 1986.3453\n",
      "Epoch 176/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7316 - val_loss: 1986.8956\n",
      "Epoch 177/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.7280 - val_loss: 1987.4144\n",
      "Epoch 178/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.7249 - val_loss: 1987.9016\n",
      "Epoch 179/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7222 - val_loss: 1988.3574\n",
      "Epoch 180/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7199 - val_loss: 1988.7837\n",
      "Epoch 181/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7182 - val_loss: 1989.1841\n",
      "Epoch 182/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7166 - val_loss: 1989.5588\n",
      "Epoch 183/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 312.7154 - val_loss: 1989.9100\n",
      "Epoch 184/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7144 - val_loss: 1990.2357\n",
      "Epoch 185/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7138 - val_loss: 1990.5420\n",
      "Epoch 186/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7132 - val_loss: 1990.8260\n",
      "Epoch 187/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7129 - val_loss: 1991.0912\n",
      "Epoch 188/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7128 - val_loss: 1991.3372\n",
      "Epoch 189/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.7128 - val_loss: 1991.5659\n",
      "Epoch 190/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.7130 - val_loss: 1991.7781\n",
      "Epoch 191/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.7134 - val_loss: 1991.9745\n",
      "Epoch 192/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.7138 - val_loss: 1992.1554\n",
      "Epoch 193/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.7143 - val_loss: 1992.3228\n",
      "Epoch 194/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 312.7150 - val_loss: 1992.4762\n",
      "Epoch 195/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 312.7156 - val_loss: 1992.6177\n",
      "Epoch 196/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7165 - val_loss: 1992.7451\n",
      "Epoch 197/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7174 - val_loss: 1992.8627\n",
      "Epoch 198/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7183 - val_loss: 1992.9662\n",
      "Epoch 199/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7192 - val_loss: 1993.0597\n",
      "Epoch 200/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7203 - val_loss: 1993.1423\n",
      "Epoch 201/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.7214 - val_loss: 1993.2141\n",
      "Epoch 202/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 312.7225 - val_loss: 1993.2748\n",
      "Epoch 203/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.7236 - val_loss: 1993.3223\n",
      "Epoch 204/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.7248 - val_loss: 1993.3582\n",
      "Epoch 205/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.7260 - val_loss: 1993.3779\n",
      "Epoch 206/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.7273 - val_loss: 1993.3809\n",
      "Epoch 207/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.7285 - val_loss: 1993.3566\n",
      "Epoch 208/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.7297 - val_loss: 1993.2788\n",
      "Epoch 209/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.7310 - val_loss: 1992.8091\n",
      "Epoch 210/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 312.6309 - val_loss: 1938.1504\n",
      "Epoch 211/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 314.7891 - val_loss: 1929.8363\n",
      "Epoch 212/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 314.6925 - val_loss: 1934.7078\n",
      "Epoch 213/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 314.4554 - val_loss: 1939.4064\n",
      "Epoch 214/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 314.2356 - val_loss: 1943.7941\n",
      "Epoch 215/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 314.0440 - val_loss: 1947.8751\n",
      "Epoch 216/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 313.8779 - val_loss: 1951.6685\n",
      "Epoch 217/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 313.7340 - val_loss: 1955.1915\n",
      "Epoch 218/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 313.6093 - val_loss: 1958.4620\n",
      "Epoch 219/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 313.5013 - val_loss: 1961.4962\n",
      "Epoch 220/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 313.4077 - val_loss: 1964.3108\n",
      "Epoch 221/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 313.3264 - val_loss: 1966.9203\n",
      "Epoch 222/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 313.2560 - val_loss: 1969.3376\n",
      "Epoch 223/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 313.1948 - val_loss: 1971.5756\n",
      "Epoch 224/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 313.1418 - val_loss: 1973.6486\n",
      "Epoch 225/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 313.0956 - val_loss: 1975.5670\n",
      "Epoch 226/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 313.0555 - val_loss: 1977.3408\n",
      "Epoch 227/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 313.0205 - val_loss: 1978.9811\n",
      "Epoch 228/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.9902 - val_loss: 1980.4971\n",
      "Epoch 229/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.9637 - val_loss: 1981.8979\n",
      "Epoch 230/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.9406 - val_loss: 1983.1915\n",
      "Epoch 231/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.9204 - val_loss: 1984.3861\n",
      "Epoch 232/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.9028 - val_loss: 1985.4885\n",
      "Epoch 233/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8875 - val_loss: 1986.5052\n",
      "Epoch 234/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8743 - val_loss: 1987.4437\n",
      "Epoch 235/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8625 - val_loss: 1988.3083\n",
      "Epoch 236/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8523 - val_loss: 1989.1061\n",
      "Epoch 237/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8433 - val_loss: 1989.8406\n",
      "Epoch 238/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8354 - val_loss: 1990.5172\n",
      "Epoch 239/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8287 - val_loss: 1991.1406\n",
      "Epoch 240/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8228 - val_loss: 1991.7141\n",
      "Epoch 241/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8174 - val_loss: 1992.2423\n",
      "Epoch 242/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8129 - val_loss: 1992.7283\n",
      "Epoch 243/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8089 - val_loss: 1993.1750\n",
      "Epoch 244/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 312.8054 - val_loss: 1993.5859\n",
      "Epoch 245/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8024 - val_loss: 1993.9639\n",
      "Epoch 246/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7996 - val_loss: 1994.3113\n",
      "Epoch 247/500\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 312.7974 - val_loss: 1994.6302\n",
      "Epoch 248/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 312.7953 - val_loss: 1994.9232\n",
      "Epoch 249/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7936 - val_loss: 1995.1917\n",
      "Epoch 250/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7922 - val_loss: 1995.4397\n",
      "Epoch 251/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7910 - val_loss: 1995.6669\n",
      "Epoch 252/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7899 - val_loss: 1995.8743\n",
      "Epoch 253/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 312.7889 - val_loss: 1996.0654\n",
      "Epoch 254/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 312.7882 - val_loss: 1996.2402\n",
      "Epoch 255/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7876 - val_loss: 1996.4014\n",
      "Epoch 256/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7871 - val_loss: 1996.5488\n",
      "Epoch 257/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7867 - val_loss: 1996.6837\n",
      "Epoch 258/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7864 - val_loss: 1996.8075\n",
      "Epoch 259/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7862 - val_loss: 1996.9198\n",
      "Epoch 260/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7860 - val_loss: 1997.0239\n",
      "Epoch 261/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7860 - val_loss: 1997.1184\n",
      "Epoch 262/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7859 - val_loss: 1997.2059\n",
      "Epoch 263/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7860 - val_loss: 1997.2856\n",
      "Epoch 264/500\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 312.7861 - val_loss: 1997.3589\n",
      "Epoch 265/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7863 - val_loss: 1997.4252\n",
      "Epoch 266/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7864 - val_loss: 1997.4863\n",
      "Epoch 267/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7866 - val_loss: 1997.5419\n",
      "Epoch 268/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7868 - val_loss: 1997.5924\n",
      "Epoch 269/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7871 - val_loss: 1997.6390\n",
      "Epoch 270/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7874 - val_loss: 1997.6819\n",
      "Epoch 271/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7876 - val_loss: 1997.7208\n",
      "Epoch 272/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7880 - val_loss: 1997.7565\n",
      "Epoch 273/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7882 - val_loss: 1997.7886\n",
      "Epoch 274/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7886 - val_loss: 1997.8190\n",
      "Epoch 275/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.7889 - val_loss: 1997.8451\n",
      "Epoch 276/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.7893 - val_loss: 1997.8698\n",
      "Epoch 277/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7896 - val_loss: 1997.8923\n",
      "Epoch 278/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7900 - val_loss: 1997.9125\n",
      "Epoch 279/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.7904 - val_loss: 1997.9316\n",
      "Epoch 280/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7907 - val_loss: 1997.9497\n",
      "Epoch 281/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7910 - val_loss: 1997.9639\n",
      "Epoch 282/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7915 - val_loss: 1997.9783\n",
      "Epoch 283/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7918 - val_loss: 1997.9913\n",
      "Epoch 284/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 312.7922 - val_loss: 1998.0032\n",
      "Epoch 285/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7926 - val_loss: 1998.0135\n",
      "Epoch 286/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.7930 - val_loss: 1998.0236\n",
      "Epoch 287/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7934 - val_loss: 1998.0321\n",
      "Epoch 288/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7937 - val_loss: 1998.0398\n",
      "Epoch 289/500\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 312.7940 - val_loss: 1998.0468\n",
      "Epoch 290/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 312.7944 - val_loss: 1998.0533\n",
      "Epoch 291/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 312.7948 - val_loss: 1998.0599\n",
      "Epoch 292/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7951 - val_loss: 1998.0646\n",
      "Epoch 293/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 312.7955 - val_loss: 1998.0696\n",
      "Epoch 294/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7959 - val_loss: 1998.0747\n",
      "Epoch 295/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7962 - val_loss: 1998.0778\n",
      "Epoch 296/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7964 - val_loss: 1998.0826\n",
      "Epoch 297/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7968 - val_loss: 1998.0851\n",
      "Epoch 298/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7971 - val_loss: 1998.0876\n",
      "Epoch 299/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7975 - val_loss: 1998.0903\n",
      "Epoch 300/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7978 - val_loss: 1998.0927\n",
      "Epoch 301/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7981 - val_loss: 1998.0942\n",
      "Epoch 302/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7984 - val_loss: 1998.0958\n",
      "Epoch 303/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7987 - val_loss: 1998.0978\n",
      "Epoch 304/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7990 - val_loss: 1998.0994\n",
      "Epoch 305/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7993 - val_loss: 1998.1014\n",
      "Epoch 306/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.7995 - val_loss: 1998.1029\n",
      "Epoch 307/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.7998 - val_loss: 1998.1038\n",
      "Epoch 308/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8001 - val_loss: 1998.1044\n",
      "Epoch 309/500\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 312.8005 - val_loss: 1998.1052\n",
      "Epoch 310/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8007 - val_loss: 1998.1061\n",
      "Epoch 311/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8010 - val_loss: 1998.1073\n",
      "Epoch 312/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8011 - val_loss: 1998.1078\n",
      "Epoch 313/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8015 - val_loss: 1998.1083\n",
      "Epoch 314/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8017 - val_loss: 1998.1086\n",
      "Epoch 315/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8019 - val_loss: 1998.1090\n",
      "Epoch 316/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8022 - val_loss: 1998.1097\n",
      "Epoch 317/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8024 - val_loss: 1998.1101\n",
      "Epoch 318/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8026 - val_loss: 1998.1101\n",
      "Epoch 319/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8028 - val_loss: 1998.1097\n",
      "Epoch 320/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8031 - val_loss: 1998.1099\n",
      "Epoch 321/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8032 - val_loss: 1998.1095\n",
      "Epoch 322/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8034 - val_loss: 1998.1090\n",
      "Epoch 323/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8036 - val_loss: 1998.1090\n",
      "Epoch 324/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8038 - val_loss: 1998.1094\n",
      "Epoch 325/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8041 - val_loss: 1998.1089\n",
      "Epoch 326/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8043 - val_loss: 1998.1086\n",
      "Epoch 327/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8044 - val_loss: 1998.1086\n",
      "Epoch 328/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8046 - val_loss: 1998.1086\n",
      "Epoch 329/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8047 - val_loss: 1998.1086\n",
      "Epoch 330/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8050 - val_loss: 1998.1086\n",
      "Epoch 331/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8051 - val_loss: 1998.1085\n",
      "Epoch 332/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8053 - val_loss: 1998.1085\n",
      "Epoch 333/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8055 - val_loss: 1998.1085\n",
      "Epoch 334/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8056 - val_loss: 1998.1082\n",
      "Epoch 335/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 312.8058 - val_loss: 1998.1083\n",
      "Epoch 336/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8059 - val_loss: 1998.1080\n",
      "Epoch 337/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8061 - val_loss: 1998.1074\n",
      "Epoch 338/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8062 - val_loss: 1998.1073\n",
      "Epoch 339/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 312.8064 - val_loss: 1998.1072\n",
      "Epoch 340/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 312.8065 - val_loss: 1998.1066\n",
      "Epoch 341/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8067 - val_loss: 1998.1060\n",
      "Epoch 342/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8069 - val_loss: 1998.1061\n",
      "Epoch 343/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8069 - val_loss: 1998.1061\n",
      "Epoch 344/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8071 - val_loss: 1998.1061\n",
      "Epoch 345/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8071 - val_loss: 1998.1060\n",
      "Epoch 346/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8073 - val_loss: 1998.1056\n",
      "Epoch 347/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8074 - val_loss: 1998.1056\n",
      "Epoch 348/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8076 - val_loss: 1998.1056\n",
      "Epoch 349/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8077 - val_loss: 1998.1057\n",
      "Epoch 350/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8078 - val_loss: 1998.1057\n",
      "Epoch 351/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8079 - val_loss: 1998.1056\n",
      "Epoch 352/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8080 - val_loss: 1998.1057\n",
      "Epoch 353/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8080 - val_loss: 1998.1057\n",
      "Epoch 354/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8082 - val_loss: 1998.1057\n",
      "Epoch 355/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8083 - val_loss: 1998.1057\n",
      "Epoch 356/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8083 - val_loss: 1998.1056\n",
      "Epoch 357/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8085 - val_loss: 1998.1056\n",
      "Epoch 358/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8086 - val_loss: 1998.1056\n",
      "Epoch 359/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8087 - val_loss: 1998.1056\n",
      "Epoch 360/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8088 - val_loss: 1998.1056\n",
      "Epoch 361/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 312.8088 - val_loss: 1998.1056\n",
      "Epoch 362/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8089 - val_loss: 1998.1053\n",
      "Epoch 363/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8090 - val_loss: 1998.1053\n",
      "Epoch 364/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8091 - val_loss: 1998.1053\n",
      "Epoch 365/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8092 - val_loss: 1998.1053\n",
      "Epoch 366/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8092 - val_loss: 1998.1047\n",
      "Epoch 367/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8094 - val_loss: 1998.1052\n",
      "Epoch 368/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8094 - val_loss: 1998.1040\n",
      "Epoch 369/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8095 - val_loss: 1998.1036\n",
      "Epoch 370/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8097 - val_loss: 1998.1040\n",
      "Epoch 371/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8097 - val_loss: 1998.1050\n",
      "Epoch 372/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8098 - val_loss: 1998.1052\n",
      "Epoch 373/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8098 - val_loss: 1998.1049\n",
      "Epoch 374/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8099 - val_loss: 1998.1049\n",
      "Epoch 375/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 312.8100 - val_loss: 1998.1045\n",
      "Epoch 376/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8100 - val_loss: 1998.1045\n",
      "Epoch 377/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8100 - val_loss: 1998.1040\n",
      "Epoch 378/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8101 - val_loss: 1998.1036\n",
      "Epoch 379/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8103 - val_loss: 1998.1041\n",
      "Epoch 380/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8103 - val_loss: 1998.1045\n",
      "Epoch 381/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8103 - val_loss: 1998.1049\n",
      "Epoch 382/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8103 - val_loss: 1998.1049\n",
      "Epoch 383/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8104 - val_loss: 1998.1041\n",
      "Epoch 384/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 312.8105 - val_loss: 1998.1040\n",
      "Epoch 385/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8105 - val_loss: 1998.1038\n",
      "Epoch 386/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8106 - val_loss: 1998.1038\n",
      "Epoch 387/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8106 - val_loss: 1998.1038\n",
      "Epoch 388/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8106 - val_loss: 1998.1040\n",
      "Epoch 389/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8107 - val_loss: 1998.1038\n",
      "Epoch 390/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8108 - val_loss: 1998.1041\n",
      "Epoch 391/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8108 - val_loss: 1998.1041\n",
      "Epoch 392/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8108 - val_loss: 1998.1045\n",
      "Epoch 393/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8108 - val_loss: 1998.1045\n",
      "Epoch 394/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8109 - val_loss: 1998.1049\n",
      "Epoch 395/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8109 - val_loss: 1998.1047\n",
      "Epoch 396/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8109 - val_loss: 1998.1047\n",
      "Epoch 397/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8110 - val_loss: 1998.1040\n",
      "Epoch 398/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8110 - val_loss: 1998.1041\n",
      "Epoch 399/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 312.8110 - val_loss: 1998.1040\n",
      "Epoch 400/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8110 - val_loss: 1998.1038\n",
      "Epoch 401/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8111 - val_loss: 1998.1040\n",
      "Epoch 402/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8111 - val_loss: 1998.1040\n",
      "Epoch 403/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8112 - val_loss: 1998.1040\n",
      "Epoch 404/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8112 - val_loss: 1998.1041\n",
      "Epoch 405/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8112 - val_loss: 1998.1041\n",
      "Epoch 406/500\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 312.8112 - val_loss: 1998.1038\n",
      "Epoch 407/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8113 - val_loss: 1998.1038\n",
      "Epoch 408/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8114 - val_loss: 1998.1045\n",
      "Epoch 409/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8114 - val_loss: 1998.1049\n",
      "Epoch 410/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8114 - val_loss: 1998.1049\n",
      "Epoch 411/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8115 - val_loss: 1998.1052\n",
      "Epoch 412/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8115 - val_loss: 1998.1052\n",
      "Epoch 413/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8115 - val_loss: 1998.1049\n",
      "Epoch 414/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8115 - val_loss: 1998.1047\n",
      "Epoch 415/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8115 - val_loss: 1998.1038\n",
      "Epoch 416/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8116 - val_loss: 1998.1040\n",
      "Epoch 417/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8117 - val_loss: 1998.1040\n",
      "Epoch 418/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8117 - val_loss: 1998.1049\n",
      "Epoch 419/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8117 - val_loss: 1998.1056\n",
      "Epoch 420/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8117 - val_loss: 1998.1060\n",
      "Epoch 421/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8117 - val_loss: 1998.1056\n",
      "Epoch 422/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8117 - val_loss: 1998.1056\n",
      "Epoch 423/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8117 - val_loss: 1998.1047\n",
      "Epoch 424/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8118 - val_loss: 1998.1052\n",
      "Epoch 425/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8118 - val_loss: 1998.1047\n",
      "Epoch 426/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8118 - val_loss: 1998.1040\n",
      "Epoch 427/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8118 - val_loss: 1998.1036\n",
      "Epoch 428/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 312.8118 - val_loss: 1998.1038\n",
      "Epoch 429/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8119 - val_loss: 1998.1036\n",
      "Epoch 430/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8119 - val_loss: 1998.1041\n",
      "Epoch 431/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8119 - val_loss: 1998.1041\n",
      "Epoch 432/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8120 - val_loss: 1998.1044\n",
      "Epoch 433/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8120 - val_loss: 1998.1047\n",
      "Epoch 434/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8120 - val_loss: 1998.1045\n",
      "Epoch 435/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8120 - val_loss: 1998.1049\n",
      "Epoch 436/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8120 - val_loss: 1998.1050\n",
      "Epoch 437/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8120 - val_loss: 1998.1052\n",
      "Epoch 438/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8120 - val_loss: 1998.1050\n",
      "Epoch 439/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8120 - val_loss: 1998.1044\n",
      "Epoch 440/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8120 - val_loss: 1998.1036\n",
      "Epoch 441/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8121 - val_loss: 1998.1028\n",
      "Epoch 442/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8121 - val_loss: 1998.1017\n",
      "Epoch 443/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8122 - val_loss: 1998.1017\n",
      "Epoch 444/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8122 - val_loss: 1998.1017\n",
      "Epoch 445/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8122 - val_loss: 1998.1019\n",
      "Epoch 446/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8122 - val_loss: 1998.1017\n",
      "Epoch 447/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8122 - val_loss: 1998.1017\n",
      "Epoch 448/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8123 - val_loss: 1998.1017\n",
      "Epoch 449/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8123 - val_loss: 1998.1023\n",
      "Epoch 450/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 312.8123 - val_loss: 1998.1031\n",
      "Epoch 451/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8122 - val_loss: 1998.1031\n",
      "Epoch 452/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8122 - val_loss: 1998.1024\n",
      "Epoch 453/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8122 - val_loss: 1998.1021\n",
      "Epoch 454/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8123 - val_loss: 1998.1019\n",
      "Epoch 455/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8123 - val_loss: 1998.1021\n",
      "Epoch 456/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8123 - val_loss: 1998.1021\n",
      "Epoch 457/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 312.8123 - val_loss: 1998.1021\n",
      "Epoch 458/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8123 - val_loss: 1998.1021\n",
      "Epoch 459/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 312.8123 - val_loss: 1998.1017\n",
      "Epoch 460/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8123 - val_loss: 1998.1017\n",
      "Epoch 461/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8123 - val_loss: 1998.1017\n",
      "Epoch 462/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8123 - val_loss: 1998.1017\n",
      "Epoch 463/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8123 - val_loss: 1998.1019\n",
      "Epoch 464/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8123 - val_loss: 1998.1016\n",
      "Epoch 465/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8123 - val_loss: 1998.1014\n",
      "Epoch 466/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8124 - val_loss: 1998.1017\n",
      "Epoch 467/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8124 - val_loss: 1998.1017\n",
      "Epoch 468/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8124 - val_loss: 1998.1017\n",
      "Epoch 469/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8124 - val_loss: 1998.1019\n",
      "Epoch 470/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8124 - val_loss: 1998.1023\n",
      "Epoch 471/500\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 312.8125 - val_loss: 1998.1023\n",
      "Epoch 472/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 312.8125 - val_loss: 1998.1023\n",
      "Epoch 473/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8125 - val_loss: 1998.1024\n",
      "Epoch 474/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8125 - val_loss: 1998.1024\n",
      "Epoch 475/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8125 - val_loss: 1998.1024\n",
      "Epoch 476/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 312.8125 - val_loss: 1998.1024\n",
      "Epoch 477/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8125 - val_loss: 1998.1024\n",
      "Epoch 478/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8125 - val_loss: 1998.1027\n",
      "Epoch 479/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8125 - val_loss: 1998.1027\n",
      "Epoch 480/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8125 - val_loss: 1998.1027\n",
      "Epoch 481/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 312.8125 - val_loss: 1998.1027\n",
      "Epoch 482/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8125 - val_loss: 1998.1029\n",
      "Epoch 483/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8125 - val_loss: 1998.1033\n",
      "Epoch 484/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8125 - val_loss: 1998.1040\n",
      "Epoch 485/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8125 - val_loss: 1998.1049\n",
      "Epoch 486/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8125 - val_loss: 1998.1052\n",
      "Epoch 487/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 312.8125 - val_loss: 1998.1053\n",
      "Epoch 488/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 312.8125 - val_loss: 1998.1061\n",
      "Epoch 489/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 312.8125 - val_loss: 1998.1064\n",
      "Epoch 490/500\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 312.8125 - val_loss: 1998.1064\n",
      "Epoch 491/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8125 - val_loss: 1998.1069\n",
      "Epoch 492/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8125 - val_loss: 1998.1066\n",
      "Epoch 493/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8125 - val_loss: 1998.1066\n",
      "Epoch 494/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8125 - val_loss: 1998.1066\n",
      "Epoch 495/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.8125 - val_loss: 1998.1064\n",
      "Epoch 496/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8125 - val_loss: 1998.1064\n",
      "Epoch 497/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8125 - val_loss: 1998.1066\n",
      "Epoch 498/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8125 - val_loss: 1998.1066\n",
      "Epoch 499/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8125 - val_loss: 1998.1066\n",
      "Epoch 500/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 312.8125 - val_loss: 1998.1066\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(88.1552, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0026, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0004, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0102, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.8734, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 444ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.09782073e+01, 7.08976891e+01, 7.08220588e+01, 7.07464286e+01,\n",
       "        7.64097366e+01, 0.00000000e+00, 0.00000000e+00, 2.90957689e-01,\n",
       "        0.00000000e+00, 9.80372650e-02, 3.11266243e-01, 0.00000000e+00,\n",
       "        1.20284572e-01, 7.22878431e+01, 7.18643137e+01, 7.14407843e+01,\n",
       "        3.64094080e-02, 0.00000000e+00, 7.32152895e+01, 7.28839216e+01,\n",
       "        7.24603922e+01, 7.20368627e+01, 7.16133333e+01, 7.12142484e+01,\n",
       "        7.11335761e+01, 7.10529038e+01, 7.09722316e+01, 0.00000000e+00,\n",
       "        8.18514680e-02, 7.17858823e+01, 7.13623529e+01, 7.11664426e+01,\n",
       "        7.10857703e+01, 7.10050980e+01, 7.09244258e+01, 7.08472689e+01,\n",
       "        7.07716387e+01, 7.06960084e+01, 1.45036131e-01, 5.25366127e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.91179383e-01,\n",
       "        0.00000000e+00, 5.29398620e-01, 3.80662382e-01, 7.07800420e+01,\n",
       "        7.07044118e+01, 0.00000000e+00, 3.02265940e-02, 7.10469281e+01,\n",
       "        7.09662558e+01, 7.08864846e+01, 7.08108543e+01, 7.07352241e+01,\n",
       "        7.44450980e+01, 7.34823296e+01, 7.23819608e+01, 8.38599100e-02,\n",
       "        0.00000000e+00, 1.07607710e-01, 7.07660364e+01, 7.06904062e+01,\n",
       "        7.38828898e+01, 7.28996078e+01, 7.16290196e+01, 7.33609477e+01,\n",
       "        7.22250980e+01, 7.11694304e+01, 0.00000000e+00, 4.22481810e-01,\n",
       "        0.00000000e+00, 6.64566879e+01, 1.26242206e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 8.87260556e-01, 7.51196520e-02,\n",
       "        6.96443939e+01, 0.00000000e+00, 0.00000000e+00, 3.49589199e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        6.50203168e-01, 6.65643692e-01, 7.74182200e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.52908349e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([55.97866366, 55.9630867 , 55.94750974, 55.93193277, 55.91635581,\n",
       "       55.90077885, 55.88520189, 55.86962492, 55.85404796, 55.838471  ,\n",
       "       55.82289404, 55.80731707, 55.79174011, 55.77616315, 55.76058619,\n",
       "       55.74500922, 55.72943226, 55.7138553 , 55.69827834, 55.68270137,\n",
       "       55.66712441, 55.65154745, 55.63597049, 55.62039352, 55.60481656,\n",
       "       55.5892396 , 55.57366264, 55.55808567, 55.54250871, 55.52693175,\n",
       "       55.51135479, 55.49577782, 55.48020086, 55.4646239 , 55.44904694,\n",
       "       55.43346997, 55.41789301, 55.40231605, 55.38673909, 55.37116212,\n",
       "       55.35558516, 55.3400082 , 55.32443124, 55.30885427, 55.29327731,\n",
       "       55.27770035, 55.26212339, 55.24654642, 55.23096946, 55.2153925 ,\n",
       "       55.19981554, 55.18423857, 55.16866161, 55.15308465, 55.13750769,\n",
       "       55.12193072, 55.10635376, 55.0907768 , 55.07519984, 55.05962287,\n",
       "       55.04404591, 55.02846895, 55.01289199, 54.99731502, 54.98173806,\n",
       "       54.9661611 , 54.95058414, 54.93500717, 54.91943021, 54.90385325,\n",
       "       54.88827629, 54.87269932, 54.85712236, 54.8415454 , 54.82596844,\n",
       "       54.81039147, 54.79481451, 54.77923755, 54.76366059, 54.74808362,\n",
       "       54.73250666, 54.7169297 , 54.70135274, 54.68577577, 54.67019881,\n",
       "       54.65462185, 54.63904489, 54.62346792, 54.60789096, 54.592314  ,\n",
       "       54.57673704, 54.56116007, 54.54558311, 54.53000615, 54.51442919,\n",
       "       54.49885222, 54.48327526, 54.4676983 , 54.45212134, 54.43654437])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.82533261296781\n",
      "37.08201524376192\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
