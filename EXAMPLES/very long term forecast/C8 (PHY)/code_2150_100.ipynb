{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "2245    57.590633\n",
       "2246    57.581615\n",
       "2247    57.572597\n",
       "2248    57.563579\n",
       "2249    57.554560\n",
       "Name: C8, Length: 2250, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c8_interpolated_2150_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "2145     0.000000\n",
       "2146     1.116794\n",
       "2147     0.000000\n",
       "2148     0.000000\n",
       "2149     0.056816\n",
       "Name: C8, Length: 2150, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2150)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.987115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.774230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.561345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.348459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.200000  0.000298\n",
       "1     87.987115  0.000298\n",
       "2     87.774230  0.000297\n",
       "3     87.561345  0.000297\n",
       "4     87.348459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 85.7336  # Value for C0\n",
    "K0 = -0.0020  # Value for K0\n",
    "K1 = 0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0125    # Value for b\n",
    "c = 2.3008    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.200000    87.987115    87.774230    87.561345    87.348459   \n",
      "351    87.987115    87.774230    87.561345    87.348459    87.135574   \n",
      "352    87.774230    87.561345    87.348459    87.135574    86.922689   \n",
      "353    87.561345    87.348459    87.135574    86.922689    86.709804   \n",
      "354    87.348459    87.135574    86.922689    86.709804    86.496919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.135574    86.922689    86.709804    86.496919    86.294538  ...   \n",
      "351    86.922689    86.709804    86.496919    86.294538    86.221709  ...   \n",
      "352    86.709804    86.496919    86.294538    86.221709    86.148880  ...   \n",
      "353    86.496919    86.294538    86.221709    86.148880    86.076050  ...   \n",
      "354    86.294538    86.221709    86.148880    86.076050    86.003221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   76.094351    0.000263   76.058870    0.000263   76.023389    0.000263   \n",
      "351   76.058870    0.000263   76.023389    0.000263   75.987909    0.000262   \n",
      "352   76.023389    0.000263   75.987909    0.000262   75.952428    0.000262   \n",
      "353   75.987909    0.000262   75.952428    0.000262   75.916947    0.000262   \n",
      "354   75.952428    0.000262   75.916947    0.000262   75.881466    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   75.987909    0.000262   75.952428    0.000262  \n",
      "351   75.952428    0.000262   75.916947    0.000262  \n",
      "352   75.916947    0.000262   75.881466    0.000262  \n",
      "353   75.881466    0.000262   75.845985    0.000262  \n",
      "354   75.845985    0.000262   75.810504    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1700, 1, 251) (1700, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 2s 24ms/step - loss: 4545.4517 - val_loss: 2587.1187\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4372.2686 - val_loss: 2489.6147\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4248.9004 - val_loss: 2418.9470\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4123.7505 - val_loss: 2344.5981\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4019.7290 - val_loss: 2290.7351\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3928.6089 - val_loss: 2240.3027\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3841.2854 - val_loss: 2192.0659\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3756.7400 - val_loss: 2145.9878\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3674.5762 - val_loss: 2101.2451\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3594.4348 - val_loss: 2057.5742\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3505.5312 - val_loss: 2008.2369\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 3422.8918 - val_loss: 1965.3019\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3343.8518 - val_loss: 1924.1475\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3267.1294 - val_loss: 1884.5243\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3192.3716 - val_loss: 1846.2815\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3119.3721 - val_loss: 1809.3256\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 3048.0017 - val_loss: 1773.5908\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2978.1692 - val_loss: 1739.0270\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2909.8066 - val_loss: 1705.5940\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2842.8599 - val_loss: 1673.2568\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2777.2832 - val_loss: 1641.9858\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2713.0371 - val_loss: 1611.7540\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2650.0896 - val_loss: 1582.5374\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2588.4082 - val_loss: 1554.3125\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2527.9666 - val_loss: 1527.0583\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2468.7390 - val_loss: 1500.7546\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2410.7012 - val_loss: 1474.8892\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2349.0925 - val_loss: 1447.2627\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2287.3965 - val_loss: 1421.5573\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2228.1958 - val_loss: 1397.3251\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2171.1135 - val_loss: 1374.3351\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2115.7717 - val_loss: 1352.4548\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2061.9595 - val_loss: 1331.6042\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2009.5479 - val_loss: 1311.7263\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1958.4509 - val_loss: 1292.7792\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1908.6033 - val_loss: 1274.7274\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1859.9546 - val_loss: 1257.5421\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1812.4626 - val_loss: 1241.1968\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1766.0923 - val_loss: 1225.6680\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1720.8119 - val_loss: 1210.9337\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1676.5928 - val_loss: 1196.9741\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1633.4113 - val_loss: 1183.7694\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1591.2419 - val_loss: 1171.3019\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1550.0640 - val_loss: 1159.5535\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 1509.8562 - val_loss: 1148.5076\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 1470.5994 - val_loss: 1138.1477\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1432.2745 - val_loss: 1128.4576\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1394.8633 - val_loss: 1119.4224\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1358.3491 - val_loss: 1111.0261\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1322.7148 - val_loss: 1103.2543\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1287.9443 - val_loss: 1096.0925\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1254.0220 - val_loss: 1089.5261\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1220.9319 - val_loss: 1083.5409\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1188.6594 - val_loss: 1078.1230\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1157.1897 - val_loss: 1073.2590\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1126.5081 - val_loss: 1068.9347\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1096.6010 - val_loss: 1065.1375\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 1067.4543 - val_loss: 1061.8539\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 1039.0538 - val_loss: 1059.0706\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1011.3868 - val_loss: 1056.7749\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 984.4395 - val_loss: 1054.9542\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 958.1993 - val_loss: 1053.5956\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 932.6528 - val_loss: 1052.6865\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 907.7877 - val_loss: 1052.2150\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 883.5912 - val_loss: 1052.1683\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 860.0510 - val_loss: 1052.5345\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 837.1545 - val_loss: 1053.3016\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 814.8903 - val_loss: 1054.4575\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 793.2458 - val_loss: 1055.9907\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 772.2094 - val_loss: 1057.8893\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 751.7697 - val_loss: 1060.1416\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 731.9144 - val_loss: 1062.7362\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 712.6326 - val_loss: 1065.6620\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 693.9126 - val_loss: 1068.9072\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 675.7435 - val_loss: 1072.4612\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 658.1141 - val_loss: 1076.3124\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 641.0131 - val_loss: 1080.4503\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 624.4298 - val_loss: 1084.8638\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 608.3538 - val_loss: 1089.5426\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 592.7737 - val_loss: 1094.4755\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 577.6795 - val_loss: 1099.6521\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 563.0605 - val_loss: 1105.0621\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 548.9064 - val_loss: 1110.6954\n",
      "Epoch 84/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 535.2069 - val_loss: 1116.5422\n",
      "Epoch 85/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 521.9520 - val_loss: 1122.5913\n",
      "Epoch 86/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 509.1317 - val_loss: 1128.8342\n",
      "Epoch 87/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 496.7357 - val_loss: 1135.2605\n",
      "Epoch 88/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 484.7545 - val_loss: 1141.8610\n",
      "Epoch 89/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 473.1784 - val_loss: 1148.6274\n",
      "Epoch 90/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 461.9977 - val_loss: 1155.5532\n",
      "Epoch 91/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 451.2030 - val_loss: 1162.6556\n",
      "Epoch 92/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 440.8017 - val_loss: 1169.5037\n",
      "Epoch 93/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 430.7339 - val_loss: 1176.8228\n",
      "Epoch 94/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 421.0411 - val_loss: 1184.2596\n",
      "Epoch 95/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 411.6971 - val_loss: 1191.8053\n",
      "Epoch 96/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 402.6932 - val_loss: 1199.4509\n",
      "Epoch 97/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 394.0205 - val_loss: 1207.1885\n",
      "Epoch 98/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 385.6701 - val_loss: 1215.0084\n",
      "Epoch 99/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 377.6336 - val_loss: 1222.9034\n",
      "Epoch 100/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 369.9025 - val_loss: 1230.8651\n",
      "Epoch 101/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 362.4682 - val_loss: 1238.8856\n",
      "Epoch 102/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 355.3226 - val_loss: 1246.9569\n",
      "Epoch 103/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 348.4576 - val_loss: 1255.0715\n",
      "Epoch 104/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 341.8649 - val_loss: 1263.2217\n",
      "Epoch 105/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 335.5368 - val_loss: 1271.4003\n",
      "Epoch 106/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 329.4655 - val_loss: 1279.6001\n",
      "Epoch 107/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 323.6430 - val_loss: 1287.8142\n",
      "Epoch 108/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 318.0622 - val_loss: 1296.0348\n",
      "Epoch 109/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 312.7154 - val_loss: 1304.2566\n",
      "Epoch 110/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 307.5953 - val_loss: 1312.4724\n",
      "Epoch 111/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 302.6947 - val_loss: 1320.6752\n",
      "Epoch 112/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 298.0067 - val_loss: 1328.8597\n",
      "Epoch 113/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 293.5241 - val_loss: 1337.0203\n",
      "Epoch 114/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 289.2402 - val_loss: 1345.1498\n",
      "Epoch 115/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 285.1486 - val_loss: 1353.2435\n",
      "Epoch 116/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 281.2423 - val_loss: 1361.2959\n",
      "Epoch 117/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 277.5152 - val_loss: 1369.3008\n",
      "Epoch 118/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 273.9609 - val_loss: 1377.2545\n",
      "Epoch 119/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 270.5732 - val_loss: 1385.1509\n",
      "Epoch 120/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 267.3461 - val_loss: 1392.9861\n",
      "Epoch 121/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 264.2737 - val_loss: 1400.7549\n",
      "Epoch 122/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 261.3503 - val_loss: 1408.4529\n",
      "Epoch 123/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 258.5703 - val_loss: 1416.0768\n",
      "Epoch 124/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 255.9280 - val_loss: 1423.6215\n",
      "Epoch 125/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 253.4182 - val_loss: 1431.0839\n",
      "Epoch 126/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 251.0358 - val_loss: 1438.4603\n",
      "Epoch 127/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 248.7753 - val_loss: 1445.7468\n",
      "Epoch 128/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 246.6321 - val_loss: 1452.9403\n",
      "Epoch 129/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 244.6013 - val_loss: 1460.0385\n",
      "Epoch 130/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 242.6780 - val_loss: 1467.0377\n",
      "Epoch 131/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 240.8578 - val_loss: 1473.9357\n",
      "Epoch 132/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 239.1362 - val_loss: 1480.7294\n",
      "Epoch 133/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 237.5090 - val_loss: 1487.4164\n",
      "Epoch 134/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 235.9720 - val_loss: 1493.9939\n",
      "Epoch 135/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 234.5210 - val_loss: 1500.3763\n",
      "Epoch 136/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 233.1522 - val_loss: 1506.8246\n",
      "Epoch 137/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 231.8616 - val_loss: 1513.0684\n",
      "Epoch 138/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 230.6458 - val_loss: 1519.1962\n",
      "Epoch 139/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 229.5011 - val_loss: 1525.2081\n",
      "Epoch 140/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 228.4241 - val_loss: 1531.1021\n",
      "Epoch 141/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 227.4115 - val_loss: 1536.8782\n",
      "Epoch 142/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 226.4600 - val_loss: 1542.5352\n",
      "Epoch 143/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 225.5666 - val_loss: 1548.0720\n",
      "Epoch 144/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 224.7283 - val_loss: 1553.4883\n",
      "Epoch 145/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 223.9425 - val_loss: 1558.7848\n",
      "Epoch 146/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 223.2060 - val_loss: 1563.9600\n",
      "Epoch 147/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 222.5166 - val_loss: 1569.0146\n",
      "Epoch 148/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 221.8715 - val_loss: 1573.9489\n",
      "Epoch 149/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 221.2683 - val_loss: 1578.7623\n",
      "Epoch 150/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 220.7048 - val_loss: 1583.4559\n",
      "Epoch 151/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 220.1786 - val_loss: 1588.0302\n",
      "Epoch 152/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 219.6878 - val_loss: 1592.4851\n",
      "Epoch 153/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 219.2303 - val_loss: 1596.8223\n",
      "Epoch 154/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 218.8040 - val_loss: 1601.0425\n",
      "Epoch 155/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 218.4072 - val_loss: 1605.1461\n",
      "Epoch 156/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 218.0383 - val_loss: 1609.1345\n",
      "Epoch 157/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 217.6952 - val_loss: 1613.0090\n",
      "Epoch 158/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 217.3766 - val_loss: 1616.7704\n",
      "Epoch 159/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 217.0809 - val_loss: 1620.4200\n",
      "Epoch 160/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 216.8067 - val_loss: 1623.9597\n",
      "Epoch 161/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 216.5526 - val_loss: 1627.3909\n",
      "Epoch 162/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 216.3174 - val_loss: 1630.7151\n",
      "Epoch 163/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 216.0997 - val_loss: 1633.9340\n",
      "Epoch 164/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 215.8984 - val_loss: 1637.0483\n",
      "Epoch 165/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 215.7125 - val_loss: 1640.0618\n",
      "Epoch 166/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 215.5409 - val_loss: 1642.9756\n",
      "Epoch 167/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 215.3827 - val_loss: 1645.7899\n",
      "Epoch 168/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 215.2368 - val_loss: 1648.5077\n",
      "Epoch 169/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 215.1024 - val_loss: 1651.1316\n",
      "Epoch 170/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 214.9788 - val_loss: 1653.6628\n",
      "Epoch 171/500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 214.8652 - val_loss: 1656.1040\n",
      "Epoch 172/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 214.7607 - val_loss: 1658.4563\n",
      "Epoch 173/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 214.6648 - val_loss: 1660.7217\n",
      "Epoch 174/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 214.5769 - val_loss: 1662.9034\n",
      "Epoch 175/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 214.4963 - val_loss: 1665.0022\n",
      "Epoch 176/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 214.4224 - val_loss: 1667.0200\n",
      "Epoch 177/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 214.3550 - val_loss: 1668.9604\n",
      "Epoch 178/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 214.2932 - val_loss: 1670.8242\n",
      "Epoch 179/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 214.2368 - val_loss: 1672.6145\n",
      "Epoch 180/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 214.1853 - val_loss: 1674.3322\n",
      "Epoch 181/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 214.1384 - val_loss: 1675.9801\n",
      "Epoch 182/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 214.0955 - val_loss: 1677.5607\n",
      "Epoch 183/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 214.0566 - val_loss: 1679.0745\n",
      "Epoch 184/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 214.0211 - val_loss: 1680.5243\n",
      "Epoch 185/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.9890 - val_loss: 1681.9132\n",
      "Epoch 186/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.9597 - val_loss: 1683.2416\n",
      "Epoch 187/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.9332 - val_loss: 1684.5114\n",
      "Epoch 188/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.9092 - val_loss: 1685.7263\n",
      "Epoch 189/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.8874 - val_loss: 1686.8866\n",
      "Epoch 190/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.8677 - val_loss: 1687.9950\n",
      "Epoch 191/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.8500 - val_loss: 1689.0533\n",
      "Epoch 192/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.8339 - val_loss: 1690.0632\n",
      "Epoch 193/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.8194 - val_loss: 1691.0291\n",
      "Epoch 194/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.8064 - val_loss: 1691.9651\n",
      "Epoch 195/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.6853 - val_loss: 1692.7002\n",
      "Epoch 196/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7844 - val_loss: 1693.5323\n",
      "Epoch 197/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7749 - val_loss: 1694.3231\n",
      "Epoch 198/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7664 - val_loss: 1695.0746\n",
      "Epoch 199/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7589 - val_loss: 1695.7899\n",
      "Epoch 200/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7522 - val_loss: 1696.4684\n",
      "Epoch 201/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7463 - val_loss: 1697.1133\n",
      "Epoch 202/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7410 - val_loss: 1697.7249\n",
      "Epoch 203/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7363 - val_loss: 1698.3053\n",
      "Epoch 204/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7322 - val_loss: 1698.8558\n",
      "Epoch 205/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7287 - val_loss: 1699.3776\n",
      "Epoch 206/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7255 - val_loss: 1699.8717\n",
      "Epoch 207/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 213.7227 - val_loss: 1700.3397\n",
      "Epoch 208/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7203 - val_loss: 1700.7827\n",
      "Epoch 209/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7183 - val_loss: 1701.2024\n",
      "Epoch 210/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7166 - val_loss: 1701.5984\n",
      "Epoch 211/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7151 - val_loss: 1701.9730\n",
      "Epoch 212/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7139 - val_loss: 1702.3264\n",
      "Epoch 213/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7129 - val_loss: 1702.6606\n",
      "Epoch 214/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7121 - val_loss: 1702.9756\n",
      "Epoch 215/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7114 - val_loss: 1703.2728\n",
      "Epoch 216/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7110 - val_loss: 1703.5520\n",
      "Epoch 217/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7107 - val_loss: 1703.8168\n",
      "Epoch 218/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7105 - val_loss: 1704.0651\n",
      "Epoch 219/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7105 - val_loss: 1704.2997\n",
      "Epoch 220/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7105 - val_loss: 1704.5197\n",
      "Epoch 221/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7107 - val_loss: 1704.7267\n",
      "Epoch 222/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7109 - val_loss: 1704.9211\n",
      "Epoch 223/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7112 - val_loss: 1705.1039\n",
      "Epoch 224/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7117 - val_loss: 1705.2761\n",
      "Epoch 225/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7121 - val_loss: 1705.4369\n",
      "Epoch 226/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7126 - val_loss: 1705.5880\n",
      "Epoch 227/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7132 - val_loss: 1705.7296\n",
      "Epoch 228/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7138 - val_loss: 1705.8625\n",
      "Epoch 229/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7144 - val_loss: 1705.9875\n",
      "Epoch 230/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7150 - val_loss: 1706.1039\n",
      "Epoch 231/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7157 - val_loss: 1706.2123\n",
      "Epoch 232/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7165 - val_loss: 1706.3140\n",
      "Epoch 233/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7173 - val_loss: 1706.4088\n",
      "Epoch 234/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7180 - val_loss: 1706.4978\n",
      "Epoch 235/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7188 - val_loss: 1706.5807\n",
      "Epoch 236/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7196 - val_loss: 1706.6570\n",
      "Epoch 237/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7204 - val_loss: 1706.7300\n",
      "Epoch 238/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 213.7212 - val_loss: 1706.7969\n",
      "Epoch 239/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7221 - val_loss: 1706.8591\n",
      "Epoch 240/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7230 - val_loss: 1706.9171\n",
      "Epoch 241/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7238 - val_loss: 1706.9702\n",
      "Epoch 242/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7248 - val_loss: 1707.0210\n",
      "Epoch 243/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7256 - val_loss: 1707.0673\n",
      "Epoch 244/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7264 - val_loss: 1707.1105\n",
      "Epoch 245/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7273 - val_loss: 1707.1494\n",
      "Epoch 246/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7282 - val_loss: 1707.1869\n",
      "Epoch 247/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7290 - val_loss: 1707.2206\n",
      "Epoch 248/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7299 - val_loss: 1707.2518\n",
      "Epoch 249/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7308 - val_loss: 1707.2809\n",
      "Epoch 250/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7317 - val_loss: 1707.3074\n",
      "Epoch 251/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7326 - val_loss: 1707.3317\n",
      "Epoch 252/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7334 - val_loss: 1707.3539\n",
      "Epoch 253/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7343 - val_loss: 1707.3748\n",
      "Epoch 254/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7351 - val_loss: 1707.3939\n",
      "Epoch 255/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7360 - val_loss: 1707.4109\n",
      "Epoch 256/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7368 - val_loss: 1707.4259\n",
      "Epoch 257/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7376 - val_loss: 1707.4404\n",
      "Epoch 258/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7385 - val_loss: 1707.4535\n",
      "Epoch 259/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7392 - val_loss: 1707.4637\n",
      "Epoch 260/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7401 - val_loss: 1707.4749\n",
      "Epoch 261/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7409 - val_loss: 1707.4830\n",
      "Epoch 262/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7418 - val_loss: 1707.4908\n",
      "Epoch 263/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7426 - val_loss: 1707.4990\n",
      "Epoch 264/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7433 - val_loss: 1707.5057\n",
      "Epoch 265/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7441 - val_loss: 1707.5109\n",
      "Epoch 266/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7449 - val_loss: 1707.5146\n",
      "Epoch 267/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7456 - val_loss: 1707.5188\n",
      "Epoch 268/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 213.7464 - val_loss: 1707.5225\n",
      "Epoch 269/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7471 - val_loss: 1707.5240\n",
      "Epoch 270/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7478 - val_loss: 1707.5258\n",
      "Epoch 271/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7486 - val_loss: 1707.5277\n",
      "Epoch 272/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7492 - val_loss: 1707.5276\n",
      "Epoch 273/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7500 - val_loss: 1707.5280\n",
      "Epoch 274/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7506 - val_loss: 1707.5272\n",
      "Epoch 275/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7514 - val_loss: 1707.5261\n",
      "Epoch 276/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7520 - val_loss: 1707.5247\n",
      "Epoch 277/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7527 - val_loss: 1707.5228\n",
      "Epoch 278/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7533 - val_loss: 1707.5203\n",
      "Epoch 279/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7540 - val_loss: 1707.5171\n",
      "Epoch 280/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7546 - val_loss: 1707.5134\n",
      "Epoch 281/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7552 - val_loss: 1707.5099\n",
      "Epoch 282/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7559 - val_loss: 1707.5050\n",
      "Epoch 283/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7565 - val_loss: 1707.5002\n",
      "Epoch 284/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7570 - val_loss: 1707.4949\n",
      "Epoch 285/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7576 - val_loss: 1707.4891\n",
      "Epoch 286/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7583 - val_loss: 1707.4825\n",
      "Epoch 287/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7588 - val_loss: 1707.4753\n",
      "Epoch 288/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7594 - val_loss: 1707.4674\n",
      "Epoch 289/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7599 - val_loss: 1707.4586\n",
      "Epoch 290/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7605 - val_loss: 1707.4489\n",
      "Epoch 291/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7610 - val_loss: 1707.4382\n",
      "Epoch 292/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7616 - val_loss: 1707.4261\n",
      "Epoch 293/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7621 - val_loss: 1707.4121\n",
      "Epoch 294/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7626 - val_loss: 1707.3965\n",
      "Epoch 295/500\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 213.7631 - val_loss: 1707.3789\n",
      "Epoch 296/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7636 - val_loss: 1707.3566\n",
      "Epoch 297/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7641 - val_loss: 1707.3311\n",
      "Epoch 298/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7645 - val_loss: 1707.2991\n",
      "Epoch 299/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7650 - val_loss: 1707.2572\n",
      "Epoch 300/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 213.7655 - val_loss: 1707.2014\n",
      "Epoch 301/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 213.7660 - val_loss: 1707.1210\n",
      "Epoch 302/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 213.7664 - val_loss: 1706.9928\n",
      "Epoch 303/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7668 - val_loss: 1706.7693\n",
      "Epoch 304/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7672 - val_loss: 1706.4875\n",
      "Epoch 305/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7648 - val_loss: 1698.8427\n",
      "Epoch 306/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7717 - val_loss: 1708.0538\n",
      "Epoch 307/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7685 - val_loss: 1708.0500\n",
      "Epoch 308/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 213.7689 - val_loss: 1708.0450\n",
      "Epoch 309/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 213.7693 - val_loss: 1708.0399\n",
      "Epoch 310/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 213.7697 - val_loss: 1708.0344\n",
      "Epoch 311/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 213.7701 - val_loss: 1708.0284\n",
      "Epoch 312/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 213.7705 - val_loss: 1708.0236\n",
      "Epoch 313/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 213.7708 - val_loss: 1708.0189\n",
      "Epoch 314/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 213.7712 - val_loss: 1708.0132\n",
      "Epoch 315/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 213.7715 - val_loss: 1708.0084\n",
      "Epoch 316/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 213.7719 - val_loss: 1708.0029\n",
      "Epoch 317/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 213.7722 - val_loss: 1707.9963\n",
      "Epoch 318/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 213.7725 - val_loss: 1707.9908\n",
      "Epoch 319/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 213.7728 - val_loss: 1707.9847\n",
      "Epoch 320/500\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 213.7732 - val_loss: 1707.9786\n",
      "Epoch 321/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 213.7735 - val_loss: 1707.9727\n",
      "Epoch 322/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 213.7738 - val_loss: 1707.9666\n",
      "Epoch 323/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7742 - val_loss: 1707.9597\n",
      "Epoch 324/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7744 - val_loss: 1707.9531\n",
      "Epoch 325/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7747 - val_loss: 1707.9462\n",
      "Epoch 326/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7750 - val_loss: 1707.9388\n",
      "Epoch 327/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7753 - val_loss: 1707.9313\n",
      "Epoch 328/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7756 - val_loss: 1707.9240\n",
      "Epoch 329/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7758 - val_loss: 1707.9165\n",
      "Epoch 330/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7761 - val_loss: 1707.9092\n",
      "Epoch 331/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7763 - val_loss: 1707.9009\n",
      "Epoch 332/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7766 - val_loss: 1707.8921\n",
      "Epoch 333/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7769 - val_loss: 1707.8837\n",
      "Epoch 334/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7771 - val_loss: 1707.8750\n",
      "Epoch 335/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7773 - val_loss: 1707.8655\n",
      "Epoch 336/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7776 - val_loss: 1707.8558\n",
      "Epoch 337/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7778 - val_loss: 1707.8458\n",
      "Epoch 338/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7780 - val_loss: 1707.8358\n",
      "Epoch 339/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7782 - val_loss: 1707.8252\n",
      "Epoch 340/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7784 - val_loss: 1707.8131\n",
      "Epoch 341/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7786 - val_loss: 1707.8015\n",
      "Epoch 342/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7788 - val_loss: 1707.7893\n",
      "Epoch 343/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7768 - val_loss: 1707.6116\n",
      "Epoch 344/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 213.7801 - val_loss: 1707.7568\n",
      "Epoch 345/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7795 - val_loss: 1707.7434\n",
      "Epoch 346/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7797 - val_loss: 1707.7290\n",
      "Epoch 347/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7798 - val_loss: 1707.7136\n",
      "Epoch 348/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7800 - val_loss: 1707.6969\n",
      "Epoch 349/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7802 - val_loss: 1707.6785\n",
      "Epoch 350/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 213.7804 - val_loss: 1707.6602\n",
      "Epoch 351/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7806 - val_loss: 1707.6396\n",
      "Epoch 352/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7808 - val_loss: 1707.6189\n",
      "Epoch 353/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7809 - val_loss: 1707.5956\n",
      "Epoch 354/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7811 - val_loss: 1707.5704\n",
      "Epoch 355/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7812 - val_loss: 1707.5428\n",
      "Epoch 356/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7814 - val_loss: 1707.5121\n",
      "Epoch 357/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7816 - val_loss: 1707.4775\n",
      "Epoch 358/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7817 - val_loss: 1707.4382\n",
      "Epoch 359/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7818 - val_loss: 1707.3925\n",
      "Epoch 360/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7820 - val_loss: 1707.3375\n",
      "Epoch 361/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7822 - val_loss: 1707.2686\n",
      "Epoch 362/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7823 - val_loss: 1707.1763\n",
      "Epoch 363/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7824 - val_loss: 1707.0333\n",
      "Epoch 364/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7825 - val_loss: 1706.7316\n",
      "Epoch 365/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7823 - val_loss: 1696.8688\n",
      "Epoch 366/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 207.1122 - val_loss: 1631.4749\n",
      "Epoch 367/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 216.4377 - val_loss: 1636.2688\n",
      "Epoch 368/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 216.1846 - val_loss: 1642.0524\n",
      "Epoch 369/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 215.8441 - val_loss: 1647.5322\n",
      "Epoch 370/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 215.5422 - val_loss: 1652.5970\n",
      "Epoch 371/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 215.2847 - val_loss: 1657.2645\n",
      "Epoch 372/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 215.0660 - val_loss: 1661.5638\n",
      "Epoch 373/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 214.8803 - val_loss: 1665.5217\n",
      "Epoch 374/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 214.7223 - val_loss: 1669.1639\n",
      "Epoch 375/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 214.5882 - val_loss: 1672.5156\n",
      "Epoch 376/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 214.4738 - val_loss: 1675.5989\n",
      "Epoch 377/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 214.3765 - val_loss: 1678.4351\n",
      "Epoch 378/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 214.2934 - val_loss: 1681.0415\n",
      "Epoch 379/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 214.2225 - val_loss: 1683.4379\n",
      "Epoch 380/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 214.1621 - val_loss: 1685.6412\n",
      "Epoch 381/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 214.1103 - val_loss: 1687.6656\n",
      "Epoch 382/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 214.0659 - val_loss: 1689.5243\n",
      "Epoch 383/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 214.0279 - val_loss: 1691.2330\n",
      "Epoch 384/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.9953 - val_loss: 1692.8030\n",
      "Epoch 385/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.9672 - val_loss: 1694.2434\n",
      "Epoch 386/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.9431 - val_loss: 1695.5668\n",
      "Epoch 387/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.9224 - val_loss: 1696.7823\n",
      "Epoch 388/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.9043 - val_loss: 1697.8979\n",
      "Epoch 389/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.8889 - val_loss: 1698.9225\n",
      "Epoch 390/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.8754 - val_loss: 1699.8625\n",
      "Epoch 391/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.8638 - val_loss: 1700.7256\n",
      "Epoch 392/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.8537 - val_loss: 1701.5177\n",
      "Epoch 393/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.8449 - val_loss: 1702.2452\n",
      "Epoch 394/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.8372 - val_loss: 1702.9116\n",
      "Epoch 395/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.8306 - val_loss: 1703.5237\n",
      "Epoch 396/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.8247 - val_loss: 1704.0854\n",
      "Epoch 397/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.8195 - val_loss: 1704.6017\n",
      "Epoch 398/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.8150 - val_loss: 1705.0737\n",
      "Epoch 399/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 213.8111 - val_loss: 1705.5061\n",
      "Epoch 400/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.8076 - val_loss: 1705.9044\n",
      "Epoch 401/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.8045 - val_loss: 1706.2681\n",
      "Epoch 402/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.8018 - val_loss: 1706.6024\n",
      "Epoch 403/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7994 - val_loss: 1706.9091\n",
      "Epoch 404/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7973 - val_loss: 1707.1910\n",
      "Epoch 405/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7954 - val_loss: 1707.4492\n",
      "Epoch 406/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7937 - val_loss: 1707.6854\n",
      "Epoch 407/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7922 - val_loss: 1707.9020\n",
      "Epoch 408/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7908 - val_loss: 1708.1010\n",
      "Epoch 409/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7897 - val_loss: 1708.2837\n",
      "Epoch 410/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7886 - val_loss: 1708.4496\n",
      "Epoch 411/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7877 - val_loss: 1708.6028\n",
      "Epoch 412/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7870 - val_loss: 1708.7439\n",
      "Epoch 413/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7860 - val_loss: 1708.8724\n",
      "Epoch 414/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7853 - val_loss: 1708.9901\n",
      "Epoch 415/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7847 - val_loss: 1709.0977\n",
      "Epoch 416/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7842 - val_loss: 1709.1963\n",
      "Epoch 417/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7838 - val_loss: 1709.2877\n",
      "Epoch 418/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7834 - val_loss: 1709.3715\n",
      "Epoch 419/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7829 - val_loss: 1709.4474\n",
      "Epoch 420/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7826 - val_loss: 1709.5181\n",
      "Epoch 421/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7823 - val_loss: 1709.5824\n",
      "Epoch 422/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7820 - val_loss: 1709.6410\n",
      "Epoch 423/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 213.7817 - val_loss: 1709.6947\n",
      "Epoch 424/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 213.7815 - val_loss: 1709.7439\n",
      "Epoch 425/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 213.7812 - val_loss: 1709.7896\n",
      "Epoch 426/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 213.7810 - val_loss: 1709.8311\n",
      "Epoch 427/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 213.7809 - val_loss: 1709.8688\n",
      "Epoch 428/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7808 - val_loss: 1709.9045\n",
      "Epoch 429/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7806 - val_loss: 1709.9358\n",
      "Epoch 430/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7804 - val_loss: 1709.9651\n",
      "Epoch 431/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7803 - val_loss: 1709.9912\n",
      "Epoch 432/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7803 - val_loss: 1710.0160\n",
      "Epoch 433/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7802 - val_loss: 1710.0397\n",
      "Epoch 434/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 213.7802 - val_loss: 1710.0608\n",
      "Epoch 435/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 213.7800 - val_loss: 1710.0795\n",
      "Epoch 436/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 213.7799 - val_loss: 1710.0958\n",
      "Epoch 437/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 213.7798 - val_loss: 1710.1112\n",
      "Epoch 438/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7798 - val_loss: 1710.1237\n",
      "Epoch 439/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7798 - val_loss: 1710.1371\n",
      "Epoch 440/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7798 - val_loss: 1710.1493\n",
      "Epoch 441/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7799 - val_loss: 1710.1610\n",
      "Epoch 442/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 213.7798 - val_loss: 1710.1715\n",
      "Epoch 443/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7798 - val_loss: 1710.1810\n",
      "Epoch 444/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 213.7799 - val_loss: 1710.1901\n",
      "Epoch 445/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 213.7798 - val_loss: 1710.1976\n",
      "Epoch 446/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 213.7798 - val_loss: 1710.2056\n",
      "Epoch 447/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7798 - val_loss: 1710.2125\n",
      "Epoch 448/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7797 - val_loss: 1710.2191\n",
      "Epoch 449/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7797 - val_loss: 1710.2246\n",
      "Epoch 450/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7797 - val_loss: 1710.2297\n",
      "Epoch 451/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7797 - val_loss: 1710.2351\n",
      "Epoch 452/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7797 - val_loss: 1710.2395\n",
      "Epoch 453/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7797 - val_loss: 1710.2434\n",
      "Epoch 454/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7796 - val_loss: 1710.2457\n",
      "Epoch 455/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7797 - val_loss: 1710.2487\n",
      "Epoch 456/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7797 - val_loss: 1710.2518\n",
      "Epoch 457/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7797 - val_loss: 1710.2540\n",
      "Epoch 458/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7797 - val_loss: 1710.2566\n",
      "Epoch 459/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7797 - val_loss: 1710.2577\n",
      "Epoch 460/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 213.7797 - val_loss: 1710.2594\n",
      "Epoch 461/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7797 - val_loss: 1710.2606\n",
      "Epoch 462/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7797 - val_loss: 1710.2623\n",
      "Epoch 463/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7797 - val_loss: 1710.2631\n",
      "Epoch 464/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 213.7797 - val_loss: 1710.2646\n",
      "Epoch 465/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7798 - val_loss: 1710.2653\n",
      "Epoch 466/500\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 213.7798 - val_loss: 1710.2670\n",
      "Epoch 467/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 213.7798 - val_loss: 1710.2686\n",
      "Epoch 468/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 213.7798 - val_loss: 1710.2703\n",
      "Epoch 469/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 213.7799 - val_loss: 1710.2715\n",
      "Epoch 470/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 213.7798 - val_loss: 1710.2728\n",
      "Epoch 471/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 213.7798 - val_loss: 1710.2732\n",
      "Epoch 472/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 213.7798 - val_loss: 1710.2733\n",
      "Epoch 473/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 213.7798 - val_loss: 1710.2732\n",
      "Epoch 474/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 213.7799 - val_loss: 1710.2736\n",
      "Epoch 475/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 213.7799 - val_loss: 1710.2739\n",
      "Epoch 476/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 213.7799 - val_loss: 1710.2743\n",
      "Epoch 477/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 213.7799 - val_loss: 1710.2751\n",
      "Epoch 478/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 213.7799 - val_loss: 1710.2761\n",
      "Epoch 479/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 213.7800 - val_loss: 1710.2770\n",
      "Epoch 480/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 213.7799 - val_loss: 1710.2773\n",
      "Epoch 481/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 213.7800 - val_loss: 1710.2773\n",
      "Epoch 482/500\n",
      "22/22 [==============================] - 1s 24ms/step - loss: 213.7800 - val_loss: 1710.2773\n",
      "Epoch 483/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 213.7800 - val_loss: 1710.2773\n",
      "Epoch 484/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 213.7800 - val_loss: 1710.2777\n",
      "Epoch 485/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 213.7801 - val_loss: 1710.2781\n",
      "Epoch 486/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 213.7800 - val_loss: 1710.2787\n",
      "Epoch 487/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 213.7801 - val_loss: 1710.2788\n",
      "Epoch 488/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 213.7800 - val_loss: 1710.2795\n",
      "Epoch 489/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 213.7800 - val_loss: 1710.2798\n",
      "Epoch 490/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 213.7800 - val_loss: 1710.2798\n",
      "Epoch 491/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 213.7801 - val_loss: 1710.2803\n",
      "Epoch 492/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 213.7801 - val_loss: 1710.2809\n",
      "Epoch 493/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 213.7801 - val_loss: 1710.2812\n",
      "Epoch 494/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 213.7801 - val_loss: 1710.2820\n",
      "Epoch 495/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 213.7801 - val_loss: 1710.2820\n",
      "Epoch 496/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 213.7801 - val_loss: 1710.2822\n",
      "Epoch 497/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 213.7801 - val_loss: 1710.2823\n",
      "Epoch 498/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 213.7802 - val_loss: 1710.2827\n",
      "Epoch 499/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 213.7802 - val_loss: 1710.2831\n",
      "Epoch 500/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 213.7802 - val_loss: 1710.2841\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(85.7336, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0020, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0125, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3008, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 672ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.77371499e+01, 6.73211835e+01, 6.69052171e+01, 6.65998716e+01,\n",
       "        6.63603758e+01, 6.61208800e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.60244560e+01, 9.38421000e-02, 0.00000000e+00, 1.24350600e-01,\n",
       "        2.88643700e-01, 6.77987745e+01, 6.73828081e+01, 6.69668417e+01,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.89225023e+01, 6.85191410e+01,\n",
       "        6.81068978e+01, 6.76909314e+01, 6.72749650e+01, 6.68589986e+01,\n",
       "        6.65732610e+01, 6.63337652e+01, 6.60942694e+01, 6.57682500e-01,\n",
       "        0.00000000e+00, 6.74444328e+01, 6.70284664e+01, 6.66708333e+01,\n",
       "        6.64313375e+01, 6.61918417e+01, 6.59523459e+01, 6.57174370e+01,\n",
       "        6.54885462e+01, 6.52586555e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.72012482e+01, 3.52068000e-01, 5.54408100e-01, 5.40280900e-01,\n",
       "        0.00000000e+00, 1.18361350e+00, 0.00000000e+00, 6.63071545e+01,\n",
       "        6.60676587e+01, 0.00000000e+00, 0.00000000e+00, 6.73982143e+01,\n",
       "        6.69822479e+01, 6.66442227e+01, 6.64047269e+01, 6.61652311e+01,\n",
       "        6.59257353e+01, 6.56922269e+01, 6.54653361e+01, 6.52384454e+01,\n",
       "        2.91093890e-01, 3.75424710e-01, 6.62628035e+01, 6.60233077e+01,\n",
       "        6.57846639e+01, 6.55577731e+01, 6.53308823e+01, 7.02306256e+01,\n",
       "        6.92511671e+01, 6.80298669e+01, 3.62565640e-01, 7.53456900e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.87698300e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.95085040e-01, 2.89124310e-01, 0.00000000e+00,\n",
       "        6.57625809e+01, 1.06855541e-01, 8.15021634e-01, 4.29208934e-01,\n",
       "        0.00000000e+00, 8.78724456e-01, 1.53553903e-01, 3.42952199e-02,\n",
       "        0.00000000e+00, 3.23673248e-01, 2.98965991e-01, 0.00000000e+00,\n",
       "        6.86178327e-01, 2.85756350e-01, 3.43016058e-01, 1.22737730e+00,\n",
       "        6.77592635e-01, 4.52215970e-01, 0.00000000e+00, 8.57866526e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58.44736626, 58.43834802, 58.42932978, 58.42031154, 58.4112933 ,\n",
       "       58.40227506, 58.39325681, 58.38423857, 58.37522033, 58.36620209,\n",
       "       58.35718385, 58.34816561, 58.33914737, 58.33012912, 58.32111088,\n",
       "       58.31209264, 58.3030744 , 58.29405616, 58.28503792, 58.27601968,\n",
       "       58.26700143, 58.25798319, 58.24896495, 58.23994671, 58.23092847,\n",
       "       58.22191023, 58.21289199, 58.20387374, 58.1948555 , 58.18583726,\n",
       "       58.17681902, 58.16780078, 58.15878254, 58.1497643 , 58.14074605,\n",
       "       58.13172781, 58.12270957, 58.11369133, 58.10467309, 58.09565485,\n",
       "       58.08663661, 58.07761836, 58.06860012, 58.05958188, 58.05056364,\n",
       "       58.0415454 , 58.03252716, 58.02350892, 58.01449067, 58.00547243,\n",
       "       57.99645419, 57.98743595, 57.97841771, 57.96939947, 57.96038123,\n",
       "       57.95136298, 57.94234474, 57.9333265 , 57.92430826, 57.91529002,\n",
       "       57.90627178, 57.89725354, 57.88823529, 57.87921705, 57.87019881,\n",
       "       57.86118057, 57.85216233, 57.84314409, 57.83412585, 57.8251076 ,\n",
       "       57.81608936, 57.80707112, 57.79805288, 57.78903464, 57.7800164 ,\n",
       "       57.77099816, 57.76197991, 57.75296167, 57.74394343, 57.73492519,\n",
       "       57.72590695, 57.71688871, 57.70787047, 57.69885222, 57.68983398,\n",
       "       57.68081574, 57.6717975 , 57.66277926, 57.65376102, 57.64474278,\n",
       "       57.63572453, 57.62670629, 57.61768805, 57.60866981, 57.59965157,\n",
       "       57.59063333, 57.58161509, 57.57259684, 57.5635786 , 57.55456036])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.59881381252737\n",
      "33.572449691637686\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
