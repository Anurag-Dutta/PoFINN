{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "1445    64.069841\n",
       "1446    64.066106\n",
       "1447    64.062372\n",
       "1448    64.058637\n",
       "1449    64.054902\n",
       "Name: C8, Length: 1450, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c8_interpolated_1350_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "1345     0.000000\n",
       "1346     0.177786\n",
       "1347     0.351550\n",
       "1348     0.596744\n",
       "1349     0.015418\n",
       "Name: C8, Length: 1350, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1350)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.987115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.774230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.561345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.348459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.200000  0.000298\n",
       "1     87.987115  0.000298\n",
       "2     87.774230  0.000297\n",
       "3     87.561345  0.000297\n",
       "4     87.348459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 85.7336  # Value for C0\n",
    "K0 = -0.0020  # Value for K0\n",
    "K1 = 0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0125    # Value for b\n",
    "c = 2.3008    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.200000    87.987115    87.774230    87.561345    87.348459   \n",
      "351    87.987115    87.774230    87.561345    87.348459    87.135574   \n",
      "352    87.774230    87.561345    87.348459    87.135574    86.922689   \n",
      "353    87.561345    87.348459    87.135574    86.922689    86.709804   \n",
      "354    87.348459    87.135574    86.922689    86.709804    86.496919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.135574    86.922689    86.709804    86.496919    86.294538  ...   \n",
      "351    86.922689    86.709804    86.496919    86.294538    86.221709  ...   \n",
      "352    86.709804    86.496919    86.294538    86.221709    86.148880  ...   \n",
      "353    86.496919    86.294538    86.221709    86.148880    86.076050  ...   \n",
      "354    86.294538    86.221709    86.148880    86.076050    86.003221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   76.094351    0.000263   76.058870    0.000263   76.023389    0.000263   \n",
      "351   76.058870    0.000263   76.023389    0.000263   75.987909    0.000262   \n",
      "352   76.023389    0.000263   75.987909    0.000262   75.952428    0.000262   \n",
      "353   75.987909    0.000262   75.952428    0.000262   75.916947    0.000262   \n",
      "354   75.952428    0.000262   75.916947    0.000262   75.881466    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   75.987909    0.000262   75.952428    0.000262  \n",
      "351   75.952428    0.000262   75.916947    0.000262  \n",
      "352   75.916947    0.000262   75.881466    0.000262  \n",
      "353   75.881466    0.000262   75.845985    0.000262  \n",
      "354   75.845985    0.000262   75.810504    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 1, 251) (900, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "12/12 [==============================] - 2s 41ms/step - loss: 5270.3105 - val_loss: 4347.1890\n",
      "Epoch 2/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 5202.5146 - val_loss: 4293.5806\n",
      "Epoch 3/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 5133.2148 - val_loss: 4229.5391\n",
      "Epoch 4/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 5076.6953 - val_loss: 4191.1748\n",
      "Epoch 5/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 5034.6846 - val_loss: 4153.0708\n",
      "Epoch 6/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4993.0908 - val_loss: 4115.4443\n",
      "Epoch 7/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4951.9995 - val_loss: 4078.2666\n",
      "Epoch 8/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4911.3613 - val_loss: 4041.4868\n",
      "Epoch 9/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4871.1216 - val_loss: 4005.0623\n",
      "Epoch 10/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4831.2412 - val_loss: 3968.9607\n",
      "Epoch 11/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4791.6895 - val_loss: 3933.1614\n",
      "Epoch 12/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4752.4453 - val_loss: 3897.6487\n",
      "Epoch 13/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4713.4946 - val_loss: 3862.4102\n",
      "Epoch 14/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4674.8262 - val_loss: 3827.4373\n",
      "Epoch 15/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4636.4316 - val_loss: 3792.7219\n",
      "Epoch 16/500\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 4598.3003 - val_loss: 3758.2590\n",
      "Epoch 17/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4560.4297 - val_loss: 3724.0437\n",
      "Epoch 18/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4522.8120 - val_loss: 3690.0715\n",
      "Epoch 19/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4485.4468 - val_loss: 3656.3396\n",
      "Epoch 20/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4448.3286 - val_loss: 3622.8435\n",
      "Epoch 21/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4411.4526 - val_loss: 3589.5808\n",
      "Epoch 22/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4374.8179 - val_loss: 3556.5491\n",
      "Epoch 23/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4338.4224 - val_loss: 3523.7461\n",
      "Epoch 24/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4302.2607 - val_loss: 3491.1694\n",
      "Epoch 25/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 4266.3345 - val_loss: 3458.8181\n",
      "Epoch 26/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4230.6382 - val_loss: 3426.6885\n",
      "Epoch 27/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4195.1714 - val_loss: 3394.7805\n",
      "Epoch 28/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4159.9331 - val_loss: 3363.0913\n",
      "Epoch 29/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4124.9209 - val_loss: 3331.6194\n",
      "Epoch 30/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4090.1333 - val_loss: 3300.3643\n",
      "Epoch 31/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4055.5681 - val_loss: 3269.3232\n",
      "Epoch 32/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4021.2246 - val_loss: 3238.4958\n",
      "Epoch 33/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3987.1003 - val_loss: 3207.8799\n",
      "Epoch 34/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3953.1958 - val_loss: 3177.4751\n",
      "Epoch 35/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3919.5076 - val_loss: 3147.2788\n",
      "Epoch 36/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3886.0361 - val_loss: 3117.2910\n",
      "Epoch 37/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3852.7788 - val_loss: 3087.5100\n",
      "Epoch 38/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3819.7346 - val_loss: 3057.9348\n",
      "Epoch 39/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3786.9038 - val_loss: 3028.5640\n",
      "Epoch 40/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3754.2827 - val_loss: 2999.3958\n",
      "Epoch 41/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3721.8718 - val_loss: 2970.4307\n",
      "Epoch 42/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3689.6702 - val_loss: 2941.6660\n",
      "Epoch 43/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3657.6750 - val_loss: 2913.1013\n",
      "Epoch 44/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3625.8875 - val_loss: 2884.7358\n",
      "Epoch 45/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3594.3052 - val_loss: 2856.5684\n",
      "Epoch 46/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3562.9270 - val_loss: 2828.5974\n",
      "Epoch 47/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3531.7517 - val_loss: 2800.8223\n",
      "Epoch 48/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3500.7791 - val_loss: 2773.2419\n",
      "Epoch 49/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3470.0076 - val_loss: 2745.8555\n",
      "Epoch 50/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3439.4365 - val_loss: 2718.6619\n",
      "Epoch 51/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3409.0642 - val_loss: 2691.6594\n",
      "Epoch 52/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3378.8899 - val_loss: 2664.8484\n",
      "Epoch 53/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3348.9136 - val_loss: 2638.2271\n",
      "Epoch 54/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3319.1326 - val_loss: 2611.7944\n",
      "Epoch 55/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3289.5471 - val_loss: 2585.5503\n",
      "Epoch 56/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3260.1565 - val_loss: 2559.4924\n",
      "Epoch 57/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3230.9587 - val_loss: 2533.6206\n",
      "Epoch 58/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3201.9534 - val_loss: 2507.9341\n",
      "Epoch 59/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3173.1396 - val_loss: 2482.4321\n",
      "Epoch 60/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3144.5164 - val_loss: 2457.1130\n",
      "Epoch 61/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3116.0823 - val_loss: 2431.9763\n",
      "Epoch 62/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3087.8379 - val_loss: 2407.0215\n",
      "Epoch 63/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3059.7803 - val_loss: 2382.2468\n",
      "Epoch 64/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3031.9102 - val_loss: 2357.6523\n",
      "Epoch 65/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3004.2256 - val_loss: 2333.2366\n",
      "Epoch 66/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2976.7263 - val_loss: 2308.9985\n",
      "Epoch 67/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2949.4114 - val_loss: 2284.9377\n",
      "Epoch 68/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2922.2798 - val_loss: 2261.0537\n",
      "Epoch 69/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2895.3308 - val_loss: 2237.3450\n",
      "Epoch 70/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2868.5635 - val_loss: 2213.8098\n",
      "Epoch 71/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2841.9768 - val_loss: 2190.4492\n",
      "Epoch 72/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2815.5701 - val_loss: 2167.2612\n",
      "Epoch 73/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2789.3423 - val_loss: 2144.2456\n",
      "Epoch 74/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2763.2932 - val_loss: 2121.4006\n",
      "Epoch 75/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2737.4216 - val_loss: 2098.7263\n",
      "Epoch 76/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2711.7263 - val_loss: 2076.2217\n",
      "Epoch 77/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2686.2065 - val_loss: 2053.8855\n",
      "Epoch 78/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2660.8618 - val_loss: 2031.7174\n",
      "Epoch 79/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2635.6914 - val_loss: 2009.7162\n",
      "Epoch 80/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2610.6943 - val_loss: 1987.8815\n",
      "Epoch 81/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2585.8694 - val_loss: 1966.2122\n",
      "Epoch 82/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2561.2166 - val_loss: 1944.7075\n",
      "Epoch 83/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2536.7346 - val_loss: 1923.3672\n",
      "Epoch 84/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2512.4221 - val_loss: 1902.1892\n",
      "Epoch 85/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2488.2795 - val_loss: 1881.1743\n",
      "Epoch 86/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2464.3054 - val_loss: 1860.3204\n",
      "Epoch 87/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2440.4988 - val_loss: 1839.6274\n",
      "Epoch 88/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2416.8596 - val_loss: 1819.0945\n",
      "Epoch 89/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2393.3860 - val_loss: 1798.7209\n",
      "Epoch 90/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2370.0779 - val_loss: 1778.5061\n",
      "Epoch 91/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2346.9346 - val_loss: 1758.4482\n",
      "Epoch 92/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2323.9551 - val_loss: 1738.5477\n",
      "Epoch 93/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2301.1389 - val_loss: 1718.8031\n",
      "Epoch 94/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2278.4846 - val_loss: 1699.2153\n",
      "Epoch 95/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2255.9924 - val_loss: 1679.7805\n",
      "Epoch 96/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2233.6609 - val_loss: 1660.5004\n",
      "Epoch 97/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 2211.4895 - val_loss: 1641.3729\n",
      "Epoch 98/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2189.4775 - val_loss: 1622.3983\n",
      "Epoch 99/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2167.6240 - val_loss: 1603.5753\n",
      "Epoch 100/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2145.9287 - val_loss: 1584.9038\n",
      "Epoch 101/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2124.3896 - val_loss: 1566.3817\n",
      "Epoch 102/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 2103.0078 - val_loss: 1548.0095\n",
      "Epoch 103/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2081.7815 - val_loss: 1529.7859\n",
      "Epoch 104/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2060.7100 - val_loss: 1511.7108\n",
      "Epoch 105/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2039.7928 - val_loss: 1493.7832\n",
      "Epoch 106/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 2019.0293 - val_loss: 1476.0013\n",
      "Epoch 107/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1998.4182 - val_loss: 1458.3656\n",
      "Epoch 108/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1977.9597 - val_loss: 1440.8756\n",
      "Epoch 109/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1957.6521 - val_loss: 1423.5297\n",
      "Epoch 110/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1937.4954 - val_loss: 1406.3280\n",
      "Epoch 111/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1917.4888 - val_loss: 1389.2684\n",
      "Epoch 112/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1897.6312 - val_loss: 1372.3527\n",
      "Epoch 113/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1877.9224 - val_loss: 1355.5779\n",
      "Epoch 114/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1858.3610 - val_loss: 1338.9435\n",
      "Epoch 115/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1838.9470 - val_loss: 1322.4501\n",
      "Epoch 116/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1819.6796 - val_loss: 1306.0963\n",
      "Epoch 117/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1800.5580 - val_loss: 1289.8807\n",
      "Epoch 118/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1781.5809 - val_loss: 1273.8038\n",
      "Epoch 119/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1762.7488 - val_loss: 1257.8640\n",
      "Epoch 120/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1744.0601 - val_loss: 1242.0613\n",
      "Epoch 121/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1725.5143 - val_loss: 1226.3951\n",
      "Epoch 122/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1707.1110 - val_loss: 1210.8640\n",
      "Epoch 123/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1688.8496 - val_loss: 1195.4681\n",
      "Epoch 124/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1670.7290 - val_loss: 1180.2057\n",
      "Epoch 125/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1652.7487 - val_loss: 1165.0776\n",
      "Epoch 126/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1634.9083 - val_loss: 1150.0817\n",
      "Epoch 127/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1617.2064 - val_loss: 1135.2177\n",
      "Epoch 128/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1599.6434 - val_loss: 1120.4860\n",
      "Epoch 129/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1582.2183 - val_loss: 1105.8849\n",
      "Epoch 130/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1564.9302 - val_loss: 1091.4139\n",
      "Epoch 131/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1547.7784 - val_loss: 1077.0720\n",
      "Epoch 132/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1530.7622 - val_loss: 1062.8586\n",
      "Epoch 133/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1513.8807 - val_loss: 1048.7742\n",
      "Epoch 134/500\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 1497.1340 - val_loss: 1034.8170\n",
      "Epoch 135/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1480.5214 - val_loss: 1020.9870\n",
      "Epoch 136/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1464.0413 - val_loss: 1007.2829\n",
      "Epoch 137/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1447.6942 - val_loss: 993.7042\n",
      "Epoch 138/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1431.4788 - val_loss: 980.2505\n",
      "Epoch 139/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1415.3944 - val_loss: 966.9214\n",
      "Epoch 140/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1399.4408 - val_loss: 953.7159\n",
      "Epoch 141/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1383.6168 - val_loss: 940.6324\n",
      "Epoch 142/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1367.9224 - val_loss: 927.6725\n",
      "Epoch 143/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1352.3566 - val_loss: 914.8335\n",
      "Epoch 144/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1336.9185 - val_loss: 902.1159\n",
      "Epoch 145/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1321.6077 - val_loss: 889.5189\n",
      "Epoch 146/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1306.4244 - val_loss: 877.0415\n",
      "Epoch 147/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1291.3666 - val_loss: 864.6835\n",
      "Epoch 148/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1276.4349 - val_loss: 852.4436\n",
      "Epoch 149/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1261.6278 - val_loss: 840.3218\n",
      "Epoch 150/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1246.9451 - val_loss: 828.3177\n",
      "Epoch 151/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1232.3860 - val_loss: 816.4301\n",
      "Epoch 152/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1217.9501 - val_loss: 804.6585\n",
      "Epoch 153/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1203.6365 - val_loss: 793.0023\n",
      "Epoch 154/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1189.4446 - val_loss: 781.4603\n",
      "Epoch 155/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1175.3739 - val_loss: 770.0339\n",
      "Epoch 156/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1161.4243 - val_loss: 758.7202\n",
      "Epoch 157/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1147.5942 - val_loss: 747.5190\n",
      "Epoch 158/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1133.8833 - val_loss: 736.4302\n",
      "Epoch 159/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1120.2913 - val_loss: 725.4537\n",
      "Epoch 160/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 1106.8176 - val_loss: 714.5886\n",
      "Epoch 161/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1093.4614 - val_loss: 703.8329\n",
      "Epoch 162/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1080.2224 - val_loss: 693.1877\n",
      "Epoch 163/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1067.0995 - val_loss: 682.6518\n",
      "Epoch 164/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1054.0920 - val_loss: 672.2248\n",
      "Epoch 165/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1041.2003 - val_loss: 661.9050\n",
      "Epoch 166/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1028.4230 - val_loss: 651.6931\n",
      "Epoch 167/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1015.7592 - val_loss: 641.5880\n",
      "Epoch 168/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1003.2093 - val_loss: 631.5893\n",
      "Epoch 169/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 990.7718 - val_loss: 621.6957\n",
      "Epoch 170/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 978.4465 - val_loss: 611.9075\n",
      "Epoch 171/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 966.2331 - val_loss: 602.2238\n",
      "Epoch 172/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 954.1304 - val_loss: 592.6443\n",
      "Epoch 173/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 942.1383 - val_loss: 583.1675\n",
      "Epoch 174/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 930.2562 - val_loss: 573.7937\n",
      "Epoch 175/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 918.4833 - val_loss: 564.5214\n",
      "Epoch 176/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 906.8189 - val_loss: 555.3511\n",
      "Epoch 177/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 895.2627 - val_loss: 546.2814\n",
      "Epoch 178/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 883.8136 - val_loss: 537.3120\n",
      "Epoch 179/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 872.4717 - val_loss: 528.4421\n",
      "Epoch 180/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 861.2360 - val_loss: 519.6713\n",
      "Epoch 181/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 850.1063 - val_loss: 510.9987\n",
      "Epoch 182/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 839.0814 - val_loss: 502.4246\n",
      "Epoch 183/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 828.1614 - val_loss: 493.9476\n",
      "Epoch 184/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 817.3456 - val_loss: 485.5675\n",
      "Epoch 185/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 806.6328 - val_loss: 477.2832\n",
      "Epoch 186/500\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 796.0228 - val_loss: 469.0944\n",
      "Epoch 187/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 785.5157 - val_loss: 461.0010\n",
      "Epoch 188/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 775.1098 - val_loss: 453.0009\n",
      "Epoch 189/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 764.8053 - val_loss: 445.0956\n",
      "Epoch 190/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 754.6014 - val_loss: 437.2837\n",
      "Epoch 191/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 744.4974 - val_loss: 429.5634\n",
      "Epoch 192/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 734.4928 - val_loss: 421.9359\n",
      "Epoch 193/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 724.5872 - val_loss: 414.3994\n",
      "Epoch 194/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 714.7799 - val_loss: 406.9536\n",
      "Epoch 195/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 705.0704 - val_loss: 399.5988\n",
      "Epoch 196/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 695.4579 - val_loss: 392.3333\n",
      "Epoch 197/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 685.9423 - val_loss: 385.1572\n",
      "Epoch 198/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 676.5226 - val_loss: 378.0695\n",
      "Epoch 199/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 667.1984 - val_loss: 371.0695\n",
      "Epoch 200/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 657.9693 - val_loss: 364.1572\n",
      "Epoch 201/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 648.8342 - val_loss: 357.3319\n",
      "Epoch 202/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 639.7933 - val_loss: 350.5928\n",
      "Epoch 203/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 630.8454 - val_loss: 343.9392\n",
      "Epoch 204/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 621.9903 - val_loss: 337.3704\n",
      "Epoch 205/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 613.2271 - val_loss: 330.8866\n",
      "Epoch 206/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 604.5557 - val_loss: 324.4863\n",
      "Epoch 207/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 595.9751 - val_loss: 318.1699\n",
      "Epoch 208/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 587.4851 - val_loss: 311.9358\n",
      "Epoch 209/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 579.0849 - val_loss: 305.7842\n",
      "Epoch 210/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 570.7743 - val_loss: 299.7146\n",
      "Epoch 211/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 562.5523 - val_loss: 293.7256\n",
      "Epoch 212/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 554.4183 - val_loss: 287.8174\n",
      "Epoch 213/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 546.3724 - val_loss: 281.9890\n",
      "Epoch 214/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 538.4135 - val_loss: 276.2399\n",
      "Epoch 215/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 530.5413 - val_loss: 270.5700\n",
      "Epoch 216/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 522.7551 - val_loss: 264.9781\n",
      "Epoch 217/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 515.0543 - val_loss: 259.4637\n",
      "Epoch 218/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 507.4385 - val_loss: 254.0272\n",
      "Epoch 219/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 499.9073 - val_loss: 248.6662\n",
      "Epoch 220/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 492.4596 - val_loss: 243.3818\n",
      "Epoch 221/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 485.0952 - val_loss: 238.1729\n",
      "Epoch 222/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 477.8136 - val_loss: 233.0385\n",
      "Epoch 223/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 470.6143 - val_loss: 227.9787\n",
      "Epoch 224/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 463.4967 - val_loss: 222.9925\n",
      "Epoch 225/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 456.4602 - val_loss: 218.0795\n",
      "Epoch 226/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 449.5043 - val_loss: 213.2393\n",
      "Epoch 227/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 442.6281 - val_loss: 208.4707\n",
      "Epoch 228/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 435.8317 - val_loss: 203.7740\n",
      "Epoch 229/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 429.1141 - val_loss: 199.1481\n",
      "Epoch 230/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 422.4751 - val_loss: 194.5928\n",
      "Epoch 231/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 415.9138 - val_loss: 190.1071\n",
      "Epoch 232/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 409.4297 - val_loss: 185.6905\n",
      "Epoch 233/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 403.0226 - val_loss: 181.3426\n",
      "Epoch 234/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 396.6916 - val_loss: 177.0632\n",
      "Epoch 235/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 390.4366 - val_loss: 172.8515\n",
      "Epoch 236/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 384.2567 - val_loss: 168.7067\n",
      "Epoch 237/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 378.1516 - val_loss: 164.6283\n",
      "Epoch 238/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 372.1203 - val_loss: 160.6155\n",
      "Epoch 239/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 366.1622 - val_loss: 156.6683\n",
      "Epoch 240/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 360.2773 - val_loss: 152.7855\n",
      "Epoch 241/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 354.4649 - val_loss: 148.9675\n",
      "Epoch 242/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 348.7247 - val_loss: 145.2129\n",
      "Epoch 243/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 343.0559 - val_loss: 141.5217\n",
      "Epoch 244/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 337.4579 - val_loss: 137.8929\n",
      "Epoch 245/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 331.9302 - val_loss: 134.3264\n",
      "Epoch 246/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 326.4723 - val_loss: 130.8209\n",
      "Epoch 247/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 321.0836 - val_loss: 127.3767\n",
      "Epoch 248/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 315.7638 - val_loss: 123.9927\n",
      "Epoch 249/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 310.5120 - val_loss: 120.6684\n",
      "Epoch 250/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 305.3279 - val_loss: 117.4039\n",
      "Epoch 251/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 300.2113 - val_loss: 114.1977\n",
      "Epoch 252/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 295.1609 - val_loss: 111.0499\n",
      "Epoch 253/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 290.1771 - val_loss: 107.9596\n",
      "Epoch 254/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 285.2585 - val_loss: 104.9267\n",
      "Epoch 255/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 280.4053 - val_loss: 101.9502\n",
      "Epoch 256/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 275.6165 - val_loss: 99.0298\n",
      "Epoch 257/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 270.8918 - val_loss: 96.1651\n",
      "Epoch 258/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 266.2307 - val_loss: 93.3547\n",
      "Epoch 259/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 261.6323 - val_loss: 90.5993\n",
      "Epoch 260/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 257.0966 - val_loss: 87.8974\n",
      "Epoch 261/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 252.6226 - val_loss: 85.2488\n",
      "Epoch 262/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 248.2100 - val_loss: 82.6533\n",
      "Epoch 263/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 243.8584 - val_loss: 80.1096\n",
      "Epoch 264/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 239.5671 - val_loss: 77.6178\n",
      "Epoch 265/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 235.3356 - val_loss: 75.1772\n",
      "Epoch 266/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 231.1637 - val_loss: 72.7873\n",
      "Epoch 267/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 227.0503 - val_loss: 70.4473\n",
      "Epoch 268/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 222.9952 - val_loss: 68.1568\n",
      "Epoch 269/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 218.9977 - val_loss: 65.9153\n",
      "Epoch 270/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 215.0577 - val_loss: 63.7224\n",
      "Epoch 271/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 211.1746 - val_loss: 61.5773\n",
      "Epoch 272/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 207.3474 - val_loss: 59.4796\n",
      "Epoch 273/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 203.5762 - val_loss: 57.4291\n",
      "Epoch 274/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 199.8603 - val_loss: 55.4249\n",
      "Epoch 275/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 196.1990 - val_loss: 53.4664\n",
      "Epoch 276/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 192.5919 - val_loss: 51.5533\n",
      "Epoch 277/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 189.0385 - val_loss: 49.6850\n",
      "Epoch 278/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 185.5384 - val_loss: 47.8608\n",
      "Epoch 279/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 182.0909 - val_loss: 46.0803\n",
      "Epoch 280/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 178.6953 - val_loss: 44.3431\n",
      "Epoch 281/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 175.3516 - val_loss: 42.6487\n",
      "Epoch 282/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 172.0591 - val_loss: 40.9964\n",
      "Epoch 283/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 168.8172 - val_loss: 39.3857\n",
      "Epoch 284/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 165.6254 - val_loss: 37.8163\n",
      "Epoch 285/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 162.4834 - val_loss: 36.2874\n",
      "Epoch 286/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 159.3907 - val_loss: 34.7988\n",
      "Epoch 287/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 156.3466 - val_loss: 33.3497\n",
      "Epoch 288/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 153.3507 - val_loss: 31.9396\n",
      "Epoch 289/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 150.4025 - val_loss: 30.5680\n",
      "Epoch 290/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 147.5014 - val_loss: 29.2346\n",
      "Epoch 291/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 144.6470 - val_loss: 27.9388\n",
      "Epoch 292/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 141.8390 - val_loss: 26.6800\n",
      "Epoch 293/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 139.0767 - val_loss: 25.4579\n",
      "Epoch 294/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 136.3597 - val_loss: 24.2718\n",
      "Epoch 295/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 133.6874 - val_loss: 23.1212\n",
      "Epoch 296/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 131.0594 - val_loss: 22.0057\n",
      "Epoch 297/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 128.4752 - val_loss: 20.9249\n",
      "Epoch 298/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 125.9343 - val_loss: 19.8779\n",
      "Epoch 299/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 123.4362 - val_loss: 18.8646\n",
      "Epoch 300/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 120.9806 - val_loss: 17.8843\n",
      "Epoch 301/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 118.5668 - val_loss: 16.9368\n",
      "Epoch 302/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 116.1943 - val_loss: 16.0212\n",
      "Epoch 303/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 113.8627 - val_loss: 15.1372\n",
      "Epoch 304/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 111.5717 - val_loss: 14.2846\n",
      "Epoch 305/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 109.3208 - val_loss: 13.4625\n",
      "Epoch 306/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 107.1093 - val_loss: 12.6705\n",
      "Epoch 307/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 104.9369 - val_loss: 11.9083\n",
      "Epoch 308/500\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 102.8030 - val_loss: 11.1753\n",
      "Epoch 309/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 100.7074 - val_loss: 10.4709\n",
      "Epoch 310/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 98.6492 - val_loss: 9.7950\n",
      "Epoch 311/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 96.6283 - val_loss: 9.1469\n",
      "Epoch 312/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 94.6443 - val_loss: 8.5261\n",
      "Epoch 313/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 92.6965 - val_loss: 7.9321\n",
      "Epoch 314/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 90.7846 - val_loss: 7.3646\n",
      "Epoch 315/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 88.9081 - val_loss: 6.8231\n",
      "Epoch 316/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 87.0665 - val_loss: 6.3071\n",
      "Epoch 317/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 85.2594 - val_loss: 5.8161\n",
      "Epoch 318/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 83.4864 - val_loss: 5.3497\n",
      "Epoch 319/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 81.7470 - val_loss: 4.9074\n",
      "Epoch 320/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 80.0408 - val_loss: 4.4889\n",
      "Epoch 321/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 78.3673 - val_loss: 4.0937\n",
      "Epoch 322/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 76.7261 - val_loss: 3.7212\n",
      "Epoch 323/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 75.1168 - val_loss: 3.3712\n",
      "Epoch 324/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 73.5389 - val_loss: 3.0431\n",
      "Epoch 325/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 71.9921 - val_loss: 2.7365\n",
      "Epoch 326/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 70.4757 - val_loss: 2.4510\n",
      "Epoch 327/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 68.9896 - val_loss: 2.1861\n",
      "Epoch 328/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 67.5332 - val_loss: 1.9415\n",
      "Epoch 329/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 66.1062 - val_loss: 1.7167\n",
      "Epoch 330/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 64.7079 - val_loss: 1.5113\n",
      "Epoch 331/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 63.3382 - val_loss: 1.3248\n",
      "Epoch 332/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 61.9966 - val_loss: 1.1569\n",
      "Epoch 333/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 60.6825 - val_loss: 1.0072\n",
      "Epoch 334/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 59.3958 - val_loss: 0.8753\n",
      "Epoch 335/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 58.1360 - val_loss: 0.7606\n",
      "Epoch 336/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 56.9024 - val_loss: 0.6630\n",
      "Epoch 337/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 55.6951 - val_loss: 0.5819\n",
      "Epoch 338/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 54.5134 - val_loss: 0.5169\n",
      "Epoch 339/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 53.3569 - val_loss: 0.4677\n",
      "Epoch 340/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 52.2254 - val_loss: 0.4339\n",
      "Epoch 341/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 51.1183 - val_loss: 0.4151\n",
      "Epoch 342/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 50.0352 - val_loss: 0.4110\n",
      "Epoch 343/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 48.9760 - val_loss: 0.4210\n",
      "Epoch 344/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 47.9400 - val_loss: 0.4450\n",
      "Epoch 345/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 46.9269 - val_loss: 0.4825\n",
      "Epoch 346/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 45.9366 - val_loss: 0.5330\n",
      "Epoch 347/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 44.9684 - val_loss: 0.5964\n",
      "Epoch 348/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 44.0220 - val_loss: 0.6722\n",
      "Epoch 349/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 43.0971 - val_loss: 0.7601\n",
      "Epoch 350/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 42.1933 - val_loss: 0.8597\n",
      "Epoch 351/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 41.3103 - val_loss: 0.9707\n",
      "Epoch 352/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 40.4476 - val_loss: 1.0927\n",
      "Epoch 353/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 39.6051 - val_loss: 1.2254\n",
      "Epoch 354/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 38.7824 - val_loss: 1.3685\n",
      "Epoch 355/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 37.9790 - val_loss: 1.5216\n",
      "Epoch 356/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 37.1945 - val_loss: 1.6844\n",
      "Epoch 357/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 36.4287 - val_loss: 1.8565\n",
      "Epoch 358/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 35.6813 - val_loss: 2.0378\n",
      "Epoch 359/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 34.9520 - val_loss: 2.2277\n",
      "Epoch 360/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 34.2403 - val_loss: 2.4261\n",
      "Epoch 361/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 33.5460 - val_loss: 2.6327\n",
      "Epoch 362/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 32.8688 - val_loss: 2.8470\n",
      "Epoch 363/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 32.2081 - val_loss: 3.0689\n",
      "Epoch 364/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 31.5640 - val_loss: 3.2980\n",
      "Epoch 365/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 30.9359 - val_loss: 3.5341\n",
      "Epoch 366/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 30.3236 - val_loss: 3.7768\n",
      "Epoch 367/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 29.7268 - val_loss: 4.0259\n",
      "Epoch 368/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 29.1452 - val_loss: 4.2811\n",
      "Epoch 369/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 28.5784 - val_loss: 4.5421\n",
      "Epoch 370/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 28.0262 - val_loss: 4.8086\n",
      "Epoch 371/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 27.4883 - val_loss: 5.0805\n",
      "Epoch 372/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 26.9644 - val_loss: 5.3573\n",
      "Epoch 373/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 26.4543 - val_loss: 5.6390\n",
      "Epoch 374/500\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 25.9575 - val_loss: 5.9252\n",
      "Epoch 375/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 25.4739 - val_loss: 6.2156\n",
      "Epoch 376/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 25.0032 - val_loss: 6.5100\n",
      "Epoch 377/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 24.5451 - val_loss: 6.8084\n",
      "Epoch 378/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 24.0993 - val_loss: 7.1101\n",
      "Epoch 379/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 23.6657 - val_loss: 7.4152\n",
      "Epoch 380/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 23.2439 - val_loss: 7.7234\n",
      "Epoch 381/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 22.8336 - val_loss: 8.0344\n",
      "Epoch 382/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 22.4347 - val_loss: 8.3482\n",
      "Epoch 383/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 22.0468 - val_loss: 8.6644\n",
      "Epoch 384/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 21.6698 - val_loss: 8.9826\n",
      "Epoch 385/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 21.3034 - val_loss: 9.3031\n",
      "Epoch 386/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 20.9472 - val_loss: 9.6253\n",
      "Epoch 387/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 20.6012 - val_loss: 9.9492\n",
      "Epoch 388/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 20.2651 - val_loss: 10.2744\n",
      "Epoch 389/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 19.9386 - val_loss: 10.6010\n",
      "Epoch 390/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 19.6216 - val_loss: 10.9285\n",
      "Epoch 391/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 19.3137 - val_loss: 11.2570\n",
      "Epoch 392/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 19.0149 - val_loss: 11.5861\n",
      "Epoch 393/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 18.7247 - val_loss: 11.9158\n",
      "Epoch 394/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 18.4432 - val_loss: 12.2459\n",
      "Epoch 395/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 18.1699 - val_loss: 12.5762\n",
      "Epoch 396/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 17.9049 - val_loss: 12.9064\n",
      "Epoch 397/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 17.6478 - val_loss: 13.2367\n",
      "Epoch 398/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.3984 - val_loss: 13.5666\n",
      "Epoch 399/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.1567 - val_loss: 13.8961\n",
      "Epoch 400/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.9223 - val_loss: 14.2251\n",
      "Epoch 401/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.6950 - val_loss: 14.5533\n",
      "Epoch 402/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 16.4748 - val_loss: 14.8809\n",
      "Epoch 403/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 16.2614 - val_loss: 15.2073\n",
      "Epoch 404/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 16.0546 - val_loss: 15.5327\n",
      "Epoch 405/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 15.8543 - val_loss: 15.8569\n",
      "Epoch 406/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 15.6603 - val_loss: 16.1798\n",
      "Epoch 407/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 15.4723 - val_loss: 16.5012\n",
      "Epoch 408/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 15.2904 - val_loss: 16.8212\n",
      "Epoch 409/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 15.1143 - val_loss: 17.1395\n",
      "Epoch 410/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.9439 - val_loss: 17.4559\n",
      "Epoch 411/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.7789 - val_loss: 17.7707\n",
      "Epoch 412/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 14.6193 - val_loss: 18.0833\n",
      "Epoch 413/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 14.4648 - val_loss: 18.3939\n",
      "Epoch 414/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 14.3155 - val_loss: 18.7024\n",
      "Epoch 415/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 14.1710 - val_loss: 19.0087\n",
      "Epoch 416/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14.0313 - val_loss: 19.3127\n",
      "Epoch 417/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 13.8962 - val_loss: 19.6143\n",
      "Epoch 418/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 13.7657 - val_loss: 19.9133\n",
      "Epoch 419/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 13.6395 - val_loss: 20.2099\n",
      "Epoch 420/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 13.5176 - val_loss: 20.5038\n",
      "Epoch 421/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 13.3998 - val_loss: 20.7951\n",
      "Epoch 422/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 13.2860 - val_loss: 21.0838\n",
      "Epoch 423/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 13.1761 - val_loss: 21.3697\n",
      "Epoch 424/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 13.0699 - val_loss: 21.6526\n",
      "Epoch 425/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 12.9674 - val_loss: 21.9327\n",
      "Epoch 426/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 12.8684 - val_loss: 22.2099\n",
      "Epoch 427/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 12.7729 - val_loss: 22.4840\n",
      "Epoch 428/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 12.6807 - val_loss: 22.7550\n",
      "Epoch 429/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 12.5917 - val_loss: 23.0232\n",
      "Epoch 430/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 12.5059 - val_loss: 23.2882\n",
      "Epoch 431/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 12.4230 - val_loss: 23.5500\n",
      "Epoch 432/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 12.3431 - val_loss: 23.8085\n",
      "Epoch 433/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 12.2661 - val_loss: 24.0640\n",
      "Epoch 434/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 12.1918 - val_loss: 24.3163\n",
      "Epoch 435/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 12.1201 - val_loss: 24.5653\n",
      "Epoch 436/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 12.0511 - val_loss: 24.8110\n",
      "Epoch 437/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 11.9845 - val_loss: 25.0532\n",
      "Epoch 438/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 11.9203 - val_loss: 25.2923\n",
      "Epoch 439/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 11.8585 - val_loss: 25.5278\n",
      "Epoch 440/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 11.7990 - val_loss: 25.7602\n",
      "Epoch 441/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 11.7416 - val_loss: 25.9892\n",
      "Epoch 442/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 11.6863 - val_loss: 26.2148\n",
      "Epoch 443/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 11.6331 - val_loss: 26.4370\n",
      "Epoch 444/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 11.5818 - val_loss: 26.6558\n",
      "Epoch 445/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 11.5325 - val_loss: 26.8714\n",
      "Epoch 446/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 11.4849 - val_loss: 27.0834\n",
      "Epoch 447/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 11.4392 - val_loss: 27.2923\n",
      "Epoch 448/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 11.3951 - val_loss: 27.4976\n",
      "Epoch 449/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 11.3527 - val_loss: 27.6996\n",
      "Epoch 450/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 11.3119 - val_loss: 27.8986\n",
      "Epoch 451/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 11.2726 - val_loss: 28.0938\n",
      "Epoch 452/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 11.2349 - val_loss: 28.2858\n",
      "Epoch 453/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 11.1985 - val_loss: 28.4744\n",
      "Epoch 454/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 11.1636 - val_loss: 28.6598\n",
      "Epoch 455/500\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 11.1299 - val_loss: 28.8418\n",
      "Epoch 456/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 11.0977 - val_loss: 29.0206\n",
      "Epoch 457/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 11.0665 - val_loss: 29.1963\n",
      "Epoch 458/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 11.0366 - val_loss: 29.3687\n",
      "Epoch 459/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 11.0078 - val_loss: 29.5375\n",
      "Epoch 460/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 10.9802 - val_loss: 29.7034\n",
      "Epoch 461/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 10.9536 - val_loss: 29.8660\n",
      "Epoch 462/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 10.9281 - val_loss: 30.0257\n",
      "Epoch 463/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 10.9035 - val_loss: 30.1820\n",
      "Epoch 464/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 10.8799 - val_loss: 30.3354\n",
      "Epoch 465/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 10.8573 - val_loss: 30.4857\n",
      "Epoch 466/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 10.8354 - val_loss: 30.6328\n",
      "Epoch 467/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 10.8145 - val_loss: 30.7770\n",
      "Epoch 468/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 10.7943 - val_loss: 30.9186\n",
      "Epoch 469/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 10.7750 - val_loss: 31.0567\n",
      "Epoch 470/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 10.7564 - val_loss: 31.1920\n",
      "Epoch 471/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 10.7385 - val_loss: 31.3245\n",
      "Epoch 472/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 10.7214 - val_loss: 31.4542\n",
      "Epoch 473/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 10.7049 - val_loss: 31.5807\n",
      "Epoch 474/500\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 10.6890 - val_loss: 31.7048\n",
      "Epoch 475/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 10.6738 - val_loss: 31.8260\n",
      "Epoch 476/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 10.6593 - val_loss: 31.9444\n",
      "Epoch 477/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 10.6452 - val_loss: 32.0602\n",
      "Epoch 478/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 10.6317 - val_loss: 32.1735\n",
      "Epoch 479/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 10.6188 - val_loss: 32.2841\n",
      "Epoch 480/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 10.6063 - val_loss: 32.3920\n",
      "Epoch 481/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 10.5944 - val_loss: 32.4975\n",
      "Epoch 482/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 10.5829 - val_loss: 32.6004\n",
      "Epoch 483/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 10.5719 - val_loss: 32.7008\n",
      "Epoch 484/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 10.5613 - val_loss: 32.7988\n",
      "Epoch 485/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 10.5512 - val_loss: 32.8946\n",
      "Epoch 486/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 10.5414 - val_loss: 32.9880\n",
      "Epoch 487/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 10.5320 - val_loss: 33.0790\n",
      "Epoch 488/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 10.5230 - val_loss: 33.1678\n",
      "Epoch 489/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 10.5144 - val_loss: 33.2545\n",
      "Epoch 490/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 10.5060 - val_loss: 33.3388\n",
      "Epoch 491/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 10.4981 - val_loss: 33.4208\n",
      "Epoch 492/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 10.4904 - val_loss: 33.5008\n",
      "Epoch 493/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 10.4831 - val_loss: 33.5785\n",
      "Epoch 494/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 10.4761 - val_loss: 33.6545\n",
      "Epoch 495/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 10.4693 - val_loss: 33.7286\n",
      "Epoch 496/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 10.4627 - val_loss: 33.8005\n",
      "Epoch 497/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 10.4565 - val_loss: 33.8702\n",
      "Epoch 498/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 10.4505 - val_loss: 33.9381\n",
      "Epoch 499/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 10.4447 - val_loss: 34.0044\n",
      "Epoch 500/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 10.4392 - val_loss: 34.0688\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(85.7336, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0020, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0125, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3008, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 536ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.90569561e+01, 6.90121382e+01, 6.89673203e+01, 6.89225023e+01,\n",
       "        6.88776844e+01, 6.88328665e+01, 6.87880485e+01, 6.87432306e+01,\n",
       "        6.86984127e+01, 6.86535948e+01, 6.86087768e+01, 6.85639589e+01,\n",
       "        6.85191410e+01, 6.84743231e+01, 6.84295051e+01, 6.83842087e+01,\n",
       "        6.83379902e+01, 6.82917717e+01, 6.82455532e+01, 6.81993347e+01,\n",
       "        6.81531162e+01, 6.81068978e+01, 6.80606793e+01, 6.80144608e+01,\n",
       "        6.79682423e+01, 6.79220238e+01, 6.78758053e+01, 6.78295868e+01,\n",
       "        6.77833684e+01, 6.77371499e+01, 6.76909314e+01, 6.76447129e+01,\n",
       "        6.75984944e+01, 6.75522759e+01, 6.75060574e+01, 6.74598389e+01,\n",
       "        6.74136204e+01, 6.73674020e+01, 6.73211835e+01, 6.72749650e+01,\n",
       "        6.72287465e+01, 6.71825280e+01, 6.71363095e+01, 6.70900910e+01,\n",
       "        6.70438726e+01, 6.69976541e+01, 6.69514356e+01, 6.69052171e+01,\n",
       "        6.68589986e+01, 6.68127801e+01, 6.67665616e+01, 6.67329248e+01,\n",
       "        6.67063142e+01, 6.66797035e+01, 6.66530929e+01, 6.66264823e+01,\n",
       "        6.65998716e+01, 6.65732610e+01, 6.65466503e+01, 6.65200397e+01,\n",
       "        6.64934290e+01, 6.64668184e+01, 6.64402077e+01, 6.64135971e+01,\n",
       "        6.63869865e+01, 6.63603758e+01, 6.63337652e+01, 6.63071545e+01,\n",
       "        6.62805439e+01, 6.62539332e+01, 6.62273226e+01, 6.62007119e+01,\n",
       "        6.61741013e+01, 6.61474907e+01, 6.61208800e+01, 6.60942694e+01,\n",
       "        6.60676587e+01, 6.60410481e+01, 6.60144374e+01, 6.59878268e+01,\n",
       "        7.27177811e+01, 0.00000000e+00, 7.22647607e-01, 0.00000000e+00,\n",
       "        6.57682478e-01, 0.00000000e+00, 7.32384622e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.74972641e-01,\n",
       "        6.75791919e-01, 8.07057545e-02, 2.59407640e-01, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([64.48697479, 64.48137255, 64.47577031, 64.47016807, 64.46456583,\n",
       "       64.45896359, 64.45336134, 64.4477591 , 64.44215686, 64.43655462,\n",
       "       64.43095238, 64.42535014, 64.4197479 , 64.41414566, 64.40854342,\n",
       "       64.40294118, 64.39733894, 64.39173669, 64.38613445, 64.38053221,\n",
       "       64.37492997, 64.36932773, 64.36372549, 64.35812325, 64.35252101,\n",
       "       64.34691877, 64.34131653, 64.33571429, 64.33011204, 64.3245098 ,\n",
       "       64.31890756, 64.31330532, 64.30770308, 64.30210084, 64.29766573,\n",
       "       64.29393091, 64.29019608, 64.28646125, 64.28272642, 64.2789916 ,\n",
       "       64.27525677, 64.27152194, 64.26778711, 64.26405229, 64.26031746,\n",
       "       64.25658263, 64.25284781, 64.24911298, 64.24537815, 64.24164332,\n",
       "       64.2379085 , 64.23417367, 64.23043884, 64.22670401, 64.22296919,\n",
       "       64.21923436, 64.21549953, 64.21176471, 64.20802988, 64.20429505,\n",
       "       64.20056022, 64.1968254 , 64.19309057, 64.18935574, 64.18562092,\n",
       "       64.18188609, 64.17815126, 64.17441643, 64.17068161, 64.16694678,\n",
       "       64.16321195, 64.15947712, 64.1557423 , 64.15200747, 64.14827264,\n",
       "       64.14453782, 64.14080299, 64.13706816, 64.13333333, 64.12959851,\n",
       "       64.12586368, 64.12212885, 64.11839402, 64.1146592 , 64.11092437,\n",
       "       64.10718954, 64.10345472, 64.09971989, 64.09598506, 64.09225023,\n",
       "       64.08851541, 64.08478058, 64.08104575, 64.07731092, 64.0735761 ,\n",
       "       64.06984127, 64.06610644, 64.06237162, 64.05863679, 64.05490196])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.029521653313942\n",
      "14.685736301018565\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
