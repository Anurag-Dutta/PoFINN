{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "1945    59.565838\n",
       "1946    59.560703\n",
       "1947    59.555567\n",
       "1948    59.550432\n",
       "1949    59.545296\n",
       "Name: C8, Length: 1950, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c8_interpolated_1850_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "1845     0.000000\n",
       "1846     1.183614\n",
       "1847     0.708772\n",
       "1848     0.000000\n",
       "1849     0.000000\n",
       "Name: C8, Length: 1850, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1850)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.987115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.774230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.561345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.348459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.200000  0.000298\n",
       "1     87.987115  0.000298\n",
       "2     87.774230  0.000297\n",
       "3     87.561345  0.000297\n",
       "4     87.348459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 85.7336  # Value for C0\n",
    "K0 = -0.0020  # Value for K0\n",
    "K1 = 0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0125    # Value for b\n",
    "c = 2.3008    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.200000    87.987115    87.774230    87.561345    87.348459   \n",
      "351    87.987115    87.774230    87.561345    87.348459    87.135574   \n",
      "352    87.774230    87.561345    87.348459    87.135574    86.922689   \n",
      "353    87.561345    87.348459    87.135574    86.922689    86.709804   \n",
      "354    87.348459    87.135574    86.922689    86.709804    86.496919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.135574    86.922689    86.709804    86.496919    86.294538  ...   \n",
      "351    86.922689    86.709804    86.496919    86.294538    86.221709  ...   \n",
      "352    86.709804    86.496919    86.294538    86.221709    86.148880  ...   \n",
      "353    86.496919    86.294538    86.221709    86.148880    86.076050  ...   \n",
      "354    86.294538    86.221709    86.148880    86.076050    86.003221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   76.094351    0.000263   76.058870    0.000263   76.023389    0.000263   \n",
      "351   76.058870    0.000263   76.023389    0.000263   75.987909    0.000262   \n",
      "352   76.023389    0.000263   75.987909    0.000262   75.952428    0.000262   \n",
      "353   75.987909    0.000262   75.952428    0.000262   75.916947    0.000262   \n",
      "354   75.952428    0.000262   75.916947    0.000262   75.881466    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   75.987909    0.000262   75.952428    0.000262  \n",
      "351   75.952428    0.000262   75.916947    0.000262  \n",
      "352   75.916947    0.000262   75.881466    0.000262  \n",
      "353   75.881466    0.000262   75.845985    0.000262  \n",
      "354   75.845985    0.000262   75.810504    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 1, 251) (1400, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "18/18 [==============================] - 3s 31ms/step - loss: 4850.7417 - val_loss: 3325.9053\n",
      "Epoch 2/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 4737.5972 - val_loss: 3248.0588\n",
      "Epoch 3/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4660.4561 - val_loss: 3198.0745\n",
      "Epoch 4/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4620.0078 - val_loss: 3167.6675\n",
      "Epoch 5/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4558.8994 - val_loss: 3125.1348\n",
      "Epoch 6/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4498.8477 - val_loss: 3083.5413\n",
      "Epoch 7/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4439.8857 - val_loss: 3042.7380\n",
      "Epoch 8/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 4381.8418 - val_loss: 3002.6228\n",
      "Epoch 9/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 4324.6021 - val_loss: 2963.1267\n",
      "Epoch 10/500\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 4268.0947 - val_loss: 2924.1804\n",
      "Epoch 11/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 4212.2690 - val_loss: 2885.8223\n",
      "Epoch 12/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4157.0962 - val_loss: 2847.9146\n",
      "Epoch 13/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4102.5498 - val_loss: 2810.5559\n",
      "Epoch 14/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 4048.6104 - val_loss: 2773.7126\n",
      "Epoch 15/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 3995.2620 - val_loss: 2737.3611\n",
      "Epoch 16/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 3942.4934 - val_loss: 2701.4893\n",
      "Epoch 17/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3890.2930 - val_loss: 2666.0894\n",
      "Epoch 18/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3838.6519 - val_loss: 2631.1536\n",
      "Epoch 19/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3787.5613 - val_loss: 2596.6758\n",
      "Epoch 20/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3737.0125 - val_loss: 2562.6494\n",
      "Epoch 21/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 3687.0007 - val_loss: 2529.0693\n",
      "Epoch 22/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3637.5183 - val_loss: 2495.9292\n",
      "Epoch 23/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3588.5593 - val_loss: 2463.2244\n",
      "Epoch 24/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3540.1179 - val_loss: 2430.9487\n",
      "Epoch 25/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3492.1887 - val_loss: 2399.0962\n",
      "Epoch 26/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3444.7676 - val_loss: 2367.6575\n",
      "Epoch 27/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3397.8481 - val_loss: 2336.6128\n",
      "Epoch 28/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3351.4260 - val_loss: 2305.7168\n",
      "Epoch 29/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3298.3103 - val_loss: 2264.0261\n",
      "Epoch 30/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3239.6431 - val_loss: 2230.1809\n",
      "Epoch 31/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3189.2695 - val_loss: 2198.1575\n",
      "Epoch 32/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3140.4631 - val_loss: 2166.7668\n",
      "Epoch 33/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3092.8889 - val_loss: 2136.9365\n",
      "Epoch 34/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3046.5317 - val_loss: 2106.1218\n",
      "Epoch 35/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3000.5466 - val_loss: 2076.7808\n",
      "Epoch 36/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 2955.5442 - val_loss: 2048.0000\n",
      "Epoch 37/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2911.2314 - val_loss: 2019.7451\n",
      "Epoch 38/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2867.5640 - val_loss: 1991.9913\n",
      "Epoch 39/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2824.5105 - val_loss: 1964.7183\n",
      "Epoch 40/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2782.0449 - val_loss: 1937.9100\n",
      "Epoch 41/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2740.1472 - val_loss: 1911.5533\n",
      "Epoch 42/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2698.8000 - val_loss: 1885.6368\n",
      "Epoch 43/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2657.9897 - val_loss: 1860.1501\n",
      "Epoch 44/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2617.7026 - val_loss: 1835.0847\n",
      "Epoch 45/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2577.9297 - val_loss: 1810.4331\n",
      "Epoch 46/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2538.6592 - val_loss: 1786.1880\n",
      "Epoch 47/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2499.8831 - val_loss: 1762.3418\n",
      "Epoch 48/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2461.5930 - val_loss: 1738.8896\n",
      "Epoch 49/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2423.7815 - val_loss: 1715.8254\n",
      "Epoch 50/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2386.4421 - val_loss: 1693.1433\n",
      "Epoch 51/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2349.5676 - val_loss: 1670.8387\n",
      "Epoch 52/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2313.1521 - val_loss: 1648.9064\n",
      "Epoch 53/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2277.1909 - val_loss: 1627.3417\n",
      "Epoch 54/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2241.6763 - val_loss: 1606.1401\n",
      "Epoch 55/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2206.6050 - val_loss: 1585.2980\n",
      "Epoch 56/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2171.9705 - val_loss: 1564.8103\n",
      "Epoch 57/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2137.7698 - val_loss: 1544.6732\n",
      "Epoch 58/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2103.9966 - val_loss: 1524.8828\n",
      "Epoch 59/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2070.6462 - val_loss: 1505.4355\n",
      "Epoch 60/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2037.7158 - val_loss: 1486.3274\n",
      "Epoch 61/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2005.1993 - val_loss: 1467.5542\n",
      "Epoch 62/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1973.0938 - val_loss: 1449.1133\n",
      "Epoch 63/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1941.3947 - val_loss: 1431.0004\n",
      "Epoch 64/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1910.0984 - val_loss: 1413.2124\n",
      "Epoch 65/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1879.2002 - val_loss: 1395.7456\n",
      "Epoch 66/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1848.6967 - val_loss: 1378.5972\n",
      "Epoch 67/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1818.5844 - val_loss: 1361.7632\n",
      "Epoch 68/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1788.8597 - val_loss: 1345.2407\n",
      "Epoch 69/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1759.5183 - val_loss: 1329.0267\n",
      "Epoch 70/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1730.5575 - val_loss: 1313.1176\n",
      "Epoch 71/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1701.9733 - val_loss: 1297.5105\n",
      "Epoch 72/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1673.7626 - val_loss: 1282.2020\n",
      "Epoch 73/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1645.9211 - val_loss: 1267.1892\n",
      "Epoch 74/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1618.4463 - val_loss: 1252.4690\n",
      "Epoch 75/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1591.3344 - val_loss: 1238.0385\n",
      "Epoch 76/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1564.5826 - val_loss: 1223.8944\n",
      "Epoch 77/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1538.1869 - val_loss: 1210.0344\n",
      "Epoch 78/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1512.1447 - val_loss: 1196.4548\n",
      "Epoch 79/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1486.4525 - val_loss: 1183.1531\n",
      "Epoch 80/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1461.1075 - val_loss: 1170.1263\n",
      "Epoch 81/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1436.1063 - val_loss: 1157.3717\n",
      "Epoch 82/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1411.4459 - val_loss: 1144.8859\n",
      "Epoch 83/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1387.1230 - val_loss: 1132.6669\n",
      "Epoch 84/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1363.1350 - val_loss: 1120.7112\n",
      "Epoch 85/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1339.4785 - val_loss: 1109.0162\n",
      "Epoch 86/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1316.1506 - val_loss: 1097.5792\n",
      "Epoch 87/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1293.1483 - val_loss: 1086.3972\n",
      "Epoch 88/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1270.4684 - val_loss: 1075.4677\n",
      "Epoch 89/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1248.1088 - val_loss: 1064.7880\n",
      "Epoch 90/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1226.0659 - val_loss: 1054.3551\n",
      "Epoch 91/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1204.3368 - val_loss: 1044.1667\n",
      "Epoch 92/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1182.9196 - val_loss: 1034.2197\n",
      "Epoch 93/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1161.8099 - val_loss: 1024.5118\n",
      "Epoch 94/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1141.0060 - val_loss: 1015.0403\n",
      "Epoch 95/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1120.5050 - val_loss: 1005.8021\n",
      "Epoch 96/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1100.3036 - val_loss: 996.7949\n",
      "Epoch 97/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1080.3993 - val_loss: 988.0161\n",
      "Epoch 98/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1060.7893 - val_loss: 979.4630\n",
      "Epoch 99/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1041.4709 - val_loss: 971.1331\n",
      "Epoch 100/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1022.4414 - val_loss: 963.0233\n",
      "Epoch 101/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1003.6981 - val_loss: 955.1319\n",
      "Epoch 102/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 985.2383 - val_loss: 947.4558\n",
      "Epoch 103/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 967.0591 - val_loss: 939.9924\n",
      "Epoch 104/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 949.1583 - val_loss: 932.7392\n",
      "Epoch 105/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 931.5328 - val_loss: 925.6937\n",
      "Epoch 106/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 914.1799 - val_loss: 918.8535\n",
      "Epoch 107/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 897.0974 - val_loss: 912.2158\n",
      "Epoch 108/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 880.2823 - val_loss: 905.7783\n",
      "Epoch 109/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 863.7321 - val_loss: 899.5383\n",
      "Epoch 110/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 847.4443 - val_loss: 893.4932\n",
      "Epoch 111/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 831.4162 - val_loss: 887.6409\n",
      "Epoch 112/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 815.6449 - val_loss: 881.9788\n",
      "Epoch 113/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 800.1285 - val_loss: 876.5042\n",
      "Epoch 114/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 784.8639 - val_loss: 871.2147\n",
      "Epoch 115/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 769.8489 - val_loss: 866.1079\n",
      "Epoch 116/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 755.0811 - val_loss: 861.1815\n",
      "Epoch 117/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 740.5573 - val_loss: 856.4327\n",
      "Epoch 118/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 726.2755 - val_loss: 851.8596\n",
      "Epoch 119/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 712.2334 - val_loss: 847.4589\n",
      "Epoch 120/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 698.4278 - val_loss: 843.2289\n",
      "Epoch 121/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 684.8568 - val_loss: 839.1670\n",
      "Epoch 122/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 671.5178 - val_loss: 835.2708\n",
      "Epoch 123/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 658.4080 - val_loss: 831.5377\n",
      "Epoch 124/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 645.5256 - val_loss: 827.9655\n",
      "Epoch 125/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 632.8675 - val_loss: 824.5518\n",
      "Epoch 126/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 620.4316 - val_loss: 821.2940\n",
      "Epoch 127/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 608.2153 - val_loss: 818.1901\n",
      "Epoch 128/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 596.2162 - val_loss: 815.2372\n",
      "Epoch 129/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 584.4319 - val_loss: 812.4333\n",
      "Epoch 130/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 572.8602 - val_loss: 809.7761\n",
      "Epoch 131/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 561.4984 - val_loss: 807.2629\n",
      "Epoch 132/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 550.3443 - val_loss: 804.8918\n",
      "Epoch 133/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 539.3954 - val_loss: 802.6600\n",
      "Epoch 134/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 528.6493 - val_loss: 800.5655\n",
      "Epoch 135/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 518.1038 - val_loss: 798.6058\n",
      "Epoch 136/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 507.7563 - val_loss: 796.7787\n",
      "Epoch 137/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 497.6047 - val_loss: 795.0817\n",
      "Epoch 138/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 487.6466 - val_loss: 793.5127\n",
      "Epoch 139/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 477.8796 - val_loss: 792.0694\n",
      "Epoch 140/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 468.3016 - val_loss: 790.7493\n",
      "Epoch 141/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 458.9098 - val_loss: 789.5504\n",
      "Epoch 142/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 449.7023 - val_loss: 788.4701\n",
      "Epoch 143/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 440.6767 - val_loss: 787.5063\n",
      "Epoch 144/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 431.8305 - val_loss: 786.6568\n",
      "Epoch 145/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 423.1617 - val_loss: 785.9191\n",
      "Epoch 146/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 414.6678 - val_loss: 785.2913\n",
      "Epoch 147/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 406.3467 - val_loss: 784.7710\n",
      "Epoch 148/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 398.1961 - val_loss: 784.3558\n",
      "Epoch 149/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 390.2137 - val_loss: 784.0438\n",
      "Epoch 150/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 382.3970 - val_loss: 783.8325\n",
      "Epoch 151/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 374.7443 - val_loss: 783.7200\n",
      "Epoch 152/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 367.2531 - val_loss: 783.7037\n",
      "Epoch 153/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 359.9212 - val_loss: 783.7818\n",
      "Epoch 154/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 352.7466 - val_loss: 783.9519\n",
      "Epoch 155/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 345.7267 - val_loss: 784.2119\n",
      "Epoch 156/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 338.8596 - val_loss: 784.5596\n",
      "Epoch 157/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 332.1428 - val_loss: 784.9929\n",
      "Epoch 158/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 325.5744 - val_loss: 785.5097\n",
      "Epoch 159/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 319.1525 - val_loss: 786.1079\n",
      "Epoch 160/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 312.8744 - val_loss: 786.7853\n",
      "Epoch 161/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 306.7382 - val_loss: 787.5397\n",
      "Epoch 162/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 300.7419 - val_loss: 788.3691\n",
      "Epoch 163/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 294.8830 - val_loss: 789.2715\n",
      "Epoch 164/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 289.1598 - val_loss: 790.2448\n",
      "Epoch 165/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 283.5703 - val_loss: 791.2868\n",
      "Epoch 166/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 278.1118 - val_loss: 792.3958\n",
      "Epoch 167/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 272.7827 - val_loss: 793.5693\n",
      "Epoch 168/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 267.5808 - val_loss: 794.8057\n",
      "Epoch 169/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 262.5040 - val_loss: 796.1028\n",
      "Epoch 170/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 257.5500 - val_loss: 797.4588\n",
      "Epoch 171/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 252.7171 - val_loss: 798.8714\n",
      "Epoch 172/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 248.0033 - val_loss: 800.3389\n",
      "Epoch 173/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 243.4064 - val_loss: 801.8591\n",
      "Epoch 174/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 238.9249 - val_loss: 803.4302\n",
      "Epoch 175/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 234.5561 - val_loss: 805.0505\n",
      "Epoch 176/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 230.2984 - val_loss: 806.7178\n",
      "Epoch 177/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 226.1498 - val_loss: 808.4307\n",
      "Epoch 178/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 222.1082 - val_loss: 810.1866\n",
      "Epoch 179/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 218.1721 - val_loss: 811.9841\n",
      "Epoch 180/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 214.3394 - val_loss: 813.8213\n",
      "Epoch 181/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 210.6079 - val_loss: 815.6964\n",
      "Epoch 182/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 206.9760 - val_loss: 817.6076\n",
      "Epoch 183/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 203.4416 - val_loss: 819.5531\n",
      "Epoch 184/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 200.0029 - val_loss: 821.5314\n",
      "Epoch 185/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 196.6581 - val_loss: 823.5405\n",
      "Epoch 186/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 193.4053 - val_loss: 825.5787\n",
      "Epoch 187/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 190.2430 - val_loss: 827.6440\n",
      "Epoch 188/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 187.1692 - val_loss: 829.7353\n",
      "Epoch 189/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 184.1820 - val_loss: 831.8508\n",
      "Epoch 190/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 181.2796 - val_loss: 833.9886\n",
      "Epoch 191/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 178.4604 - val_loss: 836.1473\n",
      "Epoch 192/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 175.7227 - val_loss: 838.3253\n",
      "Epoch 193/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 173.0647 - val_loss: 840.5210\n",
      "Epoch 194/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 170.4846 - val_loss: 842.7329\n",
      "Epoch 195/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 167.9808 - val_loss: 844.9595\n",
      "Epoch 196/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 165.5517 - val_loss: 847.1992\n",
      "Epoch 197/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 163.1955 - val_loss: 849.4509\n",
      "Epoch 198/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 160.9106 - val_loss: 851.7126\n",
      "Epoch 199/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 158.6954 - val_loss: 853.9827\n",
      "Epoch 200/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 156.5486 - val_loss: 856.2609\n",
      "Epoch 201/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 154.4682 - val_loss: 858.5447\n",
      "Epoch 202/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 152.4530 - val_loss: 860.8333\n",
      "Epoch 203/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 150.5012 - val_loss: 863.1252\n",
      "Epoch 204/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 148.6114 - val_loss: 865.4193\n",
      "Epoch 205/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 146.7819 - val_loss: 867.7145\n",
      "Epoch 206/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 145.0114 - val_loss: 870.0089\n",
      "Epoch 207/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 143.2984 - val_loss: 872.3021\n",
      "Epoch 208/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 141.6415 - val_loss: 874.5923\n",
      "Epoch 209/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 140.0392 - val_loss: 876.8784\n",
      "Epoch 210/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 138.4902 - val_loss: 879.1597\n",
      "Epoch 211/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 136.9931 - val_loss: 881.4347\n",
      "Epoch 212/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 135.5465 - val_loss: 883.7024\n",
      "Epoch 213/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 134.1490 - val_loss: 885.9618\n",
      "Epoch 214/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 132.7996 - val_loss: 888.2116\n",
      "Epoch 215/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 131.4967 - val_loss: 890.4515\n",
      "Epoch 216/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 130.2390 - val_loss: 892.6802\n",
      "Epoch 217/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 129.0253 - val_loss: 894.8963\n",
      "Epoch 218/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 127.8545 - val_loss: 897.0997\n",
      "Epoch 219/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 126.7252 - val_loss: 899.2889\n",
      "Epoch 220/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 125.6364 - val_loss: 901.4633\n",
      "Epoch 221/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 124.5869 - val_loss: 903.6221\n",
      "Epoch 222/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 123.5756 - val_loss: 905.7644\n",
      "Epoch 223/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 122.6012 - val_loss: 907.8896\n",
      "Epoch 224/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 121.6626 - val_loss: 909.9968\n",
      "Epoch 225/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 120.7589 - val_loss: 912.0854\n",
      "Epoch 226/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 119.8888 - val_loss: 914.1551\n",
      "Epoch 227/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 119.0514 - val_loss: 916.2050\n",
      "Epoch 228/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 118.2456 - val_loss: 918.2344\n",
      "Epoch 229/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 117.4707 - val_loss: 920.2422\n",
      "Epoch 230/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 116.7254 - val_loss: 922.2285\n",
      "Epoch 231/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 116.0090 - val_loss: 924.1932\n",
      "Epoch 232/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 115.3203 - val_loss: 926.1346\n",
      "Epoch 233/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 114.6585 - val_loss: 928.0531\n",
      "Epoch 234/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 114.0228 - val_loss: 929.9480\n",
      "Epoch 235/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 113.4122 - val_loss: 931.8193\n",
      "Epoch 236/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 112.8260 - val_loss: 933.6661\n",
      "Epoch 237/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 112.2632 - val_loss: 935.4879\n",
      "Epoch 238/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 111.7232 - val_loss: 937.2849\n",
      "Epoch 239/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 111.2049 - val_loss: 939.0566\n",
      "Epoch 240/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 110.7079 - val_loss: 940.8022\n",
      "Epoch 241/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 110.2313 - val_loss: 942.5228\n",
      "Epoch 242/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.7742 - val_loss: 944.2170\n",
      "Epoch 243/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.3362 - val_loss: 945.8846\n",
      "Epoch 244/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 108.9164 - val_loss: 947.5257\n",
      "Epoch 245/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 108.5142 - val_loss: 949.1406\n",
      "Epoch 246/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 108.1290 - val_loss: 950.7290\n",
      "Epoch 247/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 107.7599 - val_loss: 952.2900\n",
      "Epoch 248/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 107.4067 - val_loss: 953.8241\n",
      "Epoch 249/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 107.0686 - val_loss: 955.3312\n",
      "Epoch 250/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 106.7450 - val_loss: 956.8114\n",
      "Epoch 251/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 106.4354 - val_loss: 958.2645\n",
      "Epoch 252/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 106.1392 - val_loss: 959.6905\n",
      "Epoch 253/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 105.8559 - val_loss: 961.0896\n",
      "Epoch 254/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 105.5850 - val_loss: 962.4612\n",
      "Epoch 255/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 105.3260 - val_loss: 963.8065\n",
      "Epoch 256/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 105.0784 - val_loss: 965.1247\n",
      "Epoch 257/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 104.8419 - val_loss: 966.4162\n",
      "Epoch 258/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 104.6157 - val_loss: 967.6810\n",
      "Epoch 259/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 104.3997 - val_loss: 968.9192\n",
      "Epoch 260/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 104.1935 - val_loss: 970.1310\n",
      "Epoch 261/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 103.9964 - val_loss: 971.3167\n",
      "Epoch 262/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 103.8082 - val_loss: 972.4766\n",
      "Epoch 263/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 103.6285 - val_loss: 973.6107\n",
      "Epoch 264/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 103.4570 - val_loss: 974.7192\n",
      "Epoch 265/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 103.2933 - val_loss: 975.8021\n",
      "Epoch 266/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 103.1371 - val_loss: 976.8596\n",
      "Epoch 267/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 102.9880 - val_loss: 977.8928\n",
      "Epoch 268/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 102.8457 - val_loss: 978.9007\n",
      "Epoch 269/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 102.7100 - val_loss: 979.8849\n",
      "Epoch 270/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 102.5806 - val_loss: 980.8450\n",
      "Epoch 271/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 102.4569 - val_loss: 981.7807\n",
      "Epoch 272/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 102.3392 - val_loss: 982.6934\n",
      "Epoch 273/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 102.2268 - val_loss: 983.5828\n",
      "Epoch 274/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 102.1197 - val_loss: 984.4491\n",
      "Epoch 275/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 102.0175 - val_loss: 985.2933\n",
      "Epoch 276/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 101.9202 - val_loss: 986.1144\n",
      "Epoch 277/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 101.8274 - val_loss: 986.9142\n",
      "Epoch 278/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 101.7389 - val_loss: 987.6921\n",
      "Epoch 279/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 101.6545 - val_loss: 988.4493\n",
      "Epoch 280/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 101.5740 - val_loss: 989.1852\n",
      "Epoch 281/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 101.4974 - val_loss: 989.9007\n",
      "Epoch 282/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 101.4243 - val_loss: 990.5958\n",
      "Epoch 283/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 101.3547 - val_loss: 991.2711\n",
      "Epoch 284/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 101.2883 - val_loss: 991.9269\n",
      "Epoch 285/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 101.2251 - val_loss: 992.5637\n",
      "Epoch 286/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 101.1648 - val_loss: 993.1813\n",
      "Epoch 287/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 101.1074 - val_loss: 993.7811\n",
      "Epoch 288/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 101.0526 - val_loss: 994.3629\n",
      "Epoch 289/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 101.0005 - val_loss: 994.9267\n",
      "Epoch 290/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 100.9508 - val_loss: 995.4734\n",
      "Epoch 291/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.9034 - val_loss: 996.0031\n",
      "Epoch 292/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 100.8583 - val_loss: 996.5165\n",
      "Epoch 293/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 100.8152 - val_loss: 997.0136\n",
      "Epoch 294/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 100.7742 - val_loss: 997.4952\n",
      "Epoch 295/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.7351 - val_loss: 997.9615\n",
      "Epoch 296/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.6979 - val_loss: 998.4119\n",
      "Epoch 297/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 100.6624 - val_loss: 998.8482\n",
      "Epoch 298/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.6285 - val_loss: 999.2700\n",
      "Epoch 299/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 100.5963 - val_loss: 999.6779\n",
      "Epoch 300/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.5656 - val_loss: 1000.0720\n",
      "Epoch 301/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 100.5363 - val_loss: 1000.4521\n",
      "Epoch 302/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.5084 - val_loss: 1000.8199\n",
      "Epoch 303/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.4819 - val_loss: 1001.1751\n",
      "Epoch 304/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.4565 - val_loss: 1001.5179\n",
      "Epoch 305/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 100.4324 - val_loss: 1001.8491\n",
      "Epoch 306/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.4093 - val_loss: 1002.1681\n",
      "Epoch 307/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.3874 - val_loss: 1002.4759\n",
      "Epoch 308/500\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 100.3665 - val_loss: 1002.7731\n",
      "Epoch 309/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 100.3465 - val_loss: 1003.0591\n",
      "Epoch 310/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 100.3276 - val_loss: 1003.3353\n",
      "Epoch 311/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 100.3094 - val_loss: 1003.6013\n",
      "Epoch 312/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 100.2921 - val_loss: 1003.8576\n",
      "Epoch 313/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 100.2757 - val_loss: 1004.1042\n",
      "Epoch 314/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 100.2599 - val_loss: 1004.3420\n",
      "Epoch 315/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 100.2450 - val_loss: 1004.5703\n",
      "Epoch 316/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 100.2307 - val_loss: 1004.7902\n",
      "Epoch 317/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.2171 - val_loss: 1005.0016\n",
      "Epoch 318/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 100.2042 - val_loss: 1005.2045\n",
      "Epoch 319/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.1919 - val_loss: 1005.3998\n",
      "Epoch 320/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.1801 - val_loss: 1005.5873\n",
      "Epoch 321/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.1690 - val_loss: 1005.7683\n",
      "Epoch 322/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.1582 - val_loss: 1005.9415\n",
      "Epoch 323/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.1481 - val_loss: 1006.1085\n",
      "Epoch 324/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 100.1384 - val_loss: 1006.2683\n",
      "Epoch 325/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.1291 - val_loss: 1006.4218\n",
      "Epoch 326/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 100.1204 - val_loss: 1006.5694\n",
      "Epoch 327/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 100.1120 - val_loss: 1006.7105\n",
      "Epoch 328/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.1040 - val_loss: 1006.8460\n",
      "Epoch 329/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.0964 - val_loss: 1006.9759\n",
      "Epoch 330/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.0891 - val_loss: 1007.1006\n",
      "Epoch 331/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 100.0822 - val_loss: 1007.2201\n",
      "Epoch 332/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.0757 - val_loss: 1007.3348\n",
      "Epoch 333/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.0694 - val_loss: 1007.4442\n",
      "Epoch 334/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.0635 - val_loss: 1007.5489\n",
      "Epoch 335/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.0579 - val_loss: 1007.6499\n",
      "Epoch 336/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 100.0524 - val_loss: 1007.7461\n",
      "Epoch 337/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.0473 - val_loss: 1007.8384\n",
      "Epoch 338/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 100.0424 - val_loss: 1007.9263\n",
      "Epoch 339/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 100.0378 - val_loss: 1008.0107\n",
      "Epoch 340/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.0334 - val_loss: 1008.0914\n",
      "Epoch 341/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.0292 - val_loss: 1008.1683\n",
      "Epoch 342/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 100.0252 - val_loss: 1008.2415\n",
      "Epoch 343/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 100.0215 - val_loss: 1008.3117\n",
      "Epoch 344/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.0179 - val_loss: 1008.3789\n",
      "Epoch 345/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 100.0145 - val_loss: 1008.4434\n",
      "Epoch 346/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.0112 - val_loss: 1008.5045\n",
      "Epoch 347/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.0082 - val_loss: 1008.5632\n",
      "Epoch 348/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 100.0052 - val_loss: 1008.6188\n",
      "Epoch 349/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 100.0025 - val_loss: 1008.6725\n",
      "Epoch 350/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9999 - val_loss: 1008.7233\n",
      "Epoch 351/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9974 - val_loss: 1008.7719\n",
      "Epoch 352/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9950 - val_loss: 1008.8184\n",
      "Epoch 353/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9927 - val_loss: 1008.8626\n",
      "Epoch 354/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9906 - val_loss: 1008.9046\n",
      "Epoch 355/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9886 - val_loss: 1008.9444\n",
      "Epoch 356/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9867 - val_loss: 1008.9825\n",
      "Epoch 357/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9849 - val_loss: 1009.0190\n",
      "Epoch 358/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 99.9832 - val_loss: 1009.0538\n",
      "Epoch 359/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9816 - val_loss: 1009.0866\n",
      "Epoch 360/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9801 - val_loss: 1009.1181\n",
      "Epoch 361/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9786 - val_loss: 1009.1480\n",
      "Epoch 362/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9773 - val_loss: 1009.1768\n",
      "Epoch 363/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9760 - val_loss: 1009.2032\n",
      "Epoch 364/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9748 - val_loss: 1009.2292\n",
      "Epoch 365/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 99.9736 - val_loss: 1009.2538\n",
      "Epoch 366/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 99.9726 - val_loss: 1009.2772\n",
      "Epoch 367/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9716 - val_loss: 1009.2996\n",
      "Epoch 368/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9705 - val_loss: 1009.3206\n",
      "Epoch 369/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9697 - val_loss: 1009.3404\n",
      "Epoch 370/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9688 - val_loss: 1009.3596\n",
      "Epoch 371/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9681 - val_loss: 1009.3773\n",
      "Epoch 372/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9673 - val_loss: 1009.3944\n",
      "Epoch 373/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9667 - val_loss: 1009.4111\n",
      "Epoch 374/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9659 - val_loss: 1009.4265\n",
      "Epoch 375/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9653 - val_loss: 1009.4413\n",
      "Epoch 376/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9648 - val_loss: 1009.4554\n",
      "Epoch 377/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9642 - val_loss: 1009.4685\n",
      "Epoch 378/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9638 - val_loss: 1009.4814\n",
      "Epoch 379/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9633 - val_loss: 1009.4935\n",
      "Epoch 380/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9628 - val_loss: 1009.5049\n",
      "Epoch 381/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9624 - val_loss: 1009.5156\n",
      "Epoch 382/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9620 - val_loss: 1009.5258\n",
      "Epoch 383/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9616 - val_loss: 1009.5353\n",
      "Epoch 384/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9614 - val_loss: 1009.5446\n",
      "Epoch 385/500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 99.9611 - val_loss: 1009.5533\n",
      "Epoch 386/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9607 - val_loss: 1009.5618\n",
      "Epoch 387/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9605 - val_loss: 1009.5696\n",
      "Epoch 388/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9603 - val_loss: 1009.5769\n",
      "Epoch 389/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9600 - val_loss: 1009.5839\n",
      "Epoch 390/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9598 - val_loss: 1009.5906\n",
      "Epoch 391/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9596 - val_loss: 1009.5971\n",
      "Epoch 392/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 99.9595 - val_loss: 1009.6031\n",
      "Epoch 393/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9594 - val_loss: 1009.6085\n",
      "Epoch 394/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9592 - val_loss: 1009.6141\n",
      "Epoch 395/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9591 - val_loss: 1009.6193\n",
      "Epoch 396/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9589 - val_loss: 1009.6242\n",
      "Epoch 397/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9589 - val_loss: 1009.6282\n",
      "Epoch 398/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9588 - val_loss: 1009.6328\n",
      "Epoch 399/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9587 - val_loss: 1009.6366\n",
      "Epoch 400/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9587 - val_loss: 1009.6402\n",
      "Epoch 401/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9586 - val_loss: 1009.6440\n",
      "Epoch 402/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9585 - val_loss: 1009.6471\n",
      "Epoch 403/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9585 - val_loss: 1009.6503\n",
      "Epoch 404/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9585 - val_loss: 1009.6530\n",
      "Epoch 405/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9585 - val_loss: 1009.6560\n",
      "Epoch 406/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9585 - val_loss: 1009.6589\n",
      "Epoch 407/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9585 - val_loss: 1009.6614\n",
      "Epoch 408/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9585 - val_loss: 1009.6638\n",
      "Epoch 409/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9585 - val_loss: 1009.6662\n",
      "Epoch 410/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9585 - val_loss: 1009.6685\n",
      "Epoch 411/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9585 - val_loss: 1009.6703\n",
      "Epoch 412/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9585 - val_loss: 1009.6723\n",
      "Epoch 413/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9585 - val_loss: 1009.6739\n",
      "Epoch 414/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9586 - val_loss: 1009.6754\n",
      "Epoch 415/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 99.9586 - val_loss: 1009.6772\n",
      "Epoch 416/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 99.9587 - val_loss: 1009.6783\n",
      "Epoch 417/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9587 - val_loss: 1009.6797\n",
      "Epoch 418/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9588 - val_loss: 1009.6813\n",
      "Epoch 419/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9588 - val_loss: 1009.6821\n",
      "Epoch 420/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9589 - val_loss: 1009.6836\n",
      "Epoch 421/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9589 - val_loss: 1009.6846\n",
      "Epoch 422/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9590 - val_loss: 1009.6859\n",
      "Epoch 423/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9590 - val_loss: 1009.6870\n",
      "Epoch 424/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9590 - val_loss: 1009.6876\n",
      "Epoch 425/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9592 - val_loss: 1009.6884\n",
      "Epoch 426/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9592 - val_loss: 1009.6891\n",
      "Epoch 427/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9593 - val_loss: 1009.6899\n",
      "Epoch 428/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9593 - val_loss: 1009.6909\n",
      "Epoch 429/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9594 - val_loss: 1009.6915\n",
      "Epoch 430/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9595 - val_loss: 1009.6924\n",
      "Epoch 431/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9594 - val_loss: 1009.6926\n",
      "Epoch 432/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9596 - val_loss: 1009.6931\n",
      "Epoch 433/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9596 - val_loss: 1009.6937\n",
      "Epoch 434/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9597 - val_loss: 1009.6942\n",
      "Epoch 435/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9597 - val_loss: 1009.6949\n",
      "Epoch 436/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9598 - val_loss: 1009.6953\n",
      "Epoch 437/500\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 99.9599 - val_loss: 1009.6956\n",
      "Epoch 438/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9600 - val_loss: 1009.6959\n",
      "Epoch 439/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9600 - val_loss: 1009.6962\n",
      "Epoch 440/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9601 - val_loss: 1009.6962\n",
      "Epoch 441/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9601 - val_loss: 1009.6965\n",
      "Epoch 442/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9602 - val_loss: 1009.6967\n",
      "Epoch 443/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9603 - val_loss: 1009.6972\n",
      "Epoch 444/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9604 - val_loss: 1009.6981\n",
      "Epoch 445/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9604 - val_loss: 1009.6982\n",
      "Epoch 446/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9604 - val_loss: 1009.6982\n",
      "Epoch 447/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9605 - val_loss: 1009.6982\n",
      "Epoch 448/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9606 - val_loss: 1009.6987\n",
      "Epoch 449/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9607 - val_loss: 1009.6989\n",
      "Epoch 450/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9607 - val_loss: 1009.6992\n",
      "Epoch 451/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9608 - val_loss: 1009.6992\n",
      "Epoch 452/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9608 - val_loss: 1009.6992\n",
      "Epoch 453/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9609 - val_loss: 1009.6990\n",
      "Epoch 454/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9610 - val_loss: 1009.6991\n",
      "Epoch 455/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9610 - val_loss: 1009.6991\n",
      "Epoch 456/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9611 - val_loss: 1009.6991\n",
      "Epoch 457/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9612 - val_loss: 1009.6992\n",
      "Epoch 458/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9613 - val_loss: 1009.6996\n",
      "Epoch 459/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9613 - val_loss: 1009.6995\n",
      "Epoch 460/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9614 - val_loss: 1009.6999\n",
      "Epoch 461/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9614 - val_loss: 1009.6999\n",
      "Epoch 462/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9614 - val_loss: 1009.6995\n",
      "Epoch 463/500\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 99.9615 - val_loss: 1009.6999\n",
      "Epoch 464/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9615 - val_loss: 1009.6995\n",
      "Epoch 465/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 99.9617 - val_loss: 1009.6996\n",
      "Epoch 466/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9617 - val_loss: 1009.6996\n",
      "Epoch 467/500\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 99.9618 - val_loss: 1009.6996\n",
      "Epoch 468/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9618 - val_loss: 1009.7001\n",
      "Epoch 469/500\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 99.9618 - val_loss: 1009.7002\n",
      "Epoch 470/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9619 - val_loss: 1009.6998\n",
      "Epoch 471/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9619 - val_loss: 1009.7000\n",
      "Epoch 472/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9620 - val_loss: 1009.7000\n",
      "Epoch 473/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9620 - val_loss: 1009.6998\n",
      "Epoch 474/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9621 - val_loss: 1009.6998\n",
      "Epoch 475/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9621 - val_loss: 1009.6995\n",
      "Epoch 476/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9622 - val_loss: 1009.6993\n",
      "Epoch 477/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9622 - val_loss: 1009.6993\n",
      "Epoch 478/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9623 - val_loss: 1009.6993\n",
      "Epoch 479/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9623 - val_loss: 1009.6995\n",
      "Epoch 480/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9624 - val_loss: 1009.6998\n",
      "Epoch 481/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9624 - val_loss: 1009.6998\n",
      "Epoch 482/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9624 - val_loss: 1009.6998\n",
      "Epoch 483/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9625 - val_loss: 1009.7001\n",
      "Epoch 484/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9625 - val_loss: 1009.7001\n",
      "Epoch 485/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9626 - val_loss: 1009.7001\n",
      "Epoch 486/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9626 - val_loss: 1009.6999\n",
      "Epoch 487/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9627 - val_loss: 1009.7000\n",
      "Epoch 488/500\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 99.9627 - val_loss: 1009.7001\n",
      "Epoch 489/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9627 - val_loss: 1009.7002\n",
      "Epoch 490/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9628 - val_loss: 1009.7002\n",
      "Epoch 491/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9628 - val_loss: 1009.7007\n",
      "Epoch 492/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9628 - val_loss: 1009.7004\n",
      "Epoch 493/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9628 - val_loss: 1009.7002\n",
      "Epoch 494/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9629 - val_loss: 1009.6999\n",
      "Epoch 495/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9629 - val_loss: 1009.6995\n",
      "Epoch 496/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9630 - val_loss: 1009.6996\n",
      "Epoch 497/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9631 - val_loss: 1009.6999\n",
      "Epoch 498/500\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 99.9631 - val_loss: 1009.6998\n",
      "Epoch 499/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9631 - val_loss: 1009.6998\n",
      "Epoch 500/500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 99.9631 - val_loss: 1009.6998\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(85.7336, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0020, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0125, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3008, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 359ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[66.4668184 , 66.3869865 , 66.3071545 , 66.2273226 , 66.1474907 ,\n",
       "        66.0676587 , 65.9788268 ,  0.72264761,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.08070576, 67.53686975, 67.39821429,\n",
       "        67.25955882, 67.12090336, 66.9822479 , 66.84359244, 66.72405462,\n",
       "        66.64422269, 66.56439076, 66.48455882, 66.40472689, 66.32489496,\n",
       "        66.24506303, 66.16523109, 66.08539916, 66.00556723, 65.92573529,\n",
       "        65.84590336, 65.76785714, 65.69222689, 65.61659664, 65.54096639,\n",
       "        65.46533613, 65.38970588, 65.31407563, 65.23844538, 65.16281513,\n",
       "         0.53002274,  0.29109389,  0.        ,  0.        ,  0.37542471,\n",
       "         0.62643063,  0.46592137, 66.26280345, 66.18297152, 66.10313959,\n",
       "        66.02330766, 65.94347572, 65.86364379, 65.78466387, 65.70903361,\n",
       "        65.63340336, 65.55777311, 65.48214286, 65.40651261, 65.33088235,\n",
       "        65.2552521 , 65.17962185, 70.2306256 , 70.0289449 , 69.6545285 ,\n",
       "        69.2511671 , 68.8478058 , 68.4444444 , 68.0298669 , 67.6139006 ,\n",
       "        67.1979342 ,  0.36256564,  0.35250008, 63.53670883,  0.07534569,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        65.56256104,  0.46106115,  0.1876983 ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.39508504,  0.07071385,  0.3459914 ,  0.28912431,  0.10232724,\n",
       "         0.66768968,  0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([60.66463585, 60.65063025, 60.63662465, 60.62261905, 60.60861345,\n",
       "       60.59460784, 60.58060224, 60.56659664, 60.55259104, 60.53858543,\n",
       "       60.52457983, 60.51057423, 60.49656863, 60.48256303, 60.46855742,\n",
       "       60.45455182, 60.44054622, 60.42654062, 60.41253501, 60.39852941,\n",
       "       60.38452381, 60.37051821, 60.35651261, 60.342507  , 60.3285014 ,\n",
       "       60.3144958 , 60.3004902 , 60.28648459, 60.27247899, 60.25847339,\n",
       "       60.24446779, 60.23046218, 60.21645658, 60.20245098, 60.18844538,\n",
       "       60.17443978, 60.16043417, 60.14642857, 60.13242297, 60.11841737,\n",
       "       60.10441176, 60.09040616, 60.07640056, 60.06239496, 60.04838936,\n",
       "       60.03438375, 60.02037815, 60.00637255, 59.99236695, 59.97836134,\n",
       "       59.96435574, 59.95035014, 59.93634454, 59.92233894, 59.90833333,\n",
       "       59.89432773, 59.88032213, 59.86631653, 59.85231092, 59.83830532,\n",
       "       59.82429972, 59.81029412, 59.79628852, 59.78228291, 59.76827731,\n",
       "       59.75427171, 59.74026611, 59.7262605 , 59.7122549 , 59.69935808,\n",
       "       59.69422269, 59.6890873 , 59.68395191, 59.67881653, 59.67368114,\n",
       "       59.66854575, 59.66341036, 59.65827498, 59.65313959, 59.6480042 ,\n",
       "       59.64286881, 59.63773343, 59.63259804, 59.62746265, 59.62232726,\n",
       "       59.61719188, 59.61205649, 59.6069211 , 59.60178571, 59.59665033,\n",
       "       59.59151494, 59.58637955, 59.58124416, 59.57610878, 59.57097339,\n",
       "       59.565838  , 59.56070261, 59.55556723, 59.55043184, 59.54529645])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.06630972410961\n",
      "27.641575869750604\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
