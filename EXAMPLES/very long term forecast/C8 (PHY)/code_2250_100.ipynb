{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "2345    56.688809\n",
       "2346    56.679791\n",
       "2347    56.670773\n",
       "2348    56.661754\n",
       "2349    56.652736\n",
       "Name: C8, Length: 2350, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c8_interpolated_2250_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "2245     1.227377\n",
       "2246     0.677593\n",
       "2247     0.452216\n",
       "2248     0.000000\n",
       "2249     0.857867\n",
       "Name: C8, Length: 2250, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2250)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.987115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.774230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.561345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.348459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.200000  0.000298\n",
       "1     87.987115  0.000298\n",
       "2     87.774230  0.000297\n",
       "3     87.561345  0.000297\n",
       "4     87.348459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 85.7336  # Value for C0\n",
    "K0 = -0.0020  # Value for K0\n",
    "K1 = 0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0125    # Value for b\n",
    "c = 2.3008    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.200000    87.987115    87.774230    87.561345    87.348459   \n",
      "351    87.987115    87.774230    87.561345    87.348459    87.135574   \n",
      "352    87.774230    87.561345    87.348459    87.135574    86.922689   \n",
      "353    87.561345    87.348459    87.135574    86.922689    86.709804   \n",
      "354    87.348459    87.135574    86.922689    86.709804    86.496919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.135574    86.922689    86.709804    86.496919    86.294538  ...   \n",
      "351    86.922689    86.709804    86.496919    86.294538    86.221709  ...   \n",
      "352    86.709804    86.496919    86.294538    86.221709    86.148880  ...   \n",
      "353    86.496919    86.294538    86.221709    86.148880    86.076050  ...   \n",
      "354    86.294538    86.221709    86.148880    86.076050    86.003221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   76.094351    0.000263   76.058870    0.000263   76.023389    0.000263   \n",
      "351   76.058870    0.000263   76.023389    0.000263   75.987909    0.000262   \n",
      "352   76.023389    0.000263   75.987909    0.000262   75.952428    0.000262   \n",
      "353   75.987909    0.000262   75.952428    0.000262   75.916947    0.000262   \n",
      "354   75.952428    0.000262   75.916947    0.000262   75.881466    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   75.987909    0.000262   75.952428    0.000262  \n",
      "351   75.952428    0.000262   75.916947    0.000262  \n",
      "352   75.916947    0.000262   75.881466    0.000262  \n",
      "353   75.881466    0.000262   75.845985    0.000262  \n",
      "354   75.845985    0.000262   75.810504    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 1, 251) (1800, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "23/23 [==============================] - 3s 24ms/step - loss: 4519.0015 - val_loss: 2455.1340\n",
      "Epoch 2/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 4254.3589 - val_loss: 2328.8699\n",
      "Epoch 3/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4115.7241 - val_loss: 2266.8657\n",
      "Epoch 4/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 4012.8694 - val_loss: 2211.5879\n",
      "Epoch 5/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3915.1719 - val_loss: 2159.4597\n",
      "Epoch 6/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3821.5552 - val_loss: 2110.2563\n",
      "Epoch 7/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3732.1194 - val_loss: 2060.9651\n",
      "Epoch 8/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3636.4521 - val_loss: 2010.5308\n",
      "Epoch 9/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3545.1787 - val_loss: 1963.9191\n",
      "Epoch 10/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3456.3518 - val_loss: 1914.3264\n",
      "Epoch 11/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3360.6265 - val_loss: 1868.9148\n",
      "Epoch 12/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3273.2632 - val_loss: 1825.7891\n",
      "Epoch 13/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3189.0955 - val_loss: 1784.6730\n",
      "Epoch 14/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3107.5740 - val_loss: 1745.3063\n",
      "Epoch 15/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 3028.3374 - val_loss: 1707.5371\n",
      "Epoch 16/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2951.1743 - val_loss: 1671.2667\n",
      "Epoch 17/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2875.9451 - val_loss: 1636.4218\n",
      "Epoch 18/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2802.5481 - val_loss: 1602.9446\n",
      "Epoch 19/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2730.9067 - val_loss: 1570.7878\n",
      "Epoch 20/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2660.9558 - val_loss: 1539.9100\n",
      "Epoch 21/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2592.6416 - val_loss: 1510.2742\n",
      "Epoch 22/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2525.9180 - val_loss: 1481.8466\n",
      "Epoch 23/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2460.7422 - val_loss: 1454.5969\n",
      "Epoch 24/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2397.0769 - val_loss: 1428.4961\n",
      "Epoch 25/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2334.8872 - val_loss: 1403.5165\n",
      "Epoch 26/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2274.1409 - val_loss: 1379.6326\n",
      "Epoch 27/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2214.8081 - val_loss: 1356.8190\n",
      "Epoch 28/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 2156.8601 - val_loss: 1335.0521\n",
      "Epoch 29/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2100.2688 - val_loss: 1314.3082\n",
      "Epoch 30/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2045.0095 - val_loss: 1294.5651\n",
      "Epoch 31/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1991.0560 - val_loss: 1275.8007\n",
      "Epoch 32/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1938.3854 - val_loss: 1257.9935\n",
      "Epoch 33/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1886.9724 - val_loss: 1241.1221\n",
      "Epoch 34/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1836.7953 - val_loss: 1225.1661\n",
      "Epoch 35/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1787.8319 - val_loss: 1210.1052\n",
      "Epoch 36/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1740.0609 - val_loss: 1195.9197\n",
      "Epoch 37/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1693.4602 - val_loss: 1182.5897\n",
      "Epoch 38/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1648.0095 - val_loss: 1170.0957\n",
      "Epoch 39/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1603.6888 - val_loss: 1158.4191\n",
      "Epoch 40/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1560.4778 - val_loss: 1147.5405\n",
      "Epoch 41/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1518.3562 - val_loss: 1137.4418\n",
      "Epoch 42/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1477.3059 - val_loss: 1128.1044\n",
      "Epoch 43/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1437.3069 - val_loss: 1119.5098\n",
      "Epoch 44/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1398.3411 - val_loss: 1111.6406\n",
      "Epoch 45/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1360.3893 - val_loss: 1104.4788\n",
      "Epoch 46/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1323.4337 - val_loss: 1098.0067\n",
      "Epoch 47/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1287.4565 - val_loss: 1092.2069\n",
      "Epoch 48/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1252.4395 - val_loss: 1087.0624\n",
      "Epoch 49/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1218.3652 - val_loss: 1082.5558\n",
      "Epoch 50/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1185.2162 - val_loss: 1078.6702\n",
      "Epoch 51/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1152.9756 - val_loss: 1075.3889\n",
      "Epoch 52/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1121.6263 - val_loss: 1072.6954\n",
      "Epoch 53/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 1091.1510 - val_loss: 1070.5732\n",
      "Epoch 54/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1061.5338 - val_loss: 1069.0061\n",
      "Epoch 55/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1032.7581 - val_loss: 1067.9777\n",
      "Epoch 56/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1004.8072 - val_loss: 1067.4719\n",
      "Epoch 57/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 977.6651 - val_loss: 1067.4731\n",
      "Epoch 58/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 951.3162 - val_loss: 1067.9656\n",
      "Epoch 59/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 925.7444 - val_loss: 1068.9337\n",
      "Epoch 60/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 900.9343 - val_loss: 1070.3621\n",
      "Epoch 61/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 876.8702 - val_loss: 1072.2352\n",
      "Epoch 62/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 853.5370 - val_loss: 1074.5383\n",
      "Epoch 63/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 830.9194 - val_loss: 1077.2561\n",
      "Epoch 64/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 809.0024 - val_loss: 1080.3739\n",
      "Epoch 65/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 787.7710 - val_loss: 1083.8768\n",
      "Epoch 66/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 767.2108 - val_loss: 1087.7505\n",
      "Epoch 67/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 747.3069 - val_loss: 1091.9803\n",
      "Epoch 68/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 728.0451 - val_loss: 1096.5525\n",
      "Epoch 69/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 709.4107 - val_loss: 1101.4523\n",
      "Epoch 70/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 691.3901 - val_loss: 1106.6663\n",
      "Epoch 71/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 673.9690 - val_loss: 1112.1803\n",
      "Epoch 72/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 657.1338 - val_loss: 1117.9807\n",
      "Epoch 73/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 640.8702 - val_loss: 1124.0543\n",
      "Epoch 74/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 625.1655 - val_loss: 1130.3881\n",
      "Epoch 75/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 610.0056 - val_loss: 1136.9681\n",
      "Epoch 76/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 595.3776 - val_loss: 1143.7822\n",
      "Epoch 77/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 581.2682 - val_loss: 1150.8167\n",
      "Epoch 78/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 567.6647 - val_loss: 1158.0594\n",
      "Epoch 79/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 554.5541 - val_loss: 1165.4983\n",
      "Epoch 80/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 541.9238 - val_loss: 1173.1205\n",
      "Epoch 81/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 529.7611 - val_loss: 1180.9143\n",
      "Epoch 82/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 518.0540 - val_loss: 1188.8677\n",
      "Epoch 83/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 506.7899 - val_loss: 1196.9686\n",
      "Epoch 84/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 495.9574 - val_loss: 1205.2061\n",
      "Epoch 85/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 485.5438 - val_loss: 1213.5690\n",
      "Epoch 86/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 475.5380 - val_loss: 1222.0455\n",
      "Epoch 87/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 465.9283 - val_loss: 1230.6250\n",
      "Epoch 88/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 456.7031 - val_loss: 1239.2968\n",
      "Epoch 89/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 447.8514 - val_loss: 1248.0502\n",
      "Epoch 90/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 439.3622 - val_loss: 1256.8757\n",
      "Epoch 91/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 431.2245 - val_loss: 1265.7626\n",
      "Epoch 92/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 423.4276 - val_loss: 1274.7012\n",
      "Epoch 93/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 415.9606 - val_loss: 1283.6818\n",
      "Epoch 94/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 408.8139 - val_loss: 1292.6952\n",
      "Epoch 95/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 401.9767 - val_loss: 1301.7319\n",
      "Epoch 96/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 395.4391 - val_loss: 1310.7832\n",
      "Epoch 97/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 389.1914 - val_loss: 1319.8407\n",
      "Epoch 98/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 383.2238 - val_loss: 1328.8956\n",
      "Epoch 99/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 377.5268 - val_loss: 1337.9398\n",
      "Epoch 100/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 372.0912 - val_loss: 1346.9656\n",
      "Epoch 101/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 366.9078 - val_loss: 1355.9650\n",
      "Epoch 102/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 361.9676 - val_loss: 1364.9302\n",
      "Epoch 103/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 357.2621 - val_loss: 1373.8547\n",
      "Epoch 104/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 352.7825 - val_loss: 1382.7317\n",
      "Epoch 105/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 348.5205 - val_loss: 1391.5536\n",
      "Epoch 106/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 344.4678 - val_loss: 1400.3145\n",
      "Epoch 107/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 340.6167 - val_loss: 1409.0084\n",
      "Epoch 108/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 336.9591 - val_loss: 1417.6290\n",
      "Epoch 109/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 333.4875 - val_loss: 1426.1708\n",
      "Epoch 110/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 330.1945 - val_loss: 1434.6281\n",
      "Epoch 111/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 327.0726 - val_loss: 1442.9962\n",
      "Epoch 112/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 324.1150 - val_loss: 1451.2708\n",
      "Epoch 113/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 321.3148 - val_loss: 1459.4452\n",
      "Epoch 114/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 318.6652 - val_loss: 1467.5173\n",
      "Epoch 115/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 316.1598 - val_loss: 1475.4822\n",
      "Epoch 116/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 313.7922 - val_loss: 1483.3351\n",
      "Epoch 117/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 311.5565 - val_loss: 1491.0736\n",
      "Epoch 118/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 309.4465 - val_loss: 1498.6940\n",
      "Epoch 119/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 307.4567 - val_loss: 1506.1932\n",
      "Epoch 120/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 305.5812 - val_loss: 1513.5682\n",
      "Epoch 121/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 303.8150 - val_loss: 1520.8169\n",
      "Epoch 122/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 302.1525 - val_loss: 1527.9362\n",
      "Epoch 123/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 300.5889 - val_loss: 1534.9241\n",
      "Epoch 124/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 299.1194 - val_loss: 1541.7788\n",
      "Epoch 125/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 297.7391 - val_loss: 1548.4991\n",
      "Epoch 126/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 296.4435 - val_loss: 1555.0834\n",
      "Epoch 127/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 295.2284 - val_loss: 1561.5295\n",
      "Epoch 128/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 294.0897 - val_loss: 1567.8376\n",
      "Epoch 129/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 293.0230 - val_loss: 1574.0066\n",
      "Epoch 130/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 292.0248 - val_loss: 1580.0359\n",
      "Epoch 131/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 291.0912 - val_loss: 1585.9252\n",
      "Epoch 132/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 290.2188 - val_loss: 1591.6738\n",
      "Epoch 133/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 289.4040 - val_loss: 1597.2814\n",
      "Epoch 134/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 288.6436 - val_loss: 1602.7496\n",
      "Epoch 135/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 287.9345 - val_loss: 1608.0778\n",
      "Epoch 136/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 287.2738 - val_loss: 1613.2664\n",
      "Epoch 137/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 286.6585 - val_loss: 1618.3159\n",
      "Epoch 138/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 286.0861 - val_loss: 1623.2288\n",
      "Epoch 139/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 285.5538 - val_loss: 1628.0038\n",
      "Epoch 140/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 285.0593 - val_loss: 1632.6436\n",
      "Epoch 141/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 284.6003 - val_loss: 1637.1486\n",
      "Epoch 142/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 284.1743 - val_loss: 1641.5198\n",
      "Epoch 143/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 283.7795 - val_loss: 1645.7607\n",
      "Epoch 144/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 283.4137 - val_loss: 1649.8712\n",
      "Epoch 145/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 283.0752 - val_loss: 1653.8529\n",
      "Epoch 146/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 282.7621 - val_loss: 1657.7080\n",
      "Epoch 147/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 282.4727 - val_loss: 1661.4392\n",
      "Epoch 148/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 282.2054 - val_loss: 1665.0476\n",
      "Epoch 149/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 281.9587 - val_loss: 1668.5363\n",
      "Epoch 150/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 281.7312 - val_loss: 1671.9052\n",
      "Epoch 151/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 281.5215 - val_loss: 1675.1587\n",
      "Epoch 152/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 281.3286 - val_loss: 1678.2982\n",
      "Epoch 153/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 281.1508 - val_loss: 1681.3268\n",
      "Epoch 154/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 280.9875 - val_loss: 1684.2448\n",
      "Epoch 155/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 280.8374 - val_loss: 1687.0573\n",
      "Epoch 156/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 280.6996 - val_loss: 1689.7656\n",
      "Epoch 157/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 280.5732 - val_loss: 1692.3712\n",
      "Epoch 158/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 280.4572 - val_loss: 1694.8776\n",
      "Epoch 159/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 280.3511 - val_loss: 1697.2874\n",
      "Epoch 160/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 280.2539 - val_loss: 1699.6023\n",
      "Epoch 161/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 280.1649 - val_loss: 1701.8259\n",
      "Epoch 162/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 280.0836 - val_loss: 1703.9597\n",
      "Epoch 163/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 280.0094 - val_loss: 1706.0073\n",
      "Epoch 164/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.9416 - val_loss: 1707.9705\n",
      "Epoch 165/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.8797 - val_loss: 1709.8514\n",
      "Epoch 166/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.8233 - val_loss: 1711.6526\n",
      "Epoch 167/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.7720 - val_loss: 1713.3774\n",
      "Epoch 168/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.7253 - val_loss: 1715.0281\n",
      "Epoch 169/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.6827 - val_loss: 1716.6060\n",
      "Epoch 170/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 279.6441 - val_loss: 1718.1150\n",
      "Epoch 171/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 279.6090 - val_loss: 1719.5559\n",
      "Epoch 172/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 279.5771 - val_loss: 1720.9325\n",
      "Epoch 173/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.5483 - val_loss: 1722.2456\n",
      "Epoch 174/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.5221 - val_loss: 1723.4980\n",
      "Epoch 175/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.4984 - val_loss: 1724.6932\n",
      "Epoch 176/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.4770 - val_loss: 1725.8315\n",
      "Epoch 177/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 279.4577 - val_loss: 1726.9160\n",
      "Epoch 178/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.4402 - val_loss: 1727.9482\n",
      "Epoch 179/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.4245 - val_loss: 1728.9316\n",
      "Epoch 180/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 279.4102 - val_loss: 1729.8656\n",
      "Epoch 181/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 279.3975 - val_loss: 1730.7545\n",
      "Epoch 182/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 279.3860 - val_loss: 1731.5986\n",
      "Epoch 183/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 279.3757 - val_loss: 1732.4005\n",
      "Epoch 184/500\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 279.3665 - val_loss: 1733.1613\n",
      "Epoch 185/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 279.3582 - val_loss: 1733.8840\n",
      "Epoch 186/500\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 279.3508 - val_loss: 1734.5682\n",
      "Epoch 187/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 279.3442 - val_loss: 1735.2180\n",
      "Epoch 188/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 279.3384 - val_loss: 1735.8340\n",
      "Epoch 189/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 279.3331 - val_loss: 1736.4169\n",
      "Epoch 190/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 279.3286 - val_loss: 1736.9683\n",
      "Epoch 191/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 279.3245 - val_loss: 1737.4906\n",
      "Epoch 192/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 279.3210 - val_loss: 1737.9845\n",
      "Epoch 193/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 279.3178 - val_loss: 1738.4518\n",
      "Epoch 194/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 279.3151 - val_loss: 1738.8928\n",
      "Epoch 195/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 279.3127 - val_loss: 1739.3096\n",
      "Epoch 196/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 279.3107 - val_loss: 1739.7031\n",
      "Epoch 197/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 279.3090 - val_loss: 1740.0752\n",
      "Epoch 198/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 279.3076 - val_loss: 1740.4253\n",
      "Epoch 199/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 279.3063 - val_loss: 1740.7562\n",
      "Epoch 200/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 279.3053 - val_loss: 1741.0671\n",
      "Epoch 201/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 279.3046 - val_loss: 1741.3605\n",
      "Epoch 202/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 279.3040 - val_loss: 1741.6366\n",
      "Epoch 203/500\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 279.3035 - val_loss: 1741.8971\n",
      "Epoch 204/500\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 279.3032 - val_loss: 1742.1420\n",
      "Epoch 205/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 279.3031 - val_loss: 1742.3724\n",
      "Epoch 206/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3030 - val_loss: 1742.5889\n",
      "Epoch 207/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 279.3031 - val_loss: 1742.7922\n",
      "Epoch 208/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 279.3032 - val_loss: 1742.9836\n",
      "Epoch 209/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3035 - val_loss: 1743.1632\n",
      "Epoch 210/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3039 - val_loss: 1743.3311\n",
      "Epoch 211/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3043 - val_loss: 1743.4893\n",
      "Epoch 212/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3047 - val_loss: 1743.6377\n",
      "Epoch 213/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3054 - val_loss: 1743.7767\n",
      "Epoch 214/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 279.3058 - val_loss: 1743.9071\n",
      "Epoch 215/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 279.3065 - val_loss: 1744.0292\n",
      "Epoch 216/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3072 - val_loss: 1744.1428\n",
      "Epoch 217/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3079 - val_loss: 1744.2496\n",
      "Epoch 218/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3086 - val_loss: 1744.3496\n",
      "Epoch 219/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3094 - val_loss: 1744.4431\n",
      "Epoch 220/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 279.3102 - val_loss: 1744.5309\n",
      "Epoch 221/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 279.3110 - val_loss: 1744.6123\n",
      "Epoch 222/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3118 - val_loss: 1744.6884\n",
      "Epoch 223/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3127 - val_loss: 1744.7600\n",
      "Epoch 224/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3135 - val_loss: 1744.8265\n",
      "Epoch 225/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3144 - val_loss: 1744.8885\n",
      "Epoch 226/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3154 - val_loss: 1744.9457\n",
      "Epoch 227/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3162 - val_loss: 1744.9996\n",
      "Epoch 228/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3171 - val_loss: 1745.0505\n",
      "Epoch 229/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3180 - val_loss: 1745.0969\n",
      "Epoch 230/500\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 279.3188 - val_loss: 1745.1403\n",
      "Epoch 231/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3198 - val_loss: 1745.1810\n",
      "Epoch 232/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 279.3207 - val_loss: 1745.2191\n",
      "Epoch 233/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3216 - val_loss: 1745.2544\n",
      "Epoch 234/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3225 - val_loss: 1745.2865\n",
      "Epoch 235/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3235 - val_loss: 1745.3171\n",
      "Epoch 236/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3243 - val_loss: 1745.3442\n",
      "Epoch 237/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3253 - val_loss: 1745.3708\n",
      "Epoch 238/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3262 - val_loss: 1745.3947\n",
      "Epoch 239/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3270 - val_loss: 1745.4169\n",
      "Epoch 240/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3279 - val_loss: 1745.4375\n",
      "Epoch 241/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 279.3288 - val_loss: 1745.4568\n",
      "Epoch 242/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 279.3297 - val_loss: 1745.4744\n",
      "Epoch 243/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 279.3305 - val_loss: 1745.4905\n",
      "Epoch 244/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 279.3314 - val_loss: 1745.5054\n",
      "Epoch 245/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3322 - val_loss: 1745.5193\n",
      "Epoch 246/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3330 - val_loss: 1745.5321\n",
      "Epoch 247/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3339 - val_loss: 1745.5447\n",
      "Epoch 248/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 279.3347 - val_loss: 1745.5552\n",
      "Epoch 249/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3355 - val_loss: 1745.5653\n",
      "Epoch 250/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3363 - val_loss: 1745.5741\n",
      "Epoch 251/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3371 - val_loss: 1745.5830\n",
      "Epoch 252/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 279.3379 - val_loss: 1745.5908\n",
      "Epoch 253/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 279.3386 - val_loss: 1745.5975\n",
      "Epoch 254/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 279.3394 - val_loss: 1745.6044\n",
      "Epoch 255/500\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 279.3401 - val_loss: 1745.6099\n",
      "Epoch 256/500\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 279.3409 - val_loss: 1745.6147\n",
      "Epoch 257/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 279.3416 - val_loss: 1745.6200\n",
      "Epoch 258/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 279.3423 - val_loss: 1745.6246\n",
      "Epoch 259/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 279.3430 - val_loss: 1745.6288\n",
      "Epoch 260/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 279.3437 - val_loss: 1745.6327\n",
      "Epoch 261/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3444 - val_loss: 1745.6362\n",
      "Epoch 262/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 279.3450 - val_loss: 1745.6393\n",
      "Epoch 263/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 279.3457 - val_loss: 1745.6423\n",
      "Epoch 264/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 279.3464 - val_loss: 1745.6447\n",
      "Epoch 265/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 279.3470 - val_loss: 1745.6469\n",
      "Epoch 266/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 279.3476 - val_loss: 1745.6489\n",
      "Epoch 267/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 279.3483 - val_loss: 1745.6504\n",
      "Epoch 268/500\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 279.3488 - val_loss: 1745.6521\n",
      "Epoch 269/500\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 279.3495 - val_loss: 1745.6533\n",
      "Epoch 270/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 279.3501 - val_loss: 1745.6545\n",
      "Epoch 271/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 279.3506 - val_loss: 1745.6550\n",
      "Epoch 272/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 279.3512 - val_loss: 1745.6562\n",
      "Epoch 273/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 279.3517 - val_loss: 1745.6575\n",
      "Epoch 274/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3523 - val_loss: 1745.6576\n",
      "Epoch 275/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3528 - val_loss: 1745.6582\n",
      "Epoch 276/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3533 - val_loss: 1745.6583\n",
      "Epoch 277/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3539 - val_loss: 1745.6591\n",
      "Epoch 278/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 279.3544 - val_loss: 1745.6588\n",
      "Epoch 279/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 279.3549 - val_loss: 1745.6591\n",
      "Epoch 280/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 279.3554 - val_loss: 1745.6592\n",
      "Epoch 281/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 279.3559 - val_loss: 1745.6591\n",
      "Epoch 282/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 279.3563 - val_loss: 1745.6591\n",
      "Epoch 283/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 279.3568 - val_loss: 1745.6591\n",
      "Epoch 284/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3573 - val_loss: 1745.6591\n",
      "Epoch 285/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 279.3577 - val_loss: 1745.6587\n",
      "Epoch 286/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 279.3582 - val_loss: 1745.6587\n",
      "Epoch 287/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3586 - val_loss: 1745.6583\n",
      "Epoch 288/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3590 - val_loss: 1745.6575\n",
      "Epoch 289/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3595 - val_loss: 1745.6564\n",
      "Epoch 290/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3598 - val_loss: 1745.6564\n",
      "Epoch 291/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3603 - val_loss: 1745.6561\n",
      "Epoch 292/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3606 - val_loss: 1745.6550\n",
      "Epoch 293/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3610 - val_loss: 1745.6552\n",
      "Epoch 294/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3614 - val_loss: 1745.6552\n",
      "Epoch 295/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3618 - val_loss: 1745.6547\n",
      "Epoch 296/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3621 - val_loss: 1745.6543\n",
      "Epoch 297/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3624 - val_loss: 1745.6538\n",
      "Epoch 298/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3628 - val_loss: 1745.6537\n",
      "Epoch 299/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3631 - val_loss: 1745.6531\n",
      "Epoch 300/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 279.3635 - val_loss: 1745.6531\n",
      "Epoch 301/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3638 - val_loss: 1745.6521\n",
      "Epoch 302/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3641 - val_loss: 1745.6517\n",
      "Epoch 303/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 279.3644 - val_loss: 1745.6516\n",
      "Epoch 304/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3647 - val_loss: 1745.6510\n",
      "Epoch 305/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3650 - val_loss: 1745.6509\n",
      "Epoch 306/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3653 - val_loss: 1745.6504\n",
      "Epoch 307/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3656 - val_loss: 1745.6497\n",
      "Epoch 308/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3659 - val_loss: 1745.6492\n",
      "Epoch 309/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3662 - val_loss: 1745.6486\n",
      "Epoch 310/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3664 - val_loss: 1745.6476\n",
      "Epoch 311/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3666 - val_loss: 1745.6472\n",
      "Epoch 312/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3669 - val_loss: 1745.6472\n",
      "Epoch 313/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3671 - val_loss: 1745.6465\n",
      "Epoch 314/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3674 - val_loss: 1745.6462\n",
      "Epoch 315/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3677 - val_loss: 1745.6455\n",
      "Epoch 316/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3679 - val_loss: 1745.6451\n",
      "Epoch 317/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3681 - val_loss: 1745.6451\n",
      "Epoch 318/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3683 - val_loss: 1745.6448\n",
      "Epoch 319/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3686 - val_loss: 1745.6447\n",
      "Epoch 320/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 279.3688 - val_loss: 1745.6444\n",
      "Epoch 321/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 279.3690 - val_loss: 1745.6440\n",
      "Epoch 322/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3692 - val_loss: 1745.6438\n",
      "Epoch 323/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3694 - val_loss: 1745.6434\n",
      "Epoch 324/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3696 - val_loss: 1745.6426\n",
      "Epoch 325/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3698 - val_loss: 1745.6420\n",
      "Epoch 326/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3700 - val_loss: 1745.6415\n",
      "Epoch 327/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3701 - val_loss: 1745.6410\n",
      "Epoch 328/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3703 - val_loss: 1745.6407\n",
      "Epoch 329/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3706 - val_loss: 1745.6407\n",
      "Epoch 330/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 279.3708 - val_loss: 1745.6403\n",
      "Epoch 331/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3709 - val_loss: 1745.6403\n",
      "Epoch 332/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3711 - val_loss: 1745.6399\n",
      "Epoch 333/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3712 - val_loss: 1745.6393\n",
      "Epoch 334/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3713 - val_loss: 1745.6385\n",
      "Epoch 335/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3715 - val_loss: 1745.6382\n",
      "Epoch 336/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3716 - val_loss: 1745.6378\n",
      "Epoch 337/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 279.3719 - val_loss: 1745.6373\n",
      "Epoch 338/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 279.3720 - val_loss: 1745.6373\n",
      "Epoch 339/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3721 - val_loss: 1745.6368\n",
      "Epoch 340/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3723 - val_loss: 1745.6368\n",
      "Epoch 341/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3724 - val_loss: 1745.6362\n",
      "Epoch 342/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3726 - val_loss: 1745.6361\n",
      "Epoch 343/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 279.3727 - val_loss: 1745.6361\n",
      "Epoch 344/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3728 - val_loss: 1745.6360\n",
      "Epoch 345/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3729 - val_loss: 1745.6357\n",
      "Epoch 346/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3731 - val_loss: 1745.6360\n",
      "Epoch 347/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3732 - val_loss: 1745.6357\n",
      "Epoch 348/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3733 - val_loss: 1745.6357\n",
      "Epoch 349/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3734 - val_loss: 1745.6356\n",
      "Epoch 350/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3735 - val_loss: 1745.6353\n",
      "Epoch 351/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3735 - val_loss: 1745.6345\n",
      "Epoch 352/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3737 - val_loss: 1745.6348\n",
      "Epoch 353/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3738 - val_loss: 1745.6348\n",
      "Epoch 354/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3739 - val_loss: 1745.6344\n",
      "Epoch 355/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3740 - val_loss: 1745.6337\n",
      "Epoch 356/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 279.3741 - val_loss: 1745.6335\n",
      "Epoch 357/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 279.3742 - val_loss: 1745.6329\n",
      "Epoch 358/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3743 - val_loss: 1745.6327\n",
      "Epoch 359/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3744 - val_loss: 1745.6327\n",
      "Epoch 360/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 279.3745 - val_loss: 1745.6327\n",
      "Epoch 361/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3746 - val_loss: 1745.6323\n",
      "Epoch 362/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 279.3747 - val_loss: 1745.6323\n",
      "Epoch 363/500\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 279.3748 - val_loss: 1745.6320\n",
      "Epoch 364/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3748 - val_loss: 1745.6317\n",
      "Epoch 365/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3749 - val_loss: 1745.6316\n",
      "Epoch 366/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3750 - val_loss: 1745.6316\n",
      "Epoch 367/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3750 - val_loss: 1745.6316\n",
      "Epoch 368/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3752 - val_loss: 1745.6315\n",
      "Epoch 369/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3752 - val_loss: 1745.6315\n",
      "Epoch 370/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3752 - val_loss: 1745.6311\n",
      "Epoch 371/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3754 - val_loss: 1745.6311\n",
      "Epoch 372/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3755 - val_loss: 1745.6306\n",
      "Epoch 373/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3755 - val_loss: 1745.6302\n",
      "Epoch 374/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3756 - val_loss: 1745.6302\n",
      "Epoch 375/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3756 - val_loss: 1745.6299\n",
      "Epoch 376/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3757 - val_loss: 1745.6299\n",
      "Epoch 377/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3757 - val_loss: 1745.6299\n",
      "Epoch 378/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3758 - val_loss: 1745.6296\n",
      "Epoch 379/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3759 - val_loss: 1745.6292\n",
      "Epoch 380/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3759 - val_loss: 1745.6292\n",
      "Epoch 381/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3759 - val_loss: 1745.6288\n",
      "Epoch 382/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3760 - val_loss: 1745.6288\n",
      "Epoch 383/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3761 - val_loss: 1745.6288\n",
      "Epoch 384/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3761 - val_loss: 1745.6290\n",
      "Epoch 385/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3762 - val_loss: 1745.6292\n",
      "Epoch 386/500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 279.3762 - val_loss: 1745.6292\n",
      "Epoch 387/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3763 - val_loss: 1745.6288\n",
      "Epoch 388/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3763 - val_loss: 1745.6287\n",
      "Epoch 389/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 279.3764 - val_loss: 1745.6287\n",
      "Epoch 390/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3764 - val_loss: 1745.6287\n",
      "Epoch 391/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3764 - val_loss: 1745.6282\n",
      "Epoch 392/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3765 - val_loss: 1745.6278\n",
      "Epoch 393/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3765 - val_loss: 1745.6276\n",
      "Epoch 394/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3766 - val_loss: 1745.6278\n",
      "Epoch 395/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3767 - val_loss: 1745.6282\n",
      "Epoch 396/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3766 - val_loss: 1745.6282\n",
      "Epoch 397/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3766 - val_loss: 1745.6278\n",
      "Epoch 398/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3768 - val_loss: 1745.6272\n",
      "Epoch 399/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3768 - val_loss: 1745.6274\n",
      "Epoch 400/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3768 - val_loss: 1745.6279\n",
      "Epoch 401/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3769 - val_loss: 1745.6279\n",
      "Epoch 402/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3769 - val_loss: 1745.6278\n",
      "Epoch 403/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3770 - val_loss: 1745.6282\n",
      "Epoch 404/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3769 - val_loss: 1745.6278\n",
      "Epoch 405/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3770 - val_loss: 1745.6278\n",
      "Epoch 406/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3770 - val_loss: 1745.6276\n",
      "Epoch 407/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3771 - val_loss: 1745.6276\n",
      "Epoch 408/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3771 - val_loss: 1745.6276\n",
      "Epoch 409/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3771 - val_loss: 1745.6279\n",
      "Epoch 410/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3771 - val_loss: 1745.6278\n",
      "Epoch 411/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3772 - val_loss: 1745.6274\n",
      "Epoch 412/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3772 - val_loss: 1745.6274\n",
      "Epoch 413/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 279.3772 - val_loss: 1745.6271\n",
      "Epoch 414/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3773 - val_loss: 1745.6271\n",
      "Epoch 415/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3773 - val_loss: 1745.6271\n",
      "Epoch 416/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3773 - val_loss: 1745.6271\n",
      "Epoch 417/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3773 - val_loss: 1745.6270\n",
      "Epoch 418/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3773 - val_loss: 1745.6267\n",
      "Epoch 419/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3773 - val_loss: 1745.6263\n",
      "Epoch 420/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3774 - val_loss: 1745.6270\n",
      "Epoch 421/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3774 - val_loss: 1745.6270\n",
      "Epoch 422/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3774 - val_loss: 1745.6271\n",
      "Epoch 423/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3775 - val_loss: 1745.6270\n",
      "Epoch 424/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3775 - val_loss: 1745.6270\n",
      "Epoch 425/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3775 - val_loss: 1745.6270\n",
      "Epoch 426/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3775 - val_loss: 1745.6270\n",
      "Epoch 427/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3776 - val_loss: 1745.6270\n",
      "Epoch 428/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3776 - val_loss: 1745.6266\n",
      "Epoch 429/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3775 - val_loss: 1745.6266\n",
      "Epoch 430/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3776 - val_loss: 1745.6263\n",
      "Epoch 431/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3776 - val_loss: 1745.6263\n",
      "Epoch 432/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3776 - val_loss: 1745.6263\n",
      "Epoch 433/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3777 - val_loss: 1745.6263\n",
      "Epoch 434/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3777 - val_loss: 1745.6270\n",
      "Epoch 435/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3777 - val_loss: 1745.6270\n",
      "Epoch 436/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3777 - val_loss: 1745.6271\n",
      "Epoch 437/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 279.3778 - val_loss: 1745.6274\n",
      "Epoch 438/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3777 - val_loss: 1745.6274\n",
      "Epoch 439/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3777 - val_loss: 1745.6274\n",
      "Epoch 440/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3777 - val_loss: 1745.6276\n",
      "Epoch 441/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3777 - val_loss: 1745.6272\n",
      "Epoch 442/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3778 - val_loss: 1745.6267\n",
      "Epoch 443/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3777 - val_loss: 1745.6263\n",
      "Epoch 444/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3777 - val_loss: 1745.6255\n",
      "Epoch 445/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3778 - val_loss: 1745.6254\n",
      "Epoch 446/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3778 - val_loss: 1745.6250\n",
      "Epoch 447/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3778 - val_loss: 1745.6250\n",
      "Epoch 448/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3778 - val_loss: 1745.6251\n",
      "Epoch 449/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3778 - val_loss: 1745.6251\n",
      "Epoch 450/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3778 - val_loss: 1745.6251\n",
      "Epoch 451/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3779 - val_loss: 1745.6254\n",
      "Epoch 452/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3779 - val_loss: 1745.6255\n",
      "Epoch 453/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3779 - val_loss: 1745.6255\n",
      "Epoch 454/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3779 - val_loss: 1745.6255\n",
      "Epoch 455/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3779 - val_loss: 1745.6255\n",
      "Epoch 456/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3780 - val_loss: 1745.6257\n",
      "Epoch 457/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3780 - val_loss: 1745.6257\n",
      "Epoch 458/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3780 - val_loss: 1745.6261\n",
      "Epoch 459/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3780 - val_loss: 1745.6259\n",
      "Epoch 460/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3781 - val_loss: 1745.6261\n",
      "Epoch 461/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 279.3781 - val_loss: 1745.6259\n",
      "Epoch 462/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3780 - val_loss: 1745.6255\n",
      "Epoch 463/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3781 - val_loss: 1745.6255\n",
      "Epoch 464/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3781 - val_loss: 1745.6257\n",
      "Epoch 465/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3781 - val_loss: 1745.6259\n",
      "Epoch 466/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3781 - val_loss: 1745.6261\n",
      "Epoch 467/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3781 - val_loss: 1745.6261\n",
      "Epoch 468/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3781 - val_loss: 1745.6262\n",
      "Epoch 469/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3781 - val_loss: 1745.6263\n",
      "Epoch 470/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3781 - val_loss: 1745.6266\n",
      "Epoch 471/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3781 - val_loss: 1745.6267\n",
      "Epoch 472/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3781 - val_loss: 1745.6266\n",
      "Epoch 473/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3781 - val_loss: 1745.6263\n",
      "Epoch 474/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3781 - val_loss: 1745.6259\n",
      "Epoch 475/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3781 - val_loss: 1745.6254\n",
      "Epoch 476/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3781 - val_loss: 1745.6251\n",
      "Epoch 477/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3781 - val_loss: 1745.6250\n",
      "Epoch 478/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3781 - val_loss: 1745.6250\n",
      "Epoch 479/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3781 - val_loss: 1745.6249\n",
      "Epoch 480/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3781 - val_loss: 1745.6245\n",
      "Epoch 481/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3781 - val_loss: 1745.6245\n",
      "Epoch 482/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3782 - val_loss: 1745.6241\n",
      "Epoch 483/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 279.3782 - val_loss: 1745.6239\n",
      "Epoch 484/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3782 - val_loss: 1745.6239\n",
      "Epoch 485/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3782 - val_loss: 1745.6239\n",
      "Epoch 486/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3782 - val_loss: 1745.6239\n",
      "Epoch 487/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 279.3782 - val_loss: 1745.6241\n",
      "Epoch 488/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3783 - val_loss: 1745.6249\n",
      "Epoch 489/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3783 - val_loss: 1745.6250\n",
      "Epoch 490/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3782 - val_loss: 1745.6250\n",
      "Epoch 491/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3782 - val_loss: 1745.6250\n",
      "Epoch 492/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3782 - val_loss: 1745.6251\n",
      "Epoch 493/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3782 - val_loss: 1745.6254\n",
      "Epoch 494/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3783 - val_loss: 1745.6255\n",
      "Epoch 495/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3782 - val_loss: 1745.6255\n",
      "Epoch 496/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3782 - val_loss: 1745.6257\n",
      "Epoch 497/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3782 - val_loss: 1745.6257\n",
      "Epoch 498/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3782 - val_loss: 1745.6255\n",
      "Epoch 499/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3782 - val_loss: 1745.6259\n",
      "Epoch 500/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 279.3783 - val_loss: 1745.6262\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(85.7336, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0020, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0125, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3008, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 440ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.61120098e+01, 6.58725140e+01, 6.56418067e+01, 6.54119160e+01,\n",
       "        6.51820252e+01, 0.00000000e+00, 9.20709900e-01, 2.56794500e-01,\n",
       "        2.82769000e-02, 4.86232900e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.08772300e-01, 6.64668184e+01, 6.62273226e+01, 6.59788268e+01,\n",
       "        0.00000000e+00, 8.07057600e-02, 6.72595588e+01, 6.68435924e+01,\n",
       "        6.65643908e+01, 6.63248950e+01, 6.60853992e+01, 6.58459034e+01,\n",
       "        6.56165966e+01, 6.53897059e+01, 6.51628151e+01, 0.00000000e+00,\n",
       "        6.26430630e-01, 6.61829715e+01, 6.59434757e+01, 6.57090336e+01,\n",
       "        6.54821429e+01, 6.52552521e+01, 7.00289449e+01, 6.88478058e+01,\n",
       "        6.76139006e+01, 3.52500080e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        6.55625610e+01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.07138500e-02, 1.02327240e-01, 0.00000000e+00, 6.54401261e+01,\n",
       "        6.52132353e+01, 1.84284000e-01, 0.00000000e+00, 6.62361928e+01,\n",
       "        6.59966970e+01, 6.57594538e+01, 6.55325630e+01, 6.53056723e+01,\n",
       "        7.01633987e+01, 6.91167133e+01, 6.78912115e+01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.53981092e+01, 6.51712185e+01,\n",
       "        6.86097106e+01, 6.73996149e+01, 6.61828499e+01, 5.12230400e-01,\n",
       "        6.84295051e+01, 6.71825280e+01, 6.62805439e+01, 3.74972600e-01,\n",
       "        0.00000000e+00, 2.29632500e-01, 3.20638180e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.39785171e-01, 1.08848684e-01, 6.76236689e-01,\n",
       "        6.08381004e+01, 3.41572821e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        6.67996168e-01, 8.15639570e-02, 0.00000000e+00, 1.02618918e-01,\n",
       "        5.90325855e-02, 0.00000000e+00, 0.00000000e+00, 3.11024755e-01,\n",
       "        2.74103492e-01, 0.00000000e+00, 0.00000000e+00, 1.57439277e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.04578055e-01, 6.11705296e-02]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([57.54554212, 57.53652388, 57.52750564, 57.51848739, 57.50946915,\n",
       "       57.50045091, 57.49143267, 57.48241443, 57.47339619, 57.46437795,\n",
       "       57.4553597 , 57.44634146, 57.43732322, 57.42830498, 57.41928674,\n",
       "       57.4102685 , 57.40125026, 57.39223201, 57.38321377, 57.37419553,\n",
       "       57.36517729, 57.35615905, 57.34714081, 57.33812257, 57.32910432,\n",
       "       57.32008608, 57.31106784, 57.3020496 , 57.29303136, 57.28401312,\n",
       "       57.27499488, 57.26597663, 57.25695839, 57.24794015, 57.23892191,\n",
       "       57.22990367, 57.22088543, 57.21186719, 57.20284894, 57.1938307 ,\n",
       "       57.18481246, 57.17579422, 57.16677598, 57.15775774, 57.1487395 ,\n",
       "       57.13972125, 57.13070301, 57.12168477, 57.11266653, 57.10364829,\n",
       "       57.09463005, 57.08561181, 57.07659356, 57.06757532, 57.05855708,\n",
       "       57.04953884, 57.0405206 , 57.03150236, 57.02248412, 57.01346587,\n",
       "       57.00444763, 56.99542939, 56.98641115, 56.97739291, 56.96837467,\n",
       "       56.95935643, 56.95033818, 56.94131994, 56.9323017 , 56.92328346,\n",
       "       56.91426522, 56.90524698, 56.89622874, 56.88721049, 56.87819225,\n",
       "       56.86917401, 56.86015577, 56.85113753, 56.84211929, 56.83310105,\n",
       "       56.8240828 , 56.81506456, 56.80604632, 56.79702808, 56.78800984,\n",
       "       56.7789916 , 56.76997336, 56.76095511, 56.75193687, 56.74291863,\n",
       "       56.73390039, 56.72488215, 56.71586391, 56.70684567, 56.69782742,\n",
       "       56.68880918, 56.67979094, 56.6707727 , 56.66175446, 56.65273622])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.622774564783136\n",
      "35.37679145460413\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
