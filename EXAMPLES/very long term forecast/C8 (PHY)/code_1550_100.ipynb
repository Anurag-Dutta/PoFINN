{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "1645    62.478595\n",
       "1646    62.473926\n",
       "1647    62.469258\n",
       "1648    62.464589\n",
       "1649    62.459921\n",
       "Name: C8, Length: 1650, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c8_interpolated_1550_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "1545     0.920710\n",
       "1546     0.626431\n",
       "1547     0.000000\n",
       "1548     0.000000\n",
       "1549     0.465921\n",
       "Name: C8, Length: 1550, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1550)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.987115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.774230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.561345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.348459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.200000  0.000298\n",
       "1     87.987115  0.000298\n",
       "2     87.774230  0.000297\n",
       "3     87.561345  0.000297\n",
       "4     87.348459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 85.7336  # Value for C0\n",
    "K0 = -0.0020  # Value for K0\n",
    "K1 = 0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0125    # Value for b\n",
    "c = 2.3008    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.200000    87.987115    87.774230    87.561345    87.348459   \n",
      "351    87.987115    87.774230    87.561345    87.348459    87.135574   \n",
      "352    87.774230    87.561345    87.348459    87.135574    86.922689   \n",
      "353    87.561345    87.348459    87.135574    86.922689    86.709804   \n",
      "354    87.348459    87.135574    86.922689    86.709804    86.496919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.135574    86.922689    86.709804    86.496919    86.294538  ...   \n",
      "351    86.922689    86.709804    86.496919    86.294538    86.221709  ...   \n",
      "352    86.709804    86.496919    86.294538    86.221709    86.148880  ...   \n",
      "353    86.496919    86.294538    86.221709    86.148880    86.076050  ...   \n",
      "354    86.294538    86.221709    86.148880    86.076050    86.003221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   76.094351    0.000263   76.058870    0.000263   76.023389    0.000263   \n",
      "351   76.058870    0.000263   76.023389    0.000263   75.987909    0.000262   \n",
      "352   76.023389    0.000263   75.987909    0.000262   75.952428    0.000262   \n",
      "353   75.987909    0.000262   75.952428    0.000262   75.916947    0.000262   \n",
      "354   75.952428    0.000262   75.916947    0.000262   75.881466    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   75.987909    0.000262   75.952428    0.000262  \n",
      "351   75.952428    0.000262   75.916947    0.000262  \n",
      "352   75.916947    0.000262   75.881466    0.000262  \n",
      "353   75.881466    0.000262   75.845985    0.000262  \n",
      "354   75.845985    0.000262   75.810504    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1100, 1, 251) (1100, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "14/14 [==============================] - 2s 33ms/step - loss: 5092.3496 - val_loss: 3952.5195\n",
      "Epoch 2/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5016.7178 - val_loss: 3908.9268\n",
      "Epoch 3/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4965.1826 - val_loss: 3865.5295\n",
      "Epoch 4/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4913.9595 - val_loss: 3822.5046\n",
      "Epoch 5/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4863.1494 - val_loss: 3779.8716\n",
      "Epoch 6/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4812.7573 - val_loss: 3737.6270\n",
      "Epoch 7/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4762.7783 - val_loss: 3695.7632\n",
      "Epoch 8/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4713.2036 - val_loss: 3654.2744\n",
      "Epoch 9/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4664.0278 - val_loss: 3613.1555\n",
      "Epoch 10/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4615.2451 - val_loss: 3572.4023\n",
      "Epoch 11/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4566.8516 - val_loss: 3532.0115\n",
      "Epoch 12/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 4518.8428 - val_loss: 3491.9792\n",
      "Epoch 13/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 4471.2173 - val_loss: 3452.3040\n",
      "Epoch 14/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4423.9702 - val_loss: 3412.9827\n",
      "Epoch 15/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4377.0996 - val_loss: 3374.0117\n",
      "Epoch 16/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4330.6035 - val_loss: 3335.3892\n",
      "Epoch 17/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4284.4766 - val_loss: 3297.1130\n",
      "Epoch 18/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4238.7207 - val_loss: 3259.1804\n",
      "Epoch 19/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4193.3301 - val_loss: 3221.5901\n",
      "Epoch 20/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4148.3037 - val_loss: 3184.3389\n",
      "Epoch 21/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4103.6392 - val_loss: 3147.4243\n",
      "Epoch 22/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 4059.3337 - val_loss: 3110.8450\n",
      "Epoch 23/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4015.3857 - val_loss: 3074.5984\n",
      "Epoch 24/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3971.7930 - val_loss: 3038.6829\n",
      "Epoch 25/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3928.5525 - val_loss: 3003.0957\n",
      "Epoch 26/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3885.6636 - val_loss: 2967.8357\n",
      "Epoch 27/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3843.1233 - val_loss: 2932.9004\n",
      "Epoch 28/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3800.9292 - val_loss: 2898.2878\n",
      "Epoch 29/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3759.0796 - val_loss: 2863.9951\n",
      "Epoch 30/500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 3717.5723 - val_loss: 2830.0220\n",
      "Epoch 31/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3676.4062 - val_loss: 2796.3647\n",
      "Epoch 32/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3635.5779 - val_loss: 2763.0222\n",
      "Epoch 33/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3595.0857 - val_loss: 2729.9927\n",
      "Epoch 34/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3554.9282 - val_loss: 2697.2744\n",
      "Epoch 35/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3515.1030 - val_loss: 2664.8643\n",
      "Epoch 36/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3475.6086 - val_loss: 2632.7620\n",
      "Epoch 37/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3436.4426 - val_loss: 2600.9648\n",
      "Epoch 38/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3397.6038 - val_loss: 2569.4707\n",
      "Epoch 39/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3359.0889 - val_loss: 2538.2778\n",
      "Epoch 40/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3320.8970 - val_loss: 2507.3848\n",
      "Epoch 41/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3283.0259 - val_loss: 2476.7893\n",
      "Epoch 42/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3245.4734 - val_loss: 2446.4895\n",
      "Epoch 43/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3208.2390 - val_loss: 2416.4844\n",
      "Epoch 44/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3171.3188 - val_loss: 2386.7715\n",
      "Epoch 45/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3134.7131 - val_loss: 2357.3486\n",
      "Epoch 46/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3098.4185 - val_loss: 2328.2148\n",
      "Epoch 47/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3062.4338 - val_loss: 2299.3679\n",
      "Epoch 48/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3026.7573 - val_loss: 2270.8066\n",
      "Epoch 49/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2991.3867 - val_loss: 2242.5278\n",
      "Epoch 50/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2956.3201 - val_loss: 2214.5308\n",
      "Epoch 51/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2921.5562 - val_loss: 2186.8147\n",
      "Epoch 52/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2887.0933 - val_loss: 2159.3757\n",
      "Epoch 53/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2852.9292 - val_loss: 2132.2139\n",
      "Epoch 54/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2819.0625 - val_loss: 2105.3259\n",
      "Epoch 55/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2785.4915 - val_loss: 2078.7117\n",
      "Epoch 56/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2752.2139 - val_loss: 2052.3687\n",
      "Epoch 57/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2719.2288 - val_loss: 2026.2957\n",
      "Epoch 58/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2686.5334 - val_loss: 2000.4901\n",
      "Epoch 59/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2654.1274 - val_loss: 1974.9510\n",
      "Epoch 60/500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2622.0083 - val_loss: 1949.6765\n",
      "Epoch 61/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2590.1736 - val_loss: 1924.6653\n",
      "Epoch 62/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2558.6230 - val_loss: 1899.9153\n",
      "Epoch 63/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2527.3547 - val_loss: 1875.4247\n",
      "Epoch 64/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2496.3662 - val_loss: 1851.1925\n",
      "Epoch 65/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2465.6562 - val_loss: 1827.2169\n",
      "Epoch 66/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2435.2229 - val_loss: 1803.4956\n",
      "Epoch 67/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2405.0657 - val_loss: 1780.0281\n",
      "Epoch 68/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2375.1819 - val_loss: 1756.8125\n",
      "Epoch 69/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2345.5701 - val_loss: 1733.8463\n",
      "Epoch 70/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2316.2285 - val_loss: 1711.1294\n",
      "Epoch 71/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2287.1560 - val_loss: 1688.6583\n",
      "Epoch 72/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2258.3503 - val_loss: 1666.4332\n",
      "Epoch 73/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2229.8108 - val_loss: 1644.4519\n",
      "Epoch 74/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2201.5349 - val_loss: 1622.7120\n",
      "Epoch 75/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2173.5217 - val_loss: 1601.2130\n",
      "Epoch 76/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2145.7693 - val_loss: 1579.9540\n",
      "Epoch 77/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2118.2761 - val_loss: 1558.9316\n",
      "Epoch 78/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2091.0405 - val_loss: 1538.1458\n",
      "Epoch 79/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2064.0613 - val_loss: 1517.5944\n",
      "Epoch 80/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2037.3368 - val_loss: 1497.2759\n",
      "Epoch 81/500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2010.8657 - val_loss: 1477.1887\n",
      "Epoch 82/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1984.6458 - val_loss: 1457.3315\n",
      "Epoch 83/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1958.6759 - val_loss: 1437.7028\n",
      "Epoch 84/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1932.9546 - val_loss: 1418.3007\n",
      "Epoch 85/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1907.4807 - val_loss: 1399.1249\n",
      "Epoch 86/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1882.2524 - val_loss: 1380.1726\n",
      "Epoch 87/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1857.2681 - val_loss: 1361.4429\n",
      "Epoch 88/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1832.5261 - val_loss: 1342.9338\n",
      "Epoch 89/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1808.0254 - val_loss: 1324.6449\n",
      "Epoch 90/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1783.7645 - val_loss: 1306.5737\n",
      "Epoch 91/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1759.7413 - val_loss: 1288.7200\n",
      "Epoch 92/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1735.9556 - val_loss: 1271.0808\n",
      "Epoch 93/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1712.4045 - val_loss: 1253.6555\n",
      "Epoch 94/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1689.0875 - val_loss: 1236.4425\n",
      "Epoch 95/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1666.0026 - val_loss: 1219.4403\n",
      "Epoch 96/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1643.1487 - val_loss: 1202.6476\n",
      "Epoch 97/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1620.5240 - val_loss: 1186.0631\n",
      "Epoch 98/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1598.1276 - val_loss: 1169.6852\n",
      "Epoch 99/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1575.9578 - val_loss: 1153.5127\n",
      "Epoch 100/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1554.0132 - val_loss: 1137.5435\n",
      "Epoch 101/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1532.2922 - val_loss: 1121.7770\n",
      "Epoch 102/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1510.7935 - val_loss: 1106.2111\n",
      "Epoch 103/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1489.5160 - val_loss: 1090.8453\n",
      "Epoch 104/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1468.4580 - val_loss: 1075.6775\n",
      "Epoch 105/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1447.6184 - val_loss: 1060.7068\n",
      "Epoch 106/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1426.9951 - val_loss: 1045.9312\n",
      "Epoch 107/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1406.5874 - val_loss: 1031.3496\n",
      "Epoch 108/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1386.3942 - val_loss: 1016.9608\n",
      "Epoch 109/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1366.4132 - val_loss: 1002.7630\n",
      "Epoch 110/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1346.6433 - val_loss: 988.7556\n",
      "Epoch 111/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1327.0836 - val_loss: 974.9365\n",
      "Epoch 112/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1307.7323 - val_loss: 961.3049\n",
      "Epoch 113/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1288.5880 - val_loss: 947.8585\n",
      "Epoch 114/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1269.6495 - val_loss: 934.5971\n",
      "Epoch 115/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1250.9161 - val_loss: 921.5188\n",
      "Epoch 116/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1232.3857 - val_loss: 908.6227\n",
      "Epoch 117/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1214.0573 - val_loss: 895.9066\n",
      "Epoch 118/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1195.9293 - val_loss: 883.3702\n",
      "Epoch 119/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1178.0002 - val_loss: 871.0115\n",
      "Epoch 120/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1160.2690 - val_loss: 858.8292\n",
      "Epoch 121/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1142.7347 - val_loss: 846.8218\n",
      "Epoch 122/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1125.3955 - val_loss: 834.9882\n",
      "Epoch 123/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1108.2501 - val_loss: 823.3274\n",
      "Epoch 124/500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1091.2972 - val_loss: 811.8378\n",
      "Epoch 125/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1074.5354 - val_loss: 800.5173\n",
      "Epoch 126/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1057.9636 - val_loss: 789.3663\n",
      "Epoch 127/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1041.5808 - val_loss: 778.3823\n",
      "Epoch 128/500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1025.3853 - val_loss: 767.5643\n",
      "Epoch 129/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1009.3758 - val_loss: 756.9108\n",
      "Epoch 130/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 993.5511 - val_loss: 746.4209\n",
      "Epoch 131/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 977.9103 - val_loss: 736.0934\n",
      "Epoch 132/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 962.4515 - val_loss: 725.9261\n",
      "Epoch 133/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 947.1738 - val_loss: 715.9183\n",
      "Epoch 134/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 932.0757 - val_loss: 706.0693\n",
      "Epoch 135/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 917.1561 - val_loss: 696.3769\n",
      "Epoch 136/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 902.4135 - val_loss: 686.8400\n",
      "Epoch 137/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 887.8471 - val_loss: 677.4578\n",
      "Epoch 138/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 873.4553 - val_loss: 668.2289\n",
      "Epoch 139/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 859.2369 - val_loss: 659.1513\n",
      "Epoch 140/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 845.1905 - val_loss: 650.2245\n",
      "Epoch 141/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 831.3154 - val_loss: 641.4470\n",
      "Epoch 142/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 817.6096 - val_loss: 632.8174\n",
      "Epoch 143/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 804.0723 - val_loss: 624.3348\n",
      "Epoch 144/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 790.7022 - val_loss: 615.9974\n",
      "Epoch 145/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 777.4982 - val_loss: 607.8049\n",
      "Epoch 146/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 764.4588 - val_loss: 599.7549\n",
      "Epoch 147/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 751.5830 - val_loss: 591.8471\n",
      "Epoch 148/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 738.8694 - val_loss: 584.0796\n",
      "Epoch 149/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 726.3169 - val_loss: 576.4512\n",
      "Epoch 150/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 713.9240 - val_loss: 568.9607\n",
      "Epoch 151/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 701.6896 - val_loss: 561.6073\n",
      "Epoch 152/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 689.6130 - val_loss: 554.3893\n",
      "Epoch 153/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 677.6923 - val_loss: 547.3058\n",
      "Epoch 154/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 665.9265 - val_loss: 540.3551\n",
      "Epoch 155/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 654.3145 - val_loss: 533.5364\n",
      "Epoch 156/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 642.8550 - val_loss: 526.8481\n",
      "Epoch 157/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 631.5470 - val_loss: 520.2896\n",
      "Epoch 158/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 620.3890 - val_loss: 513.8588\n",
      "Epoch 159/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 609.3798 - val_loss: 507.5554\n",
      "Epoch 160/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 598.5186 - val_loss: 501.3773\n",
      "Epoch 161/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 587.8038 - val_loss: 495.3241\n",
      "Epoch 162/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 577.2343 - val_loss: 489.3935\n",
      "Epoch 163/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 566.8090 - val_loss: 483.5856\n",
      "Epoch 164/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 556.5269 - val_loss: 477.8981\n",
      "Epoch 165/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 546.3864 - val_loss: 472.3303\n",
      "Epoch 166/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 536.3866 - val_loss: 466.8811\n",
      "Epoch 167/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 526.5262 - val_loss: 461.5489\n",
      "Epoch 168/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 516.8041 - val_loss: 456.3328\n",
      "Epoch 169/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 507.2190 - val_loss: 451.2315\n",
      "Epoch 170/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 497.7698 - val_loss: 446.2438\n",
      "Epoch 171/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 488.4554 - val_loss: 441.3685\n",
      "Epoch 172/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 479.2745 - val_loss: 436.6042\n",
      "Epoch 173/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 470.2261 - val_loss: 431.9503\n",
      "Epoch 174/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 461.3089 - val_loss: 427.4049\n",
      "Epoch 175/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 452.5218 - val_loss: 422.9673\n",
      "Epoch 176/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 443.8638 - val_loss: 418.6360\n",
      "Epoch 177/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 435.3336 - val_loss: 414.4101\n",
      "Epoch 178/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 426.9298 - val_loss: 410.2880\n",
      "Epoch 179/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 418.6516 - val_loss: 406.2690\n",
      "Epoch 180/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 410.4977 - val_loss: 402.3516\n",
      "Epoch 181/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 402.4671 - val_loss: 398.5346\n",
      "Epoch 182/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 394.5585 - val_loss: 394.8173\n",
      "Epoch 183/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 386.7707 - val_loss: 391.1979\n",
      "Epoch 184/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 379.1029 - val_loss: 387.6755\n",
      "Epoch 185/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 371.5537 - val_loss: 384.2492\n",
      "Epoch 186/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 364.1220 - val_loss: 380.9172\n",
      "Epoch 187/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 356.8066 - val_loss: 377.6790\n",
      "Epoch 188/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 349.6064 - val_loss: 374.5330\n",
      "Epoch 189/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 342.5204 - val_loss: 371.4781\n",
      "Epoch 190/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 335.5475 - val_loss: 368.5134\n",
      "Epoch 191/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 328.6863 - val_loss: 365.6374\n",
      "Epoch 192/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 321.9359 - val_loss: 362.8494\n",
      "Epoch 193/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 315.2954 - val_loss: 360.1477\n",
      "Epoch 194/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 308.7632 - val_loss: 357.5315\n",
      "Epoch 195/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 302.3384 - val_loss: 354.9996\n",
      "Epoch 196/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 296.0199 - val_loss: 352.5508\n",
      "Epoch 197/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 289.8069 - val_loss: 350.1843\n",
      "Epoch 198/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 283.6982 - val_loss: 347.8984\n",
      "Epoch 199/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 277.6920 - val_loss: 345.6923\n",
      "Epoch 200/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 271.7880 - val_loss: 343.5650\n",
      "Epoch 201/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 265.9849 - val_loss: 341.5150\n",
      "Epoch 202/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 260.2815 - val_loss: 339.5414\n",
      "Epoch 203/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 254.6768 - val_loss: 337.6431\n",
      "Epoch 204/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 249.1694 - val_loss: 335.8188\n",
      "Epoch 205/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 243.7588 - val_loss: 334.0677\n",
      "Epoch 206/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 238.4436 - val_loss: 332.3885\n",
      "Epoch 207/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 233.2228 - val_loss: 330.7799\n",
      "Epoch 208/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 228.0951 - val_loss: 329.2411\n",
      "Epoch 209/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 223.0597 - val_loss: 327.7710\n",
      "Epoch 210/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 218.1155 - val_loss: 326.3684\n",
      "Epoch 211/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 213.2614 - val_loss: 325.0321\n",
      "Epoch 212/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 208.4960 - val_loss: 323.7611\n",
      "Epoch 213/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 203.8188 - val_loss: 322.5544\n",
      "Epoch 214/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 199.2283 - val_loss: 321.4109\n",
      "Epoch 215/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 194.7240 - val_loss: 320.3297\n",
      "Epoch 216/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 190.3045 - val_loss: 319.3092\n",
      "Epoch 217/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 185.9687 - val_loss: 318.3488\n",
      "Epoch 218/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 181.7155 - val_loss: 317.4472\n",
      "Epoch 219/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 177.5441 - val_loss: 316.6035\n",
      "Epoch 220/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 173.4535 - val_loss: 315.8166\n",
      "Epoch 221/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 169.4424 - val_loss: 315.0853\n",
      "Epoch 222/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 165.5099 - val_loss: 314.4088\n",
      "Epoch 223/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 161.6552 - val_loss: 313.7860\n",
      "Epoch 224/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 157.8771 - val_loss: 313.2155\n",
      "Epoch 225/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 154.1746 - val_loss: 312.6969\n",
      "Epoch 226/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 150.5466 - val_loss: 312.2287\n",
      "Epoch 227/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 146.9921 - val_loss: 311.8101\n",
      "Epoch 228/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 143.5103 - val_loss: 311.4401\n",
      "Epoch 229/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 140.1003 - val_loss: 311.1174\n",
      "Epoch 230/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 136.7606 - val_loss: 310.8413\n",
      "Epoch 231/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 133.4907 - val_loss: 310.6108\n",
      "Epoch 232/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 130.2894 - val_loss: 310.4246\n",
      "Epoch 233/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 127.1559 - val_loss: 310.2821\n",
      "Epoch 234/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 124.0891 - val_loss: 310.1821\n",
      "Epoch 235/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 121.0881 - val_loss: 310.1237\n",
      "Epoch 236/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 118.1519 - val_loss: 310.1058\n",
      "Epoch 237/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 115.2794 - val_loss: 310.1277\n",
      "Epoch 238/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 112.4699 - val_loss: 310.1882\n",
      "Epoch 239/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 109.7223 - val_loss: 310.2863\n",
      "Epoch 240/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 107.0359 - val_loss: 310.4214\n",
      "Epoch 241/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 104.4094 - val_loss: 310.5922\n",
      "Epoch 242/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 101.8422 - val_loss: 310.7980\n",
      "Epoch 243/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 99.3332 - val_loss: 311.0378\n",
      "Epoch 244/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 96.8816 - val_loss: 311.3107\n",
      "Epoch 245/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 94.4862 - val_loss: 311.6157\n",
      "Epoch 246/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 92.1464 - val_loss: 311.9520\n",
      "Epoch 247/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 89.8613 - val_loss: 312.3187\n",
      "Epoch 248/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 87.6298 - val_loss: 312.7149\n",
      "Epoch 249/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 85.4511 - val_loss: 313.1398\n",
      "Epoch 250/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 83.3243 - val_loss: 313.5924\n",
      "Epoch 251/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 81.2488 - val_loss: 314.0718\n",
      "Epoch 252/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 79.2234 - val_loss: 314.5772\n",
      "Epoch 253/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 77.2475 - val_loss: 315.1079\n",
      "Epoch 254/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 75.3200 - val_loss: 315.6629\n",
      "Epoch 255/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 73.4401 - val_loss: 316.2415\n",
      "Epoch 256/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 71.6070 - val_loss: 316.8427\n",
      "Epoch 257/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 69.8200 - val_loss: 317.4657\n",
      "Epoch 258/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 68.0780 - val_loss: 318.1099\n",
      "Epoch 259/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 66.3805 - val_loss: 318.7743\n",
      "Epoch 260/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 64.7265 - val_loss: 319.4583\n",
      "Epoch 261/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 63.1152 - val_loss: 320.1609\n",
      "Epoch 262/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 61.5457 - val_loss: 320.8814\n",
      "Epoch 263/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 60.0173 - val_loss: 321.6192\n",
      "Epoch 264/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 58.5294 - val_loss: 322.3734\n",
      "Epoch 265/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 57.0809 - val_loss: 323.1434\n",
      "Epoch 266/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 55.6713 - val_loss: 323.9282\n",
      "Epoch 267/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 54.2997 - val_loss: 324.7273\n",
      "Epoch 268/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 52.9654 - val_loss: 325.5398\n",
      "Epoch 269/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 51.6676 - val_loss: 326.3654\n",
      "Epoch 270/500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 50.4055 - val_loss: 327.2031\n",
      "Epoch 271/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 49.1785 - val_loss: 328.0520\n",
      "Epoch 272/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 47.9859 - val_loss: 328.9117\n",
      "Epoch 273/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 46.8271 - val_loss: 329.7815\n",
      "Epoch 274/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 45.7011 - val_loss: 330.6609\n",
      "Epoch 275/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 44.6073 - val_loss: 331.5491\n",
      "Epoch 276/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 43.5450 - val_loss: 332.4453\n",
      "Epoch 277/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 42.5136 - val_loss: 333.3493\n",
      "Epoch 278/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 41.5123 - val_loss: 334.2601\n",
      "Epoch 279/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 40.5405 - val_loss: 335.1774\n",
      "Epoch 280/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 39.5975 - val_loss: 336.1005\n",
      "Epoch 281/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 38.6826 - val_loss: 337.0288\n",
      "Epoch 282/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 37.7953 - val_loss: 337.9618\n",
      "Epoch 283/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 36.9349 - val_loss: 338.8987\n",
      "Epoch 284/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 36.1009 - val_loss: 339.8389\n",
      "Epoch 285/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 35.2926 - val_loss: 340.7824\n",
      "Epoch 286/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 34.5093 - val_loss: 341.7284\n",
      "Epoch 287/500\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 33.7505 - val_loss: 342.6761\n",
      "Epoch 288/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 33.0155 - val_loss: 343.6253\n",
      "Epoch 289/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 32.3039 - val_loss: 344.5757\n",
      "Epoch 290/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 31.6149 - val_loss: 345.5264\n",
      "Epoch 291/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 30.9481 - val_loss: 346.4771\n",
      "Epoch 292/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 30.3030 - val_loss: 347.4272\n",
      "Epoch 293/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 29.6789 - val_loss: 348.3766\n",
      "Epoch 294/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 29.0753 - val_loss: 349.3247\n",
      "Epoch 295/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 28.4916 - val_loss: 350.2710\n",
      "Epoch 296/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 27.9274 - val_loss: 351.2151\n",
      "Epoch 297/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 27.3822 - val_loss: 352.1565\n",
      "Epoch 298/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 26.8556 - val_loss: 353.0949\n",
      "Epoch 299/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 26.3468 - val_loss: 354.0301\n",
      "Epoch 300/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 25.8555 - val_loss: 354.9616\n",
      "Epoch 301/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 25.3811 - val_loss: 355.8890\n",
      "Epoch 302/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 24.9233 - val_loss: 356.8120\n",
      "Epoch 303/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 24.4815 - val_loss: 357.7301\n",
      "Epoch 304/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 24.0554 - val_loss: 358.6432\n",
      "Epoch 305/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 23.6444 - val_loss: 359.5510\n",
      "Epoch 306/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 23.2482 - val_loss: 360.4528\n",
      "Epoch 307/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 22.8662 - val_loss: 361.3490\n",
      "Epoch 308/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 22.4982 - val_loss: 362.2388\n",
      "Epoch 309/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 22.1436 - val_loss: 363.1220\n",
      "Epoch 310/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 21.8020 - val_loss: 363.9985\n",
      "Epoch 311/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 21.4731 - val_loss: 364.8677\n",
      "Epoch 312/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 21.1566 - val_loss: 365.7296\n",
      "Epoch 313/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 20.8520 - val_loss: 366.5840\n",
      "Epoch 314/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 20.5589 - val_loss: 367.4303\n",
      "Epoch 315/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 20.2771 - val_loss: 368.2693\n",
      "Epoch 316/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 20.0060 - val_loss: 369.0995\n",
      "Epoch 317/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 19.7455 - val_loss: 369.9218\n",
      "Epoch 318/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 19.4951 - val_loss: 370.7354\n",
      "Epoch 319/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 19.2545 - val_loss: 371.5403\n",
      "Epoch 320/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 19.0235 - val_loss: 372.3361\n",
      "Epoch 321/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 18.8016 - val_loss: 373.1230\n",
      "Epoch 322/500\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 18.5887 - val_loss: 373.9008\n",
      "Epoch 323/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 18.3843 - val_loss: 374.6690\n",
      "Epoch 324/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 18.1883 - val_loss: 375.4282\n",
      "Epoch 325/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 18.0002 - val_loss: 376.1778\n",
      "Epoch 326/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 17.8199 - val_loss: 376.9174\n",
      "Epoch 327/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 17.6470 - val_loss: 377.6474\n",
      "Epoch 328/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 17.4814 - val_loss: 378.3678\n",
      "Epoch 329/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 17.3227 - val_loss: 379.0779\n",
      "Epoch 330/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 17.1707 - val_loss: 379.7782\n",
      "Epoch 331/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 17.0252 - val_loss: 380.4685\n",
      "Epoch 332/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.8859 - val_loss: 381.1487\n",
      "Epoch 333/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 16.7526 - val_loss: 381.8184\n",
      "Epoch 334/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.6251 - val_loss: 382.4779\n",
      "Epoch 335/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.5031 - val_loss: 383.1273\n",
      "Epoch 336/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.3866 - val_loss: 383.7666\n",
      "Epoch 337/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.2751 - val_loss: 384.3954\n",
      "Epoch 338/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 16.1686 - val_loss: 385.0142\n",
      "Epoch 339/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.0668 - val_loss: 385.6222\n",
      "Epoch 340/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 15.9697 - val_loss: 386.2202\n",
      "Epoch 341/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 15.8769 - val_loss: 386.8079\n",
      "Epoch 342/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 15.7883 - val_loss: 387.3854\n",
      "Epoch 343/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 15.7038 - val_loss: 387.9526\n",
      "Epoch 344/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 15.6233 - val_loss: 388.5093\n",
      "Epoch 345/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 15.5464 - val_loss: 389.0558\n",
      "Epoch 346/500\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 15.4731 - val_loss: 389.5922\n",
      "Epoch 347/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 15.4033 - val_loss: 390.1185\n",
      "Epoch 348/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 15.3367 - val_loss: 390.6349\n",
      "Epoch 349/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 15.2733 - val_loss: 391.1411\n",
      "Epoch 350/500\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 15.2129 - val_loss: 391.6370\n",
      "Epoch 351/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 15.1555 - val_loss: 392.1234\n",
      "Epoch 352/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 15.1007 - val_loss: 392.5996\n",
      "Epoch 353/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 15.0487 - val_loss: 393.0659\n",
      "Epoch 354/500\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 14.9992 - val_loss: 393.5228\n",
      "Epoch 355/500\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 14.9521 - val_loss: 393.9698\n",
      "Epoch 356/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 14.9073 - val_loss: 394.4073\n",
      "Epoch 357/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 14.8648 - val_loss: 394.8353\n",
      "Epoch 358/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 14.8243 - val_loss: 395.2540\n",
      "Epoch 359/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 14.7859 - val_loss: 395.6635\n",
      "Epoch 360/500\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 14.7494 - val_loss: 396.0638\n",
      "Epoch 361/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 14.7147 - val_loss: 396.4550\n",
      "Epoch 362/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 14.6818 - val_loss: 396.8374\n",
      "Epoch 363/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 14.6506 - val_loss: 397.2105\n",
      "Epoch 364/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 14.6210 - val_loss: 397.5753\n",
      "Epoch 365/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 14.5928 - val_loss: 397.9311\n",
      "Epoch 366/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 14.5661 - val_loss: 398.2785\n",
      "Epoch 367/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 14.5408 - val_loss: 398.6172\n",
      "Epoch 368/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 14.5169 - val_loss: 398.9476\n",
      "Epoch 369/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 14.4942 - val_loss: 399.2700\n",
      "Epoch 370/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.4726 - val_loss: 399.5841\n",
      "Epoch 371/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.4522 - val_loss: 399.8900\n",
      "Epoch 372/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 14.4329 - val_loss: 400.1881\n",
      "Epoch 373/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.4146 - val_loss: 400.4787\n",
      "Epoch 374/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 14.3973 - val_loss: 400.7616\n",
      "Epoch 375/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 14.3809 - val_loss: 401.0368\n",
      "Epoch 376/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 14.3654 - val_loss: 401.3049\n",
      "Epoch 377/500\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 14.3507 - val_loss: 401.5655\n",
      "Epoch 378/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 14.3368 - val_loss: 401.8190\n",
      "Epoch 379/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 14.3237 - val_loss: 402.0658\n",
      "Epoch 380/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 14.3112 - val_loss: 402.3054\n",
      "Epoch 381/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 14.2995 - val_loss: 402.5385\n",
      "Epoch 382/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 14.2883 - val_loss: 402.7647\n",
      "Epoch 383/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 14.2778 - val_loss: 402.9850\n",
      "Epoch 384/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 14.2679 - val_loss: 403.1986\n",
      "Epoch 385/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.2585 - val_loss: 403.4058\n",
      "Epoch 386/500\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 14.2496 - val_loss: 403.6071\n",
      "Epoch 387/500\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 14.2412 - val_loss: 403.8021\n",
      "Epoch 388/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.2333 - val_loss: 403.9915\n",
      "Epoch 389/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 14.2258 - val_loss: 404.1753\n",
      "Epoch 390/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.2188 - val_loss: 404.3532\n",
      "Epoch 391/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.2121 - val_loss: 404.5259\n",
      "Epoch 392/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.2058 - val_loss: 404.6931\n",
      "Epoch 393/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1998 - val_loss: 404.8549\n",
      "Epoch 394/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1942 - val_loss: 405.0117\n",
      "Epoch 395/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1889 - val_loss: 405.1635\n",
      "Epoch 396/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1839 - val_loss: 405.3102\n",
      "Epoch 397/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1792 - val_loss: 405.4523\n",
      "Epoch 398/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1747 - val_loss: 405.5896\n",
      "Epoch 399/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1705 - val_loss: 405.7226\n",
      "Epoch 400/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1666 - val_loss: 405.8513\n",
      "Epoch 401/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1628 - val_loss: 405.9752\n",
      "Epoch 402/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1593 - val_loss: 406.0952\n",
      "Epoch 403/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1559 - val_loss: 406.2109\n",
      "Epoch 404/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1528 - val_loss: 406.3229\n",
      "Epoch 405/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1498 - val_loss: 406.4312\n",
      "Epoch 406/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1470 - val_loss: 406.5351\n",
      "Epoch 407/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1444 - val_loss: 406.6359\n",
      "Epoch 408/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1419 - val_loss: 406.7329\n",
      "Epoch 409/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1395 - val_loss: 406.8263\n",
      "Epoch 410/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1373 - val_loss: 406.9161\n",
      "Epoch 411/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1353 - val_loss: 407.0028\n",
      "Epoch 412/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1333 - val_loss: 407.0868\n",
      "Epoch 413/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1314 - val_loss: 407.1667\n",
      "Epoch 414/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1297 - val_loss: 407.2446\n",
      "Epoch 415/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1281 - val_loss: 407.3192\n",
      "Epoch 416/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1265 - val_loss: 407.3909\n",
      "Epoch 417/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1250 - val_loss: 407.4599\n",
      "Epoch 418/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1237 - val_loss: 407.5262\n",
      "Epoch 419/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 14.1224 - val_loss: 407.5901\n",
      "Epoch 420/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 14.1212 - val_loss: 407.6515\n",
      "Epoch 421/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1200 - val_loss: 407.7105\n",
      "Epoch 422/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1190 - val_loss: 407.7671\n",
      "Epoch 423/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1180 - val_loss: 407.8217\n",
      "Epoch 424/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1170 - val_loss: 407.8739\n",
      "Epoch 425/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1161 - val_loss: 407.9237\n",
      "Epoch 426/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 14.1153 - val_loss: 407.9719\n",
      "Epoch 427/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 14.1145 - val_loss: 408.0178\n",
      "Epoch 428/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 14.1138 - val_loss: 408.0620\n",
      "Epoch 429/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1131 - val_loss: 408.1044\n",
      "Epoch 430/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1125 - val_loss: 408.1453\n",
      "Epoch 431/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1118 - val_loss: 408.1843\n",
      "Epoch 432/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1112 - val_loss: 408.2213\n",
      "Epoch 433/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1107 - val_loss: 408.2567\n",
      "Epoch 434/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1103 - val_loss: 408.2909\n",
      "Epoch 435/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1098 - val_loss: 408.3233\n",
      "Epoch 436/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1094 - val_loss: 408.3544\n",
      "Epoch 437/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1090 - val_loss: 408.3840\n",
      "Epoch 438/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1086 - val_loss: 408.4124\n",
      "Epoch 439/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1083 - val_loss: 408.4395\n",
      "Epoch 440/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1080 - val_loss: 408.4652\n",
      "Epoch 441/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1077 - val_loss: 408.4899\n",
      "Epoch 442/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1075 - val_loss: 408.5135\n",
      "Epoch 443/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1072 - val_loss: 408.5359\n",
      "Epoch 444/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1070 - val_loss: 408.5573\n",
      "Epoch 445/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1068 - val_loss: 408.5778\n",
      "Epoch 446/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1066 - val_loss: 408.5972\n",
      "Epoch 447/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 14.1065 - val_loss: 408.6157\n",
      "Epoch 448/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1063 - val_loss: 408.6333\n",
      "Epoch 449/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1062 - val_loss: 408.6500\n",
      "Epoch 450/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1061 - val_loss: 408.6655\n",
      "Epoch 451/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1060 - val_loss: 408.6810\n",
      "Epoch 452/500\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 14.1059 - val_loss: 408.6955\n",
      "Epoch 453/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 14.1058 - val_loss: 408.7091\n",
      "Epoch 454/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1058 - val_loss: 408.7221\n",
      "Epoch 455/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1058 - val_loss: 408.7345\n",
      "Epoch 456/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1057 - val_loss: 408.7462\n",
      "Epoch 457/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1057 - val_loss: 408.7574\n",
      "Epoch 458/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1057 - val_loss: 408.7680\n",
      "Epoch 459/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1057 - val_loss: 408.7782\n",
      "Epoch 460/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1057 - val_loss: 408.7876\n",
      "Epoch 461/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1057 - val_loss: 408.7964\n",
      "Epoch 462/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1057 - val_loss: 408.8048\n",
      "Epoch 463/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1058 - val_loss: 408.8127\n",
      "Epoch 464/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1058 - val_loss: 408.8203\n",
      "Epoch 465/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 14.1059 - val_loss: 408.8275\n",
      "Epoch 466/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1059 - val_loss: 408.8346\n",
      "Epoch 467/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1060 - val_loss: 408.8408\n",
      "Epoch 468/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1061 - val_loss: 408.8466\n",
      "Epoch 469/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1061 - val_loss: 408.8524\n",
      "Epoch 470/500\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 14.1062 - val_loss: 408.8577\n",
      "Epoch 471/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1063 - val_loss: 408.8625\n",
      "Epoch 472/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1064 - val_loss: 408.8672\n",
      "Epoch 473/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1065 - val_loss: 408.8721\n",
      "Epoch 474/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1066 - val_loss: 408.8763\n",
      "Epoch 475/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1067 - val_loss: 408.8800\n",
      "Epoch 476/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1068 - val_loss: 408.8839\n",
      "Epoch 477/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1069 - val_loss: 408.8872\n",
      "Epoch 478/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1070 - val_loss: 408.8904\n",
      "Epoch 479/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1072 - val_loss: 408.8931\n",
      "Epoch 480/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1073 - val_loss: 408.8961\n",
      "Epoch 481/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1074 - val_loss: 408.8984\n",
      "Epoch 482/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1075 - val_loss: 408.9008\n",
      "Epoch 483/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1076 - val_loss: 408.9030\n",
      "Epoch 484/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1078 - val_loss: 408.9050\n",
      "Epoch 485/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1080 - val_loss: 408.9071\n",
      "Epoch 486/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1081 - val_loss: 408.9088\n",
      "Epoch 487/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1082 - val_loss: 408.9107\n",
      "Epoch 488/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1084 - val_loss: 408.9122\n",
      "Epoch 489/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1085 - val_loss: 408.9134\n",
      "Epoch 490/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1086 - val_loss: 408.9145\n",
      "Epoch 491/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1088 - val_loss: 408.9156\n",
      "Epoch 492/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1090 - val_loss: 408.9171\n",
      "Epoch 493/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1091 - val_loss: 408.9180\n",
      "Epoch 494/500\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 14.1092 - val_loss: 408.9187\n",
      "Epoch 495/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1094 - val_loss: 408.9195\n",
      "Epoch 496/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1095 - val_loss: 408.9201\n",
      "Epoch 497/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.1097 - val_loss: 408.9207\n",
      "Epoch 498/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 14.1099 - val_loss: 408.9212\n",
      "Epoch 499/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1100 - val_loss: 408.9218\n",
      "Epoch 500/500\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 14.1102 - val_loss: 408.9225\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(85.7336, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0020, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0125, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3008, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 399ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[66.31602474, 66.2894141 , 66.26280345, 66.23619281, 66.20958217,\n",
       "        66.18297152, 66.15636088, 66.12975023, 66.10313959, 66.07652894,\n",
       "        66.0499183 , 66.02330766, 65.99669701, 65.97008637, 65.94347572,\n",
       "        65.91686508, 65.89025444, 65.86364379, 65.83703315, 65.8104225 ,\n",
       "        65.78466387, 65.75945378, 65.7342437 , 65.70903361, 65.68382353,\n",
       "        65.65861345, 65.63340336, 65.60819328, 65.58298319, 65.55777311,\n",
       "        65.53256303, 65.50735294, 65.48214286, 65.45693277, 65.43172269,\n",
       "        65.40651261, 65.38130252, 65.35609244, 65.33088235, 65.30567227,\n",
       "        65.28046218, 65.2552521 , 65.23004202, 65.20483193, 65.17962185,\n",
       "        65.15441176, 65.12920168, 70.2306256 , 70.1633987 , 70.0961718 ,\n",
       "        70.0289449 , 69.923436  , 69.7890823 , 69.6545285 , 69.5200747 ,\n",
       "        69.3856209 , 69.2511671 , 69.1167133 , 68.9822596 , 68.8478058 ,\n",
       "        68.713352  , 68.5788982 , 68.4444444 , 68.3071779 , 68.1685224 ,\n",
       "        68.0298669 , 67.8912115 , 67.752556  , 67.6139006 , 67.4752451 ,\n",
       "        67.3365896 , 67.1979342 , 67.0592787 , 73.4346924 ,  0.36256564,\n",
       "         0.        ,  0.28288567,  0.35250008,  0.        ,  0.59674394,\n",
       "        63.53670883,  0.        ,  0.        ,  0.07534569,  0.        ,\n",
       "         0.35469484,  0.        ,  0.4269039 ,  0.16883293,  0.        ,\n",
       "         0.        ,  0.4475165 ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.17208737,  0.        ,  0.        ,  0.26660696]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([63.23305322, 63.22184874, 63.21064426, 63.19943978, 63.18823529,\n",
       "       63.17703081, 63.16582633, 63.15462185, 63.14341737, 63.13221289,\n",
       "       63.1210084 , 63.10980392, 63.09859944, 63.08739496, 63.07619048,\n",
       "       63.06498599, 63.05378151, 63.04257703, 63.03137255, 63.02016807,\n",
       "       63.00896359, 62.9977591 , 62.98655462, 62.97535014, 62.96414566,\n",
       "       62.95294118, 62.94173669, 62.93053221, 62.91932773, 62.90812325,\n",
       "       62.89691877, 62.88571429, 62.8745098 , 62.86330532, 62.85210084,\n",
       "       62.84089636, 62.82969188, 62.81848739, 62.80728291, 62.79607843,\n",
       "       62.78487395, 62.77366947, 62.76246499, 62.7512605 , 62.74005602,\n",
       "       62.72885154, 62.71764706, 62.70644258, 62.69801587, 62.69334734,\n",
       "       62.6886788 , 62.68401027, 62.67934174, 62.6746732 , 62.67000467,\n",
       "       62.66533613, 62.6606676 , 62.65599907, 62.65133053, 62.646662  ,\n",
       "       62.64199346, 62.63732493, 62.6326564 , 62.62798786, 62.62331933,\n",
       "       62.61865079, 62.61398226, 62.60931373, 62.60464519, 62.59997666,\n",
       "       62.59530812, 62.59063959, 62.58597106, 62.58130252, 62.57663399,\n",
       "       62.57196545, 62.56729692, 62.56262838, 62.55795985, 62.55329132,\n",
       "       62.54862278, 62.54395425, 62.53928571, 62.53461718, 62.52994865,\n",
       "       62.52528011, 62.52061158, 62.51594304, 62.51127451, 62.50660598,\n",
       "       62.50193744, 62.49726891, 62.49260037, 62.48793184, 62.48326331,\n",
       "       62.47859477, 62.47392624, 62.4692577 , 62.46458917, 62.45992063])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.422819294661917\n",
      "18.588241079804806\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
