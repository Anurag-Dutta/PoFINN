{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "1745    61.823483\n",
       "1746    61.814146\n",
       "1747    61.804809\n",
       "1748    61.795472\n",
       "1749    61.786134\n",
       "Name: C8, Length: 1750, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c8_interpolated_1650_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "1645     0.000000\n",
       "1646     0.172087\n",
       "1647     0.000000\n",
       "1648     0.000000\n",
       "1649     0.266607\n",
       "Name: C8, Length: 1650, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1650)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.987115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.774230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.561345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.348459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.200000  0.000298\n",
       "1     87.987115  0.000298\n",
       "2     87.774230  0.000297\n",
       "3     87.561345  0.000297\n",
       "4     87.348459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 85.7336  # Value for C0\n",
    "K0 = -0.0020  # Value for K0\n",
    "K1 = 0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0125    # Value for b\n",
    "c = 2.3008    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.200000    87.987115    87.774230    87.561345    87.348459   \n",
      "351    87.987115    87.774230    87.561345    87.348459    87.135574   \n",
      "352    87.774230    87.561345    87.348459    87.135574    86.922689   \n",
      "353    87.561345    87.348459    87.135574    86.922689    86.709804   \n",
      "354    87.348459    87.135574    86.922689    86.709804    86.496919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.135574    86.922689    86.709804    86.496919    86.294538  ...   \n",
      "351    86.922689    86.709804    86.496919    86.294538    86.221709  ...   \n",
      "352    86.709804    86.496919    86.294538    86.221709    86.148880  ...   \n",
      "353    86.496919    86.294538    86.221709    86.148880    86.076050  ...   \n",
      "354    86.294538    86.221709    86.148880    86.076050    86.003221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   76.094351    0.000263   76.058870    0.000263   76.023389    0.000263   \n",
      "351   76.058870    0.000263   76.023389    0.000263   75.987909    0.000262   \n",
      "352   76.023389    0.000263   75.987909    0.000262   75.952428    0.000262   \n",
      "353   75.987909    0.000262   75.952428    0.000262   75.916947    0.000262   \n",
      "354   75.952428    0.000262   75.916947    0.000262   75.881466    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   75.987909    0.000262   75.952428    0.000262  \n",
      "351   75.952428    0.000262   75.916947    0.000262  \n",
      "352   75.916947    0.000262   75.881466    0.000262  \n",
      "353   75.881466    0.000262   75.845985    0.000262  \n",
      "354   75.845985    0.000262   75.810504    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 1, 251) (1200, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "15/15 [==============================] - 2s 43ms/step - loss: 4976.0615 - val_loss: 3702.4021\n",
      "Epoch 2/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4924.3950 - val_loss: 3669.8958\n",
      "Epoch 3/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4881.1030 - val_loss: 3637.5505\n",
      "Epoch 4/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 4838.0229 - val_loss: 3605.4180\n",
      "Epoch 5/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 4795.1978 - val_loss: 3573.5125\n",
      "Epoch 6/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4752.6392 - val_loss: 3541.8389\n",
      "Epoch 7/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4710.3535 - val_loss: 3510.3989\n",
      "Epoch 8/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4668.3423 - val_loss: 3479.1936\n",
      "Epoch 9/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4626.6064 - val_loss: 3448.2214\n",
      "Epoch 10/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4585.1440 - val_loss: 3417.4827\n",
      "Epoch 11/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4543.9575 - val_loss: 3386.9763\n",
      "Epoch 12/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4503.0454 - val_loss: 3356.7014\n",
      "Epoch 13/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4462.4043 - val_loss: 3326.6570\n",
      "Epoch 14/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4422.0361 - val_loss: 3296.8420\n",
      "Epoch 15/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4381.9380 - val_loss: 3267.2561\n",
      "Epoch 16/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4342.1094 - val_loss: 3237.8970\n",
      "Epoch 17/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4302.5488 - val_loss: 3208.7639\n",
      "Epoch 18/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4263.2559 - val_loss: 3179.8562\n",
      "Epoch 19/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4224.2280 - val_loss: 3151.1724\n",
      "Epoch 20/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4185.4648 - val_loss: 3122.7114\n",
      "Epoch 21/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4146.9644 - val_loss: 3094.4717\n",
      "Epoch 22/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4108.7266 - val_loss: 3066.4526\n",
      "Epoch 23/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4070.7490 - val_loss: 3038.6531\n",
      "Epoch 24/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4033.0308 - val_loss: 3011.0715\n",
      "Epoch 25/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3995.5701 - val_loss: 2983.7073\n",
      "Epoch 26/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3958.3669 - val_loss: 2956.5583\n",
      "Epoch 27/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3921.4185 - val_loss: 2929.6248\n",
      "Epoch 28/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3884.7239 - val_loss: 2902.9048\n",
      "Epoch 29/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3848.2830 - val_loss: 2876.3967\n",
      "Epoch 30/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3812.0930 - val_loss: 2850.1003\n",
      "Epoch 31/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3776.1536 - val_loss: 2824.0146\n",
      "Epoch 32/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3740.4631 - val_loss: 2798.1375\n",
      "Epoch 33/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3705.0208 - val_loss: 2772.4688\n",
      "Epoch 34/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3669.8240 - val_loss: 2747.0066\n",
      "Epoch 35/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3634.8728 - val_loss: 2721.7505\n",
      "Epoch 36/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3600.1655 - val_loss: 2696.6992\n",
      "Epoch 37/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3565.7017 - val_loss: 2671.8516\n",
      "Epoch 38/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 3531.4788 - val_loss: 2647.2065\n",
      "Epoch 39/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 3497.4956 - val_loss: 2622.7625\n",
      "Epoch 40/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3463.7529 - val_loss: 2598.5195\n",
      "Epoch 41/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3430.2468 - val_loss: 2574.4756\n",
      "Epoch 42/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3396.9778 - val_loss: 2550.6304\n",
      "Epoch 43/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3363.9443 - val_loss: 2526.9819\n",
      "Epoch 44/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3331.1448 - val_loss: 2503.5303\n",
      "Epoch 45/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3298.5789 - val_loss: 2480.2737\n",
      "Epoch 46/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3266.2444 - val_loss: 2457.2114\n",
      "Epoch 47/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3234.1409 - val_loss: 2434.3420\n",
      "Epoch 48/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3202.2664 - val_loss: 2411.6646\n",
      "Epoch 49/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3170.6206 - val_loss: 2389.1787\n",
      "Epoch 50/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3139.2026 - val_loss: 2366.8823\n",
      "Epoch 51/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3108.0100 - val_loss: 2344.7751\n",
      "Epoch 52/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3077.0427 - val_loss: 2322.8564\n",
      "Epoch 53/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3046.2986 - val_loss: 2301.1248\n",
      "Epoch 54/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3015.7778 - val_loss: 2279.5791\n",
      "Epoch 55/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2985.4780 - val_loss: 2258.2180\n",
      "Epoch 56/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2955.3989 - val_loss: 2237.0417\n",
      "Epoch 57/500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 2925.5388 - val_loss: 2216.0483\n",
      "Epoch 58/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2895.8975 - val_loss: 2195.2373\n",
      "Epoch 59/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2866.4731 - val_loss: 2174.6072\n",
      "Epoch 60/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2837.2649 - val_loss: 2154.1577\n",
      "Epoch 61/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2808.2710 - val_loss: 2133.8870\n",
      "Epoch 62/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2779.4912 - val_loss: 2113.7952\n",
      "Epoch 63/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2750.9241 - val_loss: 2093.8806\n",
      "Epoch 64/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2722.5691 - val_loss: 2074.1423\n",
      "Epoch 65/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2694.4243 - val_loss: 2054.5798\n",
      "Epoch 66/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2666.4893 - val_loss: 2035.1921\n",
      "Epoch 67/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2638.7622 - val_loss: 2015.9779\n",
      "Epoch 68/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2611.2444 - val_loss: 1996.9363\n",
      "Epoch 69/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2583.9314 - val_loss: 1978.0669\n",
      "Epoch 70/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2556.8242 - val_loss: 1959.3683\n",
      "Epoch 71/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2529.9216 - val_loss: 1940.8396\n",
      "Epoch 72/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2503.2227 - val_loss: 1922.4801\n",
      "Epoch 73/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2476.7253 - val_loss: 1904.2888\n",
      "Epoch 74/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2450.4294 - val_loss: 1886.2649\n",
      "Epoch 75/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2424.3337 - val_loss: 1868.4073\n",
      "Epoch 76/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2398.4377 - val_loss: 1850.7153\n",
      "Epoch 77/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2372.7400 - val_loss: 1833.1884\n",
      "Epoch 78/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2347.2395 - val_loss: 1815.8247\n",
      "Epoch 79/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2321.9360 - val_loss: 1798.6245\n",
      "Epoch 80/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2296.8271 - val_loss: 1781.5859\n",
      "Epoch 81/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2271.9133 - val_loss: 1764.7094\n",
      "Epoch 82/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2247.1926 - val_loss: 1747.9927\n",
      "Epoch 83/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2222.6643 - val_loss: 1731.4353\n",
      "Epoch 84/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2198.3274 - val_loss: 1715.0369\n",
      "Epoch 85/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2174.1807 - val_loss: 1698.7961\n",
      "Epoch 86/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2150.2241 - val_loss: 1682.7118\n",
      "Epoch 87/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2126.4558 - val_loss: 1666.7844\n",
      "Epoch 88/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2102.8755 - val_loss: 1651.0117\n",
      "Epoch 89/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2079.4817 - val_loss: 1635.3938\n",
      "Epoch 90/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2056.2739 - val_loss: 1619.9291\n",
      "Epoch 91/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2033.2506 - val_loss: 1604.6174\n",
      "Epoch 92/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2010.4116 - val_loss: 1589.4578\n",
      "Epoch 93/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1987.7556 - val_loss: 1574.4490\n",
      "Epoch 94/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1965.2816 - val_loss: 1559.5905\n",
      "Epoch 95/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1942.9884 - val_loss: 1544.8821\n",
      "Epoch 96/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1920.8761 - val_loss: 1530.3215\n",
      "Epoch 97/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1898.9429 - val_loss: 1515.9094\n",
      "Epoch 98/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1877.1886 - val_loss: 1501.6438\n",
      "Epoch 99/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1855.6108 - val_loss: 1487.5249\n",
      "Epoch 100/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1834.2108 - val_loss: 1473.5516\n",
      "Epoch 101/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1812.9865 - val_loss: 1459.7227\n",
      "Epoch 102/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1791.9368 - val_loss: 1446.0378\n",
      "Epoch 103/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1771.0612 - val_loss: 1432.4960\n",
      "Epoch 104/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 1750.3588 - val_loss: 1419.0966\n",
      "Epoch 105/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1729.8284 - val_loss: 1405.8381\n",
      "Epoch 106/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1709.4695 - val_loss: 1392.7211\n",
      "Epoch 107/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1689.2815 - val_loss: 1379.7438\n",
      "Epoch 108/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1669.2633 - val_loss: 1366.9060\n",
      "Epoch 109/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1649.4141 - val_loss: 1354.2065\n",
      "Epoch 110/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1629.7325 - val_loss: 1341.6445\n",
      "Epoch 111/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1610.2177 - val_loss: 1329.2192\n",
      "Epoch 112/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1590.8695 - val_loss: 1316.9304\n",
      "Epoch 113/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1571.6866 - val_loss: 1304.7771\n",
      "Epoch 114/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1552.6686 - val_loss: 1292.7581\n",
      "Epoch 115/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1533.8145 - val_loss: 1280.8730\n",
      "Epoch 116/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 1515.1230 - val_loss: 1269.1212\n",
      "Epoch 117/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 1496.5939 - val_loss: 1257.5016\n",
      "Epoch 118/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 1478.2260 - val_loss: 1246.0137\n",
      "Epoch 119/500\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1460.0182 - val_loss: 1234.6562\n",
      "Epoch 120/500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 1441.9703 - val_loss: 1223.4293\n",
      "Epoch 121/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 1424.0813 - val_loss: 1212.3317\n",
      "Epoch 122/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 1406.3499 - val_loss: 1201.3622\n",
      "Epoch 123/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 1388.7758 - val_loss: 1190.5209\n",
      "Epoch 124/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 1371.3578 - val_loss: 1179.8068\n",
      "Epoch 125/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 1354.0959 - val_loss: 1169.2190\n",
      "Epoch 126/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 1336.9882 - val_loss: 1158.7567\n",
      "Epoch 127/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 1320.0345 - val_loss: 1148.4196\n",
      "Epoch 128/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 1303.2341 - val_loss: 1138.2065\n",
      "Epoch 129/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 1286.5857 - val_loss: 1128.1169\n",
      "Epoch 130/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 1270.0891 - val_loss: 1118.1501\n",
      "Epoch 131/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 1253.7433 - val_loss: 1108.3053\n",
      "Epoch 132/500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1237.5471 - val_loss: 1098.5820\n",
      "Epoch 133/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1221.5004 - val_loss: 1088.9792\n",
      "Epoch 134/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 1205.6022 - val_loss: 1079.4963\n",
      "Epoch 135/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 1189.8517 - val_loss: 1070.1324\n",
      "Epoch 136/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 1174.2477 - val_loss: 1060.8872\n",
      "Epoch 137/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 1158.7899 - val_loss: 1051.7594\n",
      "Epoch 138/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 1143.4772 - val_loss: 1042.7487\n",
      "Epoch 139/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1128.3088 - val_loss: 1033.8541\n",
      "Epoch 140/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1113.2844 - val_loss: 1025.0754\n",
      "Epoch 141/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1098.4028 - val_loss: 1016.4113\n",
      "Epoch 142/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1083.6635 - val_loss: 1007.8613\n",
      "Epoch 143/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 1069.0649 - val_loss: 999.4249\n",
      "Epoch 144/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 1054.6077 - val_loss: 991.1013\n",
      "Epoch 145/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 1040.2902 - val_loss: 982.8895\n",
      "Epoch 146/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 1026.1118 - val_loss: 974.7894\n",
      "Epoch 147/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 1012.0718 - val_loss: 966.7996\n",
      "Epoch 148/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 998.1694 - val_loss: 958.9198\n",
      "Epoch 149/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 984.4036 - val_loss: 951.1492\n",
      "Epoch 150/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 970.7742 - val_loss: 943.4873\n",
      "Epoch 151/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 957.2797 - val_loss: 935.9330\n",
      "Epoch 152/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 943.9200 - val_loss: 928.4860\n",
      "Epoch 153/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 930.6940 - val_loss: 921.1453\n",
      "Epoch 154/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 917.6013 - val_loss: 913.9105\n",
      "Epoch 155/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 904.6409 - val_loss: 906.7806\n",
      "Epoch 156/500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 891.8121 - val_loss: 899.7551\n",
      "Epoch 157/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 879.1140 - val_loss: 892.8333\n",
      "Epoch 158/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 866.5458 - val_loss: 886.0143\n",
      "Epoch 159/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 854.1074 - val_loss: 879.2978\n",
      "Epoch 160/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 841.7975 - val_loss: 872.6828\n",
      "Epoch 161/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 829.6154 - val_loss: 866.1685\n",
      "Epoch 162/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 817.5605 - val_loss: 859.7547\n",
      "Epoch 163/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 805.6321 - val_loss: 853.4406\n",
      "Epoch 164/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 793.8294 - val_loss: 847.2250\n",
      "Epoch 165/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 782.1516 - val_loss: 841.1074\n",
      "Epoch 166/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 770.5977 - val_loss: 835.0873\n",
      "Epoch 167/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 759.1672 - val_loss: 829.1639\n",
      "Epoch 168/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 747.8597 - val_loss: 823.3369\n",
      "Epoch 169/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 736.6743 - val_loss: 817.6049\n",
      "Epoch 170/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 725.6098 - val_loss: 811.9678\n",
      "Epoch 171/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 714.6664 - val_loss: 806.4246\n",
      "Epoch 172/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 703.8426 - val_loss: 800.9750\n",
      "Epoch 173/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 693.1383 - val_loss: 795.6180\n",
      "Epoch 174/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 682.5521 - val_loss: 790.3531\n",
      "Epoch 175/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 672.0840 - val_loss: 785.1791\n",
      "Epoch 176/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 661.7321 - val_loss: 780.0959\n",
      "Epoch 177/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 651.4966 - val_loss: 775.1026\n",
      "Epoch 178/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 641.3766 - val_loss: 770.1985\n",
      "Epoch 179/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 631.3718 - val_loss: 765.3829\n",
      "Epoch 180/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 621.4808 - val_loss: 760.6555\n",
      "Epoch 181/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 611.7034 - val_loss: 756.0149\n",
      "Epoch 182/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 602.0381 - val_loss: 751.4609\n",
      "Epoch 183/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 592.4851 - val_loss: 746.9927\n",
      "Epoch 184/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 583.0433 - val_loss: 742.6098\n",
      "Epoch 185/500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 573.7122 - val_loss: 738.3112\n",
      "Epoch 186/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 564.4907 - val_loss: 734.0966\n",
      "Epoch 187/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 555.3784 - val_loss: 729.9650\n",
      "Epoch 188/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 546.3746 - val_loss: 725.9159\n",
      "Epoch 189/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 537.4783 - val_loss: 721.9483\n",
      "Epoch 190/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 528.6889 - val_loss: 718.0622\n",
      "Epoch 191/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 520.0058 - val_loss: 714.2562\n",
      "Epoch 192/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 511.4284 - val_loss: 710.5298\n",
      "Epoch 193/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 502.9556 - val_loss: 706.8826\n",
      "Epoch 194/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 494.5868 - val_loss: 703.3134\n",
      "Epoch 195/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 486.3213 - val_loss: 699.8221\n",
      "Epoch 196/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 478.1584 - val_loss: 696.4075\n",
      "Epoch 197/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 470.0976 - val_loss: 693.0694\n",
      "Epoch 198/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 462.1382 - val_loss: 689.8070\n",
      "Epoch 199/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 454.2794 - val_loss: 686.6193\n",
      "Epoch 200/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 446.5201 - val_loss: 683.5059\n",
      "Epoch 201/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 438.8600 - val_loss: 680.4660\n",
      "Epoch 202/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 431.2984 - val_loss: 677.4991\n",
      "Epoch 203/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 423.8346 - val_loss: 674.6044\n",
      "Epoch 204/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 416.4677 - val_loss: 671.7809\n",
      "Epoch 205/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 409.1972 - val_loss: 669.0286\n",
      "Epoch 206/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 402.0222 - val_loss: 666.3464\n",
      "Epoch 207/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 394.9424 - val_loss: 663.7336\n",
      "Epoch 208/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 387.9565 - val_loss: 661.1893\n",
      "Epoch 209/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 381.0639 - val_loss: 658.7134\n",
      "Epoch 210/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 374.2641 - val_loss: 656.3047\n",
      "Epoch 211/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 367.5564 - val_loss: 653.9628\n",
      "Epoch 212/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 360.9401 - val_loss: 651.6870\n",
      "Epoch 213/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 354.4147 - val_loss: 649.4764\n",
      "Epoch 214/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 347.9791 - val_loss: 647.3307\n",
      "Epoch 215/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 341.6329 - val_loss: 645.2489\n",
      "Epoch 216/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 335.3751 - val_loss: 643.2303\n",
      "Epoch 217/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 329.2048 - val_loss: 641.2743\n",
      "Epoch 218/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 323.1217 - val_loss: 639.3802\n",
      "Epoch 219/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 317.1254 - val_loss: 637.5475\n",
      "Epoch 220/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 311.2145 - val_loss: 635.7755\n",
      "Epoch 221/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 305.3889 - val_loss: 634.0631\n",
      "Epoch 222/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 299.6473 - val_loss: 632.4100\n",
      "Epoch 223/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 293.9896 - val_loss: 630.8153\n",
      "Epoch 224/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 288.4146 - val_loss: 629.2786\n",
      "Epoch 225/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 282.9219 - val_loss: 627.7990\n",
      "Epoch 226/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 277.5106 - val_loss: 626.3759\n",
      "Epoch 227/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 272.1803 - val_loss: 625.0085\n",
      "Epoch 228/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 266.9299 - val_loss: 623.6963\n",
      "Epoch 229/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 261.7590 - val_loss: 622.4384\n",
      "Epoch 230/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 256.6667 - val_loss: 621.2344\n",
      "Epoch 231/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 251.6526 - val_loss: 620.0833\n",
      "Epoch 232/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 246.7159 - val_loss: 618.9846\n",
      "Epoch 233/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 241.8557 - val_loss: 617.9376\n",
      "Epoch 234/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 237.0716 - val_loss: 616.9415\n",
      "Epoch 235/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 232.3624 - val_loss: 615.9960\n",
      "Epoch 236/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 227.7276 - val_loss: 615.0999\n",
      "Epoch 237/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 223.1670 - val_loss: 614.2529\n",
      "Epoch 238/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 218.6795 - val_loss: 613.4541\n",
      "Epoch 239/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 214.2644 - val_loss: 612.7029\n",
      "Epoch 240/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 209.9210 - val_loss: 611.9987\n",
      "Epoch 241/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 205.6486 - val_loss: 611.3408\n",
      "Epoch 242/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 201.4468 - val_loss: 610.7283\n",
      "Epoch 243/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 197.3144 - val_loss: 610.1608\n",
      "Epoch 244/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 193.2510 - val_loss: 609.6375\n",
      "Epoch 245/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 189.2559 - val_loss: 609.1577\n",
      "Epoch 246/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 185.3285 - val_loss: 608.7207\n",
      "Epoch 247/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 181.4678 - val_loss: 608.3260\n",
      "Epoch 248/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 177.6733 - val_loss: 607.9727\n",
      "Epoch 249/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 173.9444 - val_loss: 607.6604\n",
      "Epoch 250/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 170.2804 - val_loss: 607.3881\n",
      "Epoch 251/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 166.6803 - val_loss: 607.1555\n",
      "Epoch 252/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 163.1436 - val_loss: 606.9616\n",
      "Epoch 253/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 159.6700 - val_loss: 606.8058\n",
      "Epoch 254/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 156.2583 - val_loss: 606.6876\n",
      "Epoch 255/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 152.9082 - val_loss: 606.6063\n",
      "Epoch 256/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 149.6185 - val_loss: 606.5611\n",
      "Epoch 257/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 146.3889 - val_loss: 606.5514\n",
      "Epoch 258/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 143.2187 - val_loss: 606.5764\n",
      "Epoch 259/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 140.1071 - val_loss: 606.6358\n",
      "Epoch 260/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 137.0535 - val_loss: 606.7286\n",
      "Epoch 261/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 134.0572 - val_loss: 606.8543\n",
      "Epoch 262/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 131.1176 - val_loss: 607.0123\n",
      "Epoch 263/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 128.2340 - val_loss: 607.2017\n",
      "Epoch 264/500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 125.4058 - val_loss: 607.4221\n",
      "Epoch 265/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 122.6321 - val_loss: 607.6727\n",
      "Epoch 266/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 119.9123 - val_loss: 607.9530\n",
      "Epoch 267/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 117.2459 - val_loss: 608.2623\n",
      "Epoch 268/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 114.6321 - val_loss: 608.5997\n",
      "Epoch 269/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 112.0703 - val_loss: 608.9650\n",
      "Epoch 270/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 109.5597 - val_loss: 609.3573\n",
      "Epoch 271/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 107.0998 - val_loss: 609.7760\n",
      "Epoch 272/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 104.6898 - val_loss: 610.2205\n",
      "Epoch 273/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 102.3292 - val_loss: 610.6901\n",
      "Epoch 274/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 100.0173 - val_loss: 611.1842\n",
      "Epoch 275/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 97.7533 - val_loss: 611.7022\n",
      "Epoch 276/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 95.5368 - val_loss: 612.2435\n",
      "Epoch 277/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 93.3671 - val_loss: 612.8074\n",
      "Epoch 278/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 91.2433 - val_loss: 613.3935\n",
      "Epoch 279/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 89.1650 - val_loss: 614.0008\n",
      "Epoch 280/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 87.1316 - val_loss: 614.6291\n",
      "Epoch 281/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 85.1421 - val_loss: 615.2775\n",
      "Epoch 282/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 83.1963 - val_loss: 615.9456\n",
      "Epoch 283/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 81.2934 - val_loss: 616.6326\n",
      "Epoch 284/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 79.4329 - val_loss: 617.3380\n",
      "Epoch 285/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 77.6138 - val_loss: 618.0612\n",
      "Epoch 286/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 75.8358 - val_loss: 618.8017\n",
      "Epoch 287/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 74.0982 - val_loss: 619.5588\n",
      "Epoch 288/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 72.4004 - val_loss: 620.3320\n",
      "Epoch 289/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 70.7415 - val_loss: 621.1207\n",
      "Epoch 290/500\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 69.1213 - val_loss: 621.9242\n",
      "Epoch 291/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 67.5392 - val_loss: 622.7422\n",
      "Epoch 292/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 65.9944 - val_loss: 623.5739\n",
      "Epoch 293/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 64.4863 - val_loss: 624.4188\n",
      "Epoch 294/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 63.0143 - val_loss: 625.2764\n",
      "Epoch 295/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 61.5778 - val_loss: 626.1462\n",
      "Epoch 296/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 60.1762 - val_loss: 627.0275\n",
      "Epoch 297/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 58.8090 - val_loss: 627.9198\n",
      "Epoch 298/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 57.4756 - val_loss: 628.8226\n",
      "Epoch 299/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 56.1753 - val_loss: 629.7355\n",
      "Epoch 300/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 54.9076 - val_loss: 630.6578\n",
      "Epoch 301/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 53.6719 - val_loss: 631.5891\n",
      "Epoch 302/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 52.4678 - val_loss: 632.5287\n",
      "Epoch 303/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 51.2946 - val_loss: 633.4763\n",
      "Epoch 304/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 50.1518 - val_loss: 634.4312\n",
      "Epoch 305/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 49.0388 - val_loss: 635.3928\n",
      "Epoch 306/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 47.9551 - val_loss: 636.3612\n",
      "Epoch 307/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 46.9000 - val_loss: 637.3354\n",
      "Epoch 308/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 45.8730 - val_loss: 638.3150\n",
      "Epoch 309/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 44.8738 - val_loss: 639.2996\n",
      "Epoch 310/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 43.9016 - val_loss: 640.2885\n",
      "Epoch 311/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 42.9560 - val_loss: 641.2818\n",
      "Epoch 312/500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 42.0364 - val_loss: 642.2784\n",
      "Epoch 313/500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 41.1423 - val_loss: 643.2783\n",
      "Epoch 314/500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 40.2733 - val_loss: 644.2809\n",
      "Epoch 315/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 39.4288 - val_loss: 645.2855\n",
      "Epoch 316/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 38.6083 - val_loss: 646.2922\n",
      "Epoch 317/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 37.8112 - val_loss: 647.3004\n",
      "Epoch 318/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 37.0372 - val_loss: 648.3094\n",
      "Epoch 319/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 36.2857 - val_loss: 649.3190\n",
      "Epoch 320/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 35.5562 - val_loss: 650.3288\n",
      "Epoch 321/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 34.8483 - val_loss: 651.3386\n",
      "Epoch 322/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 34.1614 - val_loss: 652.3475\n",
      "Epoch 323/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 33.4952 - val_loss: 653.3556\n",
      "Epoch 324/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 32.8491 - val_loss: 654.3623\n",
      "Epoch 325/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 32.2228 - val_loss: 655.3675\n",
      "Epoch 326/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 31.6156 - val_loss: 656.3704\n",
      "Epoch 327/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 31.0272 - val_loss: 657.3709\n",
      "Epoch 328/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 30.4573 - val_loss: 658.3688\n",
      "Epoch 329/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 29.9053 - val_loss: 659.3633\n",
      "Epoch 330/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 29.3708 - val_loss: 660.3547\n",
      "Epoch 331/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 28.8534 - val_loss: 661.3423\n",
      "Epoch 332/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 28.3526 - val_loss: 662.3256\n",
      "Epoch 333/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 27.8681 - val_loss: 663.3047\n",
      "Epoch 334/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 27.3996 - val_loss: 664.2791\n",
      "Epoch 335/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 26.9464 - val_loss: 665.2484\n",
      "Epoch 336/500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 26.5084 - val_loss: 666.2128\n",
      "Epoch 337/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 26.0849 - val_loss: 667.1713\n",
      "Epoch 338/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 25.6759 - val_loss: 668.1243\n",
      "Epoch 339/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 25.2807 - val_loss: 669.0714\n",
      "Epoch 340/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 24.8991 - val_loss: 670.0118\n",
      "Epoch 341/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 24.5306 - val_loss: 670.9461\n",
      "Epoch 342/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 24.1750 - val_loss: 671.8733\n",
      "Epoch 343/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 23.8320 - val_loss: 672.7936\n",
      "Epoch 344/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 23.5010 - val_loss: 673.7069\n",
      "Epoch 345/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 23.1818 - val_loss: 674.6125\n",
      "Epoch 346/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 22.8742 - val_loss: 675.5106\n",
      "Epoch 347/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 22.5777 - val_loss: 676.4011\n",
      "Epoch 348/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 22.2920 - val_loss: 677.2832\n",
      "Epoch 349/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 22.0168 - val_loss: 678.1572\n",
      "Epoch 350/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 21.7519 - val_loss: 679.0226\n",
      "Epoch 351/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 21.4969 - val_loss: 679.8801\n",
      "Epoch 352/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 21.2514 - val_loss: 680.7286\n",
      "Epoch 353/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 21.0153 - val_loss: 681.5684\n",
      "Epoch 354/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 20.7882 - val_loss: 682.3991\n",
      "Epoch 355/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 20.5698 - val_loss: 683.2204\n",
      "Epoch 356/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 20.3601 - val_loss: 684.0326\n",
      "Epoch 357/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 20.1584 - val_loss: 684.8353\n",
      "Epoch 358/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 19.9648 - val_loss: 685.6288\n",
      "Epoch 359/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 19.7788 - val_loss: 686.4127\n",
      "Epoch 360/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 19.6003 - val_loss: 687.1871\n",
      "Epoch 361/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 19.4290 - val_loss: 687.9514\n",
      "Epoch 362/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 19.2646 - val_loss: 688.7059\n",
      "Epoch 363/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 19.1071 - val_loss: 689.4505\n",
      "Epoch 364/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.9560 - val_loss: 690.1852\n",
      "Epoch 365/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 18.8112 - val_loss: 690.9100\n",
      "Epoch 366/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.6724 - val_loss: 691.6243\n",
      "Epoch 367/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.5395 - val_loss: 692.3288\n",
      "Epoch 368/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.4123 - val_loss: 693.0226\n",
      "Epoch 369/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.2906 - val_loss: 693.7068\n",
      "Epoch 370/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.1740 - val_loss: 694.3803\n",
      "Epoch 371/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 18.0626 - val_loss: 695.0438\n",
      "Epoch 372/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 17.9560 - val_loss: 695.6967\n",
      "Epoch 373/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 17.8542 - val_loss: 696.3394\n",
      "Epoch 374/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 17.7569 - val_loss: 696.9716\n",
      "Epoch 375/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 17.6639 - val_loss: 697.5936\n",
      "Epoch 376/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 17.5752 - val_loss: 698.2051\n",
      "Epoch 377/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 17.4905 - val_loss: 698.8065\n",
      "Epoch 378/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 17.4097 - val_loss: 699.3974\n",
      "Epoch 379/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 17.3326 - val_loss: 699.9782\n",
      "Epoch 380/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 17.2590 - val_loss: 700.5487\n",
      "Epoch 381/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 17.1890 - val_loss: 701.1088\n",
      "Epoch 382/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 17.1222 - val_loss: 701.6591\n",
      "Epoch 383/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 17.0586 - val_loss: 702.1990\n",
      "Epoch 384/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.9980 - val_loss: 702.7286\n",
      "Epoch 385/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 16.9404 - val_loss: 703.2485\n",
      "Epoch 386/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.8856 - val_loss: 703.7582\n",
      "Epoch 387/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.8334 - val_loss: 704.2579\n",
      "Epoch 388/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 16.7838 - val_loss: 704.7479\n",
      "Epoch 389/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 16.7367 - val_loss: 705.2281\n",
      "Epoch 390/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 16.6919 - val_loss: 705.6984\n",
      "Epoch 391/500\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 16.6494 - val_loss: 706.1591\n",
      "Epoch 392/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 16.6090 - val_loss: 706.6104\n",
      "Epoch 393/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 16.5707 - val_loss: 707.0519\n",
      "Epoch 394/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 16.5343 - val_loss: 707.4843\n",
      "Epoch 395/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 16.4998 - val_loss: 707.9072\n",
      "Epoch 396/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 16.4671 - val_loss: 708.3211\n",
      "Epoch 397/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 16.4361 - val_loss: 708.7253\n",
      "Epoch 398/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.4068 - val_loss: 709.1207\n",
      "Epoch 399/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.3790 - val_loss: 709.5073\n",
      "Epoch 400/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.3527 - val_loss: 709.8849\n",
      "Epoch 401/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 16.3278 - val_loss: 710.2539\n",
      "Epoch 402/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 16.3043 - val_loss: 710.6144\n",
      "Epoch 403/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.2820 - val_loss: 710.9662\n",
      "Epoch 404/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.2609 - val_loss: 711.3096\n",
      "Epoch 405/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.2410 - val_loss: 711.6445\n",
      "Epoch 406/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 16.2223 - val_loss: 711.9716\n",
      "Epoch 407/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.2045 - val_loss: 712.2905\n",
      "Epoch 408/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.1877 - val_loss: 712.6012\n",
      "Epoch 409/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.1720 - val_loss: 712.9044\n",
      "Epoch 410/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.1570 - val_loss: 713.1993\n",
      "Epoch 411/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.1430 - val_loss: 713.4871\n",
      "Epoch 412/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.1298 - val_loss: 713.7672\n",
      "Epoch 413/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.1173 - val_loss: 714.0400\n",
      "Epoch 414/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.1056 - val_loss: 714.3057\n",
      "Epoch 415/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.0945 - val_loss: 714.5641\n",
      "Epoch 416/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.0841 - val_loss: 714.8152\n",
      "Epoch 417/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.0743 - val_loss: 715.0600\n",
      "Epoch 418/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.0651 - val_loss: 715.2978\n",
      "Epoch 419/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.0565 - val_loss: 715.5289\n",
      "Epoch 420/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 16.0483 - val_loss: 715.7535\n",
      "Epoch 421/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.0407 - val_loss: 715.9719\n",
      "Epoch 422/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.0335 - val_loss: 716.1837\n",
      "Epoch 423/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.0268 - val_loss: 716.3895\n",
      "Epoch 424/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.0205 - val_loss: 716.5896\n",
      "Epoch 425/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.0145 - val_loss: 716.7836\n",
      "Epoch 426/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.0090 - val_loss: 716.9719\n",
      "Epoch 427/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 16.0038 - val_loss: 717.1543\n",
      "Epoch 428/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9989 - val_loss: 717.3314\n",
      "Epoch 429/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9943 - val_loss: 717.5030\n",
      "Epoch 430/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9901 - val_loss: 717.6694\n",
      "Epoch 431/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9861 - val_loss: 717.8307\n",
      "Epoch 432/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9823 - val_loss: 717.9870\n",
      "Epoch 433/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9788 - val_loss: 718.1383\n",
      "Epoch 434/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9755 - val_loss: 718.2846\n",
      "Epoch 435/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 15.9725 - val_loss: 718.4265\n",
      "Epoch 436/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9696 - val_loss: 718.5637\n",
      "Epoch 437/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9669 - val_loss: 718.6964\n",
      "Epoch 438/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9645 - val_loss: 718.8248\n",
      "Epoch 439/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9621 - val_loss: 718.9489\n",
      "Epoch 440/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9600 - val_loss: 719.0688\n",
      "Epoch 441/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 15.9580 - val_loss: 719.1848\n",
      "Epoch 442/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9560 - val_loss: 719.2965\n",
      "Epoch 443/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 15.9543 - val_loss: 719.4045\n",
      "Epoch 444/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 15.9527 - val_loss: 719.5088\n",
      "Epoch 445/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 15.9512 - val_loss: 719.6095\n",
      "Epoch 446/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9498 - val_loss: 719.7064\n",
      "Epoch 447/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9485 - val_loss: 719.7999\n",
      "Epoch 448/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 15.9473 - val_loss: 719.8905\n",
      "Epoch 449/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 15.9462 - val_loss: 719.9777\n",
      "Epoch 450/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9451 - val_loss: 720.0616\n",
      "Epoch 451/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9441 - val_loss: 720.1425\n",
      "Epoch 452/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9433 - val_loss: 720.2206\n",
      "Epoch 453/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9424 - val_loss: 720.2956\n",
      "Epoch 454/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9416 - val_loss: 720.3677\n",
      "Epoch 455/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 15.9410 - val_loss: 720.4373\n",
      "Epoch 456/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9403 - val_loss: 720.5043\n",
      "Epoch 457/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9397 - val_loss: 720.5686\n",
      "Epoch 458/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9392 - val_loss: 720.6308\n",
      "Epoch 459/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9386 - val_loss: 720.6901\n",
      "Epoch 460/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.9382 - val_loss: 720.7473\n",
      "Epoch 461/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9378 - val_loss: 720.8024\n",
      "Epoch 462/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9374 - val_loss: 720.8551\n",
      "Epoch 463/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.9371 - val_loss: 720.9057\n",
      "Epoch 464/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 15.9367 - val_loss: 720.9542\n",
      "Epoch 465/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9365 - val_loss: 721.0009\n",
      "Epoch 466/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9362 - val_loss: 721.0457\n",
      "Epoch 467/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 15.9359 - val_loss: 721.0886\n",
      "Epoch 468/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9357 - val_loss: 721.1297\n",
      "Epoch 469/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9355 - val_loss: 721.1690\n",
      "Epoch 470/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9354 - val_loss: 721.2069\n",
      "Epoch 471/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 15.9352 - val_loss: 721.2432\n",
      "Epoch 472/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 15.9351 - val_loss: 721.2780\n",
      "Epoch 473/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 15.9350 - val_loss: 721.3113\n",
      "Epoch 474/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9349 - val_loss: 721.3430\n",
      "Epoch 475/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 15.9349 - val_loss: 721.3734\n",
      "Epoch 476/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 15.9348 - val_loss: 721.4026\n",
      "Epoch 477/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 15.9347 - val_loss: 721.4302\n",
      "Epoch 478/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 15.9347 - val_loss: 721.4568\n",
      "Epoch 479/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 15.9347 - val_loss: 721.4823\n",
      "Epoch 480/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 15.9347 - val_loss: 721.5069\n",
      "Epoch 481/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 15.9347 - val_loss: 721.5300\n",
      "Epoch 482/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 15.9347 - val_loss: 721.5519\n",
      "Epoch 483/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 15.9347 - val_loss: 721.5731\n",
      "Epoch 484/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 15.9348 - val_loss: 721.5931\n",
      "Epoch 485/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 15.9348 - val_loss: 721.6124\n",
      "Epoch 486/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 15.9348 - val_loss: 721.6306\n",
      "Epoch 487/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 15.9349 - val_loss: 721.6478\n",
      "Epoch 488/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 15.9349 - val_loss: 721.6644\n",
      "Epoch 489/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 15.9350 - val_loss: 721.6802\n",
      "Epoch 490/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 15.9350 - val_loss: 721.6952\n",
      "Epoch 491/500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 15.9351 - val_loss: 721.7097\n",
      "Epoch 492/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 15.9352 - val_loss: 721.7233\n",
      "Epoch 493/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 15.9353 - val_loss: 721.7363\n",
      "Epoch 494/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 15.9354 - val_loss: 721.7487\n",
      "Epoch 495/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 15.9355 - val_loss: 721.7604\n",
      "Epoch 496/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 15.9356 - val_loss: 721.7715\n",
      "Epoch 497/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 15.9357 - val_loss: 721.7820\n",
      "Epoch 498/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 15.9358 - val_loss: 721.7923\n",
      "Epoch 499/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 15.9359 - val_loss: 721.8019\n",
      "Epoch 500/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 15.9360 - val_loss: 721.8109\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(85.7336, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0020, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0125, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3008, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 464ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.54485294e+01, 6.54233193e+01, 6.53981092e+01, 6.53728992e+01,\n",
       "        6.53476891e+01, 6.53224790e+01, 6.52972689e+01, 6.52720588e+01,\n",
       "        6.52468487e+01, 6.52216387e+01, 6.51964286e+01, 6.51712185e+01,\n",
       "        6.51460084e+01, 7.02754435e+01, 7.02082166e+01, 7.01409897e+01,\n",
       "        7.00737628e+01, 7.00065359e+01, 6.98786181e+01, 6.97441643e+01,\n",
       "        6.96097106e+01, 6.94752568e+01, 6.93408030e+01, 6.92063492e+01,\n",
       "        6.90718954e+01, 6.89374416e+01, 6.88029879e+01, 6.86685341e+01,\n",
       "        6.85340803e+01, 6.83996149e+01, 6.82609594e+01, 6.81212339e+01,\n",
       "        6.79843685e+01, 6.78449930e+01, 6.77066375e+01, 6.75676821e+01,\n",
       "        6.74290266e+01, 6.72903711e+01, 6.71517157e+01, 6.70130602e+01,\n",
       "        0.00000000e+00, 1.95016831e-01, 5.12230396e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.77785605e-01, 1.54180940e-02, 6.89673203e+01,\n",
       "        6.88328665e+01, 6.86984127e+01, 6.85639589e+01, 6.84295051e+01,\n",
       "        6.82917717e+01, 6.81531162e+01, 6.80144608e+01, 6.78758053e+01,\n",
       "        6.77371499e+01, 6.75984944e+01, 6.74598389e+01, 6.73211835e+01,\n",
       "        6.71825280e+01, 6.70438726e+01, 6.69052171e+01, 6.67665616e+01,\n",
       "        6.66797035e+01, 6.65998716e+01, 6.65200397e+01, 6.64402077e+01,\n",
       "        6.63603758e+01, 6.62805439e+01, 6.62007119e+01, 6.61208800e+01,\n",
       "        6.60410481e+01, 7.27177811e+01, 0.00000000e+00, 7.32385000e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.74972641e-01, 2.59407640e-01,\n",
       "        5.60244560e+01, 0.00000000e+00, 8.71023595e-01, 9.38421041e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.76310432e-01, 1.24350622e-01, 0.00000000e+00, 1.44706219e-01,\n",
       "        2.88643748e-01, 2.29632467e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.98987687e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([62.4552521 , 62.45058357, 62.44591503, 62.4412465 , 62.43657796,\n",
       "       62.43190943, 62.4272409 , 62.42257236, 62.41790383, 62.41323529,\n",
       "       62.40856676, 62.40389823, 62.39922969, 62.39456116, 62.38989262,\n",
       "       62.38522409, 62.38055556, 62.37588702, 62.37121849, 62.36654995,\n",
       "       62.36188142, 62.35721289, 62.35254435, 62.34787582, 62.34320728,\n",
       "       62.33853875, 62.33387021, 62.32920168, 62.32453315, 62.31986461,\n",
       "       62.31519608, 62.31052754, 62.30585901, 62.30119048, 62.29652194,\n",
       "       62.29185341, 62.28718487, 62.28251634, 62.27784781, 62.27317927,\n",
       "       62.26851074, 62.2638422 , 62.25917367, 62.25450514, 62.2498366 ,\n",
       "       62.24516807, 62.24049953, 62.235831  , 62.23116246, 62.22649393,\n",
       "       62.2218254 , 62.21715686, 62.21248833, 62.20781979, 62.20315126,\n",
       "       62.19696545, 62.18762838, 62.17829132, 62.16895425, 62.15961718,\n",
       "       62.15028011, 62.14094304, 62.13160598, 62.12226891, 62.11293184,\n",
       "       62.10359477, 62.0942577 , 62.08492063, 62.07558357, 62.0662465 ,\n",
       "       62.05690943, 62.04757236, 62.03823529, 62.02889823, 62.01956116,\n",
       "       62.01022409, 62.00088702, 61.99154995, 61.98221289, 61.97287582,\n",
       "       61.96353875, 61.95420168, 61.94486461, 61.93552754, 61.92619048,\n",
       "       61.91685341, 61.90751634, 61.89817927, 61.8888422 , 61.87950514,\n",
       "       61.87016807, 61.860831  , 61.85149393, 61.84215686, 61.83281979,\n",
       "       61.82348273, 61.81414566, 61.80480859, 61.79547152, 61.78613445])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.29047743200581\n",
      "23.42109976289498\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
