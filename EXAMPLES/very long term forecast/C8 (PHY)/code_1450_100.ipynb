{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "1545    63.289076\n",
       "1546    63.277871\n",
       "1547    63.266667\n",
       "1548    63.255462\n",
       "1549    63.244258\n",
       "Name: C8, Length: 1550, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c8_interpolated_1450_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "1445     0.374973\n",
       "1446     0.675792\n",
       "1447     0.080706\n",
       "1448     0.259408\n",
       "1449     0.000000\n",
       "Name: C8, Length: 1450, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1450)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.987115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.774230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.561345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.348459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.200000  0.000298\n",
       "1     87.987115  0.000298\n",
       "2     87.774230  0.000297\n",
       "3     87.561345  0.000297\n",
       "4     87.348459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 85.7336  # Value for C0\n",
    "K0 = -0.0020  # Value for K0\n",
    "K1 = 0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0125    # Value for b\n",
    "c = 2.3008    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.200000    87.987115    87.774230    87.561345    87.348459   \n",
      "351    87.987115    87.774230    87.561345    87.348459    87.135574   \n",
      "352    87.774230    87.561345    87.348459    87.135574    86.922689   \n",
      "353    87.561345    87.348459    87.135574    86.922689    86.709804   \n",
      "354    87.348459    87.135574    86.922689    86.709804    86.496919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.135574    86.922689    86.709804    86.496919    86.294538  ...   \n",
      "351    86.922689    86.709804    86.496919    86.294538    86.221709  ...   \n",
      "352    86.709804    86.496919    86.294538    86.221709    86.148880  ...   \n",
      "353    86.496919    86.294538    86.221709    86.148880    86.076050  ...   \n",
      "354    86.294538    86.221709    86.148880    86.076050    86.003221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   76.094351    0.000263   76.058870    0.000263   76.023389    0.000263   \n",
      "351   76.058870    0.000263   76.023389    0.000263   75.987909    0.000262   \n",
      "352   76.023389    0.000263   75.987909    0.000262   75.952428    0.000262   \n",
      "353   75.987909    0.000262   75.952428    0.000262   75.916947    0.000262   \n",
      "354   75.952428    0.000262   75.916947    0.000262   75.881466    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   75.987909    0.000262   75.952428    0.000262  \n",
      "351   75.952428    0.000262   75.916947    0.000262  \n",
      "352   75.916947    0.000262   75.881466    0.000262  \n",
      "353   75.881466    0.000262   75.845985    0.000262  \n",
      "354   75.845985    0.000262   75.810504    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1, 251) (1000, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "13/13 [==============================] - 2s 49ms/step - loss: 5079.1973 - val_loss: 4140.4194\n",
      "Epoch 2/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5031.0083 - val_loss: 4104.5840\n",
      "Epoch 3/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4991.5283 - val_loss: 4068.8906\n",
      "Epoch 4/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4952.2261 - val_loss: 4033.3999\n",
      "Epoch 5/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4913.1401 - val_loss: 3998.1294\n",
      "Epoch 6/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4874.2842 - val_loss: 3963.0823\n",
      "Epoch 7/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4835.6597 - val_loss: 3928.2578\n",
      "Epoch 8/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4797.2642 - val_loss: 3893.6553\n",
      "Epoch 9/500\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 4759.0991 - val_loss: 3859.2749\n",
      "Epoch 10/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 4721.1611 - val_loss: 3825.1140\n",
      "Epoch 11/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4683.4502 - val_loss: 3791.1719\n",
      "Epoch 12/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4645.9648 - val_loss: 3757.4482\n",
      "Epoch 13/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4608.7041 - val_loss: 3723.9407\n",
      "Epoch 14/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4571.6670 - val_loss: 3690.6492\n",
      "Epoch 15/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4534.8516 - val_loss: 3657.5718\n",
      "Epoch 16/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4498.2583 - val_loss: 3624.7085\n",
      "Epoch 17/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4461.8853 - val_loss: 3592.0576\n",
      "Epoch 18/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4425.7305 - val_loss: 3559.6179\n",
      "Epoch 19/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4389.7949 - val_loss: 3527.3894\n",
      "Epoch 20/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4354.0762 - val_loss: 3495.3701\n",
      "Epoch 21/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4318.5737 - val_loss: 3463.5591\n",
      "Epoch 22/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4283.2861 - val_loss: 3431.9561\n",
      "Epoch 23/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4248.2129 - val_loss: 3400.5591\n",
      "Epoch 24/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4213.3521 - val_loss: 3369.3682\n",
      "Epoch 25/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4178.7036 - val_loss: 3338.3816\n",
      "Epoch 26/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4144.2671 - val_loss: 3307.5991\n",
      "Epoch 27/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4110.0400 - val_loss: 3277.0186\n",
      "Epoch 28/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4076.0220 - val_loss: 3246.6396\n",
      "Epoch 29/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 4042.2129 - val_loss: 3216.4622\n",
      "Epoch 30/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 4008.6108 - val_loss: 3186.4844\n",
      "Epoch 31/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 3975.2151 - val_loss: 3156.7053\n",
      "Epoch 32/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3942.0242 - val_loss: 3127.1240\n",
      "Epoch 33/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3909.0383 - val_loss: 3097.7400\n",
      "Epoch 34/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3876.2556 - val_loss: 3068.5515\n",
      "Epoch 35/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3843.6750 - val_loss: 3039.5593\n",
      "Epoch 36/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3811.2971 - val_loss: 3010.7603\n",
      "Epoch 37/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3779.1194 - val_loss: 2982.1553\n",
      "Epoch 38/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3747.1406 - val_loss: 2953.7422\n",
      "Epoch 39/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3715.3613 - val_loss: 2925.5212\n",
      "Epoch 40/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3683.7800 - val_loss: 2897.4900\n",
      "Epoch 41/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3652.3955 - val_loss: 2869.6492\n",
      "Epoch 42/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3621.2078 - val_loss: 2841.9968\n",
      "Epoch 43/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3590.2146 - val_loss: 2814.5332\n",
      "Epoch 44/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3559.4163 - val_loss: 2787.2563\n",
      "Epoch 45/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3528.8115 - val_loss: 2760.1650\n",
      "Epoch 46/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3498.3984 - val_loss: 2733.2598\n",
      "Epoch 47/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 3468.1775 - val_loss: 2706.5391\n",
      "Epoch 48/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3438.1475 - val_loss: 2680.0015\n",
      "Epoch 49/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3408.3076 - val_loss: 2653.6475\n",
      "Epoch 50/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3378.6565 - val_loss: 2627.4751\n",
      "Epoch 51/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3349.1941 - val_loss: 2601.4839\n",
      "Epoch 52/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3319.9189 - val_loss: 2575.6724\n",
      "Epoch 53/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3290.8303 - val_loss: 2550.0410\n",
      "Epoch 54/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3261.9268 - val_loss: 2524.5874\n",
      "Epoch 55/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3233.2080 - val_loss: 2499.3125\n",
      "Epoch 56/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3204.6743 - val_loss: 2474.2141\n",
      "Epoch 57/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3176.3237 - val_loss: 2449.2917\n",
      "Epoch 58/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3148.1550 - val_loss: 2424.5449\n",
      "Epoch 59/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3120.1677 - val_loss: 2399.9719\n",
      "Epoch 60/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 3092.3613 - val_loss: 2375.5732\n",
      "Epoch 61/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 3064.7346 - val_loss: 2351.3472\n",
      "Epoch 62/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 3037.2871 - val_loss: 2327.2932\n",
      "Epoch 63/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 3010.0181 - val_loss: 2303.4109\n",
      "Epoch 64/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2982.9265 - val_loss: 2279.6982\n",
      "Epoch 65/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 2956.0115 - val_loss: 2256.1560\n",
      "Epoch 66/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2929.2725 - val_loss: 2232.7820\n",
      "Epoch 67/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2902.7087 - val_loss: 2209.5767\n",
      "Epoch 68/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2876.3181 - val_loss: 2186.5386\n",
      "Epoch 69/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2850.1025 - val_loss: 2163.6667\n",
      "Epoch 70/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2824.0593 - val_loss: 2140.9614\n",
      "Epoch 71/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2798.1877 - val_loss: 2118.4207\n",
      "Epoch 72/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2772.4878 - val_loss: 2096.0437\n",
      "Epoch 73/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2746.9578 - val_loss: 2073.8311\n",
      "Epoch 74/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2721.5979 - val_loss: 2051.7810\n",
      "Epoch 75/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2696.4065 - val_loss: 2029.8923\n",
      "Epoch 76/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2671.3835 - val_loss: 2008.1658\n",
      "Epoch 77/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2646.5278 - val_loss: 1986.5991\n",
      "Epoch 78/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2621.8389 - val_loss: 1965.1926\n",
      "Epoch 79/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2597.3157 - val_loss: 1943.9442\n",
      "Epoch 80/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 2572.9578 - val_loss: 1922.8555\n",
      "Epoch 81/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2548.7639 - val_loss: 1901.9233\n",
      "Epoch 82/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2524.7341 - val_loss: 1881.1486\n",
      "Epoch 83/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2500.8674 - val_loss: 1860.5294\n",
      "Epoch 84/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2477.1626 - val_loss: 1840.0654\n",
      "Epoch 85/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2453.6191 - val_loss: 1819.7571\n",
      "Epoch 86/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2430.2368 - val_loss: 1799.6017\n",
      "Epoch 87/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2407.0146 - val_loss: 1779.6005\n",
      "Epoch 88/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 2383.9514 - val_loss: 1759.7505\n",
      "Epoch 89/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 2361.0469 - val_loss: 1740.0540\n",
      "Epoch 90/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2338.3005 - val_loss: 1720.5072\n",
      "Epoch 91/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2315.7114 - val_loss: 1701.1110\n",
      "Epoch 92/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2293.2781 - val_loss: 1681.8651\n",
      "Epoch 93/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2271.0012 - val_loss: 1662.7673\n",
      "Epoch 94/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2248.8792 - val_loss: 1643.8188\n",
      "Epoch 95/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2226.9121 - val_loss: 1625.0178\n",
      "Epoch 96/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 2205.0984 - val_loss: 1606.3633\n",
      "Epoch 97/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2183.4380 - val_loss: 1587.8556\n",
      "Epoch 98/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2161.9294 - val_loss: 1569.4926\n",
      "Epoch 99/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2140.5728 - val_loss: 1551.2749\n",
      "Epoch 100/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2119.3667 - val_loss: 1533.2010\n",
      "Epoch 101/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2098.3118 - val_loss: 1515.2716\n",
      "Epoch 102/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2077.4060 - val_loss: 1497.4845\n",
      "Epoch 103/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2056.6492 - val_loss: 1479.8398\n",
      "Epoch 104/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 2036.0405 - val_loss: 1462.3369\n",
      "Epoch 105/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2015.5798 - val_loss: 1444.9738\n",
      "Epoch 106/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1995.2657 - val_loss: 1427.7518\n",
      "Epoch 107/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1975.0981 - val_loss: 1410.6686\n",
      "Epoch 108/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1955.0763 - val_loss: 1393.7249\n",
      "Epoch 109/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1935.1989 - val_loss: 1376.9191\n",
      "Epoch 110/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 1915.4661 - val_loss: 1360.2507\n",
      "Epoch 111/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1895.8773 - val_loss: 1343.7198\n",
      "Epoch 112/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1876.4312 - val_loss: 1327.3248\n",
      "Epoch 113/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1857.1277 - val_loss: 1311.0660\n",
      "Epoch 114/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1837.9661 - val_loss: 1294.9413\n",
      "Epoch 115/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1818.9453 - val_loss: 1278.9519\n",
      "Epoch 116/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1800.0652 - val_loss: 1263.0956\n",
      "Epoch 117/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1781.3247 - val_loss: 1247.3724\n",
      "Epoch 118/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1762.7236 - val_loss: 1231.7820\n",
      "Epoch 119/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1744.2612 - val_loss: 1216.3236\n",
      "Epoch 120/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1725.9368 - val_loss: 1200.9962\n",
      "Epoch 121/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1707.7494 - val_loss: 1185.7986\n",
      "Epoch 122/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1689.6987 - val_loss: 1170.7317\n",
      "Epoch 123/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1671.7844 - val_loss: 1155.7942\n",
      "Epoch 124/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1654.0056 - val_loss: 1140.9849\n",
      "Epoch 125/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1636.3611 - val_loss: 1126.3042\n",
      "Epoch 126/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1618.8512 - val_loss: 1111.7502\n",
      "Epoch 127/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1601.4749 - val_loss: 1097.3235\n",
      "Epoch 128/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1584.2312 - val_loss: 1083.0229\n",
      "Epoch 129/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1567.1204 - val_loss: 1068.8486\n",
      "Epoch 130/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1550.1410 - val_loss: 1054.7988\n",
      "Epoch 131/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1533.2931 - val_loss: 1040.8730\n",
      "Epoch 132/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1516.5758 - val_loss: 1027.0714\n",
      "Epoch 133/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1499.9882 - val_loss: 1013.3928\n",
      "Epoch 134/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1483.5300 - val_loss: 999.8369\n",
      "Epoch 135/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1467.2008 - val_loss: 986.4031\n",
      "Epoch 136/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1450.9996 - val_loss: 973.0905\n",
      "Epoch 137/500\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 1434.9263 - val_loss: 959.8988\n",
      "Epoch 138/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1418.9799 - val_loss: 946.8273\n",
      "Epoch 139/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1403.1597 - val_loss: 933.8756\n",
      "Epoch 140/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1387.4655 - val_loss: 921.0422\n",
      "Epoch 141/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1371.8961 - val_loss: 908.3276\n",
      "Epoch 142/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1356.4519 - val_loss: 895.7311\n",
      "Epoch 143/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1341.1313 - val_loss: 883.2513\n",
      "Epoch 144/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1325.9342 - val_loss: 870.8882\n",
      "Epoch 145/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1310.8601 - val_loss: 858.6414\n",
      "Epoch 146/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1295.9086 - val_loss: 846.5101\n",
      "Epoch 147/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1281.0790 - val_loss: 834.4937\n",
      "Epoch 148/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1266.3704 - val_loss: 822.5920\n",
      "Epoch 149/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1251.7823 - val_loss: 810.8033\n",
      "Epoch 150/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1237.3143 - val_loss: 799.1284\n",
      "Epoch 151/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1222.9661 - val_loss: 787.5656\n",
      "Epoch 152/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1208.7363 - val_loss: 776.1150\n",
      "Epoch 153/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1194.6249 - val_loss: 764.7757\n",
      "Epoch 154/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1180.6313 - val_loss: 753.5474\n",
      "Epoch 155/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1166.7549 - val_loss: 742.4294\n",
      "Epoch 156/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1152.9955 - val_loss: 731.4207\n",
      "Epoch 157/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1139.3519 - val_loss: 720.5219\n",
      "Epoch 158/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1125.8237 - val_loss: 709.7310\n",
      "Epoch 159/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1112.4106 - val_loss: 699.0483\n",
      "Epoch 160/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1099.1117 - val_loss: 688.4733\n",
      "Epoch 161/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1085.9269 - val_loss: 678.0045\n",
      "Epoch 162/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1072.8551 - val_loss: 667.6418\n",
      "Epoch 163/500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 1059.8961 - val_loss: 657.3858\n",
      "Epoch 164/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1047.0493 - val_loss: 647.2345\n",
      "Epoch 165/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1034.3143 - val_loss: 637.1879\n",
      "Epoch 166/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1021.6900 - val_loss: 627.2453\n",
      "Epoch 167/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1009.1765 - val_loss: 617.4062\n",
      "Epoch 168/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 996.7729 - val_loss: 607.6701\n",
      "Epoch 169/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 984.4788 - val_loss: 598.0362\n",
      "Epoch 170/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 972.2936 - val_loss: 588.5045\n",
      "Epoch 171/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 960.2167 - val_loss: 579.0736\n",
      "Epoch 172/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 948.2478 - val_loss: 569.7435\n",
      "Epoch 173/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 936.3859 - val_loss: 560.5135\n",
      "Epoch 174/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 924.6308 - val_loss: 551.3835\n",
      "Epoch 175/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 912.9819 - val_loss: 542.3521\n",
      "Epoch 176/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 901.4384 - val_loss: 533.4194\n",
      "Epoch 177/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 890.0002 - val_loss: 524.5846\n",
      "Epoch 178/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 878.6667 - val_loss: 515.8474\n",
      "Epoch 179/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 867.4373 - val_loss: 507.2068\n",
      "Epoch 180/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 856.3113 - val_loss: 498.6627\n",
      "Epoch 181/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 845.2881 - val_loss: 490.2143\n",
      "Epoch 182/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 834.3674 - val_loss: 481.8608\n",
      "Epoch 183/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 823.5485 - val_loss: 473.6021\n",
      "Epoch 184/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 812.8309 - val_loss: 465.4377\n",
      "Epoch 185/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 802.2143 - val_loss: 457.3668\n",
      "Epoch 186/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 791.6980 - val_loss: 449.3888\n",
      "Epoch 187/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 781.2816 - val_loss: 441.5031\n",
      "Epoch 188/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 770.9641 - val_loss: 433.7096\n",
      "Epoch 189/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 760.7454 - val_loss: 426.0074\n",
      "Epoch 190/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 750.6249 - val_loss: 418.3961\n",
      "Epoch 191/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 740.6018 - val_loss: 410.8749\n",
      "Epoch 192/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 730.6758 - val_loss: 403.4431\n",
      "Epoch 193/500\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 720.8463 - val_loss: 396.1008\n",
      "Epoch 194/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 711.1132 - val_loss: 388.8475\n",
      "Epoch 195/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 701.4758 - val_loss: 381.6818\n",
      "Epoch 196/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 691.9330 - val_loss: 374.6041\n",
      "Epoch 197/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 682.4850 - val_loss: 367.6132\n",
      "Epoch 198/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 673.1310 - val_loss: 360.7090\n",
      "Epoch 199/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 663.8704 - val_loss: 353.8907\n",
      "Epoch 200/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 654.7025 - val_loss: 347.1577\n",
      "Epoch 201/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 645.6274 - val_loss: 340.5098\n",
      "Epoch 202/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 636.6439 - val_loss: 333.9460\n",
      "Epoch 203/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 627.7515 - val_loss: 327.4659\n",
      "Epoch 204/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 618.9501 - val_loss: 321.0691\n",
      "Epoch 205/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 610.2388 - val_loss: 314.7548\n",
      "Epoch 206/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 601.6176 - val_loss: 308.5233\n",
      "Epoch 207/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 593.0858 - val_loss: 302.3731\n",
      "Epoch 208/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 584.6426 - val_loss: 296.3042\n",
      "Epoch 209/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 576.2877 - val_loss: 290.3155\n",
      "Epoch 210/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 568.0205 - val_loss: 284.4077\n",
      "Epoch 211/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 559.8410 - val_loss: 278.5788\n",
      "Epoch 212/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 551.7479 - val_loss: 272.8290\n",
      "Epoch 213/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 543.7410 - val_loss: 267.1577\n",
      "Epoch 214/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 535.8198 - val_loss: 261.5641\n",
      "Epoch 215/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 527.9836 - val_loss: 256.0482\n",
      "Epoch 216/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 520.2322 - val_loss: 250.6092\n",
      "Epoch 217/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 512.5649 - val_loss: 245.2462\n",
      "Epoch 218/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 504.9810 - val_loss: 239.9589\n",
      "Epoch 219/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 497.4803 - val_loss: 234.7466\n",
      "Epoch 220/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 490.0622 - val_loss: 229.6096\n",
      "Epoch 221/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 482.7262 - val_loss: 224.5463\n",
      "Epoch 222/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 475.4720 - val_loss: 219.5571\n",
      "Epoch 223/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 468.2989 - val_loss: 214.6410\n",
      "Epoch 224/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 461.2060 - val_loss: 209.7971\n",
      "Epoch 225/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 454.1934 - val_loss: 205.0257\n",
      "Epoch 226/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 447.2604 - val_loss: 200.3255\n",
      "Epoch 227/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 440.4063 - val_loss: 195.6964\n",
      "Epoch 228/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 433.6306 - val_loss: 191.1373\n",
      "Epoch 229/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 426.9326 - val_loss: 186.6484\n",
      "Epoch 230/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 420.3125 - val_loss: 182.2291\n",
      "Epoch 231/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 413.7691 - val_loss: 177.8781\n",
      "Epoch 232/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 407.3021 - val_loss: 173.5959\n",
      "Epoch 233/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 400.9114 - val_loss: 169.3811\n",
      "Epoch 234/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 394.5957 - val_loss: 165.2335\n",
      "Epoch 235/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 388.3553 - val_loss: 161.1532\n",
      "Epoch 236/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 382.1894 - val_loss: 157.1389\n",
      "Epoch 237/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 376.0974 - val_loss: 153.1898\n",
      "Epoch 238/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 370.0784 - val_loss: 149.3064\n",
      "Epoch 239/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 364.1327 - val_loss: 145.4872\n",
      "Epoch 240/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 358.2592 - val_loss: 141.7324\n",
      "Epoch 241/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 352.4579 - val_loss: 138.0410\n",
      "Epoch 242/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 346.7279 - val_loss: 134.4123\n",
      "Epoch 243/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 341.0685 - val_loss: 130.8465\n",
      "Epoch 244/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 335.4798 - val_loss: 127.3424\n",
      "Epoch 245/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 329.9606 - val_loss: 123.8997\n",
      "Epoch 246/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 324.5110 - val_loss: 120.5180\n",
      "Epoch 247/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 319.1301 - val_loss: 117.1966\n",
      "Epoch 248/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 313.8179 - val_loss: 113.9351\n",
      "Epoch 249/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 308.5733 - val_loss: 110.7326\n",
      "Epoch 250/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 303.3958 - val_loss: 107.5891\n",
      "Epoch 251/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 298.2855 - val_loss: 104.5039\n",
      "Epoch 252/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 293.2417 - val_loss: 101.4766\n",
      "Epoch 253/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 288.2634 - val_loss: 98.5060\n",
      "Epoch 254/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 283.3505 - val_loss: 95.5923\n",
      "Epoch 255/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 278.5023 - val_loss: 92.7347\n",
      "Epoch 256/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 273.7186 - val_loss: 89.9326\n",
      "Epoch 257/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 268.9987 - val_loss: 87.1858\n",
      "Epoch 258/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 264.3420 - val_loss: 84.4934\n",
      "Epoch 259/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 259.7482 - val_loss: 81.8548\n",
      "Epoch 260/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 255.2166 - val_loss: 79.2698\n",
      "Epoch 261/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 250.7468 - val_loss: 76.7380\n",
      "Epoch 262/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 246.3386 - val_loss: 74.2585\n",
      "Epoch 263/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 241.9909 - val_loss: 71.8309\n",
      "Epoch 264/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 237.7036 - val_loss: 69.4549\n",
      "Epoch 265/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 233.4762 - val_loss: 67.1293\n",
      "Epoch 266/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 229.3079 - val_loss: 64.8542\n",
      "Epoch 267/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 225.1985 - val_loss: 62.6291\n",
      "Epoch 268/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 221.1476 - val_loss: 60.4534\n",
      "Epoch 269/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 217.1546 - val_loss: 58.3262\n",
      "Epoch 270/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 213.2188 - val_loss: 56.2475\n",
      "Epoch 271/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 209.3397 - val_loss: 54.2163\n",
      "Epoch 272/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 205.5169 - val_loss: 52.2323\n",
      "Epoch 273/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 201.7500 - val_loss: 50.2951\n",
      "Epoch 274/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 198.0384 - val_loss: 48.4040\n",
      "Epoch 275/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 194.3818 - val_loss: 46.5586\n",
      "Epoch 276/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 190.7794 - val_loss: 44.7584\n",
      "Epoch 277/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 187.2309 - val_loss: 43.0026\n",
      "Epoch 278/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 183.7356 - val_loss: 41.2910\n",
      "Epoch 279/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 180.2933 - val_loss: 39.6230\n",
      "Epoch 280/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 176.9033 - val_loss: 37.9981\n",
      "Epoch 281/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 173.5652 - val_loss: 36.4157\n",
      "Epoch 282/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 170.2786 - val_loss: 34.8752\n",
      "Epoch 283/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 167.0430 - val_loss: 33.3765\n",
      "Epoch 284/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 163.8577 - val_loss: 31.9186\n",
      "Epoch 285/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 160.7224 - val_loss: 30.5013\n",
      "Epoch 286/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 157.6364 - val_loss: 29.1240\n",
      "Epoch 287/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 154.5994 - val_loss: 27.7861\n",
      "Epoch 288/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 151.6107 - val_loss: 26.4871\n",
      "Epoch 289/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 148.6700 - val_loss: 25.2267\n",
      "Epoch 290/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 145.7770 - val_loss: 24.0042\n",
      "Epoch 291/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 142.9311 - val_loss: 22.8192\n",
      "Epoch 292/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 140.1316 - val_loss: 21.6712\n",
      "Epoch 293/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 137.3782 - val_loss: 20.5596\n",
      "Epoch 294/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 134.6704 - val_loss: 19.4839\n",
      "Epoch 295/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 132.0077 - val_loss: 18.4437\n",
      "Epoch 296/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 129.3897 - val_loss: 17.4384\n",
      "Epoch 297/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 126.8158 - val_loss: 16.4676\n",
      "Epoch 298/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 124.2856 - val_loss: 15.5308\n",
      "Epoch 299/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 121.7986 - val_loss: 14.6273\n",
      "Epoch 300/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 119.3544 - val_loss: 13.7568\n",
      "Epoch 301/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 116.9523 - val_loss: 12.9188\n",
      "Epoch 302/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 114.5920 - val_loss: 12.1128\n",
      "Epoch 303/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 112.2729 - val_loss: 11.3382\n",
      "Epoch 304/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 109.9946 - val_loss: 10.5946\n",
      "Epoch 305/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 107.7568 - val_loss: 9.8815\n",
      "Epoch 306/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 105.5590 - val_loss: 9.1985\n",
      "Epoch 307/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 103.4005 - val_loss: 8.5450\n",
      "Epoch 308/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 101.2809 - val_loss: 7.9206\n",
      "Epoch 309/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 99.1999 - val_loss: 7.3248\n",
      "Epoch 310/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 97.1570 - val_loss: 6.7570\n",
      "Epoch 311/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 95.1517 - val_loss: 6.2169\n",
      "Epoch 312/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 93.1834 - val_loss: 5.7040\n",
      "Epoch 313/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 91.2519 - val_loss: 5.2178\n",
      "Epoch 314/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 89.3566 - val_loss: 4.7577\n",
      "Epoch 315/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 87.4970 - val_loss: 4.3234\n",
      "Epoch 316/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 85.6728 - val_loss: 3.9144\n",
      "Epoch 317/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 83.8835 - val_loss: 3.5302\n",
      "Epoch 318/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 82.1288 - val_loss: 3.1704\n",
      "Epoch 319/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 80.4078 - val_loss: 2.8344\n",
      "Epoch 320/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 78.7205 - val_loss: 2.5220\n",
      "Epoch 321/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 77.0662 - val_loss: 2.2325\n",
      "Epoch 322/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 75.4445 - val_loss: 1.9655\n",
      "Epoch 323/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 73.8550 - val_loss: 1.7206\n",
      "Epoch 324/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 72.2973 - val_loss: 1.4974\n",
      "Epoch 325/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 70.7711 - val_loss: 1.2954\n",
      "Epoch 326/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 69.2758 - val_loss: 1.1141\n",
      "Epoch 327/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 67.8109 - val_loss: 0.9531\n",
      "Epoch 328/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 66.3761 - val_loss: 0.8120\n",
      "Epoch 329/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 64.9708 - val_loss: 0.6904\n",
      "Epoch 330/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 63.5949 - val_loss: 0.5878\n",
      "Epoch 331/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 62.2477 - val_loss: 0.5038\n",
      "Epoch 332/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 60.9289 - val_loss: 0.4380\n",
      "Epoch 333/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 59.6380 - val_loss: 0.3899\n",
      "Epoch 334/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 58.3746 - val_loss: 0.3591\n",
      "Epoch 335/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 57.1383 - val_loss: 0.3453\n",
      "Epoch 336/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 55.9288 - val_loss: 0.3479\n",
      "Epoch 337/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 54.7456 - val_loss: 0.3667\n",
      "Epoch 338/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 53.5884 - val_loss: 0.4011\n",
      "Epoch 339/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 52.4567 - val_loss: 0.4508\n",
      "Epoch 340/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 51.3501 - val_loss: 0.5154\n",
      "Epoch 341/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 50.2681 - val_loss: 0.5945\n",
      "Epoch 342/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 49.2105 - val_loss: 0.6877\n",
      "Epoch 343/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 48.1767 - val_loss: 0.7946\n",
      "Epoch 344/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 47.1666 - val_loss: 0.9148\n",
      "Epoch 345/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 46.1796 - val_loss: 1.0479\n",
      "Epoch 346/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 45.2154 - val_loss: 1.1937\n",
      "Epoch 347/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 44.2736 - val_loss: 1.3516\n",
      "Epoch 348/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 43.3538 - val_loss: 1.5213\n",
      "Epoch 349/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 42.4557 - val_loss: 1.7024\n",
      "Epoch 350/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 41.5788 - val_loss: 1.8947\n",
      "Epoch 351/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 40.7228 - val_loss: 2.0977\n",
      "Epoch 352/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 39.8874 - val_loss: 2.3110\n",
      "Epoch 353/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 39.0722 - val_loss: 2.5344\n",
      "Epoch 354/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 38.2767 - val_loss: 2.7674\n",
      "Epoch 355/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 37.5009 - val_loss: 3.0098\n",
      "Epoch 356/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 36.7441 - val_loss: 3.2611\n",
      "Epoch 357/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 36.0060 - val_loss: 3.5211\n",
      "Epoch 358/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 35.2864 - val_loss: 3.7895\n",
      "Epoch 359/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 34.5849 - val_loss: 4.0657\n",
      "Epoch 360/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 33.9012 - val_loss: 4.3497\n",
      "Epoch 361/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 33.2348 - val_loss: 4.6411\n",
      "Epoch 362/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 32.5854 - val_loss: 4.9395\n",
      "Epoch 363/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 31.9528 - val_loss: 5.2446\n",
      "Epoch 364/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 31.3366 - val_loss: 5.5560\n",
      "Epoch 365/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.7367 - val_loss: 5.8737\n",
      "Epoch 366/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 30.1524 - val_loss: 6.1971\n",
      "Epoch 367/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 29.5836 - val_loss: 6.5261\n",
      "Epoch 368/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 29.0300 - val_loss: 6.8603\n",
      "Epoch 369/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 28.4912 - val_loss: 7.1995\n",
      "Epoch 370/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 27.9670 - val_loss: 7.5433\n",
      "Epoch 371/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 27.4571 - val_loss: 7.8915\n",
      "Epoch 372/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 26.9610 - val_loss: 8.2438\n",
      "Epoch 373/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 26.4786 - val_loss: 8.6001\n",
      "Epoch 374/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 26.0095 - val_loss: 8.9600\n",
      "Epoch 375/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 25.5534 - val_loss: 9.3232\n",
      "Epoch 376/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 25.1102 - val_loss: 9.6895\n",
      "Epoch 377/500\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 24.6796 - val_loss: 10.0587\n",
      "Epoch 378/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 24.2611 - val_loss: 10.4304\n",
      "Epoch 379/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 23.8546 - val_loss: 10.8046\n",
      "Epoch 380/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 23.4598 - val_loss: 11.1808\n",
      "Epoch 381/500\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 23.0764 - val_loss: 11.5590\n",
      "Epoch 382/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 22.7042 - val_loss: 11.9388\n",
      "Epoch 383/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 22.3429 - val_loss: 12.3201\n",
      "Epoch 384/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 21.9924 - val_loss: 12.7026\n",
      "Epoch 385/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 21.6522 - val_loss: 13.0862\n",
      "Epoch 386/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 21.3222 - val_loss: 13.4706\n",
      "Epoch 387/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 21.0021 - val_loss: 13.8556\n",
      "Epoch 388/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.6917 - val_loss: 14.2410\n",
      "Epoch 389/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 20.3907 - val_loss: 14.6268\n",
      "Epoch 390/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 20.0990 - val_loss: 15.0123\n",
      "Epoch 391/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 19.8163 - val_loss: 15.3980\n",
      "Epoch 392/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 19.5423 - val_loss: 15.7834\n",
      "Epoch 393/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.2769 - val_loss: 16.1681\n",
      "Epoch 394/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 19.0198 - val_loss: 16.5524\n",
      "Epoch 395/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 18.7708 - val_loss: 16.9358\n",
      "Epoch 396/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 18.5297 - val_loss: 17.3181\n",
      "Epoch 397/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 18.2963 - val_loss: 17.6993\n",
      "Epoch 398/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 18.0705 - val_loss: 18.0793\n",
      "Epoch 399/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 17.8520 - val_loss: 18.4580\n",
      "Epoch 400/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 17.6406 - val_loss: 18.8350\n",
      "Epoch 401/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 17.4360 - val_loss: 19.2103\n",
      "Epoch 402/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 17.2383 - val_loss: 19.5838\n",
      "Epoch 403/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 17.0470 - val_loss: 19.9552\n",
      "Epoch 404/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 16.8623 - val_loss: 20.3246\n",
      "Epoch 405/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.6836 - val_loss: 20.6918\n",
      "Epoch 406/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 16.5110 - val_loss: 21.0567\n",
      "Epoch 407/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.3443 - val_loss: 21.4190\n",
      "Epoch 408/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 16.1832 - val_loss: 21.7789\n",
      "Epoch 409/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.0277 - val_loss: 22.1361\n",
      "Epoch 410/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 15.8775 - val_loss: 22.4906\n",
      "Epoch 411/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 15.7325 - val_loss: 22.8423\n",
      "Epoch 412/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 15.5926 - val_loss: 23.1910\n",
      "Epoch 413/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 15.4576 - val_loss: 23.5366\n",
      "Epoch 414/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 15.3274 - val_loss: 23.8792\n",
      "Epoch 415/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 15.2018 - val_loss: 24.2185\n",
      "Epoch 416/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.0807 - val_loss: 24.5547\n",
      "Epoch 417/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 14.9639 - val_loss: 24.8877\n",
      "Epoch 418/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 14.8513 - val_loss: 25.2171\n",
      "Epoch 419/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 14.7428 - val_loss: 25.5432\n",
      "Epoch 420/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 14.6382 - val_loss: 25.8656\n",
      "Epoch 421/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 14.5374 - val_loss: 26.1846\n",
      "Epoch 422/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 14.4404 - val_loss: 26.4999\n",
      "Epoch 423/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 14.3470 - val_loss: 26.8118\n",
      "Epoch 424/500\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 14.2570 - val_loss: 27.1198\n",
      "Epoch 425/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 14.1703 - val_loss: 27.4238\n",
      "Epoch 426/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 14.0870 - val_loss: 27.7243\n",
      "Epoch 427/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 14.0067 - val_loss: 28.0208\n",
      "Epoch 428/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 13.9296 - val_loss: 28.3134\n",
      "Epoch 429/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 13.8554 - val_loss: 28.6024\n",
      "Epoch 430/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 13.7839 - val_loss: 28.8872\n",
      "Epoch 431/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 13.7153 - val_loss: 29.1682\n",
      "Epoch 432/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 13.6493 - val_loss: 29.4452\n",
      "Epoch 433/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 13.5858 - val_loss: 29.7183\n",
      "Epoch 434/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 13.5248 - val_loss: 29.9874\n",
      "Epoch 435/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 13.4662 - val_loss: 30.2523\n",
      "Epoch 436/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 13.4099 - val_loss: 30.5132\n",
      "Epoch 437/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 13.3559 - val_loss: 30.7701\n",
      "Epoch 438/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 13.3040 - val_loss: 31.0230\n",
      "Epoch 439/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 13.2542 - val_loss: 31.2720\n",
      "Epoch 440/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 13.2063 - val_loss: 31.5168\n",
      "Epoch 441/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 13.1603 - val_loss: 31.7577\n",
      "Epoch 442/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 13.1162 - val_loss: 31.9946\n",
      "Epoch 443/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 13.0739 - val_loss: 32.2275\n",
      "Epoch 444/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 13.0333 - val_loss: 32.4563\n",
      "Epoch 445/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.9943 - val_loss: 32.6812\n",
      "Epoch 446/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.9570 - val_loss: 32.9019\n",
      "Epoch 447/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.9212 - val_loss: 33.1190\n",
      "Epoch 448/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.8868 - val_loss: 33.3319\n",
      "Epoch 449/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.8539 - val_loss: 33.5410\n",
      "Epoch 450/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.8223 - val_loss: 33.7460\n",
      "Epoch 451/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.7920 - val_loss: 33.9473\n",
      "Epoch 452/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.7630 - val_loss: 34.1447\n",
      "Epoch 453/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 12.7353 - val_loss: 34.3383\n",
      "Epoch 454/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 12.7087 - val_loss: 34.5284\n",
      "Epoch 455/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.6831 - val_loss: 34.7144\n",
      "Epoch 456/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 12.6587 - val_loss: 34.8965\n",
      "Epoch 457/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.6353 - val_loss: 35.0752\n",
      "Epoch 458/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.6129 - val_loss: 35.2501\n",
      "Epoch 459/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.5915 - val_loss: 35.4213\n",
      "Epoch 460/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.5709 - val_loss: 35.5890\n",
      "Epoch 461/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.5513 - val_loss: 35.7530\n",
      "Epoch 462/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.5324 - val_loss: 35.9135\n",
      "Epoch 463/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.5144 - val_loss: 36.0703\n",
      "Epoch 464/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.4972 - val_loss: 36.2239\n",
      "Epoch 465/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 12.4807 - val_loss: 36.3743\n",
      "Epoch 466/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.4649 - val_loss: 36.5210\n",
      "Epoch 467/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 12.4498 - val_loss: 36.6642\n",
      "Epoch 468/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 12.4354 - val_loss: 36.8046\n",
      "Epoch 469/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 12.4216 - val_loss: 36.9415\n",
      "Epoch 470/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.4084 - val_loss: 37.0752\n",
      "Epoch 471/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.3957 - val_loss: 37.2056\n",
      "Epoch 472/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.3836 - val_loss: 37.3330\n",
      "Epoch 473/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.3721 - val_loss: 37.4573\n",
      "Epoch 474/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.3610 - val_loss: 37.5785\n",
      "Epoch 475/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.3504 - val_loss: 37.6969\n",
      "Epoch 476/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.3403 - val_loss: 37.8125\n",
      "Epoch 477/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.3306 - val_loss: 37.9250\n",
      "Epoch 478/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.3214 - val_loss: 38.0347\n",
      "Epoch 479/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 12.3125 - val_loss: 38.1416\n",
      "Epoch 480/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.3041 - val_loss: 38.2456\n",
      "Epoch 481/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.2960 - val_loss: 38.3469\n",
      "Epoch 482/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.2883 - val_loss: 38.4457\n",
      "Epoch 483/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 12.2809 - val_loss: 38.5420\n",
      "Epoch 484/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 12.2738 - val_loss: 38.6357\n",
      "Epoch 485/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 12.2671 - val_loss: 38.7267\n",
      "Epoch 486/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 12.2606 - val_loss: 38.8152\n",
      "Epoch 487/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.2545 - val_loss: 38.9013\n",
      "Epoch 488/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 12.2486 - val_loss: 38.9850\n",
      "Epoch 489/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 12.2430 - val_loss: 39.0665\n",
      "Epoch 490/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 12.2376 - val_loss: 39.1458\n",
      "Epoch 491/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.2324 - val_loss: 39.2225\n",
      "Epoch 492/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.2275 - val_loss: 39.2973\n",
      "Epoch 493/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.2228 - val_loss: 39.3699\n",
      "Epoch 494/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.2183 - val_loss: 39.4404\n",
      "Epoch 495/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 12.2140 - val_loss: 39.5089\n",
      "Epoch 496/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 12.2099 - val_loss: 39.5754\n",
      "Epoch 497/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 12.2060 - val_loss: 39.6398\n",
      "Epoch 498/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.2022 - val_loss: 39.7023\n",
      "Epoch 499/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.1987 - val_loss: 39.7632\n",
      "Epoch 500/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.1952 - val_loss: 39.8219\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(85.7336, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0020, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0125, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3008, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 372ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[67.53686975, 67.49065126, 67.44443277, 67.39821429, 67.3519958 ,\n",
       "        67.30577731, 67.25955882, 67.21334034, 67.16712185, 67.12090336,\n",
       "        67.07468487, 67.02846639, 66.9822479 , 66.93602941, 66.88981092,\n",
       "        66.84359244, 66.79737395, 66.75115546, 66.72405462, 66.69744398,\n",
       "        66.67083333, 66.64422269, 66.61761204, 66.5910014 , 66.56439076,\n",
       "        66.53778011, 66.51116947, 66.48455882, 66.45794818, 66.43133754,\n",
       "        66.40472689, 66.37811625, 66.3515056 , 66.32489496, 66.29828431,\n",
       "        66.27167367, 66.24506303, 66.21845238, 66.19184174, 66.16523109,\n",
       "        66.13862045, 66.1120098 , 66.08539916, 66.05878852, 66.03217787,\n",
       "        66.00556723, 65.97895658, 65.95234594, 65.92573529, 65.89912465,\n",
       "        65.87251401, 65.84590336, 65.81929272, 65.79306723, 65.76785714,\n",
       "        65.74264706, 65.71743697, 65.69222689, 65.66701681, 65.64180672,\n",
       "        65.61659664, 65.59138655, 65.56617647, 65.54096639, 65.5157563 ,\n",
       "        65.49054622, 65.46533613, 65.44012605, 65.41491597, 65.38970588,\n",
       "        65.3644958 , 65.33928571, 65.31407563, 65.28886555, 65.26365546,\n",
       "        65.23844538, 65.21323529, 65.18802521, 65.16281513, 65.13760504,\n",
       "        72.28445435,  0.53002274,  0.        ,  0.        ,  0.29109389,\n",
       "         0.184284  ,  0.        ,  0.        ,  0.57310295,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.37542471,  0.        ,\n",
       "         0.92070997,  0.62643063,  0.        ,  0.        ,  0.46592137]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([64.05116713, 64.04743231, 64.04369748, 64.03996265, 64.03622782,\n",
       "       64.032493  , 64.02875817, 64.02502334, 64.02128852, 64.01755369,\n",
       "       64.01381886, 64.01008403, 64.00634921, 64.00261438, 63.99887955,\n",
       "       63.99514472, 63.9914099 , 63.98767507, 63.98394024, 63.98020542,\n",
       "       63.97647059, 63.97273576, 63.96900093, 63.96526611, 63.96153128,\n",
       "       63.95779645, 63.95406162, 63.9503268 , 63.94659197, 63.94285714,\n",
       "       63.93912232, 63.93538749, 63.93165266, 63.92791783, 63.92418301,\n",
       "       63.92044818, 63.91671335, 63.91297852, 63.9092437 , 63.90550887,\n",
       "       63.90177404, 63.89411765, 63.88291317, 63.87170868, 63.8605042 ,\n",
       "       63.84929972, 63.83809524, 63.82689076, 63.81568627, 63.80448179,\n",
       "       63.79327731, 63.78207283, 63.77086835, 63.75966387, 63.74845938,\n",
       "       63.7372549 , 63.72605042, 63.71484594, 63.70364146, 63.69243697,\n",
       "       63.68123249, 63.67002801, 63.65882353, 63.64761905, 63.63641457,\n",
       "       63.62521008, 63.6140056 , 63.60280112, 63.59159664, 63.58039216,\n",
       "       63.56918768, 63.55798319, 63.54677871, 63.53557423, 63.52436975,\n",
       "       63.51316527, 63.50196078, 63.4907563 , 63.47955182, 63.46834734,\n",
       "       63.45714286, 63.44593838, 63.43473389, 63.42352941, 63.41232493,\n",
       "       63.40112045, 63.38991597, 63.37871148, 63.367507  , 63.35630252,\n",
       "       63.34509804, 63.33389356, 63.32268908, 63.31148459, 63.30028011,\n",
       "       63.28907563, 63.27787115, 63.26666667, 63.25546218, 63.2442577 ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.619621618590063\n",
      "13.97478703573433\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
