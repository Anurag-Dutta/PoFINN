{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "1345    64.514986\n",
       "1346    64.509384\n",
       "1347    64.503782\n",
       "1348    64.498179\n",
       "1349    64.492577\n",
       "Name: C8, Length: 1350, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c8_interpolated_1250_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.200000\n",
       "1       87.987115\n",
       "2       87.774230\n",
       "3       87.561345\n",
       "4       87.348459\n",
       "          ...    \n",
       "1245    65.162815\n",
       "1246    65.154412\n",
       "1247    65.146008\n",
       "1248    65.137605\n",
       "1249    65.129202\n",
       "Name: C8, Length: 1250, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1250)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.200000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.987115</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.774230</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.561345</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.348459</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.200000  0.000298\n",
       "1     87.987115  0.000298\n",
       "2     87.774230  0.000297\n",
       "3     87.561345  0.000297\n",
       "4     87.348459  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 85.7336  # Value for C0\n",
    "K0 = -0.0020  # Value for K0\n",
    "K1 = 0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0125    # Value for b\n",
    "c = 2.3008    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.200000    87.987115    87.774230    87.561345    87.348459   \n",
      "351    87.987115    87.774230    87.561345    87.348459    87.135574   \n",
      "352    87.774230    87.561345    87.348459    87.135574    86.922689   \n",
      "353    87.561345    87.348459    87.135574    86.922689    86.709804   \n",
      "354    87.348459    87.135574    86.922689    86.709804    86.496919   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.135574    86.922689    86.709804    86.496919    86.294538  ...   \n",
      "351    86.922689    86.709804    86.496919    86.294538    86.221709  ...   \n",
      "352    86.709804    86.496919    86.294538    86.221709    86.148880  ...   \n",
      "353    86.496919    86.294538    86.221709    86.148880    86.076050  ...   \n",
      "354    86.294538    86.221709    86.148880    86.076050    86.003221  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   76.094351    0.000263   76.058870    0.000263   76.023389    0.000263   \n",
      "351   76.058870    0.000263   76.023389    0.000263   75.987909    0.000262   \n",
      "352   76.023389    0.000263   75.987909    0.000262   75.952428    0.000262   \n",
      "353   75.987909    0.000262   75.952428    0.000262   75.916947    0.000262   \n",
      "354   75.952428    0.000262   75.916947    0.000262   75.881466    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   75.987909    0.000262   75.952428    0.000262  \n",
      "351   75.952428    0.000262   75.916947    0.000262  \n",
      "352   75.916947    0.000262   75.881466    0.000262  \n",
      "353   75.881466    0.000262   75.845985    0.000262  \n",
      "354   75.845985    0.000262   75.810504    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 1, 251) (800, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "10/10 [==============================] - 2s 53ms/step - loss: 5323.9805 - val_loss: 4497.8306\n",
      "Epoch 2/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5269.4072 - val_loss: 4462.6055\n",
      "Epoch 3/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5224.4175 - val_loss: 4418.2920\n",
      "Epoch 4/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5182.9893 - val_loss: 4382.3784\n",
      "Epoch 5/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5144.1187 - val_loss: 4346.6426\n",
      "Epoch 6/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5105.4756 - val_loss: 4311.1621\n",
      "Epoch 7/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5067.1045 - val_loss: 4275.9434\n",
      "Epoch 8/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5029.0024 - val_loss: 4240.9790\n",
      "Epoch 9/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4991.1572 - val_loss: 4206.2573\n",
      "Epoch 10/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 4953.5610 - val_loss: 4171.7705\n",
      "Epoch 11/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4916.2021 - val_loss: 4137.5112\n",
      "Epoch 12/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 4879.0767 - val_loss: 4103.4741\n",
      "Epoch 13/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 4842.1772 - val_loss: 4069.6536\n",
      "Epoch 14/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4805.0083 - val_loss: 4033.4773\n",
      "Epoch 15/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4765.2739 - val_loss: 3997.9297\n",
      "Epoch 16/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 4726.5601 - val_loss: 3962.4641\n",
      "Epoch 17/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 4688.1377 - val_loss: 3927.4023\n",
      "Epoch 18/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4650.1533 - val_loss: 3892.7520\n",
      "Epoch 19/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4612.5850 - val_loss: 3858.4746\n",
      "Epoch 20/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 4575.3921 - val_loss: 3824.5339\n",
      "Epoch 21/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4538.5381 - val_loss: 3790.9011\n",
      "Epoch 22/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 4501.9956 - val_loss: 3757.5559\n",
      "Epoch 23/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4465.7451 - val_loss: 3724.4812\n",
      "Epoch 24/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4429.7720 - val_loss: 3691.6655\n",
      "Epoch 25/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 4392.9810 - val_loss: 3650.2590\n",
      "Epoch 26/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4347.8096 - val_loss: 3615.3872\n",
      "Epoch 27/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4309.6270 - val_loss: 3580.5632\n",
      "Epoch 28/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 4271.7891 - val_loss: 3546.2429\n",
      "Epoch 29/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 4234.4990 - val_loss: 3512.4297\n",
      "Epoch 30/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4197.7236 - val_loss: 3479.0676\n",
      "Epoch 31/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 4161.4014 - val_loss: 3446.1062\n",
      "Epoch 32/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 4125.4834 - val_loss: 3413.5059\n",
      "Epoch 33/500\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 4089.9302 - val_loss: 3381.2368\n",
      "Epoch 34/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 4054.7148 - val_loss: 3349.2778\n",
      "Epoch 35/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 4019.8157 - val_loss: 3317.6101\n",
      "Epoch 36/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3985.2148 - val_loss: 3286.2222\n",
      "Epoch 37/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 3950.9011 - val_loss: 3255.1023\n",
      "Epoch 38/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3916.8628 - val_loss: 3224.2419\n",
      "Epoch 39/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 3883.0913 - val_loss: 3193.6335\n",
      "Epoch 40/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 3849.5793 - val_loss: 3163.2708\n",
      "Epoch 41/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 3816.3198 - val_loss: 3133.1484\n",
      "Epoch 42/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 3783.3086 - val_loss: 3103.2612\n",
      "Epoch 43/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 3750.5391 - val_loss: 3073.6055\n",
      "Epoch 44/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 3718.0078 - val_loss: 3044.1770\n",
      "Epoch 45/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3685.7117 - val_loss: 3014.9722\n",
      "Epoch 46/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 3653.6460 - val_loss: 2985.9895\n",
      "Epoch 47/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3621.8086 - val_loss: 2957.2236\n",
      "Epoch 48/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3590.1948 - val_loss: 2928.6733\n",
      "Epoch 49/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 3558.8040 - val_loss: 2900.3364\n",
      "Epoch 50/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3527.6323 - val_loss: 2872.2097\n",
      "Epoch 51/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3496.6777 - val_loss: 2844.2922\n",
      "Epoch 52/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3465.9387 - val_loss: 2816.5803\n",
      "Epoch 53/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3435.4126 - val_loss: 2789.0730\n",
      "Epoch 54/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3405.0969 - val_loss: 2761.7690\n",
      "Epoch 55/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3374.9910 - val_loss: 2734.6655\n",
      "Epoch 56/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3345.0918 - val_loss: 2707.7615\n",
      "Epoch 57/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 3315.3984 - val_loss: 2681.0554\n",
      "Epoch 58/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3285.9094 - val_loss: 2654.5454\n",
      "Epoch 59/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3256.6226 - val_loss: 2628.2310\n",
      "Epoch 60/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3227.5371 - val_loss: 2602.1086\n",
      "Epoch 61/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3198.6504 - val_loss: 2576.1782\n",
      "Epoch 62/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3169.9617 - val_loss: 2550.4390\n",
      "Epoch 63/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 3141.4707 - val_loss: 2524.8892\n",
      "Epoch 64/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 3113.1746 - val_loss: 2499.5266\n",
      "Epoch 65/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3085.0723 - val_loss: 2474.3518\n",
      "Epoch 66/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3057.1628 - val_loss: 2449.3618\n",
      "Epoch 67/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3029.4448 - val_loss: 2424.5566\n",
      "Epoch 68/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 3001.9177 - val_loss: 2399.9346\n",
      "Epoch 69/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2974.5798 - val_loss: 2375.4946\n",
      "Epoch 70/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2947.4294 - val_loss: 2351.2361\n",
      "Epoch 71/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 2920.4670 - val_loss: 2327.1570\n",
      "Epoch 72/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2893.6899 - val_loss: 2303.2573\n",
      "Epoch 73/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 2867.0979 - val_loss: 2279.5352\n",
      "Epoch 74/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2840.6895 - val_loss: 2255.9895\n",
      "Epoch 75/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 2814.4634 - val_loss: 2232.6196\n",
      "Epoch 76/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2788.4194 - val_loss: 2209.4253\n",
      "Epoch 77/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2762.5562 - val_loss: 2186.4038\n",
      "Epoch 78/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2736.8723 - val_loss: 2163.5559\n",
      "Epoch 79/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2711.3669 - val_loss: 2140.8794\n",
      "Epoch 80/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2686.0396 - val_loss: 2118.3743\n",
      "Epoch 81/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2660.8882 - val_loss: 2096.0393\n",
      "Epoch 82/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2635.9136 - val_loss: 2073.8726\n",
      "Epoch 83/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2611.1133 - val_loss: 2051.8745\n",
      "Epoch 84/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 2586.4868 - val_loss: 2030.0437\n",
      "Epoch 85/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2562.0339 - val_loss: 2008.3789\n",
      "Epoch 86/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2537.7532 - val_loss: 1986.8796\n",
      "Epoch 87/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2513.6428 - val_loss: 1965.5452\n",
      "Epoch 88/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2489.7034 - val_loss: 1944.3743\n",
      "Epoch 89/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2465.9331 - val_loss: 1923.3656\n",
      "Epoch 90/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2442.3311 - val_loss: 1902.5189\n",
      "Epoch 91/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2418.8970 - val_loss: 1881.8340\n",
      "Epoch 92/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2395.6296 - val_loss: 1861.3088\n",
      "Epoch 93/500\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 2372.5286 - val_loss: 1840.9430\n",
      "Epoch 94/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 2349.5920 - val_loss: 1820.7356\n",
      "Epoch 95/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 2326.8201 - val_loss: 1800.6871\n",
      "Epoch 96/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 2304.2114 - val_loss: 1780.7942\n",
      "Epoch 97/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 2281.7659 - val_loss: 1761.0582\n",
      "Epoch 98/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 2259.4817 - val_loss: 1741.4769\n",
      "Epoch 99/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 2237.3589 - val_loss: 1722.0508\n",
      "Epoch 100/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 2215.3960 - val_loss: 1702.7786\n",
      "Epoch 101/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 2193.5925 - val_loss: 1683.6584\n",
      "Epoch 102/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 2171.9478 - val_loss: 1664.6901\n",
      "Epoch 103/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 2150.4604 - val_loss: 1645.8738\n",
      "Epoch 104/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 2129.1304 - val_loss: 1627.2083\n",
      "Epoch 105/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 2107.9565 - val_loss: 1608.6917\n",
      "Epoch 106/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 2086.9380 - val_loss: 1590.3250\n",
      "Epoch 107/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 2066.0745 - val_loss: 1572.1057\n",
      "Epoch 108/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 2045.3646 - val_loss: 1554.0342\n",
      "Epoch 109/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 2024.8079 - val_loss: 1536.1096\n",
      "Epoch 110/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 2004.4033 - val_loss: 1518.3307\n",
      "Epoch 111/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1984.1508 - val_loss: 1500.6971\n",
      "Epoch 112/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1964.0491 - val_loss: 1483.2078\n",
      "Epoch 113/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 1944.0973 - val_loss: 1465.8622\n",
      "Epoch 114/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1924.2952 - val_loss: 1448.6599\n",
      "Epoch 115/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1904.6412 - val_loss: 1431.6000\n",
      "Epoch 116/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1885.1355 - val_loss: 1414.6812\n",
      "Epoch 117/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1865.7770 - val_loss: 1397.9028\n",
      "Epoch 118/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1846.5648 - val_loss: 1381.2653\n",
      "Epoch 119/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 1827.4984 - val_loss: 1364.7667\n",
      "Epoch 120/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1808.5769 - val_loss: 1348.4062\n",
      "Epoch 121/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1789.7998 - val_loss: 1332.1847\n",
      "Epoch 122/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1771.1663 - val_loss: 1316.0994\n",
      "Epoch 123/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1752.6754 - val_loss: 1300.1508\n",
      "Epoch 124/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1734.3271 - val_loss: 1284.3384\n",
      "Epoch 125/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1716.1195 - val_loss: 1268.6605\n",
      "Epoch 126/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1698.0533 - val_loss: 1253.1169\n",
      "Epoch 127/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1680.1267 - val_loss: 1237.7070\n",
      "Epoch 128/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1662.3395 - val_loss: 1222.4305\n",
      "Epoch 129/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1644.6910 - val_loss: 1207.2859\n",
      "Epoch 130/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1627.1804 - val_loss: 1192.2732\n",
      "Epoch 131/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1609.8071 - val_loss: 1177.3904\n",
      "Epoch 132/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1592.5706 - val_loss: 1162.6392\n",
      "Epoch 133/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1575.4696 - val_loss: 1148.0168\n",
      "Epoch 134/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1558.5042 - val_loss: 1133.5227\n",
      "Epoch 135/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1541.6731 - val_loss: 1119.1571\n",
      "Epoch 136/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1524.9756 - val_loss: 1104.9193\n",
      "Epoch 137/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1508.4115 - val_loss: 1090.8079\n",
      "Epoch 138/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1491.9797 - val_loss: 1076.8226\n",
      "Epoch 139/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1475.6799 - val_loss: 1062.9626\n",
      "Epoch 140/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1459.5111 - val_loss: 1049.2278\n",
      "Epoch 141/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1443.4729 - val_loss: 1035.6169\n",
      "Epoch 142/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1427.5643 - val_loss: 1022.1292\n",
      "Epoch 143/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1411.7852 - val_loss: 1008.7646\n",
      "Epoch 144/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1396.1345 - val_loss: 995.5218\n",
      "Epoch 145/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1380.6113 - val_loss: 982.4001\n",
      "Epoch 146/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1365.2156 - val_loss: 969.4000\n",
      "Epoch 147/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1349.9462 - val_loss: 956.5193\n",
      "Epoch 148/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1334.8026 - val_loss: 943.7586\n",
      "Epoch 149/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1319.7847 - val_loss: 931.1167\n",
      "Epoch 150/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1304.8910 - val_loss: 918.5924\n",
      "Epoch 151/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1290.1215 - val_loss: 906.1864\n",
      "Epoch 152/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1275.4752 - val_loss: 893.8964\n",
      "Epoch 153/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1260.9517 - val_loss: 881.7233\n",
      "Epoch 154/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1246.5497 - val_loss: 869.6655\n",
      "Epoch 155/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1232.2692 - val_loss: 857.7223\n",
      "Epoch 156/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1218.1094 - val_loss: 845.8939\n",
      "Epoch 157/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1204.0696 - val_loss: 834.1792\n",
      "Epoch 158/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1190.1499 - val_loss: 822.5778\n",
      "Epoch 159/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1176.3490 - val_loss: 811.0875\n",
      "Epoch 160/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1162.6656 - val_loss: 799.7106\n",
      "Epoch 161/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1149.1001 - val_loss: 788.4435\n",
      "Epoch 162/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1135.6516 - val_loss: 777.2876\n",
      "Epoch 163/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1122.3196 - val_loss: 766.2415\n",
      "Epoch 164/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1109.1033 - val_loss: 755.3046\n",
      "Epoch 165/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1096.0022 - val_loss: 744.4764\n",
      "Epoch 166/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 1083.0153 - val_loss: 733.7559\n",
      "Epoch 167/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1070.1423 - val_loss: 723.1429\n",
      "Epoch 168/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1057.3824 - val_loss: 712.6371\n",
      "Epoch 169/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 1044.7357 - val_loss: 702.2371\n",
      "Epoch 170/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1032.2009 - val_loss: 691.9429\n",
      "Epoch 171/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1019.7773 - val_loss: 681.7531\n",
      "Epoch 172/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1007.4644 - val_loss: 671.6678\n",
      "Epoch 173/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 995.2616 - val_loss: 661.6866\n",
      "Epoch 174/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 983.1691 - val_loss: 651.8086\n",
      "Epoch 175/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 971.1849 - val_loss: 642.0319\n",
      "Epoch 176/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 959.3094 - val_loss: 632.3578\n",
      "Epoch 177/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 947.5414 - val_loss: 622.7855\n",
      "Epoch 178/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 935.8812 - val_loss: 613.3129\n",
      "Epoch 179/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 924.3273 - val_loss: 603.9408\n",
      "Epoch 180/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 912.8791 - val_loss: 594.6681\n",
      "Epoch 181/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 901.5367 - val_loss: 585.4941\n",
      "Epoch 182/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 890.2988 - val_loss: 576.4184\n",
      "Epoch 183/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 879.1652 - val_loss: 567.4403\n",
      "Epoch 184/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 868.1351 - val_loss: 558.5593\n",
      "Epoch 185/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 857.2085 - val_loss: 549.7745\n",
      "Epoch 186/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 846.3840 - val_loss: 541.0858\n",
      "Epoch 187/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 835.6611 - val_loss: 532.4919\n",
      "Epoch 188/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 825.0397 - val_loss: 523.9930\n",
      "Epoch 189/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 814.5189 - val_loss: 515.5885\n",
      "Epoch 190/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 804.0985 - val_loss: 507.2769\n",
      "Epoch 191/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 793.7773 - val_loss: 499.0582\n",
      "Epoch 192/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 783.5551 - val_loss: 490.9320\n",
      "Epoch 193/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 773.4310 - val_loss: 482.8976\n",
      "Epoch 194/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 763.4050 - val_loss: 474.9539\n",
      "Epoch 195/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 753.4762 - val_loss: 467.1010\n",
      "Epoch 196/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 742.2527 - val_loss: 457.1764\n",
      "Epoch 197/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 730.1669 - val_loss: 447.6188\n",
      "Epoch 198/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 718.2343 - val_loss: 438.3555\n",
      "Epoch 199/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 706.6774 - val_loss: 429.4131\n",
      "Epoch 200/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 695.4836 - val_loss: 420.7533\n",
      "Epoch 201/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 684.6028 - val_loss: 412.3369\n",
      "Epoch 202/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 673.9902 - val_loss: 404.1318\n",
      "Epoch 203/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 663.6124 - val_loss: 396.1161\n",
      "Epoch 204/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 653.4435 - val_loss: 388.2714\n",
      "Epoch 205/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 643.4658 - val_loss: 380.5863\n",
      "Epoch 206/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 633.6639 - val_loss: 373.0495\n",
      "Epoch 207/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 624.0272 - val_loss: 365.6529\n",
      "Epoch 208/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 614.5461 - val_loss: 358.3901\n",
      "Epoch 209/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 605.2131 - val_loss: 351.2555\n",
      "Epoch 210/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 596.0217 - val_loss: 344.2431\n",
      "Epoch 211/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 586.9666 - val_loss: 337.3503\n",
      "Epoch 212/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 578.0435 - val_loss: 330.5725\n",
      "Epoch 213/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 569.2476 - val_loss: 323.9065\n",
      "Epoch 214/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 560.5758 - val_loss: 317.3498\n",
      "Epoch 215/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 552.0244 - val_loss: 310.8996\n",
      "Epoch 216/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 543.5908 - val_loss: 304.5531\n",
      "Epoch 217/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 535.2722 - val_loss: 298.3089\n",
      "Epoch 218/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 527.0661 - val_loss: 292.1645\n",
      "Epoch 219/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 518.9705 - val_loss: 286.1178\n",
      "Epoch 220/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 510.9828 - val_loss: 280.1671\n",
      "Epoch 221/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 503.1016 - val_loss: 274.3110\n",
      "Epoch 222/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 495.3245 - val_loss: 268.5479\n",
      "Epoch 223/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 487.6501 - val_loss: 262.8759\n",
      "Epoch 224/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 480.0770 - val_loss: 257.2935\n",
      "Epoch 225/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 472.6028 - val_loss: 251.7998\n",
      "Epoch 226/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 465.2269 - val_loss: 246.3936\n",
      "Epoch 227/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 457.9476 - val_loss: 241.0734\n",
      "Epoch 228/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 450.7633 - val_loss: 235.8378\n",
      "Epoch 229/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 443.6727 - val_loss: 230.6858\n",
      "Epoch 230/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 436.6750 - val_loss: 225.6164\n",
      "Epoch 231/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 429.7690 - val_loss: 220.6286\n",
      "Epoch 232/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 422.9531 - val_loss: 215.7206\n",
      "Epoch 233/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 416.2263 - val_loss: 210.8922\n",
      "Epoch 234/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 409.5877 - val_loss: 206.1421\n",
      "Epoch 235/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 403.0359 - val_loss: 201.4695\n",
      "Epoch 236/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 396.5703 - val_loss: 196.8731\n",
      "Epoch 237/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 390.1897 - val_loss: 192.3523\n",
      "Epoch 238/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 383.8930 - val_loss: 187.9059\n",
      "Epoch 239/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 377.6794 - val_loss: 183.5333\n",
      "Epoch 240/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 371.5478 - val_loss: 179.2333\n",
      "Epoch 241/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 365.4975 - val_loss: 175.0053\n",
      "Epoch 242/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 359.5274 - val_loss: 170.8482\n",
      "Epoch 243/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 353.6367 - val_loss: 166.7614\n",
      "Epoch 244/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 347.8247 - val_loss: 162.7442\n",
      "Epoch 245/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 342.0905 - val_loss: 158.7952\n",
      "Epoch 246/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 336.4330 - val_loss: 154.9147\n",
      "Epoch 247/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 330.8518 - val_loss: 151.1008\n",
      "Epoch 248/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 325.3457 - val_loss: 147.3529\n",
      "Epoch 249/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 319.9139 - val_loss: 143.6708\n",
      "Epoch 250/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 314.5560 - val_loss: 140.0533\n",
      "Epoch 251/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 309.2709 - val_loss: 136.5000\n",
      "Epoch 252/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 304.0581 - val_loss: 133.0099\n",
      "Epoch 253/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 298.9165 - val_loss: 129.5823\n",
      "Epoch 254/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 293.8455 - val_loss: 126.2164\n",
      "Epoch 255/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 288.8445 - val_loss: 122.9117\n",
      "Epoch 256/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 283.9128 - val_loss: 119.6674\n",
      "Epoch 257/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 279.0493 - val_loss: 116.4828\n",
      "Epoch 258/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 274.2538 - val_loss: 113.3574\n",
      "Epoch 259/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 269.5254 - val_loss: 110.2900\n",
      "Epoch 260/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 264.8633 - val_loss: 107.2806\n",
      "Epoch 261/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 260.2668 - val_loss: 104.3279\n",
      "Epoch 262/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 255.7354 - val_loss: 101.4319\n",
      "Epoch 263/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 251.2685 - val_loss: 98.5913\n",
      "Epoch 264/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 246.8651 - val_loss: 95.8060\n",
      "Epoch 265/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 242.5247 - val_loss: 93.0747\n",
      "Epoch 266/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 238.2467 - val_loss: 90.3975\n",
      "Epoch 267/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 234.0303 - val_loss: 87.7731\n",
      "Epoch 268/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 229.8752 - val_loss: 85.2011\n",
      "Epoch 269/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 225.7803 - val_loss: 82.6811\n",
      "Epoch 270/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 221.7452 - val_loss: 80.2123\n",
      "Epoch 271/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 217.7694 - val_loss: 77.7941\n",
      "Epoch 272/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 213.8522 - val_loss: 75.4261\n",
      "Epoch 273/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 209.9929 - val_loss: 73.1072\n",
      "Epoch 274/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 206.1908 - val_loss: 70.8372\n",
      "Epoch 275/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 202.4455 - val_loss: 68.6153\n",
      "Epoch 276/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 198.7563 - val_loss: 66.4411\n",
      "Epoch 277/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 195.1225 - val_loss: 64.3136\n",
      "Epoch 278/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 191.5436 - val_loss: 62.2327\n",
      "Epoch 279/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 188.0192 - val_loss: 60.1976\n",
      "Epoch 280/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 184.5484 - val_loss: 58.2077\n",
      "Epoch 281/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 181.1308 - val_loss: 56.2626\n",
      "Epoch 282/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 177.7659 - val_loss: 54.3616\n",
      "Epoch 283/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 174.4530 - val_loss: 52.5040\n",
      "Epoch 284/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 171.1915 - val_loss: 50.6894\n",
      "Epoch 285/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 167.9807 - val_loss: 48.9170\n",
      "Epoch 286/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 164.8202 - val_loss: 47.1867\n",
      "Epoch 287/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 161.7096 - val_loss: 45.4976\n",
      "Epoch 288/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 158.6483 - val_loss: 43.8491\n",
      "Epoch 289/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 155.6353 - val_loss: 42.2409\n",
      "Epoch 290/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 152.6707 - val_loss: 40.6725\n",
      "Epoch 291/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 149.7536 - val_loss: 39.1430\n",
      "Epoch 292/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 146.8835 - val_loss: 37.6521\n",
      "Epoch 293/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 144.0598 - val_loss: 36.1993\n",
      "Epoch 294/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 141.2820 - val_loss: 34.7838\n",
      "Epoch 295/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 138.5495 - val_loss: 33.4054\n",
      "Epoch 296/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 135.8620 - val_loss: 32.0635\n",
      "Epoch 297/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 133.2188 - val_loss: 30.7576\n",
      "Epoch 298/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 130.6196 - val_loss: 29.4870\n",
      "Epoch 299/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 128.0636 - val_loss: 28.2513\n",
      "Epoch 300/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 125.5503 - val_loss: 27.0500\n",
      "Epoch 301/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 123.0795 - val_loss: 25.8827\n",
      "Epoch 302/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 120.6504 - val_loss: 24.7487\n",
      "Epoch 303/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 118.2626 - val_loss: 23.6477\n",
      "Epoch 304/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 115.9156 - val_loss: 22.5790\n",
      "Epoch 305/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 113.6087 - val_loss: 21.5422\n",
      "Epoch 306/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 111.3416 - val_loss: 20.5367\n",
      "Epoch 307/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 109.1139 - val_loss: 19.5624\n",
      "Epoch 308/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 106.9252 - val_loss: 18.6185\n",
      "Epoch 309/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 104.7748 - val_loss: 17.7044\n",
      "Epoch 310/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 102.6620 - val_loss: 16.8200\n",
      "Epoch 311/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 100.5867 - val_loss: 15.9646\n",
      "Epoch 312/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 98.5482 - val_loss: 15.1376\n",
      "Epoch 313/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 96.5462 - val_loss: 14.3388\n",
      "Epoch 314/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 94.5802 - val_loss: 13.5676\n",
      "Epoch 315/500\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 92.6496 - val_loss: 12.8236\n",
      "Epoch 316/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 90.7540 - val_loss: 12.1062\n",
      "Epoch 317/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 88.8931 - val_loss: 11.4152\n",
      "Epoch 318/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 87.0662 - val_loss: 10.7500\n",
      "Epoch 319/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 85.2731 - val_loss: 10.1101\n",
      "Epoch 320/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 83.5129 - val_loss: 9.4951\n",
      "Epoch 321/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 81.7857 - val_loss: 8.9047\n",
      "Epoch 322/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 80.0909 - val_loss: 8.3383\n",
      "Epoch 323/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 78.4279 - val_loss: 7.7955\n",
      "Epoch 324/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 76.7964 - val_loss: 7.2760\n",
      "Epoch 325/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 75.1958 - val_loss: 6.7792\n",
      "Epoch 326/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 73.6259 - val_loss: 6.3047\n",
      "Epoch 327/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 72.0859 - val_loss: 5.8521\n",
      "Epoch 328/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 70.5758 - val_loss: 5.4212\n",
      "Epoch 329/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 69.0951 - val_loss: 5.0113\n",
      "Epoch 330/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 67.6433 - val_loss: 4.6221\n",
      "Epoch 331/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 66.2198 - val_loss: 4.2532\n",
      "Epoch 332/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 64.8243 - val_loss: 3.9041\n",
      "Epoch 333/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 63.4565 - val_loss: 3.5746\n",
      "Epoch 334/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 62.1161 - val_loss: 3.2642\n",
      "Epoch 335/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 60.8026 - val_loss: 2.9726\n",
      "Epoch 336/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 59.5156 - val_loss: 2.6993\n",
      "Epoch 337/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 58.2546 - val_loss: 2.4439\n",
      "Epoch 338/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 57.0193 - val_loss: 2.2061\n",
      "Epoch 339/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 55.8093 - val_loss: 1.9854\n",
      "Epoch 340/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 54.6241 - val_loss: 1.7816\n",
      "Epoch 341/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 53.4635 - val_loss: 1.5943\n",
      "Epoch 342/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 52.3269 - val_loss: 1.4230\n",
      "Epoch 343/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 51.2142 - val_loss: 1.2675\n",
      "Epoch 344/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 50.1248 - val_loss: 1.1273\n",
      "Epoch 345/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 49.0584 - val_loss: 1.0022\n",
      "Epoch 346/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 48.0148 - val_loss: 0.8917\n",
      "Epoch 347/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 46.9934 - val_loss: 0.7956\n",
      "Epoch 348/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 45.9940 - val_loss: 0.7135\n",
      "Epoch 349/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 45.0162 - val_loss: 0.6449\n",
      "Epoch 350/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 44.0596 - val_loss: 0.5897\n",
      "Epoch 351/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 43.1239 - val_loss: 0.5475\n",
      "Epoch 352/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 42.2087 - val_loss: 0.5180\n",
      "Epoch 353/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 41.3138 - val_loss: 0.5007\n",
      "Epoch 354/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 40.4387 - val_loss: 0.4955\n",
      "Epoch 355/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 39.5832 - val_loss: 0.5020\n",
      "Epoch 356/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 38.7470 - val_loss: 0.5198\n",
      "Epoch 357/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 37.9294 - val_loss: 0.5487\n",
      "Epoch 358/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 37.1305 - val_loss: 0.5884\n",
      "Epoch 359/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 36.3498 - val_loss: 0.6385\n",
      "Epoch 360/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 35.5871 - val_loss: 0.6988\n",
      "Epoch 361/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 34.8418 - val_loss: 0.7690\n",
      "Epoch 362/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 34.1138 - val_loss: 0.8487\n",
      "Epoch 363/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 33.4029 - val_loss: 0.9378\n",
      "Epoch 364/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 32.7085 - val_loss: 1.0359\n",
      "Epoch 365/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 32.0306 - val_loss: 1.1427\n",
      "Epoch 366/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 31.3688 - val_loss: 1.2579\n",
      "Epoch 367/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 30.7227 - val_loss: 1.3814\n",
      "Epoch 368/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 30.0921 - val_loss: 1.5128\n",
      "Epoch 369/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 29.4766 - val_loss: 1.6518\n",
      "Epoch 370/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 28.8762 - val_loss: 1.7982\n",
      "Epoch 371/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 28.2903 - val_loss: 1.9518\n",
      "Epoch 372/500\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 27.7188 - val_loss: 2.1123\n",
      "Epoch 373/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 27.1614 - val_loss: 2.2794\n",
      "Epoch 374/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 26.6178 - val_loss: 2.4529\n",
      "Epoch 375/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 26.0877 - val_loss: 2.6326\n",
      "Epoch 376/500\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 25.5709 - val_loss: 2.8182\n",
      "Epoch 377/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 25.0671 - val_loss: 3.0094\n",
      "Epoch 378/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 24.5761 - val_loss: 3.2062\n",
      "Epoch 379/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 24.0975 - val_loss: 3.4081\n",
      "Epoch 380/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 23.6312 - val_loss: 3.6151\n",
      "Epoch 381/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 23.1769 - val_loss: 3.8268\n",
      "Epoch 382/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 22.7344 - val_loss: 4.0432\n",
      "Epoch 383/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 22.3034 - val_loss: 4.2639\n",
      "Epoch 384/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 21.8837 - val_loss: 4.4887\n",
      "Epoch 385/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 21.4750 - val_loss: 4.7176\n",
      "Epoch 386/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 21.0771 - val_loss: 4.9502\n",
      "Epoch 387/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 20.6898 - val_loss: 5.1863\n",
      "Epoch 388/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 20.3129 - val_loss: 5.4258\n",
      "Epoch 389/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 19.9461 - val_loss: 5.6685\n",
      "Epoch 390/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 19.5893 - val_loss: 5.9142\n",
      "Epoch 391/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 19.2421 - val_loss: 6.1627\n",
      "Epoch 392/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 18.9045 - val_loss: 6.4137\n",
      "Epoch 393/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 18.5762 - val_loss: 6.6673\n",
      "Epoch 394/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 18.2570 - val_loss: 6.9231\n",
      "Epoch 395/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 17.9465 - val_loss: 7.1811\n",
      "Epoch 396/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 17.6448 - val_loss: 7.4410\n",
      "Epoch 397/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 17.3516 - val_loss: 7.7027\n",
      "Epoch 398/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 17.0667 - val_loss: 7.9661\n",
      "Epoch 399/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.7898 - val_loss: 8.2309\n",
      "Epoch 400/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.5210 - val_loss: 8.4970\n",
      "Epoch 401/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.2598 - val_loss: 8.7643\n",
      "Epoch 402/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 16.0062 - val_loss: 9.0327\n",
      "Epoch 403/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 15.7600 - val_loss: 9.3020\n",
      "Epoch 404/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 15.5209 - val_loss: 9.5719\n",
      "Epoch 405/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 15.2889 - val_loss: 9.8427\n",
      "Epoch 406/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 15.0637 - val_loss: 10.1138\n",
      "Epoch 407/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 14.8452 - val_loss: 10.3854\n",
      "Epoch 408/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 14.6332 - val_loss: 10.6573\n",
      "Epoch 409/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 14.4276 - val_loss: 10.9291\n",
      "Epoch 410/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 14.2283 - val_loss: 11.2011\n",
      "Epoch 411/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 14.0350 - val_loss: 11.4729\n",
      "Epoch 412/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 13.8476 - val_loss: 11.7445\n",
      "Epoch 413/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 13.6660 - val_loss: 12.0157\n",
      "Epoch 414/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 13.4899 - val_loss: 12.2865\n",
      "Epoch 415/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 13.3194 - val_loss: 12.5570\n",
      "Epoch 416/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 13.1541 - val_loss: 12.8266\n",
      "Epoch 417/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 12.9940 - val_loss: 13.0956\n",
      "Epoch 418/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 12.8390 - val_loss: 13.3637\n",
      "Epoch 419/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 12.6889 - val_loss: 13.6310\n",
      "Epoch 420/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 12.5436 - val_loss: 13.8972\n",
      "Epoch 421/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 12.4030 - val_loss: 14.1624\n",
      "Epoch 422/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 12.2669 - val_loss: 14.4263\n",
      "Epoch 423/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 12.1353 - val_loss: 14.6891\n",
      "Epoch 424/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 12.0079 - val_loss: 14.9506\n",
      "Epoch 425/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 11.8846 - val_loss: 15.2107\n",
      "Epoch 426/500\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 11.7654 - val_loss: 15.4693\n",
      "Epoch 427/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 11.6503 - val_loss: 15.7264\n",
      "Epoch 428/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 11.5389 - val_loss: 15.9818\n",
      "Epoch 429/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 11.4313 - val_loss: 16.2357\n",
      "Epoch 430/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 11.3273 - val_loss: 16.4878\n",
      "Epoch 431/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 11.2267 - val_loss: 16.7380\n",
      "Epoch 432/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 11.1297 - val_loss: 16.9865\n",
      "Epoch 433/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 11.0360 - val_loss: 17.2332\n",
      "Epoch 434/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 10.9455 - val_loss: 17.4779\n",
      "Epoch 435/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 10.8581 - val_loss: 17.7206\n",
      "Epoch 436/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 10.7737 - val_loss: 17.9612\n",
      "Epoch 437/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 10.6923 - val_loss: 18.1997\n",
      "Epoch 438/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 10.6138 - val_loss: 18.4363\n",
      "Epoch 439/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 10.5380 - val_loss: 18.6705\n",
      "Epoch 440/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 10.4650 - val_loss: 18.9028\n",
      "Epoch 441/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 10.3945 - val_loss: 19.1326\n",
      "Epoch 442/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 10.3266 - val_loss: 19.3604\n",
      "Epoch 443/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 10.2611 - val_loss: 19.5857\n",
      "Epoch 444/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 10.1980 - val_loss: 19.8086\n",
      "Epoch 445/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 10.1372 - val_loss: 20.0292\n",
      "Epoch 446/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 10.0787 - val_loss: 20.2473\n",
      "Epoch 447/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 10.0223 - val_loss: 20.4634\n",
      "Epoch 448/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 9.9679 - val_loss: 20.6767\n",
      "Epoch 449/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 9.9157 - val_loss: 20.8879\n",
      "Epoch 450/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 9.8653 - val_loss: 21.0964\n",
      "Epoch 451/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 9.8169 - val_loss: 21.3023\n",
      "Epoch 452/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 9.7703 - val_loss: 21.5059\n",
      "Epoch 453/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 9.7255 - val_loss: 21.7071\n",
      "Epoch 454/500\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 9.6824 - val_loss: 21.9054\n",
      "Epoch 455/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 9.6410 - val_loss: 22.1014\n",
      "Epoch 456/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 9.6011 - val_loss: 22.2949\n",
      "Epoch 457/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 9.5628 - val_loss: 22.4857\n",
      "Epoch 458/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 9.5260 - val_loss: 22.6741\n",
      "Epoch 459/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 9.4907 - val_loss: 22.8598\n",
      "Epoch 460/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 9.4567 - val_loss: 23.0430\n",
      "Epoch 461/500\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 9.4241 - val_loss: 23.2237\n",
      "Epoch 462/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 9.3928 - val_loss: 23.4019\n",
      "Epoch 463/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 9.3627 - val_loss: 23.5773\n",
      "Epoch 464/500\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 9.3339 - val_loss: 23.7504\n",
      "Epoch 465/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 9.3062 - val_loss: 23.9208\n",
      "Epoch 466/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 9.2797 - val_loss: 24.0888\n",
      "Epoch 467/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 9.2542 - val_loss: 24.2542\n",
      "Epoch 468/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 9.2297 - val_loss: 24.4170\n",
      "Epoch 469/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 9.2063 - val_loss: 24.5774\n",
      "Epoch 470/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 9.1839 - val_loss: 24.7351\n",
      "Epoch 471/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 9.1624 - val_loss: 24.8904\n",
      "Epoch 472/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 9.1418 - val_loss: 25.0433\n",
      "Epoch 473/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 9.1220 - val_loss: 25.1936\n",
      "Epoch 474/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 9.1031 - val_loss: 25.3414\n",
      "Epoch 475/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 9.0850 - val_loss: 25.4866\n",
      "Epoch 476/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 9.0677 - val_loss: 25.6296\n",
      "Epoch 477/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 9.0511 - val_loss: 25.7700\n",
      "Epoch 478/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 9.0353 - val_loss: 25.9082\n",
      "Epoch 479/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 9.0201 - val_loss: 26.0439\n",
      "Epoch 480/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 9.0055 - val_loss: 26.1772\n",
      "Epoch 481/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 8.9917 - val_loss: 26.3080\n",
      "Epoch 482/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 8.9784 - val_loss: 26.4367\n",
      "Epoch 483/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.9657 - val_loss: 26.5630\n",
      "Epoch 484/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 8.9536 - val_loss: 26.6869\n",
      "Epoch 485/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.9420 - val_loss: 26.8086\n",
      "Epoch 486/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.9310 - val_loss: 26.9279\n",
      "Epoch 487/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 8.9204 - val_loss: 27.0451\n",
      "Epoch 488/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 8.9103 - val_loss: 27.1599\n",
      "Epoch 489/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.9007 - val_loss: 27.2726\n",
      "Epoch 490/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 8.8915 - val_loss: 27.3831\n",
      "Epoch 491/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.8828 - val_loss: 27.4916\n",
      "Epoch 492/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.8744 - val_loss: 27.5977\n",
      "Epoch 493/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.8664 - val_loss: 27.7018\n",
      "Epoch 494/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 8.8588 - val_loss: 27.8038\n",
      "Epoch 495/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.8516 - val_loss: 27.9036\n",
      "Epoch 496/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.8447 - val_loss: 28.0016\n",
      "Epoch 497/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.8381 - val_loss: 28.0975\n",
      "Epoch 498/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.8318 - val_loss: 28.1914\n",
      "Epoch 499/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8.8258 - val_loss: 28.2833\n",
      "Epoch 500/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 8.8202 - val_loss: 28.3733\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(85.7336, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0020, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0125, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.3008, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 359ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.02754435e+01, 7.02530345e+01, 7.02306256e+01, 7.02082166e+01,\n",
       "        7.01858077e+01, 7.01633987e+01, 7.01409897e+01, 7.01185808e+01,\n",
       "        7.00961718e+01, 7.00737628e+01, 7.00513539e+01, 7.00289449e+01,\n",
       "        7.00065359e+01, 6.99682540e+01, 6.99234360e+01, 6.98786181e+01,\n",
       "        6.98338002e+01, 6.97889823e+01, 6.97441643e+01, 6.96993464e+01,\n",
       "        6.96545285e+01, 6.96097106e+01, 6.95648926e+01, 6.95200747e+01,\n",
       "        6.94752568e+01, 6.94304388e+01, 6.93856209e+01, 6.93408030e+01,\n",
       "        6.92959851e+01, 6.92511671e+01, 6.92063492e+01, 6.91615313e+01,\n",
       "        6.91167133e+01, 6.90718954e+01, 6.90270775e+01, 6.89822596e+01,\n",
       "        6.89374416e+01, 6.88926237e+01, 6.88478058e+01, 6.88029879e+01,\n",
       "        6.87581699e+01, 6.87133520e+01, 6.86685341e+01, 6.86237162e+01,\n",
       "        6.85788982e+01, 6.85340803e+01, 6.84892624e+01, 6.84444444e+01,\n",
       "        6.83996149e+01, 6.83533964e+01, 6.83071779e+01, 6.82609594e+01,\n",
       "        6.82147409e+01, 6.81685224e+01, 6.81223039e+01, 6.80760854e+01,\n",
       "        6.80298669e+01, 6.79836485e+01, 6.79374300e+01, 6.78912115e+01,\n",
       "        6.78449930e+01, 6.77987745e+01, 6.77525560e+01, 6.77063375e+01,\n",
       "        6.76601191e+01, 6.76139006e+01, 6.75676821e+01, 6.75214636e+01,\n",
       "        6.74752451e+01, 6.74290266e+01, 6.73828081e+01, 6.73365896e+01,\n",
       "        6.72903711e+01, 6.72441527e+01, 6.71979342e+01, 6.71517157e+01,\n",
       "        6.71054972e+01, 6.70592787e+01, 6.70130602e+01, 6.69668417e+01,\n",
       "        7.34346924e+01, 0.00000000e+00, 0.00000000e+00, 3.62565637e-01,\n",
       "        1.95016831e-01, 2.61160940e-01, 0.00000000e+00, 5.12230396e-01,\n",
       "        0.00000000e+00, 2.82885671e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.52500081e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.77785605e-01, 3.51549834e-01, 5.96743941e-01, 1.54180937e-02]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([65.12079832, 65.11239496, 65.1039916 , 65.09558824, 65.08718487,\n",
       "       65.07878151, 65.07037815, 65.06197479, 65.05357143, 65.04516807,\n",
       "       65.03676471, 65.02836134, 65.01995798, 65.01155462, 65.00315126,\n",
       "       64.9947479 , 64.98634454, 64.97794118, 64.96953782, 64.96113445,\n",
       "       64.95273109, 64.94432773, 64.93592437, 64.92752101, 64.91911765,\n",
       "       64.91071429, 64.90231092, 64.89593838, 64.89033613, 64.88473389,\n",
       "       64.87913165, 64.87352941, 64.86792717, 64.86232493, 64.85672269,\n",
       "       64.85112045, 64.84551821, 64.83991597, 64.83431373, 64.82871148,\n",
       "       64.82310924, 64.817507  , 64.81190476, 64.80630252, 64.80070028,\n",
       "       64.79509804, 64.7894958 , 64.78389356, 64.77829132, 64.77268908,\n",
       "       64.76708683, 64.76148459, 64.75588235, 64.75028011, 64.74467787,\n",
       "       64.73907563, 64.73347339, 64.72787115, 64.72226891, 64.71666667,\n",
       "       64.71106443, 64.70546218, 64.69985994, 64.6942577 , 64.68865546,\n",
       "       64.68305322, 64.67745098, 64.67184874, 64.6662465 , 64.66064426,\n",
       "       64.65504202, 64.64943978, 64.64383754, 64.63823529, 64.63263305,\n",
       "       64.62703081, 64.62142857, 64.61582633, 64.61022409, 64.60462185,\n",
       "       64.59901961, 64.59341737, 64.58781513, 64.58221289, 64.57661064,\n",
       "       64.5710084 , 64.56540616, 64.55980392, 64.55420168, 64.54859944,\n",
       "       64.5429972 , 64.53739496, 64.53179272, 64.52619048, 64.52058824,\n",
       "       64.51498599, 64.50938375, 64.50378151, 64.49817927, 64.49257703])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.304031375663605\n",
      "15.448863358229273\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
