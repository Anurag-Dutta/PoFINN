{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "2345    56.893831\n",
       "2346    56.881533\n",
       "2347    56.869236\n",
       "2348    56.856938\n",
       "2349    56.844640\n",
       "Name: C4, Length: 2350, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c4_interpolated_2250_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "2245     0.404535\n",
       "2246     0.588284\n",
       "2247     0.000000\n",
       "2248     0.000000\n",
       "2249     0.000000\n",
       "Name: C4, Length: 2250, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2250)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.409524</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.219048</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.028571</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.838095</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.600000  0.000298\n",
       "1     88.409524  0.000298\n",
       "2     88.219048  0.000297\n",
       "3     88.028571  0.000297\n",
       "4     87.838095  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6856  # Value for C0\n",
    "K0 = -0.0008  # Value for K0\n",
    "K1 = -0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0128    # Value for b\n",
    "c = -2.3003    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.600000    88.409524    88.219048    88.028571    87.838095   \n",
      "351    88.409524    88.219048    88.028571    87.838095    87.647619   \n",
      "352    88.219048    88.028571    87.838095    87.647619    87.457143   \n",
      "353    88.028571    87.838095    87.647619    87.457143    87.266667   \n",
      "354    87.838095    87.647619    87.457143    87.266667    87.076190   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.647619    87.457143    87.266667    87.076190    86.896218  ...   \n",
      "351    87.457143    87.266667    87.076190    86.896218    86.845798  ...   \n",
      "352    87.266667    87.076190    86.896218    86.845798    86.795378  ...   \n",
      "353    87.076190    86.896218    86.845798    86.795378    86.744958  ...   \n",
      "354    86.896218    86.845798    86.795378    86.744958    86.694538  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   78.880345    0.000263   78.856069    0.000263   78.831793    0.000263   \n",
      "351   78.856069    0.000263   78.831793    0.000263   78.807516    0.000262   \n",
      "352   78.831793    0.000263   78.807516    0.000262   78.783240    0.000262   \n",
      "353   78.807516    0.000262   78.783240    0.000262   78.758964    0.000262   \n",
      "354   78.783240    0.000262   78.758964    0.000262   78.734687    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   78.807516    0.000262   78.783240    0.000262  \n",
      "351   78.783240    0.000262   78.758964    0.000262  \n",
      "352   78.758964    0.000262   78.734687    0.000262  \n",
      "353   78.734687    0.000262   78.710411    0.000262  \n",
      "354   78.710411    0.000262   78.686134    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 1, 251) (1800, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "23/23 [==============================] - 3s 27ms/step - loss: 5049.8389 - val_loss: 2745.4739\n",
      "Epoch 2/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4741.6709 - val_loss: 2606.5159\n",
      "Epoch 3/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4616.1123 - val_loss: 2541.8535\n",
      "Epoch 4/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4514.8467 - val_loss: 2489.5408\n",
      "Epoch 5/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4422.9092 - val_loss: 2440.7256\n",
      "Epoch 6/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4334.5391 - val_loss: 2393.8352\n",
      "Epoch 7/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4248.7358 - val_loss: 2348.6006\n",
      "Epoch 8/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4165.0591 - val_loss: 2304.7979\n",
      "Epoch 9/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4083.2122 - val_loss: 2261.5562\n",
      "Epoch 10/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3994.6833 - val_loss: 2214.0466\n",
      "Epoch 11/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3908.9756 - val_loss: 2171.2463\n",
      "Epoch 12/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3827.2834 - val_loss: 2128.5576\n",
      "Epoch 13/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 3739.6902 - val_loss: 2084.8423\n",
      "Epoch 14/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3656.5425 - val_loss: 2044.1788\n",
      "Epoch 15/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 3576.3591 - val_loss: 2005.6327\n",
      "Epoch 16/500\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 3498.5994 - val_loss: 1968.3536\n",
      "Epoch 17/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3422.8691 - val_loss: 1932.4078\n",
      "Epoch 18/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 3348.9351 - val_loss: 1897.7159\n",
      "Epoch 19/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 3276.6494 - val_loss: 1864.2092\n",
      "Epoch 20/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 3205.9104 - val_loss: 1831.8361\n",
      "Epoch 21/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 3136.6421 - val_loss: 1800.5530\n",
      "Epoch 22/500\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 3068.7839 - val_loss: 1770.3260\n",
      "Epoch 23/500\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 3002.2871 - val_loss: 1741.1245\n",
      "Epoch 24/500\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2937.1101 - val_loss: 1712.9219\n",
      "Epoch 25/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2873.2158 - val_loss: 1685.6948\n",
      "Epoch 26/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 2810.5732 - val_loss: 1659.4094\n",
      "Epoch 27/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 2749.1404 - val_loss: 1633.6296\n",
      "Epoch 28/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 2690.1992 - val_loss: 1612.3055\n",
      "Epoch 29/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 2629.7083 - val_loss: 1583.8536\n",
      "Epoch 30/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 2571.7998 - val_loss: 1560.1467\n",
      "Epoch 31/500\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 2515.0571 - val_loss: 1539.3125\n",
      "Epoch 32/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 2459.4749 - val_loss: 1521.6554\n",
      "Epoch 33/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 2404.8936 - val_loss: 1501.5834\n",
      "Epoch 34/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2351.3921 - val_loss: 1482.3229\n",
      "Epoch 35/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2298.9456 - val_loss: 1463.8593\n",
      "Epoch 36/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2247.5354 - val_loss: 1446.1776\n",
      "Epoch 37/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2197.1458 - val_loss: 1429.2635\n",
      "Epoch 38/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2147.7605 - val_loss: 1413.1029\n",
      "Epoch 39/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2099.3647 - val_loss: 1397.6823\n",
      "Epoch 40/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 2051.9431 - val_loss: 1382.9880\n",
      "Epoch 41/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 2005.4807 - val_loss: 1369.0068\n",
      "Epoch 42/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 1959.9639 - val_loss: 1355.7252\n",
      "Epoch 43/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 1915.3783 - val_loss: 1343.1311\n",
      "Epoch 44/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1871.7106 - val_loss: 1331.2109\n",
      "Epoch 45/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 1828.9462 - val_loss: 1319.9524\n",
      "Epoch 46/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1787.0731 - val_loss: 1309.3430\n",
      "Epoch 47/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1746.0773 - val_loss: 1299.3708\n",
      "Epoch 48/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1705.9469 - val_loss: 1290.0229\n",
      "Epoch 49/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1666.6682 - val_loss: 1281.2877\n",
      "Epoch 50/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1628.2290 - val_loss: 1273.1533\n",
      "Epoch 51/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1590.6179 - val_loss: 1265.6079\n",
      "Epoch 52/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1553.8215 - val_loss: 1258.5248\n",
      "Epoch 53/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1517.8286 - val_loss: 1252.2405\n",
      "Epoch 54/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1482.6267 - val_loss: 1246.3915\n",
      "Epoch 55/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1448.2045 - val_loss: 1241.0852\n",
      "Epoch 56/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1414.5504 - val_loss: 1236.3099\n",
      "Epoch 57/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1381.6525 - val_loss: 1232.0548\n",
      "Epoch 58/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1349.4999 - val_loss: 1228.3083\n",
      "Epoch 59/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1318.0808 - val_loss: 1225.0596\n",
      "Epoch 60/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1287.3849 - val_loss: 1222.2972\n",
      "Epoch 61/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1257.4003 - val_loss: 1220.0107\n",
      "Epoch 62/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 1228.1163 - val_loss: 1218.1892\n",
      "Epoch 63/500\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1199.5222 - val_loss: 1216.8215\n",
      "Epoch 64/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 1171.6072 - val_loss: 1215.8972\n",
      "Epoch 65/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1144.3607 - val_loss: 1215.4055\n",
      "Epoch 66/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1117.7716 - val_loss: 1215.3361\n",
      "Epoch 67/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1091.8302 - val_loss: 1215.6781\n",
      "Epoch 68/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1066.5256 - val_loss: 1216.4214\n",
      "Epoch 69/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1041.8477 - val_loss: 1217.5552\n",
      "Epoch 70/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1017.7862 - val_loss: 1219.0697\n",
      "Epoch 71/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 994.3309 - val_loss: 1220.9543\n",
      "Epoch 72/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 971.4719 - val_loss: 1223.1990\n",
      "Epoch 73/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 949.1986 - val_loss: 1225.7936\n",
      "Epoch 74/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 927.5022 - val_loss: 1228.7279\n",
      "Epoch 75/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 906.3717 - val_loss: 1231.9923\n",
      "Epoch 76/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 885.7981 - val_loss: 1235.5765\n",
      "Epoch 77/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 865.7714 - val_loss: 1239.4708\n",
      "Epoch 78/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 846.2819 - val_loss: 1243.6655\n",
      "Epoch 79/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 827.3203 - val_loss: 1248.1506\n",
      "Epoch 80/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 808.8770 - val_loss: 1252.9169\n",
      "Epoch 81/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 790.9424 - val_loss: 1257.9547\n",
      "Epoch 82/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 773.5072 - val_loss: 1263.2538\n",
      "Epoch 83/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 756.5627 - val_loss: 1268.8057\n",
      "Epoch 84/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 740.0987 - val_loss: 1274.6005\n",
      "Epoch 85/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 724.1069 - val_loss: 1280.6288\n",
      "Epoch 86/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 708.5781 - val_loss: 1286.8817\n",
      "Epoch 87/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 693.5029 - val_loss: 1293.3500\n",
      "Epoch 88/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 678.8726 - val_loss: 1300.0248\n",
      "Epoch 89/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 664.6787 - val_loss: 1306.8965\n",
      "Epoch 90/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 650.9118 - val_loss: 1313.9565\n",
      "Epoch 91/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 637.5637 - val_loss: 1321.1960\n",
      "Epoch 92/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 624.6254 - val_loss: 1328.6061\n",
      "Epoch 93/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 612.0887 - val_loss: 1336.1783\n",
      "Epoch 94/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 599.9448 - val_loss: 1343.9041\n",
      "Epoch 95/500\n",
      "23/23 [==============================] - 1s 37ms/step - loss: 588.1853 - val_loss: 1351.7743\n",
      "Epoch 96/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 576.8022 - val_loss: 1359.7814\n",
      "Epoch 97/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 565.7867 - val_loss: 1367.9165\n",
      "Epoch 98/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 555.1311 - val_loss: 1376.1714\n",
      "Epoch 99/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 544.8270 - val_loss: 1384.5383\n",
      "Epoch 100/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 534.8665 - val_loss: 1393.0085\n",
      "Epoch 101/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 525.2414 - val_loss: 1401.5750\n",
      "Epoch 102/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 515.9440 - val_loss: 1410.2288\n",
      "Epoch 103/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 506.9665 - val_loss: 1418.9634\n",
      "Epoch 104/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 498.3012 - val_loss: 1427.7700\n",
      "Epoch 105/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 489.9404 - val_loss: 1436.6417\n",
      "Epoch 106/500\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 481.8765 - val_loss: 1445.5714\n",
      "Epoch 107/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 474.1022 - val_loss: 1454.5516\n",
      "Epoch 108/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 466.6098 - val_loss: 1463.5743\n",
      "Epoch 109/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 459.3922 - val_loss: 1472.6335\n",
      "Epoch 110/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 452.4424 - val_loss: 1481.7217\n",
      "Epoch 111/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 445.7528 - val_loss: 1490.8326\n",
      "Epoch 112/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 439.3166 - val_loss: 1499.9584\n",
      "Epoch 113/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 433.1270 - val_loss: 1509.0938\n",
      "Epoch 114/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 427.1769 - val_loss: 1518.2316\n",
      "Epoch 115/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 421.4598 - val_loss: 1527.3660\n",
      "Epoch 116/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 415.9687 - val_loss: 1536.4908\n",
      "Epoch 117/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 410.6972 - val_loss: 1545.6000\n",
      "Epoch 118/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 405.6389 - val_loss: 1554.6870\n",
      "Epoch 119/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 400.7875 - val_loss: 1563.7467\n",
      "Epoch 120/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 396.1365 - val_loss: 1572.7734\n",
      "Epoch 121/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 391.6799 - val_loss: 1581.7620\n",
      "Epoch 122/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 387.4114 - val_loss: 1590.7069\n",
      "Epoch 123/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 383.3254 - val_loss: 1599.6027\n",
      "Epoch 124/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 379.4156 - val_loss: 1608.4448\n",
      "Epoch 125/500\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 375.6766 - val_loss: 1617.2285\n",
      "Epoch 126/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 372.1026 - val_loss: 1625.9491\n",
      "Epoch 127/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 368.6880 - val_loss: 1634.6028\n",
      "Epoch 128/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 365.4274 - val_loss: 1643.1835\n",
      "Epoch 129/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 362.3156 - val_loss: 1651.6876\n",
      "Epoch 130/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 359.3474 - val_loss: 1660.1115\n",
      "Epoch 131/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 356.5175 - val_loss: 1668.4518\n",
      "Epoch 132/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 353.8208 - val_loss: 1676.7043\n",
      "Epoch 133/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 351.2525 - val_loss: 1684.8650\n",
      "Epoch 134/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 348.8081 - val_loss: 1692.9314\n",
      "Epoch 135/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 346.4827 - val_loss: 1700.9001\n",
      "Epoch 136/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 344.2717 - val_loss: 1708.7678\n",
      "Epoch 137/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 342.1705 - val_loss: 1716.5323\n",
      "Epoch 138/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 340.1751 - val_loss: 1724.1895\n",
      "Epoch 139/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 338.2810 - val_loss: 1731.7388\n",
      "Epoch 140/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 336.4843 - val_loss: 1739.1760\n",
      "Epoch 141/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 334.7809 - val_loss: 1746.5007\n",
      "Epoch 142/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 333.1668 - val_loss: 1753.7094\n",
      "Epoch 143/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 331.6384 - val_loss: 1760.8018\n",
      "Epoch 144/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 330.1918 - val_loss: 1767.7739\n",
      "Epoch 145/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 328.8237 - val_loss: 1774.6267\n",
      "Epoch 146/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 327.5302 - val_loss: 1781.3572\n",
      "Epoch 147/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 326.3084 - val_loss: 1787.9646\n",
      "Epoch 148/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 325.1549 - val_loss: 1794.4482\n",
      "Epoch 149/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 324.0664 - val_loss: 1800.8066\n",
      "Epoch 150/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 323.0401 - val_loss: 1807.0396\n",
      "Epoch 151/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 322.0728 - val_loss: 1813.1462\n",
      "Epoch 152/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 321.1618 - val_loss: 1819.1261\n",
      "Epoch 153/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 320.3046 - val_loss: 1824.9781\n",
      "Epoch 154/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 319.4980 - val_loss: 1830.7042\n",
      "Epoch 155/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 318.7399 - val_loss: 1836.3022\n",
      "Epoch 156/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 318.0278 - val_loss: 1841.7729\n",
      "Epoch 157/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 317.3591 - val_loss: 1847.1173\n",
      "Epoch 158/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 316.7318 - val_loss: 1852.3348\n",
      "Epoch 159/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 316.1436 - val_loss: 1857.4257\n",
      "Epoch 160/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 315.5924 - val_loss: 1862.3920\n",
      "Epoch 161/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 315.0763 - val_loss: 1867.2327\n",
      "Epoch 162/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 314.5932 - val_loss: 1871.9500\n",
      "Epoch 163/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 314.1414 - val_loss: 1876.5447\n",
      "Epoch 164/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 313.7192 - val_loss: 1881.0161\n",
      "Epoch 165/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 313.3249 - val_loss: 1885.3672\n",
      "Epoch 166/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.9567 - val_loss: 1889.5984\n",
      "Epoch 167/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.6133 - val_loss: 1893.7111\n",
      "Epoch 168/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 312.2932 - val_loss: 1897.7059\n",
      "Epoch 169/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 311.9949 - val_loss: 1901.5859\n",
      "Epoch 170/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 311.7173 - val_loss: 1905.3521\n",
      "Epoch 171/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 311.4589 - val_loss: 1909.0059\n",
      "Epoch 172/500\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 311.2186 - val_loss: 1912.5490\n",
      "Epoch 173/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 310.9953 - val_loss: 1915.9824\n",
      "Epoch 174/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 310.7879 - val_loss: 1919.3091\n",
      "Epoch 175/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 310.5955 - val_loss: 1922.5293\n",
      "Epoch 176/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 310.4170 - val_loss: 1925.6471\n",
      "Epoch 177/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 310.2516 - val_loss: 1928.6620\n",
      "Epoch 178/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 310.0983 - val_loss: 1931.5769\n",
      "Epoch 179/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 309.9565 - val_loss: 1934.3953\n",
      "Epoch 180/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 309.8252 - val_loss: 1937.1168\n",
      "Epoch 181/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 309.7038 - val_loss: 1939.7456\n",
      "Epoch 182/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 309.5916 - val_loss: 1942.2812\n",
      "Epoch 183/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 309.4881 - val_loss: 1944.7278\n",
      "Epoch 184/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 309.3925 - val_loss: 1947.0873\n",
      "Epoch 185/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 309.3043 - val_loss: 1949.3616\n",
      "Epoch 186/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 309.2230 - val_loss: 1951.5514\n",
      "Epoch 187/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 309.1481 - val_loss: 1953.6608\n",
      "Epoch 188/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 309.0792 - val_loss: 1955.6909\n",
      "Epoch 189/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 309.0157 - val_loss: 1957.6434\n",
      "Epoch 190/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.9574 - val_loss: 1959.5217\n",
      "Epoch 191/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 308.9037 - val_loss: 1961.3260\n",
      "Epoch 192/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.8545 - val_loss: 1963.0604\n",
      "Epoch 193/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.8093 - val_loss: 1964.7253\n",
      "Epoch 194/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.7677 - val_loss: 1966.3240\n",
      "Epoch 195/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.7297 - val_loss: 1967.8573\n",
      "Epoch 196/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.6948 - val_loss: 1969.3289\n",
      "Epoch 197/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.6628 - val_loss: 1970.7382\n",
      "Epoch 198/500\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 308.6336 - val_loss: 1972.0903\n",
      "Epoch 199/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.6069 - val_loss: 1973.3844\n",
      "Epoch 200/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.5824 - val_loss: 1974.6237\n",
      "Epoch 201/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.5602 - val_loss: 1975.8099\n",
      "Epoch 202/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.5397 - val_loss: 1976.9448\n",
      "Epoch 203/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 308.5211 - val_loss: 1978.0295\n",
      "Epoch 204/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 308.5041 - val_loss: 1979.0659\n",
      "Epoch 205/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 308.4886 - val_loss: 1980.0568\n",
      "Epoch 206/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 308.4745 - val_loss: 1981.0034\n",
      "Epoch 207/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.4617 - val_loss: 1981.9059\n",
      "Epoch 208/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.4501 - val_loss: 1982.7681\n",
      "Epoch 209/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.4395 - val_loss: 1983.5906\n",
      "Epoch 210/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.4298 - val_loss: 1984.3739\n",
      "Epoch 211/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.4211 - val_loss: 1985.1217\n",
      "Epoch 212/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 308.4132 - val_loss: 1985.8337\n",
      "Epoch 213/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.4061 - val_loss: 1986.5116\n",
      "Epoch 214/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3996 - val_loss: 1987.1561\n",
      "Epoch 215/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3939 - val_loss: 1987.7714\n",
      "Epoch 216/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3886 - val_loss: 1988.3560\n",
      "Epoch 217/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3839 - val_loss: 1988.9119\n",
      "Epoch 218/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3796 - val_loss: 1989.4399\n",
      "Epoch 219/500\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 308.3759 - val_loss: 1989.9413\n",
      "Epoch 220/500\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 308.3724 - val_loss: 1990.4189\n",
      "Epoch 221/500\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 308.3693 - val_loss: 1990.8716\n",
      "Epoch 222/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3667 - val_loss: 1991.3025\n",
      "Epoch 223/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 308.3643 - val_loss: 1991.7097\n",
      "Epoch 224/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3622 - val_loss: 1992.0972\n",
      "Epoch 225/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3603 - val_loss: 1992.4635\n",
      "Epoch 226/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3587 - val_loss: 1992.8108\n",
      "Epoch 227/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3573 - val_loss: 1993.1411\n",
      "Epoch 228/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 308.3561 - val_loss: 1993.4531\n",
      "Epoch 229/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 308.3550 - val_loss: 1993.7487\n",
      "Epoch 230/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3542 - val_loss: 1994.0283\n",
      "Epoch 231/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3534 - val_loss: 1994.2924\n",
      "Epoch 232/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3528 - val_loss: 1994.5432\n",
      "Epoch 233/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3523 - val_loss: 1994.7799\n",
      "Epoch 234/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3520 - val_loss: 1995.0032\n",
      "Epoch 235/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 308.3517 - val_loss: 1995.2147\n",
      "Epoch 236/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3515 - val_loss: 1995.4141\n",
      "Epoch 237/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 308.3514 - val_loss: 1995.6027\n",
      "Epoch 238/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 308.3514 - val_loss: 1995.7808\n",
      "Epoch 239/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3516 - val_loss: 1995.9498\n",
      "Epoch 240/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3516 - val_loss: 1996.1078\n",
      "Epoch 241/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3519 - val_loss: 1996.2578\n",
      "Epoch 242/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 308.3521 - val_loss: 1996.4000\n",
      "Epoch 243/500\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 308.3524 - val_loss: 1996.5332\n",
      "Epoch 244/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3527 - val_loss: 1996.6580\n",
      "Epoch 245/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3531 - val_loss: 1996.7764\n",
      "Epoch 246/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3534 - val_loss: 1996.8882\n",
      "Epoch 247/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3539 - val_loss: 1996.9941\n",
      "Epoch 248/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3543 - val_loss: 1997.0913\n",
      "Epoch 249/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3548 - val_loss: 1997.1849\n",
      "Epoch 250/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3553 - val_loss: 1997.2719\n",
      "Epoch 251/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3557 - val_loss: 1997.3535\n",
      "Epoch 252/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3564 - val_loss: 1997.4308\n",
      "Epoch 253/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3568 - val_loss: 1997.5032\n",
      "Epoch 254/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3575 - val_loss: 1997.5712\n",
      "Epoch 255/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3581 - val_loss: 1997.6361\n",
      "Epoch 256/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3586 - val_loss: 1997.6960\n",
      "Epoch 257/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3593 - val_loss: 1997.7513\n",
      "Epoch 258/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3599 - val_loss: 1997.8043\n",
      "Epoch 259/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3604 - val_loss: 1997.8535\n",
      "Epoch 260/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3610 - val_loss: 1997.9000\n",
      "Epoch 261/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3617 - val_loss: 1997.9434\n",
      "Epoch 262/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3624 - val_loss: 1997.9847\n",
      "Epoch 263/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3629 - val_loss: 1998.0233\n",
      "Epoch 264/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 308.3635 - val_loss: 1998.0601\n",
      "Epoch 265/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3642 - val_loss: 1998.0939\n",
      "Epoch 266/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3647 - val_loss: 1998.1239\n",
      "Epoch 267/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 308.3654 - val_loss: 1998.1531\n",
      "Epoch 268/500\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 308.3660 - val_loss: 1998.1804\n",
      "Epoch 269/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3666 - val_loss: 1998.2063\n",
      "Epoch 270/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3672 - val_loss: 1998.2302\n",
      "Epoch 271/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3679 - val_loss: 1998.2534\n",
      "Epoch 272/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3684 - val_loss: 1998.2738\n",
      "Epoch 273/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3690 - val_loss: 1998.2932\n",
      "Epoch 274/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3697 - val_loss: 1998.3132\n",
      "Epoch 275/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3702 - val_loss: 1998.3298\n",
      "Epoch 276/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3708 - val_loss: 1998.3455\n",
      "Epoch 277/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3714 - val_loss: 1998.3602\n",
      "Epoch 278/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3719 - val_loss: 1998.3733\n",
      "Epoch 279/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3725 - val_loss: 1998.3865\n",
      "Epoch 280/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3731 - val_loss: 1998.3977\n",
      "Epoch 281/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3736 - val_loss: 1998.4095\n",
      "Epoch 282/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3742 - val_loss: 1998.4194\n",
      "Epoch 283/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3747 - val_loss: 1998.4296\n",
      "Epoch 284/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3752 - val_loss: 1998.4386\n",
      "Epoch 285/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3757 - val_loss: 1998.4473\n",
      "Epoch 286/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3762 - val_loss: 1998.4545\n",
      "Epoch 287/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3767 - val_loss: 1998.4622\n",
      "Epoch 288/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3772 - val_loss: 1998.4683\n",
      "Epoch 289/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3777 - val_loss: 1998.4740\n",
      "Epoch 290/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3783 - val_loss: 1998.4788\n",
      "Epoch 291/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3788 - val_loss: 1998.4855\n",
      "Epoch 292/500\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 308.3792 - val_loss: 1998.4902\n",
      "Epoch 293/500\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 308.3796 - val_loss: 1998.4944\n",
      "Epoch 294/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 308.3802 - val_loss: 1998.4991\n",
      "Epoch 295/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3806 - val_loss: 1998.5024\n",
      "Epoch 296/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3810 - val_loss: 1998.5066\n",
      "Epoch 297/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3814 - val_loss: 1998.5106\n",
      "Epoch 298/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3819 - val_loss: 1998.5132\n",
      "Epoch 299/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3823 - val_loss: 1998.5155\n",
      "Epoch 300/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3827 - val_loss: 1998.5184\n",
      "Epoch 301/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3831 - val_loss: 1998.5198\n",
      "Epoch 302/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3835 - val_loss: 1998.5219\n",
      "Epoch 303/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3839 - val_loss: 1998.5238\n",
      "Epoch 304/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3842 - val_loss: 1998.5256\n",
      "Epoch 305/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3846 - val_loss: 1998.5262\n",
      "Epoch 306/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3851 - val_loss: 1998.5281\n",
      "Epoch 307/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3854 - val_loss: 1998.5293\n",
      "Epoch 308/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3857 - val_loss: 1998.5305\n",
      "Epoch 309/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3861 - val_loss: 1998.5325\n",
      "Epoch 310/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3864 - val_loss: 1998.5326\n",
      "Epoch 311/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3869 - val_loss: 1998.5333\n",
      "Epoch 312/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3871 - val_loss: 1998.5338\n",
      "Epoch 313/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3874 - val_loss: 1998.5350\n",
      "Epoch 314/500\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 308.3878 - val_loss: 1998.5374\n",
      "Epoch 315/500\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 308.3880 - val_loss: 1998.5380\n",
      "Epoch 316/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3884 - val_loss: 1998.5387\n",
      "Epoch 317/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3886 - val_loss: 1998.5387\n",
      "Epoch 318/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3889 - val_loss: 1998.5382\n",
      "Epoch 319/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3893 - val_loss: 1998.5387\n",
      "Epoch 320/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3895 - val_loss: 1998.5391\n",
      "Epoch 321/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3897 - val_loss: 1998.5387\n",
      "Epoch 322/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3901 - val_loss: 1998.5386\n",
      "Epoch 323/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3904 - val_loss: 1998.5382\n",
      "Epoch 324/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3906 - val_loss: 1998.5380\n",
      "Epoch 325/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3909 - val_loss: 1998.5386\n",
      "Epoch 326/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3911 - val_loss: 1998.5383\n",
      "Epoch 327/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3914 - val_loss: 1998.5389\n",
      "Epoch 328/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3915 - val_loss: 1998.5383\n",
      "Epoch 329/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3919 - val_loss: 1998.5383\n",
      "Epoch 330/500\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 308.3920 - val_loss: 1998.5377\n",
      "Epoch 331/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.3924 - val_loss: 1998.5380\n",
      "Epoch 332/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3925 - val_loss: 1998.5371\n",
      "Epoch 333/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3928 - val_loss: 1998.5374\n",
      "Epoch 334/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3929 - val_loss: 1998.5369\n",
      "Epoch 335/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3932 - val_loss: 1998.5371\n",
      "Epoch 336/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3932 - val_loss: 1998.5358\n",
      "Epoch 337/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3936 - val_loss: 1998.5363\n",
      "Epoch 338/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3937 - val_loss: 1998.5359\n",
      "Epoch 339/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3940 - val_loss: 1998.5361\n",
      "Epoch 340/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3941 - val_loss: 1998.5361\n",
      "Epoch 341/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3943 - val_loss: 1998.5347\n",
      "Epoch 342/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3945 - val_loss: 1998.5349\n",
      "Epoch 343/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3947 - val_loss: 1998.5354\n",
      "Epoch 344/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3948 - val_loss: 1998.5344\n",
      "Epoch 345/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3950 - val_loss: 1998.5341\n",
      "Epoch 346/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3952 - val_loss: 1998.5337\n",
      "Epoch 347/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3954 - val_loss: 1998.5337\n",
      "Epoch 348/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3955 - val_loss: 1998.5337\n",
      "Epoch 349/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3957 - val_loss: 1998.5333\n",
      "Epoch 350/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3958 - val_loss: 1998.5337\n",
      "Epoch 351/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3959 - val_loss: 1998.5328\n",
      "Epoch 352/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3962 - val_loss: 1998.5328\n",
      "Epoch 353/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3962 - val_loss: 1998.5326\n",
      "Epoch 354/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3964 - val_loss: 1998.5321\n",
      "Epoch 355/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3965 - val_loss: 1998.5321\n",
      "Epoch 356/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3966 - val_loss: 1998.5316\n",
      "Epoch 357/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3968 - val_loss: 1998.5308\n",
      "Epoch 358/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3970 - val_loss: 1998.5316\n",
      "Epoch 359/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3970 - val_loss: 1998.5314\n",
      "Epoch 360/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3971 - val_loss: 1998.5309\n",
      "Epoch 361/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3973 - val_loss: 1998.5308\n",
      "Epoch 362/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3974 - val_loss: 1998.5308\n",
      "Epoch 363/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3975 - val_loss: 1998.5302\n",
      "Epoch 364/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3976 - val_loss: 1998.5305\n",
      "Epoch 365/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3976 - val_loss: 1998.5291\n",
      "Epoch 366/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3977 - val_loss: 1998.5283\n",
      "Epoch 367/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3979 - val_loss: 1998.5284\n",
      "Epoch 368/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3980 - val_loss: 1998.5280\n",
      "Epoch 369/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3980 - val_loss: 1998.5267\n",
      "Epoch 370/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3981 - val_loss: 1998.5259\n",
      "Epoch 371/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3983 - val_loss: 1998.5260\n",
      "Epoch 372/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3986 - val_loss: 1998.5267\n",
      "Epoch 373/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3985 - val_loss: 1998.5260\n",
      "Epoch 374/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3985 - val_loss: 1998.5254\n",
      "Epoch 375/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3986 - val_loss: 1998.5247\n",
      "Epoch 376/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3988 - val_loss: 1998.5248\n",
      "Epoch 377/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3989 - val_loss: 1998.5247\n",
      "Epoch 378/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3989 - val_loss: 1998.5245\n",
      "Epoch 379/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3989 - val_loss: 1998.5236\n",
      "Epoch 380/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3990 - val_loss: 1998.5227\n",
      "Epoch 381/500\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 308.3991 - val_loss: 1998.5227\n",
      "Epoch 382/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.3991 - val_loss: 1998.5222\n",
      "Epoch 383/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3992 - val_loss: 1998.5217\n",
      "Epoch 384/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3992 - val_loss: 1998.5203\n",
      "Epoch 385/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3994 - val_loss: 1998.5203\n",
      "Epoch 386/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3995 - val_loss: 1998.5201\n",
      "Epoch 387/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3996 - val_loss: 1998.5203\n",
      "Epoch 388/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.3996 - val_loss: 1998.5198\n",
      "Epoch 389/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3997 - val_loss: 1998.5194\n",
      "Epoch 390/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3997 - val_loss: 1998.5188\n",
      "Epoch 391/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3998 - val_loss: 1998.5186\n",
      "Epoch 392/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3999 - val_loss: 1998.5184\n",
      "Epoch 393/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3999 - val_loss: 1998.5188\n",
      "Epoch 394/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4000 - val_loss: 1998.5189\n",
      "Epoch 395/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4000 - val_loss: 1998.5194\n",
      "Epoch 396/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.3999 - val_loss: 1998.5182\n",
      "Epoch 397/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4001 - val_loss: 1998.5173\n",
      "Epoch 398/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4002 - val_loss: 1998.5167\n",
      "Epoch 399/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4003 - val_loss: 1998.5168\n",
      "Epoch 400/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4004 - val_loss: 1998.5170\n",
      "Epoch 401/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4004 - val_loss: 1998.5167\n",
      "Epoch 402/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4005 - val_loss: 1998.5170\n",
      "Epoch 403/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4005 - val_loss: 1998.5168\n",
      "Epoch 404/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4005 - val_loss: 1998.5164\n",
      "Epoch 405/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4006 - val_loss: 1998.5164\n",
      "Epoch 406/500\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 308.4006 - val_loss: 1998.5165\n",
      "Epoch 407/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.4006 - val_loss: 1998.5161\n",
      "Epoch 408/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4007 - val_loss: 1998.5156\n",
      "Epoch 409/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4007 - val_loss: 1998.5151\n",
      "Epoch 410/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4008 - val_loss: 1998.5153\n",
      "Epoch 411/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4008 - val_loss: 1998.5149\n",
      "Epoch 412/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4008 - val_loss: 1998.5143\n",
      "Epoch 413/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4008 - val_loss: 1998.5132\n",
      "Epoch 414/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4009 - val_loss: 1998.5128\n",
      "Epoch 415/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4009 - val_loss: 1998.5123\n",
      "Epoch 416/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4009 - val_loss: 1998.5116\n",
      "Epoch 417/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4010 - val_loss: 1998.5110\n",
      "Epoch 418/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4010 - val_loss: 1998.5099\n",
      "Epoch 419/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4011 - val_loss: 1998.5092\n",
      "Epoch 420/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4010 - val_loss: 1998.5085\n",
      "Epoch 421/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4011 - val_loss: 1998.5082\n",
      "Epoch 422/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4012 - val_loss: 1998.5087\n",
      "Epoch 423/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4012 - val_loss: 1998.5092\n",
      "Epoch 424/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4013 - val_loss: 1998.5095\n",
      "Epoch 425/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4012 - val_loss: 1998.5095\n",
      "Epoch 426/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4013 - val_loss: 1998.5107\n",
      "Epoch 427/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4013 - val_loss: 1998.5107\n",
      "Epoch 428/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4014 - val_loss: 1998.5115\n",
      "Epoch 429/500\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 308.4013 - val_loss: 1998.5111\n",
      "Epoch 430/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.4014 - val_loss: 1998.5118\n",
      "Epoch 431/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4013 - val_loss: 1998.5120\n",
      "Epoch 432/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4014 - val_loss: 1998.5116\n",
      "Epoch 433/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4014 - val_loss: 1998.5104\n",
      "Epoch 434/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4014 - val_loss: 1998.5094\n",
      "Epoch 435/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4013 - val_loss: 1998.5083\n",
      "Epoch 436/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4014 - val_loss: 1998.5070\n",
      "Epoch 437/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4015 - val_loss: 1998.5065\n",
      "Epoch 438/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4015 - val_loss: 1998.5059\n",
      "Epoch 439/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4014 - val_loss: 1998.5056\n",
      "Epoch 440/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4015 - val_loss: 1998.5046\n",
      "Epoch 441/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4016 - val_loss: 1998.5045\n",
      "Epoch 442/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4016 - val_loss: 1998.5044\n",
      "Epoch 443/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4017 - val_loss: 1998.5045\n",
      "Epoch 444/500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 308.4017 - val_loss: 1998.5052\n",
      "Epoch 445/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4017 - val_loss: 1998.5057\n",
      "Epoch 446/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4017 - val_loss: 1998.5062\n",
      "Epoch 447/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4017 - val_loss: 1998.5061\n",
      "Epoch 448/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4018 - val_loss: 1998.5071\n",
      "Epoch 449/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4018 - val_loss: 1998.5073\n",
      "Epoch 450/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4017 - val_loss: 1998.5070\n",
      "Epoch 451/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4018 - val_loss: 1998.5059\n",
      "Epoch 452/500\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 308.4018 - val_loss: 1998.5050\n",
      "Epoch 453/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4017 - val_loss: 1998.5049\n",
      "Epoch 454/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4018 - val_loss: 1998.5042\n",
      "Epoch 455/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4017 - val_loss: 1998.5028\n",
      "Epoch 456/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4017 - val_loss: 1998.5013\n",
      "Epoch 457/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4017 - val_loss: 1998.5009\n",
      "Epoch 458/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4017 - val_loss: 1998.4988\n",
      "Epoch 459/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4018 - val_loss: 1998.4983\n",
      "Epoch 460/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4018 - val_loss: 1998.4979\n",
      "Epoch 461/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4018 - val_loss: 1998.4971\n",
      "Epoch 462/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4019 - val_loss: 1998.4966\n",
      "Epoch 463/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4019 - val_loss: 1998.4954\n",
      "Epoch 464/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4019 - val_loss: 1998.4944\n",
      "Epoch 465/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4019 - val_loss: 1998.4934\n",
      "Epoch 466/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4019 - val_loss: 1998.4927\n",
      "Epoch 467/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4019 - val_loss: 1998.4922\n",
      "Epoch 468/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4020 - val_loss: 1998.4918\n",
      "Epoch 469/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4020 - val_loss: 1998.4923\n",
      "Epoch 470/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4020 - val_loss: 1998.4927\n",
      "Epoch 471/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4020 - val_loss: 1998.4933\n",
      "Epoch 472/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4021 - val_loss: 1998.4938\n",
      "Epoch 473/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4021 - val_loss: 1998.4941\n",
      "Epoch 474/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4022 - val_loss: 1998.4951\n",
      "Epoch 475/500\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 308.4022 - val_loss: 1998.4960\n",
      "Epoch 476/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4021 - val_loss: 1998.4972\n",
      "Epoch 477/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4021 - val_loss: 1998.4972\n",
      "Epoch 478/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4021 - val_loss: 1998.4988\n",
      "Epoch 479/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4022 - val_loss: 1998.4993\n",
      "Epoch 480/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4022 - val_loss: 1998.4996\n",
      "Epoch 481/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4022 - val_loss: 1998.5005\n",
      "Epoch 482/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4022 - val_loss: 1998.5011\n",
      "Epoch 483/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4022 - val_loss: 1998.5017\n",
      "Epoch 484/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4022 - val_loss: 1998.5029\n",
      "Epoch 485/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4023 - val_loss: 1998.5037\n",
      "Epoch 486/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4023 - val_loss: 1998.5049\n",
      "Epoch 487/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4022 - val_loss: 1998.5050\n",
      "Epoch 488/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4022 - val_loss: 1998.5054\n",
      "Epoch 489/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4022 - val_loss: 1998.5062\n",
      "Epoch 490/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4023 - val_loss: 1998.5066\n",
      "Epoch 491/500\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 308.4023 - val_loss: 1998.5077\n",
      "Epoch 492/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 308.4023 - val_loss: 1998.5077\n",
      "Epoch 493/500\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 308.4022 - val_loss: 1998.5082\n",
      "Epoch 494/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.4022 - val_loss: 1998.5079\n",
      "Epoch 495/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4022 - val_loss: 1998.5079\n",
      "Epoch 496/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4022 - val_loss: 1998.5077\n",
      "Epoch 497/500\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 308.4022 - val_loss: 1998.5077\n",
      "Epoch 498/500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 308.4022 - val_loss: 1998.5074\n",
      "Epoch 499/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4022 - val_loss: 1998.5071\n",
      "Epoch 500/500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 308.4022 - val_loss: 1998.5070\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6856, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0008, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0128, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(-2.3003, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 501ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.04733193e+01, 7.03976891e+01, 7.03220588e+01, 7.02464286e+01,\n",
       "        7.60199509e+01, 4.52334553e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 8.47232521e-01, 6.27414048e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.06105742e+01, 7.05349440e+01, 7.04593137e+01,\n",
       "        0.00000000e+00, 9.55886240e-01, 7.12558823e+01, 7.08021008e+01,\n",
       "        7.06413866e+01, 7.05657563e+01, 7.04901261e+01, 7.04144958e+01,\n",
       "        7.03388655e+01, 7.02632353e+01, 7.01876050e+01, 0.00000000e+00,\n",
       "        7.03150560e-02, 7.05209384e+01, 7.04453081e+01, 7.03696779e+01,\n",
       "        7.02940476e+01, 7.02184174e+01, 7.40361811e+01, 7.29198179e+01,\n",
       "        7.16424370e+01, 2.80094090e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.78183290e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.89780742e-01, 2.00733528e-01, 7.02800420e+01,\n",
       "        7.02044118e+01, 4.45066273e-01, 0.00000000e+00, 7.05377451e+01,\n",
       "        7.04621149e+01, 7.03864846e+01, 7.03108543e+01, 7.02352241e+01,\n",
       "        7.42042484e+01, 7.31719188e+01, 7.19449580e+01, 0.00000000e+00,\n",
       "        8.16541190e-01, 0.00000000e+00, 7.02660364e+01, 7.01904062e+01,\n",
       "        7.36341036e+01, 7.24995798e+01, 7.11382353e+01, 7.30318628e+01,\n",
       "        7.17768908e+01, 7.06525910e+01, 6.53601830e-01, 7.42013680e-02,\n",
       "        0.00000000e+00, 7.18918381e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        6.82894897e+01, 0.00000000e+00, 0.00000000e+00, 8.52342695e-02,\n",
       "        0.00000000e+00, 2.72075772e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 8.05050075e-01, 1.62787922e-02, 1.71003774e-01,\n",
       "        0.00000000e+00, 1.10638225e+00, 1.19919121e+00, 4.49004531e-01,\n",
       "        0.00000000e+00, 1.95731699e-01, 8.11637282e-01, 9.95560586e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58.06210289, 58.04980529, 58.03750769, 58.02521008, 58.01291248,\n",
       "       58.00061488, 57.98831728, 57.97601968, 57.96372207, 57.95142447,\n",
       "       57.93912687, 57.92682927, 57.91453167, 57.90223406, 57.88993646,\n",
       "       57.87763886, 57.86534126, 57.85304366, 57.84074605, 57.82844845,\n",
       "       57.81615085, 57.80385325, 57.79155565, 57.77925804, 57.76696044,\n",
       "       57.75466284, 57.74236524, 57.73006764, 57.71777003, 57.70547243,\n",
       "       57.69317483, 57.68087723, 57.66857963, 57.65628203, 57.64398442,\n",
       "       57.63168682, 57.61938922, 57.60709162, 57.59479402, 57.58249641,\n",
       "       57.57019881, 57.55790121, 57.54560361, 57.53330601, 57.5210084 ,\n",
       "       57.5087108 , 57.4964132 , 57.4841156 , 57.471818  , 57.45952039,\n",
       "       57.44722279, 57.43492519, 57.42262759, 57.41032999, 57.39803238,\n",
       "       57.38573478, 57.37343718, 57.36113958, 57.34884198, 57.33654437,\n",
       "       57.32424677, 57.31194917, 57.29965157, 57.28735397, 57.27505636,\n",
       "       57.26275876, 57.25046116, 57.23816356, 57.22586596, 57.21356835,\n",
       "       57.20127075, 57.18897315, 57.17667555, 57.16437795, 57.15208034,\n",
       "       57.13978274, 57.12748514, 57.11518754, 57.10288994, 57.09059233,\n",
       "       57.07829473, 57.06599713, 57.05369953, 57.04140193, 57.02910432,\n",
       "       57.01680672, 57.00450912, 56.99221152, 56.97991392, 56.96761631,\n",
       "       56.95531871, 56.94302111, 56.93072351, 56.91842591, 56.9061283 ,\n",
       "       56.8938307 , 56.8815331 , 56.8692355 , 56.8569379 , 56.8446403 ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.336475766429196\n",
      "37.49813882942636\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
