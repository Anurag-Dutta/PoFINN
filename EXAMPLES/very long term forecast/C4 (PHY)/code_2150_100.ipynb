{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "2245    58.123591\n",
       "2246    58.111293\n",
       "2247    58.098996\n",
       "2248    58.086698\n",
       "2249    58.074400\n",
       "Name: C4, Length: 2250, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c4_interpolated_2150_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "2145     0.084953\n",
       "2146     0.000000\n",
       "2147     0.179084\n",
       "2148     0.000000\n",
       "2149     0.000000\n",
       "Name: C4, Length: 2150, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(2150)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.409524</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.219048</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.028571</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.838095</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.600000  0.000298\n",
       "1     88.409524  0.000298\n",
       "2     88.219048  0.000297\n",
       "3     88.028571  0.000297\n",
       "4     87.838095  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6856  # Value for C0\n",
    "K0 = -0.0008  # Value for K0\n",
    "K1 = -0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0128    # Value for b\n",
    "c = -2.3003    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.600000    88.409524    88.219048    88.028571    87.838095   \n",
      "351    88.409524    88.219048    88.028571    87.838095    87.647619   \n",
      "352    88.219048    88.028571    87.838095    87.647619    87.457143   \n",
      "353    88.028571    87.838095    87.647619    87.457143    87.266667   \n",
      "354    87.838095    87.647619    87.457143    87.266667    87.076190   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.647619    87.457143    87.266667    87.076190    86.896218  ...   \n",
      "351    87.457143    87.266667    87.076190    86.896218    86.845798  ...   \n",
      "352    87.266667    87.076190    86.896218    86.845798    86.795378  ...   \n",
      "353    87.076190    86.896218    86.845798    86.795378    86.744958  ...   \n",
      "354    86.896218    86.845798    86.795378    86.744958    86.694538  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   78.880345    0.000263   78.856069    0.000263   78.831793    0.000263   \n",
      "351   78.856069    0.000263   78.831793    0.000263   78.807516    0.000262   \n",
      "352   78.831793    0.000263   78.807516    0.000262   78.783240    0.000262   \n",
      "353   78.807516    0.000262   78.783240    0.000262   78.758964    0.000262   \n",
      "354   78.783240    0.000262   78.758964    0.000262   78.734687    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   78.807516    0.000262   78.783240    0.000262  \n",
      "351   78.783240    0.000262   78.758964    0.000262  \n",
      "352   78.758964    0.000262   78.734687    0.000262  \n",
      "353   78.734687    0.000262   78.710411    0.000262  \n",
      "354   78.710411    0.000262   78.686134    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1700, 1, 251) (1700, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 3s 32ms/step - loss: 5227.1523 - val_loss: 2937.7668\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 4947.1152 - val_loss: 2805.7197\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 4795.3975 - val_loss: 2734.6426\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 4700.0396 - val_loss: 2680.8425\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 4608.6104 - val_loss: 2629.8516\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 4520.5757 - val_loss: 2580.8301\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 4435.0869 - val_loss: 2533.4263\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 4351.6577 - val_loss: 2487.4258\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 4270.0088 - val_loss: 2442.6992\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 4189.9663 - val_loss: 2399.1594\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 4111.4092 - val_loss: 2356.7429\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 4034.2512 - val_loss: 2315.4016\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 3958.4265 - val_loss: 2275.0989\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 3883.8845 - val_loss: 2235.8120\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 3810.5791 - val_loss: 2197.4050\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 3738.4827 - val_loss: 2160.1279\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 3667.5583 - val_loss: 2123.7026\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 3597.7812 - val_loss: 2088.1924\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 3529.1277 - val_loss: 2053.5801\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 3461.5769 - val_loss: 2019.8499\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 3395.1074 - val_loss: 1986.9862\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 3329.7029 - val_loss: 1954.9752\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 3265.3445 - val_loss: 1923.8026\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 3202.0168 - val_loss: 1893.4556\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 3139.7051 - val_loss: 1863.9209\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 3078.3948 - val_loss: 1835.1875\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 3018.0718 - val_loss: 1807.2424\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2958.7222 - val_loss: 1780.0745\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2900.3333 - val_loss: 1753.6721\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2842.8928 - val_loss: 1728.0250\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2786.3887 - val_loss: 1703.1213\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2730.8086 - val_loss: 1678.9513\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2676.1418 - val_loss: 1655.5040\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2622.3762 - val_loss: 1632.7695\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2569.5007 - val_loss: 1610.7373\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 2517.5046 - val_loss: 1589.3979\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 2466.3774 - val_loss: 1568.7413\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 2416.1089 - val_loss: 1548.7577\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2366.6890 - val_loss: 1529.4377\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2318.1064 - val_loss: 1510.7717\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2270.3523 - val_loss: 1492.7500\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2223.4163 - val_loss: 1475.3638\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2177.2888 - val_loss: 1458.6033\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2131.9597 - val_loss: 1442.4600\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2087.4209 - val_loss: 1426.9248\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2043.6616 - val_loss: 1411.9883\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2000.6732 - val_loss: 1397.6421\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1958.4467 - val_loss: 1383.8767\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 1916.9728 - val_loss: 1370.6843\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 1876.2424 - val_loss: 1358.0557\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 1836.2472 - val_loss: 1345.9823\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1796.9778 - val_loss: 1334.4552\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 1758.4259 - val_loss: 1323.4666\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 1720.5824 - val_loss: 1313.0076\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 1683.4393 - val_loss: 1303.0701\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1646.9880 - val_loss: 1293.6454\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1611.2198 - val_loss: 1284.7253\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 1576.1268 - val_loss: 1276.3018\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1541.7002 - val_loss: 1268.3663\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1507.9324 - val_loss: 1260.9113\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1474.8146 - val_loss: 1253.9280\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1442.3392 - val_loss: 1247.4087\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1410.4982 - val_loss: 1241.3455\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1379.2834 - val_loss: 1235.7297\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1348.6871 - val_loss: 1230.5543\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1318.7013 - val_loss: 1225.8109\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1289.3181 - val_loss: 1221.4919\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 1260.5303 - val_loss: 1217.5891\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1232.3296 - val_loss: 1214.0951\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1204.7084 - val_loss: 1211.0018\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1177.6594 - val_loss: 1208.3019\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1151.1747 - val_loss: 1205.9874\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1125.2471 - val_loss: 1204.0505\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1099.8689 - val_loss: 1202.4841\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1075.0333 - val_loss: 1201.2804\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 1050.7322 - val_loss: 1200.4318\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1026.9585 - val_loss: 1199.9307\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 1003.7050 - val_loss: 1199.7698\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 980.9644 - val_loss: 1199.9414\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 958.7297 - val_loss: 1200.4385\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 936.9935 - val_loss: 1201.2534\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 915.7487 - val_loss: 1202.3789\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 894.9880 - val_loss: 1203.8075\n",
      "Epoch 84/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 874.7051 - val_loss: 1205.5320\n",
      "Epoch 85/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 854.8921 - val_loss: 1207.5452\n",
      "Epoch 86/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 835.5427 - val_loss: 1209.8398\n",
      "Epoch 87/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 816.6498 - val_loss: 1212.4089\n",
      "Epoch 88/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 798.2062 - val_loss: 1215.2449\n",
      "Epoch 89/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 780.2057 - val_loss: 1218.3409\n",
      "Epoch 90/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 762.6407 - val_loss: 1221.6897\n",
      "Epoch 91/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 745.5052 - val_loss: 1225.2848\n",
      "Epoch 92/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 728.7920 - val_loss: 1229.1180\n",
      "Epoch 93/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 712.4943 - val_loss: 1233.1833\n",
      "Epoch 94/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 696.6057 - val_loss: 1237.4735\n",
      "Epoch 95/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 681.1197 - val_loss: 1241.9818\n",
      "Epoch 96/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 666.0293 - val_loss: 1246.7007\n",
      "Epoch 97/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 651.3283 - val_loss: 1251.6240\n",
      "Epoch 98/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 637.0099 - val_loss: 1256.7449\n",
      "Epoch 99/500\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 623.0678 - val_loss: 1262.0560\n",
      "Epoch 100/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 609.4956 - val_loss: 1267.5511\n",
      "Epoch 101/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 596.2869 - val_loss: 1273.2236\n",
      "Epoch 102/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 583.4351 - val_loss: 1279.0665\n",
      "Epoch 103/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 570.9341 - val_loss: 1285.0735\n",
      "Epoch 104/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 558.7775 - val_loss: 1291.2375\n",
      "Epoch 105/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 546.9592 - val_loss: 1297.5526\n",
      "Epoch 106/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 535.4728 - val_loss: 1304.0118\n",
      "Epoch 107/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 524.3122 - val_loss: 1310.6090\n",
      "Epoch 108/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 513.4713 - val_loss: 1317.3378\n",
      "Epoch 109/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 502.9440 - val_loss: 1324.1915\n",
      "Epoch 110/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 492.7245 - val_loss: 1331.1643\n",
      "Epoch 111/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 482.8062 - val_loss: 1338.2499\n",
      "Epoch 112/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 473.1837 - val_loss: 1345.4417\n",
      "Epoch 113/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 463.8509 - val_loss: 1352.7339\n",
      "Epoch 114/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 454.8017 - val_loss: 1360.1204\n",
      "Epoch 115/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 446.0305 - val_loss: 1367.5951\n",
      "Epoch 116/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 437.5315 - val_loss: 1375.1521\n",
      "Epoch 117/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 429.2990 - val_loss: 1382.7855\n",
      "Epoch 118/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 421.3271 - val_loss: 1390.4894\n",
      "Epoch 119/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 413.6103 - val_loss: 1398.2584\n",
      "Epoch 120/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 406.1428 - val_loss: 1406.0865\n",
      "Epoch 121/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 398.9193 - val_loss: 1413.9678\n",
      "Epoch 122/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 391.9342 - val_loss: 1421.8969\n",
      "Epoch 123/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 385.1821 - val_loss: 1429.8685\n",
      "Epoch 124/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 378.6575 - val_loss: 1437.8772\n",
      "Epoch 125/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 372.3551 - val_loss: 1445.9176\n",
      "Epoch 126/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 366.2696 - val_loss: 1453.9841\n",
      "Epoch 127/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 360.3958 - val_loss: 1462.0720\n",
      "Epoch 128/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 354.7284 - val_loss: 1470.1755\n",
      "Epoch 129/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 349.2623 - val_loss: 1478.2904\n",
      "Epoch 130/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 343.9926 - val_loss: 1486.4111\n",
      "Epoch 131/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 338.9140 - val_loss: 1494.5332\n",
      "Epoch 132/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 334.0216 - val_loss: 1502.6514\n",
      "Epoch 133/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 329.3107 - val_loss: 1510.7621\n",
      "Epoch 134/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 324.7762 - val_loss: 1518.8596\n",
      "Epoch 135/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 320.4137 - val_loss: 1526.9391\n",
      "Epoch 136/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 316.2181 - val_loss: 1534.9978\n",
      "Epoch 137/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 312.1849 - val_loss: 1543.0299\n",
      "Epoch 138/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 308.3094 - val_loss: 1551.0319\n",
      "Epoch 139/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 304.5872 - val_loss: 1558.9993\n",
      "Epoch 140/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 301.0139 - val_loss: 1566.9281\n",
      "Epoch 141/500\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 297.5851 - val_loss: 1574.8147\n",
      "Epoch 142/500\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 294.2963 - val_loss: 1582.6562\n",
      "Epoch 143/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 291.1432 - val_loss: 1590.4473\n",
      "Epoch 144/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 288.1218 - val_loss: 1598.1846\n",
      "Epoch 145/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 285.2279 - val_loss: 1605.8655\n",
      "Epoch 146/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 282.4576 - val_loss: 1613.4866\n",
      "Epoch 147/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 279.8068 - val_loss: 1621.0437\n",
      "Epoch 148/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 277.2715 - val_loss: 1628.5345\n",
      "Epoch 149/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 274.8480 - val_loss: 1635.9553\n",
      "Epoch 150/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 272.5326 - val_loss: 1643.3042\n",
      "Epoch 151/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 270.3215 - val_loss: 1650.5767\n",
      "Epoch 152/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 268.2112 - val_loss: 1657.7722\n",
      "Epoch 153/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 266.1980 - val_loss: 1664.8871\n",
      "Epoch 154/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 264.2784 - val_loss: 1671.9187\n",
      "Epoch 155/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 262.4492 - val_loss: 1678.8658\n",
      "Epoch 156/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 260.7072 - val_loss: 1685.7249\n",
      "Epoch 157/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 259.0488 - val_loss: 1692.4945\n",
      "Epoch 158/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 257.4710 - val_loss: 1699.1719\n",
      "Epoch 159/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 255.9709 - val_loss: 1705.7570\n",
      "Epoch 160/500\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 254.5452 - val_loss: 1712.2461\n",
      "Epoch 161/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 253.1911 - val_loss: 1718.6392\n",
      "Epoch 162/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 251.9058 - val_loss: 1724.9335\n",
      "Epoch 163/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 250.6864 - val_loss: 1731.1287\n",
      "Epoch 164/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 249.5301 - val_loss: 1737.2238\n",
      "Epoch 165/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 248.4345 - val_loss: 1743.2155\n",
      "Epoch 166/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 247.3970 - val_loss: 1749.1046\n",
      "Epoch 167/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 246.4150 - val_loss: 1754.8903\n",
      "Epoch 168/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 245.4861 - val_loss: 1760.5709\n",
      "Epoch 169/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 244.6081 - val_loss: 1766.1467\n",
      "Epoch 170/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 243.7785 - val_loss: 1771.6160\n",
      "Epoch 171/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 242.9953 - val_loss: 1776.9794\n",
      "Epoch 172/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 242.2563 - val_loss: 1782.2361\n",
      "Epoch 173/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 241.5594 - val_loss: 1787.3855\n",
      "Epoch 174/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 240.9027 - val_loss: 1792.4280\n",
      "Epoch 175/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 240.2842 - val_loss: 1797.3625\n",
      "Epoch 176/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 239.7022 - val_loss: 1802.1907\n",
      "Epoch 177/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 239.1547 - val_loss: 1806.9125\n",
      "Epoch 178/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 238.6400 - val_loss: 1811.5262\n",
      "Epoch 179/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 238.1567 - val_loss: 1816.0348\n",
      "Epoch 180/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 237.7029 - val_loss: 1820.4369\n",
      "Epoch 181/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 237.2772 - val_loss: 1824.7330\n",
      "Epoch 182/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 236.8781 - val_loss: 1828.9244\n",
      "Epoch 183/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 236.5043 - val_loss: 1833.0116\n",
      "Epoch 184/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 236.1543 - val_loss: 1836.9960\n",
      "Epoch 185/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 235.8268 - val_loss: 1840.8774\n",
      "Epoch 186/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 235.5208 - val_loss: 1844.6572\n",
      "Epoch 187/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 235.2347 - val_loss: 1848.3362\n",
      "Epoch 188/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 234.9677 - val_loss: 1851.9156\n",
      "Epoch 189/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 234.7186 - val_loss: 1855.3956\n",
      "Epoch 190/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 234.4864 - val_loss: 1858.7787\n",
      "Epoch 191/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 234.2699 - val_loss: 1862.0658\n",
      "Epoch 192/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 234.0684 - val_loss: 1865.2577\n",
      "Epoch 193/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 233.8810 - val_loss: 1868.3562\n",
      "Epoch 194/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 233.7068 - val_loss: 1871.3622\n",
      "Epoch 195/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 233.5448 - val_loss: 1874.2781\n",
      "Epoch 196/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 233.3945 - val_loss: 1877.1046\n",
      "Epoch 197/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 233.2550 - val_loss: 1879.8431\n",
      "Epoch 198/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 233.1258 - val_loss: 1882.4946\n",
      "Epoch 199/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 233.0060 - val_loss: 1885.0615\n",
      "Epoch 200/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 232.8952 - val_loss: 1887.5448\n",
      "Epoch 201/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 232.7928 - val_loss: 1889.9474\n",
      "Epoch 202/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 232.6979 - val_loss: 1892.2699\n",
      "Epoch 203/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 232.6105 - val_loss: 1894.5138\n",
      "Epoch 204/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 232.5297 - val_loss: 1896.6798\n",
      "Epoch 205/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 232.4552 - val_loss: 1898.7719\n",
      "Epoch 206/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 232.3866 - val_loss: 1900.7910\n",
      "Epoch 207/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 232.3234 - val_loss: 1902.7386\n",
      "Epoch 208/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 232.2653 - val_loss: 1904.6155\n",
      "Epoch 209/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 232.2118 - val_loss: 1906.4241\n",
      "Epoch 210/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 232.1628 - val_loss: 1908.1665\n",
      "Epoch 211/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 232.1177 - val_loss: 1909.8444\n",
      "Epoch 212/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 232.0764 - val_loss: 1911.4590\n",
      "Epoch 213/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 232.0386 - val_loss: 1913.0121\n",
      "Epoch 214/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 232.0039 - val_loss: 1914.5059\n",
      "Epoch 215/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.9722 - val_loss: 1915.9393\n",
      "Epoch 216/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 231.9432 - val_loss: 1917.3190\n",
      "Epoch 217/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 231.9168 - val_loss: 1918.6427\n",
      "Epoch 218/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 231.8926 - val_loss: 1919.9124\n",
      "Epoch 219/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.8706 - val_loss: 1921.1316\n",
      "Epoch 220/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 231.8506 - val_loss: 1922.3002\n",
      "Epoch 221/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.8324 - val_loss: 1923.4205\n",
      "Epoch 222/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.8159 - val_loss: 1924.4938\n",
      "Epoch 223/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.8008 - val_loss: 1925.5211\n",
      "Epoch 224/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7872 - val_loss: 1926.5057\n",
      "Epoch 225/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.7749 - val_loss: 1927.4470\n",
      "Epoch 226/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.7637 - val_loss: 1928.3474\n",
      "Epoch 227/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7537 - val_loss: 1929.2085\n",
      "Epoch 228/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7446 - val_loss: 1930.0312\n",
      "Epoch 229/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7365 - val_loss: 1930.8175\n",
      "Epoch 230/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7291 - val_loss: 1931.5682\n",
      "Epoch 231/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7225 - val_loss: 1932.2844\n",
      "Epoch 232/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7166 - val_loss: 1932.9678\n",
      "Epoch 233/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 231.7113 - val_loss: 1933.6193\n",
      "Epoch 234/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7067 - val_loss: 1934.2412\n",
      "Epoch 235/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7025 - val_loss: 1934.8328\n",
      "Epoch 236/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.6988 - val_loss: 1935.3969\n",
      "Epoch 237/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.6955 - val_loss: 1935.9340\n",
      "Epoch 238/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.6926 - val_loss: 1936.4448\n",
      "Epoch 239/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.6901 - val_loss: 1936.9305\n",
      "Epoch 240/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.6879 - val_loss: 1937.3934\n",
      "Epoch 241/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.6860 - val_loss: 1937.8318\n",
      "Epoch 242/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.6843 - val_loss: 1938.2489\n",
      "Epoch 243/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.6830 - val_loss: 1938.6456\n",
      "Epoch 244/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.6818 - val_loss: 1939.0226\n",
      "Epoch 245/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 231.6809 - val_loss: 1939.3798\n",
      "Epoch 246/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.6801 - val_loss: 1939.7190\n",
      "Epoch 247/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.6794 - val_loss: 1940.0400\n",
      "Epoch 248/500\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 231.6789 - val_loss: 1940.3451\n",
      "Epoch 249/500\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 231.6785 - val_loss: 1940.6329\n",
      "Epoch 250/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 231.6783 - val_loss: 1940.9062\n",
      "Epoch 251/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 231.6781 - val_loss: 1941.1660\n",
      "Epoch 252/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 231.6781 - val_loss: 1941.4105\n",
      "Epoch 253/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.6781 - val_loss: 1941.6427\n",
      "Epoch 254/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.6783 - val_loss: 1941.8619\n",
      "Epoch 255/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 231.6785 - val_loss: 1942.0693\n",
      "Epoch 256/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.6787 - val_loss: 1942.2643\n",
      "Epoch 257/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.6791 - val_loss: 1942.4500\n",
      "Epoch 258/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.6794 - val_loss: 1942.6246\n",
      "Epoch 259/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 231.6798 - val_loss: 1942.7897\n",
      "Epoch 260/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.6802 - val_loss: 1942.9448\n",
      "Epoch 261/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.6807 - val_loss: 1943.0913\n",
      "Epoch 262/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.6812 - val_loss: 1943.2301\n",
      "Epoch 263/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.6818 - val_loss: 1943.3604\n",
      "Epoch 264/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.6823 - val_loss: 1943.4838\n",
      "Epoch 265/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.6829 - val_loss: 1943.5991\n",
      "Epoch 266/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.6835 - val_loss: 1943.7078\n",
      "Epoch 267/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.6841 - val_loss: 1943.8101\n",
      "Epoch 268/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.6847 - val_loss: 1943.9069\n",
      "Epoch 269/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.6853 - val_loss: 1943.9961\n",
      "Epoch 270/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.6861 - val_loss: 1944.0825\n",
      "Epoch 271/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.6867 - val_loss: 1944.1624\n",
      "Epoch 272/500\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 231.6873 - val_loss: 1944.2379\n",
      "Epoch 273/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.6880 - val_loss: 1944.3081\n",
      "Epoch 274/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.6887 - val_loss: 1944.3740\n",
      "Epoch 275/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.6894 - val_loss: 1944.4365\n",
      "Epoch 276/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 231.6901 - val_loss: 1944.4950\n",
      "Epoch 277/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.6907 - val_loss: 1944.5497\n",
      "Epoch 278/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 231.6914 - val_loss: 1944.6013\n",
      "Epoch 279/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 231.6921 - val_loss: 1944.6500\n",
      "Epoch 280/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 231.6927 - val_loss: 1944.6934\n",
      "Epoch 281/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.6934 - val_loss: 1944.7362\n",
      "Epoch 282/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.6941 - val_loss: 1944.7758\n",
      "Epoch 283/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.6948 - val_loss: 1944.8123\n",
      "Epoch 284/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.6954 - val_loss: 1944.8467\n",
      "Epoch 285/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.6960 - val_loss: 1944.8787\n",
      "Epoch 286/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 231.6968 - val_loss: 1944.9088\n",
      "Epoch 287/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.6974 - val_loss: 1944.9375\n",
      "Epoch 288/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.6981 - val_loss: 1944.9644\n",
      "Epoch 289/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 231.6987 - val_loss: 1944.9883\n",
      "Epoch 290/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 231.6994 - val_loss: 1945.0114\n",
      "Epoch 291/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 231.7000 - val_loss: 1945.0331\n",
      "Epoch 292/500\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 231.7006 - val_loss: 1945.0522\n",
      "Epoch 293/500\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 231.7012 - val_loss: 1945.0712\n",
      "Epoch 294/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 231.7019 - val_loss: 1945.0887\n",
      "Epoch 295/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 231.7025 - val_loss: 1945.1050\n",
      "Epoch 296/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.7030 - val_loss: 1945.1195\n",
      "Epoch 297/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.7037 - val_loss: 1945.1329\n",
      "Epoch 298/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.7042 - val_loss: 1945.1456\n",
      "Epoch 299/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7048 - val_loss: 1945.1570\n",
      "Epoch 300/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 231.7054 - val_loss: 1945.1686\n",
      "Epoch 301/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 231.7060 - val_loss: 1945.1791\n",
      "Epoch 302/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.7066 - val_loss: 1945.1888\n",
      "Epoch 303/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7071 - val_loss: 1945.1981\n",
      "Epoch 304/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7076 - val_loss: 1945.2063\n",
      "Epoch 305/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7081 - val_loss: 1945.2136\n",
      "Epoch 306/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7086 - val_loss: 1945.2201\n",
      "Epoch 307/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7092 - val_loss: 1945.2261\n",
      "Epoch 308/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7097 - val_loss: 1945.2325\n",
      "Epoch 309/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7102 - val_loss: 1945.2380\n",
      "Epoch 310/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7107 - val_loss: 1945.2427\n",
      "Epoch 311/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7112 - val_loss: 1945.2478\n",
      "Epoch 312/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 231.7117 - val_loss: 1945.2513\n",
      "Epoch 313/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7122 - val_loss: 1945.2554\n",
      "Epoch 314/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7126 - val_loss: 1945.2588\n",
      "Epoch 315/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7131 - val_loss: 1945.2627\n",
      "Epoch 316/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7135 - val_loss: 1945.2646\n",
      "Epoch 317/500\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 231.7140 - val_loss: 1945.2677\n",
      "Epoch 318/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 231.7144 - val_loss: 1945.2706\n",
      "Epoch 319/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7149 - val_loss: 1945.2728\n",
      "Epoch 320/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7153 - val_loss: 1945.2743\n",
      "Epoch 321/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7157 - val_loss: 1945.2761\n",
      "Epoch 322/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 231.7162 - val_loss: 1945.2784\n",
      "Epoch 323/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7165 - val_loss: 1945.2798\n",
      "Epoch 324/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7170 - val_loss: 1945.2805\n",
      "Epoch 325/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7174 - val_loss: 1945.2823\n",
      "Epoch 326/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7177 - val_loss: 1945.2831\n",
      "Epoch 327/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7181 - val_loss: 1945.2838\n",
      "Epoch 328/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 231.7185 - val_loss: 1945.2853\n",
      "Epoch 329/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7188 - val_loss: 1945.2856\n",
      "Epoch 330/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7192 - val_loss: 1945.2866\n",
      "Epoch 331/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.7196 - val_loss: 1945.2874\n",
      "Epoch 332/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7199 - val_loss: 1945.2878\n",
      "Epoch 333/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7203 - val_loss: 1945.2885\n",
      "Epoch 334/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7206 - val_loss: 1945.2885\n",
      "Epoch 335/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7209 - val_loss: 1945.2888\n",
      "Epoch 336/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7212 - val_loss: 1945.2888\n",
      "Epoch 337/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7216 - val_loss: 1945.2888\n",
      "Epoch 338/500\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 231.7218 - val_loss: 1945.2889\n",
      "Epoch 339/500\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 231.7221 - val_loss: 1945.2893\n",
      "Epoch 340/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 231.7225 - val_loss: 1945.2896\n",
      "Epoch 341/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.7228 - val_loss: 1945.2896\n",
      "Epoch 342/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 231.7231 - val_loss: 1945.2897\n",
      "Epoch 343/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 231.7233 - val_loss: 1945.2897\n",
      "Epoch 344/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.7237 - val_loss: 1945.2903\n",
      "Epoch 345/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7239 - val_loss: 1945.2903\n",
      "Epoch 346/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.7242 - val_loss: 1945.2896\n",
      "Epoch 347/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 231.7244 - val_loss: 1945.2896\n",
      "Epoch 348/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7246 - val_loss: 1945.2889\n",
      "Epoch 349/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7249 - val_loss: 1945.2882\n",
      "Epoch 350/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7252 - val_loss: 1945.2882\n",
      "Epoch 351/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7254 - val_loss: 1945.2875\n",
      "Epoch 352/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7257 - val_loss: 1945.2885\n",
      "Epoch 353/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7260 - val_loss: 1945.2882\n",
      "Epoch 354/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7261 - val_loss: 1945.2882\n",
      "Epoch 355/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7263 - val_loss: 1945.2878\n",
      "Epoch 356/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7266 - val_loss: 1945.2877\n",
      "Epoch 357/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7268 - val_loss: 1945.2877\n",
      "Epoch 358/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7270 - val_loss: 1945.2874\n",
      "Epoch 359/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7272 - val_loss: 1945.2875\n",
      "Epoch 360/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7273 - val_loss: 1945.2870\n",
      "Epoch 361/500\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 231.7275 - val_loss: 1945.2856\n",
      "Epoch 362/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.7278 - val_loss: 1945.2848\n",
      "Epoch 363/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 231.7281 - val_loss: 1945.2849\n",
      "Epoch 364/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 231.7282 - val_loss: 1945.2853\n",
      "Epoch 365/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.7284 - val_loss: 1945.2856\n",
      "Epoch 366/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7286 - val_loss: 1945.2853\n",
      "Epoch 367/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7287 - val_loss: 1945.2852\n",
      "Epoch 368/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7289 - val_loss: 1945.2855\n",
      "Epoch 369/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7291 - val_loss: 1945.2855\n",
      "Epoch 370/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7292 - val_loss: 1945.2852\n",
      "Epoch 371/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7294 - val_loss: 1945.2856\n",
      "Epoch 372/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7295 - val_loss: 1945.2859\n",
      "Epoch 373/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7297 - val_loss: 1945.2859\n",
      "Epoch 374/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7299 - val_loss: 1945.2864\n",
      "Epoch 375/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7300 - val_loss: 1945.2864\n",
      "Epoch 376/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7302 - val_loss: 1945.2870\n",
      "Epoch 377/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7304 - val_loss: 1945.2874\n",
      "Epoch 378/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7305 - val_loss: 1945.2878\n",
      "Epoch 379/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7306 - val_loss: 1945.2886\n",
      "Epoch 380/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7308 - val_loss: 1945.2903\n",
      "Epoch 381/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7310 - val_loss: 1945.2921\n",
      "Epoch 382/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7310 - val_loss: 1945.2926\n",
      "Epoch 383/500\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 231.7312 - val_loss: 1945.2943\n",
      "Epoch 384/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7313 - val_loss: 1945.2947\n",
      "Epoch 385/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7314 - val_loss: 1945.2965\n",
      "Epoch 386/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7316 - val_loss: 1945.2986\n",
      "Epoch 387/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7316 - val_loss: 1945.3005\n",
      "Epoch 388/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7317 - val_loss: 1945.3030\n",
      "Epoch 389/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7318 - val_loss: 1945.3063\n",
      "Epoch 390/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7319 - val_loss: 1945.3104\n",
      "Epoch 391/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7321 - val_loss: 1945.3158\n",
      "Epoch 392/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7321 - val_loss: 1945.3235\n",
      "Epoch 393/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7323 - val_loss: 1945.3346\n",
      "Epoch 394/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7323 - val_loss: 1945.6191\n",
      "Epoch 395/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7327 - val_loss: 1945.2522\n",
      "Epoch 396/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7326 - val_loss: 1945.2522\n",
      "Epoch 397/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7327 - val_loss: 1945.2526\n",
      "Epoch 398/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7327 - val_loss: 1945.2521\n",
      "Epoch 399/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7328 - val_loss: 1945.2510\n",
      "Epoch 400/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 231.7330 - val_loss: 1945.2511\n",
      "Epoch 401/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7330 - val_loss: 1945.2506\n",
      "Epoch 402/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7332 - val_loss: 1945.2500\n",
      "Epoch 403/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7332 - val_loss: 1945.2504\n",
      "Epoch 404/500\n",
      "22/22 [==============================] - 0s 22ms/step - loss: 231.7334 - val_loss: 1945.2500\n",
      "Epoch 405/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 231.7334 - val_loss: 1945.2498\n",
      "Epoch 406/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7334 - val_loss: 1945.2493\n",
      "Epoch 407/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.7336 - val_loss: 1945.2490\n",
      "Epoch 408/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7336 - val_loss: 1945.2498\n",
      "Epoch 409/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7337 - val_loss: 1945.2500\n",
      "Epoch 410/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7338 - val_loss: 1945.2500\n",
      "Epoch 411/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7338 - val_loss: 1945.2500\n",
      "Epoch 412/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7339 - val_loss: 1945.2490\n",
      "Epoch 413/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7340 - val_loss: 1945.2487\n",
      "Epoch 414/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7341 - val_loss: 1945.2483\n",
      "Epoch 415/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7342 - val_loss: 1945.2483\n",
      "Epoch 416/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7342 - val_loss: 1945.2482\n",
      "Epoch 417/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.7342 - val_loss: 1945.2471\n",
      "Epoch 418/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.7342 - val_loss: 1945.2463\n",
      "Epoch 419/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7343 - val_loss: 1945.2463\n",
      "Epoch 420/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7344 - val_loss: 1945.2467\n",
      "Epoch 421/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.7345 - val_loss: 1945.2467\n",
      "Epoch 422/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7346 - val_loss: 1945.2468\n",
      "Epoch 423/500\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 231.7346 - val_loss: 1945.2471\n",
      "Epoch 424/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.7346 - val_loss: 1945.2474\n",
      "Epoch 425/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7347 - val_loss: 1945.2474\n",
      "Epoch 426/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7347 - val_loss: 1945.2471\n",
      "Epoch 427/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7348 - val_loss: 1945.2471\n",
      "Epoch 428/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7349 - val_loss: 1945.2474\n",
      "Epoch 429/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7349 - val_loss: 1945.2474\n",
      "Epoch 430/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.7349 - val_loss: 1945.2471\n",
      "Epoch 431/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7350 - val_loss: 1945.2465\n",
      "Epoch 432/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.7350 - val_loss: 1945.2456\n",
      "Epoch 433/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7350 - val_loss: 1945.2452\n",
      "Epoch 434/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7351 - val_loss: 1945.2460\n",
      "Epoch 435/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.7352 - val_loss: 1945.2463\n",
      "Epoch 436/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7353 - val_loss: 1945.2467\n",
      "Epoch 437/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7353 - val_loss: 1945.2472\n",
      "Epoch 438/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 231.7353 - val_loss: 1945.2474\n",
      "Epoch 439/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 231.7352 - val_loss: 1945.2468\n",
      "Epoch 440/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 231.7353 - val_loss: 1945.2460\n",
      "Epoch 441/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7353 - val_loss: 1945.2445\n",
      "Epoch 442/500\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 231.7354 - val_loss: 1945.2434\n",
      "Epoch 443/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.7354 - val_loss: 1945.2430\n",
      "Epoch 444/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 231.7355 - val_loss: 1945.2430\n",
      "Epoch 445/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.7355 - val_loss: 1945.2427\n",
      "Epoch 446/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 231.7356 - val_loss: 1945.2427\n",
      "Epoch 447/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7356 - val_loss: 1945.2427\n",
      "Epoch 448/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7356 - val_loss: 1945.2423\n",
      "Epoch 449/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7356 - val_loss: 1945.2419\n",
      "Epoch 450/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7356 - val_loss: 1945.2410\n",
      "Epoch 451/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7357 - val_loss: 1945.2408\n",
      "Epoch 452/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7357 - val_loss: 1945.2408\n",
      "Epoch 453/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7357 - val_loss: 1945.2416\n",
      "Epoch 454/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7358 - val_loss: 1945.2416\n",
      "Epoch 455/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7358 - val_loss: 1945.2417\n",
      "Epoch 456/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7359 - val_loss: 1945.2417\n",
      "Epoch 457/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7359 - val_loss: 1945.2417\n",
      "Epoch 458/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7360 - val_loss: 1945.2417\n",
      "Epoch 459/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7359 - val_loss: 1945.2417\n",
      "Epoch 460/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7359 - val_loss: 1945.2412\n",
      "Epoch 461/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7359 - val_loss: 1945.2406\n",
      "Epoch 462/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 231.7360 - val_loss: 1945.2406\n",
      "Epoch 463/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7360 - val_loss: 1945.2406\n",
      "Epoch 464/500\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 231.7361 - val_loss: 1945.2408\n",
      "Epoch 465/500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 231.7361 - val_loss: 1945.2408\n",
      "Epoch 466/500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 231.7362 - val_loss: 1945.2413\n",
      "Epoch 467/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7362 - val_loss: 1945.2421\n",
      "Epoch 468/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7363 - val_loss: 1945.2434\n",
      "Epoch 469/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7363 - val_loss: 1945.2446\n",
      "Epoch 470/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7363 - val_loss: 1945.2446\n",
      "Epoch 471/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7362 - val_loss: 1945.2441\n",
      "Epoch 472/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7363 - val_loss: 1945.2438\n",
      "Epoch 473/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7363 - val_loss: 1945.2438\n",
      "Epoch 474/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7363 - val_loss: 1945.2438\n",
      "Epoch 475/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7363 - val_loss: 1945.2428\n",
      "Epoch 476/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7363 - val_loss: 1945.2423\n",
      "Epoch 477/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7362 - val_loss: 1945.2417\n",
      "Epoch 478/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7363 - val_loss: 1945.2412\n",
      "Epoch 479/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7363 - val_loss: 1945.2406\n",
      "Epoch 480/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7364 - val_loss: 1945.2405\n",
      "Epoch 481/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7364 - val_loss: 1945.2405\n",
      "Epoch 482/500\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 231.7364 - val_loss: 1945.2410\n",
      "Epoch 483/500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 231.7364 - val_loss: 1945.2413\n",
      "Epoch 484/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7365 - val_loss: 1945.2416\n",
      "Epoch 485/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7365 - val_loss: 1945.2424\n",
      "Epoch 486/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7365 - val_loss: 1945.2424\n",
      "Epoch 487/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7366 - val_loss: 1945.2427\n",
      "Epoch 488/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7366 - val_loss: 1945.2424\n",
      "Epoch 489/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 231.7366 - val_loss: 1945.2427\n",
      "Epoch 490/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7366 - val_loss: 1945.2421\n",
      "Epoch 491/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7365 - val_loss: 1945.2416\n",
      "Epoch 492/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7365 - val_loss: 1945.2410\n",
      "Epoch 493/500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 231.7365 - val_loss: 1945.2408\n",
      "Epoch 494/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7365 - val_loss: 1945.2412\n",
      "Epoch 495/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7365 - val_loss: 1945.2412\n",
      "Epoch 496/500\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 231.7365 - val_loss: 1945.2408\n",
      "Epoch 497/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7366 - val_loss: 1945.2405\n",
      "Epoch 498/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7366 - val_loss: 1945.2401\n",
      "Epoch 499/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7366 - val_loss: 1945.2399\n",
      "Epoch 500/500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 231.7366 - val_loss: 1945.2397\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6856, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0008, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0128, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(-2.3003, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 486ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.10205882e+01, 7.06778011e+01, 7.06021709e+01, 7.05265406e+01,\n",
       "        7.62993622e+01, 3.12900250e-01, 2.45064540e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.43458825e-01, 1.37494779e+00, 2.42258270e-02,\n",
       "        4.99962062e-01, 7.18441177e+01, 7.13903361e+01, 7.09365546e+01,\n",
       "        8.59306460e-02, 8.07693124e-01, 7.28637955e+01, 7.24827731e+01,\n",
       "        7.20289916e+01, 7.15752101e+01, 7.11214286e+01, 7.06946078e+01,\n",
       "        7.06189776e+01, 7.05433473e+01, 7.04677171e+01, 5.48075680e-01,\n",
       "        4.64026210e-01, 7.13063025e+01, 7.08525210e+01, 7.06497899e+01,\n",
       "        7.05741597e+01, 7.04985294e+01, 7.04228992e+01, 7.03472689e+01,\n",
       "        7.02716387e+01, 7.01960084e+01, 3.44524920e-01, 0.00000000e+00,\n",
       "        3.46844375e-01, 6.16381407e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.07402796e-01, 7.05601541e+01,\n",
       "        7.04845238e+01, 0.00000000e+00, 0.00000000e+00, 7.14071429e+01,\n",
       "        7.09533613e+01, 7.06665966e+01, 7.05909664e+01, 7.05153361e+01,\n",
       "        7.04397059e+01, 7.03640756e+01, 7.02884454e+01, 7.02128151e+01,\n",
       "        8.42299700e-01, 2.30693817e-01, 7.05461485e+01, 7.04705182e+01,\n",
       "        7.03948880e+01, 7.03192577e+01, 7.02436275e+01, 7.42882820e+01,\n",
       "        7.32979692e+01, 7.20962185e+01, 0.00000000e+00, 4.29049370e-01,\n",
       "        0.00000000e+00, 6.88514404e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.57392964e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        6.76550064e+01, 1.17278683e+00, 4.43511516e-01, 1.19287379e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.12735215e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.99144256e-01, 0.00000000e+00,\n",
       "        1.80860594e-01, 2.04088852e-01, 0.00000000e+00, 4.04535353e-01,\n",
       "        5.88283837e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([59.29186309, 59.27956548, 59.26726788, 59.25497028, 59.24267268,\n",
       "       59.23037508, 59.21807747, 59.20577987, 59.19348227, 59.18118467,\n",
       "       59.16888707, 59.15658947, 59.14429186, 59.13199426, 59.11969666,\n",
       "       59.10739906, 59.09510146, 59.08280385, 59.07050625, 59.05820865,\n",
       "       59.04591105, 59.03361345, 59.02131584, 59.00901824, 58.99672064,\n",
       "       58.98442304, 58.97212544, 58.95982783, 58.94753023, 58.93523263,\n",
       "       58.92293503, 58.91063743, 58.89833982, 58.88604222, 58.87374462,\n",
       "       58.86144702, 58.84914942, 58.83685181, 58.82455421, 58.81225661,\n",
       "       58.79995901, 58.78766141, 58.7753638 , 58.7630662 , 58.7507686 ,\n",
       "       58.738471  , 58.7261734 , 58.71387579, 58.70157819, 58.68928059,\n",
       "       58.67698299, 58.66468539, 58.65238778, 58.64009018, 58.62779258,\n",
       "       58.61549498, 58.60319738, 58.59089977, 58.57860217, 58.56630457,\n",
       "       58.55400697, 58.54170937, 58.52941176, 58.51711416, 58.50481656,\n",
       "       58.49251896, 58.48022136, 58.46792375, 58.45562615, 58.44332855,\n",
       "       58.43103095, 58.41873335, 58.40643575, 58.39413814, 58.38184054,\n",
       "       58.36954294, 58.35724534, 58.34494774, 58.33265013, 58.32035253,\n",
       "       58.30805493, 58.29575733, 58.28345973, 58.27116212, 58.25886452,\n",
       "       58.24656692, 58.23426932, 58.22197172, 58.20967411, 58.19737651,\n",
       "       58.18507891, 58.17278131, 58.16048371, 58.1481861 , 58.1358885 ,\n",
       "       58.1235909 , 58.1112933 , 58.0989957 , 58.08669809, 58.07440049])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.29408894749933\n",
      "36.63329019725546\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
