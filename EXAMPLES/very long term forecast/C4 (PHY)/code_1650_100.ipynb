{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "1745    65.261134\n",
       "1746    65.252731\n",
       "1747    65.244328\n",
       "1748    65.235924\n",
       "1749    65.227521\n",
       "Name: C4, Length: 1750, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c4_interpolated_1650_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "1645     0.000000\n",
       "1646     0.096587\n",
       "1647     0.157036\n",
       "1648     0.317975\n",
       "1649     0.000000\n",
       "Name: C4, Length: 1650, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1650)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.409524</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.219048</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.028571</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.838095</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.600000  0.000298\n",
       "1     88.409524  0.000298\n",
       "2     88.219048  0.000297\n",
       "3     88.028571  0.000297\n",
       "4     87.838095  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6856  # Value for C0\n",
    "K0 = -0.0008  # Value for K0\n",
    "K1 = -0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0128    # Value for b\n",
    "c = -2.3003    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.600000    88.409524    88.219048    88.028571    87.838095   \n",
      "351    88.409524    88.219048    88.028571    87.838095    87.647619   \n",
      "352    88.219048    88.028571    87.838095    87.647619    87.457143   \n",
      "353    88.028571    87.838095    87.647619    87.457143    87.266667   \n",
      "354    87.838095    87.647619    87.457143    87.266667    87.076190   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.647619    87.457143    87.266667    87.076190    86.896218  ...   \n",
      "351    87.457143    87.266667    87.076190    86.896218    86.845798  ...   \n",
      "352    87.266667    87.076190    86.896218    86.845798    86.795378  ...   \n",
      "353    87.076190    86.896218    86.845798    86.795378    86.744958  ...   \n",
      "354    86.896218    86.845798    86.795378    86.744958    86.694538  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   78.880345    0.000263   78.856069    0.000263   78.831793    0.000263   \n",
      "351   78.856069    0.000263   78.831793    0.000263   78.807516    0.000262   \n",
      "352   78.831793    0.000263   78.807516    0.000262   78.783240    0.000262   \n",
      "353   78.807516    0.000262   78.783240    0.000262   78.758964    0.000262   \n",
      "354   78.783240    0.000262   78.758964    0.000262   78.734687    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   78.807516    0.000262   78.783240    0.000262  \n",
      "351   78.783240    0.000262   78.758964    0.000262  \n",
      "352   78.758964    0.000262   78.734687    0.000262  \n",
      "353   78.734687    0.000262   78.710411    0.000262  \n",
      "354   78.710411    0.000262   78.686134    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 1, 251) (1200, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "15/15 [==============================] - 2s 37ms/step - loss: 5450.1719 - val_loss: 4107.8530\n",
      "Epoch 2/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5350.4263 - val_loss: 4054.4163\n",
      "Epoch 3/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5291.9751 - val_loss: 4010.1184\n",
      "Epoch 4/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5234.6255 - val_loss: 3966.3218\n",
      "Epoch 5/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5177.8916 - val_loss: 3923.0525\n",
      "Epoch 6/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5121.7593 - val_loss: 3880.2725\n",
      "Epoch 7/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5066.1855 - val_loss: 3837.9492\n",
      "Epoch 8/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5011.1343 - val_loss: 3796.0608\n",
      "Epoch 9/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4956.5845 - val_loss: 3754.5911\n",
      "Epoch 10/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4902.5176 - val_loss: 3713.5295\n",
      "Epoch 11/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4848.9219 - val_loss: 3672.8679\n",
      "Epoch 12/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4795.7891 - val_loss: 3632.5979\n",
      "Epoch 13/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4743.1104 - val_loss: 3592.7156\n",
      "Epoch 14/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4690.8804 - val_loss: 3553.2141\n",
      "Epoch 15/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4639.0928 - val_loss: 3514.0908\n",
      "Epoch 16/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4587.7427 - val_loss: 3475.3401\n",
      "Epoch 17/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4536.8271 - val_loss: 3436.9568\n",
      "Epoch 18/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 4486.2588 - val_loss: 3398.2148\n",
      "Epoch 19/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4433.3057 - val_loss: 3356.9375\n",
      "Epoch 20/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4379.0068 - val_loss: 3316.2134\n",
      "Epoch 21/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4325.7148 - val_loss: 3276.3955\n",
      "Epoch 22/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4273.4507 - val_loss: 3237.3374\n",
      "Epoch 23/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 4222.0386 - val_loss: 3198.9224\n",
      "Epoch 24/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 4171.3545 - val_loss: 3161.0730\n",
      "Epoch 25/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 4121.3188 - val_loss: 3123.7397\n",
      "Epoch 26/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4071.8762 - val_loss: 3086.8879\n",
      "Epoch 27/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4022.9932 - val_loss: 3050.4932\n",
      "Epoch 28/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3974.6396 - val_loss: 3014.5364\n",
      "Epoch 29/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3926.7952 - val_loss: 2979.0015\n",
      "Epoch 30/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3879.4429 - val_loss: 2943.8762\n",
      "Epoch 31/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3831.7976 - val_loss: 2905.5593\n",
      "Epoch 32/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3779.1023 - val_loss: 2867.1050\n",
      "Epoch 33/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3727.8716 - val_loss: 2829.4111\n",
      "Epoch 34/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3677.8616 - val_loss: 2792.7415\n",
      "Epoch 35/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3629.0200 - val_loss: 2756.9163\n",
      "Epoch 36/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3581.1335 - val_loss: 2721.7957\n",
      "Epoch 37/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3534.0540 - val_loss: 2687.2913\n",
      "Epoch 38/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3487.6863 - val_loss: 2653.3425\n",
      "Epoch 39/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3441.9656 - val_loss: 2619.9089\n",
      "Epoch 40/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3396.8455 - val_loss: 2586.9602\n",
      "Epoch 41/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3352.2935 - val_loss: 2554.4739\n",
      "Epoch 42/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3308.2825 - val_loss: 2522.4304\n",
      "Epoch 43/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3264.7915 - val_loss: 2490.8154\n",
      "Epoch 44/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3221.8030 - val_loss: 2459.6162\n",
      "Epoch 45/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3179.3018 - val_loss: 2428.8213\n",
      "Epoch 46/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3137.2759 - val_loss: 2398.4219\n",
      "Epoch 47/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3095.7141 - val_loss: 2368.4092\n",
      "Epoch 48/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 3054.6069 - val_loss: 2338.7766\n",
      "Epoch 49/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3013.9463 - val_loss: 2309.5164\n",
      "Epoch 50/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2973.7231 - val_loss: 2280.6228\n",
      "Epoch 51/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2933.9307 - val_loss: 2252.0901\n",
      "Epoch 52/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2894.5632 - val_loss: 2223.9133\n",
      "Epoch 53/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2855.6138 - val_loss: 2196.0864\n",
      "Epoch 54/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2817.0764 - val_loss: 2168.6057\n",
      "Epoch 55/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2778.9468 - val_loss: 2141.4661\n",
      "Epoch 56/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2741.2188 - val_loss: 2114.6636\n",
      "Epoch 57/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2703.8894 - val_loss: 2088.1921\n",
      "Epoch 58/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2666.9524 - val_loss: 2062.0476\n",
      "Epoch 59/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2630.4036 - val_loss: 2036.2207\n",
      "Epoch 60/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2594.2397 - val_loss: 2010.6779\n",
      "Epoch 61/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2555.1194 - val_loss: 1976.3146\n",
      "Epoch 62/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2506.3213 - val_loss: 1947.5131\n",
      "Epoch 63/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2466.0239 - val_loss: 1919.5339\n",
      "Epoch 64/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2426.9773 - val_loss: 1892.5192\n",
      "Epoch 65/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2389.0671 - val_loss: 1866.2838\n",
      "Epoch 66/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2352.0686 - val_loss: 1840.6925\n",
      "Epoch 67/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2315.8313 - val_loss: 1815.6591\n",
      "Epoch 68/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2280.2573 - val_loss: 1791.1270\n",
      "Epoch 69/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2245.2827 - val_loss: 1767.0564\n",
      "Epoch 70/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2210.8616 - val_loss: 1743.4180\n",
      "Epoch 71/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2176.9580 - val_loss: 1720.1899\n",
      "Epoch 72/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2143.5471 - val_loss: 1697.3544\n",
      "Epoch 73/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 2110.6072 - val_loss: 1674.8967\n",
      "Epoch 74/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2078.1201 - val_loss: 1652.8046\n",
      "Epoch 75/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 2046.0721 - val_loss: 1631.0676\n",
      "Epoch 76/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 2014.4497 - val_loss: 1609.6763\n",
      "Epoch 77/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1983.2423 - val_loss: 1588.6235\n",
      "Epoch 78/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1952.4398 - val_loss: 1567.9004\n",
      "Epoch 79/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 1922.0338 - val_loss: 1547.5021\n",
      "Epoch 80/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 1892.0168 - val_loss: 1527.4221\n",
      "Epoch 81/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1862.3809 - val_loss: 1507.6542\n",
      "Epoch 82/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1833.1202 - val_loss: 1488.1941\n",
      "Epoch 83/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1804.2280 - val_loss: 1469.0365\n",
      "Epoch 84/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1775.6991 - val_loss: 1450.1769\n",
      "Epoch 85/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1747.5273 - val_loss: 1431.6110\n",
      "Epoch 86/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1719.7084 - val_loss: 1413.3346\n",
      "Epoch 87/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1692.2372 - val_loss: 1395.3444\n",
      "Epoch 88/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1665.1099 - val_loss: 1377.6360\n",
      "Epoch 89/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1638.3215 - val_loss: 1360.2067\n",
      "Epoch 90/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1611.8679 - val_loss: 1343.0526\n",
      "Epoch 91/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 1585.7452 - val_loss: 1326.1697\n",
      "Epoch 92/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1559.9500 - val_loss: 1309.5560\n",
      "Epoch 93/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1534.4783 - val_loss: 1293.2075\n",
      "Epoch 94/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1509.3265 - val_loss: 1277.1216\n",
      "Epoch 95/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 1484.4910 - val_loss: 1261.2949\n",
      "Epoch 96/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1459.9684 - val_loss: 1245.7252\n",
      "Epoch 97/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1435.7556 - val_loss: 1230.4088\n",
      "Epoch 98/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 1411.8492 - val_loss: 1215.3438\n",
      "Epoch 99/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1388.2466 - val_loss: 1200.5270\n",
      "Epoch 100/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 1364.9442 - val_loss: 1185.9562\n",
      "Epoch 101/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 1341.9398 - val_loss: 1171.6281\n",
      "Epoch 102/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1319.2294 - val_loss: 1157.5408\n",
      "Epoch 103/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1296.8113 - val_loss: 1143.6909\n",
      "Epoch 104/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1274.6814 - val_loss: 1130.0769\n",
      "Epoch 105/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1252.8379 - val_loss: 1116.6960\n",
      "Epoch 106/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1231.2777 - val_loss: 1103.5463\n",
      "Epoch 107/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1209.9985 - val_loss: 1090.6239\n",
      "Epoch 108/500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 1188.9973 - val_loss: 1077.9282\n",
      "Epoch 109/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1168.2719 - val_loss: 1065.4558\n",
      "Epoch 110/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1147.8192 - val_loss: 1053.2051\n",
      "Epoch 111/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1127.6373 - val_loss: 1041.1737\n",
      "Epoch 112/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1107.7233 - val_loss: 1029.3585\n",
      "Epoch 113/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1088.0743 - val_loss: 1017.7580\n",
      "Epoch 114/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1068.6887 - val_loss: 1006.3699\n",
      "Epoch 115/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1049.5638 - val_loss: 995.1922\n",
      "Epoch 116/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1030.6968 - val_loss: 984.2224\n",
      "Epoch 117/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1012.0858 - val_loss: 973.4583\n",
      "Epoch 118/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 993.7283 - val_loss: 962.8983\n",
      "Epoch 119/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 975.6223 - val_loss: 952.5399\n",
      "Epoch 120/500\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 957.7651 - val_loss: 942.3810\n",
      "Epoch 121/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 940.1545 - val_loss: 932.4196\n",
      "Epoch 122/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 922.7883 - val_loss: 922.6536\n",
      "Epoch 123/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 905.6643 - val_loss: 913.0810\n",
      "Epoch 124/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 888.7800 - val_loss: 903.6993\n",
      "Epoch 125/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 872.1334 - val_loss: 894.5074\n",
      "Epoch 126/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 855.7227 - val_loss: 885.5027\n",
      "Epoch 127/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 839.5450 - val_loss: 876.6833\n",
      "Epoch 128/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 823.5991 - val_loss: 868.0468\n",
      "Epoch 129/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 807.8818 - val_loss: 859.5922\n",
      "Epoch 130/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 792.3917 - val_loss: 851.3167\n",
      "Epoch 131/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 777.1265 - val_loss: 843.2187\n",
      "Epoch 132/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 762.0838 - val_loss: 835.2961\n",
      "Epoch 133/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 747.2621 - val_loss: 827.5468\n",
      "Epoch 134/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 732.6586 - val_loss: 819.9692\n",
      "Epoch 135/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 718.2717 - val_loss: 812.5613\n",
      "Epoch 136/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 704.0996 - val_loss: 805.3214\n",
      "Epoch 137/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 690.1400 - val_loss: 798.2469\n",
      "Epoch 138/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 676.3906 - val_loss: 791.3366\n",
      "Epoch 139/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 662.8492 - val_loss: 784.5881\n",
      "Epoch 140/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 649.5147 - val_loss: 778.0004\n",
      "Epoch 141/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 636.3846 - val_loss: 771.5705\n",
      "Epoch 142/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 623.4567 - val_loss: 765.2972\n",
      "Epoch 143/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 610.7294 - val_loss: 759.1784\n",
      "Epoch 144/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 598.2003 - val_loss: 753.2126\n",
      "Epoch 145/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 585.8678 - val_loss: 747.3976\n",
      "Epoch 146/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 573.7297 - val_loss: 741.7315\n",
      "Epoch 147/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 561.7842 - val_loss: 736.2129\n",
      "Epoch 148/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 550.0296 - val_loss: 730.8398\n",
      "Epoch 149/500\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 538.4638 - val_loss: 725.6102\n",
      "Epoch 150/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 527.0845 - val_loss: 720.5225\n",
      "Epoch 151/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 515.8903 - val_loss: 715.5748\n",
      "Epoch 152/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 504.8792 - val_loss: 710.7655\n",
      "Epoch 153/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 494.0491 - val_loss: 706.0923\n",
      "Epoch 154/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 483.3980 - val_loss: 701.5541\n",
      "Epoch 155/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 472.9245 - val_loss: 697.1486\n",
      "Epoch 156/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 462.6264 - val_loss: 692.8741\n",
      "Epoch 157/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 452.5018 - val_loss: 688.7294\n",
      "Epoch 158/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 442.5491 - val_loss: 684.7120\n",
      "Epoch 159/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 432.7665 - val_loss: 680.8207\n",
      "Epoch 160/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 423.1517 - val_loss: 677.0531\n",
      "Epoch 161/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 413.7033 - val_loss: 673.4083\n",
      "Epoch 162/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 404.4193 - val_loss: 669.8842\n",
      "Epoch 163/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 395.2977 - val_loss: 666.4789\n",
      "Epoch 164/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 386.3372 - val_loss: 663.1908\n",
      "Epoch 165/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 377.5356 - val_loss: 660.0184\n",
      "Epoch 166/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 368.8910 - val_loss: 656.9597\n",
      "Epoch 167/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 360.4019 - val_loss: 654.0129\n",
      "Epoch 168/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 352.0663 - val_loss: 651.1768\n",
      "Epoch 169/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 343.8827 - val_loss: 648.4493\n",
      "Epoch 170/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 335.8490 - val_loss: 645.8290\n",
      "Epoch 171/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 327.9637 - val_loss: 643.3138\n",
      "Epoch 172/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 320.2248 - val_loss: 640.9026\n",
      "Epoch 173/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 312.6308 - val_loss: 638.5934\n",
      "Epoch 174/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 305.1798 - val_loss: 636.3846\n",
      "Epoch 175/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 297.8700 - val_loss: 634.2743\n",
      "Epoch 176/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 290.6996 - val_loss: 632.2614\n",
      "Epoch 177/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 283.6674 - val_loss: 630.3438\n",
      "Epoch 178/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 276.7714 - val_loss: 628.5203\n",
      "Epoch 179/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 270.0097 - val_loss: 626.7887\n",
      "Epoch 180/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 263.3807 - val_loss: 625.1478\n",
      "Epoch 181/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 256.8827 - val_loss: 623.5959\n",
      "Epoch 182/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 250.5140 - val_loss: 622.1315\n",
      "Epoch 183/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 244.2731 - val_loss: 620.7529\n",
      "Epoch 184/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 238.1580 - val_loss: 619.4583\n",
      "Epoch 185/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 232.1673 - val_loss: 618.2466\n",
      "Epoch 186/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 226.2993 - val_loss: 617.1160\n",
      "Epoch 187/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 220.5523 - val_loss: 616.0646\n",
      "Epoch 188/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 214.9245 - val_loss: 615.0914\n",
      "Epoch 189/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 209.4145 - val_loss: 614.1944\n",
      "Epoch 190/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 204.0206 - val_loss: 613.3724\n",
      "Epoch 191/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 198.7412 - val_loss: 612.6237\n",
      "Epoch 192/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 193.5746 - val_loss: 611.9468\n",
      "Epoch 193/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 188.5194 - val_loss: 611.3402\n",
      "Epoch 194/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 183.5739 - val_loss: 610.8025\n",
      "Epoch 195/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 178.7365 - val_loss: 610.3319\n",
      "Epoch 196/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 174.0057 - val_loss: 609.9271\n",
      "Epoch 197/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 169.3795 - val_loss: 609.5867\n",
      "Epoch 198/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 164.8567 - val_loss: 609.3091\n",
      "Epoch 199/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 160.4360 - val_loss: 609.0929\n",
      "Epoch 200/500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 156.1154 - val_loss: 608.9366\n",
      "Epoch 201/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 151.8935 - val_loss: 608.8387\n",
      "Epoch 202/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 147.7688 - val_loss: 608.7980\n",
      "Epoch 203/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 143.7398 - val_loss: 608.8128\n",
      "Epoch 204/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 139.8049 - val_loss: 608.8819\n",
      "Epoch 205/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 135.9625 - val_loss: 609.0038\n",
      "Epoch 206/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 132.2115 - val_loss: 609.1770\n",
      "Epoch 207/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 128.5501 - val_loss: 609.4004\n",
      "Epoch 208/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 124.9771 - val_loss: 609.6722\n",
      "Epoch 209/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 121.4906 - val_loss: 609.9915\n",
      "Epoch 210/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 118.0894 - val_loss: 610.3567\n",
      "Epoch 211/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 114.7722 - val_loss: 610.7665\n",
      "Epoch 212/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 111.5373 - val_loss: 611.2195\n",
      "Epoch 213/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 108.3835 - val_loss: 611.7146\n",
      "Epoch 214/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 105.3091 - val_loss: 612.2502\n",
      "Epoch 215/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 102.3131 - val_loss: 612.8251\n",
      "Epoch 216/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 99.3936 - val_loss: 613.4383\n",
      "Epoch 217/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 96.5495 - val_loss: 614.0881\n",
      "Epoch 218/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 93.7795 - val_loss: 614.7736\n",
      "Epoch 219/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 91.0823 - val_loss: 615.4934\n",
      "Epoch 220/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 88.4563 - val_loss: 616.2463\n",
      "Epoch 221/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 85.9003 - val_loss: 617.0310\n",
      "Epoch 222/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 83.4130 - val_loss: 617.8464\n",
      "Epoch 223/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 80.9929 - val_loss: 618.6912\n",
      "Epoch 224/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 78.6390 - val_loss: 619.5643\n",
      "Epoch 225/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 76.3498 - val_loss: 620.4646\n",
      "Epoch 226/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 74.1241 - val_loss: 621.3909\n",
      "Epoch 227/500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 71.9606 - val_loss: 622.3419\n",
      "Epoch 228/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 69.8579 - val_loss: 623.3168\n",
      "Epoch 229/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 67.8150 - val_loss: 624.3142\n",
      "Epoch 230/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 65.8307 - val_loss: 625.3332\n",
      "Epoch 231/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 63.9033 - val_loss: 626.3727\n",
      "Epoch 232/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 62.0321 - val_loss: 627.4315\n",
      "Epoch 233/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 60.2158 - val_loss: 628.5086\n",
      "Epoch 234/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 58.4534 - val_loss: 629.6032\n",
      "Epoch 235/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 56.7432 - val_loss: 630.7139\n",
      "Epoch 236/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 55.0844 - val_loss: 631.8400\n",
      "Epoch 237/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 53.4760 - val_loss: 632.9803\n",
      "Epoch 238/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 51.9166 - val_loss: 634.1341\n",
      "Epoch 239/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 50.4052 - val_loss: 635.3002\n",
      "Epoch 240/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 48.9407 - val_loss: 636.4778\n",
      "Epoch 241/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 47.5219 - val_loss: 637.6658\n",
      "Epoch 242/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 46.1479 - val_loss: 638.8636\n",
      "Epoch 243/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 44.8176 - val_loss: 640.0701\n",
      "Epoch 244/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 43.5299 - val_loss: 641.2844\n",
      "Epoch 245/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 42.2838 - val_loss: 642.5057\n",
      "Epoch 246/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 41.0783 - val_loss: 643.7333\n",
      "Epoch 247/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 39.9125 - val_loss: 644.9661\n",
      "Epoch 248/500\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 38.7851 - val_loss: 646.2037\n",
      "Epoch 249/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 37.6954 - val_loss: 647.4449\n",
      "Epoch 250/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 36.6423 - val_loss: 648.6891\n",
      "Epoch 251/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 35.6251 - val_loss: 649.9355\n",
      "Epoch 252/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 34.6426 - val_loss: 651.1835\n",
      "Epoch 253/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 33.6939 - val_loss: 652.4320\n",
      "Epoch 254/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 32.7784 - val_loss: 653.6807\n",
      "Epoch 255/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 31.8949 - val_loss: 654.9289\n",
      "Epoch 256/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 31.0425 - val_loss: 656.1759\n",
      "Epoch 257/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 30.2206 - val_loss: 657.4208\n",
      "Epoch 258/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 29.4283 - val_loss: 658.6631\n",
      "Epoch 259/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 28.6647 - val_loss: 659.9022\n",
      "Epoch 260/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 27.9290 - val_loss: 661.1376\n",
      "Epoch 261/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 27.2203 - val_loss: 662.3687\n",
      "Epoch 262/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 26.5379 - val_loss: 663.5950\n",
      "Epoch 263/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 25.8810 - val_loss: 664.8160\n",
      "Epoch 264/500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 25.2488 - val_loss: 666.0306\n",
      "Epoch 265/500\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 24.6408 - val_loss: 667.2388\n",
      "Epoch 266/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 24.0561 - val_loss: 668.4401\n",
      "Epoch 267/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 23.4941 - val_loss: 669.6340\n",
      "Epoch 268/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 22.9539 - val_loss: 670.8196\n",
      "Epoch 269/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 22.4350 - val_loss: 671.9972\n",
      "Epoch 270/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 21.9367 - val_loss: 673.1659\n",
      "Epoch 271/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 21.4582 - val_loss: 674.3253\n",
      "Epoch 272/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 20.9990 - val_loss: 675.4750\n",
      "Epoch 273/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 20.5585 - val_loss: 676.6151\n",
      "Epoch 274/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 20.1360 - val_loss: 677.7445\n",
      "Epoch 275/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 19.7309 - val_loss: 678.8635\n",
      "Epoch 276/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 19.3427 - val_loss: 679.9716\n",
      "Epoch 277/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 18.9708 - val_loss: 681.0681\n",
      "Epoch 278/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 18.6146 - val_loss: 682.1530\n",
      "Epoch 279/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 18.2736 - val_loss: 683.2260\n",
      "Epoch 280/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 17.9472 - val_loss: 684.2870\n",
      "Epoch 281/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 17.6350 - val_loss: 685.3357\n",
      "Epoch 282/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 17.3364 - val_loss: 686.3718\n",
      "Epoch 283/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 17.0509 - val_loss: 687.3947\n",
      "Epoch 284/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 16.7782 - val_loss: 688.4045\n",
      "Epoch 285/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 16.5176 - val_loss: 689.4013\n",
      "Epoch 286/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 16.2688 - val_loss: 690.3844\n",
      "Epoch 287/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 16.0313 - val_loss: 691.3541\n",
      "Epoch 288/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.8047 - val_loss: 692.3101\n",
      "Epoch 289/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.5886 - val_loss: 693.2522\n",
      "Epoch 290/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.3824 - val_loss: 694.1802\n",
      "Epoch 291/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 15.1860 - val_loss: 695.0940\n",
      "Epoch 292/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.9989 - val_loss: 695.9940\n",
      "Epoch 293/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.8207 - val_loss: 696.8795\n",
      "Epoch 294/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.6510 - val_loss: 697.7507\n",
      "Epoch 295/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4896 - val_loss: 698.6074\n",
      "Epoch 296/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.3361 - val_loss: 699.4496\n",
      "Epoch 297/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.1901 - val_loss: 700.2773\n",
      "Epoch 298/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.0513 - val_loss: 701.0904\n",
      "Epoch 299/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.9196 - val_loss: 701.8892\n",
      "Epoch 300/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.7944 - val_loss: 702.6732\n",
      "Epoch 301/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.6756 - val_loss: 703.4429\n",
      "Epoch 302/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.5628 - val_loss: 704.1980\n",
      "Epoch 303/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.4559 - val_loss: 704.9384\n",
      "Epoch 304/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 13.3546 - val_loss: 705.6649\n",
      "Epoch 305/500\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 13.2585 - val_loss: 706.3766\n",
      "Epoch 306/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 13.1675 - val_loss: 707.0739\n",
      "Epoch 307/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 13.0813 - val_loss: 707.7574\n",
      "Epoch 308/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.9997 - val_loss: 708.4263\n",
      "Epoch 309/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.9226 - val_loss: 709.0812\n",
      "Epoch 310/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 12.8496 - val_loss: 709.7219\n",
      "Epoch 311/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.7807 - val_loss: 710.3489\n",
      "Epoch 312/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.7155 - val_loss: 710.9617\n",
      "Epoch 313/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 12.6540 - val_loss: 711.5612\n",
      "Epoch 314/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.5959 - val_loss: 712.1467\n",
      "Epoch 315/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 12.5411 - val_loss: 712.7189\n",
      "Epoch 316/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.4894 - val_loss: 713.2776\n",
      "Epoch 317/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.4406 - val_loss: 713.8234\n",
      "Epoch 318/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.3947 - val_loss: 714.3563\n",
      "Epoch 319/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.3514 - val_loss: 714.8759\n",
      "Epoch 320/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.3107 - val_loss: 715.3827\n",
      "Epoch 321/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.2723 - val_loss: 715.8768\n",
      "Epoch 322/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.2363 - val_loss: 716.3585\n",
      "Epoch 323/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.2024 - val_loss: 716.8281\n",
      "Epoch 324/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.1705 - val_loss: 717.2852\n",
      "Epoch 325/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.1406 - val_loss: 717.7303\n",
      "Epoch 326/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 12.1125 - val_loss: 718.1635\n",
      "Epoch 327/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 12.0861 - val_loss: 718.5855\n",
      "Epoch 328/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 12.0614 - val_loss: 718.9957\n",
      "Epoch 329/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.0381 - val_loss: 719.3947\n",
      "Epoch 330/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 12.0164 - val_loss: 719.7825\n",
      "Epoch 331/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.9960 - val_loss: 720.1602\n",
      "Epoch 332/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.9769 - val_loss: 720.5264\n",
      "Epoch 333/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.9591 - val_loss: 720.8824\n",
      "Epoch 334/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.9423 - val_loss: 721.2278\n",
      "Epoch 335/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.9267 - val_loss: 721.5635\n",
      "Epoch 336/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.9121 - val_loss: 721.8888\n",
      "Epoch 337/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.8984 - val_loss: 722.2042\n",
      "Epoch 338/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.8857 - val_loss: 722.5099\n",
      "Epoch 339/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.8738 - val_loss: 722.8069\n",
      "Epoch 340/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 11.8627 - val_loss: 723.0942\n",
      "Epoch 341/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.8523 - val_loss: 723.3727\n",
      "Epoch 342/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 11.8427 - val_loss: 723.6426\n",
      "Epoch 343/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 11.8336 - val_loss: 723.9035\n",
      "Epoch 344/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.8253 - val_loss: 724.1560\n",
      "Epoch 345/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.8174 - val_loss: 724.4003\n",
      "Epoch 346/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.8102 - val_loss: 724.6366\n",
      "Epoch 347/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.8034 - val_loss: 724.8647\n",
      "Epoch 348/500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 11.7972 - val_loss: 725.0858\n",
      "Epoch 349/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7914 - val_loss: 725.2993\n",
      "Epoch 350/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7859 - val_loss: 725.5051\n",
      "Epoch 351/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7809 - val_loss: 725.7040\n",
      "Epoch 352/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7762 - val_loss: 725.8962\n",
      "Epoch 353/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7719 - val_loss: 726.0811\n",
      "Epoch 354/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7679 - val_loss: 726.2599\n",
      "Epoch 355/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 11.7641 - val_loss: 726.4319\n",
      "Epoch 356/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.7607 - val_loss: 726.5978\n",
      "Epoch 357/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 11.7575 - val_loss: 726.7574\n",
      "Epoch 358/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 11.7545 - val_loss: 726.9112\n",
      "Epoch 359/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.7518 - val_loss: 727.0595\n",
      "Epoch 360/500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 11.7493 - val_loss: 727.2021\n",
      "Epoch 361/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 11.7470 - val_loss: 727.3395\n",
      "Epoch 362/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.7448 - val_loss: 727.4715\n",
      "Epoch 363/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.7429 - val_loss: 727.5986\n",
      "Epoch 364/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 11.7410 - val_loss: 727.7203\n",
      "Epoch 365/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.7393 - val_loss: 727.8373\n",
      "Epoch 366/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.7378 - val_loss: 727.9498\n",
      "Epoch 367/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 11.7364 - val_loss: 728.0578\n",
      "Epoch 368/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 11.7350 - val_loss: 728.1614\n",
      "Epoch 369/500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 11.7338 - val_loss: 728.2607\n",
      "Epoch 370/500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 11.7327 - val_loss: 728.3561\n",
      "Epoch 371/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.7317 - val_loss: 728.4473\n",
      "Epoch 372/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.7308 - val_loss: 728.5346\n",
      "Epoch 373/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 11.7299 - val_loss: 728.6188\n",
      "Epoch 374/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.7292 - val_loss: 728.6989\n",
      "Epoch 375/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 11.7285 - val_loss: 728.7756\n",
      "Epoch 376/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.7278 - val_loss: 728.8491\n",
      "Epoch 377/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.7272 - val_loss: 728.9193\n",
      "Epoch 378/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.7267 - val_loss: 728.9865\n",
      "Epoch 379/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 11.7262 - val_loss: 729.0507\n",
      "Epoch 380/500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 11.7258 - val_loss: 729.1121\n",
      "Epoch 381/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.7254 - val_loss: 729.1707\n",
      "Epoch 382/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.7251 - val_loss: 729.2266\n",
      "Epoch 383/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 11.7248 - val_loss: 729.2801\n",
      "Epoch 384/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.7245 - val_loss: 729.3307\n",
      "Epoch 385/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.7242 - val_loss: 729.3793\n",
      "Epoch 386/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.7241 - val_loss: 729.4258\n",
      "Epoch 387/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.7239 - val_loss: 729.4698\n",
      "Epoch 388/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 11.7237 - val_loss: 729.5114\n",
      "Epoch 389/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.7236 - val_loss: 729.5518\n",
      "Epoch 390/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.7234 - val_loss: 729.5894\n",
      "Epoch 391/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.7234 - val_loss: 729.6254\n",
      "Epoch 392/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.7233 - val_loss: 729.6603\n",
      "Epoch 393/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.7232 - val_loss: 729.6924\n",
      "Epoch 394/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.7232 - val_loss: 729.7237\n",
      "Epoch 395/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7232 - val_loss: 729.7535\n",
      "Epoch 396/500\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 11.7232 - val_loss: 729.7811\n",
      "Epoch 397/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 11.7232 - val_loss: 729.8077\n",
      "Epoch 398/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.7232 - val_loss: 729.8326\n",
      "Epoch 399/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 11.7232 - val_loss: 729.8565\n",
      "Epoch 400/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.7233 - val_loss: 729.8790\n",
      "Epoch 401/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 11.7233 - val_loss: 729.9001\n",
      "Epoch 402/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.7234 - val_loss: 729.9202\n",
      "Epoch 403/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.7235 - val_loss: 729.9395\n",
      "Epoch 404/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 11.7236 - val_loss: 729.9573\n",
      "Epoch 405/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7236 - val_loss: 729.9744\n",
      "Epoch 406/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 11.7237 - val_loss: 729.9902\n",
      "Epoch 407/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 11.7238 - val_loss: 730.0051\n",
      "Epoch 408/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 11.7240 - val_loss: 730.0197\n",
      "Epoch 409/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.7240 - val_loss: 730.0330\n",
      "Epoch 410/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7242 - val_loss: 730.0457\n",
      "Epoch 411/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7243 - val_loss: 730.0574\n",
      "Epoch 412/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7245 - val_loss: 730.0687\n",
      "Epoch 413/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7246 - val_loss: 730.0790\n",
      "Epoch 414/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7247 - val_loss: 730.0892\n",
      "Epoch 415/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7249 - val_loss: 730.0983\n",
      "Epoch 416/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7250 - val_loss: 730.1073\n",
      "Epoch 417/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7252 - val_loss: 730.1154\n",
      "Epoch 418/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7254 - val_loss: 730.1229\n",
      "Epoch 419/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7255 - val_loss: 730.1303\n",
      "Epoch 420/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7257 - val_loss: 730.1367\n",
      "Epoch 421/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7259 - val_loss: 730.1430\n",
      "Epoch 422/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7260 - val_loss: 730.1488\n",
      "Epoch 423/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7262 - val_loss: 730.1544\n",
      "Epoch 424/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7264 - val_loss: 730.1591\n",
      "Epoch 425/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7266 - val_loss: 730.1638\n",
      "Epoch 426/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.7267 - val_loss: 730.1681\n",
      "Epoch 427/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 11.7269 - val_loss: 730.1722\n",
      "Epoch 428/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7271 - val_loss: 730.1758\n",
      "Epoch 429/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7273 - val_loss: 730.1794\n",
      "Epoch 430/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7275 - val_loss: 730.1824\n",
      "Epoch 431/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7277 - val_loss: 730.1857\n",
      "Epoch 432/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7279 - val_loss: 730.1880\n",
      "Epoch 433/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7281 - val_loss: 730.1905\n",
      "Epoch 434/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7283 - val_loss: 730.1927\n",
      "Epoch 435/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7285 - val_loss: 730.1951\n",
      "Epoch 436/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7287 - val_loss: 730.1970\n",
      "Epoch 437/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7289 - val_loss: 730.1989\n",
      "Epoch 438/500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 11.7291 - val_loss: 730.2003\n",
      "Epoch 439/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7293 - val_loss: 730.2020\n",
      "Epoch 440/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7295 - val_loss: 730.2031\n",
      "Epoch 441/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7297 - val_loss: 730.2044\n",
      "Epoch 442/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7299 - val_loss: 730.2052\n",
      "Epoch 443/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7301 - val_loss: 730.2061\n",
      "Epoch 444/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7303 - val_loss: 730.2073\n",
      "Epoch 445/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7305 - val_loss: 730.2078\n",
      "Epoch 446/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7308 - val_loss: 730.2084\n",
      "Epoch 447/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7310 - val_loss: 730.2089\n",
      "Epoch 448/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7312 - val_loss: 730.2095\n",
      "Epoch 449/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7314 - val_loss: 730.2098\n",
      "Epoch 450/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7316 - val_loss: 730.2099\n",
      "Epoch 451/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7318 - val_loss: 730.2101\n",
      "Epoch 452/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7320 - val_loss: 730.2103\n",
      "Epoch 453/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7322 - val_loss: 730.2103\n",
      "Epoch 454/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7324 - val_loss: 730.2104\n",
      "Epoch 455/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7326 - val_loss: 730.2104\n",
      "Epoch 456/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7329 - val_loss: 730.2103\n",
      "Epoch 457/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7331 - val_loss: 730.2103\n",
      "Epoch 458/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7333 - val_loss: 730.2101\n",
      "Epoch 459/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7335 - val_loss: 730.2097\n",
      "Epoch 460/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7337 - val_loss: 730.2093\n",
      "Epoch 461/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7339 - val_loss: 730.2089\n",
      "Epoch 462/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7341 - val_loss: 730.2086\n",
      "Epoch 463/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7343 - val_loss: 730.2080\n",
      "Epoch 464/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7345 - val_loss: 730.2078\n",
      "Epoch 465/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7348 - val_loss: 730.2073\n",
      "Epoch 466/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7350 - val_loss: 730.2064\n",
      "Epoch 467/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7352 - val_loss: 730.2061\n",
      "Epoch 468/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7354 - val_loss: 730.2053\n",
      "Epoch 469/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7356 - val_loss: 730.2050\n",
      "Epoch 470/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7358 - val_loss: 730.2048\n",
      "Epoch 471/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7360 - val_loss: 730.2042\n",
      "Epoch 472/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7362 - val_loss: 730.2037\n",
      "Epoch 473/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7364 - val_loss: 730.2031\n",
      "Epoch 474/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7367 - val_loss: 730.2025\n",
      "Epoch 475/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 11.7369 - val_loss: 730.2020\n",
      "Epoch 476/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7371 - val_loss: 730.2012\n",
      "Epoch 477/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7373 - val_loss: 730.2008\n",
      "Epoch 478/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7375 - val_loss: 730.2001\n",
      "Epoch 479/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7377 - val_loss: 730.1992\n",
      "Epoch 480/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7379 - val_loss: 730.1990\n",
      "Epoch 481/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7381 - val_loss: 730.1984\n",
      "Epoch 482/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7383 - val_loss: 730.1976\n",
      "Epoch 483/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7385 - val_loss: 730.1974\n",
      "Epoch 484/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7388 - val_loss: 730.1970\n",
      "Epoch 485/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7390 - val_loss: 730.1961\n",
      "Epoch 486/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7392 - val_loss: 730.1957\n",
      "Epoch 487/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7394 - val_loss: 730.1952\n",
      "Epoch 488/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7395 - val_loss: 730.1943\n",
      "Epoch 489/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7397 - val_loss: 730.1939\n",
      "Epoch 490/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7399 - val_loss: 730.1931\n",
      "Epoch 491/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7402 - val_loss: 730.1927\n",
      "Epoch 492/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7403 - val_loss: 730.1917\n",
      "Epoch 493/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7405 - val_loss: 730.1911\n",
      "Epoch 494/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7408 - val_loss: 730.1905\n",
      "Epoch 495/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7409 - val_loss: 730.1898\n",
      "Epoch 496/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7411 - val_loss: 730.1891\n",
      "Epoch 497/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7413 - val_loss: 730.1882\n",
      "Epoch 498/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 11.7415 - val_loss: 730.1874\n",
      "Epoch 499/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 11.7417 - val_loss: 730.1871\n",
      "Epoch 500/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 11.7419 - val_loss: 730.1868\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6856, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0008, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0128, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(-2.3003, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 453ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.02828431e+01, 7.02744398e+01, 7.02660364e+01, 7.02576330e+01,\n",
       "        7.02492297e+01, 7.02408263e+01, 7.02324230e+01, 7.02240196e+01,\n",
       "        7.02156162e+01, 7.02072129e+01, 7.01988095e+01, 7.01904062e+01,\n",
       "        7.01820028e+01, 7.43443044e+01, 7.42602708e+01, 7.41762372e+01,\n",
       "        7.40922035e+01, 7.40081699e+01, 7.38862045e+01, 7.37601541e+01,\n",
       "        7.36341036e+01, 7.35080532e+01, 7.33820028e+01, 7.32559524e+01,\n",
       "        7.31299020e+01, 7.30038515e+01, 7.28778011e+01, 7.27517507e+01,\n",
       "        7.26257003e+01, 7.24995798e+01, 7.23483193e+01, 7.21970588e+01,\n",
       "        7.20457983e+01, 7.18945378e+01, 7.17432773e+01, 7.15920168e+01,\n",
       "        7.14407563e+01, 7.12894958e+01, 7.11382353e+01, 7.09869748e+01,\n",
       "        4.20791447e-01, 1.00360058e-01, 0.00000000e+00, 6.41931440e-02,\n",
       "        0.00000000e+00, 6.39821112e-01, 2.46869147e-01, 7.30318628e+01,\n",
       "        7.29058123e+01, 7.27797619e+01, 7.26537115e+01, 7.25276611e+01,\n",
       "        7.23819328e+01, 7.22306723e+01, 7.20794118e+01, 7.19281513e+01,\n",
       "        7.17768908e+01, 7.16256303e+01, 7.14743697e+01, 7.13231092e+01,\n",
       "        7.11718487e+01, 7.10205882e+01, 7.08693277e+01, 7.07180672e+01,\n",
       "        7.06778011e+01, 7.06525910e+01, 7.06273810e+01, 7.06021709e+01,\n",
       "        7.05769608e+01, 7.05517507e+01, 7.05265406e+01, 7.05013305e+01,\n",
       "        7.04761204e+01, 7.62993622e+01, 6.53601830e-01, 8.41440100e-02,\n",
       "        3.12900250e-01, 1.33408070e-01, 0.00000000e+00, 2.45064540e-01,\n",
       "        6.51591797e+01, 4.50676084e-01, 0.00000000e+00, 7.42013678e-02,\n",
       "        5.84851563e-01, 0.00000000e+00, 3.12022328e-01, 0.00000000e+00,\n",
       "        4.43458825e-01, 0.00000000e+00, 7.85947621e-01, 1.37494779e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.42258273e-02, 4.34099138e-02,\n",
       "        0.00000000e+00, 4.99962062e-01, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([66.17686975, 66.16631886, 66.15576797, 66.14521709, 66.1346662 ,\n",
       "       66.12411531, 66.11356443, 66.10301354, 66.09246265, 66.08191176,\n",
       "       66.07136088, 66.06080999, 66.0502591 , 66.03970822, 66.02915733,\n",
       "       66.01860644, 66.00805556, 65.99750467, 65.98695378, 65.97640289,\n",
       "       65.96585201, 65.95530112, 65.94475023, 65.93419935, 65.92364846,\n",
       "       65.91309757, 65.90254669, 65.8919958 , 65.88144491, 65.87089402,\n",
       "       65.86034314, 65.84979225, 65.83924136, 65.82869048, 65.81813959,\n",
       "       65.8075887 , 65.79703782, 65.78648693, 65.77593604, 65.76538515,\n",
       "       65.75483427, 65.74428338, 65.73373249, 65.72318161, 65.71263072,\n",
       "       65.70207983, 65.69152894, 65.68097806, 65.67042717, 65.65987628,\n",
       "       65.6493254 , 65.63877451, 65.62822362, 65.61767274, 65.60712185,\n",
       "       65.59726891, 65.58886555, 65.58046218, 65.57205882, 65.56365546,\n",
       "       65.5552521 , 65.54684874, 65.53844538, 65.53004202, 65.52163866,\n",
       "       65.51323529, 65.50483193, 65.49642857, 65.48802521, 65.47962185,\n",
       "       65.47121849, 65.46281513, 65.45441176, 65.4460084 , 65.43760504,\n",
       "       65.42920168, 65.42079832, 65.41239496, 65.4039916 , 65.39558824,\n",
       "       65.38718487, 65.37878151, 65.37037815, 65.36197479, 65.35357143,\n",
       "       65.34516807, 65.33676471, 65.32836134, 65.31995798, 65.31155462,\n",
       "       65.30315126, 65.2947479 , 65.28634454, 65.27794118, 65.26953782,\n",
       "       65.26113445, 65.25273109, 65.24432773, 65.23592437, 65.22752101])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.219759608264184\n",
      "24.900312707565078\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
