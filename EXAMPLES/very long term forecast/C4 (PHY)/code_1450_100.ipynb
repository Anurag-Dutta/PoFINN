{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "1545    67.255259\n",
       "1546    67.245268\n",
       "1547    67.235278\n",
       "1548    67.225287\n",
       "1549    67.215296\n",
       "Name: C4, Length: 1550, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c4_interpolated_1450_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "1445     0.000000\n",
       "1446     0.464026\n",
       "1447     0.955886\n",
       "1448     0.245065\n",
       "1449     0.809545\n",
       "Name: C4, Length: 1450, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1450)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.409524</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.219048</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.028571</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.838095</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.600000  0.000298\n",
       "1     88.409524  0.000298\n",
       "2     88.219048  0.000297\n",
       "3     88.028571  0.000297\n",
       "4     87.838095  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6856  # Value for C0\n",
    "K0 = -0.0008  # Value for K0\n",
    "K1 = -0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0128    # Value for b\n",
    "c = -2.3003    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.600000    88.409524    88.219048    88.028571    87.838095   \n",
      "351    88.409524    88.219048    88.028571    87.838095    87.647619   \n",
      "352    88.219048    88.028571    87.838095    87.647619    87.457143   \n",
      "353    88.028571    87.838095    87.647619    87.457143    87.266667   \n",
      "354    87.838095    87.647619    87.457143    87.266667    87.076190   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.647619    87.457143    87.266667    87.076190    86.896218  ...   \n",
      "351    87.457143    87.266667    87.076190    86.896218    86.845798  ...   \n",
      "352    87.266667    87.076190    86.896218    86.845798    86.795378  ...   \n",
      "353    87.076190    86.896218    86.845798    86.795378    86.744958  ...   \n",
      "354    86.896218    86.845798    86.795378    86.744958    86.694538  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   78.880345    0.000263   78.856069    0.000263   78.831793    0.000263   \n",
      "351   78.856069    0.000263   78.831793    0.000263   78.807516    0.000262   \n",
      "352   78.831793    0.000263   78.807516    0.000262   78.783240    0.000262   \n",
      "353   78.807516    0.000262   78.783240    0.000262   78.758964    0.000262   \n",
      "354   78.783240    0.000262   78.758964    0.000262   78.734687    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   78.807516    0.000262   78.783240    0.000262  \n",
      "351   78.783240    0.000262   78.758964    0.000262  \n",
      "352   78.758964    0.000262   78.734687    0.000262  \n",
      "353   78.734687    0.000262   78.710411    0.000262  \n",
      "354   78.710411    0.000262   78.686134    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1, 251) (1000, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "13/13 [==============================] - 3s 51ms/step - loss: 5640.9731 - val_loss: 4741.8716\n",
      "Epoch 2/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5535.2275 - val_loss: 4670.2373\n",
      "Epoch 3/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5470.5776 - val_loss: 4604.6660\n",
      "Epoch 4/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 5402.3564 - val_loss: 4555.1392\n",
      "Epoch 5/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 5348.7485 - val_loss: 4505.8608\n",
      "Epoch 6/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5290.0225 - val_loss: 4450.6279\n",
      "Epoch 7/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5234.7310 - val_loss: 4400.3452\n",
      "Epoch 8/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 5180.4248 - val_loss: 4350.7651\n",
      "Epoch 9/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 5126.8511 - val_loss: 4301.8477\n",
      "Epoch 10/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 5073.9395 - val_loss: 4253.5234\n",
      "Epoch 11/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 5021.6191 - val_loss: 4205.7324\n",
      "Epoch 12/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4969.8354 - val_loss: 4158.4380\n",
      "Epoch 13/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4918.5552 - val_loss: 4111.6108\n",
      "Epoch 14/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4867.7505 - val_loss: 4065.2312\n",
      "Epoch 15/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 4817.4033 - val_loss: 4019.2844\n",
      "Epoch 16/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 4767.4971 - val_loss: 3973.7578\n",
      "Epoch 17/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 4718.0205 - val_loss: 3928.6409\n",
      "Epoch 18/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 4668.9648 - val_loss: 3883.9275\n",
      "Epoch 19/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 4620.3223 - val_loss: 3839.6086\n",
      "Epoch 20/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 4572.0840 - val_loss: 3795.6797\n",
      "Epoch 21/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 4524.2451 - val_loss: 3752.1340\n",
      "Epoch 22/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 4476.8013 - val_loss: 3708.9678\n",
      "Epoch 23/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 4429.7451 - val_loss: 3666.1760\n",
      "Epoch 24/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 4383.0742 - val_loss: 3623.7554\n",
      "Epoch 25/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 4336.7842 - val_loss: 3581.7009\n",
      "Epoch 26/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 4290.8701 - val_loss: 3540.0100\n",
      "Epoch 27/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 4245.3301 - val_loss: 3498.6794\n",
      "Epoch 28/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4200.1602 - val_loss: 3457.7061\n",
      "Epoch 29/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4155.3574 - val_loss: 3417.0862\n",
      "Epoch 30/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4110.9175 - val_loss: 3376.8181\n",
      "Epoch 31/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4066.8391 - val_loss: 3336.8987\n",
      "Epoch 32/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4023.1184 - val_loss: 3297.3245\n",
      "Epoch 33/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 3979.7532 - val_loss: 3258.0935\n",
      "Epoch 34/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 3936.7412 - val_loss: 3219.2029\n",
      "Epoch 35/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3894.0789 - val_loss: 3180.6514\n",
      "Epoch 36/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3851.7654 - val_loss: 3142.4346\n",
      "Epoch 37/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3809.7966 - val_loss: 3104.5515\n",
      "Epoch 38/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3768.1707 - val_loss: 3067.0007\n",
      "Epoch 39/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3726.8862 - val_loss: 3029.7788\n",
      "Epoch 40/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3685.9407 - val_loss: 2992.8835\n",
      "Epoch 41/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3645.3306 - val_loss: 2956.3127\n",
      "Epoch 42/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3605.0547 - val_loss: 2920.0645\n",
      "Epoch 43/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3565.1116 - val_loss: 2884.1372\n",
      "Epoch 44/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3525.4980 - val_loss: 2848.5283\n",
      "Epoch 45/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3486.2119 - val_loss: 2813.2346\n",
      "Epoch 46/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3447.2520 - val_loss: 2778.2568\n",
      "Epoch 47/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3408.6157 - val_loss: 2743.5903\n",
      "Epoch 48/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3370.3013 - val_loss: 2709.2351\n",
      "Epoch 49/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3332.3066 - val_loss: 2675.1877\n",
      "Epoch 50/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3294.6296 - val_loss: 2641.4468\n",
      "Epoch 51/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3257.2688 - val_loss: 2608.0115\n",
      "Epoch 52/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3220.2219 - val_loss: 2574.8779\n",
      "Epoch 53/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3183.4866 - val_loss: 2542.0454\n",
      "Epoch 54/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3147.0618 - val_loss: 2509.5125\n",
      "Epoch 55/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3110.9456 - val_loss: 2477.2764\n",
      "Epoch 56/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3075.1360 - val_loss: 2445.3357\n",
      "Epoch 57/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3039.6299 - val_loss: 2413.6877\n",
      "Epoch 58/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3004.4277 - val_loss: 2382.3323\n",
      "Epoch 59/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2969.5259 - val_loss: 2351.2666\n",
      "Epoch 60/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2934.9241 - val_loss: 2320.4890\n",
      "Epoch 61/500\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 2900.6191 - val_loss: 2289.9988\n",
      "Epoch 62/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2866.6106 - val_loss: 2259.7925\n",
      "Epoch 63/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2832.8953 - val_loss: 2229.8696\n",
      "Epoch 64/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2799.4724 - val_loss: 2200.2268\n",
      "Epoch 65/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2766.3406 - val_loss: 2170.8652\n",
      "Epoch 66/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2733.4968 - val_loss: 2141.7803\n",
      "Epoch 67/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2700.9404 - val_loss: 2112.9717\n",
      "Epoch 68/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2668.6694 - val_loss: 2084.4380\n",
      "Epoch 69/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2636.6816 - val_loss: 2056.1763\n",
      "Epoch 70/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2604.9763 - val_loss: 2028.1859\n",
      "Epoch 71/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2573.5510 - val_loss: 2000.4653\n",
      "Epoch 72/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2542.4048 - val_loss: 1973.0122\n",
      "Epoch 73/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2511.5354 - val_loss: 1945.8252\n",
      "Epoch 74/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2480.9412 - val_loss: 1918.9033\n",
      "Epoch 75/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2450.6213 - val_loss: 1892.2437\n",
      "Epoch 76/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2420.5728 - val_loss: 1865.8456\n",
      "Epoch 77/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2390.7954 - val_loss: 1839.7070\n",
      "Epoch 78/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2361.2861 - val_loss: 1813.8269\n",
      "Epoch 79/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2332.0452 - val_loss: 1788.2030\n",
      "Epoch 80/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2303.0698 - val_loss: 1762.8347\n",
      "Epoch 81/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2274.3584 - val_loss: 1737.7191\n",
      "Epoch 82/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2245.9097 - val_loss: 1712.8556\n",
      "Epoch 83/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2217.7219 - val_loss: 1688.2418\n",
      "Epoch 84/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2189.7939 - val_loss: 1663.8773\n",
      "Epoch 85/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2162.1240 - val_loss: 1639.7593\n",
      "Epoch 86/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2134.7102 - val_loss: 1615.8878\n",
      "Epoch 87/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2107.5510 - val_loss: 1592.2605\n",
      "Epoch 88/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2080.6458 - val_loss: 1568.8745\n",
      "Epoch 89/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2053.9915 - val_loss: 1545.7305\n",
      "Epoch 90/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2027.5884 - val_loss: 1522.8263\n",
      "Epoch 91/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2001.4342 - val_loss: 1500.1600\n",
      "Epoch 92/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1975.5269 - val_loss: 1477.7295\n",
      "Epoch 93/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1949.8657 - val_loss: 1455.5350\n",
      "Epoch 94/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1924.4485 - val_loss: 1433.5735\n",
      "Epoch 95/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1899.2745 - val_loss: 1411.8441\n",
      "Epoch 96/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1874.3420 - val_loss: 1390.3455\n",
      "Epoch 97/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1849.6494 - val_loss: 1369.0767\n",
      "Epoch 98/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1825.1953 - val_loss: 1348.0347\n",
      "Epoch 99/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1800.9784 - val_loss: 1327.2197\n",
      "Epoch 100/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1776.9968 - val_loss: 1306.6290\n",
      "Epoch 101/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1753.2494 - val_loss: 1286.2615\n",
      "Epoch 102/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1729.7344 - val_loss: 1266.1162\n",
      "Epoch 103/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1706.4510 - val_loss: 1246.1918\n",
      "Epoch 104/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1683.3977 - val_loss: 1226.4854\n",
      "Epoch 105/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1660.5724 - val_loss: 1206.9973\n",
      "Epoch 106/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1637.9744 - val_loss: 1187.7256\n",
      "Epoch 107/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1615.6021 - val_loss: 1168.6685\n",
      "Epoch 108/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1593.4539 - val_loss: 1149.8250\n",
      "Epoch 109/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1571.5284 - val_loss: 1131.1934\n",
      "Epoch 110/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1549.8248 - val_loss: 1112.7722\n",
      "Epoch 111/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1528.3408 - val_loss: 1094.5602\n",
      "Epoch 112/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1507.0754 - val_loss: 1076.5562\n",
      "Epoch 113/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1486.0273 - val_loss: 1058.7588\n",
      "Epoch 114/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1465.1957 - val_loss: 1041.1659\n",
      "Epoch 115/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1444.5780 - val_loss: 1023.7773\n",
      "Epoch 116/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 1424.1735 - val_loss: 1006.5901\n",
      "Epoch 117/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 1403.9810 - val_loss: 989.6049\n",
      "Epoch 118/500\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 1383.9990 - val_loss: 972.8189\n",
      "Epoch 119/500\n",
      "13/13 [==============================] - 0s 24ms/step - loss: 1364.2262 - val_loss: 956.2312\n",
      "Epoch 120/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 1344.6613 - val_loss: 939.8406\n",
      "Epoch 121/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 1325.3026 - val_loss: 923.6449\n",
      "Epoch 122/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 1306.1486 - val_loss: 907.6435\n",
      "Epoch 123/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 1287.1991 - val_loss: 891.8354\n",
      "Epoch 124/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1268.4518 - val_loss: 876.2184\n",
      "Epoch 125/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1249.9059 - val_loss: 860.7924\n",
      "Epoch 126/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1231.5598 - val_loss: 845.5543\n",
      "Epoch 127/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1213.4116 - val_loss: 830.5042\n",
      "Epoch 128/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1195.4611 - val_loss: 815.6401\n",
      "Epoch 129/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1177.7063 - val_loss: 800.9604\n",
      "Epoch 130/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1160.1458 - val_loss: 786.4648\n",
      "Epoch 131/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1142.7788 - val_loss: 772.1512\n",
      "Epoch 132/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1125.6039 - val_loss: 758.0190\n",
      "Epoch 133/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1108.6196 - val_loss: 744.0665\n",
      "Epoch 134/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1091.8247 - val_loss: 730.2913\n",
      "Epoch 135/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 1075.2178 - val_loss: 716.6937\n",
      "Epoch 136/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1058.7977 - val_loss: 703.2721\n",
      "Epoch 137/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1042.5632 - val_loss: 690.0248\n",
      "Epoch 138/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1026.5132 - val_loss: 676.9506\n",
      "Epoch 139/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1010.6456 - val_loss: 664.0480\n",
      "Epoch 140/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 994.9596 - val_loss: 651.3156\n",
      "Epoch 141/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 979.4542 - val_loss: 638.7528\n",
      "Epoch 142/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 964.1284 - val_loss: 626.3590\n",
      "Epoch 143/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 948.9805 - val_loss: 614.1311\n",
      "Epoch 144/500\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 934.0093 - val_loss: 602.0682\n",
      "Epoch 145/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 919.2133 - val_loss: 590.1703\n",
      "Epoch 146/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 904.5916 - val_loss: 578.4354\n",
      "Epoch 147/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 890.1433 - val_loss: 566.8619\n",
      "Epoch 148/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 875.8661 - val_loss: 555.4485\n",
      "Epoch 149/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 861.7595 - val_loss: 544.1946\n",
      "Epoch 150/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 847.8220 - val_loss: 533.0990\n",
      "Epoch 151/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 834.0527 - val_loss: 522.1596\n",
      "Epoch 152/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 820.4501 - val_loss: 511.3755\n",
      "Epoch 153/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 807.0131 - val_loss: 500.7457\n",
      "Epoch 154/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 793.7405 - val_loss: 490.2695\n",
      "Epoch 155/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 780.6309 - val_loss: 479.9438\n",
      "Epoch 156/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 767.6830 - val_loss: 469.7698\n",
      "Epoch 157/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 754.8964 - val_loss: 459.7445\n",
      "Epoch 158/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 742.2690 - val_loss: 449.8671\n",
      "Epoch 159/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 729.7997 - val_loss: 440.1366\n",
      "Epoch 160/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 717.4875 - val_loss: 430.5515\n",
      "Epoch 161/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 705.3310 - val_loss: 421.1108\n",
      "Epoch 162/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 693.3293 - val_loss: 411.8131\n",
      "Epoch 163/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 681.4812 - val_loss: 402.6576\n",
      "Epoch 164/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 669.7851 - val_loss: 393.6424\n",
      "Epoch 165/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 658.2400 - val_loss: 384.7668\n",
      "Epoch 166/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 646.8447 - val_loss: 376.0294\n",
      "Epoch 167/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 635.5984 - val_loss: 367.4288\n",
      "Epoch 168/500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 624.4994 - val_loss: 358.9644\n",
      "Epoch 169/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 613.5468 - val_loss: 350.6346\n",
      "Epoch 170/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 602.7393 - val_loss: 342.4382\n",
      "Epoch 171/500\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 592.0754 - val_loss: 334.3743\n",
      "Epoch 172/500\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 581.5549 - val_loss: 326.4408\n",
      "Epoch 173/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 571.1757 - val_loss: 318.6375\n",
      "Epoch 174/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 560.9369 - val_loss: 310.9628\n",
      "Epoch 175/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 550.8374 - val_loss: 303.4156\n",
      "Epoch 176/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 540.8762 - val_loss: 295.9946\n",
      "Epoch 177/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 531.0519 - val_loss: 288.6988\n",
      "Epoch 178/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 521.3633 - val_loss: 281.5273\n",
      "Epoch 179/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 511.8096 - val_loss: 274.4777\n",
      "Epoch 180/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 502.3893 - val_loss: 267.5505\n",
      "Epoch 181/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 493.1011 - val_loss: 260.7430\n",
      "Epoch 182/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 483.9443 - val_loss: 254.0549\n",
      "Epoch 183/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 474.9173 - val_loss: 247.4848\n",
      "Epoch 184/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 466.0195 - val_loss: 241.0319\n",
      "Epoch 185/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 457.2493 - val_loss: 234.6944\n",
      "Epoch 186/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 448.6059 - val_loss: 228.4720\n",
      "Epoch 187/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 440.0877 - val_loss: 222.3624\n",
      "Epoch 188/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 431.6942 - val_loss: 216.3652\n",
      "Epoch 189/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 423.4239 - val_loss: 210.4794\n",
      "Epoch 190/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 415.2757 - val_loss: 204.7033\n",
      "Epoch 191/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 407.2483 - val_loss: 199.0358\n",
      "Epoch 192/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 399.3410 - val_loss: 193.4764\n",
      "Epoch 193/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 391.5523 - val_loss: 188.0233\n",
      "Epoch 194/500\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 383.8814 - val_loss: 182.6758\n",
      "Epoch 195/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 376.3271 - val_loss: 177.4323\n",
      "Epoch 196/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 368.8878 - val_loss: 172.2916\n",
      "Epoch 197/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 361.5629 - val_loss: 167.2533\n",
      "Epoch 198/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 354.3515 - val_loss: 162.3158\n",
      "Epoch 199/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 347.2520 - val_loss: 157.4778\n",
      "Epoch 200/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 340.2633 - val_loss: 152.7387\n",
      "Epoch 201/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 333.3848 - val_loss: 148.0970\n",
      "Epoch 202/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 326.6151 - val_loss: 143.5517\n",
      "Epoch 203/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 319.9530 - val_loss: 139.1013\n",
      "Epoch 204/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 313.3974 - val_loss: 134.7453\n",
      "Epoch 205/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 306.9474 - val_loss: 130.4821\n",
      "Epoch 206/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 300.6018 - val_loss: 126.3111\n",
      "Epoch 207/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 294.3596 - val_loss: 122.2309\n",
      "Epoch 208/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 288.2198 - val_loss: 118.2404\n",
      "Epoch 209/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 282.1812 - val_loss: 114.3382\n",
      "Epoch 210/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 276.2426 - val_loss: 110.5240\n",
      "Epoch 211/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 270.4032 - val_loss: 106.7959\n",
      "Epoch 212/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 264.6617 - val_loss: 103.1528\n",
      "Epoch 213/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 259.0170 - val_loss: 99.5947\n",
      "Epoch 214/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 253.4687 - val_loss: 96.1197\n",
      "Epoch 215/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 248.0151 - val_loss: 92.7268\n",
      "Epoch 216/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 242.6552 - val_loss: 89.4148\n",
      "Epoch 217/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 237.3881 - val_loss: 86.1826\n",
      "Epoch 218/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 232.2125 - val_loss: 83.0296\n",
      "Epoch 219/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 227.1279 - val_loss: 79.9547\n",
      "Epoch 220/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 222.1332 - val_loss: 76.9563\n",
      "Epoch 221/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 217.2268 - val_loss: 74.0336\n",
      "Epoch 222/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 212.4079 - val_loss: 71.1857\n",
      "Epoch 223/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 207.6755 - val_loss: 68.4115\n",
      "Epoch 224/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 203.0287 - val_loss: 65.7099\n",
      "Epoch 225/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 198.4666 - val_loss: 63.0799\n",
      "Epoch 226/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 193.9880 - val_loss: 60.5208\n",
      "Epoch 227/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 189.5918 - val_loss: 58.0305\n",
      "Epoch 228/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 185.2769 - val_loss: 55.6090\n",
      "Epoch 229/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 181.0424 - val_loss: 53.2550\n",
      "Epoch 230/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 176.8874 - val_loss: 50.9675\n",
      "Epoch 231/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 172.8110 - val_loss: 48.7455\n",
      "Epoch 232/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 168.8123 - val_loss: 46.5880\n",
      "Epoch 233/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 164.8900 - val_loss: 44.4939\n",
      "Epoch 234/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 161.0432 - val_loss: 42.4623\n",
      "Epoch 235/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 157.2711 - val_loss: 40.4921\n",
      "Epoch 236/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 153.5723 - val_loss: 38.5823\n",
      "Epoch 237/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 149.9461 - val_loss: 36.7322\n",
      "Epoch 238/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 146.3916 - val_loss: 34.9404\n",
      "Epoch 239/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 142.9077 - val_loss: 33.2062\n",
      "Epoch 240/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 139.4935 - val_loss: 31.5286\n",
      "Epoch 241/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 136.1481 - val_loss: 29.9065\n",
      "Epoch 242/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 132.8704 - val_loss: 28.3391\n",
      "Epoch 243/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 129.6598 - val_loss: 26.8255\n",
      "Epoch 244/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 126.5151 - val_loss: 25.3648\n",
      "Epoch 245/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 123.4355 - val_loss: 23.9558\n",
      "Epoch 246/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 120.4200 - val_loss: 22.5978\n",
      "Epoch 247/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 117.4678 - val_loss: 21.2897\n",
      "Epoch 248/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 114.5777 - val_loss: 20.0307\n",
      "Epoch 249/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 111.7491 - val_loss: 18.8198\n",
      "Epoch 250/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 108.9808 - val_loss: 17.6562\n",
      "Epoch 251/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 106.2723 - val_loss: 16.5388\n",
      "Epoch 252/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 103.6222 - val_loss: 15.4670\n",
      "Epoch 253/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 101.0296 - val_loss: 14.4394\n",
      "Epoch 254/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 98.4940 - val_loss: 13.4556\n",
      "Epoch 255/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 96.0143 - val_loss: 12.5147\n",
      "Epoch 256/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 93.5900 - val_loss: 11.6156\n",
      "Epoch 257/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 91.2199 - val_loss: 10.7576\n",
      "Epoch 258/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 88.9034 - val_loss: 9.9398\n",
      "Epoch 259/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 86.6394 - val_loss: 9.1614\n",
      "Epoch 260/500\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 84.4272 - val_loss: 8.4214\n",
      "Epoch 261/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 82.2658 - val_loss: 7.7191\n",
      "Epoch 262/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 80.1546 - val_loss: 7.0537\n",
      "Epoch 263/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 78.0924 - val_loss: 6.4241\n",
      "Epoch 264/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 76.0785 - val_loss: 5.8298\n",
      "Epoch 265/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 74.1122 - val_loss: 5.2699\n",
      "Epoch 266/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 72.1928 - val_loss: 4.7436\n",
      "Epoch 267/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 70.3194 - val_loss: 4.2500\n",
      "Epoch 268/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 68.4911 - val_loss: 3.7885\n",
      "Epoch 269/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 66.7071 - val_loss: 3.3581\n",
      "Epoch 270/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 64.9667 - val_loss: 2.9582\n",
      "Epoch 271/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 63.2691 - val_loss: 2.5880\n",
      "Epoch 272/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 61.6136 - val_loss: 2.2467\n",
      "Epoch 273/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 59.9994 - val_loss: 1.9337\n",
      "Epoch 274/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 58.4258 - val_loss: 1.6481\n",
      "Epoch 275/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 56.8919 - val_loss: 1.3891\n",
      "Epoch 276/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 55.3969 - val_loss: 1.1562\n",
      "Epoch 277/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 53.9403 - val_loss: 0.9486\n",
      "Epoch 278/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 52.5212 - val_loss: 0.7655\n",
      "Epoch 279/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 51.1388 - val_loss: 0.6063\n",
      "Epoch 280/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 49.7927 - val_loss: 0.4703\n",
      "Epoch 281/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 48.4822 - val_loss: 0.3568\n",
      "Epoch 282/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 47.2062 - val_loss: 0.2652\n",
      "Epoch 283/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 45.9646 - val_loss: 0.1948\n",
      "Epoch 284/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 44.7563 - val_loss: 0.1448\n",
      "Epoch 285/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 43.5805 - val_loss: 0.1148\n",
      "Epoch 286/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 42.4367 - val_loss: 0.1039\n",
      "Epoch 287/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 41.3244 - val_loss: 0.1117\n",
      "Epoch 288/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 40.2428 - val_loss: 0.1375\n",
      "Epoch 289/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 39.1913 - val_loss: 0.1806\n",
      "Epoch 290/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 38.1691 - val_loss: 0.2406\n",
      "Epoch 291/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 37.1758 - val_loss: 0.3167\n",
      "Epoch 292/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 36.2108 - val_loss: 0.4084\n",
      "Epoch 293/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 35.2733 - val_loss: 0.5151\n",
      "Epoch 294/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 34.3628 - val_loss: 0.6363\n",
      "Epoch 295/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 33.4786 - val_loss: 0.7714\n",
      "Epoch 296/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 32.6203 - val_loss: 0.9198\n",
      "Epoch 297/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 31.7870 - val_loss: 1.0811\n",
      "Epoch 298/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 30.9784 - val_loss: 1.2547\n",
      "Epoch 299/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 30.1939 - val_loss: 1.4400\n",
      "Epoch 300/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 29.4329 - val_loss: 1.6365\n",
      "Epoch 301/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 28.6949 - val_loss: 1.8438\n",
      "Epoch 302/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 27.9793 - val_loss: 2.0614\n",
      "Epoch 303/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 27.2855 - val_loss: 2.2887\n",
      "Epoch 304/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 26.6131 - val_loss: 2.5254\n",
      "Epoch 305/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 25.9616 - val_loss: 2.7709\n",
      "Epoch 306/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 25.3303 - val_loss: 3.0248\n",
      "Epoch 307/500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 24.7188 - val_loss: 3.2866\n",
      "Epoch 308/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 24.1267 - val_loss: 3.5560\n",
      "Epoch 309/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 23.5533 - val_loss: 3.8324\n",
      "Epoch 310/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 22.9983 - val_loss: 4.1155\n",
      "Epoch 311/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 22.4613 - val_loss: 4.4048\n",
      "Epoch 312/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 21.9416 - val_loss: 4.6999\n",
      "Epoch 313/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 21.4390 - val_loss: 5.0005\n",
      "Epoch 314/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 20.9528 - val_loss: 5.3062\n",
      "Epoch 315/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 20.4828 - val_loss: 5.6165\n",
      "Epoch 316/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 20.0285 - val_loss: 5.9313\n",
      "Epoch 317/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.5894 - val_loss: 6.2499\n",
      "Epoch 318/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 19.1651 - val_loss: 6.5722\n",
      "Epoch 319/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 18.7552 - val_loss: 6.8977\n",
      "Epoch 320/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 18.3594 - val_loss: 7.2263\n",
      "Epoch 321/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 17.9771 - val_loss: 7.5575\n",
      "Epoch 322/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 17.6081 - val_loss: 7.8910\n",
      "Epoch 323/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 17.2520 - val_loss: 8.2266\n",
      "Epoch 324/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 16.9083 - val_loss: 8.5639\n",
      "Epoch 325/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 16.5769 - val_loss: 8.9026\n",
      "Epoch 326/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 16.2572 - val_loss: 9.2426\n",
      "Epoch 327/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 15.9489 - val_loss: 9.5834\n",
      "Epoch 328/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.6517 - val_loss: 9.9249\n",
      "Epoch 329/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.3653 - val_loss: 10.2666\n",
      "Epoch 330/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.0893 - val_loss: 10.6086\n",
      "Epoch 331/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 14.8234 - val_loss: 10.9505\n",
      "Epoch 332/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 14.5674 - val_loss: 11.2922\n",
      "Epoch 333/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 14.3208 - val_loss: 11.6331\n",
      "Epoch 334/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 14.0835 - val_loss: 11.9736\n",
      "Epoch 335/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 13.8550 - val_loss: 12.3129\n",
      "Epoch 336/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 13.6352 - val_loss: 12.6511\n",
      "Epoch 337/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 13.4238 - val_loss: 12.9879\n",
      "Epoch 338/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 13.2204 - val_loss: 13.3233\n",
      "Epoch 339/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 13.0249 - val_loss: 13.6569\n",
      "Epoch 340/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 12.8369 - val_loss: 13.9886\n",
      "Epoch 341/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 12.6563 - val_loss: 14.3185\n",
      "Epoch 342/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 12.4827 - val_loss: 14.6460\n",
      "Epoch 343/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 12.3160 - val_loss: 14.9714\n",
      "Epoch 344/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 12.1558 - val_loss: 15.2942\n",
      "Epoch 345/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 12.0021 - val_loss: 15.6145\n",
      "Epoch 346/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 11.8545 - val_loss: 15.9319\n",
      "Epoch 347/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.7129 - val_loss: 16.2465\n",
      "Epoch 348/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 11.5770 - val_loss: 16.5581\n",
      "Epoch 349/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 11.4467 - val_loss: 16.8668\n",
      "Epoch 350/500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 11.3216 - val_loss: 17.1723\n",
      "Epoch 351/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 11.2018 - val_loss: 17.4743\n",
      "Epoch 352/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.0869 - val_loss: 17.7732\n",
      "Epoch 353/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 10.9768 - val_loss: 18.0686\n",
      "Epoch 354/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 10.8713 - val_loss: 18.3605\n",
      "Epoch 355/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 10.7702 - val_loss: 18.6487\n",
      "Epoch 356/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 10.6734 - val_loss: 18.9332\n",
      "Epoch 357/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 10.5808 - val_loss: 19.2142\n",
      "Epoch 358/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 10.4921 - val_loss: 19.4911\n",
      "Epoch 359/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 10.4072 - val_loss: 19.7645\n",
      "Epoch 360/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 10.3259 - val_loss: 20.0339\n",
      "Epoch 361/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 10.2482 - val_loss: 20.2993\n",
      "Epoch 362/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 10.1739 - val_loss: 20.5609\n",
      "Epoch 363/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 10.1028 - val_loss: 20.8185\n",
      "Epoch 364/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 10.0348 - val_loss: 21.0719\n",
      "Epoch 365/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.9698 - val_loss: 21.3215\n",
      "Epoch 366/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.9078 - val_loss: 21.5668\n",
      "Epoch 367/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.8485 - val_loss: 21.8081\n",
      "Epoch 368/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.7918 - val_loss: 22.0452\n",
      "Epoch 369/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.7377 - val_loss: 22.2783\n",
      "Epoch 370/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 9.6861 - val_loss: 22.5075\n",
      "Epoch 371/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.6367 - val_loss: 22.7323\n",
      "Epoch 372/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.5896 - val_loss: 22.9533\n",
      "Epoch 373/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.5446 - val_loss: 23.1699\n",
      "Epoch 374/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 9.5018 - val_loss: 23.3825\n",
      "Epoch 375/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.4609 - val_loss: 23.5912\n",
      "Epoch 376/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.4218 - val_loss: 23.7954\n",
      "Epoch 377/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.3846 - val_loss: 23.9956\n",
      "Epoch 378/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.3492 - val_loss: 24.1920\n",
      "Epoch 379/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.3153 - val_loss: 24.3841\n",
      "Epoch 380/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.2832 - val_loss: 24.5725\n",
      "Epoch 381/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.2524 - val_loss: 24.7569\n",
      "Epoch 382/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.2231 - val_loss: 24.9373\n",
      "Epoch 383/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 9.1952 - val_loss: 25.1136\n",
      "Epoch 384/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.1686 - val_loss: 25.2864\n",
      "Epoch 385/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 9.1433 - val_loss: 25.4551\n",
      "Epoch 386/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 9.1192 - val_loss: 25.6198\n",
      "Epoch 387/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 9.0963 - val_loss: 25.7809\n",
      "Epoch 388/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.0744 - val_loss: 25.9382\n",
      "Epoch 389/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.0536 - val_loss: 26.0920\n",
      "Epoch 390/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 9.0338 - val_loss: 26.2419\n",
      "Epoch 391/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 9.0149 - val_loss: 26.3884\n",
      "Epoch 392/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 8.9970 - val_loss: 26.5310\n",
      "Epoch 393/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 8.9799 - val_loss: 26.6702\n",
      "Epoch 394/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 8.9637 - val_loss: 26.8061\n",
      "Epoch 395/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.9482 - val_loss: 26.9386\n",
      "Epoch 396/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.9335 - val_loss: 27.0675\n",
      "Epoch 397/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.9195 - val_loss: 27.1932\n",
      "Epoch 398/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.9062 - val_loss: 27.3156\n",
      "Epoch 399/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 8.8936 - val_loss: 27.4347\n",
      "Epoch 400/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 8.8815 - val_loss: 27.5508\n",
      "Epoch 401/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.8701 - val_loss: 27.6637\n",
      "Epoch 402/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.8592 - val_loss: 27.7733\n",
      "Epoch 403/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.8489 - val_loss: 27.8801\n",
      "Epoch 404/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 8.8390 - val_loss: 27.9839\n",
      "Epoch 405/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 8.8296 - val_loss: 28.0848\n",
      "Epoch 406/500\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 8.8207 - val_loss: 28.1829\n",
      "Epoch 407/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.8123 - val_loss: 28.2780\n",
      "Epoch 408/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.8042 - val_loss: 28.3704\n",
      "Epoch 409/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.7966 - val_loss: 28.4600\n",
      "Epoch 410/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 8.7893 - val_loss: 28.5471\n",
      "Epoch 411/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 8.7824 - val_loss: 28.6315\n",
      "Epoch 412/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 8.7758 - val_loss: 28.7134\n",
      "Epoch 413/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.7696 - val_loss: 28.7928\n",
      "Epoch 414/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.7637 - val_loss: 28.8701\n",
      "Epoch 415/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 8.7580 - val_loss: 28.9448\n",
      "Epoch 416/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.7526 - val_loss: 29.0170\n",
      "Epoch 417/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.7475 - val_loss: 29.0871\n",
      "Epoch 418/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.7427 - val_loss: 29.1548\n",
      "Epoch 419/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 8.7381 - val_loss: 29.2203\n",
      "Epoch 420/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 8.7337 - val_loss: 29.2838\n",
      "Epoch 421/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 8.7295 - val_loss: 29.3452\n",
      "Epoch 422/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.7255 - val_loss: 29.4046\n",
      "Epoch 423/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.7218 - val_loss: 29.4619\n",
      "Epoch 424/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 8.7182 - val_loss: 29.5176\n",
      "Epoch 425/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 8.7148 - val_loss: 29.5713\n",
      "Epoch 426/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 8.7116 - val_loss: 29.6230\n",
      "Epoch 427/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 8.7085 - val_loss: 29.6728\n",
      "Epoch 428/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 8.7056 - val_loss: 29.7211\n",
      "Epoch 429/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.7028 - val_loss: 29.7677\n",
      "Epoch 430/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.7002 - val_loss: 29.8126\n",
      "Epoch 431/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.6977 - val_loss: 29.8560\n",
      "Epoch 432/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.6953 - val_loss: 29.8978\n",
      "Epoch 433/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 8.6930 - val_loss: 29.9381\n",
      "Epoch 434/500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 8.6909 - val_loss: 29.9768\n",
      "Epoch 435/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 8.6888 - val_loss: 30.0144\n",
      "Epoch 436/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 8.6869 - val_loss: 30.0505\n",
      "Epoch 437/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 8.6850 - val_loss: 30.0850\n",
      "Epoch 438/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 8.6833 - val_loss: 30.1185\n",
      "Epoch 439/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 8.6816 - val_loss: 30.1506\n",
      "Epoch 440/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 8.6800 - val_loss: 30.1815\n",
      "Epoch 441/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 8.6785 - val_loss: 30.2112\n",
      "Epoch 442/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 8.6771 - val_loss: 30.2399\n",
      "Epoch 443/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 8.6758 - val_loss: 30.2674\n",
      "Epoch 444/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 8.6744 - val_loss: 30.2938\n",
      "Epoch 445/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 8.6732 - val_loss: 30.3190\n",
      "Epoch 446/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 8.6721 - val_loss: 30.3436\n",
      "Epoch 447/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 8.6710 - val_loss: 30.3668\n",
      "Epoch 448/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 8.6699 - val_loss: 30.3893\n",
      "Epoch 449/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 8.6689 - val_loss: 30.4106\n",
      "Epoch 450/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 8.6680 - val_loss: 30.4314\n",
      "Epoch 451/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 8.6671 - val_loss: 30.4510\n",
      "Epoch 452/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 8.6663 - val_loss: 30.4698\n",
      "Epoch 453/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 8.6655 - val_loss: 30.4880\n",
      "Epoch 454/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 8.6647 - val_loss: 30.5052\n",
      "Epoch 455/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.6640 - val_loss: 30.5218\n",
      "Epoch 456/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 8.6634 - val_loss: 30.5377\n",
      "Epoch 457/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 8.6627 - val_loss: 30.5530\n",
      "Epoch 458/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 8.6621 - val_loss: 30.5674\n",
      "Epoch 459/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 8.6616 - val_loss: 30.5814\n",
      "Epoch 460/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 8.6611 - val_loss: 30.5947\n",
      "Epoch 461/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 8.6606 - val_loss: 30.6075\n",
      "Epoch 462/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 8.6601 - val_loss: 30.6195\n",
      "Epoch 463/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 8.6596 - val_loss: 30.6311\n",
      "Epoch 464/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 8.6592 - val_loss: 30.6420\n",
      "Epoch 465/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 8.6588 - val_loss: 30.6524\n",
      "Epoch 466/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 8.6585 - val_loss: 30.6625\n",
      "Epoch 467/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 8.6581 - val_loss: 30.6722\n",
      "Epoch 468/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 8.6578 - val_loss: 30.6810\n",
      "Epoch 469/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 8.6576 - val_loss: 30.6899\n",
      "Epoch 470/500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 8.6572 - val_loss: 30.6980\n",
      "Epoch 471/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 8.6570 - val_loss: 30.7060\n",
      "Epoch 472/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 8.6568 - val_loss: 30.7137\n",
      "Epoch 473/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.6565 - val_loss: 30.7208\n",
      "Epoch 474/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 8.6563 - val_loss: 30.7276\n",
      "Epoch 475/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.6561 - val_loss: 30.7338\n",
      "Epoch 476/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 8.6560 - val_loss: 30.7400\n",
      "Epoch 477/500\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 8.6558 - val_loss: 30.7457\n",
      "Epoch 478/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 8.6557 - val_loss: 30.7512\n",
      "Epoch 479/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 8.6555 - val_loss: 30.7563\n",
      "Epoch 480/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.6554 - val_loss: 30.7611\n",
      "Epoch 481/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 8.6553 - val_loss: 30.7659\n",
      "Epoch 482/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 8.6552 - val_loss: 30.7703\n",
      "Epoch 483/500\n",
      "13/13 [==============================] - 0s 21ms/step - loss: 8.6551 - val_loss: 30.7745\n",
      "Epoch 484/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 8.6550 - val_loss: 30.7784\n",
      "Epoch 485/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 8.6550 - val_loss: 30.7823\n",
      "Epoch 486/500\n",
      "13/13 [==============================] - 0s 31ms/step - loss: 8.6550 - val_loss: 30.7860\n",
      "Epoch 487/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.6549 - val_loss: 30.7894\n",
      "Epoch 488/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 8.6549 - val_loss: 30.7926\n",
      "Epoch 489/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 8.6548 - val_loss: 30.7956\n",
      "Epoch 490/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 8.6548 - val_loss: 30.7984\n",
      "Epoch 491/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 8.6548 - val_loss: 30.8010\n",
      "Epoch 492/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.6548 - val_loss: 30.8035\n",
      "Epoch 493/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 8.6548 - val_loss: 30.8059\n",
      "Epoch 494/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.6548 - val_loss: 30.8080\n",
      "Epoch 495/500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 8.6548 - val_loss: 30.8100\n",
      "Epoch 496/500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 8.6549 - val_loss: 30.8120\n",
      "Epoch 497/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.6549 - val_loss: 30.8138\n",
      "Epoch 498/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 8.6549 - val_loss: 30.8156\n",
      "Epoch 499/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 8.6550 - val_loss: 30.8173\n",
      "Epoch 500/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 8.6550 - val_loss: 30.8188\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6856, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0008, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0128, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(-2.3003, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.15584034e+01, 7.15079832e+01, 7.14575630e+01, 7.14071429e+01,\n",
       "        7.13567227e+01, 7.13063025e+01, 7.12558823e+01, 7.12054622e+01,\n",
       "        7.11550420e+01, 7.11046219e+01, 7.10542017e+01, 7.10037815e+01,\n",
       "        7.09533613e+01, 7.09029412e+01, 7.08525210e+01, 7.08021008e+01,\n",
       "        7.07516807e+01, 7.07012605e+01, 7.06918067e+01, 7.06834034e+01,\n",
       "        7.06750000e+01, 7.06665966e+01, 7.06581933e+01, 7.06497899e+01,\n",
       "        7.06413866e+01, 7.06329832e+01, 7.06245798e+01, 7.06161765e+01,\n",
       "        7.06077731e+01, 7.05993697e+01, 7.05909664e+01, 7.05825630e+01,\n",
       "        7.05741597e+01, 7.05657563e+01, 7.05573529e+01, 7.05489496e+01,\n",
       "        7.05405462e+01, 7.05321429e+01, 7.05237395e+01, 7.05153361e+01,\n",
       "        7.05069328e+01, 7.04985294e+01, 7.04901261e+01, 7.04817227e+01,\n",
       "        7.04733193e+01, 7.04649160e+01, 7.04565126e+01, 7.04481092e+01,\n",
       "        7.04397059e+01, 7.04313025e+01, 7.04228992e+01, 7.04144958e+01,\n",
       "        7.04060924e+01, 7.03976891e+01, 7.03892857e+01, 7.03808823e+01,\n",
       "        7.03724790e+01, 7.03640756e+01, 7.03556723e+01, 7.03472689e+01,\n",
       "        7.03388655e+01, 7.03304622e+01, 7.03220588e+01, 7.03136555e+01,\n",
       "        7.03052521e+01, 7.02968487e+01, 7.02884454e+01, 7.02800420e+01,\n",
       "        7.02716387e+01, 7.02632353e+01, 7.02548319e+01, 7.02464286e+01,\n",
       "        7.02380252e+01, 7.02296219e+01, 7.02212185e+01, 7.02128151e+01,\n",
       "        7.02044118e+01, 7.01960084e+01, 7.01876050e+01, 7.01792017e+01,\n",
       "        7.60199509e+01, 3.27307880e-01, 0.00000000e+00, 1.62238002e-01,\n",
       "        8.42299700e-01, 4.45066273e-01, 3.44524920e-01, 0.00000000e+00,\n",
       "        5.86903811e-01, 4.52334553e-01, 3.95751446e-02, 5.00404894e-01,\n",
       "        0.00000000e+00, 2.30693817e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.03150555e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([68.55583567, 68.53716153, 68.51848739, 68.49981326, 68.48113912,\n",
       "       68.46246499, 68.44379085, 68.42511671, 68.40644258, 68.38776844,\n",
       "       68.3690943 , 68.35042017, 68.33174603, 68.3130719 , 68.29439776,\n",
       "       68.27572362, 68.25704949, 68.23837535, 68.21970121, 68.20102708,\n",
       "       68.18235294, 68.1636788 , 68.14500467, 68.12633053, 68.1076564 ,\n",
       "       68.08898226, 68.07030812, 68.05163399, 68.03295985, 68.01428571,\n",
       "       67.99561158, 67.97693744, 67.95826331, 67.93958917, 67.92091503,\n",
       "       67.9022409 , 67.88356676, 67.86489262, 67.84621849, 67.82754435,\n",
       "       67.80887021, 67.7947549 , 67.78476424, 67.77477358, 67.76478291,\n",
       "       67.75479225, 67.74480159, 67.73481092, 67.72482026, 67.7148296 ,\n",
       "       67.70483894, 67.69484827, 67.68485761, 67.67486695, 67.66487628,\n",
       "       67.65488562, 67.64489496, 67.6349043 , 67.62491363, 67.61492297,\n",
       "       67.60493231, 67.59494164, 67.58495098, 67.57496032, 67.56496965,\n",
       "       67.55497899, 67.54498833, 67.53499767, 67.525007  , 67.51501634,\n",
       "       67.50502568, 67.49503501, 67.48504435, 67.47505369, 67.46506303,\n",
       "       67.45507236, 67.4450817 , 67.43509104, 67.42510037, 67.41510971,\n",
       "       67.40511905, 67.39512838, 67.38513772, 67.37514706, 67.3651564 ,\n",
       "       67.35516573, 67.34517507, 67.33518441, 67.32519374, 67.31520308,\n",
       "       67.30521242, 67.29522176, 67.28523109, 67.27524043, 67.26524977,\n",
       "       67.2552591 , 67.24526844, 67.23527778, 67.22528711, 67.21529645])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.357697728680616\n",
      "14.986816084567758\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
