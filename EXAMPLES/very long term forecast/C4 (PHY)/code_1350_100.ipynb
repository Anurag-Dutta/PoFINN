{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "1445    68.649206\n",
       "1446    68.630532\n",
       "1447    68.611858\n",
       "1448    68.593184\n",
       "1449    68.574510\n",
       "Name: C4, Length: 1450, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c4_interpolated_1350_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       88.600000\n",
       "1       88.409524\n",
       "2       88.219048\n",
       "3       88.028571\n",
       "4       87.838095\n",
       "          ...    \n",
       "1345     0.000000\n",
       "1346     0.639821\n",
       "1347     0.807693\n",
       "1348     0.326411\n",
       "1349     0.246869\n",
       "Name: C4, Length: 1350, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1350)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.409524</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.219048</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.028571</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.838095</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     88.600000  0.000298\n",
       "1     88.409524  0.000298\n",
       "2     88.219048  0.000297\n",
       "3     88.028571  0.000297\n",
       "4     87.838095  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.6856  # Value for C0\n",
    "K0 = -0.0008  # Value for K0\n",
    "K1 = -0.0002  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0128    # Value for b\n",
    "c = -2.3003    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    88.600000    88.409524    88.219048    88.028571    87.838095   \n",
      "351    88.409524    88.219048    88.028571    87.838095    87.647619   \n",
      "352    88.219048    88.028571    87.838095    87.647619    87.457143   \n",
      "353    88.028571    87.838095    87.647619    87.457143    87.266667   \n",
      "354    87.838095    87.647619    87.457143    87.266667    87.076190   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.647619    87.457143    87.266667    87.076190    86.896218  ...   \n",
      "351    87.457143    87.266667    87.076190    86.896218    86.845798  ...   \n",
      "352    87.266667    87.076190    86.896218    86.845798    86.795378  ...   \n",
      "353    87.076190    86.896218    86.845798    86.795378    86.744958  ...   \n",
      "354    86.896218    86.845798    86.795378    86.744958    86.694538  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   78.880345    0.000263   78.856069    0.000263   78.831793    0.000263   \n",
      "351   78.856069    0.000263   78.831793    0.000263   78.807516    0.000262   \n",
      "352   78.831793    0.000263   78.807516    0.000262   78.783240    0.000262   \n",
      "353   78.807516    0.000262   78.783240    0.000262   78.758964    0.000262   \n",
      "354   78.783240    0.000262   78.758964    0.000262   78.734687    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   78.807516    0.000262   78.783240    0.000262  \n",
      "351   78.783240    0.000262   78.758964    0.000262  \n",
      "352   78.758964    0.000262   78.734687    0.000262  \n",
      "353   78.734687    0.000262   78.710411    0.000262  \n",
      "354   78.710411    0.000262   78.686134    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 1, 251) (900, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "12/12 [==============================] - 3s 74ms/step - loss: 5652.7886 - val_loss: 4758.9150\n",
      "Epoch 2/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 5560.0684 - val_loss: 4711.9429\n",
      "Epoch 3/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 5517.4717 - val_loss: 4674.0459\n",
      "Epoch 4/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 5458.0522 - val_loss: 4614.8716\n",
      "Epoch 5/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 5411.6187 - val_loss: 4575.5898\n",
      "Epoch 6/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 5369.1558 - val_loss: 4536.6245\n",
      "Epoch 7/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 5327.0527 - val_loss: 4498.0132\n",
      "Epoch 8/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 5285.3115 - val_loss: 4459.7334\n",
      "Epoch 9/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 5243.9033 - val_loss: 4421.7603\n",
      "Epoch 10/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 5202.8047 - val_loss: 4384.0732\n",
      "Epoch 11/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 5161.9956 - val_loss: 4346.6592\n",
      "Epoch 12/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 5121.4644 - val_loss: 4309.5059\n",
      "Epoch 13/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 5081.2002 - val_loss: 4272.6094\n",
      "Epoch 14/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 5041.1968 - val_loss: 4235.9604\n",
      "Epoch 15/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 5001.4487 - val_loss: 4199.5557\n",
      "Epoch 16/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4961.9502 - val_loss: 4163.3926\n",
      "Epoch 17/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 4922.6978 - val_loss: 4127.4653\n",
      "Epoch 18/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4883.6895 - val_loss: 4091.7737\n",
      "Epoch 19/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4844.9209 - val_loss: 4056.3135\n",
      "Epoch 20/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4806.3901 - val_loss: 4021.0837\n",
      "Epoch 21/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4768.0957 - val_loss: 3986.0808\n",
      "Epoch 22/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 4730.0342 - val_loss: 3951.3049\n",
      "Epoch 23/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 4692.2051 - val_loss: 3916.7524\n",
      "Epoch 24/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 4654.6055 - val_loss: 3882.4236\n",
      "Epoch 25/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 4617.2349 - val_loss: 3848.3149\n",
      "Epoch 26/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 4580.0913 - val_loss: 3814.4268\n",
      "Epoch 27/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 4543.1738 - val_loss: 3780.7563\n",
      "Epoch 28/500\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 4506.4795 - val_loss: 3747.3035\n",
      "Epoch 29/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 4470.0078 - val_loss: 3714.0657\n",
      "Epoch 30/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 4433.7578 - val_loss: 3681.0425\n",
      "Epoch 31/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 4397.7290 - val_loss: 3648.2322\n",
      "Epoch 32/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 4361.9175 - val_loss: 3615.6348\n",
      "Epoch 33/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 4324.7744 - val_loss: 3579.6528\n",
      "Epoch 34/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4285.1719 - val_loss: 3543.5393\n",
      "Epoch 35/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 4245.9014 - val_loss: 3508.1125\n",
      "Epoch 36/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4207.3770 - val_loss: 3473.3479\n",
      "Epoch 37/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4169.5117 - val_loss: 3439.1389\n",
      "Epoch 38/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 4132.1987 - val_loss: 3405.4011\n",
      "Epoch 39/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4095.3572 - val_loss: 3372.0750\n",
      "Epoch 40/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4058.9316 - val_loss: 3339.1191\n",
      "Epoch 41/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 4022.8816 - val_loss: 3306.5034\n",
      "Epoch 42/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3987.1792 - val_loss: 3274.2058\n",
      "Epoch 43/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3951.8020 - val_loss: 3242.2095\n",
      "Epoch 44/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3916.7336 - val_loss: 3210.5002\n",
      "Epoch 45/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3881.9614 - val_loss: 3179.0676\n",
      "Epoch 46/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3847.4736 - val_loss: 3147.9036\n",
      "Epoch 47/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 3813.2622 - val_loss: 3117.0000\n",
      "Epoch 48/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3779.3181 - val_loss: 3086.3494\n",
      "Epoch 49/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3745.6357 - val_loss: 3055.9465\n",
      "Epoch 50/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3712.2090 - val_loss: 3025.7869\n",
      "Epoch 51/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3679.0330 - val_loss: 2995.8652\n",
      "Epoch 52/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3646.1028 - val_loss: 2966.1794\n",
      "Epoch 53/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 3613.4146 - val_loss: 2936.7244\n",
      "Epoch 54/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 3580.9653 - val_loss: 2907.4966\n",
      "Epoch 55/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3548.7502 - val_loss: 2878.4937\n",
      "Epoch 56/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 3516.7681 - val_loss: 2849.7144\n",
      "Epoch 57/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3485.0149 - val_loss: 2821.1531\n",
      "Epoch 58/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3453.4883 - val_loss: 2792.8096\n",
      "Epoch 59/500\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 3422.1851 - val_loss: 2764.6809\n",
      "Epoch 60/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 3391.1042 - val_loss: 2736.7646\n",
      "Epoch 61/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 3360.2427 - val_loss: 2709.0598\n",
      "Epoch 62/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 3329.5986 - val_loss: 2681.5642\n",
      "Epoch 63/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 3299.1702 - val_loss: 2654.2744\n",
      "Epoch 64/500\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 3268.9558 - val_loss: 2627.1914\n",
      "Epoch 65/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 3238.9529 - val_loss: 2600.3113\n",
      "Epoch 66/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 3209.1606 - val_loss: 2573.6331\n",
      "Epoch 67/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 3179.5767 - val_loss: 2547.1555\n",
      "Epoch 68/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 3150.2004 - val_loss: 2520.8777\n",
      "Epoch 69/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 3121.0288 - val_loss: 2494.7971\n",
      "Epoch 70/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 3092.0618 - val_loss: 2468.9126\n",
      "Epoch 71/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3063.2971 - val_loss: 2443.2231\n",
      "Epoch 72/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 3034.7344 - val_loss: 2417.7275\n",
      "Epoch 73/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 3006.3706 - val_loss: 2392.4233\n",
      "Epoch 74/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 2978.2058 - val_loss: 2367.3115\n",
      "Epoch 75/500\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 2950.2385 - val_loss: 2342.3894\n",
      "Epoch 76/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 2922.4678 - val_loss: 2317.6548\n",
      "Epoch 77/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 2894.8904 - val_loss: 2293.1091\n",
      "Epoch 78/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 2867.5081 - val_loss: 2268.7483\n",
      "Epoch 79/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 2840.3176 - val_loss: 2244.5730\n",
      "Epoch 80/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 2813.3186 - val_loss: 2220.5823\n",
      "Epoch 81/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 2786.5098 - val_loss: 2196.7744\n",
      "Epoch 82/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 2759.8896 - val_loss: 2173.1479\n",
      "Epoch 83/500\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 2733.4578 - val_loss: 2149.7029\n",
      "Epoch 84/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 2707.2134 - val_loss: 2126.4375\n",
      "Epoch 85/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 2681.1543 - val_loss: 2103.3506\n",
      "Epoch 86/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 2655.2800 - val_loss: 2080.4419\n",
      "Epoch 87/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 2629.5898 - val_loss: 2057.7097\n",
      "Epoch 88/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2604.0820 - val_loss: 2035.1534\n",
      "Epoch 89/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2578.7566 - val_loss: 2012.7715\n",
      "Epoch 90/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 2553.6111 - val_loss: 1990.5637\n",
      "Epoch 91/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2528.6462 - val_loss: 1968.5292\n",
      "Epoch 92/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2503.8604 - val_loss: 1946.6665\n",
      "Epoch 93/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 2479.2517 - val_loss: 1924.9744\n",
      "Epoch 94/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2454.8203 - val_loss: 1903.4524\n",
      "Epoch 95/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2430.5652 - val_loss: 1882.1003\n",
      "Epoch 96/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2406.4854 - val_loss: 1860.9160\n",
      "Epoch 97/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2382.5791 - val_loss: 1839.8992\n",
      "Epoch 98/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2358.8467 - val_loss: 1819.0486\n",
      "Epoch 99/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2335.2864 - val_loss: 1798.3643\n",
      "Epoch 100/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2311.8982 - val_loss: 1777.8445\n",
      "Epoch 101/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2288.6809 - val_loss: 1757.4890\n",
      "Epoch 102/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2265.6331 - val_loss: 1737.2961\n",
      "Epoch 103/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2242.7537 - val_loss: 1717.2648\n",
      "Epoch 104/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2220.0422 - val_loss: 1697.3951\n",
      "Epoch 105/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2197.4985 - val_loss: 1677.6862\n",
      "Epoch 106/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2175.1211 - val_loss: 1658.1365\n",
      "Epoch 107/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2152.9089 - val_loss: 1638.7458\n",
      "Epoch 108/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2130.8618 - val_loss: 1619.5134\n",
      "Epoch 109/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2108.9788 - val_loss: 1600.4376\n",
      "Epoch 110/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2087.2581 - val_loss: 1581.5189\n",
      "Epoch 111/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2065.7004 - val_loss: 1562.7557\n",
      "Epoch 112/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2044.3040 - val_loss: 1544.1471\n",
      "Epoch 113/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2023.0679 - val_loss: 1525.6920\n",
      "Epoch 114/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 2001.9923 - val_loss: 1507.3910\n",
      "Epoch 115/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1981.0747 - val_loss: 1489.2417\n",
      "Epoch 116/500\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 1960.3158 - val_loss: 1471.2441\n",
      "Epoch 117/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1939.7144 - val_loss: 1453.3976\n",
      "Epoch 118/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1919.2700 - val_loss: 1435.7012\n",
      "Epoch 119/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1898.9811 - val_loss: 1418.1538\n",
      "Epoch 120/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1878.8475 - val_loss: 1400.7544\n",
      "Epoch 121/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1858.8685 - val_loss: 1383.5038\n",
      "Epoch 122/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1839.0435 - val_loss: 1366.3998\n",
      "Epoch 123/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1819.3705 - val_loss: 1349.4419\n",
      "Epoch 124/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1799.8502 - val_loss: 1332.6296\n",
      "Epoch 125/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1780.4810 - val_loss: 1315.9618\n",
      "Epoch 126/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1761.2622 - val_loss: 1299.4382\n",
      "Epoch 127/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1742.1936 - val_loss: 1283.0585\n",
      "Epoch 128/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1723.2743 - val_loss: 1266.8206\n",
      "Epoch 129/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1704.5033 - val_loss: 1250.7241\n",
      "Epoch 130/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1685.8796 - val_loss: 1234.7693\n",
      "Epoch 131/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1667.4031 - val_loss: 1218.9545\n",
      "Epoch 132/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1649.0724 - val_loss: 1203.2793\n",
      "Epoch 133/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1630.8872 - val_loss: 1187.7432\n",
      "Epoch 134/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1612.8470 - val_loss: 1172.3451\n",
      "Epoch 135/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1594.9510 - val_loss: 1157.0840\n",
      "Epoch 136/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1577.1979 - val_loss: 1141.9609\n",
      "Epoch 137/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1559.5880 - val_loss: 1126.9725\n",
      "Epoch 138/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1542.1200 - val_loss: 1112.1202\n",
      "Epoch 139/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1524.7931 - val_loss: 1097.4019\n",
      "Epoch 140/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1507.6064 - val_loss: 1082.8186\n",
      "Epoch 141/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1490.5597 - val_loss: 1068.3676\n",
      "Epoch 142/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1473.6521 - val_loss: 1054.0488\n",
      "Epoch 143/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1456.8829 - val_loss: 1039.8622\n",
      "Epoch 144/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1440.2513 - val_loss: 1025.8069\n",
      "Epoch 145/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 1423.7568 - val_loss: 1011.8820\n",
      "Epoch 146/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1407.3989 - val_loss: 998.0863\n",
      "Epoch 147/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1391.1765 - val_loss: 984.4198\n",
      "Epoch 148/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1375.0887 - val_loss: 970.8818\n",
      "Epoch 149/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1359.1357 - val_loss: 957.4713\n",
      "Epoch 150/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1343.3163 - val_loss: 944.1884\n",
      "Epoch 151/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1327.6299 - val_loss: 931.0307\n",
      "Epoch 152/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1312.0753 - val_loss: 917.9995\n",
      "Epoch 153/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1296.6527 - val_loss: 905.0930\n",
      "Epoch 154/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1281.3611 - val_loss: 892.3109\n",
      "Epoch 155/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1266.2000 - val_loss: 879.6527\n",
      "Epoch 156/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1251.1683 - val_loss: 867.1166\n",
      "Epoch 157/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1236.2655 - val_loss: 854.7038\n",
      "Epoch 158/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1221.4911 - val_loss: 842.4116\n",
      "Epoch 159/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1206.8441 - val_loss: 830.2408\n",
      "Epoch 160/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1192.3241 - val_loss: 818.1908\n",
      "Epoch 161/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1177.9304 - val_loss: 806.2597\n",
      "Epoch 162/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1163.6626 - val_loss: 794.4476\n",
      "Epoch 163/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1149.5197 - val_loss: 782.7538\n",
      "Epoch 164/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1135.5011 - val_loss: 771.1785\n",
      "Epoch 165/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1121.6063 - val_loss: 759.7195\n",
      "Epoch 166/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1107.8348 - val_loss: 748.3765\n",
      "Epoch 167/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1094.1853 - val_loss: 737.1494\n",
      "Epoch 168/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1080.6577 - val_loss: 726.0378\n",
      "Epoch 169/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1067.2515 - val_loss: 715.0401\n",
      "Epoch 170/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1053.9653 - val_loss: 704.1556\n",
      "Epoch 171/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1040.7993 - val_loss: 693.3849\n",
      "Epoch 172/500\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 1027.7523 - val_loss: 682.7268\n",
      "Epoch 173/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1014.8245 - val_loss: 672.1801\n",
      "Epoch 174/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1002.0140 - val_loss: 661.7455\n",
      "Epoch 175/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 989.3214 - val_loss: 651.4199\n",
      "Epoch 176/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 976.7451 - val_loss: 641.2051\n",
      "Epoch 177/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 964.2849 - val_loss: 631.0994\n",
      "Epoch 178/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 951.9399 - val_loss: 621.1023\n",
      "Epoch 179/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 939.7097 - val_loss: 611.2133\n",
      "Epoch 180/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 927.5944 - val_loss: 601.4308\n",
      "Epoch 181/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 915.5919 - val_loss: 591.7556\n",
      "Epoch 182/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 903.7026 - val_loss: 582.1869\n",
      "Epoch 183/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 891.9261 - val_loss: 572.7230\n",
      "Epoch 184/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 880.2607 - val_loss: 563.3647\n",
      "Epoch 185/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 868.7071 - val_loss: 554.1100\n",
      "Epoch 186/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 857.2637 - val_loss: 544.9593\n",
      "Epoch 187/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 845.9301 - val_loss: 535.9111\n",
      "Epoch 188/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 834.7060 - val_loss: 526.9655\n",
      "Epoch 189/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 823.5900 - val_loss: 518.1214\n",
      "Epoch 190/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 812.5826 - val_loss: 509.3785\n",
      "Epoch 191/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 801.6823 - val_loss: 500.7361\n",
      "Epoch 192/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 790.8887 - val_loss: 492.1930\n",
      "Epoch 193/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 780.2017 - val_loss: 483.7495\n",
      "Epoch 194/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 769.6199 - val_loss: 475.4044\n",
      "Epoch 195/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 759.1431 - val_loss: 467.1576\n",
      "Epoch 196/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 748.7706 - val_loss: 459.0075\n",
      "Epoch 197/500\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 738.5022 - val_loss: 450.9544\n",
      "Epoch 198/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 728.3367 - val_loss: 442.9974\n",
      "Epoch 199/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 718.2737 - val_loss: 435.1361\n",
      "Epoch 200/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 708.3128 - val_loss: 427.3695\n",
      "Epoch 201/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 698.4535 - val_loss: 419.6974\n",
      "Epoch 202/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 688.6950 - val_loss: 412.1187\n",
      "Epoch 203/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 679.0364 - val_loss: 404.6332\n",
      "Epoch 204/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 669.4773 - val_loss: 397.2403\n",
      "Epoch 205/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 660.0176 - val_loss: 389.9387\n",
      "Epoch 206/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 650.6557 - val_loss: 382.7289\n",
      "Epoch 207/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 641.3920 - val_loss: 375.6093\n",
      "Epoch 208/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 632.2255 - val_loss: 368.5794\n",
      "Epoch 209/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 623.1553 - val_loss: 361.6392\n",
      "Epoch 210/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 614.1813 - val_loss: 354.7877\n",
      "Epoch 211/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 605.3025 - val_loss: 348.0247\n",
      "Epoch 212/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 596.5187 - val_loss: 341.3487\n",
      "Epoch 213/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 587.8291 - val_loss: 334.7602\n",
      "Epoch 214/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 579.2330 - val_loss: 328.2576\n",
      "Epoch 215/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 570.7299 - val_loss: 321.8407\n",
      "Epoch 216/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 562.3195 - val_loss: 315.5092\n",
      "Epoch 217/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 554.0007 - val_loss: 309.2617\n",
      "Epoch 218/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 545.7733 - val_loss: 303.0991\n",
      "Epoch 219/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 537.6366 - val_loss: 297.0190\n",
      "Epoch 220/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 529.5901 - val_loss: 291.0220\n",
      "Epoch 221/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 521.6334 - val_loss: 285.1072\n",
      "Epoch 222/500\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 513.7654 - val_loss: 279.2738\n",
      "Epoch 223/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 505.9855 - val_loss: 273.5211\n",
      "Epoch 224/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 498.2934 - val_loss: 267.8485\n",
      "Epoch 225/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 490.6885 - val_loss: 262.2562\n",
      "Epoch 226/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 483.1706 - val_loss: 256.7430\n",
      "Epoch 227/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 475.7388 - val_loss: 251.3086\n",
      "Epoch 228/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 468.3923 - val_loss: 245.9520\n",
      "Epoch 229/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 461.1309 - val_loss: 240.6725\n",
      "Epoch 230/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 453.9533 - val_loss: 235.4696\n",
      "Epoch 231/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 446.8595 - val_loss: 230.3432\n",
      "Epoch 232/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 439.8489 - val_loss: 225.2920\n",
      "Epoch 233/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 432.9208 - val_loss: 220.3157\n",
      "Epoch 234/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 426.0747 - val_loss: 215.4143\n",
      "Epoch 235/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 419.3103 - val_loss: 210.5863\n",
      "Epoch 236/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 412.6266 - val_loss: 205.8317\n",
      "Epoch 237/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 406.0232 - val_loss: 201.1497\n",
      "Epoch 238/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 399.4997 - val_loss: 196.5399\n",
      "Epoch 239/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 393.0550 - val_loss: 192.0011\n",
      "Epoch 240/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 386.6891 - val_loss: 187.5336\n",
      "Epoch 241/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 380.4011 - val_loss: 183.1359\n",
      "Epoch 242/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 374.1904 - val_loss: 178.8081\n",
      "Epoch 243/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 368.0567 - val_loss: 174.5494\n",
      "Epoch 244/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 361.9992 - val_loss: 170.3588\n",
      "Epoch 245/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 356.0174 - val_loss: 166.2367\n",
      "Epoch 246/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 350.1108 - val_loss: 162.1816\n",
      "Epoch 247/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 344.2788 - val_loss: 158.1931\n",
      "Epoch 248/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 338.5207 - val_loss: 154.2709\n",
      "Epoch 249/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 332.8359 - val_loss: 150.4141\n",
      "Epoch 250/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 327.2243 - val_loss: 146.6226\n",
      "Epoch 251/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 321.6848 - val_loss: 142.8954\n",
      "Epoch 252/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 316.2171 - val_loss: 139.2318\n",
      "Epoch 253/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 310.8205 - val_loss: 135.6313\n",
      "Epoch 254/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 305.4945 - val_loss: 132.0937\n",
      "Epoch 255/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 300.2387 - val_loss: 128.6181\n",
      "Epoch 256/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 295.0524 - val_loss: 125.2042\n",
      "Epoch 257/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 289.9350 - val_loss: 121.8508\n",
      "Epoch 258/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 284.8859 - val_loss: 118.5582\n",
      "Epoch 259/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 279.9045 - val_loss: 115.3248\n",
      "Epoch 260/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 274.9904 - val_loss: 112.1509\n",
      "Epoch 261/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 270.1432 - val_loss: 109.0355\n",
      "Epoch 262/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 265.3621 - val_loss: 105.9787\n",
      "Epoch 263/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 260.6466 - val_loss: 102.9790\n",
      "Epoch 264/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 255.9964 - val_loss: 100.0360\n",
      "Epoch 265/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 251.4105 - val_loss: 97.1498\n",
      "Epoch 266/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 246.8887 - val_loss: 94.3192\n",
      "Epoch 267/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 242.4303 - val_loss: 91.5436\n",
      "Epoch 268/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 238.0347 - val_loss: 88.8227\n",
      "Epoch 269/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 233.7013 - val_loss: 86.1561\n",
      "Epoch 270/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 229.4297 - val_loss: 83.5427\n",
      "Epoch 271/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 225.2194 - val_loss: 80.9823\n",
      "Epoch 272/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 221.0696 - val_loss: 78.4743\n",
      "Epoch 273/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 216.9798 - val_loss: 76.0178\n",
      "Epoch 274/500\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 212.9497 - val_loss: 73.6129\n",
      "Epoch 275/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 208.9786 - val_loss: 71.2584\n",
      "Epoch 276/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 205.0659 - val_loss: 68.9544\n",
      "Epoch 277/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 201.2114 - val_loss: 66.6997\n",
      "Epoch 278/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 197.4140 - val_loss: 64.4940\n",
      "Epoch 279/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 193.6734 - val_loss: 62.3366\n",
      "Epoch 280/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 189.9893 - val_loss: 60.2273\n",
      "Epoch 281/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 186.3611 - val_loss: 58.1655\n",
      "Epoch 282/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 182.7883 - val_loss: 56.1502\n",
      "Epoch 283/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 179.2700 - val_loss: 54.1813\n",
      "Epoch 284/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 175.8058 - val_loss: 52.2581\n",
      "Epoch 285/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 172.3953 - val_loss: 50.3798\n",
      "Epoch 286/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 169.0380 - val_loss: 48.5463\n",
      "Epoch 287/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 165.7333 - val_loss: 46.7568\n",
      "Epoch 288/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 162.4805 - val_loss: 45.0108\n",
      "Epoch 289/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 159.2793 - val_loss: 43.3078\n",
      "Epoch 290/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 156.1290 - val_loss: 41.6470\n",
      "Epoch 291/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 153.0293 - val_loss: 40.0281\n",
      "Epoch 292/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 149.9795 - val_loss: 38.4508\n",
      "Epoch 293/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 146.9791 - val_loss: 36.9141\n",
      "Epoch 294/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 144.0277 - val_loss: 35.4177\n",
      "Epoch 295/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 141.1246 - val_loss: 33.9610\n",
      "Epoch 296/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 138.2693 - val_loss: 32.5433\n",
      "Epoch 297/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 135.4614 - val_loss: 31.1644\n",
      "Epoch 298/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 132.7004 - val_loss: 29.8239\n",
      "Epoch 299/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 129.9856 - val_loss: 28.5206\n",
      "Epoch 300/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 127.3167 - val_loss: 27.2547\n",
      "Epoch 301/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 124.6931 - val_loss: 26.0253\n",
      "Epoch 302/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 122.1145 - val_loss: 24.8319\n",
      "Epoch 303/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 119.5800 - val_loss: 23.6740\n",
      "Epoch 304/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 117.0892 - val_loss: 22.5512\n",
      "Epoch 305/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 114.6418 - val_loss: 21.4627\n",
      "Epoch 306/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 112.2371 - val_loss: 20.4083\n",
      "Epoch 307/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 109.8747 - val_loss: 19.3874\n",
      "Epoch 308/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 107.5542 - val_loss: 18.3996\n",
      "Epoch 309/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 105.2750 - val_loss: 17.4440\n",
      "Epoch 310/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 103.0365 - val_loss: 16.5204\n",
      "Epoch 311/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 100.8382 - val_loss: 15.6284\n",
      "Epoch 312/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 98.6800 - val_loss: 14.7673\n",
      "Epoch 313/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 96.5612 - val_loss: 13.9366\n",
      "Epoch 314/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 94.4810 - val_loss: 13.1359\n",
      "Epoch 315/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 92.4392 - val_loss: 12.3648\n",
      "Epoch 316/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 90.4355 - val_loss: 11.6225\n",
      "Epoch 317/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 88.4689 - val_loss: 10.9086\n",
      "Epoch 318/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 86.5392 - val_loss: 10.2230\n",
      "Epoch 319/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 84.6460 - val_loss: 9.5646\n",
      "Epoch 320/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 82.7888 - val_loss: 8.9335\n",
      "Epoch 321/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 80.9671 - val_loss: 8.3289\n",
      "Epoch 322/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 79.1805 - val_loss: 7.7503\n",
      "Epoch 323/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 77.4284 - val_loss: 7.1974\n",
      "Epoch 324/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 75.7104 - val_loss: 6.6698\n",
      "Epoch 325/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 74.0261 - val_loss: 6.1667\n",
      "Epoch 326/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 72.3750 - val_loss: 5.6880\n",
      "Epoch 327/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 70.7566 - val_loss: 5.2329\n",
      "Epoch 328/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 69.1704 - val_loss: 4.8012\n",
      "Epoch 329/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 67.6160 - val_loss: 4.3922\n",
      "Epoch 330/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 66.0930 - val_loss: 4.0058\n",
      "Epoch 331/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 64.6010 - val_loss: 3.6414\n",
      "Epoch 332/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 63.1395 - val_loss: 3.2984\n",
      "Epoch 333/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 61.7080 - val_loss: 2.9765\n",
      "Epoch 334/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 60.3060 - val_loss: 2.6752\n",
      "Epoch 335/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 58.9333 - val_loss: 2.3942\n",
      "Epoch 336/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 57.5894 - val_loss: 2.1329\n",
      "Epoch 337/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 56.2739 - val_loss: 1.8910\n",
      "Epoch 338/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 54.9860 - val_loss: 1.6680\n",
      "Epoch 339/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 53.7257 - val_loss: 1.4635\n",
      "Epoch 340/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 52.4923 - val_loss: 1.2771\n",
      "Epoch 341/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 51.2856 - val_loss: 1.1083\n",
      "Epoch 342/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 50.1051 - val_loss: 0.9568\n",
      "Epoch 343/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 48.9504 - val_loss: 0.8222\n",
      "Epoch 344/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 47.8211 - val_loss: 0.7040\n",
      "Epoch 345/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 46.7167 - val_loss: 0.6019\n",
      "Epoch 346/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 45.6370 - val_loss: 0.5154\n",
      "Epoch 347/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 44.5814 - val_loss: 0.4442\n",
      "Epoch 348/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 43.5496 - val_loss: 0.3879\n",
      "Epoch 349/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 42.5412 - val_loss: 0.3461\n",
      "Epoch 350/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 41.5558 - val_loss: 0.3184\n",
      "Epoch 351/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 40.5930 - val_loss: 0.3044\n",
      "Epoch 352/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 39.6524 - val_loss: 0.3038\n",
      "Epoch 353/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 38.7337 - val_loss: 0.3161\n",
      "Epoch 354/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 37.8365 - val_loss: 0.3411\n",
      "Epoch 355/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 36.9604 - val_loss: 0.3784\n",
      "Epoch 356/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 36.1050 - val_loss: 0.4276\n",
      "Epoch 357/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 35.2700 - val_loss: 0.4883\n",
      "Epoch 358/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 34.4549 - val_loss: 0.5602\n",
      "Epoch 359/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 33.6596 - val_loss: 0.6431\n",
      "Epoch 360/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 32.8835 - val_loss: 0.7364\n",
      "Epoch 361/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 32.1263 - val_loss: 0.8399\n",
      "Epoch 362/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 31.3877 - val_loss: 0.9533\n",
      "Epoch 363/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 30.6674 - val_loss: 1.0762\n",
      "Epoch 364/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 29.9649 - val_loss: 1.2084\n",
      "Epoch 365/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 29.2799 - val_loss: 1.3494\n",
      "Epoch 366/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 28.6122 - val_loss: 1.4990\n",
      "Epoch 367/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 27.9614 - val_loss: 1.6569\n",
      "Epoch 368/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 27.3273 - val_loss: 1.8227\n",
      "Epoch 369/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 26.7093 - val_loss: 1.9962\n",
      "Epoch 370/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 26.1074 - val_loss: 2.1770\n",
      "Epoch 371/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 25.5210 - val_loss: 2.3649\n",
      "Epoch 372/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 24.9499 - val_loss: 2.5595\n",
      "Epoch 373/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 24.3937 - val_loss: 2.7607\n",
      "Epoch 374/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 23.8521 - val_loss: 2.9682\n",
      "Epoch 375/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 23.3249 - val_loss: 3.1815\n",
      "Epoch 376/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 22.8119 - val_loss: 3.4004\n",
      "Epoch 377/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 22.3127 - val_loss: 3.6248\n",
      "Epoch 378/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 21.8270 - val_loss: 3.8544\n",
      "Epoch 379/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 21.3544 - val_loss: 4.0889\n",
      "Epoch 380/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 20.8948 - val_loss: 4.3279\n",
      "Epoch 381/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 20.4478 - val_loss: 4.5714\n",
      "Epoch 382/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 20.0132 - val_loss: 4.8191\n",
      "Epoch 383/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 19.5907 - val_loss: 5.0704\n",
      "Epoch 384/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 19.1801 - val_loss: 5.3256\n",
      "Epoch 385/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 18.7810 - val_loss: 5.5842\n",
      "Epoch 386/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 18.3932 - val_loss: 5.8461\n",
      "Epoch 387/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 18.0164 - val_loss: 6.1108\n",
      "Epoch 388/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 17.6506 - val_loss: 6.3784\n",
      "Epoch 389/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 17.2952 - val_loss: 6.6485\n",
      "Epoch 390/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 16.9501 - val_loss: 6.9211\n",
      "Epoch 391/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 16.6151 - val_loss: 7.1957\n",
      "Epoch 392/500\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 16.2899 - val_loss: 7.4723\n",
      "Epoch 393/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 15.9743 - val_loss: 7.7507\n",
      "Epoch 394/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 15.6681 - val_loss: 8.0307\n",
      "Epoch 395/500\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 15.3711 - val_loss: 8.3119\n",
      "Epoch 396/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 15.0829 - val_loss: 8.5944\n",
      "Epoch 397/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 14.8034 - val_loss: 8.8780\n",
      "Epoch 398/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 14.5325 - val_loss: 9.1624\n",
      "Epoch 399/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 14.2698 - val_loss: 9.4474\n",
      "Epoch 400/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 14.0151 - val_loss: 9.7330\n",
      "Epoch 401/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 13.7684 - val_loss: 10.0189\n",
      "Epoch 402/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 13.5293 - val_loss: 10.3051\n",
      "Epoch 403/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 13.2976 - val_loss: 10.5913\n",
      "Epoch 404/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 13.0733 - val_loss: 10.8773\n",
      "Epoch 405/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 12.8560 - val_loss: 11.1632\n",
      "Epoch 406/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 12.6456 - val_loss: 11.4485\n",
      "Epoch 407/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 12.4420 - val_loss: 11.7336\n",
      "Epoch 408/500\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 12.2448 - val_loss: 12.0179\n",
      "Epoch 409/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 12.0540 - val_loss: 12.3015\n",
      "Epoch 410/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 11.8694 - val_loss: 12.5842\n",
      "Epoch 411/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 11.6908 - val_loss: 12.8657\n",
      "Epoch 412/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 11.5181 - val_loss: 13.1462\n",
      "Epoch 413/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 11.3511 - val_loss: 13.4255\n",
      "Epoch 414/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 11.1896 - val_loss: 13.7035\n",
      "Epoch 415/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 11.0334 - val_loss: 13.9800\n",
      "Epoch 416/500\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 10.8825 - val_loss: 14.2550\n",
      "Epoch 417/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 10.7366 - val_loss: 14.5282\n",
      "Epoch 418/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 10.5957 - val_loss: 14.7999\n",
      "Epoch 419/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 10.4596 - val_loss: 15.0696\n",
      "Epoch 420/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 10.3281 - val_loss: 15.3376\n",
      "Epoch 421/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 10.2010 - val_loss: 15.6035\n",
      "Epoch 422/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 10.0784 - val_loss: 15.8674\n",
      "Epoch 423/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 9.9600 - val_loss: 16.1292\n",
      "Epoch 424/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 9.8457 - val_loss: 16.3886\n",
      "Epoch 425/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 9.7355 - val_loss: 16.6459\n",
      "Epoch 426/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 9.6290 - val_loss: 16.9008\n",
      "Epoch 427/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 9.5264 - val_loss: 17.1532\n",
      "Epoch 428/500\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 9.4274 - val_loss: 17.4033\n",
      "Epoch 429/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 9.3319 - val_loss: 17.6508\n",
      "Epoch 430/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 9.2398 - val_loss: 17.8956\n",
      "Epoch 431/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 9.1511 - val_loss: 18.1381\n",
      "Epoch 432/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 9.0655 - val_loss: 18.3778\n",
      "Epoch 433/500\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 8.9830 - val_loss: 18.6148\n",
      "Epoch 434/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 8.9035 - val_loss: 18.8491\n",
      "Epoch 435/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 8.8269 - val_loss: 19.0804\n",
      "Epoch 436/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 8.7532 - val_loss: 19.3093\n",
      "Epoch 437/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 8.6821 - val_loss: 19.5348\n",
      "Epoch 438/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 8.6137 - val_loss: 19.7577\n",
      "Epoch 439/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 8.5479 - val_loss: 19.9781\n",
      "Epoch 440/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 8.4844 - val_loss: 20.1951\n",
      "Epoch 441/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 8.4234 - val_loss: 20.4093\n",
      "Epoch 442/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 8.3646 - val_loss: 20.6206\n",
      "Epoch 443/500\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 8.3080 - val_loss: 20.8289\n",
      "Epoch 444/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 8.2536 - val_loss: 21.0341\n",
      "Epoch 445/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 8.2013 - val_loss: 21.2363\n",
      "Epoch 446/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 8.1509 - val_loss: 21.4356\n",
      "Epoch 447/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 8.1025 - val_loss: 21.6319\n",
      "Epoch 448/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 8.0559 - val_loss: 21.8252\n",
      "Epoch 449/500\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 8.0111 - val_loss: 22.0155\n",
      "Epoch 450/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 7.9681 - val_loss: 22.2027\n",
      "Epoch 451/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 7.9266 - val_loss: 22.3869\n",
      "Epoch 452/500\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 7.8869 - val_loss: 22.5679\n",
      "Epoch 453/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 7.8487 - val_loss: 22.7459\n",
      "Epoch 454/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 7.8119 - val_loss: 22.9210\n",
      "Epoch 455/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 7.7766 - val_loss: 23.0932\n",
      "Epoch 456/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 7.7427 - val_loss: 23.2620\n",
      "Epoch 457/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 7.7102 - val_loss: 23.4280\n",
      "Epoch 458/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 7.6789 - val_loss: 23.5912\n",
      "Epoch 459/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 7.6488 - val_loss: 23.7514\n",
      "Epoch 460/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 7.6200 - val_loss: 23.9086\n",
      "Epoch 461/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 7.5923 - val_loss: 24.0627\n",
      "Epoch 462/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 7.5657 - val_loss: 24.2140\n",
      "Epoch 463/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 7.5401 - val_loss: 24.3623\n",
      "Epoch 464/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 7.5156 - val_loss: 24.5079\n",
      "Epoch 465/500\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 7.4921 - val_loss: 24.6506\n",
      "Epoch 466/500\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 7.4695 - val_loss: 24.7904\n",
      "Epoch 467/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 7.4478 - val_loss: 24.9274\n",
      "Epoch 468/500\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 7.4270 - val_loss: 25.0617\n",
      "Epoch 469/500\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 7.4071 - val_loss: 25.1932\n",
      "Epoch 470/500\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 7.3879 - val_loss: 25.3218\n",
      "Epoch 471/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 7.3695 - val_loss: 25.4479\n",
      "Epoch 472/500\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 7.3519 - val_loss: 25.5712\n",
      "Epoch 473/500\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 7.3350 - val_loss: 25.6917\n",
      "Epoch 474/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 7.3188 - val_loss: 25.8098\n",
      "Epoch 475/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 7.3033 - val_loss: 25.9253\n",
      "Epoch 476/500\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 7.2883 - val_loss: 26.0380\n",
      "Epoch 477/500\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 7.2740 - val_loss: 26.1483\n",
      "Epoch 478/500\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 7.2603 - val_loss: 26.2562\n",
      "Epoch 479/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 7.2471 - val_loss: 26.3616\n",
      "Epoch 480/500\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 7.2345 - val_loss: 26.4644\n",
      "Epoch 481/500\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 7.2224 - val_loss: 26.5648\n",
      "Epoch 482/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 7.2108 - val_loss: 26.6629\n",
      "Epoch 483/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 7.1997 - val_loss: 26.7586\n",
      "Epoch 484/500\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 7.1890 - val_loss: 26.8521\n",
      "Epoch 485/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 7.1788 - val_loss: 26.9431\n",
      "Epoch 486/500\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 7.1690 - val_loss: 27.0323\n",
      "Epoch 487/500\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 7.1596 - val_loss: 27.1189\n",
      "Epoch 488/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 7.1506 - val_loss: 27.2034\n",
      "Epoch 489/500\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 7.1419 - val_loss: 27.2857\n",
      "Epoch 490/500\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 7.1337 - val_loss: 27.3662\n",
      "Epoch 491/500\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 7.1257 - val_loss: 27.4444\n",
      "Epoch 492/500\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 7.1181 - val_loss: 27.5208\n",
      "Epoch 493/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 7.1108 - val_loss: 27.5950\n",
      "Epoch 494/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 7.1038 - val_loss: 27.6672\n",
      "Epoch 495/500\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 7.0971 - val_loss: 27.7377\n",
      "Epoch 496/500\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 7.0906 - val_loss: 27.8060\n",
      "Epoch 497/500\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 7.0845 - val_loss: 27.8726\n",
      "Epoch 498/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 7.0786 - val_loss: 27.9374\n",
      "Epoch 499/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 7.0729 - val_loss: 28.0004\n",
      "Epoch 500/500\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 7.0675 - val_loss: 28.0617\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.6856, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0008, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0002, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0128, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(-2.3003, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 811ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[73.11589636, 73.07387955, 73.03186275, 72.98984594, 72.94782913,\n",
       "        72.90581232, 72.86379552, 72.82177871, 72.7797619 , 72.7377451 ,\n",
       "        72.69572829, 72.65371148, 72.61169468, 72.56967787, 72.52766106,\n",
       "        72.48277311, 72.43235294, 72.38193277, 72.33151261, 72.28109244,\n",
       "        72.23067227, 72.1802521 , 72.12983193, 72.07941176, 72.0289916 ,\n",
       "        71.97857143, 71.92815126, 71.87773109, 71.82731092, 71.77689076,\n",
       "        71.72647059, 71.67605042, 71.62563025, 71.57521008, 71.52478992,\n",
       "        71.47436975, 71.42394958, 71.37352941, 71.32310924, 71.27268908,\n",
       "        71.22226891, 71.17184874, 71.12142857, 71.0710084 , 71.02058824,\n",
       "        70.97016807, 70.9197479 , 70.86932773, 70.81890756, 70.76848739,\n",
       "        70.71806723, 70.69460784, 70.68620448, 70.67780112, 70.66939776,\n",
       "        70.6609944 , 70.65259104, 70.64418768, 70.63578431, 70.62738095,\n",
       "        70.61897759, 70.61057423, 70.60217087, 70.59376751, 70.58536415,\n",
       "        70.57696078, 70.56855742, 70.56015406, 70.5517507 , 70.54334734,\n",
       "        70.53494398, 70.52654062, 70.51813725, 70.50973389, 70.50133053,\n",
       "        70.49292717, 70.48452381, 70.47612045, 70.46771709, 70.45931373,\n",
       "        76.29936218,  0.        ,  0.        ,  0.65360183,  0.        ,\n",
       "         0.        ,  0.08414401,  0.54807568,  0.        ,  0.31290025,\n",
       "         0.76110649,  0.        ,  0.13340807,  0.20198622,  0.        ,\n",
       "         0.        ,  0.46402621,  0.95588624,  0.24506454,  0.80954486]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([69.89348739, 69.89068627, 69.88788515, 69.88508403, 69.88228291,\n",
       "       69.87948179, 69.87668067, 69.87387955, 69.87107843, 69.86827731,\n",
       "       69.86547619, 69.86267507, 69.85987395, 69.85707283, 69.85427171,\n",
       "       69.85147059, 69.84866947, 69.84586835, 69.84306723, 69.84026611,\n",
       "       69.83746499, 69.83466387, 69.83186275, 69.82906162, 69.8262605 ,\n",
       "       69.82345938, 69.82065826, 69.81785714, 69.81505602, 69.8122549 ,\n",
       "       69.80945378, 69.80665266, 69.80385154, 69.80105042, 69.78832866,\n",
       "       69.76965453, 69.75098039, 69.73230626, 69.71363212, 69.69495798,\n",
       "       69.67628385, 69.65760971, 69.63893557, 69.62026144, 69.6015873 ,\n",
       "       69.58291317, 69.56423903, 69.54556489, 69.52689076, 69.50821662,\n",
       "       69.48954248, 69.47086835, 69.45219421, 69.43352007, 69.41484594,\n",
       "       69.3961718 , 69.37749767, 69.35882353, 69.34014939, 69.32147526,\n",
       "       69.30280112, 69.28412698, 69.26545285, 69.24677871, 69.22810458,\n",
       "       69.20943044, 69.1907563 , 69.17208217, 69.15340803, 69.13473389,\n",
       "       69.11605976, 69.09738562, 69.07871148, 69.06003735, 69.04136321,\n",
       "       69.02268908, 69.00401494, 68.9853408 , 68.96666667, 68.94799253,\n",
       "       68.92931839, 68.91064426, 68.89197012, 68.87329599, 68.85462185,\n",
       "       68.83594771, 68.81727358, 68.79859944, 68.7799253 , 68.76125117,\n",
       "       68.74257703, 68.72390289, 68.70522876, 68.68655462, 68.66788049,\n",
       "       68.64920635, 68.63053221, 68.61185808, 68.59318394, 68.5745098 ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.908079908962126\n",
      "14.59240402120447\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
