{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.140000\n",
       "1       88.911429\n",
       "2       88.682857\n",
       "3       88.454286\n",
       "4       88.225714\n",
       "          ...    \n",
       "1545    66.292717\n",
       "1546    66.285247\n",
       "1547    66.277778\n",
       "1548    66.270308\n",
       "1549    66.262838\n",
       "Name: C5, Length: 1550, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c5_interpolated_1450_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.140000\n",
       "1       88.911429\n",
       "2       88.682857\n",
       "3       88.454286\n",
       "4       88.225714\n",
       "          ...    \n",
       "1445     0.000000\n",
       "1446     0.000000\n",
       "1447     0.000000\n",
       "1448     0.137733\n",
       "1449     0.000000\n",
       "Name: C5, Length: 1450, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1450)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.140000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.911429</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.682857</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.454286</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.225714</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     89.140000  0.000298\n",
       "1     88.911429  0.000298\n",
       "2     88.682857  0.000297\n",
       "3     88.454286  0.000297\n",
       "4     88.225714  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.8407  # Value for C0\n",
    "K0 = -0.0015  # Value for K0\n",
    "K1 = -0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0118    # Value for b\n",
    "c = 2.5775    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    89.140000    88.911429    88.682857    88.454286    88.225714   \n",
      "351    88.911429    88.682857    88.454286    88.225714    87.997143   \n",
      "352    88.682857    88.454286    88.225714    87.997143    87.768571   \n",
      "353    88.454286    88.225714    87.997143    87.768571    87.540000   \n",
      "354    88.225714    87.997143    87.768571    87.540000    87.311429   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.997143    87.768571    87.540000    87.311429    87.095798  ...   \n",
      "351    87.768571    87.540000    87.311429    87.095798    87.039776  ...   \n",
      "352    87.540000    87.311429    87.095798    87.039776    86.983754  ...   \n",
      "353    87.311429    87.095798    87.039776    86.983754    86.927731  ...   \n",
      "354    87.095798    87.039776    86.983754    86.927731    86.871709  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   79.161181    0.000263   79.132236    0.000263   79.103291    0.000263   \n",
      "351   79.132236    0.000263   79.103291    0.000263   79.074346    0.000262   \n",
      "352   79.103291    0.000263   79.074346    0.000262   79.045401    0.000262   \n",
      "353   79.074346    0.000262   79.045401    0.000262   79.016457    0.000262   \n",
      "354   79.045401    0.000262   79.016457    0.000262   78.987512    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   79.074346    0.000262   79.045401    0.000262  \n",
      "351   79.045401    0.000262   79.016457    0.000262  \n",
      "352   79.016457    0.000262   78.987512    0.000262  \n",
      "353   78.987512    0.000262   78.958567    0.000262  \n",
      "354   78.958567    0.000262   78.929622    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1, 251) (1000, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "13/13 [==============================] - 2s 38ms/step - loss: 5586.1465 - val_loss: 4644.7246\n",
      "Epoch 2/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 5507.9233 - val_loss: 4588.4692\n",
      "Epoch 3/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5460.2832 - val_loss: 4547.4829\n",
      "Epoch 4/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 5415.6475 - val_loss: 4506.7896\n",
      "Epoch 5/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5371.3550 - val_loss: 4466.4478\n",
      "Epoch 6/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5317.5840 - val_loss: 4409.8887\n",
      "Epoch 7/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5264.6475 - val_loss: 4367.8955\n",
      "Epoch 8/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 5218.9102 - val_loss: 4326.3672\n",
      "Epoch 9/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5173.7061 - val_loss: 4285.3477\n",
      "Epoch 10/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5129.0137 - val_loss: 4244.7764\n",
      "Epoch 11/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 5084.7705 - val_loss: 4204.6055\n",
      "Epoch 12/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5040.9292 - val_loss: 4164.8008\n",
      "Epoch 13/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 4997.4575 - val_loss: 4125.3359\n",
      "Epoch 14/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4954.3325 - val_loss: 4086.1946\n",
      "Epoch 15/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4911.5386 - val_loss: 4047.3643\n",
      "Epoch 16/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4869.0640 - val_loss: 4008.8359\n",
      "Epoch 17/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4826.8979 - val_loss: 3970.6018\n",
      "Epoch 18/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4785.0332 - val_loss: 3932.6550\n",
      "Epoch 19/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 4743.4644 - val_loss: 3894.9910\n",
      "Epoch 20/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 4702.1860 - val_loss: 3857.6047\n",
      "Epoch 21/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4661.1934 - val_loss: 3820.4932\n",
      "Epoch 22/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4620.4839 - val_loss: 3783.6526\n",
      "Epoch 23/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4580.0527 - val_loss: 3747.0789\n",
      "Epoch 24/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4539.8965 - val_loss: 3710.7712\n",
      "Epoch 25/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4500.0137 - val_loss: 3674.7256\n",
      "Epoch 26/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4460.4004 - val_loss: 3638.9404\n",
      "Epoch 27/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4421.0552 - val_loss: 3603.4131\n",
      "Epoch 28/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4381.9756 - val_loss: 3568.1406\n",
      "Epoch 29/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4343.1597 - val_loss: 3533.1230\n",
      "Epoch 30/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4304.6045 - val_loss: 3498.3574\n",
      "Epoch 31/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4266.3086 - val_loss: 3463.8408\n",
      "Epoch 32/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4228.2705 - val_loss: 3429.5732\n",
      "Epoch 33/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4190.4883 - val_loss: 3395.5525\n",
      "Epoch 34/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4152.9600 - val_loss: 3361.7766\n",
      "Epoch 35/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4115.6846 - val_loss: 3328.2441\n",
      "Epoch 36/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4078.6594 - val_loss: 3294.9543\n",
      "Epoch 37/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 4041.8840 - val_loss: 3261.9043\n",
      "Epoch 38/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4005.3569 - val_loss: 3229.0938\n",
      "Epoch 39/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3969.0750 - val_loss: 3196.5203\n",
      "Epoch 40/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3933.0393 - val_loss: 3164.1838\n",
      "Epoch 41/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3897.2463 - val_loss: 3132.0815\n",
      "Epoch 42/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3861.6953 - val_loss: 3100.2131\n",
      "Epoch 43/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3826.3853 - val_loss: 3068.5774\n",
      "Epoch 44/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3791.3149 - val_loss: 3037.1721\n",
      "Epoch 45/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3756.4822 - val_loss: 3005.9971\n",
      "Epoch 46/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3721.8865 - val_loss: 2975.0498\n",
      "Epoch 47/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3687.5266 - val_loss: 2944.3293\n",
      "Epoch 48/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3653.4004 - val_loss: 2913.8357\n",
      "Epoch 49/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3619.5078 - val_loss: 2883.5657\n",
      "Epoch 50/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3585.8462 - val_loss: 2853.5203\n",
      "Epoch 51/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3552.4160 - val_loss: 2823.6960\n",
      "Epoch 52/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3519.2146 - val_loss: 2794.0935\n",
      "Epoch 53/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3486.2412 - val_loss: 2764.7109\n",
      "Epoch 54/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3453.4954 - val_loss: 2735.5476\n",
      "Epoch 55/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3420.9751 - val_loss: 2706.6006\n",
      "Epoch 56/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3388.6790 - val_loss: 2677.8706\n",
      "Epoch 57/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3356.6069 - val_loss: 2649.3564\n",
      "Epoch 58/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3324.7568 - val_loss: 2621.0562\n",
      "Epoch 59/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3293.1282 - val_loss: 2592.9697\n",
      "Epoch 60/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3261.7200 - val_loss: 2565.0945\n",
      "Epoch 61/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3230.5305 - val_loss: 2537.4307\n",
      "Epoch 62/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3199.5593 - val_loss: 2509.9766\n",
      "Epoch 63/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3168.8044 - val_loss: 2482.7314\n",
      "Epoch 64/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3138.2654 - val_loss: 2455.6943\n",
      "Epoch 65/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3107.9407 - val_loss: 2428.8625\n",
      "Epoch 66/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3077.8306 - val_loss: 2402.2375\n",
      "Epoch 67/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3047.9321 - val_loss: 2375.8171\n",
      "Epoch 68/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3018.2458 - val_loss: 2349.6001\n",
      "Epoch 69/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2988.7698 - val_loss: 2323.5857\n",
      "Epoch 70/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2959.5022 - val_loss: 2297.7725\n",
      "Epoch 71/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 2930.4443 - val_loss: 2272.1599\n",
      "Epoch 72/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2901.5930 - val_loss: 2246.7466\n",
      "Epoch 73/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2872.9487 - val_loss: 2221.5322\n",
      "Epoch 74/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2844.5090 - val_loss: 2196.5154\n",
      "Epoch 75/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2816.2742 - val_loss: 2171.6938\n",
      "Epoch 76/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2788.2424 - val_loss: 2147.0688\n",
      "Epoch 77/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2760.4131 - val_loss: 2122.6377\n",
      "Epoch 78/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2732.7847 - val_loss: 2098.4009\n",
      "Epoch 79/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2705.3572 - val_loss: 2074.3562\n",
      "Epoch 80/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2678.1287 - val_loss: 2050.5020\n",
      "Epoch 81/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2651.0986 - val_loss: 2026.8395\n",
      "Epoch 82/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2624.2664 - val_loss: 2003.3667\n",
      "Epoch 83/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2597.6299 - val_loss: 1980.0824\n",
      "Epoch 84/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2571.1897 - val_loss: 1956.9860\n",
      "Epoch 85/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2544.9436 - val_loss: 1934.0759\n",
      "Epoch 86/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2518.8914 - val_loss: 1911.3519\n",
      "Epoch 87/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2493.0315 - val_loss: 1888.8126\n",
      "Epoch 88/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2467.3638 - val_loss: 1866.4574\n",
      "Epoch 89/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2441.8865 - val_loss: 1844.2853\n",
      "Epoch 90/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2416.5996 - val_loss: 1822.2959\n",
      "Epoch 91/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2391.5010 - val_loss: 1800.4871\n",
      "Epoch 92/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2366.5906 - val_loss: 1778.8586\n",
      "Epoch 93/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2341.8672 - val_loss: 1757.4097\n",
      "Epoch 94/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2317.3306 - val_loss: 1736.1395\n",
      "Epoch 95/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2292.9785 - val_loss: 1715.0465\n",
      "Epoch 96/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2268.8115 - val_loss: 1694.1301\n",
      "Epoch 97/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2244.8279 - val_loss: 1673.3899\n",
      "Epoch 98/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2221.0266 - val_loss: 1652.8239\n",
      "Epoch 99/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2197.4070 - val_loss: 1632.4325\n",
      "Epoch 100/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2173.9690 - val_loss: 1612.2144\n",
      "Epoch 101/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2150.7107 - val_loss: 1592.1691\n",
      "Epoch 102/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 2127.6311 - val_loss: 1572.2944\n",
      "Epoch 103/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2104.7300 - val_loss: 1552.5900\n",
      "Epoch 104/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2082.0061 - val_loss: 1533.0563\n",
      "Epoch 105/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2059.4590 - val_loss: 1513.6903\n",
      "Epoch 106/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2037.0869 - val_loss: 1494.4923\n",
      "Epoch 107/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2014.8903 - val_loss: 1475.4617\n",
      "Epoch 108/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1992.8672 - val_loss: 1456.5983\n",
      "Epoch 109/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1971.0175 - val_loss: 1437.8990\n",
      "Epoch 110/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1949.3397 - val_loss: 1419.3649\n",
      "Epoch 111/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1927.8336 - val_loss: 1400.9939\n",
      "Epoch 112/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1906.4979 - val_loss: 1382.7867\n",
      "Epoch 113/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1885.3320 - val_loss: 1364.7405\n",
      "Epoch 114/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1864.3342 - val_loss: 1346.8558\n",
      "Epoch 115/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1843.5051 - val_loss: 1329.1316\n",
      "Epoch 116/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1822.8430 - val_loss: 1311.5664\n",
      "Epoch 117/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1802.3473 - val_loss: 1294.1606\n",
      "Epoch 118/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1782.0172 - val_loss: 1276.9122\n",
      "Epoch 119/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1761.8517 - val_loss: 1259.8210\n",
      "Epoch 120/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1741.8505 - val_loss: 1242.8861\n",
      "Epoch 121/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1722.0117 - val_loss: 1226.1067\n",
      "Epoch 122/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1702.3358 - val_loss: 1209.4814\n",
      "Epoch 123/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1682.8213 - val_loss: 1193.0105\n",
      "Epoch 124/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1663.4677 - val_loss: 1176.6924\n",
      "Epoch 125/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1644.2738 - val_loss: 1160.5264\n",
      "Epoch 126/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1625.2388 - val_loss: 1144.5118\n",
      "Epoch 127/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1606.3627 - val_loss: 1128.6483\n",
      "Epoch 128/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1587.6434 - val_loss: 1112.9340\n",
      "Epoch 129/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1569.0811 - val_loss: 1097.3690\n",
      "Epoch 130/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1550.6748 - val_loss: 1081.9519\n",
      "Epoch 131/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 1532.4237 - val_loss: 1066.6827\n",
      "Epoch 132/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1514.3269 - val_loss: 1051.5597\n",
      "Epoch 133/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1496.3838 - val_loss: 1036.5831\n",
      "Epoch 134/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1478.5938 - val_loss: 1021.7508\n",
      "Epoch 135/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1460.9552 - val_loss: 1007.0641\n",
      "Epoch 136/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1443.4681 - val_loss: 992.5192\n",
      "Epoch 137/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1426.1313 - val_loss: 978.1177\n",
      "Epoch 138/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1408.9446 - val_loss: 963.8585\n",
      "Epoch 139/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1391.9066 - val_loss: 949.7400\n",
      "Epoch 140/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1375.0168 - val_loss: 935.7621\n",
      "Epoch 141/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1358.2747 - val_loss: 921.9238\n",
      "Epoch 142/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1341.6787 - val_loss: 908.2243\n",
      "Epoch 143/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1325.2292 - val_loss: 894.6633\n",
      "Epoch 144/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1308.9249 - val_loss: 881.2396\n",
      "Epoch 145/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1292.7651 - val_loss: 867.9526\n",
      "Epoch 146/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1276.7488 - val_loss: 854.8011\n",
      "Epoch 147/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1260.8752 - val_loss: 841.7848\n",
      "Epoch 148/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1245.1438 - val_loss: 828.9027\n",
      "Epoch 149/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1229.5540 - val_loss: 816.1543\n",
      "Epoch 150/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1214.1050 - val_loss: 803.5390\n",
      "Epoch 151/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1198.7957 - val_loss: 791.0559\n",
      "Epoch 152/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1183.6260 - val_loss: 778.7029\n",
      "Epoch 153/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1168.5945 - val_loss: 766.4819\n",
      "Epoch 154/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1153.7007 - val_loss: 754.3904\n",
      "Epoch 155/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1138.9441 - val_loss: 742.4274\n",
      "Epoch 156/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1124.3235 - val_loss: 730.5930\n",
      "Epoch 157/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1109.8386 - val_loss: 718.8862\n",
      "Epoch 158/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1095.4885 - val_loss: 707.3067\n",
      "Epoch 159/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1081.2723 - val_loss: 695.8530\n",
      "Epoch 160/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 1067.1897 - val_loss: 684.5248\n",
      "Epoch 161/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1053.2397 - val_loss: 673.3210\n",
      "Epoch 162/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1039.4218 - val_loss: 662.2415\n",
      "Epoch 163/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1025.7349 - val_loss: 651.2847\n",
      "Epoch 164/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1012.1785 - val_loss: 640.4508\n",
      "Epoch 165/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 998.7519 - val_loss: 629.7380\n",
      "Epoch 166/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 985.4542 - val_loss: 619.1464\n",
      "Epoch 167/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 972.2851 - val_loss: 608.6760\n",
      "Epoch 168/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 959.2438 - val_loss: 598.3246\n",
      "Epoch 169/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 946.3290 - val_loss: 588.0918\n",
      "Epoch 170/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 933.5405 - val_loss: 577.9775\n",
      "Epoch 171/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 920.8777 - val_loss: 567.9798\n",
      "Epoch 172/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 908.3392 - val_loss: 558.0991\n",
      "Epoch 173/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 895.9250 - val_loss: 548.3348\n",
      "Epoch 174/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 883.6346 - val_loss: 538.6856\n",
      "Epoch 175/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 871.4665 - val_loss: 529.1506\n",
      "Epoch 176/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 859.4204 - val_loss: 519.7296\n",
      "Epoch 177/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 847.4955 - val_loss: 510.4212\n",
      "Epoch 178/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 835.6914 - val_loss: 501.2256\n",
      "Epoch 179/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 824.0071 - val_loss: 492.1417\n",
      "Epoch 180/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 812.4419 - val_loss: 483.1685\n",
      "Epoch 181/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 800.9954 - val_loss: 474.3057\n",
      "Epoch 182/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 789.6665 - val_loss: 465.5525\n",
      "Epoch 183/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 778.4548 - val_loss: 456.9080\n",
      "Epoch 184/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 767.3594 - val_loss: 448.3710\n",
      "Epoch 185/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 756.3798 - val_loss: 439.9427\n",
      "Epoch 186/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 745.5151 - val_loss: 431.6201\n",
      "Epoch 187/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 734.7649 - val_loss: 423.4035\n",
      "Epoch 188/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 724.1284 - val_loss: 415.2923\n",
      "Epoch 189/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 713.6049 - val_loss: 407.2855\n",
      "Epoch 190/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 703.1934 - val_loss: 399.3831\n",
      "Epoch 191/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 692.8937 - val_loss: 391.5836\n",
      "Epoch 192/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 682.7050 - val_loss: 383.8859\n",
      "Epoch 193/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 672.6262 - val_loss: 376.2905\n",
      "Epoch 194/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 662.6571 - val_loss: 368.7959\n",
      "Epoch 195/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 652.7968 - val_loss: 361.4021\n",
      "Epoch 196/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 643.0447 - val_loss: 354.1073\n",
      "Epoch 197/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 633.4000 - val_loss: 346.9117\n",
      "Epoch 198/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 623.8621 - val_loss: 339.8142\n",
      "Epoch 199/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 614.4303 - val_loss: 332.8143\n",
      "Epoch 200/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 605.1041 - val_loss: 325.9115\n",
      "Epoch 201/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 595.8826 - val_loss: 319.1045\n",
      "Epoch 202/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 586.7653 - val_loss: 312.3932\n",
      "Epoch 203/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 577.7512 - val_loss: 305.7762\n",
      "Epoch 204/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 568.8400 - val_loss: 299.2533\n",
      "Epoch 205/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 560.0308 - val_loss: 292.8242\n",
      "Epoch 206/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 551.3231 - val_loss: 286.4873\n",
      "Epoch 207/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 542.7158 - val_loss: 280.2424\n",
      "Epoch 208/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 534.2088 - val_loss: 274.0887\n",
      "Epoch 209/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 525.8010 - val_loss: 268.0259\n",
      "Epoch 210/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 517.4921 - val_loss: 262.0528\n",
      "Epoch 211/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 509.2812 - val_loss: 256.1690\n",
      "Epoch 212/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 501.1677 - val_loss: 250.3737\n",
      "Epoch 213/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 493.1511 - val_loss: 244.6664\n",
      "Epoch 214/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 485.2302 - val_loss: 239.0455\n",
      "Epoch 215/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 477.4047 - val_loss: 233.5115\n",
      "Epoch 216/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 469.6740 - val_loss: 228.0633\n",
      "Epoch 217/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 462.0374 - val_loss: 222.7001\n",
      "Epoch 218/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 454.4941 - val_loss: 217.4210\n",
      "Epoch 219/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 447.0433 - val_loss: 212.2260\n",
      "Epoch 220/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 439.6849 - val_loss: 207.1135\n",
      "Epoch 221/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 432.4177 - val_loss: 202.0835\n",
      "Epoch 222/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 425.2411 - val_loss: 197.1351\n",
      "Epoch 223/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 418.1545 - val_loss: 192.2675\n",
      "Epoch 224/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 411.1575 - val_loss: 187.4803\n",
      "Epoch 225/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 404.2490 - val_loss: 182.7728\n",
      "Epoch 226/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 397.4287 - val_loss: 178.1440\n",
      "Epoch 227/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 390.6960 - val_loss: 173.5931\n",
      "Epoch 228/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 384.0498 - val_loss: 169.1202\n",
      "Epoch 229/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 377.4896 - val_loss: 164.7236\n",
      "Epoch 230/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 371.0152 - val_loss: 160.4037\n",
      "Epoch 231/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 364.6254 - val_loss: 156.1591\n",
      "Epoch 232/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 358.3198 - val_loss: 151.9890\n",
      "Epoch 233/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 352.0974 - val_loss: 147.8932\n",
      "Epoch 234/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 345.9579 - val_loss: 143.8707\n",
      "Epoch 235/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 339.9006 - val_loss: 139.9206\n",
      "Epoch 236/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 333.9247 - val_loss: 136.0432\n",
      "Epoch 237/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 328.0298 - val_loss: 132.2371\n",
      "Epoch 238/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 322.2150 - val_loss: 128.5014\n",
      "Epoch 239/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 316.4798 - val_loss: 124.8360\n",
      "Epoch 240/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 310.8236 - val_loss: 121.2397\n",
      "Epoch 241/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 305.2455 - val_loss: 117.7123\n",
      "Epoch 242/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 299.7451 - val_loss: 114.2529\n",
      "Epoch 243/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 294.3216 - val_loss: 110.8606\n",
      "Epoch 244/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 288.9741 - val_loss: 107.5349\n",
      "Epoch 245/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 283.7023 - val_loss: 104.2751\n",
      "Epoch 246/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 278.5056 - val_loss: 101.0809\n",
      "Epoch 247/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 273.3833 - val_loss: 97.9513\n",
      "Epoch 248/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 268.3347 - val_loss: 94.8855\n",
      "Epoch 249/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 263.3591 - val_loss: 91.8833\n",
      "Epoch 250/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 258.4561 - val_loss: 88.9436\n",
      "Epoch 251/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 253.6249 - val_loss: 86.0658\n",
      "Epoch 252/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 248.8647 - val_loss: 83.2493\n",
      "Epoch 253/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 244.1751 - val_loss: 80.4935\n",
      "Epoch 254/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 239.5551 - val_loss: 77.7976\n",
      "Epoch 255/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 235.0046 - val_loss: 75.1611\n",
      "Epoch 256/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 230.5227 - val_loss: 72.5831\n",
      "Epoch 257/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 226.1085 - val_loss: 70.0634\n",
      "Epoch 258/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 221.7617 - val_loss: 67.6007\n",
      "Epoch 259/500\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 217.4817 - val_loss: 65.1948\n",
      "Epoch 260/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 213.2676 - val_loss: 62.8450\n",
      "Epoch 261/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 209.1191 - val_loss: 60.5502\n",
      "Epoch 262/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 205.0352 - val_loss: 58.3103\n",
      "Epoch 263/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 201.0154 - val_loss: 56.1245\n",
      "Epoch 264/500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 197.0592 - val_loss: 53.9921\n",
      "Epoch 265/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 193.1659 - val_loss: 51.9124\n",
      "Epoch 266/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 189.3349 - val_loss: 49.8845\n",
      "Epoch 267/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 185.5654 - val_loss: 47.9085\n",
      "Epoch 268/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 181.8572 - val_loss: 45.9831\n",
      "Epoch 269/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 178.2092 - val_loss: 44.1078\n",
      "Epoch 270/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 174.6209 - val_loss: 42.2821\n",
      "Epoch 271/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 171.0918 - val_loss: 40.5053\n",
      "Epoch 272/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 167.6212 - val_loss: 38.7764\n",
      "Epoch 273/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 164.2083 - val_loss: 37.0953\n",
      "Epoch 274/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 160.8530 - val_loss: 35.4610\n",
      "Epoch 275/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 157.5542 - val_loss: 33.8732\n",
      "Epoch 276/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 154.3115 - val_loss: 32.3310\n",
      "Epoch 277/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 151.1243 - val_loss: 30.8337\n",
      "Epoch 278/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 147.9919 - val_loss: 29.3810\n",
      "Epoch 279/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 144.9138 - val_loss: 27.9721\n",
      "Epoch 280/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 141.8893 - val_loss: 26.6061\n",
      "Epoch 281/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 138.9179 - val_loss: 25.2829\n",
      "Epoch 282/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 135.9989 - val_loss: 24.0015\n",
      "Epoch 283/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 133.1317 - val_loss: 22.7613\n",
      "Epoch 284/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 130.3156 - val_loss: 21.5619\n",
      "Epoch 285/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 127.5503 - val_loss: 20.4025\n",
      "Epoch 286/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 124.8350 - val_loss: 19.2824\n",
      "Epoch 287/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 122.1690 - val_loss: 18.2013\n",
      "Epoch 288/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 119.5519 - val_loss: 17.1583\n",
      "Epoch 289/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 116.9831 - val_loss: 16.1530\n",
      "Epoch 290/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 114.4620 - val_loss: 15.1846\n",
      "Epoch 291/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 111.9879 - val_loss: 14.2526\n",
      "Epoch 292/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 109.5602 - val_loss: 13.3565\n",
      "Epoch 293/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 107.1785 - val_loss: 12.4955\n",
      "Epoch 294/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 104.8421 - val_loss: 11.6692\n",
      "Epoch 295/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 102.5505 - val_loss: 10.8768\n",
      "Epoch 296/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 100.3031 - val_loss: 10.1178\n",
      "Epoch 297/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 98.0992 - val_loss: 9.3918\n",
      "Epoch 298/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 95.9384 - val_loss: 8.6979\n",
      "Epoch 299/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 93.8200 - val_loss: 8.0358\n",
      "Epoch 300/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 91.7435 - val_loss: 7.4047\n",
      "Epoch 301/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 89.7084 - val_loss: 6.8042\n",
      "Epoch 302/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 87.7140 - val_loss: 6.2336\n",
      "Epoch 303/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 85.7598 - val_loss: 5.6924\n",
      "Epoch 304/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 83.8454 - val_loss: 5.1801\n",
      "Epoch 305/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 81.9700 - val_loss: 4.6960\n",
      "Epoch 306/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 80.1333 - val_loss: 4.2395\n",
      "Epoch 307/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 78.3344 - val_loss: 3.8103\n",
      "Epoch 308/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 76.5730 - val_loss: 3.4077\n",
      "Epoch 309/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 74.8487 - val_loss: 3.0311\n",
      "Epoch 310/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 73.1608 - val_loss: 2.6800\n",
      "Epoch 311/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 71.5088 - val_loss: 2.3539\n",
      "Epoch 312/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 69.8920 - val_loss: 2.0522\n",
      "Epoch 313/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 68.3101 - val_loss: 1.7745\n",
      "Epoch 314/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 66.7624 - val_loss: 1.5200\n",
      "Epoch 315/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 65.2484 - val_loss: 1.2885\n",
      "Epoch 316/500\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 63.7677 - val_loss: 1.0793\n",
      "Epoch 317/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 62.3198 - val_loss: 0.8919\n",
      "Epoch 318/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 60.9040 - val_loss: 0.7258\n",
      "Epoch 319/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 59.5199 - val_loss: 0.5805\n",
      "Epoch 320/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 58.1673 - val_loss: 0.4555\n",
      "Epoch 321/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 56.8452 - val_loss: 0.3503\n",
      "Epoch 322/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 55.5533 - val_loss: 0.2643\n",
      "Epoch 323/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 54.2911 - val_loss: 0.1972\n",
      "Epoch 324/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 53.0582 - val_loss: 0.1484\n",
      "Epoch 325/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 51.8539 - val_loss: 0.1174\n",
      "Epoch 326/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 50.6779 - val_loss: 0.1038\n",
      "Epoch 327/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 49.5296 - val_loss: 0.1071\n",
      "Epoch 328/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 48.4087 - val_loss: 0.1268\n",
      "Epoch 329/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 47.3145 - val_loss: 0.1624\n",
      "Epoch 330/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 46.2467 - val_loss: 0.2135\n",
      "Epoch 331/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 45.2048 - val_loss: 0.2797\n",
      "Epoch 332/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 44.1883 - val_loss: 0.3605\n",
      "Epoch 333/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 43.1967 - val_loss: 0.4554\n",
      "Epoch 334/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 42.2298 - val_loss: 0.5641\n",
      "Epoch 335/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 41.2869 - val_loss: 0.6860\n",
      "Epoch 336/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 40.3676 - val_loss: 0.8208\n",
      "Epoch 337/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 39.4715 - val_loss: 0.9679\n",
      "Epoch 338/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 38.5982 - val_loss: 1.1271\n",
      "Epoch 339/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 37.7473 - val_loss: 1.2979\n",
      "Epoch 340/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 36.9182 - val_loss: 1.4799\n",
      "Epoch 341/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 36.1106 - val_loss: 1.6727\n",
      "Epoch 342/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 35.3240 - val_loss: 1.8758\n",
      "Epoch 343/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 34.5581 - val_loss: 2.0889\n",
      "Epoch 344/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 33.8125 - val_loss: 2.3117\n",
      "Epoch 345/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 33.0866 - val_loss: 2.5437\n",
      "Epoch 346/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 32.3802 - val_loss: 2.7845\n",
      "Epoch 347/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 31.6929 - val_loss: 3.0337\n",
      "Epoch 348/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 31.0242 - val_loss: 3.2911\n",
      "Epoch 349/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 30.3738 - val_loss: 3.5563\n",
      "Epoch 350/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 29.7412 - val_loss: 3.8289\n",
      "Epoch 351/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 29.1261 - val_loss: 4.1085\n",
      "Epoch 352/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 28.5282 - val_loss: 4.3948\n",
      "Epoch 353/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 27.9471 - val_loss: 4.6875\n",
      "Epoch 354/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 27.3823 - val_loss: 4.9863\n",
      "Epoch 355/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 26.8336 - val_loss: 5.2907\n",
      "Epoch 356/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 26.3006 - val_loss: 5.6006\n",
      "Epoch 357/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 25.7828 - val_loss: 5.9156\n",
      "Epoch 358/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 25.2802 - val_loss: 6.2353\n",
      "Epoch 359/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 24.7921 - val_loss: 6.5594\n",
      "Epoch 360/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 24.3184 - val_loss: 6.8879\n",
      "Epoch 361/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 23.8586 - val_loss: 7.2202\n",
      "Epoch 362/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 23.4126 - val_loss: 7.5560\n",
      "Epoch 363/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 22.9798 - val_loss: 7.8952\n",
      "Epoch 364/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 22.5601 - val_loss: 8.2375\n",
      "Epoch 365/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 22.1531 - val_loss: 8.5824\n",
      "Epoch 366/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 21.7586 - val_loss: 8.9300\n",
      "Epoch 367/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 21.3761 - val_loss: 9.2797\n",
      "Epoch 368/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 21.0055 - val_loss: 9.6315\n",
      "Epoch 369/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.6464 - val_loss: 9.9851\n",
      "Epoch 370/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 20.2985 - val_loss: 10.3401\n",
      "Epoch 371/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 19.9616 - val_loss: 10.6965\n",
      "Epoch 372/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 19.6353 - val_loss: 11.0540\n",
      "Epoch 373/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 19.3194 - val_loss: 11.4123\n",
      "Epoch 374/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 19.0137 - val_loss: 11.7712\n",
      "Epoch 375/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 18.7178 - val_loss: 12.1306\n",
      "Epoch 376/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 18.4315 - val_loss: 12.4901\n",
      "Epoch 377/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 18.1546 - val_loss: 12.8497\n",
      "Epoch 378/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 17.8868 - val_loss: 13.2091\n",
      "Epoch 379/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 17.6278 - val_loss: 13.5682\n",
      "Epoch 380/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 17.3774 - val_loss: 13.9266\n",
      "Epoch 381/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 17.1355 - val_loss: 14.2844\n",
      "Epoch 382/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 16.9017 - val_loss: 14.6412\n",
      "Epoch 383/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.6757 - val_loss: 14.9971\n",
      "Epoch 384/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.4576 - val_loss: 15.3516\n",
      "Epoch 385/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 16.2468 - val_loss: 15.7049\n",
      "Epoch 386/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 16.0433 - val_loss: 16.0566\n",
      "Epoch 387/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 15.8469 - val_loss: 16.4065\n",
      "Epoch 388/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.6574 - val_loss: 16.7547\n",
      "Epoch 389/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.4745 - val_loss: 17.1008\n",
      "Epoch 390/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.2980 - val_loss: 17.4448\n",
      "Epoch 391/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 15.1279 - val_loss: 17.7868\n",
      "Epoch 392/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 14.9638 - val_loss: 18.1263\n",
      "Epoch 393/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 14.8056 - val_loss: 18.4635\n",
      "Epoch 394/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 14.6530 - val_loss: 18.7980\n",
      "Epoch 395/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 14.5061 - val_loss: 19.1299\n",
      "Epoch 396/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 14.3645 - val_loss: 19.4592\n",
      "Epoch 397/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 14.2281 - val_loss: 19.7856\n",
      "Epoch 398/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 14.0967 - val_loss: 20.1089\n",
      "Epoch 399/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 13.9702 - val_loss: 20.4291\n",
      "Epoch 400/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 13.8485 - val_loss: 20.7463\n",
      "Epoch 401/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 13.7313 - val_loss: 21.0604\n",
      "Epoch 402/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 13.6185 - val_loss: 21.3710\n",
      "Epoch 403/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 13.5100 - val_loss: 21.6785\n",
      "Epoch 404/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 13.4056 - val_loss: 21.9825\n",
      "Epoch 405/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 13.3052 - val_loss: 22.2830\n",
      "Epoch 406/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 13.2087 - val_loss: 22.5802\n",
      "Epoch 407/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 13.1159 - val_loss: 22.8735\n",
      "Epoch 408/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 13.0267 - val_loss: 23.1634\n",
      "Epoch 409/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 12.9410 - val_loss: 23.4495\n",
      "Epoch 410/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 12.8587 - val_loss: 23.7321\n",
      "Epoch 411/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 12.7796 - val_loss: 24.0106\n",
      "Epoch 412/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 12.7037 - val_loss: 24.2856\n",
      "Epoch 413/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 12.6307 - val_loss: 24.5567\n",
      "Epoch 414/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 12.5607 - val_loss: 24.8240\n",
      "Epoch 415/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 12.4935 - val_loss: 25.0872\n",
      "Epoch 416/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 12.4290 - val_loss: 25.3468\n",
      "Epoch 417/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 12.3671 - val_loss: 25.6024\n",
      "Epoch 418/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 12.3078 - val_loss: 25.8540\n",
      "Epoch 419/500\n",
      "13/13 [==============================] - 0s 14ms/step - loss: 12.2508 - val_loss: 26.1017\n",
      "Epoch 420/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 12.1963 - val_loss: 26.3455\n",
      "Epoch 421/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 12.1439 - val_loss: 26.5853\n",
      "Epoch 422/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 12.0937 - val_loss: 26.8212\n",
      "Epoch 423/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 12.0456 - val_loss: 27.0530\n",
      "Epoch 424/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.9996 - val_loss: 27.2808\n",
      "Epoch 425/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.9555 - val_loss: 27.5049\n",
      "Epoch 426/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 11.9132 - val_loss: 27.7247\n",
      "Epoch 427/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 11.8727 - val_loss: 27.9410\n",
      "Epoch 428/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 11.8338 - val_loss: 28.1532\n",
      "Epoch 429/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 11.7967 - val_loss: 28.3614\n",
      "Epoch 430/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 11.7612 - val_loss: 28.5660\n",
      "Epoch 431/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 11.7271 - val_loss: 28.7664\n",
      "Epoch 432/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.6945 - val_loss: 28.9629\n",
      "Epoch 433/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.6634 - val_loss: 29.1558\n",
      "Epoch 434/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.6335 - val_loss: 29.3449\n",
      "Epoch 435/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 11.6050 - val_loss: 29.5301\n",
      "Epoch 436/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.5777 - val_loss: 29.7117\n",
      "Epoch 437/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.5516 - val_loss: 29.8893\n",
      "Epoch 438/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.5266 - val_loss: 30.0635\n",
      "Epoch 439/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.5027 - val_loss: 30.2338\n",
      "Epoch 440/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 11.4799 - val_loss: 30.4006\n",
      "Epoch 441/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 11.4581 - val_loss: 30.5636\n",
      "Epoch 442/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 11.4372 - val_loss: 30.7233\n",
      "Epoch 443/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 11.4173 - val_loss: 30.8795\n",
      "Epoch 444/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 11.3982 - val_loss: 31.0322\n",
      "Epoch 445/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 11.3800 - val_loss: 31.1813\n",
      "Epoch 446/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 11.3626 - val_loss: 31.3271\n",
      "Epoch 447/500\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 11.3460 - val_loss: 31.4694\n",
      "Epoch 448/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 11.3301 - val_loss: 31.6085\n",
      "Epoch 449/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 11.3149 - val_loss: 31.7444\n",
      "Epoch 450/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 11.3004 - val_loss: 31.8770\n",
      "Epoch 451/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 11.2865 - val_loss: 32.0062\n",
      "Epoch 452/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 11.2733 - val_loss: 32.1324\n",
      "Epoch 453/500\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 11.2607 - val_loss: 32.2554\n",
      "Epoch 454/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 11.2486 - val_loss: 32.3753\n",
      "Epoch 455/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 11.2372 - val_loss: 32.4924\n",
      "Epoch 456/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 11.2261 - val_loss: 32.6064\n",
      "Epoch 457/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 11.2156 - val_loss: 32.7175\n",
      "Epoch 458/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 11.2056 - val_loss: 32.8260\n",
      "Epoch 459/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.1960 - val_loss: 32.9313\n",
      "Epoch 460/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.1869 - val_loss: 33.0339\n",
      "Epoch 461/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.1782 - val_loss: 33.1336\n",
      "Epoch 462/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.1699 - val_loss: 33.2309\n",
      "Epoch 463/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 11.1619 - val_loss: 33.3255\n",
      "Epoch 464/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.1543 - val_loss: 33.4176\n",
      "Epoch 465/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 11.1471 - val_loss: 33.5068\n",
      "Epoch 466/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 11.1402 - val_loss: 33.5939\n",
      "Epoch 467/500\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 11.1336 - val_loss: 33.6784\n",
      "Epoch 468/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 11.1273 - val_loss: 33.7606\n",
      "Epoch 469/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.1213 - val_loss: 33.8401\n",
      "Epoch 470/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 11.1155 - val_loss: 33.9177\n",
      "Epoch 471/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.1101 - val_loss: 33.9928\n",
      "Epoch 472/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 11.1049 - val_loss: 34.0659\n",
      "Epoch 473/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 11.0998 - val_loss: 34.1367\n",
      "Epoch 474/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 11.0951 - val_loss: 34.2057\n",
      "Epoch 475/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 11.0905 - val_loss: 34.2723\n",
      "Epoch 476/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 11.0862 - val_loss: 34.3370\n",
      "Epoch 477/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 11.0821 - val_loss: 34.3996\n",
      "Epoch 478/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 11.0781 - val_loss: 34.4602\n",
      "Epoch 479/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.0744 - val_loss: 34.5191\n",
      "Epoch 480/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 11.0707 - val_loss: 34.5761\n",
      "Epoch 481/500\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 11.0673 - val_loss: 34.6313\n",
      "Epoch 482/500\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 11.0641 - val_loss: 34.6848\n",
      "Epoch 483/500\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 11.0609 - val_loss: 34.7366\n",
      "Epoch 484/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.0579 - val_loss: 34.7864\n",
      "Epoch 485/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.0551 - val_loss: 34.8350\n",
      "Epoch 486/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 11.0524 - val_loss: 34.8817\n",
      "Epoch 487/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.0498 - val_loss: 34.9270\n",
      "Epoch 488/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 11.0473 - val_loss: 34.9709\n",
      "Epoch 489/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.0450 - val_loss: 35.0132\n",
      "Epoch 490/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.0427 - val_loss: 35.0541\n",
      "Epoch 491/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 11.0405 - val_loss: 35.0936\n",
      "Epoch 492/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 11.0385 - val_loss: 35.1318\n",
      "Epoch 493/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 11.0365 - val_loss: 35.1687\n",
      "Epoch 494/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 11.0347 - val_loss: 35.2040\n",
      "Epoch 495/500\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 11.0329 - val_loss: 35.2385\n",
      "Epoch 496/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 11.0312 - val_loss: 35.2715\n",
      "Epoch 497/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.0296 - val_loss: 35.3032\n",
      "Epoch 498/500\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 11.0281 - val_loss: 35.3339\n",
      "Epoch 499/500\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 11.0266 - val_loss: 35.3636\n",
      "Epoch 500/500\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 11.0252 - val_loss: 35.3921\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.8407, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0015, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0118, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.5775, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 406ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[70.80609244, 70.75287115, 70.69964986, 70.64642857, 70.59320728,\n",
       "        70.53998599, 70.48676471, 70.43354342, 70.38032213, 70.32710084,\n",
       "        70.27387955, 70.22065826, 70.16743697, 70.11421569, 70.0609944 ,\n",
       "        70.00777311, 69.95455182, 69.90133053, 69.89453782, 69.88893557,\n",
       "        69.88333333, 69.87773109, 69.87212885, 69.86652661, 69.86092437,\n",
       "        69.85532213, 69.84971989, 69.84411765, 69.83851541, 69.83291317,\n",
       "        69.82731092, 69.82170868, 69.81610644, 69.8105042 , 69.80490196,\n",
       "        69.79929972, 69.79369748, 69.78809524, 69.782493  , 69.77689076,\n",
       "        69.77128852, 69.76568627, 69.76008403, 69.75448179, 69.74887955,\n",
       "        69.74327731, 69.73767507, 69.73207283, 69.72647059, 69.72086835,\n",
       "        69.71526611, 69.70966387, 69.70406162, 69.69691877, 69.68571429,\n",
       "        69.6745098 , 69.66330532, 69.65210084, 69.64089636, 69.62969188,\n",
       "        69.61848739, 69.60728291, 69.59607843, 69.58487395, 69.57366947,\n",
       "        69.56246499, 69.5512605 , 69.54005602, 69.52885154, 69.51764706,\n",
       "        69.50644258, 69.4952381 , 69.48403361, 69.47282913, 69.46162465,\n",
       "        69.45042017, 69.43921569, 69.4280112 , 69.41680672, 69.40560224,\n",
       "        75.67118835,  0.        ,  0.        ,  0.        ,  0.13426104,\n",
       "         0.        ,  0.21786909,  0.        ,  0.        ,  0.        ,\n",
       "         0.14823352,  0.        ,  0.30652425,  0.50972205,  0.        ,\n",
       "         0.41961008,  0.53856009,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([67.26687675, 67.25287115, 67.23886555, 67.22485994, 67.21085434,\n",
       "       67.19684874, 67.18284314, 67.16883754, 67.15483193, 67.14082633,\n",
       "       67.12682073, 67.11281513, 67.09880952, 67.08480392, 67.07079832,\n",
       "       67.05679272, 67.04278711, 67.02878151, 67.01477591, 67.00077031,\n",
       "       66.98676471, 66.9727591 , 66.9587535 , 66.9447479 , 66.9307423 ,\n",
       "       66.91673669, 66.90273109, 66.88872549, 66.87471989, 66.86071429,\n",
       "       66.84670868, 66.83270308, 66.81869748, 66.80469188, 66.79068627,\n",
       "       66.77668067, 66.76267507, 66.74866947, 66.73466387, 66.72065826,\n",
       "       66.70665266, 66.69607843, 66.68860878, 66.68113912, 66.67366947,\n",
       "       66.66619981, 66.65873016, 66.6512605 , 66.64379085, 66.6363212 ,\n",
       "       66.62885154, 66.62138189, 66.61391223, 66.60644258, 66.59897292,\n",
       "       66.59150327, 66.58403361, 66.57656396, 66.5690943 , 66.56162465,\n",
       "       66.554155  , 66.54668534, 66.53921569, 66.53174603, 66.52427638,\n",
       "       66.51680672, 66.50933707, 66.50186741, 66.49439776, 66.4869281 ,\n",
       "       66.47945845, 66.4719888 , 66.46451914, 66.45704949, 66.44957983,\n",
       "       66.44211018, 66.43464052, 66.42717087, 66.41970121, 66.41223156,\n",
       "       66.4047619 , 66.39729225, 66.3898226 , 66.38235294, 66.37488329,\n",
       "       66.36741363, 66.35994398, 66.35247432, 66.34500467, 66.33753501,\n",
       "       66.33006536, 66.3225957 , 66.31512605, 66.3076564 , 66.30018674,\n",
       "       66.29271709, 66.28524743, 66.27777778, 66.27030812, 66.26283847])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.005461534271557\n",
      "15.124198209300662\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
