{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.140000\n",
       "1       88.911429\n",
       "2       88.682857\n",
       "3       88.454286\n",
       "4       88.225714\n",
       "          ...    \n",
       "1745    65.536438\n",
       "1746    65.529902\n",
       "1747    65.523366\n",
       "1748    65.516830\n",
       "1749    65.510294\n",
       "Name: C5, Length: 1750, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c5_interpolated_1650_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.140000\n",
       "1       88.911429\n",
       "2       88.682857\n",
       "3       88.454286\n",
       "4       88.225714\n",
       "          ...    \n",
       "1645     0.383580\n",
       "1646     0.000000\n",
       "1647     0.184291\n",
       "1648     0.247834\n",
       "1649     0.134163\n",
       "Name: C5, Length: 1650, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1650)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.140000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.911429</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.682857</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.454286</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.225714</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     89.140000  0.000298\n",
       "1     88.911429  0.000298\n",
       "2     88.682857  0.000297\n",
       "3     88.454286  0.000297\n",
       "4     88.225714  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.8407  # Value for C0\n",
    "K0 = -0.0015  # Value for K0\n",
    "K1 = -0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0118    # Value for b\n",
    "c = 2.5775    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    89.140000    88.911429    88.682857    88.454286    88.225714   \n",
      "351    88.911429    88.682857    88.454286    88.225714    87.997143   \n",
      "352    88.682857    88.454286    88.225714    87.997143    87.768571   \n",
      "353    88.454286    88.225714    87.997143    87.768571    87.540000   \n",
      "354    88.225714    87.997143    87.768571    87.540000    87.311429   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.997143    87.768571    87.540000    87.311429    87.095798  ...   \n",
      "351    87.768571    87.540000    87.311429    87.095798    87.039776  ...   \n",
      "352    87.540000    87.311429    87.095798    87.039776    86.983754  ...   \n",
      "353    87.311429    87.095798    87.039776    86.983754    86.927731  ...   \n",
      "354    87.095798    87.039776    86.983754    86.927731    86.871709  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   79.161181    0.000263   79.132236    0.000263   79.103291    0.000263   \n",
      "351   79.132236    0.000263   79.103291    0.000263   79.074346    0.000262   \n",
      "352   79.103291    0.000263   79.074346    0.000262   79.045401    0.000262   \n",
      "353   79.074346    0.000262   79.045401    0.000262   79.016457    0.000262   \n",
      "354   79.045401    0.000262   79.016457    0.000262   78.987512    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   79.074346    0.000262   79.045401    0.000262  \n",
      "351   79.045401    0.000262   79.016457    0.000262  \n",
      "352   79.016457    0.000262   78.987512    0.000262  \n",
      "353   78.987512    0.000262   78.958567    0.000262  \n",
      "354   78.958567    0.000262   78.929622    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 1, 251) (1200, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "15/15 [==============================] - 3s 55ms/step - loss: 5380.5210 - val_loss: 4098.6855\n",
      "Epoch 2/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5307.1001 - val_loss: 4051.7146\n",
      "Epoch 3/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 5245.3047 - val_loss: 4005.3369\n",
      "Epoch 4/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5183.9463 - val_loss: 3959.4080\n",
      "Epoch 5/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5123.1582 - val_loss: 3913.9646\n",
      "Epoch 6/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 5062.9531 - val_loss: 3856.8855\n",
      "Epoch 7/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4984.5156 - val_loss: 3808.5796\n",
      "Epoch 8/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4922.4839 - val_loss: 3762.2839\n",
      "Epoch 9/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4861.2080 - val_loss: 3716.7349\n",
      "Epoch 10/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4800.8091 - val_loss: 3671.8621\n",
      "Epoch 11/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4741.1899 - val_loss: 3627.5962\n",
      "Epoch 12/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4682.2749 - val_loss: 3583.8901\n",
      "Epoch 13/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4624.0151 - val_loss: 3540.7131\n",
      "Epoch 14/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4562.8232 - val_loss: 3488.4934\n",
      "Epoch 15/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4495.3799 - val_loss: 3443.0034\n",
      "Epoch 16/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4434.8638 - val_loss: 3398.3640\n",
      "Epoch 17/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4375.5049 - val_loss: 3354.6648\n",
      "Epoch 18/500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 4317.2144 - val_loss: 3311.7651\n",
      "Epoch 19/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 4259.8369 - val_loss: 3269.5593\n",
      "Epoch 20/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4201.3096 - val_loss: 3221.5425\n",
      "Epoch 21/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4137.0562 - val_loss: 3176.8547\n",
      "Epoch 22/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4077.3135 - val_loss: 3133.1367\n",
      "Epoch 23/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4018.9177 - val_loss: 3090.5061\n",
      "Epoch 24/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3961.7629 - val_loss: 3048.7920\n",
      "Epoch 25/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3905.6516 - val_loss: 3007.8665\n",
      "Epoch 26/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3850.4500 - val_loss: 2967.6445\n",
      "Epoch 27/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3796.0688 - val_loss: 2928.0703\n",
      "Epoch 28/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3742.4456 - val_loss: 2889.1030\n",
      "Epoch 29/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3689.5383 - val_loss: 2850.7136\n",
      "Epoch 30/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3637.3115 - val_loss: 2812.8772\n",
      "Epoch 31/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3585.7393 - val_loss: 2775.5754\n",
      "Epoch 32/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3534.8000 - val_loss: 2738.7927\n",
      "Epoch 33/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3484.4761 - val_loss: 2702.5151\n",
      "Epoch 34/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3434.7522 - val_loss: 2666.7314\n",
      "Epoch 35/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3385.6133 - val_loss: 2631.4314\n",
      "Epoch 36/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3337.0483 - val_loss: 2596.6057\n",
      "Epoch 37/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3289.0486 - val_loss: 2562.2456\n",
      "Epoch 38/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3241.6018 - val_loss: 2528.3438\n",
      "Epoch 39/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3194.7007 - val_loss: 2494.8923\n",
      "Epoch 40/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3148.3372 - val_loss: 2461.8826\n",
      "Epoch 41/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3101.9646 - val_loss: 2427.3796\n",
      "Epoch 42/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3052.1797 - val_loss: 2391.0051\n",
      "Epoch 43/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3002.2463 - val_loss: 2355.6958\n",
      "Epoch 44/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 2953.6936 - val_loss: 2321.4373\n",
      "Epoch 45/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2906.3435 - val_loss: 2288.0459\n",
      "Epoch 46/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 2859.9905 - val_loss: 2255.3936\n",
      "Epoch 47/500\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 2814.4995 - val_loss: 2223.3972\n",
      "Epoch 48/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2769.7817 - val_loss: 2192.0012\n",
      "Epoch 49/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2725.7776 - val_loss: 2161.1675\n",
      "Epoch 50/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 2682.4421 - val_loss: 2130.8647\n",
      "Epoch 51/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2639.7427 - val_loss: 2101.0715\n",
      "Epoch 52/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2597.6521 - val_loss: 2071.7681\n",
      "Epoch 53/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2556.1506 - val_loss: 2042.9399\n",
      "Epoch 54/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2515.2185 - val_loss: 2014.5730\n",
      "Epoch 55/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2474.8423 - val_loss: 1986.6562\n",
      "Epoch 56/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 2435.0073 - val_loss: 1959.1797\n",
      "Epoch 57/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2395.7021 - val_loss: 1932.1340\n",
      "Epoch 58/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2356.9158 - val_loss: 1905.5112\n",
      "Epoch 59/500\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 2318.6394 - val_loss: 1879.3033\n",
      "Epoch 60/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2280.8638 - val_loss: 1853.5033\n",
      "Epoch 61/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2243.5808 - val_loss: 1828.1056\n",
      "Epoch 62/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 2206.7830 - val_loss: 1803.1034\n",
      "Epoch 63/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2170.4629 - val_loss: 1778.4910\n",
      "Epoch 64/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2134.6140 - val_loss: 1754.2632\n",
      "Epoch 65/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 2099.2302 - val_loss: 1730.4141\n",
      "Epoch 66/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 2064.3057 - val_loss: 1706.9393\n",
      "Epoch 67/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 2029.8341 - val_loss: 1683.8344\n",
      "Epoch 68/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 1995.8105 - val_loss: 1661.0940\n",
      "Epoch 69/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 1962.2288 - val_loss: 1638.7137\n",
      "Epoch 70/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 1929.0844 - val_loss: 1616.6898\n",
      "Epoch 71/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1896.3719 - val_loss: 1595.0175\n",
      "Epoch 72/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 1864.0875 - val_loss: 1573.6931\n",
      "Epoch 73/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 1832.2257 - val_loss: 1552.7124\n",
      "Epoch 74/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 1800.7823 - val_loss: 1532.0717\n",
      "Epoch 75/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1769.7523 - val_loss: 1511.7670\n",
      "Epoch 76/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1739.1324 - val_loss: 1491.7949\n",
      "Epoch 77/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1708.9174 - val_loss: 1472.1512\n",
      "Epoch 78/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1679.1035 - val_loss: 1452.8328\n",
      "Epoch 79/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1649.6866 - val_loss: 1433.8359\n",
      "Epoch 80/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1620.6630 - val_loss: 1415.1570\n",
      "Epoch 81/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1592.0287 - val_loss: 1396.7933\n",
      "Epoch 82/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 1563.7795 - val_loss: 1378.7408\n",
      "Epoch 83/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1535.9117 - val_loss: 1360.9966\n",
      "Epoch 84/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1508.4225 - val_loss: 1343.5569\n",
      "Epoch 85/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 1481.3066 - val_loss: 1326.4192\n",
      "Epoch 86/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1454.5614 - val_loss: 1309.5795\n",
      "Epoch 87/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1428.1833 - val_loss: 1293.0348\n",
      "Epoch 88/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1402.1683 - val_loss: 1276.7823\n",
      "Epoch 89/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1376.5132 - val_loss: 1260.8187\n",
      "Epoch 90/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1351.2150 - val_loss: 1245.1417\n",
      "Epoch 91/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1326.2695 - val_loss: 1229.7472\n",
      "Epoch 92/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1301.6736 - val_loss: 1214.6328\n",
      "Epoch 93/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1277.4238 - val_loss: 1199.7948\n",
      "Epoch 94/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1253.5168 - val_loss: 1185.2312\n",
      "Epoch 95/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1229.9501 - val_loss: 1170.9388\n",
      "Epoch 96/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1206.7198 - val_loss: 1156.9142\n",
      "Epoch 97/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1183.8221 - val_loss: 1143.1552\n",
      "Epoch 98/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1161.2548 - val_loss: 1129.6586\n",
      "Epoch 99/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1139.0140 - val_loss: 1116.4211\n",
      "Epoch 100/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1117.0969 - val_loss: 1103.4409\n",
      "Epoch 101/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1095.5006 - val_loss: 1090.7142\n",
      "Epoch 102/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1074.2218 - val_loss: 1078.2390\n",
      "Epoch 103/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1053.2570 - val_loss: 1066.0125\n",
      "Epoch 104/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1032.6040 - val_loss: 1054.0315\n",
      "Epoch 105/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1012.2587 - val_loss: 1042.2927\n",
      "Epoch 106/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 992.2187 - val_loss: 1030.7946\n",
      "Epoch 107/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 972.4810 - val_loss: 1019.5344\n",
      "Epoch 108/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 953.0432 - val_loss: 1008.5087\n",
      "Epoch 109/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 933.9014 - val_loss: 997.7155\n",
      "Epoch 110/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 915.0533 - val_loss: 987.1515\n",
      "Epoch 111/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 896.4951 - val_loss: 976.8145\n",
      "Epoch 112/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 878.2247 - val_loss: 966.7018\n",
      "Epoch 113/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 860.2391 - val_loss: 956.8109\n",
      "Epoch 114/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 842.5353 - val_loss: 947.1389\n",
      "Epoch 115/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 825.1105 - val_loss: 937.6832\n",
      "Epoch 116/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 807.9619 - val_loss: 928.4416\n",
      "Epoch 117/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 791.0866 - val_loss: 919.4112\n",
      "Epoch 118/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 774.4816 - val_loss: 910.5897\n",
      "Epoch 119/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 758.1445 - val_loss: 901.9742\n",
      "Epoch 120/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 742.0723 - val_loss: 893.5626\n",
      "Epoch 121/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 726.2623 - val_loss: 885.3526\n",
      "Epoch 122/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 710.7119 - val_loss: 877.3408\n",
      "Epoch 123/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 695.4178 - val_loss: 869.5253\n",
      "Epoch 124/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 680.3779 - val_loss: 861.9036\n",
      "Epoch 125/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 665.5890 - val_loss: 854.4729\n",
      "Epoch 126/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 651.0488 - val_loss: 847.2314\n",
      "Epoch 127/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 636.7545 - val_loss: 840.1760\n",
      "Epoch 128/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 622.7035 - val_loss: 833.3048\n",
      "Epoch 129/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 608.8929 - val_loss: 826.6150\n",
      "Epoch 130/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 595.3203 - val_loss: 820.1036\n",
      "Epoch 131/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 581.9825 - val_loss: 813.7695\n",
      "Epoch 132/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 568.8777 - val_loss: 807.6095\n",
      "Epoch 133/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 556.0029 - val_loss: 801.6213\n",
      "Epoch 134/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 543.3554 - val_loss: 795.8029\n",
      "Epoch 135/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 530.9327 - val_loss: 790.1514\n",
      "Epoch 136/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 518.7321 - val_loss: 784.6648\n",
      "Epoch 137/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 506.7513 - val_loss: 779.3401\n",
      "Epoch 138/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 494.9876 - val_loss: 774.1758\n",
      "Epoch 139/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 483.4383 - val_loss: 769.1693\n",
      "Epoch 140/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 472.1011 - val_loss: 764.3182\n",
      "Epoch 141/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 460.9736 - val_loss: 759.6202\n",
      "Epoch 142/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 450.0529 - val_loss: 755.0728\n",
      "Epoch 143/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 439.3368 - val_loss: 750.6742\n",
      "Epoch 144/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 428.8225 - val_loss: 746.4217\n",
      "Epoch 145/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 418.5078 - val_loss: 742.3132\n",
      "Epoch 146/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 408.3903 - val_loss: 738.3466\n",
      "Epoch 147/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 398.4673 - val_loss: 734.5193\n",
      "Epoch 148/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 388.7368 - val_loss: 730.8292\n",
      "Epoch 149/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 379.1958 - val_loss: 727.2742\n",
      "Epoch 150/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 369.8423 - val_loss: 723.8519\n",
      "Epoch 151/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 360.6736 - val_loss: 720.5603\n",
      "Epoch 152/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 351.6874 - val_loss: 717.3972\n",
      "Epoch 153/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 342.8813 - val_loss: 714.3599\n",
      "Epoch 154/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 334.2529 - val_loss: 711.4467\n",
      "Epoch 155/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 325.7999 - val_loss: 708.6554\n",
      "Epoch 156/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 317.5200 - val_loss: 705.9838\n",
      "Epoch 157/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 309.4107 - val_loss: 703.4298\n",
      "Epoch 158/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 301.4697 - val_loss: 700.9911\n",
      "Epoch 159/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 293.6949 - val_loss: 698.6656\n",
      "Epoch 160/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 286.0837 - val_loss: 696.4513\n",
      "Epoch 161/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 278.6341 - val_loss: 694.3463\n",
      "Epoch 162/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 271.3436 - val_loss: 692.3482\n",
      "Epoch 163/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 264.2101 - val_loss: 690.4545\n",
      "Epoch 164/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 257.2308 - val_loss: 688.6641\n",
      "Epoch 165/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 250.4043 - val_loss: 686.9743\n",
      "Epoch 166/500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 243.7278 - val_loss: 685.3831\n",
      "Epoch 167/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 237.1993 - val_loss: 683.8887\n",
      "Epoch 168/500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 230.8164 - val_loss: 682.4886\n",
      "Epoch 169/500\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 224.5771 - val_loss: 681.1813\n",
      "Epoch 170/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 218.4790 - val_loss: 679.9643\n",
      "Epoch 171/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 212.5202 - val_loss: 678.8358\n",
      "Epoch 172/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 206.6982 - val_loss: 677.7936\n",
      "Epoch 173/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 201.0112 - val_loss: 676.8356\n",
      "Epoch 174/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 195.4570 - val_loss: 675.9592\n",
      "Epoch 175/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 190.0333 - val_loss: 675.1617\n",
      "Epoch 176/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 184.7381 - val_loss: 674.4382\n",
      "Epoch 177/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 179.5694 - val_loss: 673.7639\n",
      "Epoch 178/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 173.8108 - val_loss: 672.0891\n",
      "Epoch 179/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 166.0438 - val_loss: 670.8720\n",
      "Epoch 180/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 159.7518 - val_loss: 669.8767\n",
      "Epoch 181/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 153.9025 - val_loss: 669.0927\n",
      "Epoch 182/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 148.4336 - val_loss: 668.4765\n",
      "Epoch 183/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 143.2678 - val_loss: 667.9980\n",
      "Epoch 184/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 138.3536 - val_loss: 667.6377\n",
      "Epoch 185/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 133.6573 - val_loss: 667.3820\n",
      "Epoch 186/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 129.1558 - val_loss: 667.2207\n",
      "Epoch 187/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 124.8318 - val_loss: 667.1462\n",
      "Epoch 188/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 120.6722 - val_loss: 667.1521\n",
      "Epoch 189/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 116.6665 - val_loss: 667.2327\n",
      "Epoch 190/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 112.8059 - val_loss: 667.3836\n",
      "Epoch 191/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 109.0827 - val_loss: 667.6006\n",
      "Epoch 192/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 105.4904 - val_loss: 667.8800\n",
      "Epoch 193/500\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 102.0233 - val_loss: 668.2184\n",
      "Epoch 194/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 98.6763 - val_loss: 668.6125\n",
      "Epoch 195/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 95.4445 - val_loss: 669.0594\n",
      "Epoch 196/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 92.3237 - val_loss: 669.5565\n",
      "Epoch 197/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 89.3098 - val_loss: 670.1010\n",
      "Epoch 198/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 86.3992 - val_loss: 670.6905\n",
      "Epoch 199/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 83.5882 - val_loss: 671.3229\n",
      "Epoch 200/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 80.8736 - val_loss: 671.9955\n",
      "Epoch 201/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 78.2521 - val_loss: 672.7067\n",
      "Epoch 202/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 75.7208 - val_loss: 673.4542\n",
      "Epoch 203/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 73.2768 - val_loss: 674.2357\n",
      "Epoch 204/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 70.9173 - val_loss: 675.0499\n",
      "Epoch 205/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 68.6398 - val_loss: 675.8944\n",
      "Epoch 206/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 66.4418 - val_loss: 676.7677\n",
      "Epoch 207/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 64.3208 - val_loss: 677.6681\n",
      "Epoch 208/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 62.2744 - val_loss: 678.5939\n",
      "Epoch 209/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 60.3006 - val_loss: 679.5432\n",
      "Epoch 210/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 58.3969 - val_loss: 680.5148\n",
      "Epoch 211/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 56.5612 - val_loss: 681.5071\n",
      "Epoch 212/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 54.7915 - val_loss: 682.5184\n",
      "Epoch 213/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 53.0859 - val_loss: 683.5477\n",
      "Epoch 214/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 51.4423 - val_loss: 684.5931\n",
      "Epoch 215/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 49.8588 - val_loss: 685.6536\n",
      "Epoch 216/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 48.3337 - val_loss: 686.7278\n",
      "Epoch 217/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 46.8650 - val_loss: 687.8146\n",
      "Epoch 218/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 45.4512 - val_loss: 688.9120\n",
      "Epoch 219/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 44.0905 - val_loss: 690.0198\n",
      "Epoch 220/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 42.7812 - val_loss: 691.1364\n",
      "Epoch 221/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 41.5217 - val_loss: 692.2607\n",
      "Epoch 222/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 40.3104 - val_loss: 693.3915\n",
      "Epoch 223/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 39.1459 - val_loss: 694.5280\n",
      "Epoch 224/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 38.0266 - val_loss: 695.6691\n",
      "Epoch 225/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 36.9511 - val_loss: 696.8135\n",
      "Epoch 226/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 35.9180 - val_loss: 697.9607\n",
      "Epoch 227/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 34.9260 - val_loss: 699.1091\n",
      "Epoch 228/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 33.9736 - val_loss: 700.2585\n",
      "Epoch 229/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 33.0596 - val_loss: 701.4078\n",
      "Epoch 230/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 32.1826 - val_loss: 702.5560\n",
      "Epoch 231/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 31.3414 - val_loss: 703.7026\n",
      "Epoch 232/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 30.5348 - val_loss: 704.8467\n",
      "Epoch 233/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 29.7617 - val_loss: 705.9872\n",
      "Epoch 234/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 29.0210 - val_loss: 707.1238\n",
      "Epoch 235/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 28.3114 - val_loss: 708.2555\n",
      "Epoch 236/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 27.6320 - val_loss: 709.3821\n",
      "Epoch 237/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 26.9815 - val_loss: 710.5027\n",
      "Epoch 238/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 26.3591 - val_loss: 711.6165\n",
      "Epoch 239/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.7636 - val_loss: 712.7233\n",
      "Epoch 240/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 25.1942 - val_loss: 713.8219\n",
      "Epoch 241/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 24.6499 - val_loss: 714.9127\n",
      "Epoch 242/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 24.1296 - val_loss: 715.9940\n",
      "Epoch 243/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 23.6326 - val_loss: 717.0667\n",
      "Epoch 244/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 23.1581 - val_loss: 718.1293\n",
      "Epoch 245/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 22.7051 - val_loss: 719.1820\n",
      "Epoch 246/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 22.2728 - val_loss: 720.2237\n",
      "Epoch 247/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 21.8604 - val_loss: 721.2545\n",
      "Epoch 248/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 21.4672 - val_loss: 722.2744\n",
      "Epoch 249/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 21.0923 - val_loss: 723.2823\n",
      "Epoch 250/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 20.7352 - val_loss: 724.2779\n",
      "Epoch 251/500\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 20.3950 - val_loss: 725.2610\n",
      "Epoch 252/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 20.0712 - val_loss: 726.2321\n",
      "Epoch 253/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 19.7629 - val_loss: 727.1902\n",
      "Epoch 254/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 19.4696 - val_loss: 728.1350\n",
      "Epoch 255/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 19.1907 - val_loss: 729.0665\n",
      "Epoch 256/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 18.9256 - val_loss: 729.9846\n",
      "Epoch 257/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 18.6736 - val_loss: 730.8887\n",
      "Epoch 258/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 18.4343 - val_loss: 731.7794\n",
      "Epoch 259/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 18.2070 - val_loss: 732.6555\n",
      "Epoch 260/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 17.9914 - val_loss: 733.5173\n",
      "Epoch 261/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 17.7868 - val_loss: 734.3651\n",
      "Epoch 262/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 17.5928 - val_loss: 735.1983\n",
      "Epoch 263/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 17.4089 - val_loss: 736.0175\n",
      "Epoch 264/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 17.2346 - val_loss: 736.8218\n",
      "Epoch 265/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 17.0695 - val_loss: 737.6115\n",
      "Epoch 266/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 16.9132 - val_loss: 738.3867\n",
      "Epoch 267/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 16.7653 - val_loss: 739.1472\n",
      "Epoch 268/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 16.6254 - val_loss: 739.8928\n",
      "Epoch 269/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 16.4931 - val_loss: 740.6240\n",
      "Epoch 270/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.3681 - val_loss: 741.3409\n",
      "Epoch 271/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.2499 - val_loss: 742.0428\n",
      "Epoch 272/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.1384 - val_loss: 742.7305\n",
      "Epoch 273/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 16.0330 - val_loss: 743.4031\n",
      "Epoch 274/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.9337 - val_loss: 744.0620\n",
      "Epoch 275/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.8399 - val_loss: 744.7058\n",
      "Epoch 276/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 15.7516 - val_loss: 745.3358\n",
      "Epoch 277/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.6683 - val_loss: 745.9519\n",
      "Epoch 278/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.5899 - val_loss: 746.5533\n",
      "Epoch 279/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.5160 - val_loss: 747.1407\n",
      "Epoch 280/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.4466 - val_loss: 747.7142\n",
      "Epoch 281/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.3812 - val_loss: 748.2740\n",
      "Epoch 282/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.3198 - val_loss: 748.8201\n",
      "Epoch 283/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.2621 - val_loss: 749.3532\n",
      "Epoch 284/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.2078 - val_loss: 749.8729\n",
      "Epoch 285/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 15.1568 - val_loss: 750.3792\n",
      "Epoch 286/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.1090 - val_loss: 750.8724\n",
      "Epoch 287/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 15.0642 - val_loss: 751.3533\n",
      "Epoch 288/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 15.0221 - val_loss: 751.8212\n",
      "Epoch 289/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.9826 - val_loss: 752.2764\n",
      "Epoch 290/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.9457 - val_loss: 752.7195\n",
      "Epoch 291/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.9111 - val_loss: 753.1504\n",
      "Epoch 292/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.8787 - val_loss: 753.5693\n",
      "Epoch 293/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.8484 - val_loss: 753.9764\n",
      "Epoch 294/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.8201 - val_loss: 754.3720\n",
      "Epoch 295/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.7936 - val_loss: 754.7560\n",
      "Epoch 296/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.7689 - val_loss: 755.1290\n",
      "Epoch 297/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.7458 - val_loss: 755.4910\n",
      "Epoch 298/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.7242 - val_loss: 755.8423\n",
      "Epoch 299/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.7041 - val_loss: 756.1830\n",
      "Epoch 300/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.6853 - val_loss: 756.5133\n",
      "Epoch 301/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 14.6678 - val_loss: 756.8333\n",
      "Epoch 302/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.6515 - val_loss: 757.1435\n",
      "Epoch 303/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.6363 - val_loss: 757.4440\n",
      "Epoch 304/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.6221 - val_loss: 757.7347\n",
      "Epoch 305/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.6090 - val_loss: 758.0166\n",
      "Epoch 306/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.5968 - val_loss: 758.2888\n",
      "Epoch 307/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.5854 - val_loss: 758.5524\n",
      "Epoch 308/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.5748 - val_loss: 758.8068\n",
      "Epoch 309/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.5650 - val_loss: 759.0527\n",
      "Epoch 310/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.5559 - val_loss: 759.2910\n",
      "Epoch 311/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.5474 - val_loss: 759.5204\n",
      "Epoch 312/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.5395 - val_loss: 759.7419\n",
      "Epoch 313/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.5323 - val_loss: 759.9562\n",
      "Epoch 314/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.5255 - val_loss: 760.1627\n",
      "Epoch 315/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.5192 - val_loss: 760.3617\n",
      "Epoch 316/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.5135 - val_loss: 760.5535\n",
      "Epoch 317/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.5081 - val_loss: 760.7382\n",
      "Epoch 318/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.5032 - val_loss: 760.9164\n",
      "Epoch 319/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4986 - val_loss: 761.0883\n",
      "Epoch 320/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4943 - val_loss: 761.2539\n",
      "Epoch 321/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4904 - val_loss: 761.4128\n",
      "Epoch 322/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4868 - val_loss: 761.5660\n",
      "Epoch 323/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 14.4835 - val_loss: 761.7131\n",
      "Epoch 324/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4804 - val_loss: 761.8547\n",
      "Epoch 325/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4776 - val_loss: 761.9905\n",
      "Epoch 326/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4750 - val_loss: 762.1211\n",
      "Epoch 327/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4726 - val_loss: 762.2466\n",
      "Epoch 328/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4703 - val_loss: 762.3671\n",
      "Epoch 329/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4683 - val_loss: 762.4824\n",
      "Epoch 330/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4665 - val_loss: 762.5931\n",
      "Epoch 331/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4648 - val_loss: 762.6995\n",
      "Epoch 332/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4632 - val_loss: 762.8013\n",
      "Epoch 333/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4618 - val_loss: 762.8991\n",
      "Epoch 334/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4604 - val_loss: 762.9926\n",
      "Epoch 335/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4592 - val_loss: 763.0822\n",
      "Epoch 336/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4581 - val_loss: 763.1674\n",
      "Epoch 337/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4572 - val_loss: 763.2496\n",
      "Epoch 338/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4562 - val_loss: 763.3282\n",
      "Epoch 339/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 14.4554 - val_loss: 763.4030\n",
      "Epoch 340/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4546 - val_loss: 763.4744\n",
      "Epoch 341/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4539 - val_loss: 763.5429\n",
      "Epoch 342/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4533 - val_loss: 763.6082\n",
      "Epoch 343/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4528 - val_loss: 763.6705\n",
      "Epoch 344/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4523 - val_loss: 763.7295\n",
      "Epoch 345/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4519 - val_loss: 763.7867\n",
      "Epoch 346/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 14.4515 - val_loss: 763.8406\n",
      "Epoch 347/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4511 - val_loss: 763.8919\n",
      "Epoch 348/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4508 - val_loss: 763.9410\n",
      "Epoch 349/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 14.4506 - val_loss: 763.9880\n",
      "Epoch 350/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4503 - val_loss: 764.0324\n",
      "Epoch 351/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4501 - val_loss: 764.0749\n",
      "Epoch 352/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4499 - val_loss: 764.1151\n",
      "Epoch 353/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4498 - val_loss: 764.1534\n",
      "Epoch 354/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4497 - val_loss: 764.1896\n",
      "Epoch 355/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4496 - val_loss: 764.2242\n",
      "Epoch 356/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4495 - val_loss: 764.2569\n",
      "Epoch 357/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4495 - val_loss: 764.2875\n",
      "Epoch 358/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4495 - val_loss: 764.3173\n",
      "Epoch 359/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4495 - val_loss: 764.3450\n",
      "Epoch 360/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4495 - val_loss: 764.3710\n",
      "Epoch 361/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4495 - val_loss: 764.3961\n",
      "Epoch 362/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4496 - val_loss: 764.4195\n",
      "Epoch 363/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4497 - val_loss: 764.4422\n",
      "Epoch 364/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4497 - val_loss: 764.4633\n",
      "Epoch 365/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4498 - val_loss: 764.4836\n",
      "Epoch 366/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4499 - val_loss: 764.5024\n",
      "Epoch 367/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4500 - val_loss: 764.5200\n",
      "Epoch 368/500\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 14.4501 - val_loss: 764.5370\n",
      "Epoch 369/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4502 - val_loss: 764.5527\n",
      "Epoch 370/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4504 - val_loss: 764.5678\n",
      "Epoch 371/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 14.4505 - val_loss: 764.5817\n",
      "Epoch 372/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4507 - val_loss: 764.5951\n",
      "Epoch 373/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4508 - val_loss: 764.6077\n",
      "Epoch 374/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4510 - val_loss: 764.6191\n",
      "Epoch 375/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4512 - val_loss: 764.6301\n",
      "Epoch 376/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4513 - val_loss: 764.6407\n",
      "Epoch 377/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4515 - val_loss: 764.6503\n",
      "Epoch 378/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4517 - val_loss: 764.6593\n",
      "Epoch 379/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4519 - val_loss: 764.6673\n",
      "Epoch 380/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4521 - val_loss: 764.6755\n",
      "Epoch 381/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4523 - val_loss: 764.6827\n",
      "Epoch 382/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4525 - val_loss: 764.6898\n",
      "Epoch 383/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4528 - val_loss: 764.6966\n",
      "Epoch 384/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4530 - val_loss: 764.7028\n",
      "Epoch 385/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4532 - val_loss: 764.7081\n",
      "Epoch 386/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4534 - val_loss: 764.7137\n",
      "Epoch 387/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4536 - val_loss: 764.7186\n",
      "Epoch 388/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4539 - val_loss: 764.7228\n",
      "Epoch 389/500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 14.4541 - val_loss: 764.7269\n",
      "Epoch 390/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4544 - val_loss: 764.7312\n",
      "Epoch 391/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4546 - val_loss: 764.7347\n",
      "Epoch 392/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4549 - val_loss: 764.7382\n",
      "Epoch 393/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4550 - val_loss: 764.7411\n",
      "Epoch 394/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 14.4553 - val_loss: 764.7441\n",
      "Epoch 395/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4555 - val_loss: 764.7467\n",
      "Epoch 396/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4558 - val_loss: 764.7486\n",
      "Epoch 397/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 14.4561 - val_loss: 764.7512\n",
      "Epoch 398/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4563 - val_loss: 764.7534\n",
      "Epoch 399/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 14.4566 - val_loss: 764.7548\n",
      "Epoch 400/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4568 - val_loss: 764.7566\n",
      "Epoch 401/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4571 - val_loss: 764.7579\n",
      "Epoch 402/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4574 - val_loss: 764.7598\n",
      "Epoch 403/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4576 - val_loss: 764.7607\n",
      "Epoch 404/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4579 - val_loss: 764.7623\n",
      "Epoch 405/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4582 - val_loss: 764.7629\n",
      "Epoch 406/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4584 - val_loss: 764.7639\n",
      "Epoch 407/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 14.4587 - val_loss: 764.7647\n",
      "Epoch 408/500\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 14.4589 - val_loss: 764.7653\n",
      "Epoch 409/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 14.4592 - val_loss: 764.7656\n",
      "Epoch 410/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4595 - val_loss: 764.7658\n",
      "Epoch 411/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 14.4597 - val_loss: 764.7661\n",
      "Epoch 412/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4600 - val_loss: 764.7664\n",
      "Epoch 413/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4603 - val_loss: 764.7663\n",
      "Epoch 414/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4606 - val_loss: 764.7664\n",
      "Epoch 415/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4609 - val_loss: 764.7663\n",
      "Epoch 416/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4611 - val_loss: 764.7664\n",
      "Epoch 417/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4614 - val_loss: 764.7660\n",
      "Epoch 418/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4617 - val_loss: 764.7659\n",
      "Epoch 419/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4619 - val_loss: 764.7657\n",
      "Epoch 420/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4622 - val_loss: 764.7655\n",
      "Epoch 421/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4625 - val_loss: 764.7647\n",
      "Epoch 422/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4628 - val_loss: 764.7647\n",
      "Epoch 423/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4630 - val_loss: 764.7639\n",
      "Epoch 424/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4633 - val_loss: 764.7636\n",
      "Epoch 425/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4636 - val_loss: 764.7631\n",
      "Epoch 426/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4638 - val_loss: 764.7628\n",
      "Epoch 427/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4641 - val_loss: 764.7624\n",
      "Epoch 428/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4644 - val_loss: 764.7618\n",
      "Epoch 429/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4646 - val_loss: 764.7610\n",
      "Epoch 430/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4649 - val_loss: 764.7603\n",
      "Epoch 431/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 14.4652 - val_loss: 764.7598\n",
      "Epoch 432/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4655 - val_loss: 764.7592\n",
      "Epoch 433/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4658 - val_loss: 764.7586\n",
      "Epoch 434/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4660 - val_loss: 764.7582\n",
      "Epoch 435/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4663 - val_loss: 764.7574\n",
      "Epoch 436/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4665 - val_loss: 764.7568\n",
      "Epoch 437/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4668 - val_loss: 764.7562\n",
      "Epoch 438/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4671 - val_loss: 764.7556\n",
      "Epoch 439/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4673 - val_loss: 764.7548\n",
      "Epoch 440/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4676 - val_loss: 764.7540\n",
      "Epoch 441/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4679 - val_loss: 764.7535\n",
      "Epoch 442/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4682 - val_loss: 764.7527\n",
      "Epoch 443/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4684 - val_loss: 764.7523\n",
      "Epoch 444/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4687 - val_loss: 764.7514\n",
      "Epoch 445/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4690 - val_loss: 764.7509\n",
      "Epoch 446/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4692 - val_loss: 764.7501\n",
      "Epoch 447/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4695 - val_loss: 764.7495\n",
      "Epoch 448/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 14.4698 - val_loss: 764.7487\n",
      "Epoch 449/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 14.4700 - val_loss: 764.7478\n",
      "Epoch 450/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 14.4703 - val_loss: 764.7468\n",
      "Epoch 451/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4705 - val_loss: 764.7462\n",
      "Epoch 452/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4708 - val_loss: 764.7455\n",
      "Epoch 453/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4710 - val_loss: 764.7446\n",
      "Epoch 454/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4713 - val_loss: 764.7441\n",
      "Epoch 455/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4716 - val_loss: 764.7433\n",
      "Epoch 456/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 14.4719 - val_loss: 764.7430\n",
      "Epoch 457/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4721 - val_loss: 764.7419\n",
      "Epoch 458/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4724 - val_loss: 764.7411\n",
      "Epoch 459/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4726 - val_loss: 764.7403\n",
      "Epoch 460/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4729 - val_loss: 764.7399\n",
      "Epoch 461/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4731 - val_loss: 764.7394\n",
      "Epoch 462/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4734 - val_loss: 764.7384\n",
      "Epoch 463/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4736 - val_loss: 764.7377\n",
      "Epoch 464/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4738 - val_loss: 764.7370\n",
      "Epoch 465/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4741 - val_loss: 764.7366\n",
      "Epoch 466/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4743 - val_loss: 764.7360\n",
      "Epoch 467/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4745 - val_loss: 764.7349\n",
      "Epoch 468/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4748 - val_loss: 764.7341\n",
      "Epoch 469/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4750 - val_loss: 764.7332\n",
      "Epoch 470/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4753 - val_loss: 764.7326\n",
      "Epoch 471/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4755 - val_loss: 764.7319\n",
      "Epoch 472/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4758 - val_loss: 764.7310\n",
      "Epoch 473/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4761 - val_loss: 764.7305\n",
      "Epoch 474/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4763 - val_loss: 764.7302\n",
      "Epoch 475/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4765 - val_loss: 764.7298\n",
      "Epoch 476/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4767 - val_loss: 764.7286\n",
      "Epoch 477/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4770 - val_loss: 764.7281\n",
      "Epoch 478/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4772 - val_loss: 764.7275\n",
      "Epoch 479/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4775 - val_loss: 764.7270\n",
      "Epoch 480/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4777 - val_loss: 764.7267\n",
      "Epoch 481/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4779 - val_loss: 764.7262\n",
      "Epoch 482/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4781 - val_loss: 764.7250\n",
      "Epoch 483/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 14.4783 - val_loss: 764.7245\n",
      "Epoch 484/500\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 14.4785 - val_loss: 764.7236\n",
      "Epoch 485/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4788 - val_loss: 764.7231\n",
      "Epoch 486/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 14.4790 - val_loss: 764.7221\n",
      "Epoch 487/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 14.4793 - val_loss: 764.7214\n",
      "Epoch 488/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 14.4794 - val_loss: 764.7207\n",
      "Epoch 489/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4796 - val_loss: 764.7199\n",
      "Epoch 490/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4799 - val_loss: 764.7190\n",
      "Epoch 491/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4801 - val_loss: 764.7184\n",
      "Epoch 492/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4803 - val_loss: 764.7177\n",
      "Epoch 493/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 14.4805 - val_loss: 764.7172\n",
      "Epoch 494/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4807 - val_loss: 764.7164\n",
      "Epoch 495/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 14.4810 - val_loss: 764.7158\n",
      "Epoch 496/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4811 - val_loss: 764.7152\n",
      "Epoch 497/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4813 - val_loss: 764.7146\n",
      "Epoch 498/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4815 - val_loss: 764.7138\n",
      "Epoch 499/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 14.4818 - val_loss: 764.7136\n",
      "Epoch 500/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 14.4819 - val_loss: 764.7127\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.8407, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0015, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0118, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.5775, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 441ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.95437908e+01, 6.95325864e+01, 6.95213819e+01, 6.95101774e+01,\n",
       "        6.94989729e+01, 6.94877684e+01, 6.94765640e+01, 6.94653595e+01,\n",
       "        6.94541550e+01, 6.94429505e+01, 6.94317460e+01, 6.94205415e+01,\n",
       "        6.94093371e+01, 7.36820261e+01, 7.35643791e+01, 7.34467320e+01,\n",
       "        7.33290850e+01, 7.32114379e+01, 7.30937909e+01, 7.29761438e+01,\n",
       "        7.28584967e+01, 7.27408497e+01, 7.26232026e+01, 7.25055556e+01,\n",
       "        7.23879085e+01, 7.22702614e+01, 7.21526144e+01, 7.20349673e+01,\n",
       "        7.19173203e+01, 7.17995565e+01, 7.16398926e+01, 7.14802288e+01,\n",
       "        7.13205649e+01, 7.11609010e+01, 7.10012372e+01, 7.08415733e+01,\n",
       "        7.06819094e+01, 7.05222456e+01, 7.03625817e+01, 7.02029178e+01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.97199330e-01, 1.01878700e-01, 0.00000000e+00, 7.22964052e+01,\n",
       "        7.21787582e+01, 7.20611111e+01, 7.21943464e+01, 7.21825817e+01,\n",
       "        7.21675374e+01, 7.21515710e+01, 7.21356046e+01, 7.21196382e+01,\n",
       "        7.21036718e+01, 7.20877054e+01, 7.20717390e+01, 7.20557726e+01,\n",
       "        7.20398063e+01, 7.20238399e+01, 7.20078735e+01, 7.19919071e+01,\n",
       "        7.19885201e+01, 7.19868394e+01, 7.19851587e+01, 7.19834781e+01,\n",
       "        7.19817974e+01, 7.19801167e+01, 7.19784360e+01, 7.19767554e+01,\n",
       "        7.19750747e+01, 7.56149979e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.89798886e-01, 0.00000000e+00, 0.00000000e+00, 1.37733370e-01,\n",
       "        6.67380219e+01, 0.00000000e+00, 2.77729988e-01, 3.09567869e-01,\n",
       "        4.58358377e-02, 0.00000000e+00, 0.00000000e+00, 2.37724364e-01,\n",
       "        3.91463578e-01, 2.54072487e-01, 2.55735099e-01, 7.28807449e-01,\n",
       "        4.72652316e-01, 1.01991725e+00, 7.58990943e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.07554305e-01, 4.14305925e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([65.85105042, 65.85011671, 65.84918301, 65.8482493 , 65.84731559,\n",
       "       65.84638189, 65.84544818, 65.84451447, 65.84358077, 65.84264706,\n",
       "       65.84171335, 65.84077965, 65.83984594, 65.83891223, 65.83797852,\n",
       "       65.83704482, 65.83611111, 65.8351774 , 65.8342437 , 65.83330999,\n",
       "       65.83237628, 65.83144258, 65.83050887, 65.82957516, 65.82864146,\n",
       "       65.82770775, 65.82677404, 65.82584034, 65.82490663, 65.82397292,\n",
       "       65.82303922, 65.82210551, 65.8211718 , 65.8202381 , 65.81930439,\n",
       "       65.81837068, 65.81743697, 65.81650327, 65.81556956, 65.81463585,\n",
       "       65.81370215, 65.81276844, 65.81183473, 65.81090103, 65.80996732,\n",
       "       65.80903361, 65.80809991, 65.8071662 , 65.80623249, 65.80529879,\n",
       "       65.80436508, 65.80343137, 65.80249767, 65.80156396, 65.80063025,\n",
       "       65.79787582, 65.79133987, 65.78480392, 65.77826797, 65.77173203,\n",
       "       65.76519608, 65.75866013, 65.75212418, 65.74558824, 65.73905229,\n",
       "       65.73251634, 65.72598039, 65.71944444, 65.7129085 , 65.70637255,\n",
       "       65.6998366 , 65.69330065, 65.68676471, 65.68022876, 65.67369281,\n",
       "       65.66715686, 65.66062092, 65.65408497, 65.64754902, 65.64101307,\n",
       "       65.63447712, 65.62794118, 65.62140523, 65.61486928, 65.60833333,\n",
       "       65.60179739, 65.59526144, 65.58872549, 65.58218954, 65.57565359,\n",
       "       65.56911765, 65.5625817 , 65.55604575, 65.5495098 , 65.54297386,\n",
       "       65.53643791, 65.52990196, 65.52336601, 65.51683007, 65.51029412])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.33870814862047\n",
      "24.846740875433206\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
