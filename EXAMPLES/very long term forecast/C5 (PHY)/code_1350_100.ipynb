{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.140000\n",
       "1       88.911429\n",
       "2       88.682857\n",
       "3       88.454286\n",
       "4       88.225714\n",
       "          ...    \n",
       "1445    67.336905\n",
       "1446    67.322899\n",
       "1447    67.308894\n",
       "1448    67.294888\n",
       "1449    67.280882\n",
       "Name: C5, Length: 1450, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c5_interpolated_1350_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       89.140000\n",
       "1       88.911429\n",
       "2       88.682857\n",
       "3       88.454286\n",
       "4       88.225714\n",
       "          ...    \n",
       "1345     0.000000\n",
       "1346     0.101879\n",
       "1347     0.000000\n",
       "1348     0.721141\n",
       "1349     0.000000\n",
       "Name: C5, Length: 1350, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1350)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.140000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.911429</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.682857</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.454286</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.225714</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     89.140000  0.000298\n",
       "1     88.911429  0.000298\n",
       "2     88.682857  0.000297\n",
       "3     88.454286  0.000297\n",
       "4     88.225714  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 86.8407  # Value for C0\n",
    "K0 = -0.0015  # Value for K0\n",
    "K1 = -0.0001  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0118    # Value for b\n",
    "c = 2.5775    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    89.140000    88.911429    88.682857    88.454286    88.225714   \n",
      "351    88.911429    88.682857    88.454286    88.225714    87.997143   \n",
      "352    88.682857    88.454286    88.225714    87.997143    87.768571   \n",
      "353    88.454286    88.225714    87.997143    87.768571    87.540000   \n",
      "354    88.225714    87.997143    87.768571    87.540000    87.311429   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    87.997143    87.768571    87.540000    87.311429    87.095798  ...   \n",
      "351    87.768571    87.540000    87.311429    87.095798    87.039776  ...   \n",
      "352    87.540000    87.311429    87.095798    87.039776    86.983754  ...   \n",
      "353    87.311429    87.095798    87.039776    86.983754    86.927731  ...   \n",
      "354    87.095798    87.039776    86.983754    86.927731    86.871709  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   79.161181    0.000263   79.132236    0.000263   79.103291    0.000263   \n",
      "351   79.132236    0.000263   79.103291    0.000263   79.074346    0.000262   \n",
      "352   79.103291    0.000263   79.074346    0.000262   79.045401    0.000262   \n",
      "353   79.074346    0.000262   79.045401    0.000262   79.016457    0.000262   \n",
      "354   79.045401    0.000262   79.016457    0.000262   78.987512    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   79.074346    0.000262   79.045401    0.000262  \n",
      "351   79.045401    0.000262   79.016457    0.000262  \n",
      "352   79.016457    0.000262   78.987512    0.000262  \n",
      "353   78.987512    0.000262   78.958567    0.000262  \n",
      "354   78.958567    0.000262   78.929622    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 1, 251) (900, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "12/12 [==============================] - 2s 42ms/step - loss: 5772.2412 - val_loss: 4796.8076\n",
      "Epoch 2/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 5693.1733 - val_loss: 4762.4658\n",
      "Epoch 3/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 5655.6421 - val_loss: 4728.0391\n",
      "Epoch 4/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 5618.1582 - val_loss: 4693.7725\n",
      "Epoch 5/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 5580.8589 - val_loss: 4659.6992\n",
      "Epoch 6/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 5543.7627 - val_loss: 4625.8257\n",
      "Epoch 7/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 5506.8721 - val_loss: 4592.1475\n",
      "Epoch 8/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 5470.1816 - val_loss: 4558.6597\n",
      "Epoch 9/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 5433.6855 - val_loss: 4525.3604\n",
      "Epoch 10/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 5395.6357 - val_loss: 4485.9668\n",
      "Epoch 11/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 5353.2256 - val_loss: 4450.3779\n",
      "Epoch 12/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 5314.3623 - val_loss: 4415.0293\n",
      "Epoch 13/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 5275.9043 - val_loss: 4380.1372\n",
      "Epoch 14/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 5237.9189 - val_loss: 4345.6582\n",
      "Epoch 15/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 5200.3442 - val_loss: 4311.5322\n",
      "Epoch 16/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 5163.1230 - val_loss: 4277.7178\n",
      "Epoch 17/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 5126.2139 - val_loss: 4244.1831\n",
      "Epoch 18/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 5089.5889 - val_loss: 4210.9067\n",
      "Epoch 19/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 5053.2271 - val_loss: 4177.8735\n",
      "Epoch 20/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 5017.1138 - val_loss: 4145.0728\n",
      "Epoch 21/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4981.2383 - val_loss: 4112.4946\n",
      "Epoch 22/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4945.5918 - val_loss: 4080.1331\n",
      "Epoch 23/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4910.1665 - val_loss: 4047.9812\n",
      "Epoch 24/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4874.9575 - val_loss: 4016.0347\n",
      "Epoch 25/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4839.9600 - val_loss: 3984.2900\n",
      "Epoch 26/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4805.1699 - val_loss: 3952.7432\n",
      "Epoch 27/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4770.5835 - val_loss: 3921.3909\n",
      "Epoch 28/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4736.1978 - val_loss: 3890.2319\n",
      "Epoch 29/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4702.0098 - val_loss: 3859.2622\n",
      "Epoch 30/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4668.0176 - val_loss: 3828.4805\n",
      "Epoch 31/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4634.2192 - val_loss: 3797.8850\n",
      "Epoch 32/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4600.6123 - val_loss: 3767.4736\n",
      "Epoch 33/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 4567.1943 - val_loss: 3737.2444\n",
      "Epoch 34/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4533.9639 - val_loss: 3707.1958\n",
      "Epoch 35/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4500.9214 - val_loss: 3677.3274\n",
      "Epoch 36/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4468.0620 - val_loss: 3647.6365\n",
      "Epoch 37/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4435.3872 - val_loss: 3618.1223\n",
      "Epoch 38/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4402.8940 - val_loss: 3588.7844\n",
      "Epoch 39/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4370.5820 - val_loss: 3559.6201\n",
      "Epoch 40/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4338.4487 - val_loss: 3530.6294\n",
      "Epoch 41/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4306.4946 - val_loss: 3501.8108\n",
      "Epoch 42/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4274.7178 - val_loss: 3473.1636\n",
      "Epoch 43/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4243.1182 - val_loss: 3444.6860\n",
      "Epoch 44/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4211.6924 - val_loss: 3416.3782\n",
      "Epoch 45/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4180.4419 - val_loss: 3388.2378\n",
      "Epoch 46/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4149.3633 - val_loss: 3360.2649\n",
      "Epoch 47/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4118.4580 - val_loss: 3332.4580\n",
      "Epoch 48/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4087.7239 - val_loss: 3304.8174\n",
      "Epoch 49/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4057.1604 - val_loss: 3277.3411\n",
      "Epoch 50/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4026.7659 - val_loss: 3250.0286\n",
      "Epoch 51/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3996.5410 - val_loss: 3222.8792\n",
      "Epoch 52/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3966.4841 - val_loss: 3195.8914\n",
      "Epoch 53/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3936.5945 - val_loss: 3169.0649\n",
      "Epoch 54/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3906.8699 - val_loss: 3142.3989\n",
      "Epoch 55/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3877.3115 - val_loss: 3115.8931\n",
      "Epoch 56/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3847.9177 - val_loss: 3089.5454\n",
      "Epoch 57/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3818.6877 - val_loss: 3063.3567\n",
      "Epoch 58/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3789.6208 - val_loss: 3037.3257\n",
      "Epoch 59/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3760.7166 - val_loss: 3011.4509\n",
      "Epoch 60/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3731.9736 - val_loss: 2985.7319\n",
      "Epoch 61/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3703.3921 - val_loss: 2960.1685\n",
      "Epoch 62/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3674.9697 - val_loss: 2934.7600\n",
      "Epoch 63/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3646.7080 - val_loss: 2909.5049\n",
      "Epoch 64/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3618.6042 - val_loss: 2884.4038\n",
      "Epoch 65/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 3590.6584 - val_loss: 2859.4541\n",
      "Epoch 66/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3562.8708 - val_loss: 2834.6575\n",
      "Epoch 67/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3535.2388 - val_loss: 2810.0115\n",
      "Epoch 68/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3507.7639 - val_loss: 2785.5159\n",
      "Epoch 69/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3480.4434 - val_loss: 2761.1699\n",
      "Epoch 70/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3453.2778 - val_loss: 2736.9734\n",
      "Epoch 71/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3426.2664 - val_loss: 2712.9255\n",
      "Epoch 72/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3399.4072 - val_loss: 2689.0254\n",
      "Epoch 73/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3372.7021 - val_loss: 2665.2732\n",
      "Epoch 74/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3346.1482 - val_loss: 2641.6665\n",
      "Epoch 75/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3319.7461 - val_loss: 2618.2063\n",
      "Epoch 76/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3293.4937 - val_loss: 2594.8916\n",
      "Epoch 77/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3267.3931 - val_loss: 2571.7214\n",
      "Epoch 78/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3241.4407 - val_loss: 2548.6956\n",
      "Epoch 79/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3215.6372 - val_loss: 2525.8130\n",
      "Epoch 80/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3189.9827 - val_loss: 2503.0737\n",
      "Epoch 81/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3164.4751 - val_loss: 2480.4763\n",
      "Epoch 82/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3139.1145 - val_loss: 2458.0208\n",
      "Epoch 83/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3113.9006 - val_loss: 2435.7063\n",
      "Epoch 84/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3088.8323 - val_loss: 2413.5327\n",
      "Epoch 85/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3063.9094 - val_loss: 2391.4990\n",
      "Epoch 86/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3039.1309 - val_loss: 2369.6042\n",
      "Epoch 87/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3014.4968 - val_loss: 2347.8489\n",
      "Epoch 88/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2990.0059 - val_loss: 2326.2310\n",
      "Epoch 89/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2965.6577 - val_loss: 2304.7512\n",
      "Epoch 90/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2941.4521 - val_loss: 2283.4087\n",
      "Epoch 91/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2917.3879 - val_loss: 2262.2021\n",
      "Epoch 92/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2893.4644 - val_loss: 2241.1316\n",
      "Epoch 93/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2869.6824 - val_loss: 2220.1968\n",
      "Epoch 94/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2846.0398 - val_loss: 2199.3962\n",
      "Epoch 95/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2822.5361 - val_loss: 2178.7300\n",
      "Epoch 96/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 2799.1719 - val_loss: 2158.1975\n",
      "Epoch 97/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2775.9465 - val_loss: 2137.7986\n",
      "Epoch 98/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2752.8584 - val_loss: 2117.5310\n",
      "Epoch 99/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2729.9070 - val_loss: 2097.3970\n",
      "Epoch 100/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2707.0928 - val_loss: 2077.3938\n",
      "Epoch 101/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2684.4146 - val_loss: 2057.5212\n",
      "Epoch 102/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2661.8718 - val_loss: 2037.7802\n",
      "Epoch 103/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2639.4641 - val_loss: 2018.1677\n",
      "Epoch 104/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2617.1909 - val_loss: 1998.6854\n",
      "Epoch 105/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 2595.0520 - val_loss: 1979.3315\n",
      "Epoch 106/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2573.0459 - val_loss: 1960.1061\n",
      "Epoch 107/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2551.1733 - val_loss: 1941.0092\n",
      "Epoch 108/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2529.4331 - val_loss: 1922.0386\n",
      "Epoch 109/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2507.8242 - val_loss: 1903.1958\n",
      "Epoch 110/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2486.3467 - val_loss: 1884.4777\n",
      "Epoch 111/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2465.0002 - val_loss: 1865.8861\n",
      "Epoch 112/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2443.7839 - val_loss: 1847.4194\n",
      "Epoch 113/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2422.6975 - val_loss: 1829.0782\n",
      "Epoch 114/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2401.7405 - val_loss: 1810.8607\n",
      "Epoch 115/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2380.9119 - val_loss: 1792.7664\n",
      "Epoch 116/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2360.2114 - val_loss: 1774.7961\n",
      "Epoch 117/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2339.6394 - val_loss: 1756.9479\n",
      "Epoch 118/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 2319.1943 - val_loss: 1739.2222\n",
      "Epoch 119/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2298.8757 - val_loss: 1721.6173\n",
      "Epoch 120/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2278.6833 - val_loss: 1704.1350\n",
      "Epoch 121/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2258.6172 - val_loss: 1686.7729\n",
      "Epoch 122/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2238.6760 - val_loss: 1669.5305\n",
      "Epoch 123/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2218.8596 - val_loss: 1652.4087\n",
      "Epoch 124/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2199.1670 - val_loss: 1635.4045\n",
      "Epoch 125/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 2179.5981 - val_loss: 1618.5198\n",
      "Epoch 126/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2160.1528 - val_loss: 1601.7534\n",
      "Epoch 127/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2140.8303 - val_loss: 1585.1049\n",
      "Epoch 128/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2121.6306 - val_loss: 1568.5736\n",
      "Epoch 129/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2102.5522 - val_loss: 1552.1592\n",
      "Epoch 130/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2083.5952 - val_loss: 1535.8611\n",
      "Epoch 131/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2064.7595 - val_loss: 1519.6788\n",
      "Epoch 132/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2046.0436 - val_loss: 1503.6118\n",
      "Epoch 133/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2027.4486 - val_loss: 1487.6600\n",
      "Epoch 134/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2008.9722 - val_loss: 1471.8229\n",
      "Epoch 135/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1990.6155 - val_loss: 1456.0990\n",
      "Epoch 136/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1972.3770 - val_loss: 1440.4890\n",
      "Epoch 137/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1954.2565 - val_loss: 1424.9917\n",
      "Epoch 138/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1936.2533 - val_loss: 1409.6069\n",
      "Epoch 139/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1918.3672 - val_loss: 1394.3340\n",
      "Epoch 140/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1900.5981 - val_loss: 1379.1742\n",
      "Epoch 141/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1882.9457 - val_loss: 1364.1245\n",
      "Epoch 142/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1865.4088 - val_loss: 1349.1858\n",
      "Epoch 143/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1847.9872 - val_loss: 1334.3568\n",
      "Epoch 144/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1830.6801 - val_loss: 1319.6383\n",
      "Epoch 145/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1813.4875 - val_loss: 1305.0291\n",
      "Epoch 146/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1796.4091 - val_loss: 1290.5281\n",
      "Epoch 147/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1779.4440 - val_loss: 1276.1359\n",
      "Epoch 148/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1762.5918 - val_loss: 1261.8516\n",
      "Epoch 149/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1745.8523 - val_loss: 1247.6753\n",
      "Epoch 150/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1729.2252 - val_loss: 1233.6062\n",
      "Epoch 151/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1712.7101 - val_loss: 1219.6434\n",
      "Epoch 152/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1696.3059 - val_loss: 1205.7871\n",
      "Epoch 153/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1680.0125 - val_loss: 1192.0359\n",
      "Epoch 154/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1663.8292 - val_loss: 1178.3910\n",
      "Epoch 155/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1647.7562 - val_loss: 1164.8501\n",
      "Epoch 156/500\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 1631.7925 - val_loss: 1151.4142\n",
      "Epoch 157/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1615.9384 - val_loss: 1137.8611\n",
      "Epoch 158/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1598.4833 - val_loss: 1121.3735\n",
      "Epoch 159/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1578.6954 - val_loss: 1104.7727\n",
      "Epoch 160/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1559.2612 - val_loss: 1088.7603\n",
      "Epoch 161/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1540.4960 - val_loss: 1073.2853\n",
      "Epoch 162/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1522.2976 - val_loss: 1058.2454\n",
      "Epoch 163/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1504.5560 - val_loss: 1043.5621\n",
      "Epoch 164/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1487.1917 - val_loss: 1029.1814\n",
      "Epoch 165/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1470.1484 - val_loss: 1015.0642\n",
      "Epoch 166/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1453.3872 - val_loss: 1001.1840\n",
      "Epoch 167/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1436.8794 - val_loss: 987.5191\n",
      "Epoch 168/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1420.6033 - val_loss: 974.0547\n",
      "Epoch 169/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1404.5422 - val_loss: 960.7777\n",
      "Epoch 170/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1388.6830 - val_loss: 947.6780\n",
      "Epoch 171/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1373.0143 - val_loss: 934.7477\n",
      "Epoch 172/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1357.5272 - val_loss: 921.9790\n",
      "Epoch 173/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1342.2142 - val_loss: 909.3661\n",
      "Epoch 174/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1327.0688 - val_loss: 896.9036\n",
      "Epoch 175/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1312.0850 - val_loss: 884.5884\n",
      "Epoch 176/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1297.2584 - val_loss: 872.4143\n",
      "Epoch 177/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1282.5837 - val_loss: 860.3790\n",
      "Epoch 178/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1268.0580 - val_loss: 848.4794\n",
      "Epoch 179/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1253.6772 - val_loss: 836.7115\n",
      "Epoch 180/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1239.4381 - val_loss: 825.0740\n",
      "Epoch 181/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1225.3379 - val_loss: 813.5632\n",
      "Epoch 182/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1211.3738 - val_loss: 802.1782\n",
      "Epoch 183/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 1197.5439 - val_loss: 790.9160\n",
      "Epoch 184/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1183.8456 - val_loss: 779.7747\n",
      "Epoch 185/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1170.2765 - val_loss: 768.7526\n",
      "Epoch 186/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1156.8351 - val_loss: 757.8481\n",
      "Epoch 187/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1143.5193 - val_loss: 747.0599\n",
      "Epoch 188/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1130.3276 - val_loss: 736.3859\n",
      "Epoch 189/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1117.2579 - val_loss: 725.8251\n",
      "Epoch 190/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1104.3094 - val_loss: 715.3759\n",
      "Epoch 191/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1091.4802 - val_loss: 705.0367\n",
      "Epoch 192/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1078.7686 - val_loss: 694.8072\n",
      "Epoch 193/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1066.1740 - val_loss: 684.6850\n",
      "Epoch 194/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1053.6947 - val_loss: 674.6700\n",
      "Epoch 195/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1041.3295 - val_loss: 664.7611\n",
      "Epoch 196/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1029.0776 - val_loss: 654.9570\n",
      "Epoch 197/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1016.9373 - val_loss: 645.2560\n",
      "Epoch 198/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1004.9077 - val_loss: 635.6579\n",
      "Epoch 199/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 992.9881 - val_loss: 626.1622\n",
      "Epoch 200/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 981.1774 - val_loss: 616.7667\n",
      "Epoch 201/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 969.4744 - val_loss: 607.4714\n",
      "Epoch 202/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 957.8785 - val_loss: 598.2748\n",
      "Epoch 203/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 946.3887 - val_loss: 589.1774\n",
      "Epoch 204/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 935.0033 - val_loss: 580.1761\n",
      "Epoch 205/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 923.7226 - val_loss: 571.2722\n",
      "Epoch 206/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 912.5449 - val_loss: 562.4642\n",
      "Epoch 207/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 901.4700 - val_loss: 553.7508\n",
      "Epoch 208/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 890.4968 - val_loss: 545.1318\n",
      "Epoch 209/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 879.6246 - val_loss: 536.6063\n",
      "Epoch 210/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 868.8528 - val_loss: 528.1739\n",
      "Epoch 211/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 858.1802 - val_loss: 519.8336\n",
      "Epoch 212/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 847.6063 - val_loss: 511.5841\n",
      "Epoch 213/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 837.1305 - val_loss: 503.4257\n",
      "Epoch 214/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 826.7518 - val_loss: 495.3571\n",
      "Epoch 215/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 816.4700 - val_loss: 487.3783\n",
      "Epoch 216/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 806.2835 - val_loss: 479.4875\n",
      "Epoch 217/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 796.1925 - val_loss: 471.6855\n",
      "Epoch 218/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 786.1960 - val_loss: 463.9704\n",
      "Epoch 219/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 776.2935 - val_loss: 456.3423\n",
      "Epoch 220/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 766.4841 - val_loss: 448.8000\n",
      "Epoch 221/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 756.7673 - val_loss: 441.3438\n",
      "Epoch 222/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 747.1429 - val_loss: 433.9721\n",
      "Epoch 223/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 737.6096 - val_loss: 426.6844\n",
      "Epoch 224/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 728.1666 - val_loss: 419.4812\n",
      "Epoch 225/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 718.8142 - val_loss: 412.3603\n",
      "Epoch 226/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 709.5511 - val_loss: 405.3223\n",
      "Epoch 227/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 700.3773 - val_loss: 398.3663\n",
      "Epoch 228/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 691.2919 - val_loss: 391.4920\n",
      "Epoch 229/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 682.2938 - val_loss: 384.6975\n",
      "Epoch 230/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 673.3832 - val_loss: 377.9844\n",
      "Epoch 231/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 664.5594 - val_loss: 371.3507\n",
      "Epoch 232/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 655.8217 - val_loss: 364.7958\n",
      "Epoch 233/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 647.1693 - val_loss: 358.3197\n",
      "Epoch 234/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 638.6021 - val_loss: 351.9210\n",
      "Epoch 235/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 630.1189 - val_loss: 345.6003\n",
      "Epoch 236/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 621.7198 - val_loss: 339.3568\n",
      "Epoch 237/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 613.4044 - val_loss: 333.1892\n",
      "Epoch 238/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 605.1711 - val_loss: 327.0974\n",
      "Epoch 239/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 597.0205 - val_loss: 321.0809\n",
      "Epoch 240/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 588.9515 - val_loss: 315.1392\n",
      "Epoch 241/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 580.9635 - val_loss: 309.2721\n",
      "Epoch 242/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 573.0566 - val_loss: 303.4790\n",
      "Epoch 243/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 565.2301 - val_loss: 297.7582\n",
      "Epoch 244/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 557.4828 - val_loss: 292.1110\n",
      "Epoch 245/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 549.8150 - val_loss: 286.5356\n",
      "Epoch 246/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 542.2258 - val_loss: 281.0323\n",
      "Epoch 247/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 534.7146 - val_loss: 275.5999\n",
      "Epoch 248/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 527.2811 - val_loss: 270.2384\n",
      "Epoch 249/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 519.9250 - val_loss: 264.9467\n",
      "Epoch 250/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 512.6454 - val_loss: 259.7248\n",
      "Epoch 251/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 505.4416 - val_loss: 254.5719\n",
      "Epoch 252/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 498.3136 - val_loss: 249.4877\n",
      "Epoch 253/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 491.2608 - val_loss: 244.4718\n",
      "Epoch 254/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 484.2832 - val_loss: 239.5237\n",
      "Epoch 255/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 477.3792 - val_loss: 234.6423\n",
      "Epoch 256/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 470.5491 - val_loss: 229.8284\n",
      "Epoch 257/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 463.7923 - val_loss: 225.0801\n",
      "Epoch 258/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 457.1081 - val_loss: 220.3977\n",
      "Epoch 259/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 450.4963 - val_loss: 215.7808\n",
      "Epoch 260/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 443.9563 - val_loss: 211.2282\n",
      "Epoch 261/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 437.4875 - val_loss: 206.7401\n",
      "Epoch 262/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 431.0896 - val_loss: 202.3155\n",
      "Epoch 263/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 424.7621 - val_loss: 197.9543\n",
      "Epoch 264/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 418.5042 - val_loss: 193.6556\n",
      "Epoch 265/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 412.3159 - val_loss: 189.4198\n",
      "Epoch 266/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 406.1966 - val_loss: 185.2454\n",
      "Epoch 267/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 400.1458 - val_loss: 181.1322\n",
      "Epoch 268/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 394.1628 - val_loss: 177.0803\n",
      "Epoch 269/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 388.2476 - val_loss: 173.0890\n",
      "Epoch 270/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 382.3997 - val_loss: 169.1574\n",
      "Epoch 271/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 376.6182 - val_loss: 165.2852\n",
      "Epoch 272/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 370.9028 - val_loss: 161.4722\n",
      "Epoch 273/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 365.2537 - val_loss: 157.7177\n",
      "Epoch 274/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 359.6695 - val_loss: 154.0210\n",
      "Epoch 275/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 354.1502 - val_loss: 150.3824\n",
      "Epoch 276/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 348.6953 - val_loss: 146.8006\n",
      "Epoch 277/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 343.3042 - val_loss: 143.2755\n",
      "Epoch 278/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 337.9766 - val_loss: 139.8069\n",
      "Epoch 279/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 332.7124 - val_loss: 136.3938\n",
      "Epoch 280/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 327.5104 - val_loss: 133.0358\n",
      "Epoch 281/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 322.3705 - val_loss: 129.7328\n",
      "Epoch 282/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 317.2923 - val_loss: 126.4841\n",
      "Epoch 283/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 312.2754 - val_loss: 123.2894\n",
      "Epoch 284/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 307.3194 - val_loss: 120.1481\n",
      "Epoch 285/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 302.4235 - val_loss: 117.0597\n",
      "Epoch 286/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 297.5874 - val_loss: 114.0235\n",
      "Epoch 287/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 292.8109 - val_loss: 111.0396\n",
      "Epoch 288/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 288.0933 - val_loss: 108.1072\n",
      "Epoch 289/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 283.4342 - val_loss: 105.2261\n",
      "Epoch 290/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 278.8334 - val_loss: 102.3958\n",
      "Epoch 291/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 274.2904 - val_loss: 99.6155\n",
      "Epoch 292/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 269.8045 - val_loss: 96.8850\n",
      "Epoch 293/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 265.3754 - val_loss: 94.2037\n",
      "Epoch 294/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 261.0027 - val_loss: 91.5716\n",
      "Epoch 295/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 256.6861 - val_loss: 88.9878\n",
      "Epoch 296/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 252.4250 - val_loss: 86.4519\n",
      "Epoch 297/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 248.2188 - val_loss: 83.9633\n",
      "Epoch 298/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 244.0671 - val_loss: 81.5219\n",
      "Epoch 299/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 239.9697 - val_loss: 79.1272\n",
      "Epoch 300/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 235.9260 - val_loss: 76.7786\n",
      "Epoch 301/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 231.9357 - val_loss: 74.4757\n",
      "Epoch 302/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 227.9982 - val_loss: 72.2178\n",
      "Epoch 303/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 224.1131 - val_loss: 70.0048\n",
      "Epoch 304/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 220.2800 - val_loss: 67.8363\n",
      "Epoch 305/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 216.4988 - val_loss: 65.7120\n",
      "Epoch 306/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 212.7689 - val_loss: 63.6309\n",
      "Epoch 307/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 209.0895 - val_loss: 61.5928\n",
      "Epoch 308/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 205.4604 - val_loss: 59.5972\n",
      "Epoch 309/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 201.8811 - val_loss: 57.6436\n",
      "Epoch 310/500\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 198.3512 - val_loss: 55.7320\n",
      "Epoch 311/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 194.8705 - val_loss: 53.8616\n",
      "Epoch 312/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 191.4384 - val_loss: 52.0318\n",
      "Epoch 313/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 188.0544 - val_loss: 50.2427\n",
      "Epoch 314/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 184.7183 - val_loss: 48.4933\n",
      "Epoch 315/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 181.4295 - val_loss: 46.7837\n",
      "Epoch 316/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 178.1877 - val_loss: 45.1130\n",
      "Epoch 317/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 174.9925 - val_loss: 43.4809\n",
      "Epoch 318/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 171.8430 - val_loss: 41.8869\n",
      "Epoch 319/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 168.7395 - val_loss: 40.3307\n",
      "Epoch 320/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 165.6812 - val_loss: 38.8121\n",
      "Epoch 321/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 162.6678 - val_loss: 37.3302\n",
      "Epoch 322/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 159.6989 - val_loss: 35.8847\n",
      "Epoch 323/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 156.7738 - val_loss: 34.4753\n",
      "Epoch 324/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 153.8925 - val_loss: 33.1015\n",
      "Epoch 325/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 151.0542 - val_loss: 31.7629\n",
      "Epoch 326/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 148.2589 - val_loss: 30.4592\n",
      "Epoch 327/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 145.5060 - val_loss: 29.1897\n",
      "Epoch 328/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 142.7949 - val_loss: 27.9541\n",
      "Epoch 329/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 140.1255 - val_loss: 26.7520\n",
      "Epoch 330/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 137.4972 - val_loss: 25.5830\n",
      "Epoch 331/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 134.9096 - val_loss: 24.4466\n",
      "Epoch 332/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 132.3625 - val_loss: 23.3424\n",
      "Epoch 333/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 129.8551 - val_loss: 22.2700\n",
      "Epoch 334/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 127.3872 - val_loss: 21.2289\n",
      "Epoch 335/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 124.9584 - val_loss: 20.2187\n",
      "Epoch 336/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 122.5685 - val_loss: 19.2392\n",
      "Epoch 337/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 120.2168 - val_loss: 18.2898\n",
      "Epoch 338/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 117.9031 - val_loss: 17.3700\n",
      "Epoch 339/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 115.6268 - val_loss: 16.4795\n",
      "Epoch 340/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 113.3876 - val_loss: 15.6181\n",
      "Epoch 341/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 111.1853 - val_loss: 14.7849\n",
      "Epoch 342/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 109.0192 - val_loss: 13.9800\n",
      "Epoch 343/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 106.8891 - val_loss: 13.2027\n",
      "Epoch 344/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 104.7946 - val_loss: 12.4526\n",
      "Epoch 345/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 102.7351 - val_loss: 11.7294\n",
      "Epoch 346/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 100.7105 - val_loss: 11.0326\n",
      "Epoch 347/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 98.7202 - val_loss: 10.3619\n",
      "Epoch 348/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 96.7639 - val_loss: 9.7169\n",
      "Epoch 349/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 94.8413 - val_loss: 9.0971\n",
      "Epoch 350/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 92.9517 - val_loss: 8.5022\n",
      "Epoch 351/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 91.0951 - val_loss: 7.9317\n",
      "Epoch 352/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 89.2709 - val_loss: 7.3852\n",
      "Epoch 353/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 87.4786 - val_loss: 6.8625\n",
      "Epoch 354/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 85.7180 - val_loss: 6.3631\n",
      "Epoch 355/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 83.9889 - val_loss: 5.8865\n",
      "Epoch 356/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 82.2907 - val_loss: 5.4325\n",
      "Epoch 357/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 80.6230 - val_loss: 5.0006\n",
      "Epoch 358/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 78.9855 - val_loss: 4.5905\n",
      "Epoch 359/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 77.3777 - val_loss: 4.2017\n",
      "Epoch 360/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 75.7994 - val_loss: 3.8339\n",
      "Epoch 361/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 74.2502 - val_loss: 3.4868\n",
      "Epoch 362/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 72.7298 - val_loss: 3.1599\n",
      "Epoch 363/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 71.2377 - val_loss: 2.8528\n",
      "Epoch 364/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 69.7734 - val_loss: 2.5653\n",
      "Epoch 365/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 68.3369 - val_loss: 2.2969\n",
      "Epoch 366/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 66.9276 - val_loss: 2.0473\n",
      "Epoch 367/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 65.5453 - val_loss: 1.8161\n",
      "Epoch 368/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 64.1894 - val_loss: 1.6029\n",
      "Epoch 369/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 62.8597 - val_loss: 1.4074\n",
      "Epoch 370/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 61.5559 - val_loss: 1.2293\n",
      "Epoch 371/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 60.2776 - val_loss: 1.0681\n",
      "Epoch 372/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 59.0242 - val_loss: 0.9235\n",
      "Epoch 373/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 57.7958 - val_loss: 0.7952\n",
      "Epoch 374/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 56.5917 - val_loss: 0.6829\n",
      "Epoch 375/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 55.4119 - val_loss: 0.5861\n",
      "Epoch 376/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 54.2558 - val_loss: 0.5046\n",
      "Epoch 377/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 53.1232 - val_loss: 0.4379\n",
      "Epoch 378/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 52.0136 - val_loss: 0.3858\n",
      "Epoch 379/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 50.9268 - val_loss: 0.3480\n",
      "Epoch 380/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 49.8625 - val_loss: 0.3240\n",
      "Epoch 381/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 48.8201 - val_loss: 0.3137\n",
      "Epoch 382/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 47.7995 - val_loss: 0.3165\n",
      "Epoch 383/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 46.8003 - val_loss: 0.3323\n",
      "Epoch 384/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 45.8222 - val_loss: 0.3606\n",
      "Epoch 385/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 44.8649 - val_loss: 0.4013\n",
      "Epoch 386/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 43.9281 - val_loss: 0.4539\n",
      "Epoch 387/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 43.0114 - val_loss: 0.5181\n",
      "Epoch 388/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 42.1147 - val_loss: 0.5937\n",
      "Epoch 389/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 41.2374 - val_loss: 0.6803\n",
      "Epoch 390/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 40.3793 - val_loss: 0.7776\n",
      "Epoch 391/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 39.5401 - val_loss: 0.8853\n",
      "Epoch 392/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 38.7194 - val_loss: 1.0032\n",
      "Epoch 393/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 37.9171 - val_loss: 1.1309\n",
      "Epoch 394/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 37.1328 - val_loss: 1.2682\n",
      "Epoch 395/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 36.3663 - val_loss: 1.4146\n",
      "Epoch 396/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 35.6170 - val_loss: 1.5701\n",
      "Epoch 397/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 34.8849 - val_loss: 1.7342\n",
      "Epoch 398/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 34.1697 - val_loss: 1.9067\n",
      "Epoch 399/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 33.4709 - val_loss: 2.0874\n",
      "Epoch 400/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 32.7883 - val_loss: 2.2759\n",
      "Epoch 401/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 32.1218 - val_loss: 2.4720\n",
      "Epoch 402/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 31.4709 - val_loss: 2.6754\n",
      "Epoch 403/500\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 30.8355 - val_loss: 2.8858\n",
      "Epoch 404/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 30.2151 - val_loss: 3.1031\n",
      "Epoch 405/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 29.6096 - val_loss: 3.3268\n",
      "Epoch 406/500\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 29.0187 - val_loss: 3.5569\n",
      "Epoch 407/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 28.4421 - val_loss: 3.7929\n",
      "Epoch 408/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 27.8796 - val_loss: 4.0348\n",
      "Epoch 409/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 27.3308 - val_loss: 4.2822\n",
      "Epoch 410/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 26.7956 - val_loss: 4.5348\n",
      "Epoch 411/500\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 26.2737 - val_loss: 4.7926\n",
      "Epoch 412/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 25.7648 - val_loss: 5.0551\n",
      "Epoch 413/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 25.2686 - val_loss: 5.3223\n",
      "Epoch 414/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 24.7850 - val_loss: 5.5939\n",
      "Epoch 415/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 24.3137 - val_loss: 5.8695\n",
      "Epoch 416/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 23.8544 - val_loss: 6.1491\n",
      "Epoch 417/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 23.4069 - val_loss: 6.4325\n",
      "Epoch 418/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 22.9710 - val_loss: 6.7193\n",
      "Epoch 419/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 22.5464 - val_loss: 7.0093\n",
      "Epoch 420/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 22.1329 - val_loss: 7.3025\n",
      "Epoch 421/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 21.7303 - val_loss: 7.5986\n",
      "Epoch 422/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 21.3383 - val_loss: 7.8974\n",
      "Epoch 423/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 20.9568 - val_loss: 8.1987\n",
      "Epoch 424/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 20.5855 - val_loss: 8.5021\n",
      "Epoch 425/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 20.2241 - val_loss: 8.8078\n",
      "Epoch 426/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 19.8726 - val_loss: 9.1155\n",
      "Epoch 427/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 19.5305 - val_loss: 9.4248\n",
      "Epoch 428/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 19.1979 - val_loss: 9.7357\n",
      "Epoch 429/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 18.8744 - val_loss: 10.0480\n",
      "Epoch 430/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 18.5598 - val_loss: 10.3616\n",
      "Epoch 431/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 18.2541 - val_loss: 10.6761\n",
      "Epoch 432/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.9568 - val_loss: 10.9916\n",
      "Epoch 433/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 17.6680 - val_loss: 11.3078\n",
      "Epoch 434/500\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 17.3873 - val_loss: 11.6247\n",
      "Epoch 435/500\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 17.1146 - val_loss: 11.9420\n",
      "Epoch 436/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 16.8497 - val_loss: 12.2595\n",
      "Epoch 437/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.5924 - val_loss: 12.5771\n",
      "Epoch 438/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.3426 - val_loss: 12.8948\n",
      "Epoch 439/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 16.1000 - val_loss: 13.2125\n",
      "Epoch 440/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 15.8645 - val_loss: 13.5297\n",
      "Epoch 441/500\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 15.6359 - val_loss: 13.8465\n",
      "Epoch 442/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 15.4141 - val_loss: 14.1627\n",
      "Epoch 443/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 15.1989 - val_loss: 14.4784\n",
      "Epoch 444/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 14.9902 - val_loss: 14.7932\n",
      "Epoch 445/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 14.7877 - val_loss: 15.1073\n",
      "Epoch 446/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 14.5913 - val_loss: 15.4202\n",
      "Epoch 447/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 14.4008 - val_loss: 15.7320\n",
      "Epoch 448/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 14.2162 - val_loss: 16.0427\n",
      "Epoch 449/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 14.0372 - val_loss: 16.3519\n",
      "Epoch 450/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 13.8637 - val_loss: 16.6598\n",
      "Epoch 451/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 13.6957 - val_loss: 16.9661\n",
      "Epoch 452/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 13.5328 - val_loss: 17.2707\n",
      "Epoch 453/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 13.3750 - val_loss: 17.5738\n",
      "Epoch 454/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 13.2222 - val_loss: 17.8749\n",
      "Epoch 455/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 13.0742 - val_loss: 18.1744\n",
      "Epoch 456/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 12.9309 - val_loss: 18.4716\n",
      "Epoch 457/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 12.7922 - val_loss: 18.7669\n",
      "Epoch 458/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 12.6579 - val_loss: 19.0601\n",
      "Epoch 459/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 12.5280 - val_loss: 19.3511\n",
      "Epoch 460/500\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 12.4022 - val_loss: 19.6398\n",
      "Epoch 461/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 12.2806 - val_loss: 19.9262\n",
      "Epoch 462/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 12.1629 - val_loss: 20.2101\n",
      "Epoch 463/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 12.0491 - val_loss: 20.4915\n",
      "Epoch 464/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 11.9390 - val_loss: 20.7707\n",
      "Epoch 465/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 11.8326 - val_loss: 21.0471\n",
      "Epoch 466/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 11.7297 - val_loss: 21.3208\n",
      "Epoch 467/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 11.6303 - val_loss: 21.5919\n",
      "Epoch 468/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 11.5342 - val_loss: 21.8603\n",
      "Epoch 469/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 11.4413 - val_loss: 22.1260\n",
      "Epoch 470/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 11.3515 - val_loss: 22.3886\n",
      "Epoch 471/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 11.2649 - val_loss: 22.6485\n",
      "Epoch 472/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 11.1812 - val_loss: 22.9055\n",
      "Epoch 473/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 11.1004 - val_loss: 23.1597\n",
      "Epoch 474/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 11.0223 - val_loss: 23.4110\n",
      "Epoch 475/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 10.9469 - val_loss: 23.6592\n",
      "Epoch 476/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 10.8742 - val_loss: 23.9045\n",
      "Epoch 477/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 10.8040 - val_loss: 24.1465\n",
      "Epoch 478/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 10.7362 - val_loss: 24.3858\n",
      "Epoch 479/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 10.6708 - val_loss: 24.6216\n",
      "Epoch 480/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 10.6078 - val_loss: 24.8546\n",
      "Epoch 481/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 10.5469 - val_loss: 25.0844\n",
      "Epoch 482/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 10.4883 - val_loss: 25.3111\n",
      "Epoch 483/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 10.4316 - val_loss: 25.5347\n",
      "Epoch 484/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 10.3771 - val_loss: 25.7552\n",
      "Epoch 485/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 10.3245 - val_loss: 25.9725\n",
      "Epoch 486/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 10.2737 - val_loss: 26.1866\n",
      "Epoch 487/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 10.2248 - val_loss: 26.3975\n",
      "Epoch 488/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 10.1777 - val_loss: 26.6053\n",
      "Epoch 489/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 10.1323 - val_loss: 26.8100\n",
      "Epoch 490/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 10.0885 - val_loss: 27.0115\n",
      "Epoch 491/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 10.0463 - val_loss: 27.2098\n",
      "Epoch 492/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 10.0057 - val_loss: 27.4050\n",
      "Epoch 493/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 9.9666 - val_loss: 27.5970\n",
      "Epoch 494/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 9.9289 - val_loss: 27.7859\n",
      "Epoch 495/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 9.8926 - val_loss: 27.9717\n",
      "Epoch 496/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 9.8577 - val_loss: 28.1541\n",
      "Epoch 497/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 9.8241 - val_loss: 28.3338\n",
      "Epoch 498/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 9.7917 - val_loss: 28.5100\n",
      "Epoch 499/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 9.7605 - val_loss: 28.6832\n",
      "Epoch 500/500\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 9.7305 - val_loss: 28.8534\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(86.8407, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0015, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0001, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0118, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(2.5775, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 344ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.23748366e+01, 7.23356209e+01, 7.22964052e+01, 7.22571895e+01,\n",
       "        7.22179739e+01, 7.21787582e+01, 7.21395425e+01, 7.21003268e+01,\n",
       "        7.20611111e+01, 7.20218954e+01, 7.19826797e+01, 7.19434641e+01,\n",
       "        7.19042484e+01, 7.18650327e+01, 7.18258170e+01, 7.17818161e+01,\n",
       "        7.17285948e+01, 7.16753735e+01, 7.16221522e+01, 7.15689309e+01,\n",
       "        7.15157096e+01, 7.14624883e+01, 7.14092670e+01, 7.13560458e+01,\n",
       "        7.13028245e+01, 7.12496032e+01, 7.11963819e+01, 7.11431606e+01,\n",
       "        7.10899393e+01, 7.10367180e+01, 7.09834967e+01, 7.09302754e+01,\n",
       "        7.08770542e+01, 7.08238329e+01, 7.07706116e+01, 7.07173903e+01,\n",
       "        7.06641690e+01, 7.06109477e+01, 7.05577264e+01, 7.05045051e+01,\n",
       "        7.04512838e+01, 7.03980626e+01, 7.03448413e+01, 7.02916200e+01,\n",
       "        7.02383987e+01, 7.01851774e+01, 7.01319561e+01, 7.00787348e+01,\n",
       "        7.00255135e+01, 6.99722922e+01, 6.99190710e+01, 6.98964052e+01,\n",
       "        6.98908030e+01, 6.98852007e+01, 6.98795985e+01, 6.98739963e+01,\n",
       "        6.98683940e+01, 6.98627918e+01, 6.98571895e+01, 6.98515873e+01,\n",
       "        6.98459851e+01, 6.98403828e+01, 6.98347806e+01, 6.98291783e+01,\n",
       "        6.98235761e+01, 6.98179739e+01, 6.98123716e+01, 6.98067694e+01,\n",
       "        6.98011671e+01, 6.97955649e+01, 6.97899627e+01, 6.97843604e+01,\n",
       "        6.97787582e+01, 6.97731559e+01, 6.97675537e+01, 6.97619514e+01,\n",
       "        6.97563492e+01, 6.97507470e+01, 6.97451447e+01, 6.97395425e+01,\n",
       "        7.56149979e+01, 0.00000000e+00, 3.49429607e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.16309315e-01, 0.00000000e+00, 2.65566498e-01,\n",
       "        6.11796200e-01, 2.89798886e-01, 1.03668118e+00, 2.80301392e-01,\n",
       "        0.00000000e+00, 7.14013502e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.37733370e-01, 0.00000000e+00, 2.93034706e-02]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([68.54278711, 68.53251634, 68.52224556, 68.51197479, 68.50170401,\n",
       "       68.49143324, 68.48116246, 68.47089169, 68.46062092, 68.45035014,\n",
       "       68.44007937, 68.42980859, 68.41953782, 68.40926704, 68.39899627,\n",
       "       68.38872549, 68.37845472, 68.36818394, 68.35791317, 68.34764239,\n",
       "       68.33737162, 68.32710084, 68.31683007, 68.30655929, 68.29628852,\n",
       "       68.28601774, 68.27574697, 68.26547619, 68.25520542, 68.24493464,\n",
       "       68.23466387, 68.22439309, 68.21412232, 68.20385154, 68.1912465 ,\n",
       "       68.1772409 , 68.16323529, 68.14922969, 68.13522409, 68.12121849,\n",
       "       68.10721289, 68.09320728, 68.07920168, 68.06519608, 68.05119048,\n",
       "       68.03718487, 68.02317927, 68.00917367, 67.99516807, 67.98116246,\n",
       "       67.96715686, 67.95315126, 67.93914566, 67.92514006, 67.91113445,\n",
       "       67.89712885, 67.88312325, 67.86911765, 67.85511204, 67.84110644,\n",
       "       67.82710084, 67.81309524, 67.79908964, 67.78508403, 67.77107843,\n",
       "       67.75707283, 67.74306723, 67.72906162, 67.71505602, 67.70105042,\n",
       "       67.68704482, 67.67303922, 67.65903361, 67.64502801, 67.63102241,\n",
       "       67.61701681, 67.6030112 , 67.5890056 , 67.575     , 67.5609944 ,\n",
       "       67.5469888 , 67.53298319, 67.51897759, 67.50497199, 67.49096639,\n",
       "       67.47696078, 67.46295518, 67.44894958, 67.43494398, 67.42093838,\n",
       "       67.40693277, 67.39292717, 67.37892157, 67.36491597, 67.35091036,\n",
       "       67.33690476, 67.32289916, 67.30889356, 67.29488796, 67.28088235])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.41923407131468\n",
      "14.954591228793085\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
