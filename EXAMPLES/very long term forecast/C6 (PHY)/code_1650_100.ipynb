{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCGKeZ2gyuoQ"
   },
   "source": [
    "_Importing Required Libraries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-6LN-zXiLcM",
    "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "By_d9uXpaFvZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from hampel import hampel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOjBMFayuoR"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5QqIY_GyuoR"
   },
   "source": [
    "The `capa_intermittency.dat` feeds the model with the dynamics of the Capacitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9dV4a8yfyuoR"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('capa_intermittency.dat')\n",
    "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
    "training_set = training_set.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7easoxByuoR"
   },
   "source": [
    "## Computing the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SnyolJTyuoR"
   },
   "source": [
    "_Calculating the value of_ $\\frac{dx}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmIbVfIvyuoR",
    "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1        0.000298\n",
      "2        0.000298\n",
      "3        0.000297\n",
      "4        0.000297\n",
      "5        0.000297\n",
      "           ...   \n",
      "9996     0.000018\n",
      "9997     0.000018\n",
      "9998     0.000018\n",
      "9999     0.000018\n",
      "10000    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t_diff = 1\n",
    "print(training_set.max())\n",
    "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2eVeeoxyuoS"
   },
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0J-NKyIEyuoS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       84.600000\n",
       "1       84.431933\n",
       "2       84.263866\n",
       "3       84.095798\n",
       "4       83.927731\n",
       "          ...    \n",
       "1745    53.961134\n",
       "1746    53.952731\n",
       "1747    53.944328\n",
       "1748    53.935924\n",
       "1749    53.927521\n",
       "Name: C6, Length: 1750, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"c6_interpolated_1650_100.csv\")\n",
    "training_set = data.iloc[:, 1]\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CbNUhJ74UqF",
    "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       84.600000\n",
       "1       84.431933\n",
       "2       84.263866\n",
       "3       84.095798\n",
       "4       83.927731\n",
       "          ...    \n",
       "1645     0.000000\n",
       "1646     0.000000\n",
       "1647     0.382706\n",
       "1648     0.000000\n",
       "1649     0.000000\n",
       "Name: C6, Length: 1650, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = training_set.tail(100)\n",
    "test\n",
    "training_set = training_set.head(1650)\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0TwTcq0yuoS",
    "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000298\n",
      "1       0.000298\n",
      "2       0.000297\n",
      "3       0.000297\n",
      "4       0.000297\n",
      "          ...   \n",
      "9995    0.000018\n",
      "9996    0.000018\n",
      "9997    0.000018\n",
      "9998    0.000018\n",
      "9999    0.000018\n",
      "Name: 0, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "training_set = training_set.reset_index(drop=True)\n",
    "gradient_t = gradient_t.reset_index(drop=True)\n",
    "print(gradient_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2biznZQyuoS"
   },
   "outputs": [],
   "source": [
    "df = pd.concat((training_set, gradient_t), axis=1)\n",
    "df.columns = ['y_t', 'grad_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sk_a5v3tyuoS",
    "outputId": "17563625-e550-45ae-faab-fafa353e44da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_t</th>\n",
       "      <th>grad_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84.600000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.431933</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84.263866</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84.095798</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83.927731</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y_t    grad_t\n",
       "0     84.600000  0.000298\n",
       "1     84.431933  0.000298\n",
       "2     84.263866  0.000297\n",
       "3     84.095798  0.000297\n",
       "4     83.927731  0.000297\n",
       "...         ...       ...\n",
       "9995        NaN  0.000018\n",
       "9996        NaN  0.000018\n",
       "9997        NaN  0.000018\n",
       "9998        NaN  0.000018\n",
       "9999        NaN  0.000018\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5esyHu5aFvg"
   },
   "source": [
    "## Plot of the External Forcing from Chaotic Differential Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "hGnE43tOh-4p",
    "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.iloc[:, 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ym4xWUUxaFvg",
    "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag Dutta\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAAsTAAALEwEAmpwYAAAtV0lEQVR4nO3dd3hUZdrH8e+dSSOFdFoSSIBQAlIj0tcGYgNXUdFVcWXFtZfdtb6rrvvurmVX0dVdxbXvq6jYcFdFQMEGSuhFIAEpCS30XkLu9485CUNIIJmZZCaZ+3Ndc83MmXPO3DOE85vneU4RVcUYY0zoCgt0AcYYYwLLgsAYY0KcBYExxoQ4CwJjjAlxFgTGGBPiwgNdgDdSU1M1Kysr0GUYY0yDMmfOnC2qmlZ5eoMMgqysLPLz8wNdhjHGNCgisqaq6dY1ZIwxIc6CwBhjQpwFgTHGhDgLAmOMCXEWBMYYE+IsCIwxJsRZEBhjTIjzSxCIyDARWS4ihSJybxWv3yUiS0VkoYhME5E2Hq+NFpEC5zbaH/VU5/WZq/l4wfq6fAtjjGlwfA4CEXEBzwHnArnAFSKSW2m2eUCeqnYDJgKPO8smAw8BpwF9gIdEJMnXmqrzTv463p1TVFerN8aYBskfLYI+QKGqrlLVQ8AEYITnDKr6paruc57OAjKcx+cAU1R1m6puB6YAw/xQU5VymsVTuGl3Xa3eGGMaJH8EQTqwzuN5kTOtOmOAT2u7rIiMFZF8EckvKSnxqtD2zeJYv/MAew6WerW8McY0RvU6WCwiVwF5wBO1XVZVx6tqnqrmpaUdd86kGmnfLA6AlZv3eLW8McY0Rv4IgmIg0+N5hjPtGCJyNvAAMFxVD9ZmWX/JcYKgwILAGGMq+CMIZgM5IpItIpHAKGCS5wwi0hN4AXcIbPZ4aTIwVESSnEHioc60OtE6OYZIVxiFFgTGGFPB59NQq2qpiNyCewPuAl5W1SUi8giQr6qTcHcFxQHvigjAWlUdrqrbROSPuMME4BFV3eZrTdUJd4WRnRpL4WYbMDbGmHJ+uR6Bqn4CfFJp2oMej88+wbIvAy/7o46aaN88jsXFO+vr7YwxJuiF3JHF7dPiWLdtHwcOHwl0KcYYExRCLghymsdRprCqZG+gSzHGmKAQekHQLB6AAhsnMMYYIASDICs1hvAwYdlGCwJjjIEQDIKocBfdMxOZuXJroEsxxpigEHJBADCwfSoLi3awc9/hQJdijDEBF5pBkJNKmcLMVdYqMMaYkAyCHpmJxEa6+LZwS6BLMcaYgAvJIIhwhXFa2xQLAmOMIUSDAGBA+1RWbdlL8Y79gS7FGGMCKmSDYGD7VAC+LbBWgTEmtIVsEHRoHkdafBTfWPeQMSbEhWwQiAgD26fybeEWyso00OUYY0zAhGwQgHucYOveQyy36xgbY0JYiAdBCgDf2DiBMSaEhXQQtExoQqcW8bz63Wq27T0U6HKMMSYgQjoIAB4f2Y2SPQe57a15HLGxAmNMCAr5IOiWkcj/jujKN4Vb+OvnywNdjjHG1LuQDwKAy07N5MrTWvPP6Sv5bPGGQJdjjDH1yoLA8dCFufTITOQ37yygcPOeQJdjjDH1xoLAERXu4p9X9SI6wsUNb+Sz52BpoEsyxph64ZcgEJFhIrJcRApF5N4qXh8sInNFpFRERlZ67YiIzHduk/xRj7daJjTh2St7sXrrPn77zgJUbfDYGNP4+RwEIuICngPOBXKBK0Qkt9Jsa4FrgTerWMV+Ve3h3Ib7Wo+v+rVL4b5zO/HZko08P2NVoMsxxpg6548WQR+gUFVXqeohYAIwwnMGVV2tqguBMj+8X50bMzCbC7q15InJy+xgM2NMo+ePIEgH1nk8L3Km1VS0iOSLyCwRuai6mURkrDNffklJiZel1oyI8Ngl3WjfLI5b35pL0fZ9dfp+xhgTSMEwWNxGVfOAK4FxItKuqplUdbyq5qlqXlpaWp0XFRsVzgtX51F6RLnx33M5cPhInb+nMcYEgj+CoBjI9Hie4UyrEVUtdu5XAdOBnn6oyS+yU2N58vIeLCreycOTlgS6HGOMqRP+CILZQI6IZItIJDAKqNHePyKSJCJRzuNUYACw1A81+c2Q3ObcdHo7Jsxex8cL1ge6HGOM8Tufg0BVS4FbgMnAj8A7qrpERB4RkeEAInKqiBQBlwIviEj5z+vOQL6ILAC+BB5V1aAKAoA7h3SgZ+tE7n9/Eeu22XiBMaZxkYa4r3xeXp7m5+fX63uu27aP857+mvbN43jnhn5EuIJheMUYY2pOROY4Y7LHsK1ZDWUmx/CXS05h3todjJu6ItDlGGOM31gQ1MIF3Vox6tRM/jF9Jd/ZtY6NMY2EBUEtPXhhLm1TY7nj7fns3H840OUYY4zPLAhqKSYynHGX92Tz7oOM/2ploMsxxhifWRB44ZSMBC7s3oqXv1lNye6DgS7HGGN8YkHgpTvPzuHQkTKe+7Iw0KUYY4xPLAi81DYtjkt7Z/Dm92vtXETGmAbNgsAHt52VAwJPTy0IdCnGGOM1CwIftEpswtV92/De3CK7vKUxpsGyIPDRTae3o0mEi6em2EFmxpiGyYLARylxUYwZmM1/F21gcfHOQJdjjDG1ZkHgB78a3JbEmAiemLw80KUYY0ytWRD4QdPoCG78WTtmrCjh+1VbA12OMcbUigWBn1zTL4tm8VE8MXk5DfGMrsaY0GVB4CdNIl3celYO+Wu2825+UaDLMcaYGrMg8KPL8zLpk53M3e8t5MkpKygrs5aBMSb4WRD4UWR4GG+M6cPI3hk8M62AWyfMY/8hu+i9MSa4hQe6gMYmKtzFEyO7kdMsjkc/W8a6bft48Zo8mjeNDnRpxhhTJWsR1AER4YaftWP81XkUbt7D8Ge/YVGRHWNgjAlOFgR1aEhuc967sT/hYWFc+sJ3fLJoQ6BLMsaY41gQ1LHOLZvy4c0DyG3ZlJv+by5/n1Zgu5caY4KKBUE9SIuP4s3r+3JRj1b8bcoK7nh7PgcO2yCyMSY4+CUIRGSYiCwXkUIRubeK1weLyFwRKRWRkZVeGy0iBc5ttD/qCUbRES6eurwHvzunIx/NX8+o8bPYvPtAoMsyxhjfg0BEXMBzwLlALnCFiORWmm0tcC3wZqVlk4GHgNOAPsBDIpLka03BSkS4+Yz2PH9VL5Zv3M1Fz37L0vW7Al2WMSbE+aNF0AcoVNVVqnoImACM8JxBVVer6kKgrNKy5wBTVHWbqm4HpgDD/FBTUBvWtSXv/rofZQojn/+OKUs3BbokY0wI80cQpAPrPJ4XOdP8uqyIjBWRfBHJLykp8arQYNI1PYGPbhlA+2ZxjH0jn+dnrLRBZGNMQDSYwWJVHa+qeaqal5aWFuhy/KJ502jeHtuP805pyaOfLuM37y6wQWRjTL3zRxAUA5kezzOcaXW9bKPQJNLFs1f05M6zO/D+3GIue2Em63fsD3RZxpgQ4o8gmA3kiEi2iEQCo4BJNVx2MjBURJKcQeKhzrSQIiLcfnYOL16Tx6qSvQx/9hu7roExpt74HASqWgrcgnsD/iPwjqouEZFHRGQ4gIicKiJFwKXACyKyxFl2G/BH3GEyG3jEmRaShuQ258Ob+9M0OoJf/Ot73pi52sYNjDF1ThrihiYvL0/z8/MDXUad2XXgMHdMmM8XyzZzeV4mj1zUhahwV6DLMsY0cCIyR1XzKk9vMIPFoaRpdAT/uiaPW89sz9v567j8hVls3GkHnxlj6oYFQZAKCxN+M7Qjz1/VixWbdnPhs98wZ03I9poZY+qQBUGQG9a1JR/cNICYSBejxs/ize/XBrokY0wjY0HQAHRsEc+kmwfSv10q93+wiPs/WMSh0soHaRtjjHcsCBqIhJgIXr72VG48vR1vfr+WK1+0k9YZY/zDgqABcYUJ9wzrxLNX9mTJ+l1c+PdvmLd2e6DLMsY0cBYEDdAF3Vrx/k39iQwP4/IXZvFO/rqTL2SMMdWwIGigOrdsyqSbB9InO5m7Jy7kwY8W27iBMcYrFgQNWFJsJK/+8lTGDm7L6zPX2HmKjDFesSBo4MJdYdx/Xmf+8YteFG7ew/nPfM2MFQ3/NN3GmPpjQdBInHdKSybdMoDmTaO59pUf+ON/lrJ1z8FAl2WMaQAsCBqRtmlxfHDTAEadmskr3/7EoMe/5LHPlrF976FAl2aMCWJ20rlGqnDzHp6ZVsDHC9cTE+Hi2gFZXD+oLYkxkYEuzRgTINWddM6CoJEr2LSbcdMK+GTRBmIjw7luQBZjBrYlISYi0KUZY+qZBUGIW75xN09PW8EnizYSHx3OdQOyuW5gNglNLBCMCRUWBAaAHzfsYtzUFUxesomm0eH8alBbfjkgi/hoCwRjGjsLAnOMJet3Mm5qAVOWbiKhSQTXD8rm2gHZxEWFB7o0Y0wdsSAwVVpUtJNxU1cwbdlmkmIiuH5wW0b3yyLWAsGYRseCwJzQgnU7GDd1BV8uLyE5NpKxg9tyTb82xERaIBjTWFgQmBqZt3Y7T00t4KsVJaTGRXLD4HZc1bcNTSLtmsnGNHQWBKZW5qzZzripK/i6YAtp8VG8fl0fOrdsGuiyjDE+qNOL14vIMBFZLiKFInJvFa9Hicjbzuvfi0iWMz1LRPaLyHzn9rw/6jG+690miTfGnMa7v+6HALe+NY/9h44EuixjTB3wOQhExAU8B5wL5AJXiEhupdnGANtVtT3wFPCYx2srVbWHc/u1r/UY/zo1K5knL+tB4eY9/PmTHwNdjjGmDvijRdAHKFTVVap6CJgAjKg0zwjgNefxROAsERE/vLepBwNzUrl+UDZvzFrD1KWbAl2OMcbP/BEE6YDnJbKKnGlVzqOqpcBOIMV5LVtE5onIDBEZVN2biMhYEckXkfySEjvNcn377TkdyW3ZlLvfW2jXSjamkQn02Uc3AK1VtSdwF/CmiFQ5Iqmq41U1T1Xz0tLS6rVIA1HhLp65ogd7D5by23cXUlbW8HYyMMZUzR9BUAxkejzPcKZVOY+IhAMJwFZVPaiqWwFUdQ6wEujgh5pMHWjfLJ7/uSCXr1aU8Op3qwNdjjHGT/wRBLOBHBHJFpFIYBQwqdI8k4DRzuORwBeqqiKS5gw2IyJtgRxglR9qMnXkqtNac3bnZjz66TJ+3LAr0OUYY/zA5yBw+vxvASYDPwLvqOoSEXlERIY7s70EpIhIIe4uoPJdTAcDC0VkPu5B5F+r6jZfazJ1R0R47JJuJMREcNtb8zhw2HYpNaahswPKjFdmrChh9Ms/MLpfG/4womugyzHG1ECdHlBmQs/POqRx3YBsXpu5hi+W2S6lxjRkFgTGa3cP60inFvH87t2FlOw+GOhyjDFesiAwXouOcPHMFT3Zc7CU301cQEPsZjTGWBAYH3VoHs8D53dm+nLbpdSYhsqCwPjs6r5tOLNTM/7yyTKWrN8Z6HKMMbVkQWB8JiI8MbIbSbER3PrmPPYeLA10ScaYWrAgMH6REhfFuMt78tPWvTz40ZJAl2OMqQULAuM3/dqlcOuZObw3t4j35xYFuhxjTA1ZEBi/uu3M9vTJSuZ/PlzMqpI9gS7HGFMDFgTGr8JdYYwb1YPI8DBufWseB0vtFBTGBDsLAuN3rRKb8MTI7ixZv4tHP10W6HKMMSdhQWDqxJDc5lzbP4tXvl1tVzUzJshZEJg6c995nejSqim/m7iADTv3B7ocY0w1LAhMnYkKd/H3K3pysLSM2yfM54hd1cyYoGRBYOpU27Q4/veirvzw0zb+/kVBoMsxxlTBgsDUuYt7ZXBxr3SemVbArFVbA12OMaYSCwJTL/44oittUmK5fcI8Nu8+EOhyjDEeLAhMvYiNCucfv+jFzv2Hue7V2bwxaw2LinZyqLQs0KUZE/LCA12ACR2dWzZl3OU9+J8PF/P7DxcDEOkKo3OrpnTPSKB7RiLdMxNomxpHWJgEuFpjQodds9jUO1WleMd+FqzbycKiHSwo2sGiop3sPeQ+CjkuKpyu6U2dYEikW0YC6YlNELFwMMdSVf7w8VJG9cmkU4umPq1r/rodrNu2jwu7t/JTdb658d9z6NU6iesHt/XbOqu7ZrG1CEy9ExEykmLISIrh/G4tAThSpqwq2cOCop0sWLeDhUU7eOXb1Rw64u46SomNpFtGAt0yEunhhENKXFQgP4YJApt3H+TV71bz30UbmP3A2T6t64O5RXw4f33QBMHs1dtJio2sl/fySxCIyDDgacAF/EtVH630ehTwOtAb2ApcrqqrndfuA8YAR4DbVHWyP2oyDYsrTMhpHk9O83hG9s4A4GDpEZZt2O20Gtyth+krSihvxKYnNqF7ZgL92qUy6tRMIlw25BVqyhuJ/ujZUI/1BQNVpb7K8TkIRMQFPAcMAYqA2SIySVWXesw2Btiuqu1FZBTwGHC5iOQCo4AuQCtgqoh0UFU7U5khKtxF90x399DVzrQ9B0tZXLyzIhwWrNvBJ4s28ub3a3liZDe6picEtGZTv1zOltsfxyqqUm8b3pqoz2Dyx0+oPkChqq5S1UPABGBEpXlGAK85jycCZ4m7w3cEMEFVD6rqT0Chsz5jqhQXFU7ftimMHdyO567sxTf3nMkLV/dmy56DjHjuWx77bBkHDtvviFAR5mwp/XHUuqJ+GYfaf+gI3f/wOV8u2+xbPapIPUWTP4IgHVjn8bzImVblPKpaCuwEUmq4LAAiMlZE8kUkv6SkxA9lm8binC4tmHrnz7i4Zzr/nL6S8575mvzV2wJdlqlHZf7oGvJTi+CnLXvZuf8wj322zFmvUuZFUClw4PARzn36a96YtaZOT9HSYDpVVXW8quapal5aWlqgyzFBJiEmgicu7c7r1/Xh4OEyLn1hJg9PWmLXT27kyjeN3mxoq1pXeYPgwOEjHD7i3TEuR8ctYPWWvWTf9wkfLSiufT0Kh4+U8eOGXfz+w8Us37jbq3pqwh9BUAxkejzPcKZVOY+IhAMJuAeNa7KsMTU2uEMan985mNH9snht5mqGPvUVXxdYC7KxKh8kPuKnFkF5m6DLQ5N5eqp358aqCAKUiHD3JtabAydVlSMeHyusDn+2+2PVs4EcEckWkUjcg7+TKs0zCRjtPB4JfKHuf8FJwCgRiRKRbCAH+MEPNZkQFhsVzsPDu/DODf2Iigjj6pd+4HfvLmDnvsOBLs34Wfl28sDhMl799icfW4BasREPE/90N0W6vA+CQ0fKmL786DiDqw5Hjn0OAqfP/xZgMvAj8I6qLhGRR0RkuDPbS0CKiBQCdwH3OssuAd4BlgKfATfbHkPGX07NSuaT2wZx4+nteH9eMUOemsHkJRsDXZbxI8+N9cMfL2Xnfu/D3nOM4PAR5R/TV7J2675ar6d8gHfPgVIue2EmAAe9CIIDh8vYfeBosNXlAZV+aWyo6ieq2kFV26nqn5xpD6rqJOfxAVW9VFXbq2ofVV3lseyfnOU6quqn/qjHmHLRES7uGdaJD28aQEpcFDe8MYeb35zLlj0HA12a8YdKP9o37NzPvkPetQpUj+6FVO71matrvR7PVfy0ZS/gDhZfuerwtCsNZrDYGF+ckpHApFsG8JshHZiyZBNDnpzBh/OK/XIgkgmcyv96l/xzJh/NX+/VuspU2bjrAPPWbq+YtseLrqbyzbXLdXTD7Y+TKwZ115AxDUWEK4xbz8rhv7cNJCs1ljvens+Y1/Ip2l775r8JDlXl+MKind6ty7l/8euKDgt2exMEzvY63GN099AR33u86/LgMgsCE3Jymscz8df9+f0FucxcuZUhT37F+K9Wer27oAkcPa5N4P2upOWh4tmN412L0b3F/mnL3oruHL+0CKxryBj/coUJYwZmM+WuwQxon8KfP1nGhX//hjlrtp98YRM0qtpOe7u3T3moeB645euRveFhQoRLOKNTM5/WA8ePX/iTnX3UhLSMpBhevCaPz5du4uFJSxj5/HcM6dyc09qm0LtNErktmxIZbr+XglVVm3yvjylwFiv1bFF4se313F5HuMK4LC+T/u1SvavJQ10eR2BBYEKeiHBOlxYMbJ/KM9MK+M/CDXy+dBMAUeFhdMtIoFebJHq1dt/S4u3018Giqq4br7uGnPtSjy5Cb36Dey4T4RK/dTnW5WCxBYExjtiocO47rzP3ndeZTbsOMHfNduas2c6ctdt55ZvVvHDEPYjYJiXGHQptkujdOomOLeLrtP/WVK+qH//e7qlZHiqeLQJf990Pd4VRWuafILCuIWPqWfOm0Zx7SkvOPcV94ZwDh4+wZP1O5qzZztw1O/i6YAsfzHOfDSU20kWP1on0bp1EzzZJ9MpMIiEmIpDlNzh//M9Stuw5yAPnd6ZZfHSNlysPgg7N41ixaQ/ge4vA15O7eS4d6Qqr0TEEXyzbxPa9h7nEuRZHVery8q0WBMbUQHSEi95tkundJhlw/3os2r6fuWudVsOa7Tz7ZWHFefFzmsXRq3USvdskMbRLcxJj6udKUw3V9OWbWVmylxkrSvjzz0/hPCeAT6Z8gLd3m+SjQeDtYHH5GIGPXUOeb/+rQdmkJzY56TLXveq+9G7fdinVzl+XjU4LAmO8ICJkJseQmRzDiB7uM6fvPVjKgqIdFV1Kny3ZyNv563h8chR/ufgUhuQ2D3DVwUuB3m2SKD1Sxh0T5tM9M7FGG9DyjW6Ex8Fb3v6irxgj8PlMpkeX/+WA7Bot0SYlhjVb9zHtx01c0y+rynls91FjGoDYqHD6t0vlljNzeOWXfZj3+yF8ePMAmsVHcf3r+dz19nw78V01VN2XHv3HVb0BePaLwpot59w3jY6gZ+tEwJcWQRW7j3qx7fXm7VsnxwCwpHhXtfPU5RiBBYExdSQsTOiRmchHtwzg9rNymLRgPUPHzeDL5b5duaoxKlMlTNxhcPmpmUycs65GR3yXb7zbN4vjg5sGcEp6gs8tAs+9fLzZqHvz7uXvs2zjLue5e8LFvY5ep8uCwJgGLMIVxp1DOvDhzQNIbBLJL1+ZzT0TF7LrgLUOynme8O3G09sB8I/pK0++nHNfcfroMPF6r6HylXkGiTetC2/Co/x9VmzaQ1mZVqyjvKUA1jVkTKPQNT2BSbcO4KbT2/HunHUMs4vmVCjzOAd0K6dV8G7+yVsF5b+cy3fzDBNvTwvhHnhOi49i4o39Pdbv3XpqqzwI9h8+QtH2/RXPw0RIaBLhPK59LTVlQWBMPYoKd3H3sE68f9MAmkS6uPqlH7j/g0VeneWyMal8CuibTm8PnLxVUL6hrjjjp4jXXUNlZZDYJILUuKMHDHrTIvDmsAFVKo5gX75p99GWDvDhzQP408+7Bv/1CIwxtdMjM5H/3jaIsYPb8tYPaxk27iu+W7kl0GUFjKoes6umZ6ugeMf+6pdz7o/pGvJ6jECP64evrxaBKnRsHg/Aik27jwacQHZqLL84rU3tC6kFCwJjAiQ6wsX953Xm3Rv6ER4mXPni9zw8aYnXF1ZpyMqquCjMjeWtgi+r34PoaIvg6Bk/v/9p2zHHAtSU6vF7CdXnGEF8dDjpiU1YvnF3RZjUZSvAkwWBMQGWl5XMp7cP5tr+Wbz63WrOe/prZq/eFuiy6pWix51ULT2xCZflZfLOCVoFRzeY7uclu91XnntvbpEXNRwv3FU/G2LFHYQdW8Qf0yJ4YvJyHvtsWZ2/vwWBMUGgSaSLh4d34a3r+1Japlz2wkwenrTEx4uxNxzu3pzjN7o3nXHiVkHlMYKxg9sC8PTUAg6W1u5iMO4WwbE1tGh68oPaqqupNspUEYEOzeNZWbLnmF1Yl22o/tgCf7EgMCaI9GuXwuQ7BnN13za8NnM1Q5/6iukhcNyBOscRVObZKlhfRavgaF+6e+H7z+vMv8ecxvqdB3jz+7W1reK4KPKqa8hpW/z556fUfBknhDq2iOPwEWVVyV6P9dU9CwJjgkxsVDiPjOjKuzf0IzoijGtfmc2db89n295DgS6tzlR14fhyFa2C6ce3Cip3DQEMaJ9C37bJPPdlYa3GWzzHCPq2PXpOqdoqX6R505qfrrw8CHOauQeMl2/cfdz66pJPQSAiySIyRUQKnPukauYb7cxTICKjPaZPF5HlIjLfufl+GR9jGom8rGQ+uX0Qt53Zno8XrGfIkzP4aH6x1/vJB7PyrpGqpCc24dK8TN6efXyroHLXELh/Wf/unI5s2XOIV79bXeMalKNBMGFsP5JiIrw7SriijpovUz5Y3r5ZHGECyzyDwIsaasvXFsG9wDRVzQGmOc+PISLJwEPAaUAf4KFKgfELVe3h3Bp/G9iYWogKd3HX0I7857aBZCTHcPuE+Yx+ZTaLi727QHuwKh8src5NztHGf528/NjlKnUNlevdJpkzOqbxwoxV7NxfsyO43buwHl1PmIiXew05rZRanLtUnW6p6AgXWSmxLN90dFygPoLf1yAYAbzmPH4NuKiKec4BpqjqNlXdDkwBhvn4vsaElE4tmvL+jf35/QW5LFi3gwv+/g03/nsOKzbtPvnCDcDJriGQkRTD2MFteX9eMTNWHD0au6JrqIplfjO0I7sOHD4uPKrj2SLAeezNIQkVi9SmRVB2NMw6NI8/pmuoPvgaBM1VdYPzeCNQ1Xl204F1Hs+LnGnlXnG6hX4vJ9hpVkTGiki+iOSXlNhh+Sb0uMKEMQOz+fqeM7jtrBy+LtjCOeO+4o4J81i9Ze/JVxDETjRGUO7WM3NomxbL/e8vqtibyvPAq8q6pidw3YBs3pi1hu9Xba1RDZW7mHwZI6jNjqeeXWO5rZqyZc/R8aCgGCMQkakisriK2wjP+dT9jdW25F+o6inAIOd2dXUzqup4Vc1T1by0tLRavo0xjUfT6AjuGtKBr+4+g7GD2/LZko2c9eQM7pm4sEZn7AxG7q6hE88THeHi8Uu6sX7nfp5wfuWfrD/+N0M7kJnchHvfX8SBwyfenVQrrShMvD1dhHcHg5V//h6ZiZXqCoKuIVU9W1W7VnH7CNgkIi0BnPuq+viLgUyP5xnONFS1/H438CbuMQRjTA0kx0Zy37md+eruM7i6bxs+mFfMGX+dzoMfLWbTrgOBLq9WTjRY7CkvK7li19o5a7aftD8+JjKcRy/uxk9b9jJuasEJ1135NBdhIt6dLqJi+Zov4z4Nt3uB7pWDIBhaBCcxCSjfC2g08FEV80wGhopIkjNIPBSYLCLhIpIKICIRwAXAYh/rMSbkNIuP5uHhXZj+u9MZ2TuTN79fy+DHv+RP/13K1j0HA11ejXhuCE/m7mGdaNk0mnveW8ihUvdP9hMtOqB9KpfnZfLi16tYVHTiQXbP9YSJcOCwd6eqgNoNFpd57Lqa0CSCdmmxx62vLvkaBI8CQ0SkADjbeY6I5InIvwBUdRvwR2C2c3vEmRaFOxAWAvNxtxJe9LEeY0JWq8Qm/OXiU/jiN6dzQbdWvPTNTwx6/Ev+Onl50F8ZTZUad6rHRYXzp4tPoXDzHp51jjg+WTfM/ed3JiU2krvfW3jMUbuVa/BcS15WEp8t2Ujh5toN3B7tGqrdMp6foUdmlXvi1xmfgkBVt6rqWaqa43QhbXOm56vqrzzme1lV2zu3V5xpe1W1t6p2U9Uuqnq7qtbumHBjzHFap8Twt8u68/mdP+PMTs149stCBj7+BX+fVhC0p7uuyWCxpzM6NuPnPdP5usB9xtaTLZnQJII/XtSVHzfsYvxXq6qcp6zSxvh/zs8lJtLF3RMX1uqMpp6nkK7xMpU+fw/nspvu9QXBGIExpmFq3yyOZ6/sxSe3DaJv2xT+NmUFgx77ghdmrAy6cxi5TwFdu2V+f0EuybGRQM1+fZ/TpQXnd2vJ01MLKNy85/gaKrUI0uKjeOjCXOau3cFrtTkwzYskKKs0PtHTY5ygIXQNGWOCXG6rprx4TR4f3jyArukJ/OXTZQx87Aue/aIgaC6XWaa161MH92D5H4Z3ASApJrJGyzx8YRdiolzc897C445dqOp6BBf1SOfMTs14YvJy1m6t2R5ZR49tqM0BZccOLue2bMpvh3agTUpMvRxZHF4P72GMCQI9MhN5Y8xpzF27nee+KOSvn6/gmWmFZCQ3ISMphoykJs7N/TgzKYbUuMh6OSd++cXra+vC7q3o2zaF1LiaBUFafBQPXpDLXe8s4PLxM7miT2vO7dqSJpGuKscpRIQ//bwrQ5/8iitenMWoUzO5uHcG6YknOCtppWMbysrce0Sd6HusPFgeFibccmYO3xRu8WoX1tqyIDAmxPRqncRL157K4uKdfLxgPeu276No+34WF+887sR2UeFhx4RD5cDwV1BUdQromkqLr/nJ3QB+3jOdnfsP8+p3q7nrnQU8+NESzj+lJSV7DpIWd/y6WiY0Yfw1eTwzrYC/TVnBk1NXMKBdKpfmZTA0twVNIl3HfhbnvvzTTF+xmf/974+M7J3BxT0zaJEQfdx7lJVRZVeSICh1nwQWBMaEqK7pCXRNTzhm2t6DpRTv2E+REw7rtrnvi7bvZ2HRDrZX2vvIH0HhzV42vhARfjkgm2v7Z/HDT9uYOKeIjxeuZ9+hI7SsYiMN7tOD92uXwrpt+5g4p4iJc4q4fcJ84qPCuaB7S0b2zqBX6yTnaOSj7wMQHe4iNTaKxz9bzl8nL2dgThqX9ErnnC4tiI44GiJVDZaL1M8YgQWBMaZCbFQ4HZrH08G5fm5lew6WUrz9aFAcva86KKIjwkhPdIdCZvLxgZESG1lxPp/a7DXkDyLCaW1TOK1tCg8P78KUpZvITo094TKZyTHcOaQDt5+Vw6yftjJxThEfzlvPWz+sIzs1lkt6pdMsPtpZv3uZ/u1T6d8+ldVb9vL+3CLem1vsDpHocC7o1oqRvTOq7RoTqZ+zj1oQGGNqLC4qnI4t4unYouZBsW7bfop27GNB0Q52VBMUULvdLf0tNiqci3qmn3xGR1iY0L9dKv3bpfLIiFI+XbSBiXOK+OvnKyrmqfx5slJjuWtoR+44u4NHiBTz1g9rnfmraBHg3fmOasuCwBjjNycLit0HDru7nrYd25po2iSCU7OT67la/4iLCufSvEwuzctk7dZ9vDe3iAVFOyouMlNZ5RD5ZNEG/rtwA4M6pB43r7UIjDGNTnx0BJ1aRNCpRdNAl1InWqe4u45qKi4qnMvyMrksL7Paeew4AmOMCWEi0iCuUGaMMaaOCNRLk8CCwBhjglR9jRFYEBhjTJASbIzAGGNCmnh5cZzasiAwxpggVV/HVlgQGGNMELOuIWOMCWH1da4hCwJjjAladhyBMcaENHeLwAaLjTEmZNlgsTHGhLgGMUYgIskiMkVECpz7pGrm+0xEdojIfypNzxaR70WkUETeFpGaXW/OGGNCgPsKZcHfNXQvME1Vc4BpzvOqPAFcXcX0x4CnVLU9sB0Y42M9xhjTaDSIFgEwAnjNefwacFFVM6nqNGC35zRxX8ftTGDiyZY3xphQ1FDONdRcVTc4jzcCzWuxbAqwQ1VLnedFQM0vEWSMMY1c0FyhTESmAi2qeOkBzyeqqiJSZxWLyFhgLEDr1q3r6m2MMSZ4BMsVylT17OpeE5FNItJSVTeISEtgcy3eeyuQKCLhTqsgAyg+QR3jgfEAeXl59fHdGGNMQLmvR1D37+Nr19AkYLTzeDTwUU0XVHd750tgpDfLG2NMY9dQrlD2KDBERAqAs53niEieiPyrfCYR+Rp4FzhLRIpE5BznpXuAu0SkEPeYwUs+1mOMMY2G+3oEQTBGcCKquhU4q4rp+cCvPJ4Pqmb5VUAfX2owxpjGSurp0GI7stgYY4JYQ+gaMsYYU0fsUpXGGBPi7FKVxhgT4qxFYIwxoa6BnGvIGGNMHZF6uiKBBYExxgQpu0KZMcaEOMF2HzXGmJBWX9cj8OnIYmOMMXWnW0Yi4a66/71uQWCMMUHqqr5t6uV9rGvIGGNCnAWBMcaEOAsCY4wJcRYExhgT4iwIjDEmxFkQGGNMiLMgMMaYEGdBYIwxIU7q44RG/iYiJcAaLxdPBbb4sZz6YnXXv4Zau9VdvxpS3W1UNa3yxAYZBL4QkXxVzQt0HbVldde/hlq71V2/GmrdnqxryBhjQpwFgTHGhLhQDILxgS7AS1Z3/WuotVvd9auh1l0h5MYIjDHGHCsUWwTGGGM8WBAYY0yIC5kgEJFhIrJcRApF5N5A1+NJRDJF5EsRWSoiS0Tkdmf6wyJSLCLzndt5Hsvc53yW5SJyTuCqBxFZLSKLnBrznWnJIjJFRAqc+yRnuojIM07tC0WkV4Bq7ujxvc4XkV0ickcwfuci8rKIbBaRxR7Tav39ishoZ/4CERkdoLqfEJFlTm0fiEiiMz1LRPZ7fO/PeyzT2/n7KnQ+mwSo9lr/bQTzducYqtrob4ALWAm0BSKBBUBuoOvyqK8l0Mt5HA+sAHKBh4HfVjF/rvMZooBs57O5Alj/aiC10rTHgXudx/cCjzmPzwM+xX1d7r7A90Hw/buAjUCbYPzOgcFAL2Cxt98vkAyscu6TnMdJAah7KBDuPH7Mo+4sz/kqrecH57OI89nODdB3Xqu/jWDf7njeQqVF0AcoVNVVqnoImACMCHBNFVR1g6rOdR7vBn4E0k+wyAhggqoeVNWfgELcnzGYjABecx6/BlzkMf11dZsFJIpIywDU5+ksYKWqnuho9YB956r6FbCtinpq8/2eA0xR1W2quh2YAgyr77pV9XNVLXWezgIyTrQOp/amqjpL3Vvd1zn6WetMNd95dar72wjq7Y6nUAmCdGCdx/MiTryhDRgRyQJ6At87k25xmtEvlzf/Cb7Po8DnIjJHRMY605qr6gbn8UagufM42GoHGAW85fG8IXzntf1+g61+gOtw/8Ivly0i80RkhogMcqal4661XKDrrs3fRjB+51UKlSBoEEQkDngPuENVdwH/BNoBPYANwN8CV90JDVTVXsC5wM0iMtjzReeXXFDupywikcBw4F1nUkP5zisE8/dbHRF5ACgF/s+ZtAForao9gbuAN0WkaaDqq0aD+9uoqVAJgmIg0+N5hjMtaIhIBO4Q+D9VfR9AVTep6hFVLQNe5GhXRFB9HlUtdu43Ax/grnNTeZePc7/ZmT2oascdXnNVdRM0nO+c2n+/QVO/iFwLXAD8wgkxnG6Vrc7jObj71js4NXp2HwWsbi/+NoLmOz+ZUAmC2UCOiGQ7vwBHAZMCXFMFZy+Il4AfVfVJj+mefec/B8r3YJgEjBKRKBHJBnJwD6jVOxGJFZH48se4BwMXOzWW75kyGvjIeTwJuMbZu6UvsNOjiyMQrsCjW6ghfOce9dTm+50MDBWRJKdLY6gzrV6JyDDgbmC4qu7zmJ4mIi7ncVvc3+8qp/ZdItLX+X9yDUc/a73y4m8jqLc7xwj0aHV93XDvTbEC9y+NBwJdT6XaBuJu2i8E5ju384A3gEXO9ElAS49lHnA+y3LqYS+KE9TeFvfeEAuAJeXfLZACTAMKgKlAsjNdgOec2hcBeQGsPRbYCiR4TAu67xx3UG0ADuPuZx7jzfeLu0++0Ln9MkB1F+LuNy//O3/emfcS5+9nPjAXuNBjPXm4N7orgWdxzogQgNpr/bcRzNsdz5udYsIYY0JcqHQNGWOMqYYFgTHGhDgLAmOMCXEWBMYYE+IsCIwxJsRZEBhjTIizIDDGmBD3/9oEmeRCC/AAAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c0 = 82.7524  # Value for C0\n",
    "K0 = -0.0031  # Value for K0\n",
    "K1 = -0.0003  # Value for K1\n",
    "a = 0.0000    # Value for a\n",
    "b = 0.0171    # Value for b\n",
    "c = 3.0230    # Value for c\n",
    "\n",
    "L = np.minimum(c0, (df.iloc[:, 1] - (df.iloc[:, 0] * (K0 - K1 * (9 * a * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c)**2 + 4 * b * np.log(df.iloc[:, 0] / c0) / (K0 - K1 * c) + c)))))\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VyEywnwaFvh"
   },
   "source": [
    "## Preprocessing the data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6V9dXqzdaFvh"
   },
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n_in, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n_out)\n",
    "    for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrzSrT1HnyfH",
    "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-350)  var1(t-349)  var1(t-348)  var1(t-347)  var1(t-346)  \\\n",
      "350    84.600000    84.431933    84.263866    84.095798    83.927731   \n",
      "351    84.431933    84.263866    84.095798    83.927731    83.759664   \n",
      "352    84.263866    84.095798    83.927731    83.759664    83.591597   \n",
      "353    84.095798    83.927731    83.759664    83.591597    83.423529   \n",
      "354    83.927731    83.759664    83.591597    83.423529    83.255462   \n",
      "\n",
      "     var1(t-345)  var1(t-344)  var1(t-343)  var1(t-342)  var1(t-341)  ...  \\\n",
      "350    83.759664    83.591597    83.423529    83.255462    83.092437  ...   \n",
      "351    83.591597    83.423529    83.255462    83.092437    82.991597  ...   \n",
      "352    83.423529    83.255462    83.092437    82.991597    82.890756  ...   \n",
      "353    83.255462    83.092437    82.991597    82.890756    82.789916  ...   \n",
      "354    83.092437    82.991597    82.890756    82.789916    82.689076  ...   \n",
      "\n",
      "     var1(t+95)  var2(t+95)  var1(t+96)  var2(t+96)  var1(t+97)  var2(t+97)  \\\n",
      "350   70.003688    0.000263   69.960738    0.000263   69.917787    0.000263   \n",
      "351   69.960738    0.000263   69.917787    0.000263   69.874837    0.000262   \n",
      "352   69.917787    0.000263   69.874837    0.000262   69.831886    0.000262   \n",
      "353   69.874837    0.000262   69.831886    0.000262   69.788936    0.000262   \n",
      "354   69.831886    0.000262   69.788936    0.000262   69.745985    0.000262   \n",
      "\n",
      "     var1(t+98)  var2(t+98)  var1(t+99)  var2(t+99)  \n",
      "350   69.874837    0.000262   69.831886    0.000262  \n",
      "351   69.831886    0.000262   69.788936    0.000262  \n",
      "352   69.788936    0.000262   69.745985    0.000262  \n",
      "353   69.745985    0.000262   69.703035    0.000262  \n",
      "354   69.703035    0.000262   69.660084    0.000262  \n",
      "\n",
      "[5 rows x 551 columns]\n",
      "Index(['var1(t-350)', 'var1(t-349)', 'var1(t-348)', 'var1(t-347)',\n",
      "       'var1(t-346)', 'var1(t-345)', 'var1(t-344)', 'var1(t-343)',\n",
      "       'var1(t-342)', 'var1(t-341)',\n",
      "       ...\n",
      "       'var1(t+95)', 'var2(t+95)', 'var1(t+96)', 'var2(t+96)', 'var1(t+97)',\n",
      "       'var2(t+97)', 'var1(t+98)', 'var2(t+98)', 'var1(t+99)', 'var2(t+99)'],\n",
      "      dtype='object', length=551)\n"
     ]
    }
   ],
   "source": [
    "data = Supervised(df.values, n_in = 350, n_out = 100)\n",
    "\n",
    "\n",
    "cols_to_drop = []\n",
    "for i in range(2, 351):\n",
    "    cols_to_drop.extend([f'var2(t-{i})'])\n",
    "\n",
    "data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AfPf60oy6Pe4"
   },
   "outputs": [],
   "source": [
    "train = np.array(data[0:len(data)-1])\n",
    "forecast = np.array(data.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WSAafzI37KiT"
   },
   "outputs": [],
   "source": [
    "trainy = train[:,-300:]\n",
    "trainX = train[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2SrOqVJA7f50"
   },
   "outputs": [],
   "source": [
    "forecasty = forecast[:,-300:]\n",
    "forecastX = forecast[:,:-300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qno_k8Nw7saY",
    "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 1, 251) (1200, 300) (1, 1, 251)\n"
     ]
    }
   ],
   "source": [
    "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
    "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
    "print(trainX.shape, trainy.shape, forecastX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1Jp2DvNuNFx",
    "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "15/15 [==============================] - 2s 36ms/step - loss: 4066.8291 - val_loss: 2855.5471\n",
      "Epoch 2/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3991.6577 - val_loss: 2822.6604\n",
      "Epoch 3/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3948.1992 - val_loss: 2790.7634\n",
      "Epoch 4/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3904.9526 - val_loss: 2759.2322\n",
      "Epoch 5/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3862.1125 - val_loss: 2728.0586\n",
      "Epoch 6/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 3819.6804 - val_loss: 2697.2200\n",
      "Epoch 7/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3777.6401 - val_loss: 2666.7017\n",
      "Epoch 8/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3735.2097 - val_loss: 2634.8625\n",
      "Epoch 9/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3691.0854 - val_loss: 2602.7805\n",
      "Epoch 10/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3647.3025 - val_loss: 2571.2651\n",
      "Epoch 11/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3604.2412 - val_loss: 2540.2905\n",
      "Epoch 12/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3561.8188 - val_loss: 2509.7842\n",
      "Epoch 13/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3519.9480 - val_loss: 2479.6941\n",
      "Epoch 14/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3478.5713 - val_loss: 2449.9873\n",
      "Epoch 15/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3437.6521 - val_loss: 2420.6416\n",
      "Epoch 16/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3397.1643 - val_loss: 2391.6411\n",
      "Epoch 17/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3357.0898 - val_loss: 2362.9744\n",
      "Epoch 18/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3317.4143 - val_loss: 2334.6318\n",
      "Epoch 19/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3278.1272 - val_loss: 2306.6006\n",
      "Epoch 20/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 3230.1670 - val_loss: 2267.9766\n",
      "Epoch 21/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3184.3865 - val_loss: 2237.6001\n",
      "Epoch 22/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3142.5261 - val_loss: 2208.0071\n",
      "Epoch 23/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3101.6282 - val_loss: 2179.1101\n",
      "Epoch 24/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3061.5359 - val_loss: 2150.7893\n",
      "Epoch 25/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3022.1160 - val_loss: 2122.9636\n",
      "Epoch 26/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2983.2803 - val_loss: 2095.5825\n",
      "Epoch 27/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2944.9722 - val_loss: 2068.6106\n",
      "Epoch 28/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2907.1526 - val_loss: 2042.0233\n",
      "Epoch 29/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2869.7930 - val_loss: 2015.8019\n",
      "Epoch 30/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2832.8728 - val_loss: 1989.9318\n",
      "Epoch 31/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2796.3738 - val_loss: 1964.4019\n",
      "Epoch 32/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2760.2827 - val_loss: 1939.2018\n",
      "Epoch 33/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2724.5889 - val_loss: 1914.3240\n",
      "Epoch 34/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2689.2817 - val_loss: 1889.7614\n",
      "Epoch 35/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2654.3530 - val_loss: 1865.5074\n",
      "Epoch 36/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2619.7964 - val_loss: 1841.5565\n",
      "Epoch 37/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2585.6040 - val_loss: 1817.9043\n",
      "Epoch 38/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2551.7705 - val_loss: 1794.5458\n",
      "Epoch 39/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2518.2903 - val_loss: 1771.4769\n",
      "Epoch 40/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2485.1592 - val_loss: 1748.6936\n",
      "Epoch 41/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 2452.3713 - val_loss: 1726.1923\n",
      "Epoch 42/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2419.9231 - val_loss: 1703.9701\n",
      "Epoch 43/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2387.8103 - val_loss: 1682.0228\n",
      "Epoch 44/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2356.0300 - val_loss: 1660.3479\n",
      "Epoch 45/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2324.5774 - val_loss: 1638.9425\n",
      "Epoch 46/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 2293.4495 - val_loss: 1617.8036\n",
      "Epoch 47/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2262.6433 - val_loss: 1596.9283\n",
      "Epoch 48/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2232.1553 - val_loss: 1576.3146\n",
      "Epoch 49/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2201.9819 - val_loss: 1555.9595\n",
      "Epoch 50/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2172.1213 - val_loss: 1535.8605\n",
      "Epoch 51/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 2142.5701 - val_loss: 1516.0151\n",
      "Epoch 52/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2113.3257 - val_loss: 1496.4216\n",
      "Epoch 53/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2084.3853 - val_loss: 1477.0770\n",
      "Epoch 54/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2055.7458 - val_loss: 1457.9794\n",
      "Epoch 55/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 2027.4056 - val_loss: 1439.1268\n",
      "Epoch 56/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1999.3615 - val_loss: 1420.5165\n",
      "Epoch 57/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1971.6111 - val_loss: 1402.1469\n",
      "Epoch 58/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1944.1522 - val_loss: 1384.0154\n",
      "Epoch 59/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1916.9823 - val_loss: 1366.1210\n",
      "Epoch 60/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1890.0991 - val_loss: 1348.4604\n",
      "Epoch 61/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1863.5005 - val_loss: 1331.0323\n",
      "Epoch 62/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1837.1842 - val_loss: 1313.8346\n",
      "Epoch 63/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1811.1476 - val_loss: 1296.8654\n",
      "Epoch 64/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1785.3889 - val_loss: 1280.1230\n",
      "Epoch 65/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1759.9058 - val_loss: 1263.6053\n",
      "Epoch 66/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1734.6960 - val_loss: 1247.3104\n",
      "Epoch 67/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1709.7579 - val_loss: 1231.2368\n",
      "Epoch 68/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1685.0892 - val_loss: 1215.3822\n",
      "Epoch 69/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 1660.6877 - val_loss: 1199.7452\n",
      "Epoch 70/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1636.5515 - val_loss: 1184.3237\n",
      "Epoch 71/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1612.6786 - val_loss: 1169.1160\n",
      "Epoch 72/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1589.0675 - val_loss: 1154.1206\n",
      "Epoch 73/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1565.7147 - val_loss: 1139.3354\n",
      "Epoch 74/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1542.6198 - val_loss: 1124.7592\n",
      "Epoch 75/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1519.7809 - val_loss: 1110.3899\n",
      "Epoch 76/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1497.1949 - val_loss: 1096.2255\n",
      "Epoch 77/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1474.8605 - val_loss: 1082.2649\n",
      "Epoch 78/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1452.7756 - val_loss: 1068.5060\n",
      "Epoch 79/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1430.9390 - val_loss: 1054.9475\n",
      "Epoch 80/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1409.3483 - val_loss: 1041.5874\n",
      "Epoch 81/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1388.0017 - val_loss: 1028.4242\n",
      "Epoch 82/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1366.8977 - val_loss: 1015.4565\n",
      "Epoch 83/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1346.0341 - val_loss: 1002.6827\n",
      "Epoch 84/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1325.4093 - val_loss: 990.1005\n",
      "Epoch 85/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1305.0214 - val_loss: 977.7091\n",
      "Epoch 86/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1284.8687 - val_loss: 965.5061\n",
      "Epoch 87/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1264.9491 - val_loss: 953.4909\n",
      "Epoch 88/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1245.2618 - val_loss: 941.6614\n",
      "Epoch 89/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1225.8043 - val_loss: 930.0160\n",
      "Epoch 90/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1206.5747 - val_loss: 918.5531\n",
      "Epoch 91/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1187.5714 - val_loss: 907.2712\n",
      "Epoch 92/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1168.7932 - val_loss: 896.1688\n",
      "Epoch 93/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1150.2378 - val_loss: 885.2443\n",
      "Epoch 94/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1131.9041 - val_loss: 874.4965\n",
      "Epoch 95/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1113.7903 - val_loss: 863.9237\n",
      "Epoch 96/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1095.8938 - val_loss: 853.5242\n",
      "Epoch 97/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1078.2141 - val_loss: 843.2964\n",
      "Epoch 98/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1060.7491 - val_loss: 833.2393\n",
      "Epoch 99/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1043.4968 - val_loss: 823.3512\n",
      "Epoch 100/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1026.4565 - val_loss: 813.6303\n",
      "Epoch 101/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1009.6255 - val_loss: 804.0754\n",
      "Epoch 102/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 993.0027 - val_loss: 794.6849\n",
      "Epoch 103/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 976.5865 - val_loss: 785.4578\n",
      "Epoch 104/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 960.3756 - val_loss: 776.3917\n",
      "Epoch 105/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 944.3675 - val_loss: 767.4861\n",
      "Epoch 106/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 928.5613 - val_loss: 758.7388\n",
      "Epoch 107/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 912.9553 - val_loss: 750.1489\n",
      "Epoch 108/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 897.5480 - val_loss: 741.7146\n",
      "Epoch 109/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 882.3372 - val_loss: 733.4347\n",
      "Epoch 110/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 867.3221 - val_loss: 725.3077\n",
      "Epoch 111/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 852.5010 - val_loss: 717.3323\n",
      "Epoch 112/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 837.8721 - val_loss: 709.5066\n",
      "Epoch 113/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 823.4340 - val_loss: 701.8298\n",
      "Epoch 114/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 809.1850 - val_loss: 694.2999\n",
      "Epoch 115/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 795.1237 - val_loss: 686.9158\n",
      "Epoch 116/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 781.2487 - val_loss: 679.6762\n",
      "Epoch 117/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 767.5579 - val_loss: 672.5797\n",
      "Epoch 118/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 754.0506 - val_loss: 665.6246\n",
      "Epoch 119/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 740.7249 - val_loss: 658.8100\n",
      "Epoch 120/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 727.5792 - val_loss: 652.1340\n",
      "Epoch 121/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 714.6121 - val_loss: 645.5954\n",
      "Epoch 122/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 701.8221 - val_loss: 639.1928\n",
      "Epoch 123/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 689.2074 - val_loss: 632.9251\n",
      "Epoch 124/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 676.7672 - val_loss: 626.7905\n",
      "Epoch 125/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 664.4993 - val_loss: 620.7880\n",
      "Epoch 126/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 652.4027 - val_loss: 614.9159\n",
      "Epoch 127/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 640.4759 - val_loss: 609.1733\n",
      "Epoch 128/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 628.7175 - val_loss: 603.5583\n",
      "Epoch 129/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 617.1254 - val_loss: 598.0699\n",
      "Epoch 130/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 605.6988 - val_loss: 592.7067\n",
      "Epoch 131/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 594.4360 - val_loss: 587.4672\n",
      "Epoch 132/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 583.3357 - val_loss: 582.3502\n",
      "Epoch 133/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 572.3961 - val_loss: 577.3546\n",
      "Epoch 134/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 561.6161 - val_loss: 572.4784\n",
      "Epoch 135/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 550.9943 - val_loss: 567.7208\n",
      "Epoch 136/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 540.5291 - val_loss: 563.0806\n",
      "Epoch 137/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 530.2192 - val_loss: 558.5560\n",
      "Epoch 138/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 520.0630 - val_loss: 554.1459\n",
      "Epoch 139/500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 510.0591 - val_loss: 549.8491\n",
      "Epoch 140/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 500.2064 - val_loss: 545.6642\n",
      "Epoch 141/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 490.5032 - val_loss: 541.5898\n",
      "Epoch 142/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 480.9482 - val_loss: 537.6247\n",
      "Epoch 143/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 471.5399 - val_loss: 533.7675\n",
      "Epoch 144/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 462.2771 - val_loss: 530.0172\n",
      "Epoch 145/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 453.1583 - val_loss: 526.3719\n",
      "Epoch 146/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 444.1819 - val_loss: 522.8309\n",
      "Epoch 147/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 435.3469 - val_loss: 519.3926\n",
      "Epoch 148/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 426.6517 - val_loss: 516.0558\n",
      "Epoch 149/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 418.0951 - val_loss: 512.8193\n",
      "Epoch 150/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 409.6754 - val_loss: 509.6817\n",
      "Epoch 151/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 401.3915 - val_loss: 506.6416\n",
      "Epoch 152/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 393.2419 - val_loss: 503.6979\n",
      "Epoch 153/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 385.2253 - val_loss: 500.8495\n",
      "Epoch 154/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 377.3404 - val_loss: 498.0948\n",
      "Epoch 155/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 369.5859 - val_loss: 495.4327\n",
      "Epoch 156/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 361.9601 - val_loss: 492.8620\n",
      "Epoch 157/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 354.4621 - val_loss: 490.3812\n",
      "Epoch 158/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 347.0903 - val_loss: 487.9893\n",
      "Epoch 159/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 339.8434 - val_loss: 485.6849\n",
      "Epoch 160/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 332.7202 - val_loss: 483.4668\n",
      "Epoch 161/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 325.7191 - val_loss: 481.3338\n",
      "Epoch 162/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 318.8391 - val_loss: 479.2847\n",
      "Epoch 163/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 312.0786 - val_loss: 477.3180\n",
      "Epoch 164/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 305.4365 - val_loss: 475.4328\n",
      "Epoch 165/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 298.9115 - val_loss: 473.6278\n",
      "Epoch 166/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 292.5021 - val_loss: 471.9016\n",
      "Epoch 167/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 286.2072 - val_loss: 470.2531\n",
      "Epoch 168/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 280.0252 - val_loss: 468.6810\n",
      "Epoch 169/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 273.9551 - val_loss: 467.1842\n",
      "Epoch 170/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 267.9955 - val_loss: 465.7616\n",
      "Epoch 171/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 262.1451 - val_loss: 464.4116\n",
      "Epoch 172/500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 256.4028 - val_loss: 463.1334\n",
      "Epoch 173/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 250.7671 - val_loss: 461.9257\n",
      "Epoch 174/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 245.2368 - val_loss: 460.7872\n",
      "Epoch 175/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 239.8107 - val_loss: 459.7167\n",
      "Epoch 176/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 234.4875 - val_loss: 458.7131\n",
      "Epoch 177/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 229.2658 - val_loss: 457.7753\n",
      "Epoch 178/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 224.1444 - val_loss: 456.9020\n",
      "Epoch 179/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 219.1222 - val_loss: 456.0921\n",
      "Epoch 180/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 214.1979 - val_loss: 455.3443\n",
      "Epoch 181/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 209.3702 - val_loss: 454.6576\n",
      "Epoch 182/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 204.6379 - val_loss: 454.0308\n",
      "Epoch 183/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 199.9997 - val_loss: 453.4627\n",
      "Epoch 184/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 195.4546 - val_loss: 452.9522\n",
      "Epoch 185/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 191.0010 - val_loss: 452.4982\n",
      "Epoch 186/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 186.6380 - val_loss: 452.0995\n",
      "Epoch 187/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 182.3643 - val_loss: 451.7550\n",
      "Epoch 188/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 178.1787 - val_loss: 451.4635\n",
      "Epoch 189/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 174.0798 - val_loss: 451.2240\n",
      "Epoch 190/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 170.0667 - val_loss: 451.0352\n",
      "Epoch 191/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 166.1381 - val_loss: 450.8963\n",
      "Epoch 192/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 162.2928 - val_loss: 450.8060\n",
      "Epoch 193/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 158.5297 - val_loss: 450.7632\n",
      "Epoch 194/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 154.8475 - val_loss: 450.7669\n",
      "Epoch 195/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 151.2450 - val_loss: 450.8159\n",
      "Epoch 196/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 147.7212 - val_loss: 450.9092\n",
      "Epoch 197/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 144.2750 - val_loss: 451.0458\n",
      "Epoch 198/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 140.9052 - val_loss: 451.2245\n",
      "Epoch 199/500\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 137.6104 - val_loss: 451.4443\n",
      "Epoch 200/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 134.3897 - val_loss: 451.7042\n",
      "Epoch 201/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 131.2419 - val_loss: 452.0030\n",
      "Epoch 202/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 128.1658 - val_loss: 452.3399\n",
      "Epoch 203/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 125.1605 - val_loss: 452.7137\n",
      "Epoch 204/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 122.2247 - val_loss: 453.1234\n",
      "Epoch 205/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 119.3575 - val_loss: 453.5681\n",
      "Epoch 206/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 116.5576 - val_loss: 454.0468\n",
      "Epoch 207/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 113.8239 - val_loss: 454.5582\n",
      "Epoch 208/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 111.1554 - val_loss: 455.1017\n",
      "Epoch 209/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 108.5511 - val_loss: 455.6761\n",
      "Epoch 210/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 106.0098 - val_loss: 456.2805\n",
      "Epoch 211/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 103.5304 - val_loss: 456.9140\n",
      "Epoch 212/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 101.1120 - val_loss: 457.5754\n",
      "Epoch 213/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 98.7537 - val_loss: 458.2640\n",
      "Epoch 214/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 96.4540 - val_loss: 458.9789\n",
      "Epoch 215/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 94.2121 - val_loss: 459.7188\n",
      "Epoch 216/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 92.0272 - val_loss: 460.4832\n",
      "Epoch 217/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 89.8979 - val_loss: 461.2709\n",
      "Epoch 218/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 87.8234 - val_loss: 462.0813\n",
      "Epoch 219/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 85.8027 - val_loss: 462.9131\n",
      "Epoch 220/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 83.8348 - val_loss: 463.7658\n",
      "Epoch 221/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 81.9188 - val_loss: 464.6383\n",
      "Epoch 222/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 80.0535 - val_loss: 465.5299\n",
      "Epoch 223/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 78.2380 - val_loss: 466.4396\n",
      "Epoch 224/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 76.4715 - val_loss: 467.3667\n",
      "Epoch 225/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 74.7529 - val_loss: 468.3102\n",
      "Epoch 226/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 73.0813 - val_loss: 469.2694\n",
      "Epoch 227/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 71.4557 - val_loss: 470.2434\n",
      "Epoch 228/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 69.8753 - val_loss: 471.2316\n",
      "Epoch 229/500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 68.3391 - val_loss: 472.2328\n",
      "Epoch 230/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 66.8464 - val_loss: 473.2466\n",
      "Epoch 231/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 65.3960 - val_loss: 474.2722\n",
      "Epoch 232/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 63.9872 - val_loss: 475.3087\n",
      "Epoch 233/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 62.6191 - val_loss: 476.3554\n",
      "Epoch 234/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 61.2908 - val_loss: 477.4116\n",
      "Epoch 235/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 60.0014 - val_loss: 478.4766\n",
      "Epoch 236/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 58.7501 - val_loss: 479.5496\n",
      "Epoch 237/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 57.5361 - val_loss: 480.6299\n",
      "Epoch 238/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 56.3586 - val_loss: 481.7168\n",
      "Epoch 239/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 55.2168 - val_loss: 482.8099\n",
      "Epoch 240/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 54.1097 - val_loss: 483.9083\n",
      "Epoch 241/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 53.0366 - val_loss: 485.0111\n",
      "Epoch 242/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 51.9968 - val_loss: 486.1182\n",
      "Epoch 243/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 50.9894 - val_loss: 487.2286\n",
      "Epoch 244/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 50.0137 - val_loss: 488.3419\n",
      "Epoch 245/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 49.0690 - val_loss: 489.4573\n",
      "Epoch 246/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 48.1544 - val_loss: 490.5743\n",
      "Epoch 247/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 47.2693 - val_loss: 491.6923\n",
      "Epoch 248/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 46.4129 - val_loss: 492.8108\n",
      "Epoch 249/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 45.5845 - val_loss: 493.9293\n",
      "Epoch 250/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 44.7834 - val_loss: 495.0471\n",
      "Epoch 251/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 44.0089 - val_loss: 496.1637\n",
      "Epoch 252/500\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 43.2605 - val_loss: 497.2787\n",
      "Epoch 253/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 42.5372 - val_loss: 498.3914\n",
      "Epoch 254/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 41.8386 - val_loss: 499.5014\n",
      "Epoch 255/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 41.1639 - val_loss: 500.6084\n",
      "Epoch 256/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 40.5124 - val_loss: 501.7119\n",
      "Epoch 257/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 39.8837 - val_loss: 502.8110\n",
      "Epoch 258/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 39.2771 - val_loss: 503.9058\n",
      "Epoch 259/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 38.6919 - val_loss: 504.9956\n",
      "Epoch 260/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 38.1276 - val_loss: 506.0802\n",
      "Epoch 261/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 37.5835 - val_loss: 507.1591\n",
      "Epoch 262/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 37.0591 - val_loss: 508.2318\n",
      "Epoch 263/500\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 36.5539 - val_loss: 509.2980\n",
      "Epoch 264/500\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 36.0672 - val_loss: 510.3574\n",
      "Epoch 265/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 35.5985 - val_loss: 511.4096\n",
      "Epoch 266/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 35.1474 - val_loss: 512.4542\n",
      "Epoch 267/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 34.7132 - val_loss: 513.4909\n",
      "Epoch 268/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 34.2955 - val_loss: 514.5195\n",
      "Epoch 269/500\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 33.8937 - val_loss: 515.5396\n",
      "Epoch 270/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 33.5074 - val_loss: 516.5509\n",
      "Epoch 271/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 33.1361 - val_loss: 517.5533\n",
      "Epoch 272/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 32.7793 - val_loss: 518.5462\n",
      "Epoch 273/500\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 32.4366 - val_loss: 519.5298\n",
      "Epoch 274/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 32.1075 - val_loss: 520.5035\n",
      "Epoch 275/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 31.7915 - val_loss: 521.4674\n",
      "Epoch 276/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 31.4883 - val_loss: 522.4208\n",
      "Epoch 277/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 31.1974 - val_loss: 523.3636\n",
      "Epoch 278/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 30.9184 - val_loss: 524.2960\n",
      "Epoch 279/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 30.6509 - val_loss: 525.2176\n",
      "Epoch 280/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 30.3945 - val_loss: 526.1282\n",
      "Epoch 281/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 30.1488 - val_loss: 527.0278\n",
      "Epoch 282/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 29.9135 - val_loss: 527.9160\n",
      "Epoch 283/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 29.6882 - val_loss: 528.7928\n",
      "Epoch 284/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 29.4725 - val_loss: 529.6579\n",
      "Epoch 285/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 29.2662 - val_loss: 530.5116\n",
      "Epoch 286/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 29.0688 - val_loss: 531.3534\n",
      "Epoch 287/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 28.8800 - val_loss: 532.1833\n",
      "Epoch 288/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 28.6996 - val_loss: 533.0016\n",
      "Epoch 289/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 28.5271 - val_loss: 533.8074\n",
      "Epoch 290/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 28.3625 - val_loss: 534.6014\n",
      "Epoch 291/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 28.2052 - val_loss: 535.3831\n",
      "Epoch 292/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 28.0550 - val_loss: 536.1528\n",
      "Epoch 293/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 27.9117 - val_loss: 536.9101\n",
      "Epoch 294/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 27.7751 - val_loss: 537.6553\n",
      "Epoch 295/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 27.6447 - val_loss: 538.3880\n",
      "Epoch 296/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 27.5204 - val_loss: 539.1087\n",
      "Epoch 297/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 27.4020 - val_loss: 539.8171\n",
      "Epoch 298/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 27.2892 - val_loss: 540.5130\n",
      "Epoch 299/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 27.1818 - val_loss: 541.1967\n",
      "Epoch 300/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 27.0795 - val_loss: 541.8683\n",
      "Epoch 301/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 26.9822 - val_loss: 542.5273\n",
      "Epoch 302/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 26.8897 - val_loss: 543.1746\n",
      "Epoch 303/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 26.8016 - val_loss: 543.8097\n",
      "Epoch 304/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 26.7179 - val_loss: 544.4324\n",
      "Epoch 305/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 26.6384 - val_loss: 545.0434\n",
      "Epoch 306/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 26.5629 - val_loss: 545.6420\n",
      "Epoch 307/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 26.4911 - val_loss: 546.2286\n",
      "Epoch 308/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 26.4231 - val_loss: 546.8038\n",
      "Epoch 309/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 26.3584 - val_loss: 547.3669\n",
      "Epoch 310/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 26.2972 - val_loss: 547.9182\n",
      "Epoch 311/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 26.2391 - val_loss: 548.4581\n",
      "Epoch 312/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 26.1840 - val_loss: 548.9863\n",
      "Epoch 313/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 26.1319 - val_loss: 549.5035\n",
      "Epoch 314/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 26.0824 - val_loss: 550.0089\n",
      "Epoch 315/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 26.0356 - val_loss: 550.5033\n",
      "Epoch 316/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.9914 - val_loss: 550.9866\n",
      "Epoch 317/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 25.9495 - val_loss: 551.4589\n",
      "Epoch 318/500\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 25.9098 - val_loss: 551.9207\n",
      "Epoch 319/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.8723 - val_loss: 552.3715\n",
      "Epoch 320/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 25.8369 - val_loss: 552.8116\n",
      "Epoch 321/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.8034 - val_loss: 553.2413\n",
      "Epoch 322/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.7718 - val_loss: 553.6604\n",
      "Epoch 323/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.7420 - val_loss: 554.0696\n",
      "Epoch 324/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.7139 - val_loss: 554.4686\n",
      "Epoch 325/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.6873 - val_loss: 554.8575\n",
      "Epoch 326/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.6623 - val_loss: 555.2370\n",
      "Epoch 327/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.6387 - val_loss: 555.6064\n",
      "Epoch 328/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.6165 - val_loss: 555.9670\n",
      "Epoch 329/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.5954 - val_loss: 556.3174\n",
      "Epoch 330/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.5757 - val_loss: 556.6591\n",
      "Epoch 331/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.5571 - val_loss: 556.9916\n",
      "Epoch 332/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.5396 - val_loss: 557.3151\n",
      "Epoch 333/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.5232 - val_loss: 557.6299\n",
      "Epoch 334/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.5077 - val_loss: 557.9362\n",
      "Epoch 335/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.4932 - val_loss: 558.2341\n",
      "Epoch 336/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.4795 - val_loss: 558.5233\n",
      "Epoch 337/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.4667 - val_loss: 558.8046\n",
      "Epoch 338/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.4546 - val_loss: 559.0779\n",
      "Epoch 339/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.4433 - val_loss: 559.3432\n",
      "Epoch 340/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.4327 - val_loss: 559.6012\n",
      "Epoch 341/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.4228 - val_loss: 559.8512\n",
      "Epoch 342/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.4135 - val_loss: 560.0942\n",
      "Epoch 343/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.4047 - val_loss: 560.3300\n",
      "Epoch 344/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.3965 - val_loss: 560.5582\n",
      "Epoch 345/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.3888 - val_loss: 560.7800\n",
      "Epoch 346/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 25.3816 - val_loss: 560.9945\n",
      "Epoch 347/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.3749 - val_loss: 561.2026\n",
      "Epoch 348/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.3686 - val_loss: 561.4045\n",
      "Epoch 349/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 25.3627 - val_loss: 561.6000\n",
      "Epoch 350/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.3572 - val_loss: 561.7892\n",
      "Epoch 351/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.3520 - val_loss: 561.9721\n",
      "Epoch 352/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.3473 - val_loss: 562.1494\n",
      "Epoch 353/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.3427 - val_loss: 562.3211\n",
      "Epoch 354/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 25.3386 - val_loss: 562.4871\n",
      "Epoch 355/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.3346 - val_loss: 562.6474\n",
      "Epoch 356/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.3310 - val_loss: 562.8026\n",
      "Epoch 357/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.3276 - val_loss: 562.9523\n",
      "Epoch 358/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.3244 - val_loss: 563.0971\n",
      "Epoch 359/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.3215 - val_loss: 563.2371\n",
      "Epoch 360/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.3188 - val_loss: 563.3721\n",
      "Epoch 361/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.3162 - val_loss: 563.5028\n",
      "Epoch 362/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.3138 - val_loss: 563.6288\n",
      "Epoch 363/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.3116 - val_loss: 563.7504\n",
      "Epoch 364/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.3095 - val_loss: 563.8676\n",
      "Epoch 365/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.3076 - val_loss: 563.9805\n",
      "Epoch 366/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.3058 - val_loss: 564.0892\n",
      "Epoch 367/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.3042 - val_loss: 564.1943\n",
      "Epoch 368/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.3027 - val_loss: 564.2958\n",
      "Epoch 369/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 25.3012 - val_loss: 564.3929\n",
      "Epoch 370/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.3000 - val_loss: 564.4871\n",
      "Epoch 371/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2988 - val_loss: 564.5773\n",
      "Epoch 372/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2977 - val_loss: 564.6643\n",
      "Epoch 373/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2966 - val_loss: 564.7479\n",
      "Epoch 374/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2957 - val_loss: 564.8285\n",
      "Epoch 375/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2948 - val_loss: 564.9059\n",
      "Epoch 376/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2940 - val_loss: 564.9803\n",
      "Epoch 377/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2933 - val_loss: 565.0519\n",
      "Epoch 378/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2926 - val_loss: 565.1208\n",
      "Epoch 379/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2920 - val_loss: 565.1868\n",
      "Epoch 380/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2914 - val_loss: 565.2501\n",
      "Epoch 381/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2909 - val_loss: 565.3111\n",
      "Epoch 382/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2904 - val_loss: 565.3693\n",
      "Epoch 383/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2900 - val_loss: 565.4254\n",
      "Epoch 384/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2897 - val_loss: 565.4794\n",
      "Epoch 385/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2893 - val_loss: 565.5305\n",
      "Epoch 386/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2890 - val_loss: 565.5802\n",
      "Epoch 387/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2887 - val_loss: 565.6275\n",
      "Epoch 388/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 25.2885 - val_loss: 565.6728\n",
      "Epoch 389/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 25.2883 - val_loss: 565.7159\n",
      "Epoch 390/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2881 - val_loss: 565.7577\n",
      "Epoch 391/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2880 - val_loss: 565.7975\n",
      "Epoch 392/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2878 - val_loss: 565.8356\n",
      "Epoch 393/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2877 - val_loss: 565.8718\n",
      "Epoch 394/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2876 - val_loss: 565.9068\n",
      "Epoch 395/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2875 - val_loss: 565.9401\n",
      "Epoch 396/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2875 - val_loss: 565.9719\n",
      "Epoch 397/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 25.2875 - val_loss: 566.0023\n",
      "Epoch 398/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2875 - val_loss: 566.0315\n",
      "Epoch 399/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2875 - val_loss: 566.0592\n",
      "Epoch 400/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2875 - val_loss: 566.0858\n",
      "Epoch 401/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2875 - val_loss: 566.1112\n",
      "Epoch 402/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2875 - val_loss: 566.1350\n",
      "Epoch 403/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2876 - val_loss: 566.1584\n",
      "Epoch 404/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2877 - val_loss: 566.1804\n",
      "Epoch 405/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2877 - val_loss: 566.2016\n",
      "Epoch 406/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 25.2877 - val_loss: 566.2213\n",
      "Epoch 407/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2878 - val_loss: 566.2404\n",
      "Epoch 408/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2879 - val_loss: 566.2582\n",
      "Epoch 409/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2881 - val_loss: 566.2754\n",
      "Epoch 410/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2882 - val_loss: 566.2918\n",
      "Epoch 411/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2883 - val_loss: 566.3076\n",
      "Epoch 412/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2884 - val_loss: 566.3226\n",
      "Epoch 413/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2886 - val_loss: 566.3365\n",
      "Epoch 414/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 25.2887 - val_loss: 566.3503\n",
      "Epoch 415/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2889 - val_loss: 566.3632\n",
      "Epoch 416/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2890 - val_loss: 566.3752\n",
      "Epoch 417/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2892 - val_loss: 566.3865\n",
      "Epoch 418/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2893 - val_loss: 566.3977\n",
      "Epoch 419/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2895 - val_loss: 566.4080\n",
      "Epoch 420/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2896 - val_loss: 566.4177\n",
      "Epoch 421/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2898 - val_loss: 566.4268\n",
      "Epoch 422/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2900 - val_loss: 566.4358\n",
      "Epoch 423/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2901 - val_loss: 566.4439\n",
      "Epoch 424/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2904 - val_loss: 566.4516\n",
      "Epoch 425/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2906 - val_loss: 566.4593\n",
      "Epoch 426/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2907 - val_loss: 566.4665\n",
      "Epoch 427/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2908 - val_loss: 566.4733\n",
      "Epoch 428/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2911 - val_loss: 566.4794\n",
      "Epoch 429/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2912 - val_loss: 566.4855\n",
      "Epoch 430/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2914 - val_loss: 566.4909\n",
      "Epoch 431/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2916 - val_loss: 566.4961\n",
      "Epoch 432/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2918 - val_loss: 566.5012\n",
      "Epoch 433/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2920 - val_loss: 566.5060\n",
      "Epoch 434/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 25.2922 - val_loss: 566.5101\n",
      "Epoch 435/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2924 - val_loss: 566.5143\n",
      "Epoch 436/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2926 - val_loss: 566.5184\n",
      "Epoch 437/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2928 - val_loss: 566.5222\n",
      "Epoch 438/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 25.2930 - val_loss: 566.5255\n",
      "Epoch 439/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2932 - val_loss: 566.5287\n",
      "Epoch 440/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2934 - val_loss: 566.5319\n",
      "Epoch 441/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2936 - val_loss: 566.5345\n",
      "Epoch 442/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 25.2938 - val_loss: 566.5373\n",
      "Epoch 443/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2940 - val_loss: 566.5399\n",
      "Epoch 444/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2942 - val_loss: 566.5422\n",
      "Epoch 445/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2944 - val_loss: 566.5444\n",
      "Epoch 446/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2945 - val_loss: 566.5463\n",
      "Epoch 447/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2948 - val_loss: 566.5482\n",
      "Epoch 448/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2950 - val_loss: 566.5500\n",
      "Epoch 449/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2952 - val_loss: 566.5519\n",
      "Epoch 450/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2954 - val_loss: 566.5535\n",
      "Epoch 451/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2956 - val_loss: 566.5549\n",
      "Epoch 452/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2957 - val_loss: 566.5559\n",
      "Epoch 453/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2960 - val_loss: 566.5570\n",
      "Epoch 454/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2962 - val_loss: 566.5580\n",
      "Epoch 455/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2964 - val_loss: 566.5589\n",
      "Epoch 456/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2966 - val_loss: 566.5600\n",
      "Epoch 457/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2967 - val_loss: 566.5607\n",
      "Epoch 458/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2969 - val_loss: 566.5613\n",
      "Epoch 459/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2972 - val_loss: 566.5624\n",
      "Epoch 460/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2974 - val_loss: 566.5630\n",
      "Epoch 461/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2975 - val_loss: 566.5635\n",
      "Epoch 462/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2977 - val_loss: 566.5640\n",
      "Epoch 463/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2979 - val_loss: 566.5645\n",
      "Epoch 464/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2981 - val_loss: 566.5651\n",
      "Epoch 465/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2983 - val_loss: 566.5654\n",
      "Epoch 466/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2984 - val_loss: 566.5656\n",
      "Epoch 467/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 25.2986 - val_loss: 566.5657\n",
      "Epoch 468/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2988 - val_loss: 566.5663\n",
      "Epoch 469/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2990 - val_loss: 566.5665\n",
      "Epoch 470/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2992 - val_loss: 566.5668\n",
      "Epoch 471/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2993 - val_loss: 566.5670\n",
      "Epoch 472/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2995 - val_loss: 566.5670\n",
      "Epoch 473/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.2997 - val_loss: 566.5668\n",
      "Epoch 474/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.2999 - val_loss: 566.5670\n",
      "Epoch 475/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.3001 - val_loss: 566.5671\n",
      "Epoch 476/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.3002 - val_loss: 566.5670\n",
      "Epoch 477/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.3004 - val_loss: 566.5671\n",
      "Epoch 478/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 25.3006 - val_loss: 566.5670\n",
      "Epoch 479/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.3008 - val_loss: 566.5670\n",
      "Epoch 480/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.3009 - val_loss: 566.5670\n",
      "Epoch 481/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.3011 - val_loss: 566.5667\n",
      "Epoch 482/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.3012 - val_loss: 566.5665\n",
      "Epoch 483/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.3014 - val_loss: 566.5667\n",
      "Epoch 484/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.3016 - val_loss: 566.5663\n",
      "Epoch 485/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 25.3017 - val_loss: 566.5660\n",
      "Epoch 486/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.3019 - val_loss: 566.5658\n",
      "Epoch 487/500\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 25.3021 - val_loss: 566.5656\n",
      "Epoch 488/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 25.3022 - val_loss: 566.5654\n",
      "Epoch 489/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.3024 - val_loss: 566.5653\n",
      "Epoch 490/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 25.3025 - val_loss: 566.5651\n",
      "Epoch 491/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.3027 - val_loss: 566.5651\n",
      "Epoch 492/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.3028 - val_loss: 566.5648\n",
      "Epoch 493/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.3030 - val_loss: 566.5647\n",
      "Epoch 494/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.3031 - val_loss: 566.5645\n",
      "Epoch 495/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.3033 - val_loss: 566.5639\n",
      "Epoch 496/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.3034 - val_loss: 566.5636\n",
      "Epoch 497/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.3036 - val_loss: 566.5635\n",
      "Epoch 498/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.3037 - val_loss: 566.5633\n",
      "Epoch 499/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 25.3039 - val_loss: 566.5629\n",
      "Epoch 500/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 25.3040 - val_loss: 566.5626\n"
     ]
    }
   ],
   "source": [
    "C0 = tf.Variable(82.7524, name=\"C0\", trainable=True, dtype=tf.float32)\n",
    "K0 = tf.Variable(-0.0031, name=\"K0\", trainable=True, dtype=tf.float32)\n",
    "K1 = tf.Variable(-0.0003, name=\"K1\", trainable=True, dtype=tf.float32)\n",
    "a = tf.Variable(0.0000, name=\"a\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.0171, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(3.0230, name=\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "splitr = 0.8\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
    "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
    "    epsilon = 1\n",
    "    squared_difference3 = tf.square(\n",
    "        y_pred[:, 1] - (\n",
    "            y_pred[:, 0] * (\n",
    "                K0 - K1 * (\n",
    "                    9 * a * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c)**2 +\n",
    "                    4 * b * tf.math.log((y_pred[:, 0] + epsilon) / C0) / (K0 - K1 * c) + c\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dense(60))\n",
    "model.compile(loss=loss_fn, optimizer='adam')\n",
    "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJL101rPyuoT",
    "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 401ms/step\n"
     ]
    }
   ],
   "source": [
    "forecast_without_mc = forecastX\n",
    "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
    "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9dQELcJ8wbp",
    "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 251)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS2kyIKG1Kbr",
    "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0u6VIzaDyuoT"
   },
   "outputs": [],
   "source": [
    "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
    "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUEcw0LX07oU",
    "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 311)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_yhat_without_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "31OWVbSh_305"
   },
   "outputs": [],
   "source": [
    "fforecast = inv_yhat_without_mc[:,-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BlpGH2FOAiRF"
   },
   "outputs": [],
   "source": [
    "final_forecast = fforecast[:,0:300:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fforecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "CXkgkj_LBk_t"
   },
   "outputs": [],
   "source": [
    "# code to replace all negative value with 0\n",
    "final_forecast[final_forecast<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.75047386e+01, 5.74907330e+01, 5.74767274e+01, 5.74627218e+01,\n",
       "        5.74487162e+01, 5.74347105e+01, 5.74207050e+01, 5.74066993e+01,\n",
       "        5.73926937e+01, 5.73786881e+01, 5.73646825e+01, 5.73506769e+01,\n",
       "        5.73366713e+01, 6.29754435e+01, 6.29082166e+01, 6.28409897e+01,\n",
       "        6.27737628e+01, 6.27065360e+01, 6.25103408e+01, 6.23002568e+01,\n",
       "        6.20901727e+01, 6.18800887e+01, 6.16700047e+01, 6.14599206e+01,\n",
       "        6.12498366e+01, 6.10397526e+01, 6.08296685e+01, 6.06195845e+01,\n",
       "        6.04095005e+01, 6.01995332e+01, 6.00314659e+01, 5.98633987e+01,\n",
       "        5.96953315e+01, 5.95272642e+01, 5.93591970e+01, 5.91911298e+01,\n",
       "        5.90230626e+01, 5.88549953e+01, 5.86869281e+01, 5.85188609e+01,\n",
       "        3.11634690e-01, 1.22189420e-01, 0.00000000e+00, 6.02592350e-01,\n",
       "        0.00000000e+00, 2.02563290e-01, 0.00000000e+00, 6.10864379e+01,\n",
       "        6.08763539e+01, 6.06662698e+01, 6.04561858e+01, 6.02461018e+01,\n",
       "        6.00688142e+01, 5.99007470e+01, 5.97326797e+01, 5.95646125e+01,\n",
       "        5.93965453e+01, 5.92284781e+01, 5.90604108e+01, 5.88923436e+01,\n",
       "        5.87242764e+01, 5.85562092e+01, 5.83881419e+01, 5.82200747e+01,\n",
       "        5.81630019e+01, 5.81209851e+01, 5.80789683e+01, 5.80369514e+01,\n",
       "        5.79949346e+01, 5.79529178e+01, 5.79109010e+01, 5.78688842e+01,\n",
       "        5.78268674e+01, 6.61062164e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.14280570e+00, 1.37929660e-01,\n",
       "        5.66044693e+01, 4.29714561e-01, 3.76796424e-01, 3.47389132e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        6.02260232e-01, 1.50181502e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.86974573e-01, 0.00000000e+00, 1.83812842e-01, 3.23349893e-01,\n",
       "        0.00000000e+00, 9.63653088e-01, 2.73746312e-01, 4.81267124e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set)\n",
    "test = np.array(test)\n",
    "final_forecast = np.array(final_forecast.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([54.60630252, 54.60070028, 54.59509804, 54.5894958 , 54.58389356,\n",
       "       54.57829132, 54.57268908, 54.56708683, 54.56148459, 54.55588235,\n",
       "       54.55028011, 54.54467787, 54.53907563, 54.53347339, 54.52787115,\n",
       "       54.52226891, 54.51666667, 54.51106443, 54.50546218, 54.49985994,\n",
       "       54.4942577 , 54.48865546, 54.48305322, 54.47745098, 54.47184874,\n",
       "       54.4662465 , 54.46064426, 54.45504202, 54.44943978, 54.44383754,\n",
       "       54.43823529, 54.43263305, 54.42703081, 54.42142857, 54.41582633,\n",
       "       54.41022409, 54.40462185, 54.39901961, 54.39341737, 54.38781513,\n",
       "       54.38221289, 54.37661064, 54.3710084 , 54.36540616, 54.35980392,\n",
       "       54.35420168, 54.34859944, 54.3429972 , 54.33739496, 54.33179272,\n",
       "       54.32619048, 54.32058824, 54.31498599, 54.30938375, 54.30378151,\n",
       "       54.29726891, 54.28886555, 54.28046218, 54.27205882, 54.26365546,\n",
       "       54.2552521 , 54.24684874, 54.23844538, 54.23004202, 54.22163866,\n",
       "       54.21323529, 54.20483193, 54.19642857, 54.18802521, 54.17962185,\n",
       "       54.17121849, 54.16281513, 54.15441176, 54.1460084 , 54.13760504,\n",
       "       54.12920168, 54.12079832, 54.11239496, 54.1039916 , 54.09558824,\n",
       "       54.08718487, 54.07878151, 54.07037815, 54.06197479, 54.05357143,\n",
       "       54.04516807, 54.03676471, 54.02836134, 54.01995798, 54.01155462,\n",
       "       54.00315126, 53.9947479 , 53.98634454, 53.97794118, 53.96953782,\n",
       "       53.96113445, 53.95273109, 53.94432773, 53.93592437, 53.92752101])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.81901068346327\n",
      "20.732374863554682\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "rsme = math.sqrt(MSE)\n",
    "print(rsme)  \n",
    "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
    "mae = MAE\n",
    "print(mae)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
